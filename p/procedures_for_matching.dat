12|10000|Public
40|$|We {{present results}} {{for a test}} of a mixed action {{approach}} with Osterwalder-Seiler valence quarks on a Wilson twisted mass sea for the example η, η' mesons. Flavour singlet pseudoscalar mesons obtain significant contributions from disconnected diagrams and are, therefore, expected to be particularly sensitive to mixed regularisations. We employ different <b>procedures</b> <b>for</b> <b>matching</b> valence and sea quark actions and show that the results agree in the continuum limit...|$|E
40|$|Electron {{energy loss}} {{spectroscopy}} {{has become an}} indispensable tool in surface analysis Although the basic physics of this technique is well understood, instrument design has previously largely been left to intuition This book {{is the first to}} provide a comprehensive treatment of the electron optics involved in the production of intense monochromatic beams and the detection of scattered electrons It includes a full three-dimensional analysis of the electron optical properties of electron emission systems, monochromators and lens systems, placing particular emphasis on the <b>procedures</b> <b>for</b> <b>matching</b> the various components The description is kept mathematically simple and focuses on practical aspects, with many hints for writing computer codes to calculate and optimize electrostatic lens element...|$|E
40|$|This paper {{discusses}} a {{new approach}} to this problem that overcomes some of the weaknesses of other representations. This new approach, called the two-tiered (IT) concept representation, considers human concepts not as fixed, well-defined structures, but as flexible, context-modifiable and background-knowledge-dependent units of our knowledge. To represent concepts so understood, the TT view recommends splitting the total concept representation into two components ("tiers"), an explicit one that defines the most stable concept properties, and an implicit one, that handles rare or exceptional cases and context-dependency of a concept. The latter component consists of flexible <b>procedures</b> <b>for</b> <b>matching</b> the concepts with instances, and a body of rules for characterizing concept variability in different contexts and situation...|$|E
50|$|Conditional {{logistic}} regression {{is an extension}} of {{logistic regression}} that allows one to take into account stratification and matching. Its main field of application is observational studies and in particular epidemiology. It was designed in 1978 by Norman Breslow, Nicholas Day, K. T. Halvorsen, Ross L. Prentice and C. Sabai. It is the most flexible and general <b>procedure</b> <b>for</b> <b>matched</b> data.|$|R
40|$|In {{this paper}} a design {{procedure}} of parallel-connected LLC converters for PV powered car battery charging is presented. Matching {{the characteristics of}} the converters and the PV generator is shown to be critical to achieving efficient conversion. Experimental results are presented to demonstrate the efficiency of the proposed design procedure. The aim of this research is to investigate the design <b>procedure</b> <b>for</b> <b>matching</b> between the parameters of the converters and the photovoltaic generator. <br/...|$|R
40|$|In {{this paper}} {{we present a}} new Delaunay <b>procedure</b> <b>for</b> <b>matching</b> spots in 2 D-PAGE images. For each spot {{similarities}} are found in terms of geometric characteristic and intensity. For this reason in our procedure two phases are considered addressing separately geometrical matching and intensity matching. In comparison with other more complex matching tools our procedure is simple, rpbust and shows good accuracy and eciency. An example of an 2 D-PAGE images matching is also shown {{in order to better}} illustrate the procedure...|$|R
40|$|We {{calculate}} the leading and next-to-leading logarithmic resummed {{distribution for the}} jet broadening in deep inelastic scattering, {{as well as the}} power correction for both the distribution and mean value. A truncation of the answer at NLL accuracy, as is standard, leads to unphysical divergences. We discuss their origin and show how the problem can be resolved. We then examine DIS-specific <b>procedures</b> <b>for</b> <b>matching</b> to fixed-order calculations and compare our results to data. One of the tools developed for the comparison is an NLO parton distribution evolution code. When compared to PDF sets from MRST and CTEQ it reveals limited discrepancies in both. (orig.) Available from TIB Hannover: RA 2999 (01 - 160) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|Abstract: We are {{investigating}} how digital libraries {{can be used}} collaboratively by teams of students to engage in problem solving of school mathematics. We are experimenting with <b>procedures</b> <b>for</b> <b>matching</b> students {{who come to the}} mathforum. org digital library site to form effective collaborative learning online teams. Our hypothesis is that group learning will be greatest in groups that carefully combine diverse perspectives and backgrounds. We {{are investigating}} how to design curriculum, community practices and groupware support to foster deep learning of math. In the first half year of this fiveyear research project, we have generated the following pilot studies: 14 videotaped sessions in middle school and university classrooms and 33 online chat sessions among researchers, college students or K- 12 students. In analyzing these studies, we will embrace a diversity of learning science methodologies within a design research paradigm...|$|E
40|$|We {{introduce}} {{and study}} methods for inferring {{and learning from}} correspondences among neurons. The approach enables alignment of data from distinct multiunit studies of nervous systems. We show that the methods for inferring correspondences combine data effectively from cross-animal studies to make joint inferences about behavioral decision making that are not possible with the data from a single animal. We focus on data collection, machine learning, and prediction in the representative and long-studied invertebrate nervous system of the European medicinal leech. Acknowledging the computational intractability of the general problem of identifying correspondences among neurons, we introduce efficient computational <b>procedures</b> <b>for</b> <b>matching</b> neurons across animals. The methods include techniques that adjust for missing cells or additional cells in the different data sets that may reflect biological or experimental variation. The methods highlight the value harnessing inference and learning in new kinds of computational microscopes for multiunit neurobiological studies...|$|E
40|$|We present {{numerical}} <b>procedures</b> <b>for</b> {{analyzing the}} properties of periodic structures and associated couplers based upon time domain simulation. Simple post processing <b>procedures</b> are given <b>for</b> determining Brillouin diagrams and complex field distributions of the traveling wave solutions, and the reflection coefficient of the traveling waves by the input and output. The availability of the reflection coefficient information facilitates a systematic and efficient <b>procedure</b> <b>for</b> <b>matching</b> the input and output. The method has been extensively applied to coupler design {{for a wide variety}} of structures and to a study directed towards elimination of the surface field enhancement commonly experienced in coupler cells. Comment: 3 pages, 4 figures, LINAC Conference 200...|$|R
40|$|A role <b>for</b> {{sequential}} test <b>procedures</b> is emerging in genetic and epidemiological studies using banked biological resources. This {{stems from the}} methodology's potential for improved use of information relative to comparable fixed sample designs. Studies in which cost, time and ethics feature prominently are particularly suited to a sequential approach. In this paper sequential <b>procedures</b> <b>for</b> <b>matched</b> case–control studies with binary data will be investigated and assessed. Design issues such as sample size evaluation and error rates are identified and addressed. The methodology is illustrated and evaluated using both real and simulated data sets...|$|R
40|$|Abstract—We {{present a}} simple Markovian {{framework}} for mod-eling packet traffic with variability over several time scales. We present a fitting <b>procedure</b> <b>for</b> <b>matching</b> second-order properties of counts {{to that of}} a second-order self-similar process. Our models essentially consist of superpositions of two-state MMPP’s. We illustrate that a superposition of four two-state MMPP’s suffices to model second-order self-similar behavior over several time scales. Our modeling approach allows us to fit to addi-tional descriptors while maintaining the second-order behavior of the counting process. We use this to match interarrival time correlations. I...|$|R
40|$|Despite {{repeated}} driver {{identification of}} the need for increased information, the difference between a good traveller information service and a poor one depends on the driver rather than the level of information. Some drivers learn quickly and want more specific detail, others are overwhelmed, lack the processing ability to utilise given information or, misinterpret based on previous patterning. Design of an effective traveller information system necessitates the targeted provision of information sensitive to evolving driver capacity. This paper proposes that future work must proceed on twin fronts: developing effective techniques for categorising and profiling driver experience, preferred learning style and cultural personality type at entry and in evolution; and, developing <b>procedures</b> <b>for</b> <b>matching</b> content to the driver's processing capacity. Customised traveller information will become effective when it meets the current understanding and needs of the driver as an active learner whose information requirements change over time and from time to time. Further work in these areas will lead to better customisation of information for the driver...|$|E
40|$|We {{propose a}} CFD-based {{approach}} for the non-invasive hemodynamic assessment of pre- and post-operative coarctation of aorta (CoA) patients. Under our approach, the pressure gradient across the coarctation is determined from computational modeling based on physiological principles, medical imaging data, and routine non-invasive clinical measurements. The main constituents of our approach are a reduced-order model for computing blood flow in patientspecific aortic geometries, a parameter estimation procedure for determining patient-specific boundary conditions and vessel wall parameters from non-invasive measurements, and a comprehensive pressure-drop formulation {{coupled with the}} overall reduced-order model. The proposed CFD-based algorithm is fully automatic, requiring no iterative tuning <b>procedures</b> <b>for</b> <b>matching</b> the computed results to observed patient data, and requires approximately 6 - 8 minutes of computation time on a standard personal computer (Intel Core 2 Duo CPU, 3. 06 GHz), thus making it feasible {{for use in a}} clinical setting. The initial validation studies for the pressure-drop computations have been performed on four patient datasets with native or recurrent coarctation, by comparing the results with the invasively measured peak pressure gradients recorded during routine cardiac catheterization procedure. The preliminary results are promising, with a mean absolute error of less than 2 mmHg in all the patients...|$|E
40|$|The 0 / 1 primal {{separation}} problem is: Given {{an extreme}} point x ̄ of a 0 / 1 polytope P and some point x, find an inequality which is tight at x̄, violated by x and valid for P or assert that no such inequality exists. It {{is known that}} this separation variant {{can be reduced to}} the standard separation problem for P. We show that 0 / 1 optimization and 0 / 1 primal separation are polynomial time equivalent. This implies that the problems 0 / 1 optimization, 0 / 1 standard separation, 0 / 1 augmenta-tion, and 0 / 1 primal separation are polynomial time equivalent. Then we provide polynomial time primal separation <b>procedures</b> <b>for</b> <b>matching,</b> stable set, maximum cut, and maximum bipartite graph problems, giving evidence that these algorithms are conceptually simpler and easier to implement than their corresponding counterparts for standard separation. In particular, for perfect matching we present an algorithm for primal separation that rests only on simple max-flow computations. In contrast, the known standard separation method relies on an explicit minimum odd cut algorithm. Consequently, we obtain a very simple proof that a maximum weight perfect matching of a graph can be computed in polynomial time...|$|E
40|$|This paper {{addresses}} {{a number of}} issues related to the matched asymptotic expansions analysis of skip trajectories, or any class of problems that give rise to inner layers that are not associated directly with satisfying boundary conditions. The <b>procedure</b> <b>for</b> <b>matching</b> inner and outer solutions, and using the composite solution to satisfy boundary conditions is rigorously followed in developing a complete algebraic {{solution to the problem of}} inclination change with minimum energy loss. Repeated solution of these algebraic equations along the trajectory, treating each current state as an initial state, constitute a feedback guidance algorithm...|$|R
40|$|Abstract Objective: {{to compare}} record linkage (RL) {{procedures}} adopted in several Italian settings {{and a standard}} probabilistic RL <b>procedure</b> <b>for</b> <b>matching</b> data from electronic health care databases. Design: two health care archives are matched: the hospital discharges (HD) archive and the population registry of four Italian areas. Exact deterministic, stepwise deterministic techniques and a standard probabilistic RL procedure are applied to <b>match</b> HD <b>for</b> acute myocardial infarction (AMI) and diabetes mellitus. Sensitivity and specificity <b>for</b> RL <b>procedures</b> are estimated after manual review. Age and gender standardized annual hospitalization rates for AMI and diabetes are computed using different RL procedures and compared...|$|R
40|$|This work {{proposes a}} system for the {{automatic}} construction of multi-spectral three-dimensional (3 D) models of architecture. Besides the specific application, which concerns the interactive visualization and the restoration of historical buildings, {{the interest of the}} proposed system lies in the instrumental gap it fills in the multi-spectral nature of the textures, in general needed for rendering with faithful colors, and in the automatism of the 3 D model construction. The paper presents a robust <b>procedure</b> <b>for</b> <b>matching</b> 3 D points of architecture scenes and a new multiresolution method for texture generation. The proposed system is an effective tool for producing 3 D content amenable to a great number of usages...|$|R
40|$|Most real-life {{concepts}} are flexible • that is they lack precise defInitions and are context dependent. Representing and learning flexible concepts presents a fundamental challenge for ArtifIcial Intelligence. This paper describes {{a method for}} learning such concepts, {{which is based on}} a twotiered concept representation. In such a representation the flI'St tier, called the Base Concept Representation (BCR), describes the most relevant properties of a concept in an explicit, comprehensible, and efficient form. The second tier, called the Inferential Concept Interpreration (ICI), contains <b>procedures</b> <b>for</b> <b>matching</b> instances with concept descriptions. and inference rules defining allowable transformations of the concept under different contexts and exceptional cases. In the method. the BCR is obtained by first creating a complete and consistent concept description, and then optimizing it according to a general description quality criterion. The complete and consistent description is obtained by applying the AQ inductive learning methodology. The optimization process is done by a double level best fJrSt search. The lei is defmed in pan by a method of flexible matching and in pan by a set of inference rules. The method has been implemented in the AQTI- 15 learning system, and experimental results show that such a twotiered concept representation not only produces simpler concept descriptions, but may also increase their predictive power. Key words: two-tiered concept representation, inductive learning, simplification of concept descriptions. description quality measure, heuristic searc...|$|E
40|$|Identifying the {{infrared}} counterparts of X-ray sources in Galactic Plane {{fields such as}} those of the MYStIX project presents particular difficulties due to the high density of infrared sources. This high stellar density makes it inevitable that a large fraction of X-ray positions will have a faint field star close to them, which standard matching techniques may incorrectly take to be the counterpart. Instead we use {{the infrared}} data to create a model of both the field star and counterpart magnitude distributions, which we then combine with a Bayesian technique to yield a probability that any star is the counterpart of an X-ray source. In our more crowded fields, between 10 and 20 % of counterparts that would be identified on the grounds of being the closest star to X-ray position within a 99 % confidence error circle are instead identified by the Bayesian technique as field stars. These stars are preferentially concentrated at faint magnitudes. Equally importantly the technique also gives a probability that the true counterpart to the X-ray source falls beneath the magnitude limit of the infrared catalog. In deriving our method, we place {{it in the context of}} other <b>procedures</b> <b>for</b> <b>matching</b> astronomical catalogs. Comment: 12 pages, 6 figures. Accepted for publication in Astrophysical Journal Supplements (with 6 other MYStIX papers). Complete versions of all MYStIX papers are available at [URL]...|$|E
40|$|Identifying the {{infrared}} counterparts of X-ray sources in Galactic plane {{fields such as}} those of the MYStIX project presents particular difficulties due to the high density of infrared sources. This high stellar density makes it inevitable that a large fraction of X-ray positions will have a faint field star close to them, which standard matching techniques may incorrectly take to be the counterpart. Instead we use {{the infrared}} data to create a model of both the field star and counterpart magnitude distributions, which we then combine with a Bayesian technique to yield a probability that any star is the counterpart of an X-ray source. In our more crowded fields, between 10 % and 20 % of counterparts that would be identified on the grounds of being the closest star to an X-ray position within a 99 % confidence error circle are instead identified by the Bayesian technique as field stars. These stars are preferentially concentrated at faint magnitudes. Equally importantly the technique also gives a probability that the true counterpart to the X-ray source falls beneath the magnitude limit of the infrared catalog. In deriving our method, we place {{it in the context of}} other <b>procedures</b> <b>for</b> <b>matching</b> astronomical catalogs. © 2013. The American Astronomical Society. All rights reserved. The MYStIX project is supported at Penn State by NASA grant NNX 09 AC 74 G, NSF grant AST- 0908038, and the Chandra ACIS Team contract SV 4 - 74018 (G. Garmire & L. Townsley, Principal Investigators), issued by the Chandra X-ray Center, which is operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS 8 - 03060. This research made use of data products from the Chandra Data Archive and the Spitzer Space Telescope, which is operated by the Jet Propulsion Laboratory (California Institute of Technology) under a contract with NASA...|$|E
40|$|Traditional {{methods for}} partial {{envelope}} analysis run into {{difficulties in the}} presence of tremolo and noise. This paper investigates an alternative method of analysis that uses a variety of non-linear warping techniques to match a synthetic envelope template to the partial envelope. Generation of suitable templates is discussed and the variety of implementations for the warping algorithm is examined. A complete <b>procedure</b> <b>for</b> <b>matching</b> the template is then explained and justified. Application of this procedure to partial envelope analysis of flute sounds is described and the results presented demonstrate the best choice of warping implementation. This technique allows for better envelope segmentation thus enabling improved modelling in representations such as the Timbre Mode...|$|R
40|$|Abstract- This paper {{presents}} an efficient design <b>procedure</b> <b>for</b> Impedance <b>matching</b> of L band & S Band patch antennas for Navigational applications. Computation used in designing is {{the transmission line}} method as it offers good physical insight. The designs are simulated using “ADS Advanced Design System”. The paper presents simulated results for return loss, bandwidth & gain...|$|R
40|$|Active Appearance Models (AAMs) is a {{computer}} vision <b>procedure</b> <b>for</b> statistical <b>matching</b> of object shape and appearance between images. The present work will analyze how AAMs can be employed with RGB-Depth technology, so robots endowing a Kinect sensor can perceive and recognize humans more effectively in order to unequivocally identify them. This research is associated to the RoCKIn@Home challenge, an initiative from the European RoCKIn project focusing on domestic service robots...|$|R
40|$|The {{types of}} {{knowledge}} used during requirements acquisition are identified and {{a tool to}} aid in this process, ReqColl (Requirements Collector) is introduced. The tool uses conceptual graphs to represent domain concepts and attempts to recognise new concepts {{through the use of}} a matching facility. The overall approach to requirements capture is first described and the approach to matching illustrated informally. The detailed <b>procedure</b> <b>for</b> <b>matching</b> conceptual graphs is then given. Finally ReqColl is compared to similar work elsewhere and some future research directions indicated. 1. Introduction In making the transition from the informalities of the real world into the unambiguous, clearly-defined representations which are needed for computer manipulation, various types of knowledge must be used. Problems arise from the insufficient knowledge which a software designer commonly has about the application domain of the required software. Conversely the end user, or client, often has limit [...] ...|$|R
40|$|The exact {{representation}} of a two-electron wave function near the origin is the Fock expansion, i. e., a double summation over powers of R and of lnR [where R≡(r 12 +r 22) 1 / 2] with coefficients dependent on the five remaining angular variables. Using a {{representation of}} hyperspherical harmonics, we present here the first numerical solution of the equations for the Fock coefficients. We present also a general <b>procedure</b> <b>for</b> <b>matching</b> a linear combination of Fock-series solutions onto a basis of adiabatic hyperspherical functions at a matching radius R 0. This matching procedure ensures that the proper asymptotic boundary conditions are satisfied. Exploratory numerical results are presented for 1 S wave functions of He and H- in which four Fock-series solutions are matched onto the lowest 1 S adiabatic hyperspherical wave function at a matching radius near the first antinode in the adiabatic wave function...|$|R
40|$|Language {{for number}} is an {{important}} case study {{of the relationship between}} language and cognition because the mechanisms of non-verbal numerical cognition are well-understood. When the Pirahã (an Amazonian hunter-gatherer tribe who have no exact number words) are tested in non-verbal numerical tasks, they are able to perform one-to-one matching tasks but make errors in more difficult tasks. Their pattern of errors suggests that they are using analog magnitude estimation, an evolutionarily- and developmentally-conserved mechanism for estimating quantities. Here we show that English-speaking participants rely on the same mechanisms when verbal number representations are unavailable due to verbal interference. Followup experiments demonstrate that the effects of verbal interference are primarily manifest during encoding of quantity information, and—using a new <b>procedure</b> <b>for</b> <b>matching</b> difficulty of interference tasks for individual participants—that the effects are restricted to verbal interference. These results are consistent with the hypothesis that number words are used online to encode, store, and manipulate numerical information. This linguistic strategy complements, rather than altering or replacing, non-verbal representations...|$|R
40|$|The use of {{biometrics}} is {{an evolving}} component in today's society. Fingerprint recognition {{continues to be}} one of the most widely used biometric systems. This thesis explores the various steps present in a fingerprint recognition system. The study develops a working algorithm to extract fingerprint minutiae from an input fingerprint image. This stage incorporates a variety of image pre-processing steps necessary for accurate minutiae extraction and includes two different methods of ridge thinning. Next, it implements a <b>procedure</b> <b>for</b> <b>matching</b> sets of minutiae data. This process goes through all possible alignments of the datasets and returns the <b>matching</b> score <b>for</b> the best possible alignment. Finally, it conducts a series of matching experiments to compare the performance of the two different thinning methods considered. Results show that thinning by the central line method produces better False Non-match Rates and False Match Rates than those obtained through thinning by the block filter method. US Navy (USN) author...|$|R
5000|$|From 1977 through 1984 the NASL had a {{variation}} of the penalty shoot-out <b>procedure</b> <b>for</b> tied <b>matches.</b> The shoot-out started 35 yards from the goal and allowed the player 5 seconds to attempt a shot. The player could make as many moves as he wanted in a breakaway situation within the time frame. NASL procedure during this era called for the box score to show an additional [...] "goal" [...] given to the winning side of a shoot-out.|$|R
5000|$|...From 1977 through 1984 the NASL had a {{variation}} of the penalty shoot-out <b>procedure</b> <b>for</b> tied <b>matches.</b> The shoot-out started 35 yards from the goal and allowed the player 5 seconds to attempt a shot. The player could make as many moves as he wanted in a breakaway situation within the time frame. Even though this particular match was a scoreless tie after overtime, NASL <b>procedure</b> also called <b>for</b> the box score to show an additional [...] "goal" [...] given to the winning team.|$|R
50|$|Global was disqualified {{from the}} UFL Cup for fielding Satoshi Ōtomo as a Filipino player and ruled {{the club has}} {{violated}} the UFL's five-foreigner-rule. Matches of Global were originally decided to be forfeited after complaints from Ceres La-Salle FC, Kaya FC, Stallion FC, and Pachanga Diliman FC but the decision was overturn after the Appeals Committee ruled that the complainants did not follow proper <b>procedure</b> <b>for</b> reversing <b>match</b> outcomes. Last placers, Manila Nomads qualifies for the next round as a result.|$|R
5000|$|...From 1977 through 1984 the NASL had a {{variation}} of the penalty shoot-out <b>procedure</b> <b>for</b> tied <b>matches.</b> The shoot-out started 35 yards from the goal and allowed the player 5 seconds to attempt a shot. The player could make as many moves as he wanted in a breakaway situation within the time frame. Even though this particular match was a goalless draw after extra time, NASL <b>procedure</b> also called <b>for</b> the box score to show an additional [...] "goal" [...] given to the winning side of a shoot-out.|$|R
40|$|Abstract—In this paper, {{we present}} a robust image {{alignment}} algorithm based on matching of relative gradient maps. This algorithm consists of two stages; namely, a learning-based approximate pattern search and an iterative energy-minimization <b>procedure</b> <b>for</b> <b>matching</b> relative image gradient. The first stage finds some candidate poses of the pattern from the image through a fast nearestneighbor search of the best match of the relative gradient features computed from training database of feature vectors, which are obtained from the synthesis of the geometrically transformed template image with the transformation parameters uniformly sampled from a given transformation parameter space. Subsequently, the candidate poses are further verified and refined by matching the relative gradient images through an iterative energy- minimization procedure. This approach based on the matching of relative gradients is robust against nonuniform illumination variations. Experimental results on both simulated and real images are shown to demonstrate superior efficiency and robustness of the proposed algorithm over the conventional normalized correlation method. Index Terms—Energy minimization, illumination variations, image alignment, image matching, industrial inspection, nearestneighbor search, robust image matching. I...|$|R
5000|$|... a Global FC {{tops the}} group but disqualified from the UFL Cup for fielding Satoshi Ōtomo as a Filipino player and ruled {{the club has}} {{violated}} the UFL's five-foreigner-rule. Matches of Global were originally decided to be forfeited after complaints from Ceres La-Salle FC, Kaya FC, Stallion FC, and Pachanga Diliman FC but the decision was overturn after the Appeals Committee ruled that the complainants did not follow proper <b>procedure</b> <b>for</b> reversing <b>match</b> outcomes. Last placers, Manila Nomads qualifies for the next round as a result.|$|R
40|$|We {{provide a}} simple system, based on {{transformation}} rules, which is complete for certain classes of semantic matching problem, where the equational theory {{with respect to}} which the semantic matching is performed has a convergent rewrite system. We also use this transformation SySteM to describe decision <b>procedures</b> <b>for</b> semantic <b>matching</b> problems. We give counterexamples to show that semantic matching becomes undecidable (as it generally is) when the conditions we give are weakened. Our main result pertains to convergent systems with variable preserving rules, with some particular patterns of defined functions on the right hand sides...|$|R
40|$|Number Verbal {{interference}} Cross-linguistic differences a b s t r a c t Language {{for number}} {{is an important}} case study {{of the relationship between}} language and cognition because the mechanisms of non-verbal numerical cognition are well-understood. When the Pirahã (an Amazonian hunter-gatherer tribe who have no exact number words) are tested in non-verbal numerical tasks, they are able to perform one-to-one matching tasks but make errors in more diffi-cult tasks. Their pattern of errors suggests that they are using analog magnitude estimation, an evolutionarily- and developmen-tally-conserved mechanism for estimating quantities. Here we show that English-speaking participants rely on the same mecha-nisms when verbal number representations are unavailable due to verbal interference. Followup experiments demonstrate that the effects of verbal interference are primarily manifest during encoding of quantity information, and—using a new <b>procedure</b> <b>for</b> <b>matching</b> difficulty of interference tasks for individual partici-pants—that the effects are restricted to verbal interference. These results are consistent with the hypothesis that number words are used online to encode, store, and manipulate numerical informa-tion. This linguistic strategy complements, rather than altering or replacing, non-verbal representations. ! 2011 Elsevier Inc. All rights reserved. 1...|$|R
