94|724|Public
25|$|Optimising the <b>precision</b> <b>parameters</b> {{corresponds}} to optimising the gain of prediction errors (c.f., Kalman gain). In neuronally plausible implementations of predictive coding, this {{corresponds to}} optimising the excitability superficial pyramidal cells {{and has been}} interpreted in terms of attentional gain.|$|E
5000|$|The {{appearance}} of each feature {{is represented by}} a point in appearance space (discussed below in implementation). [...] "Each part [...] in the constellation model has a Gaussian density within this space with mean and <b>precision</b> <b>parameters</b> [...]" [...] From these the appearance likelihood described above is computed {{as a product of}} Gaussians over the model parts for a give hypothesis h and mixture component [...]|$|E
40|$|In {{this paper}} {{we present a}} new Bayesian {{methodology}} {{for the restoration of}} blurred and noisy images. Bayesian methods rely on image priors that encapsulate prior image knowledge and avoid the ill-posedness of image restoration problems. We use a spatially varying image prior utilizing a Gamma-Normal hyperprior distribution on the local <b>precision</b> <b>parameters.</b> This kind of hyperprior distribution, which to our knowledge has not been used before in image restoration, allows for the incorporation of information on local as well as global image variability, models correlation of the local <b>precision</b> <b>parameters</b> and is a conjugate hyperprior to the image model used in the paper. The proposed restoration technique is compared with other image restoration approaches, demonstrating its improved performance...|$|E
40|$|Despite their diverse {{applications}} in many domains, the variable precision rough sets (VPRS) model lacks a feasible method {{to determine a}} <b>precision</b> <b>parameter</b> ([beta]) value to control the choice of [beta]-reducts. In this study we propose an effective method to find the [beta]-reducts. First, we calculate a <b>precision</b> <b>parameter</b> value to find the subsets of information system {{that are based on}} the least upper bound of the data misclassification error. Next, we measure the quality of classification and remove redundant attributes from each subset. We use a simple example to explain this method and even a real-world example is analyzed. Comparing the implementation results from the proposed method with the neural network approach, our proposed method demonstrates a better performance. VPRS model [beta]-reduct <b>Precision</b> <b>parameter</b> Neural networks...|$|R
5000|$|In Bayesian statistics, the Wishart {{distribution}} is a conjugate prior for the <b>precision</b> <b>parameter</b> of the multivariate normal distribution, when the mean parameter is known.|$|R
40|$|The {{specification}} of prior parameters {{is a common}} practical problem when implementing Bayesian approaches to portfolio optimization. The <b>precision</b> <b>parameter</b> of the prior on the expected asset returns reflects {{the confidence of the}} investor in the prior knowledge. Within the framework of the normal-inverse-Wishart model, this paper investigates which factors drive this confidence in order to deduce reasonable values of the <b>precision</b> <b>parameter.</b> We recommend that the investor concentrates on the {{specification of}} the <b>precision</b> <b>parameter.</b> By contrast, experts should assess the values of the prior location and dispersion parameter. In {{the second part of the}} paper, the impact of the investor's confidence on the performance of investment strategies is examined by a simulation study. The study focusses less on detecting superior portfolio strategies, and more on providing a sensitivity analysis for different levels of confidence. In addition, we show how the posterior distribution of the normal-inverse-Wishart model can be used as a starting point of a simulation process...|$|R
40|$|This paper {{summarizes}} the beta regression models, with joint modeling {{of the mean}} and <b>precision</b> <b>parameters,</b> and the Bayesian methodology proposed by Cepeda (2001) and Cepeda and Gamenrman (2005) to fit these models. This Bayesian methodology is implemented and applied {{in the development of}} simulated and applied studies...|$|E
40|$|The last {{multiannual}} Community {{program for}} the collection, management and use of data in the fisheries sector (Commission Decision 2008 / 949 /EC) stated the provision of precision levels and sampling intensities of the estimates at national level. However, the unequal compliance of this standard has hindered its application in stock assessment and the consequent scientific advice. The cost-benefit analysis of a sampling program, besides addressing logistical and economic constraints, should deepen {{the potential of the}} tools currently available. This article proposes to test the calculation, provision and use in stock assessment of extensively collected <b>precision</b> <b>parameters.</b> First, sampling intensities and coefficients of variation of fisheries-dependent parameters are calculated using the COST software, a statistical tool specifically designed to quantify uncertainty in marine sampled data. Secondly, alternative ways are explored to provide <b>precision</b> <b>parameters</b> to the stock assessment coordinators by using InterCatch, the existing ICES web-based system to submit national data and compile international catch matrices. Finally, the incorporation of these <b>precision</b> <b>parameters</b> in the assessment model is tested, through a stock assessed by statistical assessment models (such as SS 3) which can account for sampling errors. Thus, {{it will be possible to}} quantify how errors in input data propagate through stock assessment models to affect harvest rules, and also to help identify the most cost-effective data collections that adequately support the advisory process...|$|E
40|$|The {{results of}} the {{development}} of a portable logical generator/analyzer to be used in an automated hardware and software measurement system for automation of research and testing of TID hardness of digitally controlled RF ICs, are provided. The device has been tested in research of <b>precision</b> <b>parameters</b> of integrated multi-bit RF vector phase shifters...|$|E
3000|$|... 95 = 7.0 °, and k (<b>precision</b> <b>parameter)</b> = 23.9 by {{averaging}} 19 (2 normal and 17 reverse) site-mean directions satisfying the condition n > 2 (where n {{is the number}} of samples) and α [...]...|$|R
3000|$|Δ, where n∈Z^+ and Δ is the {{interval}} size, i.e. a <b>precision</b> <b>parameter.</b> We will refer to instantaneous quantities using the integer value n henceforth. We {{assume that the}} relay node (e.g. satellite) simply forwards the packet without any processing. Therefore, R [...]...|$|R
30|$|Geo-hydraulic {{evaluation}} tools like Oda’s {{procedure of}} permeability determination contain arbitrary correction factors which shoot down their <b>precision.</b> <b>Parameter</b> {{variations in the}} numerical models, however, are still instructive, as they can limit the unknown quantities by comparison of the resulting seepage with the accomplished measurements.|$|R
30|$|Bayesian {{models are}} {{estimated}} using jagsUI in R. Normal priors with mean 0 and variance 1000 are assumed on regression parameters, and gamma priors, with shape 1 and inverse scale (rate) 0.001, are assumed on <b>precision</b> <b>parameters</b> and the HQRPLN parameter δ. Two chains are used, with convergence assessed using Brooks-Gelman-Rubin scale reduction factors (Brooks and Gelman, 1998).|$|E
40|$|In {{this paper}} joint mean and {{variance}} beta regression models are proposed. The proposed models are fitted applying Bayesian methodology and assuming normal prior {{distribution for the}} regression parameters. An analysis of structural and real data is included, assuming the proposed model, together with {{a comparison of the}} result obtained assuming joint modeling of the mean and <b>precision</b> <b>parameters...</b>|$|E
40|$|I {{compare the}} tree level {{estimate}} of the electro-weak <b>precision</b> <b>parameters</b> in two (exactly solvable) toy models of dynamical symmetry breaking in which the strong dynamics {{is assumed to be}} described by a five-dimensional (weakly coupled) gravity dual. I discuss the effect of brane-localized kinetic terms, their use as regulators for the couplings of otherwise non-normalizable modes, and the impact of a large deviation from its natural value for the scaling dimension of the background field responsible for spontaneous symmetry breaking. The latter is assumed to model the effects of walking dynamics, i. e. of a large anomalous dimension of the chiral condensate, it has a strong impact of the spectrum of spin- 1 fields and, as a consequence, on the electro-weak <b>precision</b> <b>parameters.</b> The main conclusion is that models of dynamical symmetry breaking based on a large-Nc strongly interacting SU(Nc) gauge theory are compatible with precision electro-weak constraints, and produce a very distinctive signature testable at the LHC. Some of the considerations discussed are directly relevant for analogous models in the context of AdS-QCD. Comment: 40 page...|$|E
40|$|An obvious Bayesian nonparametric {{generalization}} of ridge regression assumes that coefficients are exchangeable, from a prior distribution of unknown form, which {{is given a}} Dirichlet process prior with a normal base measure. The {{purpose of this paper}} is to explore predictive performance of this generalization, which does not seem to have received any detailed attention, despite related applications of the Dirichlet process for shrinkage estimation in multivariate normal means, analysis of randomized block experiments and nonparametric extensions of random effects models in longitudinal data analysis. We consider issues of prior specification and computation, as well as applications in penalized spline smoothing. With a normal base measure in the Dirichlet process and letting the <b>precision</b> <b>parameter</b> approach infinity the procedure is equivalent to ridge regression, whereas for finite values of the <b>precision</b> <b>parameter</b> the discreteness of the Dirichlet process means that some predictors can be estimated as having the same coefficient. Estimating the <b>precision</b> <b>parameter</b> from the data gives a flexible method for shrinkage estimation of mean parameters which can work well when ridge regression does, but also adapts well to sparse situations. We compare our approach with ridge regression, the lasso and the recently proposed elastic net in simulation studies and also consider applications to penalized spline smoothing. ...|$|R
40|$|We {{define the}} {{algorithmic}} complexity of a quantum state {{relative to a}} given <b>precision</b> <b>parameter,</b> and give upper bounds for various examples of states. We also establish {{a connection between the}} entanglement of a quantum state and its algorithmic complexity. Comment: 4 pages. Replaced with published versio...|$|R
40|$|Table 1. Paleomagnetic {{results by}} site. The {{positions}} of paleomagnetic sites are indicated in centimeters (Fig. 4). Sites are classified according to Johnson's system (16) : class 1 if <b>precision</b> <b>parameter</b> k (15) {{was greater than}} 10, class 2 if k was less than 10, and class 3 if one specimen was used...|$|R
40|$|In this paper, the {{simultaneous}} {{estimation of the}} <b>precision</b> <b>parameters</b> of k normal distributions is considered under the squared loss function in a decision-theoretic framework. Several classes of minimax estimators are derived by using the chi-square identity, and the generalized Bayes minimax estimators are developed out of the classes. It is also shown that the improvement on the unbiased estimators {{is characterized by the}} superharmonic function. This corresponds to Stein (1981) 's result in simultaneous estimation of normal means...|$|E
40|$|In {{this work}} we propose a new {{discrete}} probability distribution useful when {{we work with}} ordered categorical data. The discrete-beta distribution has a highly flexible shape {{and it can be}} either over-dispersed or under-dispersed with respect to the binomial distribution. It has only two parameters, which have a very clear interpretation: the mean and the <b>precision</b> <b>parameters</b> of the beta latent variable. Adding directly covariates on parameters (according to CUB model framework), it is very suitable to regressio...|$|E
40|$|In this paper, we {{consider}} Gaussian models Markov {{with respect to}} an arbitrary DAG. We first construct a family of conjugate priors for the Cholesky parametrization of the covariance matrix of such models. This family has as many shape parameters as the DAG has vertices, and naturally extends the work of Geiger and Heckerman [8]. From these distributions, we derive prior distributions for the covariance and <b>precision</b> <b>parameters</b> of the Gaussian DAG Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and Letac and Massam [16] for Gaussian models Markov {{with respect to a}} decomposable graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart distributions. An advantage of these distributions is that they possess strong hyper Markov properties and thus allow for explicit estimation of the covariance and <b>precision</b> <b>parameters,</b> regardless of the dimension of the problem. They also allow us to develop methodology for model selection and covariance estimation in the space of DAG-Markov models. We demonstrate via several numerical examples that the proposed method scales well to high-dimensions. Comment: 55 pages, 8 figures, 12 tabl...|$|E
40|$|Within an AdS/CFT {{inspired}} {{model of}} electroweak symmetry breaking {{the effects of}} various boundary terms and modifications to the background are studied. The effect on the S <b>precision</b> <b>parameter</b> is discussed with particular attention to its sign and whether the theory is unitary when S< 0. Connections between the various possible AdS slice models of symmetry breaking are discussed...|$|R
40|$|In {{hierarchical}} mixture models the Dirichlet {{process is}} used to specify latent patterns of heterogeneity, particularly when the distribution of latent param-eters {{is thought to be}} clustered (multimodal). The parameters of a Dirichlet process include a <b>precision</b> <b>parameter</b> α and a base probability measure G 0. In problems where α is unknown and must be estimated, inferences about the level of clustering can be sensitive to the choice of prior assumed for α. In this paper an approach is developed for computing a prior for the <b>precision</b> <b>parameter</b> α {{that can be used in}} the presence or absence of prior information about the level of clustering. This approach is illustrated in an analysis of counts of stream fishes. The results of this fully Bayesian analysis are compared with an empir-ical Bayes analysis of the same data and with a Bayesian analysis based on an alternative commonly used prior...|$|R
40|$|The paper {{proposes a}} digital circuit {{design for the}} logistic-map module used in chaotic stream ciphers, {{analyzes}} the factors that may affect {{the output of the}} sequences, and develops a calculation method for estimating the output sequential correlative peak interval. With the respective tests using different initial values, the values of parameter u and the computational precisions, extensive experiments   have   been   carried   out.    A   formula   for calculating correlative peak interval is proposed. Moreover, the relationships among <b>precision,</b> <b>parameter</b> u and correlative peak interval is provided. To ensure the security of the plaintext which is encrypted by the output sequence of the logistic-map, a proper precision could be chosen according to the formula. It provides a theoretic basis for the actual application of the chaos cryptology. The basic theory and methods have a significant implication on the statistical analysis and practical applications of the digital chaotic sequences. A diagram that presents the relationship among <b>precision,</b> <b>parameter</b> u and correlative peak interval has been generated for analysis. </p...|$|R
40|$|The paper {{deals with}} {{analysis}} of optimal control problems arising in models of economic growth. The Pontryagin maximum principle is applied {{for analysis of}} the optimal investment problem. Specifically, the research is based on existence results and necessary conditions of optimality in problems with infinite horizon. Properties of Hamiltonian systems are examined for different regimes of optimal control. The existence and uniqueness result is proved for a steady state of the Hamiltonian system. Analysis of properties of eigenvalues and eigenvectors is completed for the linearized system in a neighborhood of the steady state. Description of behavior of the nonlinear Hamiltonian system is provided {{on the basis of}} results of the qualitative theory of differential equations. This analysis allows us to outline proportions of the main economic factors and trends of optimal growth in the model. A numerical algorithm for construction of optimal trajectories of economic growth is elaborated on the basis of constructions of backward procedures and conjugation of an approximation linear dynamics with the nonlinear Hamiltonian dynamics. High order precision estimates are obtained for the proposed algorithm. These estimates establish connection between <b>precision</b> <b>parameters</b> in the phase space and <b>precision</b> <b>parameters</b> for functional indices...|$|E
40|$|The {{research}} {{is devoted to}} analysis of optimal control problems arising in models of economic growth. The Pontryagin maximum principle is applied for analysis of the optimal investment problem. Specifically, the {{research is}} based on existence results and necessary conditions of optimality in problems with infinite horizon. Properties of Hamiltonian systems are examined for different regimes of optimal control. The existence and uniqueness result is proved for a steady state of the Hamiltonian system. Analysis of properties of eigenvalues and eigenvectors is completed for the linearized system in a neighborhood of the steady state. Description of behavior of the nonlinear Hamiltonian system is provided {{on the basis of}} results of the qualitative theory of differential equations. This analysis allows us to outline proportions of the main economic factors and trends of optimal growth in the model. A numerical algorithm for construction of optimal trajectories of economic growth is elaborated on the basis of constructions of backward procedures and conjugation of an approximation linear dynamics with the nonlinear Hamiltonian dynamics. High order precision estimates are obtained for the proposed algorithm. These estimates establish connection between <b>precision</b> <b>parameters</b> in the phase space and <b>precision</b> <b>parameters</b> for functional indices. The results of numerical experiments illustrating algorithm's constructions are given for real data of US and Japan economies...|$|E
40|$|A novel {{method for}} extracting {{physical}} parameters from experimental and simulation data is presented. The method {{is based on}} statistical concepts and it relies on Monte Carlo simulation techniques. It identifies and determines with maximal <b>precision</b> <b>parameters</b> that {{are sensitive to the}} data. The method has been extensively studied and it is shown to produce unbiased results. It is applicable {{to a wide range of}} scientific and engineering problems. It has been successfully applied in the analysis of experimental data in hadronic physics and of lattice QCD correlators. Comment: 4 pages, 2 Figure...|$|E
40|$|In this paper, we derive {{a simple}} matrix formula for second-order covariances of maximum {{likelihood}} estimates in generalized linear models. The formula covers many important and commonly used models {{and is also}} simple enough to be used algebraically to obtain closed-form expressions in special models. The practical use of this formula is illustrated in a simulation study. Asymptotic Expansion Canonical Model Link function <b>Precision</b> <b>parameter</b> Variance function...|$|R
40|$|Confidence {{limits are}} {{calculated}} for the <b>precision</b> <b>parameter</b> K used {{in the analysis of}} palaeomagnetic data and for the angular standard deviation 0. A set of tables for 95 per cent and 99 per cent confidence limits is presented. Most analyses of palaeomagnetic data are made using the statistical method of Fisher (1953) in which magnetic vectors are represented by unit vectors with angular probability density K exp (K cos 6) 4 n sinh K where 6 is the angle from the mean direction and K is a <b>precision</b> <b>parameter</b> which describes the dispersion of the vectors. In most palaeomagnetic research the main objective of the statistical analysis is to determine the mean direction of the vectors and a confidence limit about this mean. In recent work there has been increased interest in the <b>precision</b> <b>parameter</b> IC itself. If the individual vectors determined palaeomagnetically correspond to individual directions of the Earth's magnetic field in the past, the parameter K then provides an inverse measure of the angular dispersion produced by geomagnetic secular variation. A need has developed for calculating confidence limits in order to determine whether observed values of K are consistent with different models to describe variations of IC with latitude (Creer 1962; Cox 1962; Irving & Ward 1964). An analysis of variance may be used similar to that employed by Watson (1956) and Watson & Irving (1957) to develop numerous other useful statistical tests. The best estimate k of K if the true polar angle is unknown is, for K> 3, N- 1 k=-N- R where N is the number of unit vectors and R the length of their vector sum. For large K, the statisti...|$|R
30|$|Finally, to {{investigate}} the influence of sampling frequency on the fit <b>parameters</b> and fitting <b>precision</b> (<b>parameter</b> standard deviations), IF sampling intervals were prolonged from the experimental 1  s to 30  s and 60  s, respectively, by deleting the data between these time points from both the experimental and above simulated noise-containing IFs. Kinetic modeling was performed with the identical simulated noise-containing TACs as used for the complete IF datasets.|$|R
40|$|This master’s thesis {{deals with}} {{technology}} of wire {{electrical discharge machining}} in theoretical and practical level. Theoretical part of the thesis explains in detail the principle of electrical discharge machining, describes functional parts and settings of a current wire EDM machines and also the possibility of using method for production of specific parts. Practical part of the thesis solves manufacturing of gearing on pinion manufactured by wire cutter EXCETEK V 650 and statistically evaluates <b>precision</b> <b>parameters</b> on surfaces of the carriers taken by specific technological conditions with the same machine...|$|E
40|$|The Higgsless {{model in}} warped extra {{dimension}} is reexamined. Dirichlet boundary {{conditions on the}} TeV brane are replaced with Robin boundary conditions which are parameterized by a mass parameter M. We calculate the Peskin-Takeuchi <b>precision</b> <b>parameters</b> S, T and U at tree level. We find that to satisfy the constraints on the <b>precision</b> <b>parameters</b> at 99 % [95 %] confidence level (CL) the first Kaluza-Klein excited Z boson, Z', should be heavier than 5 TeV [8 TeV]. The Magnitude of M, which is infinitely large in the original model, should be smaller than 200 GeV (70 GeV) for the curvature of the warped space R^- 1 = 10 ^ 16 GeV (10 ^ 8 GeV) at 95 % CL. If the Robin boundary conditions are induced by the mass terms localized on the TeV brane, from the 99 % [95 %] bound {{we find that the}} brane mass interactions account for more than 97 % [99 %] of the masses of Z and W bosons. Such a brane mass term is naturally interpreted as a vacuum expectation value of the Higgs scalar field in the standard model localized on the TeV brane. If so, the model can be tested by precise measurements of HWW, HZZ couplings and search for 1 st Kaluza-Klein excited states. Comment: 22 pages, 1 table, 3 figure...|$|E
40|$|Once the Higgs boson {{will have}} been discovered, the {{determination}} of the Higgs boson parameters is a major goal. Precision measurements of these parameters allow {{a deeper understanding of the}} electroweak symmetry-breaking mechanism. They may also be useful to distinguish a Standard Model Higgs boson from a MSSM Higgs boson. In the present note it has been studied with what <b>precision</b> <b>parameters</b> like the Higgs boson mass, production rates, couplings to bosons and fermions, total width and spin can be measured in the ATLAS experiment. The results are given for integrated luminosities of 100 $fb^{- 1 }$ and 300 $fb^{- 1 }$...|$|E
40|$|Waveforms known {{well enough}} to detect some EMRIs today. Soon, enough to realize Gair et al {{estimate}} of approx. 100 's to 1000 's of detections to z = 1. Not yet enough to for <b>precision</b> <b>parameter</b> estimation of Barack and Cutler (mass and spin to 10 (exp - 4)). Some turning to the more exotic: non-Kerr background, gas interaction, third body, [...] . More status and refs: Drasco, gr-qc/ 0604115...|$|R
40|$|Abstract—The paper {{proposes a}} digital circuit {{design for the}} logistic-map module used in chaotic stream ciphers, {{analyzes}} the factors that may affect {{the output of the}} sequences, and develops a calculation method for estimating the output sequential correlative peak interval. With the respective tests using different initial values, the values of parameter u and the computational precisions, extensive experiments have been carried out. A formula for calculating correlative peak interval is proposed. Moreover, the relationships among <b>precision,</b> <b>parameter</b> u and correlative peak interval is provided. To ensure the security of the plaintext which is encrypted by the output sequence of the logistic-map, a proper precision could be chosen according to the formula. It provides a theoretic basis for the actual application of the chaos cryptology. The basic theory and methods have a significant implication on the statistical analysis and practical applications of the digital chaotic sequences. A diagram that presents the relationship among <b>precision,</b> <b>parameter</b> u and correlative peak interval has been generated for analysis. Index Terms—discrete chaotic systems, correlative peak interval, finite precision, encryption I...|$|R
40|$|We {{study the}} effects of {{external}} magnetic fields on the <b>precision</b> of <b>parameter</b> estimation with thermal entanglement of two spins in XX model, in the presence of Dzyaloshinskii-Moriya (DM) interaction. Calculating the quantum Fisher information, we show that homogeneous magnetic field B, inhomogeneous magnetic field b or DM interaction D increases the <b>precision</b> of <b>parameter</b> estimation, overwhelming the destructive effects of thermalization. We also show that for the model in consideration, {{the effects of}} b and D are the same. However, the existence of both b and B (or both D and B) decreases the precision. We find that in order to increase the <b>precision</b> in <b>parameter</b> estimation tasks, applying b in the ferromagnetic case and B in the antiferromagnetic case should be preferred. Comment: 4 pages, 2 color figures, Comments Welcom...|$|R
