0|6402|Public
2500|$|... website with {{educational}} <b>pictures,</b> <b>information</b> how to plant your own tree.|$|R
50|$|Many {{monochrome}} alphanumerical displays without <b>picture</b> <b>information</b> {{still use}} TN LCDs.|$|R
5000|$|A 'steering' {{signal is}} {{transmitted}} to indicate to the 16:9 receiver whereabouts the 4:3 <b>picture</b> <b>information.</b>|$|R
50|$|The {{table below shows}} which TV lines {{will contain}} <b>picture</b> <b>information</b> when {{letterbox}} <b>pictures</b> are displayed on either 4:3 or 16:9 screens.|$|R
40|$|AbstractThis paper {{concerns}} an axiomatic {{characterization of}} <b>information</b> <b>measures</b> of dimension k. The Shannon entropy is an <b>information</b> <b>measure</b> of dimension one. Directed divergence and information improvement {{are examples of}} 2 -dimensional and 3 -dimensional <b>information</b> <b>measures.</b> From these k-dimensional <b>information</b> <b>measures,</b> one can obtain all dimensions (including 1, 2, and 3, which have already proven useful) at once. We determine all the <b>information</b> <b>measures</b> that depend upon several discrete probability distributions, have the sum property, and satisfy the additivity property. It is shown that the additive k-dimensional <b>information</b> <b>measures</b> along with the sum property are essentially the linear combination of the Shannon entropies and Kerridge inaccuracies...|$|R
5000|$|Scan {{conversion}} involves {{changing the}} <b>picture</b> <b>information</b> data rate and wrapping the new picture in appropriate synchronization signals.There are two distinct methods for changing a picture's data rate: ...|$|R
40|$|The R-norm <b>information</b> <b>measure</b> is {{discussed}} and its properties, {{as well as}} an axiomatic characterization, are given. The measure is extended to conditional and joint measures. Applications to coding and hypothesis testing are given. The R-norm <b>information</b> <b>measure</b> includes Shannon's <b>information</b> <b>measure</b> as a special case...|$|R
40|$|In statistics, Fisher was {{the first}} to {{introduce}} the measure of the amount of information supplied by the data about the unknown parameter. We analyze the disadvantages of Fisher <b>information</b> <b>measure</b> for optimization of sampling designs. To overcome this problem, we modify Fisher <b>information</b> <b>measure</b> and we upgrade it to the multivariate setting. It turns out that a reasonable modification of Fisher <b>information</b> <b>measure</b> leads to a special case of Kullback <b>information</b> <b>measure,</b> both in the univariate and multivariate setting. Using Shannon’s and Wiener’s concept of information we also show a simple derivation of Kullback <b>information</b> <b>measure</b> for a special case when the prior distribution of the parameter is uniform and the posterior distribution is truncated normal. ...|$|R
50|$|Elias Savada is an American film {{historian}} and critic. Since 1977 he has {{owned and operated}} the Motion <b>Picture</b> <b>Information</b> Service, which has provided customized copyright research reports to over 1,200 clients.|$|R
5000|$|Crosspoints {{can also}} be {{switched}} in the vertical interval to avoid losing <b>picture</b> <b>information,</b> for this the router {{would need to be}} genlocked to either black and burst or tri level sync ...|$|R
40|$|We {{propose a}} {{generalized}} cumulative residual <b>information</b> <b>measure</b> based on Tsallis entropy and its dynamic version. We study the characterizations {{of the proposed}} <b>information</b> <b>measure</b> and define new classes of life distributions based on this measure. Some applications are provided in relation to weighted and equilibrium probability models. Finally the empirical cumulative Tsallis entropy is proposed to estimate the new <b>information</b> <b>measure...</b>|$|R
40|$|DE 1004024756 A UPAB: 20060112 NOVELTY - The {{system is}} {{installed}} {{with a video}} sensor system (2) at rail and guide sections for detecting rail track regions traveled by rail vehicles such that characteristic train stopping signals are detectable as <b>picture</b> <b>information.</b> The detected <b>picture</b> <b>information</b> signals of an electronic processing and communication unit (4) are conveyed with the sensor system for automatic evaluation relating to characteristic train stopping signal. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for train stopping status signal of a rail vehicle. USE - Used for monitoring rail track traveled by a rail vehicle. ADVANTAGE - The system is installed with the video sensor system at the rail and guide sections for detecting rail track regions traveled by the rail vehicles such that characteristic train stopping signals are detectable as <b>picture</b> <b>information,</b> thus avoiding errors that occur {{as a consequence of}} human malfunctions. The system hence avoids considerable danger potential with the operating security on respective rail and guide sections...|$|R
50|$|The film uses <b>pictures,</b> <b>information</b> {{and other}} data from various sources. The project {{consciously}} focuses on V G Damle's career, his creative contributions and his life. Along with his journey the film portrays The story of Prabhat, simultaneously.|$|R
40|$|It is {{proved that}} the only {{additive}} and isotropic <b>information</b> <b>measure</b> that can depend on the probability distribution and also on its first derivative is a linear combination of the Boltzmann-Gibbs-Shannon and Fisher <b>information</b> <b>measures.</b> Power law equilibrium distributions are found {{as a result of}} the interaction of the two terms. The case of second order derivative dependence is investigated and a corresponding additive <b>information</b> <b>measure</b> is given. Comment: 10 pages, 1 figures, shortene...|$|R
50|$|For {{marketing}} {{the new album}} Orbit has created a blog on his website in which he weekly posts <b>pictures,</b> <b>information</b> and song downloads. He has been using the microblog service Twitter to spread news, opinions and personal pictures as well.|$|R
40|$|The net Fisher <b>information</b> <b>measure</b> IT, {{defined as}} the product of {{position}} and momentum Fisher <b>information</b> <b>measures</b> Ir and Ik and derived from the non-relativistic Hartree-Fock wave functions for atoms with Z = 1 − 102, is found to correlate well with the inverse of the experimental ionization potential. Strong direct correlations of IT are also reported for the static dipole polarizability of atoms with Z = 1 − 88. The complexity measure, {{defined as the}} ratio of the net Onicescu <b>information</b> <b>measure</b> ET and IT, exhibits clearly marked regions corresponding to the periodicity of the atomic shell structure. The reported correlations highlight the need for using the net <b>information</b> <b>measures</b> in addition to either the position or momentum space analogues. With reference to the correlation of the experimental properties considered here, the net Fisher <b>information</b> <b>measure</b> is found to be superior than the net Shannon information entropy...|$|R
40|$|The present {{invention}} {{is based}} on the finding that pictures or a picture stream can be encoded highly efficient when a representation of pictures is chosen that is having different picture blocks, wherein each picture block is carrying <b>picture</b> <b>information</b> for <b>picture</b> areas smaller than the full area of the picture and when the different picture blocks are carrying the <b>picture</b> <b>information</b> either in a first color-space-representation or in a second color-space-representation. Since different color-space-representations have individual inherent properties with respect to their describing parameters, choosing an appropriate color-space-representation individually for the picture blocks results in an encoded representation of pictures that is having a better quality at a given size or bit rate...|$|R
40|$|We study data {{processing}} inequalities that {{are derived from}} a certain class of generalized <b>information</b> <b>measures,</b> where a series of convex functions and multiplicative likelihood ratios are nested alternately. While these <b>information</b> <b>measures</b> {{can be viewed as}} a special case of the most general Zakai–Ziv generalized <b>information</b> <b>measure,</b> this special nested structure calls for attention and motivates our study. Specifically, a certain choice of the convex functions leads to an <b>information</b> <b>measure</b> that extends the notion of the Bhattacharyya distance (or the Chernoff divergence) : While the ordinary Bhattacharyya distance is based on the (weighted) geometric mean of two replicas of the channel’s conditional distribution, the more general <b>information</b> <b>measure</b> allows an arbitrary number of such replicas. We apply the {{data processing}} inequality induced by this <b>information</b> <b>measure</b> to a detailed study of lower bounds of parameter estimation under additive white Gaussian noise (AWGN) and show that in certain cases, tighter bounds can be obtained by using more than two replicas. While the resulting lower bound may not compete favorably with the best bounds available for the ordinary AWGN channel, the advantage of the new lower bound, relative to the other bounds, becomes significant in the presence of channel uncertainty, like unknown fading. This different behavior in the presence of channel uncertainty is explained by the convexity property of the <b>information</b> <b>measure...</b>|$|R
40|$|The paper {{introduces}} two new parametric generalizations of one {{of existing}} R norm fuzzy <b>information</b> <b>measures</b> with the proof of their validity. In addition, particular cases and important properties of the proposed measures are discussed. A numerical example is given to establish the similarity between the proposed R norm fuzzy <b>information</b> <b>measures</b> {{with one of the}} existing R norm fuzzy <b>information</b> <b>measures.</b> Further, a comparison among them is shown {{with the help of a}} table and graph...|$|R
40|$|Information {{theory is}} widely {{accepted}} as {{a powerful tool for}} analyzing complex systems and it has been applied in many disciplines. Recently, some central components of information theory - multivariate <b>information</b> <b>measures</b> - have found expanded use in the study of several phenomena. These <b>information</b> <b>measures</b> differ in subtle yet significant ways. Here, we will review the information theory behind each measure, as well as examine the differences between these measures by applying them to several simple model systems. In addition to these systems, we will illustrate the usefulness of the <b>information</b> <b>measures</b> by analyzing neural spiking data from a dissociated culture through early stages of its development. We hope that this work will aid other researchers as they seek the best multivariate <b>information</b> <b>measure</b> for their specific research goals and system. Finally, we have made software available online which allows the user to calculate all of the <b>information</b> <b>measures</b> discussed within this paper. Comment: Manuscript (15 pages, 3 figures, 8 tables...|$|R
40|$|We {{deal with}} {{conditional}} decomposable <b>information</b> <b>measures,</b> directly defined as functions on a suitable set of conditional events satisfying {{a class of}} axioms. For these general measures we introduce a notion of independence and study its main properties in order to compare it with classical definitions present in the literature. The particular case of Wiener-Shannon <b>information</b> <b>measure</b> is taken in consideration and the links between the provided independence for <b>information</b> <b>measures</b> and the independence for the underlying probability are analyzed...|$|R
40|$|Abstract — It is {{well known}} that the Shannon <b>information</b> <b>measures</b> are {{continuous}} functions of the probability distribution when the support is finite. This, however, does not hold when the support is countably infinite. In this paper, we investigate the continuity of the Shannon <b>information</b> <b>measures</b> for countably infinite support. With respect to a distance based on the Kullback-Liebler divergence, we use two different approaches to show that all the Shannon <b>information</b> <b>measures</b> are in fact discontinuous at all probability distributions with countably infinite support. I...|$|R
40|$|In {{the present}} paper the {{generalized}} mean codeword length is studied and characterized a new generalized <b>information</b> <b>measure</b> by obtaining bounds {{in terms of a}} new generalized <b>information</b> <b>measure</b> using Lagrange’s Multiplier method. The Shannon’s Noiseless coding theorem is verified by considering Huffman coding scheme and Shannon Fano coding scheme on taking empirical data. We study the monotone behaviour of the new generalized <b>information</b> <b>measure</b> with respect to parameters and. The important properties of the new generalized <b>measure</b> of <b>information</b> have also been studied...|$|R
40|$|The {{contribution}} of the thesis lies {{in the definition of}} new <b>information</b> <b>measures</b> in the context of censoring (random censoring, quantal random censoring), which results from classical <b>information</b> <b>measures,</b> and the study of various properties of the statistical information theory and new properties resulting from the nature of the censored <b>information</b> <b>measures.</b> In addition, the property of the loss of information on the uncensored case, due to descretization of a random variable, is also under study, based on a general and finite partition of the sample space. ...|$|R
40|$|We {{evaluate}} the asymptotics of equivocations, their exponents {{as well as}} their second-order coding rates under various Rényi <b>information</b> <b>measures.</b> Specifically, we consider the effect of applying a hash function on a source and we quantify the level of non-uniformity and dependence of the compressed source from another correlated source when the number of copies of the sources is large. Unlike previous works that use Shannon <b>information</b> <b>measures</b> to quantify randomness, information or uniformity, we define our security measures in terms of a more general class of <b>information</b> <b>measures</b> [...] the Rényi <b>information</b> <b>measures</b> and their Gallager-type counterparts. A special case of these Rényi <b>information</b> <b>measure</b> is the class of Shannon <b>information</b> <b>measures.</b> We prove tight asymptotic results for the security measures and their exponential rates of decay. We also prove bounds on the second-order asymptotics and show that these bounds match when the magnitudes of the second-order coding rates are large. We do so by establishing new classes non-asymptotic bounds on the equivocation and evaluating these bounds using various probabilistic limit theorems asymptotically. Comment: 47 pages, 9 figures; Presented at the 2015 International Symposium on Information Theory (Hong Kong); Submitted to the IEEE Transactions on Information Theory; v 3 : fixed typos and added some clarifications to the proof...|$|R
2500|$|Arndt, C. [...] <b>Information</b> <b>Measures,</b> <b>Information</b> and its Description in Science and Engineering (Springer Series: Signals and Communication Technology), 2004, ...|$|R
5000|$|... #Subtitle level 3: <b>Information</b> <b>measure</b> for {{stereoscopic}} images ...|$|R
50|$|When {{engineers}} {{sought to}} process and record {{in real time}} the huge amount of digital data {{needed to make the}} first digital video tape format, keeping the Y, R-Y, B-Y or YUV algorithm was key to simplifying and reducing the initial <b>picture</b> <b>information</b> sampled, saving valuable space.|$|R
40|$|Abstract. In {{the present}} communication, I have defined the new <b>information</b> <b>measure</b> called “α-R-norm informa-tion measure”. It has been {{characterized}} using infimum oiperation in Section 2 and axiomatically in Section 3. Its properties have been studied in Section 4, joint and conditional α-R-norm <b>information</b> <b>measure</b> are studied in Sec-tion 5. 1...|$|R
40|$|We {{analyze the}} problem of {{estimating}} some particular indices (based on <b>information</b> <b>measures)</b> in simple-stage cluster sampling, and conclude that the indices based on <b>information</b> <b>measures</b> of degree [beta] = 2 allow us to construct unbiased estimates of their population values. diversity finite population income inequality simple-stage cluster sampling unbiased estimation. ...|$|R
40|$|Information Theory is {{a branch}} of mathematics, more {{specifically}} probability theory, that studies information quantification. Recently, several researches have been successful {{with the use of}} Information Theoretic Learning (ITL) as a new technique of unsupervised learning. In these works, <b>information</b> <b>measures</b> are used as criterion of optimality in learning. In this article, we will analyze a still unexplored aspect of these <b>information</b> <b>measures,</b> their dynamic behavior. Autoregressive models (linear and non-linear) will be used to represent the dynamics in <b>information</b> <b>measures.</b> As a source of dynamic information, videos with different characteristics like fading, monotonous sequences, etc., will be used...|$|R
5000|$|<b>Pictures</b> and <b>information</b> about Cuba's {{license plate}} (in spanish) ...|$|R
40|$|I n {{the present}} communication, we review the {{existing}} <b>measures</b> of fuzzy <b>information.</b> Wedefine and characterize two fuzzy <b>information</b> <b>measures</b> which are sub additive and differentfrom known <b>measures</b> of fuzzy <b>information.</b> We also study monotonic behavior and particularcases of these fuzzy <b>information</b> <b>measures.</b> 2000 Mathematics Subject Classification: 94 A 17 and 94 D 0...|$|R
30|$|The {{measurement}} system {{can only be}} {{used to measure the}} geometric dimensions of two-dimensional images. It can be considered to collect <b>picture</b> <b>information</b> by using double cameras or even multiple cameras. The system can be used to measure 3 D images and broaden the application field of image size measurement.|$|R
50|$|The St. Mary's Church {{is on the}} National Register of Historic Places, No. 76001697. The South Carolina Department of Archives and History has {{additional}} <b>pictures</b> and <b>information.</b> and {{copies of}} the nomination forms. There are additional <b>pictures</b> and <b>information</b> available from the Historic American Buildings Survey at the Library of Congress.|$|R
5000|$|Institute of Nanotechnologies, <b>Information</b> <b>Measuring</b> and Specialized Computer Systems in Power Industry ...|$|R
