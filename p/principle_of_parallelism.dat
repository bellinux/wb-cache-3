7|10000|Public
50|$|In {{computer}} engineering, {{the creation}} {{and development of the}} pipeline burst cache memory is an integral part {{in the development of the}} superscalar architecture. It was introduced in the mid 1990s as a replacement for the Synchronous Burst Cache and the Asynchronous Cache and is still in use till date in computers. It basically increases the speed of the operation of the cache memory by minimizing the wait states and hence maximizing the processor computing speed. Implementing the techniques of pipelining and bursting, high performance computing is assured. It works on the <b>principle</b> <b>of</b> <b>parallelism,</b> the very principle on which the development of superscalar architecture rests. Pipeline burst cache can be found in DRAM controllers and chipset designs.|$|E
40|$|An {{effect is}} a {{function}} of a cause as well as of 4 other factors: recipient, setting, time, and outcome variable. The <b>principle</b> <b>of</b> <b>parallelism</b> states that if a design option exists for any 1 of these 4 factors, a parallel option exists for each of the others. For example, effects are often estimated by drawing a comparison across recipients who receive different treatments. The <b>principle</b> <b>of</b> <b>parallelism</b> implies that an effect can also be estimated by drawing a comparison across settings, times, or outcome variables. Typologies of methodological options are derived from the <b>principle</b> <b>of</b> <b>parallelism.</b> The typologies can help researchers recognize a broader set of options than they would otherwise and thereby improve the quality of research designs...|$|E
40|$|Estonian {{municipalities}} should {{perform a}} broad range of functions, while their fiscal resources are often limited and large disparities in fiscal capacity prevail among them. Moreover, the power to regulate fiscal affairs is mostly {{in the hands of the}} central government. We discuss how a strict application of the connexity principle can protect municipalities from the fiscal bottleneck. We also recommend the introduction of the <b>principle</b> <b>of</b> <b>parallelism</b> and investigate its effects on the unconditional, down-flow grant system in Estonia. In particular the procedure of determining the total sum of block grants appears to be changed. fiscal equalization, municipal finance, connexity, principles of parallelism, Estonia...|$|E
40|$|The {{programming}} language L, oriented to parallel computers, is introduced. <b>Principles</b> <b>of</b> <b>parallelism</b> in {{the solution of}} problems and the requirements for the computing architecture are presented, for adequate use of the L language. Principles for problem initialization are commented upon, in view of possible real time constraints. The language structure and an example of its application are also presented in this work...|$|R
50|$|The Sony Toshiba IBM Center of Competence for the Cell Processor is {{the first}} Center of Competence {{dedicated}} to the promotion and development of Sony Toshiba IBM's Cell microprocessor, an eight-core multiprocessor designed using <b>principles</b> <b>of</b> <b>parallelism</b> and memory latency. The center {{is part of the}} Georgia Institute of Technology's College of Computing and is headed by David A. Bader.|$|R
5000|$|Ueber psychische Causalität und das Princip des psycho-physischen Parallelismus. (On mental Causality and the <b>Principle</b> <b>of</b> psycho-physical <b>Parallelism).</b> (Philosophische Studien. 1894, Vol 10, pp. 1-124).|$|R
40|$|Estonian {{municipalities}} have {{to perform}} a broad range, while their fiscal resources are often limited in comparison to functions and large disparities in fiscal capacity prevail among them. Moreover, the power to regulate fiscal affairs is mostly {{in the hands of}} the central government. Municipalities do not possess satisfactory development planning perspectives. In particular municipalities in the North-East region and South Estonia have experienced considerable fiscal stress. We discuss how a strict application of the connexity principle can protect municipalities from the fiscal bottleneck. We also recommend the introduction of the <b>principle</b> <b>of</b> <b>parallelism</b> and investigate its effects on the down-flow grant system in Estonia. The procedure of determining the total sum of block grants needs to be changed. In most cases a high degree of parallelism applied when providing the unconditional grant via the equalization fund improves the fiscal stability and predictability of Estonian municipalities...|$|E
40|$|In many {{countries}} occur problems from unfunded mandates {{related to a}} shift of a function by an upper tier of government to lower rank governments without providing the lower one with financial means to perform these functions. The authors define the unfunded mandates and clarify how they relate to functions, uncovered costs and financial means of municipalities. Then they discuss the connexity principle {{in order to avoid}} unfunded mandates and respective functions variations in crises to avoid unfunded mandates. They point to conflicts which stem from allocation, stabilization and distribution activities. In Estonia the development of revenues and expenditures of central and local governments reflect tendencies to cause unfunded mandates especially in the period of crisis from 2009 to 2011. To fight unfunded mandates the connexity principle should be applied for municipal performance of central state functions and compulsory municipal functions. The central and local government finances should vary according to the <b>principle</b> <b>of</b> <b>parallelism.</b> If the total tax revenues vary the changes of block grants have to be parallel to the disposable incomes of the tiers of government...|$|E
40|$|This {{dissertation}} {{studies the}} prosody, poetic theory, theme, and affective nature {{found in the}} poetry of Gerard Manley Hopkins. The prosody, striking in his time, is still controversial; the theory employs the rhetorical <b>principle</b> <b>of</b> <b>parallelism</b> extensively, and the theme (which {{is the reason for}} the affective nature of his work) deals always with Christ: Christ in nature and Christ in man. The study lays emphasis on Hopkins' religious vision. These insights pervade all his work and are prime factors in his poetry. The vision gained from his religion appears throughout all his work. Although recent critics have suggested that the material of his great ode, "The Wreck of the Deutschland" was "recalcitrant" and that his symmetry was "laboured," explication of the poem reveals his early intense voice, sprung rhythm, and his use of the techniques of cynghanedd and dysfalu. His prosody reveals his sense of parallel structure (noted in his art work and in his journals as symmetry) which increased with "number and distinctiveness" with the rise of passion. His "dark night," noted in the sonnets written during 1884 - 85, have caused some readers to suspect a crisis of faith occurring. Hopkins experienced trauma, but the prolonged depression suggested by the present numbering of the sonnets is inconsistent with his unquestioned faith. The night becomes less dark if chronology is followed. Hopkins' deepest message was delivered in his poetry and throughout his life. As a Catholic priest, teacher, and poet, he sought Christ. Common knowledge informs us that emotional and physical hardships follow such seekers. Teilhard de Chardin's philosophy as ennobling is certainly applicable to any study of Hopkins' life and works. This philosophy provides supplementary confirmation of the poet. Hopkins' achievements surpass the prescriptive condemners of his art. Thesis (Ph. D. ...|$|E
40|$|Determining an {{appropriate}} population size {{is an important}} task {{in the design of}} genetic algorithms and is closely related to the <b>principle</b> <b>of</b> implicit <b>parallelism.</b> In this paper, the problem of bounding the population size is formulated as that of minimization of sampling errors. Using techniques from computational learning theory, two lower bounds on the population size are established. The lower bounds enable us to interpret the <b>principle</b> <b>of</b> implicit <b>parallelism</b> from a new perspective, and depict how the necessary population size is related to the mutation probability and some population statistics...|$|R
40|$|Recent {{past has}} {{witnessed}} rapid growth on {{the usage of}} CCTV and video camera to beep up security in almost all aspects of life. This has resulted in tremendous growth of video content and its processing time. One of the challenges faced {{in this area is}} fast and efficient processing of these huge contents of video images. Therefore, integrating the <b>principles</b> <b>of</b> <b>parallelism</b> with the processing of the video images techniques has almost become mandatory for extraction of the desired information and achieving better performance. In this paper, a parallel algorithm has been proposed for extraction of person features from video frames that can execute on a cluster of workstations. The person behaviors are classified using Minimum distance classifier by the extracted feature vector. The algorithm has been implemented using MPI to estimate the time complexity and efficiency...|$|R
40|$|Two {{parallel}} algorithms for computing Fast Fourier Transform(FFT) on a {{new type}} of architecture called the n Cycles (n Γ SCC) have been considered. The n Γ SCC architecture is based on the n Γ star interconnection graph and has all the desirable features which exist in the n Γ star interconnection graph. To demonstrate the main properties of an n Γ SCC network, we present two parallel algorithms for computing the FFT. These algorithms are based on the Cooley-Tukey FFT algorithm and combine the <b>principles</b> <b>of</b> <b>parallelism</b> and pipelining. Considering the same number of inputs, the second algorithm in this paper has better performance in compare to the algorithm implemented on star graph. Key Words: FFT, parallel algorithm, n Γ star graph, and n Γ SCC network. 1 Introduction Since the introduction of Discrete Fourier Transform (DFT) and Fast Fourier Transform (FFT) in 1965 [3], FFT algorithms have been extensively studied. In o [...] ...|$|R
40|$|The {{doctoral}} thesis addresses {{the issue of}} relations between ordinary legislation and referendum, following {{the decision of the}} people. It is an analysis of the different dynamics that characterize the complex relationship between parliamentary legislator and popular legislator, with regard to their "sovereignty. " The subject of investigation, in particular, focuses on the effect of legal conditioning of the legislature following the direct expression of the people in referendums. We started from the notion that, although the <b>principle</b> <b>of</b> <b>parallelism</b> between the referendum and the ordinary law would exclude any mutual constraint, on the contrary the principles under articles. 1 and 75 of the Constitution could be used to support the opposite hypothesis. The choice of the job structure and content selection was then made so that it can synthesize all the elements necessary to verify if this constraint has the right to exist in our constitutional order. In particular, the thesis has begun with a referendum’s historical analysis, with a focus on the referendum's ability to affect the political system. Going through the work of the Constituent Assembly, from Mortati’s proposal until the current wording of art. 75 of the Constitution, outlining referendum’s complex process, specifically identifying the referendum role respect to the form of government. Indeed, faced with the insufficiency of a purely theoretical reconstruction of this ratio due to sparse data rules (both ordinary and constitutional), we opted for a different approach, aimed to study the practical modalities of referendum’s actuation. And so a protean nature institution was "discovered" that has managed to become an element of political debate, sometimes taking on the (false) appearance of a true antagonist of the parliamentary system. From the political point of view, therefore, it is clear how the referendum has its place in the form of government, but this consideration was not sufficient to settle the issue concerning the possible legal effects of a popular consultation. Only a reading of the referendums through the lens of the principle of sovereignty under art. 1 of the Constitution has been opened up prospects that make the referendum’s effects hardly compressible in the mere repeal. Considering referendum as an instrument of expression of popular sovereignty, it determines the need for the outcome of the consultation to be granted. Therefore, it is the encounter between the art. 75 and art 1 of the Constitution that seems to suggest the possibility of theorizing a bond not only political, but legal, paid by the legislator who wishes to intervene on a matter affected by a referendum vote. The second part relates to the old "betrayal" referendums (socalled "test cases”), cases in which following a referendum, the legislature has subsequently intervened violating the referendum outcome and summing, sometimes only in part, the legislation deleted by popular vote. The conduct of the legislature, analyzed in different contexts in which you could easily find an excess over the referendum result, has had as a constant: the method of referendum’s bond violation. A circumstance which certainly showed how, at least at the political level, the legislature felt a sort of responsibility in putting the referendum’s results to nothingness. The last part of the work is devoted {{to the study of the}} bond referendum hypothesis. Despite the placement, is the central part of this thesis, in which you try a reconstruction under different profiles of the relationship between referendum and ordinary law, by analyzing the various problems that right now prevent its solution in the sense of the existence of a referendary bond of legal type. It was analyzed, at first, the positive data, to understand if at legal level exists some conditions to support a legal constraint that limits the legislature following referendum expression on a particular subject. Also Constitutional Court is aware of these issues and its judjements were involved in my research. About the analysis of the doctrine, we focus in particular on the theory that admits the existence of a legal referendum’s bond, addressing some problematic aspects. In particular, it was initially approached one of the most important problems, the one related to the duration of the bond, starting from the belief that one can not imagine an endless ban. None of the solutions seemed satisfying, but it certainly can be taken in condideration the right of the electorate to a rethinking of legislation voted in a referendum. Another problem to solve was the one about the "guardian" of the referendum’s bond. Imagining that this role can be covered by the Constitutional Court, you should accept that it will also use political arguments to undo a law that violates the bond referendum. Opting instead for the President (through the re-send power and Parliament dissolution power), there would not be some solution, since it is strongly linked to mostly political considerations. Another issue raised was the one concerning the referendum question interpretation (for the purpose of identifying the exact scope of the bond), with particular reference to the role of the promoter and to the possible introduction of a motivation that can eliminate any ambiguities related to an investigation of the subjective intentions of the owners of the initiative referendum. Hence also some remarks on the need to ensure the authenticity of the vote and, therefore, in this sense the importance of the referendum campaign. The conclusion of the investigation has partially disproved the initial thesis: despite having identified several factors that can certainly support the idea of a referendum bond, does not seem possible to deduce from the current regulatory system a rule that limits the legislative function of all the times that there were a referendum vote...|$|E
40|$|Abstract. The Bootable Cluster CD (BCCD) is an established, well maintained, cluster toolkit used {{nationally and}} internationally within several levels of the {{academic}} system. During the Education Programs of Supercomputing conferences 2002, 2003, and 2004, the BCCD image was used to support instruction of issues related to parallel computing education. It {{has been used in}} the undergraduate curriculum to illustrate <b>principles</b> <b>of</b> <b>parallelism</b> and distributed com-puting and widely used to facilitate graduate research in parallel environments. The standard BCCD image is packaged in the 3 ”, mini-CD format, easily fitting inside most wallets and purses. Variations include PXE-bootable (network-bootable) and USB-stick bootable images. All software components are pre-configured to work together making the time required to go from boot-up to functional cluster less than five minutes. A typical Windows or Macintosh lab can be temporarily converted into a working GNU/Linux-based computational cluster without modification to original disk or operating system. Students can immediately use this computa-tional cluster framework to run a variety of real scientific models conveniently located on the BCCD and downloadable into any running BCCD environment. This paper discusses building, configuring, modifying, and deploying aspects of the Bootable Cluster CD...|$|R
40|$|Abstract Two {{parallel}} algorithms for computing Fast Fourier Transform(FFT) on a {{new type}} of architecture called the nΓ StarΓ Connected Cycles (n Γ SCC) have been considered. The n Γ SCC architecture is based on the n Γ star interconnection graph and has all the desirable features which exist in the n Γ star interconnection graph. To demonstrate the main properties of an n Γ SCC network, we present two parallel algorithms for computing the FFT. These algorithms are based on the Cooley-Tukey FFT algorithm and combine the <b>principles</b> <b>of</b> <b>parallelism</b> and pipelining. Considering the same number of inputs, the second algorithm in this paper has better performance in compare to the algorithm implemented on star graph. Key Words: FFT, parallel algorithm, n Γ star graph, and n Γ SCC network. 1 Introduction Since the introduction of Discrete Fourier Transform (DFT) and Fast Fourier Transform (FFT) in 1965 [3], FFT algorithms have been extensively studied. In order to obtain higher throughput, researchers have been putting their efforts into two aspects: effective algorithm design and faster architecture. During the 1970 s, array processors were produced and widely used to execut...|$|R
5000|$|Hugh Everetts response: If we try {{to limit}} the {{applicability}} so as to exclude the measuring apparatus, or in general systems of macroscopic size, {{we are faced with}} the difficulty of sharply defining the region of validity. For what n might a group of n particles be construed as forming a measuring device so that the quantum description fails? And to draw the line at human or animal observers, i.e., to assume that all mechanical aparata obey the usual laws, but that they are not valid for living observers, does violence to the so-called <b>principle</b> <b>of</b> psycho-physical <b>parallelism.</b>|$|R
40|$|Abstract. Petri nets are {{a widely}} used {{formalism}} to qualitatively model concurrent {{systems such as}} a biological cell. We present techniques for modelling biological processes as Petri nets for further analyses and insilico experiments. Instead of extending the formalism with,,colours ” or rates, as is most often done, we focus on preserving {{the simplicity of the}} formalism and developing an execution semantics which resembles biology – we apply a <b>principle</b> <b>of</b> maximal <b>parallelism</b> and introduce the novel concept of bounded execution with overshooting. A number of modelling solutions are demonstrated using the example of the wellstudied C. elegans vulval development process. Our model is still under development, but first results, based on Monte Carlo simulations, are promising. ...|$|R
40|$|Abstract-Image {{and video}} {{compression}} {{is one of}} the major components used in video –telephony, videoconferencing and multimedia –related applications. In this paper we describe the design and implementation of a fully pipelined architecture for implementing the JPEG image compression standard. The architecture exploits the <b>principles</b> <b>of</b> pipelining and <b>parallelism</b> in order to obtain high speed and throughput. This design aimed to be implemented in Spartan- 3 E X C 3 S 500 E FPGA...|$|R
40|$|Advances in {{computational}} {{fluid dynamics}} are paced by simulation methodology and computer resources. Examples of three-dimensional fluid dynamic simulations are presented to illustrate recent developments in equation modeling and numerical methods and {{to point out the}} need for increased computer power. Electronic technology dictates that to fill this need, computers will be based on parallel processing <b>principles.</b> The identification <b>of</b> <b>parallelism</b> in three dimensions is illustrated by examining an implicit, approximate-factorization approach to the Navier-Stokes equations. Finally, two computer concepts aimed at satisfying the demands of the three-dimensional Reynolds averaged Navier-Stokes simulations are discussed...|$|R
40|$|AbstractThe {{goal of this}} {{research}} paper is to outline a general approach to tackling the problem of conveying a coherent system of thought displayed by Walt Whitman in his poem “Leaves of Grass” in translation. Firstly, this approach {{is based on the}} <b>principles</b> <b>of</b> Biblical <b>parallelism,</b> which help to explain means of correlating the text of “Leaves of Grass” with the reality. Secondly, it centers around the territory of translation solutions based on the author's explanations and comments. The proposed study is aimed at assessing the efficiency of the above method when translating Whitman's formal contradictions, to get a better insight into which one needs to recourse to a phenomenon crucial for Whitman's works – a twofold nature of the Self (Soul and I) ...|$|R
5000|$|Sub-Saharan harmony {{is based}} on the <b>principles</b> <b>of</b> {{homophonic}} <b>parallelism</b> (chords based around a leading melody that follow its rhythm and contour), homophonic polyphony (independent parts moving together), counter melody (secondary melody) and ostinato-variation (variations based on a repeated theme). Polyphony (contrapuntal and ostinato variation) is common in African music and heterophony (the voices move at different times) is a common technique as well. Although these <b>principles</b> <b>of</b> traditional (precolonial and pre-Arab) African music are of pan-African validity, {{the degree to which they}} are used in one area over another (or in the same community) varies. Specific techniques that used to generate harmony in Africa are the [...] "span process", [...] "pedal notes" [...] (a held note, typically in the bass, around which other parts move), [...] "Rhythmic harmony", [...] "harmony by imitation", and [...] "scalar clusters" [...] (see below for explanation of these terms).|$|R
40|$|This paper {{presents}} an epistemological perspective on quantum communication between parties that highlights {{the choices that}} must be made in order to send and obtain information. The notion of information obtained in such a communication is a property associated with the observers and while dependent {{on the nature of}} the physical system its fundamental basis is epistemic. We argue that the observation process is in accord with the <b>principle</b> <b>of</b> psychophysical <b>parallelism</b> that was used by Bohr, von Neumann, Schrodinger and others to establish the philosophical basis of complementarity but has since fallen out <b>of</b> fashion. This <b>principle</b> gave coherence to the original Copenhagen Interpretation without which the latter has come to have some sort of an ad hoc character. Comment: 11 pages in Presented at International Conference on Quantum and Beyond. Linnaeus University, Vaxjo, Sweden, June 13 - 16, 201...|$|R
40|$|In this paper, we {{describe}} the design and implementation of a prototype single chip VLSI architecture for implementing the JPEG baseline image compression standard. The chip exploits the <b>principles</b> <b>of</b> pipelining and <b>parallelism</b> to the maximum extent {{in order to obtain}} high speed and throughput. The architecture for discrete cosine transform and the entropy encoder are based on efficient algorithms designed for high speed VLSI implementation. The chip was implemented using the Cadence tools and based on the prototype implementation the proposed chip architecture can yield a clock rate of about 100 MHz which would allow an input rate of 30 frames per second for 1024 x 1024 color images. 1...|$|R
40|$|In this report, we {{describe}} the design and implementation of a fully pipelined architecture for implementing the JPEG baseline image compression standard. The architecture exploits the <b>principles</b> <b>of</b> pipelining and <b>parallelism</b> {{in order to obtain}} high speed and throughput. The design was synthesized to Altera’s FLEX 10 K series FPGAs, and synthesis was carried out using Altera’s Max+PlusII environment. It has been estimated that the entire architecture can be implemented on a single FPGA to yield a clock rate of about 20 Mhz which would allow an input rate of 20 mega-samples/sec. 3 Acknowledgements The group {{would like to thank the}} following people for their kind and impartial support throughout the project design process: � Mr. Muhammad Nauman, our project advisor, for providing us with much needed advice, arranging for the necessary tools, and of course, for having introduced us t...|$|R
40|$|Image {{and video}} {{compression}} {{is one of}} the major components used in video-telephony, videoconferencing and multimedia-related applications where digital pixel information can comprise considerably large amounts of data. Management of such data can involve significant overhead in computational complexity and data processing. Compression allows efficient utilization of channel bandwidth and storage size. In this paper we describe the design and implementation of a fully pipelined architecture for implementing the JPEG image compression standard. The architecture exploits the <b>principles</b> <b>of</b> pipelining and <b>parallelism</b> in order to obtain high speed and throughput. The design was synthesized using Xilinx 9. 2 i and Spartan 3 FPGAs, and simulation was carried out using ModelSim environment. It has been estimated that the entire architecture can be implemented on a single FPGA to yield a clock rate of about 100 MHz which allow an input rate of 24 bit input RBG...|$|R
40|$|Because {{intelligent}} agents employ physically embodied cognitive {{systems to}} reason about the world, their cognitive abilities are {{constrained by the}} laws of physics. Scientists have used digital computers to develop and validate theories of physically embodied cognition. Computational theories of intelligence have advanced our {{understanding of the nature of}} intelligence and have yielded practically useful systems exhibiting some degree of intelligence. However, the view of cognition as algorithms running on digital computers rests on implicit assumptions about the physical world that are incorrect. Recently, the view is emerging of computing systems as goaldirected agents, evolving during problem solving toward improved world models and better task performance. A full realization of this vision requires a new logic for computing that incorporates learning from experience as an intrinsic part of the logic, and that permits full exploitation of the quantum nature of the physical world. This paper proposes a theory of physically embodied cognitive agents founded upon first-order logic, Bayesian decision theory, and quantum physics. An abstract architecture for a physically embodied cognitive agent is presented. The cognitive aspect is represented as a Bayesian decision theoretic agent; the physical aspect is represented as a quantum process; and these aspects are related through von Neumann’s <b>principle</b> <b>of</b> psychophysical <b>parallelism.</b> Alternative ontological stances regarding the meaning of quantum agent architecture. The concepts are illustrated with an extended example from the domain of science fiction...|$|R
40|$|Routing in Wireless Multihop Networks (WMHNs) {{relies on}} a {{delicate}} balance of diverse and often conflicting parameters, when aiming for maximizing the WMHN performance. Classified as a Non-deterministic Polynomial-time hard problem (NP-hard), routing in WMHNs requires sophisticated methods. As a benefit of observing numerous variables in parallel, quantum computing offers a promising range of algorithms for complexity reduction by exploiting the <b>principle</b> <b>of</b> Quantum <b>Parallelism</b> (QP), while achieving the optimum full-search-based performance. In fact, the so-called Non-Dominated Quantum Optimization (NDQO) algorithm has been proposed for addressing the multi-objective routing problem {{with the goal of}} achieving a near-optimal performance, while imposing a complexity of the order of O(N) and O(N√(N)) in the best- and worst-case scenarios, respectively. However, as the number of nodes in the WMHN increases, the total number of routes increases exponentially, making its employment infeasible despite the complexity reduction offered. Therefore, we propose a novel optimal quantum-assisted algorithm, namely the Non-Dominated Quantum Iterative Optimization (NDQIO) algorithm, which exploits the synergy between the hardware and the quantum parallelism for the sake of achieving a further complexity reduction, which is on the order of O(√(N)) and O(N√(N)) in the best- and worst-case scenarios, respectively. Additionally, we provide simulation results for demonstrating that our NDQIO algorithm achieves an average complexity reduction of almost an order of magnitude compared to the near-optimal NDQO algorithm, while having the same order of power consumptio...|$|R
40|$|Data {{compression}} is {{the reduction}} or elimination of redundancy in data representation {{in order to}} achieve savings in storage and communication costs. Data compression techniques can be broadly classified into two categories: Lossless, Lossy schemes. Digital images require an enormous amount of space for storage. The architecture exploits the <b>principles</b> <b>of</b> pipelining and <b>parallelism</b> to the maximum extent in order to obtain high speed the architecture for discrete cosine transforms and the entropy encoder are based on efficient algorithms designed for high speed VLSI. For example, a colour image with a resolution of 1024 x 1024 picture elements (pixels) with 24 bits per pixel would require 3. 15 Mbytes in uncompressed form. Very high-speed design of efficient compression techniques will significantly help in meeting that challenge. The JPEG baseline algorithm consists mainly of two parts: (i) Discrete Cosine Transform (DCT) computation and (ii) Entropy encoding. The architecture for entropy encoding is based on a hardware algorithm designed to yield maximum clock spee...|$|R
40|$|For the {{foreseeable}} future reliable hardware speedup will only be delivered by {{increasing the number of}} cores (circuits for carrying out instructions) per computer. This means that our CS undergraduate students need to learn more parallelism now, in order to be prepared for careers that will demand increasing knowledge and practice of parallel computing. We can no longer delegate study <b>of</b> <b>parallelism</b> and concurrency to hardware architecture and operating systems courses, because now knowledge of parallel algorithms and thread-safe data structures will be factors in ordinary high-level software. There is an additional demand for services that require data-intensive scalable computing, which often require distributed-computing parallelism. In order to create a necessary transition towards teaching more parallelism in undergraduate CS curricula, we propose an incremental approach to adding <b>principles</b> and practices <b>of</b> <b>parallelism</b> to existing CS courses. This calls for modular teaching materials requiring perhaps a couple of class days to present, designed for flexible application to a variety of courses that may appear in many institutional and curricular contexts. To date, we have produced several flexible teaching modules (available at csinparallel. org) for courses ranging from CS 1 to upper-level courses in algorithms, programming languages, etc., together with supporting materials and hands-on exercises involving appropriate software and hardware tools. The applied exercise assignments can be deployed on local standard systems (e. g., Java threads), or on established and reliable external systems (e. g., cloud computing services or Intel’s Manycore Testing Lab). We are currently in the process of class-testing and formally assessing these materials, and have plans to create several more modules. ...|$|R
40|$|The {{processor}} accelerators {{are effective}} {{because they are}} working not (completely) on <b>principles</b> <b>of</b> stored program computers. They use some kind <b>of</b> <b>parallelism,</b> and it is rather hard to program them effectively: a parallel architecture by means of (and thinking in) sequential programming. The recently introduced EMPA architecture uses a new kind <b>of</b> <b>parallelism,</b> which offers the potential of reaching higher degree <b>of</b> <b>parallelism,</b> and also provides extra possibilities and challenges. It not only provides synchronization and inherent parallelization, but also takes over some duties typically offered by the OS, and even opens the till now closed machine instructions for the end-user. A toolchain for EMPA architecture with Y 86 cores has been prepared, including an assembler and a cycle-accurate simulator. The assembler is equipped with some meta-instructions, which allow to use all advanced possibilities of the EMPA architecture, {{and at the same}} time provide a (nearly) conventional-style programming. The cycle accurate simulator is able to execute the EMPA-aware object code, and is a good tool for developing algorithms for EMPAComment: 13 pages. 7 figures, 2 tables, 5 program listing...|$|R
5000|$|Gottfried Wilhelm Leibniz had a {{far greater}} and more {{constructive}} influence on Wundt's psychology, philosophy, epistemology and ethics. This can be gleaned from Wundt's Leibniz publication (1917) and from his central terms and principles, but has since received almost no attention. Wundt gave up his plans for a biography of Leibniz, but praised Leibniz's thinking on the two-hundredth anniversary of his death in 1916. He did, however, disagree with Leibniz's monadology as well as theories on the mathematisation of the world by removing {{the domain of the}} mind from this view. Leibniz developed a new concept of the soul through his discussion on substance and actuality, on dynamic spiritual change, and on the correspondence between body and soul (parallelism). Wundt secularised such guiding principles and reformulated important philosophical positions of Leibniz away from belief in God as the creator and belief in an immortal soul. Wundt gained important ideas and exploited them in an original way in his <b>principles</b> and methodology <b>of</b> empirical psychology: the <b>principle</b> <b>of</b> actuality, psychophysical <b>parallelism,</b> combination <b>of</b> causal and teleological analysis, apperception theory, the psychology of striving, i.e. volition and voluntary tendency, <b>principles</b> <b>of</b> epistemology and the perspectivism of thought. Wundt's differentiation between the [...] "natural causality" [...] of neurophysiology and the [...] "mental causality" [...] of psychology (the intellect), is a direct rendering from Leibniz's epistemology.|$|R
5000|$|The below table {{shows the}} {{relationship}} between levels <b>of</b> <b>parallelism,</b> grain size and degree <b>of</b> <b>parallelism</b> ...|$|R
40|$|This paper {{presents}} {{the phenomenon of}} directed nuclear radiation based on the analysis of Lobachevsky angle <b>of</b> <b>parallelism.</b> In spite <b>of</b> the wide application of Lobachevsky geometry in relativistic physics [1 - 3], one of its basic concepts, the angle <b>of</b> <b>parallelism,</b> has not received due interpretation. The angle <b>of</b> <b>parallelism</b> characterizes the closeness of a produced particle to the absolute in the Lobachevsky geometry. For an arbitrary distance ρ the corresponding angle <b>of</b> <b>parallelism</b> is determined a...|$|R
5000|$|The {{symmetry}} <b>of</b> <b>parallelism</b> {{cannot be}} proven in ordered geometry. Therefore, the [...] "ordered" [...] concept <b>of</b> <b>parallelism</b> does not form an equivalence relation on lines.|$|R
40|$|Two {{different}} {{definitions of}} the two vectors parallelism are investigated and compared. The first definitions {{is based on the}} <b>principle</b> <b>of</b> geometry deformation, when any physical geometry is obtained as a result of some deformation of the proper Euclidean geometry. The second definition is the conventional definition <b>of</b> <b>parallelism,</b> which is used in the Riemannian geometry. It is shown, that the second definition is inconsistent. It leads to absence <b>of</b> absolute <b>parallelism</b> in Riemannian geometry and to discrimination of outcome outside the framework of the Riemannian geometry at description of the space-time geometry. The reason of the inconsistency appearance is discussed. Problems of the inconsistency elimination and consequences of this elimination for development of the microcosm physics are considered...|$|R
5000|$|Synchronisation <b>of</b> <b>parallelism</b> <b>of</b> the {{printing}} to the lenticules: ...|$|R
