119|150|Public
50|$|The <b>pooling</b> <b>layer</b> {{operates}} independently {{on every}} depth {{slice of the}} input and resizes it spatially. The most common form is a <b>pooling</b> <b>layer</b> with filters of size 2x2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.|$|E
50|$|Due to the {{aggressive}} {{reduction in the}} size of the representation, the trend is towards using smaller filters or discarding the <b>pooling</b> <b>layer</b> altogether.|$|E
50|$|Another {{important}} {{concept of}} CNNs is pooling, {{which is a}} form of non-linear down-sampling. There are several non-linear functions to implement pooling among which max pooling is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. The intuition is that the exact location of a feature is less important than its rough location relative to other features. The <b>pooling</b> <b>layer</b> serves to progressively reduce the spatial size of the representation, {{to reduce the number of}} parameters and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a <b>pooling</b> <b>layer</b> between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.|$|E
40|$|In this paper, {{we present}} the storage {{management}} of the WHOWEDA web warehousing system, which warehouses historical web information. To facilitate inter-table and intra-table sharing of web pages, we propose a three-layer storage architecture, that consists of tuple, table, and <b>pool</b> <b>layers</b> of storage modules storing different part of warehoused web information. To improve retrieval efficiency, we have chosen to replicate some mode attributes across web tables in the table layer while keeping only unique copies of web pages at the <b>pool</b> <b>layer.</b> The separation of table and <b>pool</b> <b>layer</b> storage also allows different valid times to be maintained by multiple web tables for the same web pages due to different schedules of global coupling across web tables. As the sharing of web pages may lead to valid time inconsistency between different web tables, we propose an update synchronization scheme to resolve the valid time differences on user request...|$|R
30|$|Convolutional {{neural network}} (CNN) {{is a typical}} deep {{learning}} model which consists of the input layer, the convolutional <b>layers,</b> the <b>pooling</b> <b>layers,</b> the fully connected layers, and the output layer. Convolutional layers are used for extracting multi-level features of data according to convolution kernels of different sizes. <b>Pooling</b> <b>layers</b> aim at reducing the dimensions of feature representation and making the feature invariant from the location through a pooling function. Fully connected layers combine the outputs of all the previous layers into high-level features. Generally, CNN can automatically learn high-level features and has proved its powerful classification capability.|$|R
3000|$|... • Convolutional {{neural network}} is {{composed}} of multiple building blocks, such as convolution <b>layers,</b> <b>pooling</b> <b>layers,</b> and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm.|$|R
30|$|Pooling layer: While {{convolution}} layer learns features, the <b>pooling</b> <b>layer</b> combines semantically {{related features}} {{into a single}} feature. Each unit in a <b>pooling</b> <b>layer</b> takes input from a patch of units in the previous layer and outputs a maximum or average of these values.|$|E
30|$|When using 1 -of-m embedding, a 2 × 2 max <b>pooling</b> <b>layer</b> {{is placed}} after every {{convolutional}} layer (3 in total), while {{only a single}} max <b>pooling</b> <b>layer</b> {{is included in the}} network for log-m embedding. Due to having an input volume of 8 × l, less max pooling layers could be used with for log-m embedding. Since each max <b>pooling</b> <b>layer</b> halves both dimensions of the input for the next layer and stacking three 3 × 3 convolutional layers creates an effective receptive field of 7 × 7, max pooling could not be performed on the 8 × l input volume generated by log-m embedding between each convolutional layer. Only one max <b>pooling</b> <b>layer</b> could be used in this network and it is placed between the convolutional and fully connected layers. When using log- 70 embedding, we padded the character vectors with an extra zero to make them 8 dimensional instead of 7 so that the same network architecture could be used for both log-m approaches.|$|E
30|$|In {{addition}} to the full connected layer, another problem with image segmentation using the CNN network is the <b>pooling</b> <b>layer</b> structure. The <b>pooling</b> <b>layer</b> also discards part of the location information while increasing the upper layer convolution core and aggregated background information. However, the semantic segmentation method is sensitive to the position information of the feature map. To keep this information as far as possible, the researchers propose two structures to solve this problem.|$|E
30|$|A {{convolutional}} {{neural networks}} (CNN) {{is a type}} of neural network with its architecture primarily designed for object recognition tasks [8]. Generally, CNN architecture comprises three basic structures: convolutional <b>layers,</b> <b>pooling</b> <b>layers,</b> and fully connected layers.|$|R
40|$|In this work, {{we address}} the problem to model all the nodes (words or phrases) in a {{dependency}} tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and <b>pooling</b> <b>layers,</b> which can model a variety of compositions by the feature maps and choose the most informative compositions by the <b>pooling</b> <b>layers.</b> Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets...|$|R
30|$|There {{may be one}} or {{more fully}} {{connected}} layers to perform high-level reasoning after several convolutional <b>layers</b> and <b>pooling</b> <b>layers.</b> They take all neurons in the previous layer and connect them to every single neuron of current layer to generate global semantic information.|$|R
30|$|In this study, {{we checked}} the {{convolution}} operation with m different filters and denoted resulting {{features such as}} c_t∈R^n-h- 1, each of whose dimensions comes from a distinct filter. We repeated the convolution operations for each window of h consecutive POIs in the trajectory and obtained c_ 1 :n-h+ 1. The trajectory representation is computed in the max <b>pooling</b> <b>layer,</b> s∈R^n-h- 1, where the element-wise maximum of c_ 1 :n-h+ 1 is {{the output of the}} max <b>pooling</b> <b>layer.</b>|$|E
30|$|After this {{inception}} module, part 3 {{marked in}} green in Fig.  2 with a max <b>pooling</b> <b>layer</b> and a convolutional layer is inserted. Convolutional layer 4 (Conv 4) has 3 [*]×[*] 3 filters, {{and the depth}} is 40. The filter size of the max <b>pooling</b> <b>layer</b> (Pool 4) in this inception is set to be 2 [*]×[*] 2 to have a same output size with Conv 4. Following the inception is also a concat layer (Conc 2) as part 2.|$|E
30|$|The {{bounding}} box information of RoIs produced by saliency method mainly {{consists of a}} four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w). In addition, since the area of RoIs is any size, we divide the h[*]×[*]w RoI into h/ 7 [*]×[*]w/ 7 sub-regions of 7  ×[*] 7 size for calculation convenience. Then, max-pooling the values in each sub-region into the corresponding output grid cell. The RoI <b>pooling</b> <b>layer</b> is simply the special-case of the only one spatial pyramid <b>pooling</b> <b>layer</b> used in SPPnets [22].|$|E
30|$|A D-CNN, an {{extremely}} efficient and automatic feature-learning approach, transforms an original input to a higher-level and more abstract representation using non-linear models. A D-CNN {{is composed of}} multiple convolution <b>layers,</b> <b>pooling</b> <b>layers,</b> fully connected layers, and classification layers. The network parameters are optimized through the back-propagation algorithm.|$|R
30|$|There {{are three}} {{main reasons for}} using an {{abstract}} neural network to abstract feature computation and extraction: Firstly, because of the inherent characteristics of convolution <b>layer</b> and <b>pool</b> <b>layer</b> of convolution neural network, image translation and other operations will not cause changes in convolution characteristics. Secondly, the use of convolution neural network to extract the independent learning features can avoid the use of artificial selection of fixed features, thus avoiding feature extraction as the bottleneck and ceiling of the later enhancement algorithm. Third, the size of eigenvectors of convolution <b>layer,</b> <b>pool</b> <b>layer,</b> and output layer can be controlled freely. The dimension of eigenvectors can be reduced when overfitting occurs in training, and the output of convolution layer can be improved when underfitting. Compared with other networks, convolution neural network has higher flexibility.|$|R
30|$|MT vs. LB In LB, two raw GEIs are {{compared}} {{at the first}} part, on the other hand, in MT, two compressed GEIs by convolution and <b>pooling</b> <b>layers</b> {{are compared}} at the middle part of the CNN. LB where comparing the raw GEIs more leverages appearance difference considering spatial proximity than MT where comparing the compressed GEIs.|$|R
30|$|Convolution {{layer of}} {{convolution}} neural network, {{also known as}} feature extraction layer, consists of two parts. The first part is the real convolution layer in which the main role is to extract the input data features. The characteristics {{of each of the}} different convolution kernels extracting input data are different. The more the convolution layer’s convolution kernels, the more features of the input data can be extracted. The second part is the <b>pooling</b> <b>layer,</b> also known as the downsampling layer, in which the main purpose is to retain useful information on the basis of reducing the amount of data processing and speed up the training network. Generally, convolutional neural networks contain at least four layers of convolution (in this case, the real convolution layer and the lower sampling layer are collectively called convolution layer), namely convolution layer, <b>pooling</b> <b>layer,</b> convolution layer, and <b>pooling</b> <b>layer.</b> The more the convolution level, the more abstract features can be extracted {{on the basis of the}} previous layer.|$|E
30|$|Convolutional layer 2. There are 96 kernels (size 15 × 15, stride of 2) in {{the second}} layer, which is {{combined}} with one maxout operator and one max <b>pooling</b> <b>layer.</b>|$|E
30|$|Convolutional layer 1. There are 48 kernels (size 17 × 17, stride of 2) in {{the first}} {{convolutional}} layer, which is combined with one maxout operator and one max <b>pooling</b> <b>layer.</b>|$|E
50|$|Finally, {{after several}} {{convolutional}} and max <b>pooling</b> <b>layers,</b> the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations {{in the previous}} layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.|$|R
40|$|Skin {{cancer is}} a deadly disease {{and is on the}} rise in the world. Computerized {{diagnosis}} of skin cancer can accelerate the detection of this type of cancer that is a key point in increasing the survival rate of patients. Lesion segmentation in skin images is an important step in computerized detection of the skin cancer. Existing methods for this aim usually lack accuracy especially in fuzzy borders of lesions. In this paper, we propose a new class of fully convolutional networks with novel dense <b>pooling</b> <b>layers</b> for segmentation of lesion regions in non-dermoscopic images. Unlike other existing convolutional networks, the proposed dense <b>pooling</b> <b>layers</b> are designed to preserve all of the input features. This has led to highly accurate segmentation of lesions. Our proposed method produces dice score of 91. 6 % which outperforms all state-of-the-art algorithms in segmentation of skin lesions based on the Dermquest dataset...|$|R
25|$|A {{convolutional}} {{neural network}} (CNN) is a class of deep, feed-forward networks, composed {{of one or more}} convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and <b>pooling</b> <b>layers.</b> In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs {{to take advantage of the}} 2D structure of input data.|$|R
30|$|Fully-{{connected}} layer: In this layer, {{each unit}} {{is connected to}} all the units in the previous layer. Typically, the convolution and <b>pooling</b> <b>layer</b> are stacked {{in two or three}} stages before using fully-connected layers.|$|E
30|$|The <b>pooling</b> <b>layer</b> is {{used for}} {{sub-sampling}} (dimensionality reduction) and feature selection by merging local information. Therefore, it compresses or generalizes feature representations and generally reduce the overfitting of the model to the training data.|$|E
30|$|Convolutional layer 2. 128 kernels of size 3 × 5 × 5 (with a stride of 2) {{are applied}} to the input banana image in the second layer {{combined}} with the ReLU. A max <b>pooling</b> <b>layer</b> follows this convolutional layer.|$|E
40|$|Learning {{transformation}} invariant {{representations of}} visual data {{is an important}} problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and <b>pooling</b> <b>layers</b> in deep networks with graph spectral convolution and dynamic graph <b>pooling</b> <b>layers</b> that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that {{are very sensitive to}} transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets...|$|R
30|$|We used CNN with {{rectified}} linear units (ReLUs) [41], max pooling, dropout, and softmax regression. CNNs are feed-forward {{neural networks}} designed {{to deal with}} large input data, such as those seen in image classification tasks. CNNs are mainly comprised of three types of layers. These are convolutional <b>layers,</b> <b>pooling</b> <b>layers,</b> and fully connected layers. When these layers are stacked, a CNN architecture has been formed.|$|R
50|$|Convolutional {{networks}} {{may include}} local or global <b>pooling</b> <b>layers,</b> which combine the outputs of neuron clusters at one layer {{into a single}} neuron in the next layer. For example, max pooling uses the maximum value from each of a cluster of neurons at the prior layer. Another example is average pooling, which uses the average value from each of a cluster of neurons at the prior layer.|$|R
30|$|As noted above, {{when using}} log-m {{embedding}} for our two alphabet sizes, {{we have an}} input volume of 8 × l. Without padding, each convolutional layer reduces the dimension by 2. Thus, after three layers {{we are looking at}} an (8 - 3 * 2) × (l- 3 * 2) or 2 × (l- 6) volume. Since we have a dimension of 2 in one direction, no further convolutional layers with 3 × 3 filters can be applied. This sets a limit of three for the maximum number of 3 × 3 convolutional layers that can be used with this embedding. By using padding, this limit can be removed. Since padding adds a border of zeroes around our input volume to prevent its dimension from being reduced by each convolutional layer, we no longer have a limit to the number of layers employed. Thus, we can design a network similar to AlexNet [7], one of the highest performing network architectures for image classification. This architecture consists of stacking three 3 × 3 convolutional layers, followed by a 2 × 2 max <b>pooling</b> <b>layer.</b> Again, our small dimension (8) imposes a limit on the number of layer stacks we can have due to max pooling. After our first max <b>pooling</b> <b>layer,</b> we have a 4 × (l/ 2) volume, after the second we have a 2 × (l/ 4) volume and no further 3 × 3 convolutions can be performed. Thus, for the deepest network, we can construct an architecture consisting of three convolutional layers followed by a max <b>pooling</b> <b>layer,</b> three more convolutional layers, a second max <b>pooling</b> <b>layer,</b> then the fully connected layers.|$|E
40|$|Recently, {{convolutional}} {{neural networks}} (CNNs) {{have been shown}} to outperform the standard fully connected deep neu-ral networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recogni-tion task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate sev-eral CNN architectures, including full and limited weight shar-ing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax <b>pooling</b> <b>layer</b> so that the size in the <b>pooling</b> <b>layer</b> can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have in-vestigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recog-nition tasks. The architecture with limited weight sharing pro-vides additional gains over the full weight sharing architecture. The softmax <b>pooling</b> <b>layer</b> performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretrain-ing produces significantly better results on a large vocabulary speech recognition task...|$|E
30|$|Convolutional layer 1. 48 kernels of size 3 × 7 × 7 (3 {{represents}} {{the number of}} RGB channels, with a stride of 2) are applied to the input banana image in the first layer combined with the ReLU. A max <b>pooling</b> <b>layer</b> follows this convolutional layer.|$|E
30|$|In recent years, CNNs have {{achieved}} great success in classification task [21] due to large-scale databases and efficient computational abilities. Simonyan and Zisserman [31] proposed very deep neural networks which consist of more convolutional and <b>pooling</b> <b>layers.</b> The deep CNNs not only obtain outstanding performance on imagenet large-scale visual recognition challenge (ILSVRC), but also show promising performance on other classification tasks. Hence, we employ a deep CNN to extract features for cloud images.|$|R
40|$|In {{this work}} we compute lower Lipschitz bounds of ℓ_p pooling {{operators}} for p= 1, 2, ∞ {{as well as}} ℓ_p pooling operators preceded by half-rectification layers. These give sufficient conditions {{for the design of}} invertible neural network layers. Numerical experiments on MNIST and image patches confirm that <b>pooling</b> <b>layers</b> can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression. Comment: 17 pages, 3 figure...|$|R
40|$|This study {{determined}} and com-pared Salmonella contamination rates of pools of surplus, early and culled hatching eggs from layer and broiler breeder flocks, and of pools of early and regular table eggs from <b>layer</b> flocks. Each <b>pool</b> contained 6 eggs. Five methods {{were used for}} the isolation of Salmonella. Nine of 126 <b>pools</b> of culled <b>layer</b> hatching eggs, 2 of 126 <b>pools</b> of surplus <b>layer</b> hatching eggs, and one of 126 <b>pools</b> of early <b>layer</b> hatching eggs were contaminated with Salmonella. Al...|$|R
