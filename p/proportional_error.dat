48|183|Public
50|$|In simple cases, a {{forecast}} is {{compared with an}} outcome at a single time-point and a summary of forecast errors is constructed over a collection of such time-points. Here the forecast may be assessed using the difference or using a <b>proportional</b> <b>error.</b> By convention, the error is defined using {{the value of the}} outcome minus the value of the forecast.|$|E
40|$|The least {{upper bound}} on the overall <b>proportional</b> <b>error</b> that results from the {{simplification}} of an input—output matrix is a useful measure of the information loss. In particular, it is proven {{that some of the}} available results on this bound can be used to reduce the computations required for an optimal introduction of zeros. Furthermore, it is shown that the matrix that solves the simplification problem for any given level of error is generally not unique, so {{that it is possible to}} impose a priori constraints on the pattern of zeros in the matrix. Input–output, simplification, overall <b>proportional</b> <b>error,...</b>|$|E
40|$|CHANGES IN BayesFactor VERSION 0. 9. 10 CHANGES 	Fixed bug {{in model}} {{enumeration}} code in generalTestBF (affected "withmain" analyses with neverExclude argument) 	Various bug fixes 	Analyses are more error tolerant (problem analyses will yield NA in BayesFactor object) 	Fixed some typos in citation information 	Improved numerical stability of <b>proportional</b> <b>error</b> estimate...|$|E
40|$|We {{consider}} {{the extent to}} which Markov chain convergence properties are affected by the presence of computer floating-point roundoff error. Both geometric ergodicity and polynomial ergodicity are considered. This paper extends previous work of Roberts et al. (J. Appl. Probab. 35 (1998) 1) to the case of <b>proportional</b> <b>errors.</b> ...|$|R
3000|$|Velocity {{calculations}} {{also provide}} a further improvement in RISS since it uses speed sensor reading {{as a replacement for}} accelerometers. Velocity calculation based on accelerometers introduces velocity <b>error</b> <b>proportional</b> to t and position <b>error</b> <b>proportional</b> to t [...]...|$|R
40|$|Abstract: Dentin {{permeability}} {{was measured}} alternatively with two methods: a 10 -ul capillary method with visual evaluation (PC) and a motorized automatic measuring device (Flodec, FD), both interposed in a simulated perfusion system. Eight human third molar coronal fragments {{were connected to}} systems, and their permeability to distilled water measured at 0, 5, 10, 15, 20, 25, and 29 cmH 2 O pressure. Resultant permeabilities (in ul/s) for both techniques were interrelated {{with the use of}} the Passing and Bablok nonparametric method, which gives information about the range of constant and <b>proportional</b> <b>errors</b> and their 95...|$|R
30|$|Column (3) {{presents}} the interval by interval population growth rates. After some 130  years, {{the growth rate}} of the projected population becomes quite close to the ultimate stable population growth rate of 0.9518. Column (5) compares the Leslie and analytical projections, showing the <b>proportional</b> <b>error</b> in the Eq. (10) projection. The direction of the error varies because of the fluctuations in {{the growth rate of}} the projected population. After 120  years, the two trajectories are within 1 % of each other, with the difference inconsequential after 145  years.|$|E
40|$|Model 919 kinetic {{methods for}} {{determination}} of serum creatinine. These methods, which involve the Jaff#{ 233 }reac-tion, all showed {{a statistically significant}} <b>proportional</b> <b>error</b> relative to an ion-exchange pretreatment method for creatinine in 92 serum samples; the GEMSAEC method also showed a bias. The positive interference of several acid metabolites was quantified {{in each of the}} methods. The evidence indicates that bilirubin, although associated with a negative creatinine interference, is not the sole in-terferent. We determined the magnitude of this interfer-ence and studied several methods for minimizing it...|$|E
40|$|This {{document}} proposes analysing {{and designing}} two control strategies for permanent current DC-DC buck converter. These kinds of electronic devices convert a constant-voltage {{to a lower}} constant-voltage (nonlinearity characteristics being demonstra- ted). Two nonlinear control techniques are shown. The first is a conventional optimal <b>proportional</b> <b>error</b> and integral error (PI) controller based on minimising integral of time per squared errors (ITSE) criteria. A model of the plant at an operation point was thus obtained. The second one was fuzzy control where input and output sets were also defined by minimising ITSE criteria in the overall system and establishing inputs such as <b>proportional</b> <b>error</b> and integral error. Load was then varied to establish the system’s efficiency with both the aforementioned controllers. It must be borne in mind that such systems should not present super-voltage since this can cause damage. The parameters found in designing both controllers thus corresponded to analytical and descriptive methodology. Simulation results, the performance index mentioned above (ITSE) and power consumption showed that the sys- tem’s response for the fuzzy control drew more power consumption than the optimal controller; otherwise, obtained ITSE was lar- ger for the optimal control than the fuzzy control. It is concluded that exploring these types of converter is applicable when using nonlinear control techniques and minimising the different performance indices...|$|E
40|$|A note on {{geometric}} ergodicity and floating-point roundoff error by Laird Breyer*, Gareth O. Roberts*, and Jeffrey S. Rosenthal** (August 8, 2000; {{revised and}} shortened, November 15, 2000.) Abstract. We consider {{the extent to}} which Markov chain convergence properties are affected by the presence of computer floating-point roundoff error. This paper extends previous work of Roberts, Rosenthal, and Schwartz (1998) to the case of <b>proportional</b> <b>errors.</b> 1. Introduction. Geometric ergodicity is an important concept in convergence of Markov chains to their stationary distributions. For example, this property is used to justify the applicability of the central limit theorem to ergodic averages along the path of the chain. When run on an actual computer, Markov chains are subject to floating-point roundoff errors. This pape...|$|R
3000|$|A good {{approximation}} {{of the power}} series involved in the computation, with a relatively sharp estimate of the approximation error, {{can be obtained by}} calculating the sum of a small number of its leading summands. Thus, it is possible to significantly reduce the computational effort involved in the calculation of the derivatives. The calculation of the upper bound of the approximation error is described in Appendix 2. The calculation of the derivative is the crucial step to determine the relevant derivatives of the current configuration x([...] ξ,u) = e^Z̅([...] ξ,u) x_ 0 ([...] ξ), which is otherwise straightforward. We also note that small <b>proportional</b> <b>errors</b> in the calculation of the derivatives do not affect the order of convergence of the Newton-Raphson root-finding algorithm [1, p.  155].|$|R
50|$|Unfortunately, {{this is not}} {{a useful}} {{algorithm}} for generating sine tables because it has a significant <b>error,</b> <b>proportional</b> to 1/N.|$|R
40|$|With {{respect to}} the problem of {{complexity}} and uncertainty in the MRO (Maintenance, Repair and Overhaul) spare parts inventory, an optimized grey forecasting model is employed to forecast the demand of spare parts. The parameter g is optimized based on genetic algorithm (GA) method in the unbiased GM (1, 1) power model to minimize the ARPE (Average Relative <b>Proportional</b> <b>Error)</b> of accuracy. And the optimal model is used to forecast the prediction demand in a practical example. The experiment results indicate the forecasting accuracy can be accepted by using the optimized unbiased GM (1, 1) power model. </p...|$|E
40|$|We {{evaluated}} three commercial radioimmunoassay kits (Amersham, Dainabot, Clinical Assays) and two non-commercial {{methods for}} determining aipha-fetoprotein in maternal serum during pregnancy. All five procedures {{were found to}} be acceptable with respect to practicability, sensitivity, linearity, and precision. Similar results were obtained with Dainabot, Clinical Assays, and the two non-Commercial methods, but the Amersham method revealed a <b>proportional</b> <b>error,</b> results being about 20 % lower than those by the other methods. Use of the inter-national unit system is suggested for reporting results for AFP, to facilitate comparison between methods and lab-oratories. AdditIonal Keyphrases: “kit”methods. fetal status neural tube defec...|$|E
40|$|Aggregation {{methods are}} the most common way of {{upscaling}} land cover maps. To analyze the impact of land cover mapping error on upscaling agricultural maps, we utilized the Cropland Data Layer (CDL) data with corresponding confidence level data and simulated eight levels of error using a Monte Carlo simulation for two Agriculture Statistic Districts (ASD) in the U. S. A. The results of the simulations were used as base maps for subsequent upscaling, utilizing the majority rule based aggregation method. The results show that increasing error level resulted in higher proportional errors for each crop in both study areas. As a result of increasing error level, landscape characteristics of the base map also changed greatly resulting in higher <b>proportional</b> <b>error</b> in the upscaled maps. Furthermore, the <b>proportional</b> <b>error</b> is sensitive to the crop area proportion in the base map and decreases as the crop proportion increases. These findings indicate that three factors, the error level of the thematic map, the change in landscape pattern/characteristics of the thematic map, and the objective of the project, should be considered before performing any upscaling. The first two factors can be estimated by using pre-existing land cover maps with relatively high accuracy. The third factor is dependent on the project requirements (e. g., landscape characteristics, proportions of cover types, and use of the upscaled map). Overall, improving our understanding of the impacts of land cover mapping error is necessary to the proper design for upscaling and obtaining the optimal upscaled map...|$|E
40|$|Thesis (M. S.) University of Alaska Fairbanks, 2014 In {{this thesis}} we develop an {{intuitive}} process of encoding any phylogenetic tree {{and its associated}} tree-distance matrix {{as a collection of}} points in Euclidean space. Using this encoding, we find that information about the structure of the tree can easily be recovered by applying the inner product operation to vector combinations of the Euclidean points. By applying Classical Scaling to the tree-distance matrix, we are able to find the Euclidean points even when the phylogenetic tree is not known. We use the insight gained by encoding the tree as a collection of Euclidean points to modify the Neighbor Joining Algorithm, a method to recover an unknown phylogenetic tree from its tree-distance matrix, to be more resistant to tree-distance <b>proportional</b> <b>errors...</b>|$|R
5000|$|Assign {{a weight}} [...] to [...] that is {{inversely}} <b>proportional</b> to the <b>error</b> rate. In this way best classifiers are considered more.|$|R
25|$|The Goodman and Kruskal's lambda in {{statistics}} {{indicates the}} <b>proportional</b> reduction in <b>error</b> when one variable's values {{are used to}} predict the values of another variable.|$|R
40|$|Approved {{for public}} release; {{distribution}} in unlimited. This research is {{an investigation of}} the effects of instruction set on tracking performance. Two sets of instructions, one emphasized accuracy while the other emphasized speed, were tested. Number of errors, time of errors and time to completion under each instruction set were measured. <b>Proportional</b> <b>error</b> in time, mean time of single errors and mean interval between successive errors were extracted and discussed. Time to completion under the accuracw condition was approximatelw four times longer than under the speed condition, but the number and time of errors showed {{no significant difference between the}} two experimental conditions. [URL] Korean Air Forc...|$|E
40|$|We {{have studied}} the {{usefulness}} of common statisti-cal tests as applied to method comparison studies. We simulated different types of errors in test sets of data to determine the sensitivity of different statisti-cal parameters. Least-squares parameters (slope of least-squares line, its y intercept, and the {{standard error of estimate}} in the y direction) provide specific estimates of proportional, constant, and random er-rors, but comparison data must be presented graphi-cally to detect limitations caused by nonlinearity and errant points. t-test parameters (bias, standard de-viation of difference) provide estimates of constant and random errors, but only when <b>proportional</b> <b>error</b> is absent. Least-squares analysis can estimate pro-portional error and should be considered a prerequi...|$|E
40|$|To {{estimate}} {{the value of}} pulse wave velocity (PWV) in pediatric cardiovascular disease, prospective studies are needed. Various instruments based on different measurement principles are proposed for use in children, hence the need to test the comparability of these devices in this younger population. The objective {{of this study was}} to compare PWV measured by oscillometry (Vicorder (VIC)) with the gold standard of applanation tonometry (PulsePen (PP), Sphygmocor (SC)). PWV was measured in 98 children and young adults (age: 16. 7 (6. 3 - 26. 6) years (median(range)) with the above three devices at the same visit under standardized conditions. Mean PWV measured by VIC was significantly lower than that measured by SC and PP. There was no difference following path length correction of the VIC measurement (using the distance between the jugular notch and the center of the femoral cuff), (PP: 6. 12 (1. 00), SC: 5. 94 (0. 91), VIC: 6. 14 (0. 75) m s(- 1)). Velocities measured by the three devices showed highly significant correlations. Bland- Altman analysis revealed excellent concordance between all three devices, however, there was a small but significant <b>proportional</b> <b>error</b> in the VIC measurements showing a trend toward lower PWV measured by VIC at higher PWV values. Our study provides data on the three most frequently used instruments in pediatrics. Following path length correction of the VIC, all three devices provided comparable results. Thus, our work allows extrapolating data between previously established normal PWV values for children and forthcoming studies using these instruments to assess children at long-term risk of cardiovascular disease. The small <b>proportional</b> <b>error</b> of VIC needs additional technical development to improve the accuracy of the measurements. Hypertension Research advance online publication, 28 July 2011; doi: 10. 1038 /hr. 2011. 103...|$|E
50|$|Another simple type of {{controller}} is a proportional controller. With {{this type}} of controller, the controller output (control action) is <b>proportional</b> to the <b>error</b> in the measured variable.|$|R
50|$|The {{error signal}} e must be {{transformed}} to the output quantity qo (representing the participant's muscular efforts affecting the mouse position). Experiments {{have shown that}} in the best model for the output function, the mouse velocity Vcursor is <b>proportional</b> to the <b>error</b> signal e by a gain factor G (that is, Vcursor = G*e). Thus, when the perceptual signal p is smaller than the reference signal r, the error signal e has a positive sign, and from it the model computes an upward velocity of the cursor that is <b>proportional</b> to the <b>error.</b>|$|R
40|$|Apparatus {{consists}} of thermocouple connected to semiconductor reference junction. Junction {{is connected to}} amplifier that boosts signal by 1, 000. High-level signal is displayed on recorder and fed into second amplifier where it is compared with signal from potentiometer of programmed dc reference in bag-temperature programer. Difference in signals indicates output voltage. Remaining circuitry provides zero-phase, time-proportion control of heaters such that heater power is directly <b>proportional</b> to <b>error</b> signal...|$|R
3000|$|... is the inter-animal variability, {{which is}} assumed to be {{normally}} distributed around zero with a standard deviation ω, to distinguish the i th animal from the typical value as predicted from the regression model. Inter-animal variation was studied on all parameters and was included if the model was improved significantly (OFV > 3.83). To test the significance of the covariate inclusion, e.g., effect of tariquidar and SE, a stepwise forward addition and backward deletion approach was applied, and covariates were only kept in the model if they significantly improved the model. Finally, <b>proportional</b> <b>error</b> models were included for the residual variability, i.e., variability that remained unexplained after inclusion of inter-animal variability and covariates. More comprehensive description of NLME modeling can be found elsewhere, for example in the paper by Pillai et al.|$|E
40|$|It {{is often}} {{necessary}} to compare two measurement methods {{in medicine and}} other experimental sciences. This problem covers {{a broad range of}} data with applications arising from many different fields. The Bland-Altman method has been a favorite method for concordance assessment. However, the Bland-Altman approach creates a problem of interpretation for many applications when a mixture of fixed bias, proportional bias and/or <b>proportional</b> <b>error</b> occurs. In this paper, an improved Bland-Altman method is proposed to handle more complicated scenarios in practice. This new approach includes Bland-Altman's approach as its special case. We evaluate concordance by defining an agreement interval for each individual paired observation and assessing the overall concordance. The proposed interval approach is very informative and offers many advantages over existing approaches. Data sets are used to demonstrate the advantages of the new method. ...|$|E
40|$|We {{evaluated}} four spectrophotometric {{methods for}} suitability in determining total protein in human milk samples. Results {{were compared with}} those by an established method for the determination of total nitrogen (micro-Kjeldahl). Values for total protein by all four methods significantly correlated with micro-Kjeldahl. For the biuret assay, Lowry-Peterson assay, Bio-Rad Coomassie Blue assay, and Pierce BCA assay, the correlation coefficients were 0. 96, 0. 97, 0. 89, 0. 99, respectively. We also assessed the sensitivity of each assay to several purified human milk proteins. The Pierce BCA assay showed the least difference in values among different types of protein. For this reason, and because it showed the greatest precision (with acceptable constant and <b>proportional</b> <b>error),</b> we recommend the BCA method. The prevalence and duration of breast feeding in th...|$|E
40|$|Aim: {{to study}} the {{effectiveness}} of an electronic apex locator (Justy II) in locating simulated horizontal and vertical fractures in single roots. Methods: an electronic apex locator (EAL) (Justy II, Yoshida Dentcraft, Tokyo, Japan) {{was used to measure}} the distance within the canal of horizontal (n= 31) and vertical (n= 31) fractures, created with a disk in single-rooted teeth. Accuracy of the EAL was evaluated by comparing the measurements with those made using a size 10 file. Data were analyzed with the non-parametric Passing and Bablok method. Results: for simulated horizontal fractures the EAL measured exactly the same length as a size 10 file, without constant or <b>proportional</b> <b>errors.</b> In vertical simulated fractures the EAL measured (on average) with a constant error of 7. 5 mm shorter than the size 10 file; the difference had a wide confidence interval (– 72. 3 to 2. 6 mm). Conclusion: in this laboratory study, the Justy II EAL was able to determine accurately the position of simulated horizontal fractures but was unreliable when measuring simulated vertical fractures. ...|$|R
5000|$|The <b>error</b> is {{asymptotically}} <b>proportional</b> to [...] However, {{the above}} derivations suggest an <b>error</b> <b>proportional</b> to [...] Simpson's rule gains an extra order because the points {{at which the}} integrand is evaluated are distributed symmetrically in the interval b.|$|R
30|$|The above {{equation}} {{shows that}} the overall error probability is directly <b>proportional</b> to the <b>error</b> happening at both the MA and BC stages, where the MA stage is the dominant factor due to {{the addition of the}} two signals (MA interference).|$|R
40|$|For a {{class of}} high-gain stabilizable multivariable linear infinite-dimensional systems we present an {{adaptive}} control law which achieves approximate asymptotic tracking {{in the sense that}} the tracking error tends asymptotically to a ball centred at 0 and of arbitrary prescribed radius ? 0. This control strategy, called -tracking, combines <b>proportional</b> <b>error</b> feedback with a simple nonlinear adaptation of the feedback gain. It does not involve any parameter estimation algorithms, nor is it based on the internal model principle. The class of reference signals is W 1; 1, the Sobolev space of absolutely continuous functions which are bounded and have essentially bounded derivative. The control strategy is robust with respect to output measurement noise in W 1; 1 and bounded input disturbances. We apply our results to retarded systems and integrodifferential systems. Keywords: Adaptive control; tracking; high-gain control; infinite-dimensional systems; functional differential equations [...] ...|$|E
40|$|In {{the last}} decades both {{technological}} process’ improvement and primary energy resources saving are the main tasks of oil refineries. Using various oil products does impose an accurate knowledge of their properties. The dispersion analysis applied makes possible to construct a model simulating the primary oil refining products’ and raw materials’ thermal physical properties. As a result of data approximation there were obtained polynomials with coefficients differing from attributable to the studied oil products fractions. The research represents graphic dependences of thermal physical properties on temperature values for diesel oil fraction. The linear character of density and calorific capacity dependencies from temperature is represented with a <b>proportional</b> <b>error</b> in calculations. The relative minimum error is below 2 % that confirms the implemented calculations’ adequacy. The resulting model {{can be used in}} calculations for further technological process improvements...|$|E
40|$|Liposomal {{amphotericin}} B (LAMB) and caspofungin (CAS) {{are important}} antifungal agents in allogeneic {{hematopoietic stem cell}} transplant (aHSCT) recipients. Little is known, however, about the pharmacokinetics (PK) of both agents and their combination in this population. The PK of LAMB and CAS {{and the potential for}} PK interactions between both agents were investigated within a risk-stratified, randomized phase II clinical trial in 53 adult aHSCT recipients with granulocytopenia and refractory fever. Patients received either LAMB (n = 17; 3 mg/kg once a day [QD]), CAS (n = 19; 50 mg QD; day 1, 70 mg), or the combination of both (CAS-LAMB; n = 17) for a median duration of 10 to 13 days (range, 4 to 28 days) until defervescence and granulocyte recovery. PK sampling was performed on days 1 and 4. Drug concentrations in plasma (LAMB, 405 samples; CAS, 458 samples) were quantified by high-pressure liquid chromatography and were analyzed using population pharmacokinetic modeling. CAS concentration data best fitted a two-compartment model with a <b>proportional</b> <b>error</b> model and interindividual variability (IIV) for clearance (CL) and central volume of distribution (V 1) (CL, 0. 462 liter/h ± 25 %; V 1, 8. 33 liters ± 29 %; intercompartmental clearance [Q], 1. 25 liters/h; peripheral volume of distribution [V 2], 3. 59 liters). Concentration data for LAMB best fitted a two-compartment model with a <b>proportional</b> <b>error</b> model and IIV for all parameters (CL, 1. 22 liters/h ± 64 %; V 1, 19. 2 liters ± 38 %; Q, 2. 18 liters/h ± 47 %; V 2, 52. 8 liters ± 84 %). Internal model validation showed predictability and robustness of both models. None of the covariates tested (LAMB or CAS comedication, gender, body weight, age, body surface area, serum bilirubin, and creatinine clearance) further improved the models. In summary, the disposition of LAMB and CAS was best described by two-compartment models. Drug exposures in aHSCT patients were comparable to those in other populations, and no PK interactions were observed between the two compounds...|$|E
40|$|Abstract: In {{this paper}} we study {{conditions}} under which an unstable stochastic scalar linear plant with unbounded noise can be internally stabilised using ‘zooming’-like coding and control schemes having dynamic, finite-dimensional internal states. Such structures {{are known to be}} needed in communicationconstrained control when no bound on the plant noise is available. However, previous schemes were based on coders and controllers starting with identical internal states. In this paper, we remove this assumption and explicitly construct a finite-dimensional coding and control policy that yields mean square stability of all state variables, for a random initial plant state and arbitrary initial encoder and controller states. This holds for any bit rate down to the universal minimum of the Data Rate Theorem. Furthermore, we show that despite the unbounded noise, the <b>error</b> and <b>proportional</b> <b>errors</b> between the scaling factors of the encoder and controller tend to zero in mean square and almost sure senses respectively. This suggests that the policy will still maintain mean square internal stability in the presence of channel bit errors, provided the bit error rate is sufficiently low. We support these conclusions with simulations...|$|R
40|$|This paper {{proposes a}} new method to {{estimate}} the ballistic coefficient (BC) of low earth orbit space debris. The data sources are the historical two-line elements (TLEs). Since the secular variation of semi-major axes is mainly caused by the drag perturbation for space objects with perigee altitude below 600  km, the ballistic coefficients are estimated based on variation of the mean semi-major axes derived from the TLEs. However, the approximate parameters used in the calculation have error, especially when the upper atmosphere densities are difficult to obtain and always estimated by empirical model. The <b>proportional</b> <b>errors</b> of the approximate parameters are cancelled out {{in the form of}} ratios, greatly mitigating the effects of model error. This method has been also been validated for space objects with perigee altitude higher than 600  km. The relative errors of estimated BC values from the new method are significantly smaller than those from the direct estimation methods used in numerical experiments. The estimated BC values are used for the prediction of the semi-major axes, and good performance is obtained. This process is also a feasible method for prediction {{over a long period of}} time without an orbital propagator model...|$|R
50|$|<b>Proportional</b> {{reduction}} in <b>error</b> {{is a more}} restrictive framework widely used in statistics, in which the general loss function {{is replaced by a}} more direct measure of error such as the mean square error. Examples are the coefficient of determination and Goodman and Kruskal's lambda.|$|R
