3751|0|Public
5|$|The rough idea {{of these}} inapproximability results is {{to form a}} graph that {{represents}} a <b>probabilistically</b> checkable proof system for an NP-complete problem such as the Boolean satisfiability problem. In a <b>probabilistically</b> checkable proof system, a proof is represented as a sequence of bits. An instance of the satisfiability problem should have a valid proof {{if and only if}} it is satisfiable. The proof is checked by an algorithm that, after a polynomial-time computation on the input to the satisfiability problem, chooses to examine a small number of randomly chosen positions of the proof string. Depending on what values are found at that sample of bits, the checker will either accept or reject the proof, without looking at the rest of the bits. False negatives are not allowed: a valid proof must always be accepted. However, an invalid proof may sometimes mistakenly be accepted. For every invalid proof, the probability that the checker will accept it must be low.|$|E
5|$|Photons, {{like all}} quantum objects, exhibit wave-like and particle-like properties. Their dual wave–particle nature can be {{difficult}} to visualize. The photon displays clearly wave-like phenomena such as diffraction and interference on the length scale of its wavelength. For example, a single photon passing through a double-slit experiment exhibits interference phenomena but only if no measure was made at the slit. A single photon passing through a double-slit experiment lands on the screen with a probability distribution given by its interference pattern determined by Maxwell's equations. However, experiments confirm that the photon is not a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted as a whole by arbitrarily small systems, systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron. Nevertheless, the photon is not a point-like particle whose trajectory is shaped <b>probabilistically</b> by the electromagnetic field, as conceived by Einstein and others; that hypothesis was also refuted by the photon-correlation experiments cited above. According to our present understanding, the electromagnetic field itself is produced by photons, which in turn result from a local gauge symmetry and the laws of quantum field theory (see the Second quantization and Gauge boson sections below).|$|E
5|$|To {{transform}} a <b>probabilistically</b> checkable proof system {{of this type}} into a clique problem, one forms a graph with a vertex for each possible accepting run of the proof checker. That is, a vertex is defined {{by one of the}} possible random choices of sets of positions to examine, and by bit values for those positions that would cause the checker to accept the proof. Two vertices are adjacent, in this graph, if the corresponding two accepting runs see the same bit values at every position they both examine. Each (valid or invalid) proof string corresponds to a clique, the set of accepting runs that see that proof string, and all maximal cliques arise in this way. One of these cliques is large if and only if it corresponds to a proof string that many proof checkers accept. If the original satisfiability instance is satisfiable, it will have a valid proof string, one that is accepted by all runs of the checker, and this string will correspond to a large clique in the graph. However, if the original instance is not satisfiable, then all proof strings are invalid, each proof string has {{only a small number of}} checker runs that mistakenly accept it, and all cliques are small. Therefore, if one could distinguish in polynomial time between graphs that have large cliques and graphs in which all cliques are small, or if one could accurately approximate the clique problem, then applying this approximation to the graphs generated from satisfiability instances would allow satisfiable instances to be distinguished from unsatisfiable instances. However, this is not possible unless P=NP.|$|E
25|$|The two sums of squares are <b>probabilistically</b> independent.|$|E
25|$|Las Vegas {{algorithms}} always {{return the}} correct answer, but their running time is only <b>probabilistically</b> bound, e.g. ZPP.|$|E
25|$|In a large {{population}} model, players choose their next action <b>probabilistically</b> based on which strategies are best responses to {{the population as a}} whole.|$|E
25|$|A more {{intelligent}} surfer that <b>probabilistically</b> hops from page to page {{depending on the}} content of the pages and query terms the surfer that it is looking for.|$|E
25|$|Though {{probability}} initially had somewhat mundane motivations, {{its modern}} influence and use is widespread ranging from evidence-based medicine, through Six sigma, {{all the way}} to the <b>Probabilistically</b> checkable proof and the String theory landscape.|$|E
25|$|Simulated {{annealing}} (SA) is {{a related}} global optimization technique which traverses the search space by generating neighboring solutions {{of the current}} solution. A superior neighbor is always accepted. An inferior neighbor is accepted <b>probabilistically</b> based on the difference in quality and a temperature parameter. The temperature parameter is modified as the algorithm progresses to alter {{the nature of the}} search.|$|E
25|$|The {{concept of}} the {{probability}} distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that {{can be measured in}} a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described <b>probabilistically,</b> from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.|$|E
25|$|ACS {{sends out}} {{a large number of}} virtual ant agents to explore many {{possible}} routes on the map. Each ant <b>probabilistically</b> chooses the next city to visit based on a heuristic combining the distance to the city and the amount of virtual pheromone deposited on the edge to the city. The ants explore, depositing pheromone on each edge that they cross, until they have all completed a tour. At this point the ant which completed the shortest tour deposits virtual pheromone along its complete tour route (global trail updating). The amount of pheromone deposited is inversely proportional to the tour length: the shorter the tour, the more it deposits.|$|E
500|$|... "A home {{built in}} any of the <b>probabilistically</b> defined {{inundation}} areas on the new maps {{is more likely to be}} damaged or destroyed by a lahar than by fire...For example, a home built in an area that would be inundated every 100 years, on the average, is 27 times more likely to be damaged or destroyed by a flow than by fire. People know the danger of fire, so they buy fire insurance and they have smoke alarms, but most people are not aware of the risks of lahars, and few have applicable flood insurance." ...|$|E
500|$|Weak results {{hinting that}} the clique problem {{might be hard}} to {{approximate}} have been {{known for a long}} time. [...] observed that, {{because of the fact that}} the clique number takes on small integer values and is NP-hard to compute, it cannot have a fully polynomial-time approximation scheme. If too accurate an approximation were available, rounding its value to an integer would give the exact clique number. However, little more was known until the early 1990s, when several authors began to make connections between the approximation of maximum cliques and <b>probabilistically</b> checkable proofs. They used these connections to prove hardness of approximation results for the maximum clique problem.|$|E
2500|$|... where , [...] and [...] are <b>probabilistically</b> independent: [...] and so on. The {{condition}} {{that is necessary}} for such a splitting of H and E to be possible is , that is, that [...] is <b>probabilistically</b> supported by [...]|$|E
2500|$|Physical theory {{will evolve}} {{so as to}} {{strengthen}} the hypothesis that early phase transitions occur <b>probabilistically</b> rather than deterministically, in which case {{there will be no}} deep physical reason for the values of fundamental constants; ...|$|E
2500|$|Energy {{minimization}} and PCFG provide ways {{of predicting}} RNA secondary structure with comparable performance. However structure prediction by PCFGs is scored <b>probabilistically</b> {{rather than by}} minimum free energy calculation. PCFG model parameters are directly derived from frequencies of different features observed in databases of RNA structures [...] rather than by experimental ...|$|E
2500|$|This {{leads to}} the predict and update steps of the Kalman filter written <b>probabilistically.</b> The {{probability}} distribution associated with the predicted state is the sum (integral) of {{the products of the}} probability distribution associated with the transition from the (k−1)-th timestep to the k-th and the probability distribution associated with the previous state, over all possible [...]|$|E
2500|$|All anti-coordination {{games have}} three Nash equilibria. [...] Two {{of these are}} pure {{contingent}} strategy profiles, in which each player plays one of the pair of strategies, and the other player chooses the opposite strategy. [...] The third one is a mixed equilibrium, in which each player <b>probabilistically</b> chooses between the two pure strategies. Either the pure, or mixed, Nash equilibria will be evolutionarily stable strategies depending upon whether uncorrelated asymmetries exist.|$|E
2500|$|In some {{contexts}} {{the concept}} of stopping time is defined by requiring only that the occurrence or non-occurrence of the event τ=t is <b>probabilistically</b> independent of Xt+1,Xt+2,... but not that it is completely determined by {{the history of the}} process up to time t. [...] That is a weaker condition than the one appearing in the paragraph above, but is strong enough to serve in some of the proofs in which stopping times are used.|$|E
2500|$|At each step, the SA {{heuristic}} considers some {{neighbouring state}} s' {{of the current}} state s, and <b>probabilistically</b> decides between moving the system to state s' or staying in state s. [...] These probabilities ultimately lead the system to move to states of lower energy. [...] Typically this step is repeated until the system reaches a state that {{is good enough for}} the application, or until a given computation budget has been exhausted.|$|E
2500|$|Some {{versions}} of the Copenhagen interpretation of quantum mechanics proposed a process of [...] "collapse" [...] in which an indeterminate quantum system would <b>probabilistically</b> collapse down onto, or select, just one determinate outcome to [...] "explain" [...] this phenomenon of observation. Wavefunction collapse was widely regarded as artificial and ad hoc, so an alternative interpretation in which the behavior of measurement could be understood from more fundamental physical principles was considered desirable.|$|E
2500|$|Some (including Albert Einstein) {{argue that}} our {{inability}} to predict any more than probabilities is simply due to ignorance. The idea is that, beyond the conditions and laws we can observe or deduce, there are also hidden factors or [...] "hidden variables" [...] that determine absolutely in which order photons reach the detector screen. They argue that {{the course of the}} universe is absolutely determined, but that humans are screened from knowledge of the determinative factors. So, they say, it only appears that things proceed in a merely <b>probabilistically</b> determinative way. In actuality, they proceed in an absolutely deterministic way.|$|E
2500|$|In Niels Bohr's mature view, quantum {{mechanical}} phenomena {{are required}} to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics. The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, {{in terms of both}} position and momentum, of an initial condition or [...] "state" [...] (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition. In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous [...] "state" [...] in the classical sense of that word. Thus {{there are two kinds of}} processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined. Given its initial condition, prediction of its final condition is possible, causally but only <b>probabilistically,</b> because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only <b>probabilistically.</b>|$|E
2500|$|Propensities, or chances, are not {{relative}} frequencies, but purported {{causes of}} the observed stable relative frequencies. [...] Propensities are invoked to explain why repeating {{a certain kind of}} experiment will generate given outcome types at persistent rates, which are known as propensities or chances. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives (see [...] "single case possible" [...] in the table above). In contrast, a propensitist is able to use the law of large numbers to explain the behaviour of long-run frequencies. [...] This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, {{in such a way that}} its probability of landing heads is the same on each toss, and the outcomes are <b>probabilistically</b> independent, then the relative frequency of heads will be close to the probability of heads on each single toss. [...] This law allows that stable long-run frequencies are a manifestation of invariant single-case probabilities. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time.|$|E
2500|$|Writers do not all {{follow the}} same terminology. The phrase [...] "statistical interpretation", {{referring}} to the [...] "ensemble interpretation", often indicates {{an interpretation of the}} Born rule somewhat different from the Copenhagen interpretation. For the Copenhagen interpretation, it is axiomatic that the wave function exhausts all that can ever be known in advance about a particular occurrence of the system. The [...] "statistical" [...] or [...] "ensemble" [...] interpretation, on the other hand, is explicitly agnostic about whether the information in the wave function is exhaustive of what might be known in advance. It sees itself as more 'minimal' than the Copenhagen interpretation in its claims. It only goes as far as saying that on every occasion of observation, some actual value of some property is found, and that such values are found <b>probabilistically,</b> as detected by many occasions of observation of the same system. The many occurrences of the system are said to constitute an 'ensemble', and they jointly reveal the probability through these occasions of observation. Though they all have the same wave function, the elements of the ensemble might not be identical to one another in all respects, according to the 'agnostic' interpretations. They may, for all we know, beyond current knowledge and beyond the wave function, have individual distinguishing properties. For present-day science, the experimental significance of these various forms of Born's rule is the same, since they make the same predictions about the probability distribution of outcomes of observations, and the unobserved or unactualized potential properties are not accessible to experiment.|$|E
50|$|Property testing {{algorithms}} {{are central}} to the definition of <b>probabilistically</b> checkable proofs, as a <b>probabilistically</b> checkable proof is essentially a proof that can be verified by a property testing algorithm.|$|E
5000|$|... where , [...] and [...] are <b>probabilistically</b> independent: [...] and so on. The {{condition}} {{that is necessary}} for such a splitting of H and E to be possible is , that is, that [...] is <b>probabilistically</b> supported by [...]|$|E
5000|$|The PCP theorem is the {{culmination}} of a long line of work on interactive proofs and <b>probabilistically</b> checkable proofs. The first theorem relating standard proofs and <b>probabilistically</b> checkable proofs is the statement that NEXP ⊆ PCPpoly(n), proved by [...]|$|E
50|$|The {{theory of}} <b>probabilistically</b> checkable proofs studies {{the power of}} <b>probabilistically</b> checkable proof systems under various {{restrictions}} of the parameters (completeness, soundness, randomness complexity, query complexity, and alphabet size). It has applications to computational complexity (in particular hardness of approximation) and cryptography.|$|E
50|$|Probabilistic {{completeness}} is {{the property}} that as more “work” is performed, {{the probability that}} the planner fails to find a path, if one exists, asymptotically approaches zero. Several sample-based methods are <b>probabilistically</b> complete. The performance of a <b>probabilistically</b> complete planner is measured by the rate of convergence.|$|E
50|$|The {{research}} strategy predicts {{that it is}} more likely that in the long term infrastructure <b>probabilistically</b> determines structure, which <b>probabilistically</b> determines the superstructures, than otherwise. Thus, much as in earlier Marxist thought, material changes (such as in technology or environment) are seen as largely determining patterns of social organization and ideology in turn.|$|E
50|$|The {{definition}} of a <b>probabilistically</b> checkable proof was explicitly introduced by Arora and Safra in 1992, although their properties were studied earlier. In 1990 Babai, Fortnow, and Lund proved that PCPpoly(n) = NEXP, providing the first nontrivial equivalence between standard proofs (NEXP) and <b>probabilistically</b> checkable proofs. The PCP theorem proved in 1992 states that PCPn),O(1) = NP.|$|E
5000|$|Las Vegas {{algorithms}} always {{return the}} correct answer, but their running time is only <b>probabilistically</b> bound, e.g. ZPP.|$|E
50|$|Sanjeev Arora (born January 1968) is an Indian American {{theoretical}} {{computer scientist}} who {{is best known}} for his work on <b>probabilistically</b> checkable proofs and, in particular, the PCP theorem. He is currently the Charles C. Fitzmorris Professor of Computer Science at Princeton University, and his research interests include computational complexity theory, uses of randomness in computation, <b>probabilistically</b> checkable proofs, computing approximate solutions to NP-hard problems, and geometric embeddings of metric spaces.|$|E
50|$|Alternatively, {{the unique}} games {{conjecture}} postulates {{the existence of}} a certain type of <b>probabilistically</b> checkable proof for problems in NP.|$|E
50|$|Postpositivists {{believe that}} a reality exists, like positivists do, though they hold {{that it can be}} known only imperfectly and <b>probabilistically.</b>|$|E
