626|2214|Public
5000|$|<b>Partial</b> <b>regression</b> plot : In applied statistics, a <b>partial</b> <b>regression</b> plot {{attempts}} {{to show the}} effect of adding another variable to the model (given that one or more independent variables are already in the model). <b>Partial</b> <b>regression</b> plots are also referred to as added variable plots, adjusted variable plots, and individual coefficient plots.|$|E
50|$|In applied statistics, a <b>partial</b> <b>regression</b> plot {{attempts}} {{to show the}} effect of adding another variable to a model already having one or more independent variables. <b>Partial</b> <b>regression</b> plots are also referred to as added variable plots, adjusted variable plots, and individual coefficient plots.|$|E
50|$|<b>Partial</b> <b>regression</b> plots {{are related}} to, but {{distinct}} from, partial residual plots. <b>Partial</b> <b>regression</b> plots are {{most commonly used}} to identify data points with high leverage and influential data points {{that might not have}} high leverage. Partial residual plots are most commonly used to identify {{the nature of the relationship}} between Y and Xi (given the effect of the other independent variables in the model). Note that since the simple correlation between the two sets of residuals plotted is equal to the partial correlation between the response variable and Xi, <b>partial</b> <b>regression</b> plots will show the correct strength of the linear relationship between the response variable and Xi. This is not true for partial residual plots. On the other hand, for the <b>partial</b> <b>regression</b> plot, the x-axis is not Xi. This limits its usefulness in determining the need for a transformation (which is the primary purpose of the partial residual plot).|$|E
40|$|Abstract—In this paper, {{estimation}} of the linear regression model is made by ordinary least squares method and the partially linear regression model is estimated by penalized least squares method using smoothing spline. Then, it is investigated that differences and similarity in the sum of squares related for linear <b>regression</b> and <b>partial</b> linear <b>regression</b> models (semi-parametric regression models). It is denoted that the sum of squares in linear regression is reduced to sum of squares in <b>partial</b> linear <b>regression</b> models. Furthermore, we indicated that various sums of squares in the linear regression are similar to different deviance statements in <b>partial</b> linear <b>regression.</b> In addition to, coefficient of the determination derived in linear regression model is easily generalized to coefficient of {{the determination of the}} <b>partial</b> linear <b>regression</b> model. For this aim, it is made two different applications. A simulated and a real data set are considered to prove the claim mentioned here. In this way, this study is supported with a simulation and a real data example...|$|R
40|$|This paper studies {{efficient}} {{estimation of}} <b>partial</b> linear <b>regression</b> in time series models. In particular, it combines two topics that have attracted {{a good deal}} of attention in econometrics, viz. spectral <b>regression</b> and <b>partial</b> linear <b>regression,</b> and proposes an efficient frequency domain estimator for partial linear models with serially correlated residuals. A nonparametric treatment of regression errors is permitted so that {{it is not necessary to}} be explicit about the dynamic specification of the errors other than to assume stationarity. A new concept of weak dependence is introduced based on regularity conditions on the joint density. Under these and some other regularity conditions, it is shown that the spectral estimator is root-n-consistent, asymptotically normal, and asymptotically efficient. Efficient estimation, <b>Partial</b> linear <b>regression,</b> Spectral regression, Kernel estimation, Nonparametric, Semiparametric, Weak dependence...|$|R
40|$|We have {{previously}} proposed the <b>partial</b> quantile <b>regression</b> (PQR) prediction procedure for functional linear model by using partial quantile covariance techniques and developed the simple <b>partial</b> quantile <b>regression</b> (SIMPQR) algorithm to efficiently extract PQR basis for estimating functional coefficients. However, although the PQR approach is considered as an attractive alternative to projections onto the principal component basis, {{there are certain}} limitations to uncovering the corresponding asymptotic properties mainly because of its iterative nature and the non-differentiability of the quantile loss function. In this article, we propose and implement an alternative formulation of <b>partial</b> quantile <b>regression</b> (APQR) for functional linear model by using block relaxation method and finite smoothing techniques. The proposed reformulation leads to insightful results and motivates new theory, demonstrating consistency and establishing convergence rates by applying advanced techniques from empirical process theory. Two simulations and two real data from ADHD- 200 sample and ADNI are investigated to show the superiority of our proposed methods...|$|R
5000|$|The meiosis {{determination}} (b2) is {{the coefficient}} of determination of meiosis, which is the cell-division whereby parents generate gametes. Following the principles of standardized <b>partial</b> <b>regression,</b> of which path analysis is a pictorially oriented version, Sewall Wright analyzed the paths of gene-flow during sexual reproduction, and established the [...] "strengths of contribution" [...] (coefficients of determination) of various components to the overall result. Path analysis includes partial correlations as well as <b>partial</b> <b>regression</b> coefficients (the latter are the path coefficients). Lines with a single arrow-head are directional determinative paths, and lines with double arrow-heads are correlation connections. Tracing various routes according to path analysis rules emulates the algebra of standardized <b>partial</b> <b>regression.</b>|$|E
5000|$|... #Caption: <b>Partial</b> <b>regression</b> is used {{to detect}} the maximum range of no {{influence}} in wheat fields. Tolerance is about ECe=5 dS/m ...|$|E
50|$|Note {{that the}} partial {{leverage}} is {{the leverage of}} the ith point in the <b>partial</b> <b>regression</b> plot for the jth variable. Data points with large partial leverage for an independent variable can exert undue influence on the selection of that variable in automatic regression model building procedures.|$|E
40|$|Radiotherapy (RT) is a {{standard}} treatment for breast cancer (BC) patients with metastatic brain involvement. All patients (n = 15) had already received chemotherapy (CT) for the underlying disease when they {{were found to have}} brain metastases. To develop effective CT regimens for patients with recurrent brain metastases, who have received RT to the brain, is a serious problem. Combination CT with gemcitabine and cisplatin showed a high efficacy (complete and <b>partial</b> <b>regressions</b> were achieved in 47. 7 % of cases) and fair survival rates (median 10 months) in a group of patients with BC brain metastases and a poor prognosis...|$|R
40|$|Abstract. The {{rolling motion}} {{analysis}} of ships or other marine structures is of paramount importance. However, {{one of the}} thorniest issues in the analysis is the determination of roll damping. The main objective of this work is to apply the <b>Partial</b> Least-Squares <b>regression</b> into the „Bass Energy Method ‟ and „Roberts Method‟, which are used for the identification of non-linear roll damping parameters. When the number of sample points decreases due to limitations of the experimental conditions and other factors, {{the differences between the}} results obtained from <b>Partial</b> Least-Squares <b>Regression</b> and from traditional Least-Squares method demonstrate the applicability of the proposed method...|$|R
40|$|In this chapter, {{a survey}} of the theory behind the main chemometric methods used for multivariate {{calibration}} is presented. Ordinary least squares, multiple linear regression, principal component <b>regression,</b> <b>partial</b> least squares <b>regression</b> and principal covariate regression are discussed in detail. Tools for model diagnostics and model interpretation are presented, together with strategies for variable selection...|$|R
50|$|A third {{model is}} based on the method of <b>partial</b> <b>regression,</b> whereby one finds the longest {{horizontal}} stretch (the range of no effect) of the yield-ECe relation while beyond that stretch the yield decline sets in (figure below). With this method the trend at the tail-end plays no role.|$|E
5000|$|... where Φ(·) is the {{cumulative}} distribution function of a Gaussian distribution with zero mean and unit standard deviation, and N is the sample size. This z-transform is approximate and that the actual distribution of the sample (partial) correlation coefficient is not straightforward. However, an exact t-test based {{on a combination of}} the <b>partial</b> <b>regression</b> coefficient, the partial correlation coefficient and the partial variances is available.|$|E
50|$|Path {{coefficients}} are standardized {{versions of}} linear regression weights {{which can be}} used in examining the possible causal linkage between statistical variables in the structural equation modeling approach. The standardization involves multiplying the ordinary regression coefficient by the standard deviations of the corresponding explanatory variable: these can then be compared to assess the relative effects of the variables within the fitted regression model. The idea of standardization can be extended to apply to <b>partial</b> <b>regression</b> coefficients.|$|E
40|$|An {{important}} {{application of}} microarray technology is to predict various clinical phenotypes {{based on the}} gene expression profile. Success has been demonstrated in molecular classification of cancer in which different types of cancer serve as categorical outcome variable. However, there has been less research in linking gene expression profile to censored survival outcome such as patients' overall survival time or time to cancer relapse. In this paper, we develop a <b>partial</b> Cox <b>regression</b> method for constructing mutually uncorrelated components based on microarray gene expression data for predicting the survival of future patients. The proposed <b>partial</b> Cox <b>regression</b> method involves constructing predictive components by repeated least square fitting of residuals and Cox regression fitting. The key difference from the standard principal components Cox regression analysis is that in constructing the predictive components, our method utilizes the observed survival/censoring information. We also propose to apply the time dependent receiver operating characteristic curve analysis to evaluate the results. We applied our methods to a publicly available data set of diffuse large B-cell lymphoma. The results indicated that combining the <b>partial</b> Cox <b>regression</b> method with principal components analysis results in parsimonious model with fewer components and better predictive performance. We conclude that the proposed <b>partial</b> Cox <b>regression</b> method can be very useful in building a parsimonious predictive model that can accurately predict the survival of future patients based on the gene expression profile and survival times of previous patients...|$|R
5000|$|Helland IS, On the {{structure}} of <b>partial</b> least squares <b>regression,</b> 1988, 246 cites.|$|R
40|$|The {{problem of}} semiparametric {{modelling}} in time series is considered. For this, <b>partial</b> linear <b>regression</b> models are used, that is, regression models where the regression func-tion {{is the sum}} of a linear and a nonparametric component. Two estimators for the nonparametric component are shown: one estimator takes into account the depend-ence structure in the errors of the regression function and the another estimator not. Both estimators are compared in practice for several real time series concerning eco-nomics and finance. In addition to the <b>partial</b> linear <b>regression</b> model, other regression models are included in the comparison. Key-Words: • nonparametric regression; time series. AMS Subject Classification...|$|R
50|$|As an {{alternative}} to regressions at {{both sides of the}} breakpoint (threshold), the method of <b>partial</b> <b>regression</b> can be used to find the longest possible horizontal stretch with insignificant regression coefficient, outside of which there is a definite slope with a significant regression coefficient. The alternative method can be used for segmented regressions of Type 3 and Type 4 when it is the intention to detect a tolerance level of the dependent variable for varying quantities of the independent, explanatory, variable (also called predictor).|$|E
50|$|The {{attached}} figure {{concerns the}} same data {{as shown in the}} blue graph in the infobox {{at the top of this}} page. Here, the wheat crop has a tolerance for soil salinity up to the level of EC=7.1 dS/m instead of 4.6 in the blue figure. However, the fit of the data beyond the threshold is not as well as in the blue figure that has been made using the principle of minimization of the sum of squares of devations of the observed values from the regression lines over the whole domain of explanatory variable X (i.e. maximization of the coefficient of determination), while the <b>partial</b> <b>regression</b> is designed only to find the point where the horizontal trend changes into a sloping trend.|$|E
5000|$|In statistics, {{canonical}} analysis (from κανων bar, measuring rod, ruler) {{belongs to the}} family of regression methods for data analysis. Regression analysis quantifies a relationship between a predictor variable and a criterion variable by the coefficient of correlation r, coefficient of determination r2, and the standard regression coefficient β. Multiple regression analysis expresses a relationship between a set of predictor variables and a single criterion variable by the multiple correlation R, multiple coefficient of determination R², and a set of standard <b>partial</b> <b>regression</b> weights β1, β2, etc. Canonical variate analysis captures a relationship between a set of predictor variables and a set of criterion variables by the canonical correlations ρ1, ρ2, ..., and by the sets of canonical weights C and D.|$|E
5000|$|Ridge {{regression}} or {{principal component}} <b>regression</b> or <b>partial</b> least squares <b>regression</b> can be used.|$|R
40|$|A {{data set}} {{comprising}} of the selectivity index of pentachlorophenol-imprinted polymers against 53 pentachlorophenol and related compounds {{was obtained from}} the excellent work of Baggiani et al. Molecular descriptors of the phenol compounds were calculated with E-DRAGON to obtain a total of 1, 666 descriptors spanning 20 categories of molecular properties. Multivariate analysis of the data set was performed using multiple linear <b>regression,</b> <b>partial</b> least squares <b>regression,</b> and principal component <b>regression.</b> <b>Partial</b> least squares <b>regression</b> was found to deliver an excellent predictive model and was chosen for further investigation. The descriptor dimension was reduced by the combined use of partial least squares and Unsupervised Forward Selection algorithm. The obtained Quantitative Structure-Property Relationship (QSPR) model based on the smaller subset of the molecular descriptors displayed substantial gain in predictive ability when compared to the model of Baggiani et al. Such QSPR model can help in the computational design of MIPs with predefined selectivity toward template molecules of interest...|$|R
40|$|This paper revisits {{a number}} of data-rich {{prediction}} methods that are widely used in macroeconomic forecasting, such as factor models and Bayesian shrinkage regression, and compares these methods with a lesser known alternative: <b>partial</b> least squares <b>regression.</b> In this method, linear, orthogonal combinations {{of a large number}} of predictor variables are constructed such that the linear combinations maximize the covariance between the target variable and each of the common components constructed from the predictor variables. We provide a theorem that shows that when the data comply with a factor structure, principal components and <b>partial</b> least squares <b>regressions</b> provide asymptotically similar results. We also argue that forecast combinations can be interpreted as a restricted form of <b>partial</b> least squares <b>regression.</b> Monte Carlo experiments confirm our theoretical results that <b>partial</b> least squares <b>regression</b> performs at least as well as principal components regression and rivals Bayesian regression when the data have a factor structure. These experiments also indicate that when there is no factor structure in the data, <b>partial</b> least square <b>regression</b> outperforms both principal components and Bayesian regressions. Finally, we apply partial least squares, principal components, and Bayesian regressions on a large panel of monthly U. S. macroeconomic and financial data to forecast CPI inflation, core CPI inflation, industrial production, unemployment, and the federal funds rate across different subperiods. The results indicate that <b>partial</b> least squares <b>regression</b> usually has the best out-of-sample performance when compared with the two other data-rich prediction methods. Time-series analysis; Economic forecasting; Business cycles; Econometric models...|$|R
5000|$|Segmented {{regression}} {{is often}} used to detect over which range an explanatory variable (X) has no effect on the dependent variable (Y), while beyond the reach there is a clear response, be it positive or negative.The reach of no effect may be found at the initial part of X domain or conversely at its last part. For the [...] "no effect" [...] analysis, application of the least squares method for the segmented regression analysis [...] {{may not be the most}} appropriate technique because the aim is rather to find the longest stretch over which the Y-X relation can be considered to possess zero slope while beyond the reach the slope is significantly different from zero but knowledge about the best value of this slope is not material. The method to find the no-effect range is progressive <b>partial</b> <b>regression</b> [...] over the range, extending the range with small steps until the regression coefficient gets significantly different from zero.|$|E
40|$|The <b>partial</b> <b>regression</b> {{coefficient}} is {{also called}} regression coefficient, regression weight, <b>partial</b> <b>regression</b> weight, slope coefficient or partial slope coefficient. It {{is used in}} the context of multiple linear regression (mlr) analysis and gives the amount by which the dependent variable (DV) increases when on...|$|E
30|$|Because of word limits, we only {{report the}} unstandardized {{coefficients}} of this {{multiple linear regression}} model. If there are no further explanations, then the unstandardized coefficients {{are consistent with the}} standard <b>partial</b> <b>regression</b> coefficient.|$|E
30|$|One {{particular}} type of categorical variable encompasses the Boolean variable, which only has values of zero or one. A block of Boolean variables replaces a categorical variable (e.g., a block of five Boolean variables represents the five types of a categorical variable). In other words, across all types of a categorical variables, {{only one of the}} respective Boolean block variables has the value one and all others zero. The Boolean variables become the indicators of a categorical construct (i.e., the Boolean block) and the categorical constructs can be included in the <b>partial</b> <b>regressions</b> of the PLS path model. When using categorical scales in PLS-SEM’s <b>partial</b> linear <b>regression</b> models, in keeping with Pearson’s tradition, the analyses follow the assumption of an underlying continuum (i.e., without distributional assumptions). 1 As a result, for the PLS path model, we obtain a super contingency table (Lohmöller 1989, Chapter 4). When considering, for example, a PLS path model that consists of two categorical constructs (i.e., two Boolean blocks), we obtain a super matrix, which includes the bivariate relative frequency (i.e., the contingency table or the super contingency table, as it can contain the pairwise contingencies of even more than two categorical variables).|$|R
40|$|Background: The {{objective}} {{of the present study}} was to test the ability of the <b>partial</b> least squares <b>regression</b> technique to impute genotypes from low density single nucleotide polymorphisms (SNP) panels i. e. 3 K or 7 K to a high density panel with 50 K SNP. No pedigree information was used. Methods: Data consisted of 2093 Holstein, 749 Brown Swiss and 479 Simmental bulls genotyped with the Illumina 50 K Beadchip. First, a single-breed approach was applied by using only data from Holstein animals. Then, to enlarge the training population, data from the three breeds were combined and a multi-breed analysis was performed. Accuracies of genotypes imputed using the <b>partial</b> least squares <b>regression</b> method were compared with those obtained by using the Beagle software. The impact of genotype imputation on breeding value prediction was evaluated for milk yield, fat content and protein content. Results: In the single-breed approach, the accuracy of imputation using <b>partial</b> least squares <b>regression</b> was around 90 and 94 % for the 3 K and 7 K platforms, respectively; corresponding accuracies obtained with Beagle were around 85 % and 90 %. Moreover, computing time required by the <b>partial</b> least squares <b>regression</b> method was on average around 10 times lower than computing time required by Beagle. Using the <b>partial</b> least squares <b>regression</b> method in the multi-breed resulted in lower imputation accuracies than using single-breed data. The impact of th...|$|R
40|$|In our work, we {{explored}} multicollinearity problem from a complex {{point of view}} - from diagnostic methods to the solving of the problems which are caused by the multicollinearity. We compared the Least Squares method with some alternative methods - Principal Component <b>Regression,</b> <b>Partial</b> Least Squares <b>Regression</b> and Ridge Regression on the theoretical basis. In the last section, we demonstrated all methods on practical example computed in the program R...|$|R
40|$|The <b>partial</b> <b>regression</b> {{coefficient}} is {{also called}} regression coefficient, regression weight, <b>partial</b> <b>regression</b> weight, slope coefficient or partial slope coefficient. It {{is used in}} the context of multiple linear regression (mlr) analysis and gives the amount by which the dependent variable (DV) increases when one independent variable (IV) is increased by one unit and all the other independent variables are held constant. This coefficient is called partial because its value depends, in general, upon the other independent variables. Specifically, the value of the partial coefficient for one independent variable will vary, in general, depending upon the other independent variables included in the regression equatio...|$|E
40|$|The {{purpose of}} the {{research}} {{was to examine the}} influence of financial leverage to the stock prices at manufacturing companies listed on the BEI. Research data using secondary data in the form of income statement and closing stock prices, while the sample is determined by non-probability purposive sampling. Analysis tool used is a simple regression, whereas when using the <b>partial</b> <b>regression</b> test (T-test). Where to financial leverage measured by the percentage change in EBIT percentage change against EPS, while using the stock price year end closing stock prices. The result of <b>partial</b> <b>regression</b> (T-test) analysis shows that significant financial leverage has no effect on stock prices...|$|E
40|$|Tests of {{significance}} of a single <b>partial</b> <b>regression</b> coefficient in a multiple regression model are often made in situations where the standard assumptions underlying the probability calculation (for example assumption of normally of random error term) do not hold. When the random error term fails to fulfill some of these assumptions, one need resort to some other nonparametric methods to carry out statistical inferences. Permutation methods are a branch of nonparametric methods. This study compared empirical type one error of different permutation strategies that proposed for testing nullity of a <b>partial</b> <b>regression</b> coefficient in a multiple regression model, using simulation and show that the type one error of Freedman and Lanes strategy is lower to than the other methods...|$|E
40|$|We {{propose a}} {{prediction}} procedure for the functional linear quantile regression model by using partial quantile covariance techniques {{and develop a}} simple <b>partial</b> quantile <b>regression</b> (SIMPQR) algorithm to efficiently extract <b>partial</b> quantile <b>regression</b> (PQR) basis for estimating functional coefficients. We further extend our partial quantile covariance techniques to functional composite quantile <b>regression</b> (CQR) defining <b>partial</b> composite quantile covariance. There are three major contributions. (1) We define partial quantile covariance between two scalar variables through linear quantile regression. We compute PQR basis by sequentially maximizing the partial quantile covariance between the response and projections of functional covariates. (2) In order to efficiently extract PQR basis, we develop a SIMPQR algorithm analogous to simple partial least squares (SIMPLS). (3) Under the homoscedasticity assumption, we extend our techniques to partial composite quantile covariance {{and use it to}} find the <b>partial</b> composite quantile <b>regression</b> (PCQR) basis. The SIMPQR algorithm is then modified to obtain the SIMPCQR algorithm. Two simulation studies show the superiority of our proposed methods. Two real data from ADHD- 200 sample and ADNI are analyzed using our proposed methods...|$|R
5000|$|Covering {{methods such as}} {{multiple}} linear <b>regression,</b> <b>partial</b> least squares, recursive partitioning, Genetic Function approximation and 3D field-based QSAR ...|$|R
40|$|The {{flexibility}} coefficient is popularly used {{to implement the}} macroevaluation of shape, safety, and economy for arch dam. However, the description of {{flexibility coefficient}} has not drawn a widely consensus all the time. Based on {{a large number of}} relative instance data, the relationship between influencing factor and flexibility coefficient is analyzed by means of <b>partial</b> least-squares <b>regression.</b> The <b>partial</b> least-squares <b>regression</b> equation of flexibility coefficient in certain height range between 30 [*]m and 70 [*]m is established. Regressive precision and equation stability are further investigated. The analytical model of statistical flexibility coefficient is provided. The flexibility coefficient criterion is determined preliminarily to evaluate the shape of low- and medium-sized arch dam. A case study is finally presented to illustrate the potential engineering application. According to the analysis result of <b>partial</b> least-squares <b>regression,</b> it is shown that there is strong relationship between flexibility coefficient and average thickness of dam, thickness-height ratio of crown cantilever, arc height ratio, and dam height, but the effect of rise-span ratio is little relatively. The considered factors in the proposed model are more comprehensive, and the applied scope is clearer than that of the traditional calculation methods. It is more suitable for the analogy analysis in engineering design and the safety evaluation for arch dam...|$|R
