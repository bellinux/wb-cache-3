0|208|Public
30|$|Maximum {{number of}} file <b>metadata</b> <b>operations</b> per second (includes file creation, {{deletion}} or gathering of file information).|$|R
50|$|<b>Metadata</b> <b>operations</b> such as {{permitting}} or restricting {{access the}} a directory by various users {{or groups of}} users are usually included.|$|R
50|$|GeoCat Bridge allows to edit, {{validate}} {{and directly}} <b>publish</b> <b>metadata</b> from ArcGIS Desktop to GeoNetwork (and generic CSW catalogs) and publishes data as map services on GeoServer. Several metadata profiles are supported.|$|R
50|$|Bonnie++ {{allows you}} to {{benchmark}} how your file systems perform with respect to data read and write speed, the number of seeks that can be performed per second, {{and the number of}} file <b>metadata</b> <b>operations</b> that can be performed per second.|$|R
5000|$|The final CSV output {{includes}} the {{information related to}} data read and write speed, number of seeks that can be performed per second, and number of file <b>metadata</b> <b>operations</b> that can be performed per second and the CPU usage statistics for the below given tests: ...|$|R
50|$|Open {{data format}} files have their {{internal}} structures available to {{users of the}} file {{through a process of}} <b>metadata</b> <b>publishing.</b> <b>Metadata</b> <b>publishing</b> implies that the structure and semantics of all the possible data elements within a file are available to users.|$|R
40|$|Abstract—The {{demand for}} {{scalable}} I/O {{continues to grow}} rapidly as computer clusters keep growing. Much of the research in storage systems {{has been focused on}} improving the scale and performance of I/O throughput. Scalable file systems {{do a good job of}} scaling large file access bandwidth by striping or sharing I/O resources across many servers or disks. However, the same cannot be said about scaling file <b>metadata</b> <b>operation</b> rates. Most existing parallel filesystems choose to concentrate all the metadata processing load on a single server. This centralized processing can guarantee the correctness, but it severely hampers scalability. This downside is becoming more and more unacceptable as metadata throughput is critical for large scale applications. Distributing metadata processing load is critical to improve metadata scalability when handling huge number of client nodes. However, a solution to speed up <b>metadata</b> <b>operations</b> has to address two challenges simultaneously, namely the scalability and reliability. In this paper, we have designed a decentralized metadata service layer and evaluated its benefits and shortcomings that concern parallel filesystems. The main aim of this service layer is to maintain reliability and consistency in a distributed metadata environment. At the same time we also focus on improving the scalability of the <b>metadata</b> <b>operations,</b> and in turn, the scalability of the underlying parallel filesystem. As demonstrated by experiments, the approach presented in this paper achieves significant improvements over native parallel filesystems by large margin for all the major <b>metadata</b> <b>operations.</b> With 256 client processes, our decentralized metadata service outperforms Lustre and PVFS 2 by a factor of 1. 9 and 23, respectively, to create directories. With respect to stat() operation on files, our approach is 1. 3 and 3. 0 times faster than Lustre and PVFS. I...|$|R
5000|$|... rNews is a {{standard}} for using semantic markup to annotate online news. It defines a data model for embedding machine-readable <b>publishing</b> <b>metadata</b> in web documents {{and a set of}} suggested implementations. Some properties of this standard have been adopted by Schema.org for its metadata schema.|$|R
40|$|Large HPC {{installations}} typically {{make use}} of parallel file systems that adhere to POSIX I/O conventions, and that implement a separation of data and metadata {{in order to maintain}} high performance. File systems such as GPFS and Lustre have evolved to enable an increase in data bandwidth that is primarily achieved by adding more disk drives behind an increasing number of disk controllers. Improvements in metadata performance cannot be achieved by just deploying a large volume of hardware, as the defining characteristics are the number of simultaneous operations that can be carried out and the latency of those operations. For highly scalable applications using parallel I/O libraries, the speed of <b>metadata</b> <b>operations,</b> such as opening a file on thousands of processes, has the potential to become the major bottleneck to improved I/O performance. This Metadata Wall has the ability to grow such that <b>metadata</b> <b>operations</b> can take much longer than the subsequent data operations, even on systems with very large amounts of I/O data bandwidth. We present results showing the performance of <b>metadata</b> <b>operations</b> with standard disk equipment and with solid state storage hardware, and extrapolate whether we expect the evolution in hardware alone will be sufficient to limit the effects of this I/O Metadata Wall. We also report challenges in making the metadata I/O measurements and subsequent analysis for parallel file systems...|$|R
40|$|This {{presentation}} {{will provide}} {{a review of the}} work on the two RDF vocabularies for DDI: (1) the DDI-RDF Discovery vocabulary for <b>publishing</b> <b>metadata</b> about datasets into the Web of Linked Data, and (2) XKOS, an RDF vocabulary for describing statistical classifications, which is an extension of the popular SKOS vocabular...|$|R
40|$|We {{present the}} first {{systematic}} analysis of read, write, and space amplification in Linux file systems. While many researchers are tackling write amplification in key-value stores, IO amplification in file systems {{has been largely}} unexplored. We analyze data and <b>metadata</b> <b>operations</b> on five widely-used Linux file systems: ext 2, ext 4, XFS, btrfs, and F 2 FS. We find that data operations result in significant write amplification (2 - 32 X) and that <b>metadata</b> <b>operations</b> have a large IO cost. For example, a single rename requires 648 KB write IO in btrfs. We also find that small random reads result in read amplification of 2 - 13 X. Based on these observations, we present the CReWS conjecture {{about the relationship between}} IO amplification, consistency, and storage space utilization. We hope this paper spurs people to design future file systems with less IO amplification, especially for non-volatile memory technologies...|$|R
40|$|Abstract—The {{ever-increasing}} {{scale of}} modern highperformance computing (HPC) systems presents {{a variety of}} challenges to the parallel file system (PFS) based storage in these systems. The scalability of application checkpointing is a particularly important challenge because {{it is critical to}} the reliability of computing and it often dominates the I/Os in a HPC system. When a large number of parallel processes simultaneously perform checkpointing, the PFS metadata servers can become a serious bottleneck due to the large volume of concurrent <b>metadata</b> <b>operations.</b> This paper specifically addresses this PFS metadata management issue in order to support scalable application checkpointing in large HPC systems. It proposes a new technique named PFSdelegation which delegates the management of the PFS storage space used for checkpointing to applications, thereby relieving the load of <b>metadata</b> <b>operations</b> on the PFS during their checkpointing. This proposed technique is prototyped on PVFS 2, a widely used PFS implementation, and evaluated on a HPC cluster using a representative parallel I/O benchmark, IOR. Experiments with up to 128 parallel processes show that the PFS-delegation based checkpointing is significantly faster than the traditional shared-file and file-per-process based checkpointing methods (7 % and 10 % speedup when the underlying PVFS 2 uses a centralized metadata server; 22 % and 31 % speedup when using distributed metadata servers). The results also demonstrate that the PFS-delegation based checkpointing substantially reduces the total number of <b>metadata</b> <b>operations</b> handled by the metadata servers during the checkpointing...|$|R
40|$|Nowadays, Linux file {{systems have}} to manage {{millions}} of tiny files for different applications, and face with higher <b>metadata</b> <b>operations.</b> So {{how to provide}} such high metadata performance with such enormous number of files and large scale directories is a big challenge for Linux file system. We viewed that <b>metadata</b> lookup <b>operations</b> dominate <b>metadata</b> workload and incur low metadata performance. In this paper, we present a metadata cache to accelerate metadata access for Linux file system. Through this optimization, the Linux file system (such as EXT 2, EXT 4, BTRFS, etc.) can gain improvement in read rates as well as write rates. Comment: 4 pages, 8 figure...|$|R
5000|$|<b>Publishing</b> {{approved}} <b>metadata</b> {{elements in}} a variety of output formats (see below) ...|$|R
40|$|Joined the Saudi Aramco in mid 2001 as GIS System Analyst with eMap with {{following}} tasks ► Handling GIS Conversion Projects ► Creating {{and managing}} GIS datasets ► Development of database driven GIS solutions ► Development of programs to incrementally update different GIS datasets ► Maintaining and <b>publishing</b> <b>metadata</b> documents ► Web development for internet mapping solutions Educational Informatio...|$|R
50|$|Organizations {{that create}} {{applications}} that store data in file systems can also <b>publish</b> <b>metadata</b> definitions. One common way to perform {{this is to}} store application data in a compressed XML file format. The XML files can be uncompressed and validated against an external XML Schema. An {{example of this is}} done by the Open Source FreeMind tool.|$|R
40|$|In petabyte-scale {{distributed}} file {{systems that}} decouple {{read and write}} from <b>metadata</b> <b>operations,</b> behavior of the metadata server cluster will be critical to overall system performance. We examine aspects of the workload that {{make it difficult to}} distribute effectively, and present a few potential strategies to demonstrate the issues involved. Finally, we describe the advantages of intelligent metadata management and a simulation environment we have developed to validate design possibilities. ...|$|R
50|$|The Protocol for Web Description Resources (POWDER) is the W3C {{recommended}} {{method for}} describing Web resources.It specifies a protocol for <b>publishing</b> <b>metadata</b> about Web resources using RDF, OWL, and HTTP. The initial working party {{was formed in}} February 2007 with the W3C Content Label Incubator Group's 2006 work as an input. On 1 September 2009 POWDER became a W3C recommendation and the Working Group is now closed.|$|R
40|$|In petabyte-scale {{distributed}} file {{systems that}} decouple {{read and write}} from <b>metadata</b> <b>operations,</b> behavior of the metadata server cluster will be critical to overall system performance and scalability. We present a dynamic subtree partitioning and adaptive metadata management system designed to efficiently manage hierarchical metadata workloads that evolve over time. We examine {{the relative merits of}} our approach in the context of traditional workload partitioning strategies, and demonstrate the performance, scalability and adaptability advantages in a simulation environment. ...|$|R
40|$|University of Minnesota Ph. D. dissertation. February 2016. Major: Health Informatics. Advisors: Laël Gatewood, Rui Zhang. 1 {{computer}} file (PDF); viii, 132 pages. The Open Government Initiative began {{an era of}} information sharing by publishing data that is accessible to the public. HealthData. gov is a data portal that {{was developed by the}} U. S. Federal Government to <b>publish</b> <b>metadata</b> to disseminate information about healthcare datasets to the American people. Despite the {{growth in the number of}} datasets published, there has been limited public participation in the use of the data, which has been attributed to the currently implemented methods for data storage and retrieval. An automated assessment of the HealthData. gov metadata was conducted to assess completeness, accuracy, and consistency of <b>metadata</b> <b>published</b> from 2012 to 2014. Also, a method for indexing the datasets using Medical Subject Headings (MeSH) was evaluated using a term coverage study. The results of these studies demonstrated that <b>metadata</b> <b>published</b> in earlier years were less complete, lower quality, and less consistent. Also, metadata that underwent modifications following their original creation were of higher quality. MeSH offered adequate coverage of the metadata concepts, thereby lending support for the adoption of the terminology for indexing purposes. The results suggested that greater standardization is needed when <b>publishing</b> <b>metadata.</b> This research contributed to the development of automated metrics for assessing metadata quality, design recommendations for a framework to supports high quality metadata, and recommendations for expanding MeSH to offer greater coverage of concepts from HealthData. gov...|$|R
50|$|SPDX {{attempts}} to standardize {{the way in}} which organizations <b>publish</b> their <b>metadata</b> on software licenses and components in bills of material.|$|R
50|$|<b>Metadata</b> <b>publishing</b> is the {{foundation}} upon which advanced distributed computing functions are being built. But like building foundations, {{care must be taken}} in <b>metadata</b> <b>publishing</b> systems to ensure the structural integrity of the systems built on top of them.|$|R
5000|$|<b>Metadata</b> <b>operations</b> in XFS have {{historically}} been slower than with other file systems, resulting in, for example, poor performance with operations such as deletions {{of large numbers of}} files. However, a new XFS feature implemented by Dave Chinner and called delayed logging, available since version 2.6.39 of the Linux kernel mainline, is claimed to resolve this; performance benchmarks done by the developer in 2010 revealed performance levels to be similar to ext4 at low thread counts, and superior at high thread counts.|$|R
5000|$|Organizations that <b>publish</b> their <b>metadata</b> {{could make}} it easier for {{unauthorized}} people to find sensitive data if they breach an organization's firewall ...|$|R
30|$|Although {{most of the}} {{improvements}} resulted in adding features to GeoNetwork, in some cases we removed elements which are unnecessary in this context, {{in order to simplify}} the portal. This was the case of the metadata editor, which can be adjusted through the configuration options. The final result was a simpler editor, with new templates to support COBWEB types of metadata (e.g.: survey, field session, map) and a convenience button to <b>publish</b> <b>metadata</b> directly from the editor form.|$|R
40|$|Abstract. Metadata Registries (MDR) are {{information}} systems defined {{to manage and}} <b>publish</b> <b>metadata</b> related to information and data models. An inherent part of the data models registered within an MDR is the vocabularies used to assign values to data elements. These vocabularies relate to concepts that can be managed by Knowledge Organization Systems (KOS). This paper discusses how an MDR can import {{and take advantage of}} the information managed within a KOS. Keywords: MDR, ISO 11179, KOS. ...|$|R
40|$|International audienceThe Hadoop Distributed File System (HDFS) scales {{to store}} tens of {{petabytes}} of data {{despite the fact}} that the entire file system’s metadata must fit on the heap of a single Java virtual machine. The size of HDFS’ metadata is limited to under 100 GB in production, as garbage collection events in bigger clusters result in heartbeats timing out to the metadata server (NameNode). In this paper, we address the problem of how to migrate the HDFS’ metadata to a relational model, so that we can support larger amounts of storage on a shared-nothing, in-memory, distributed database. Our main contribution is that we show how to provide at least as strong consistency semantics as HDFS while adding support for a multiple-writer, multiple-reader concurrency model. We guarantee freedom from deadlocks by logically organizing inodes (and their constituent blocks and replicas) into a hierarchy and having all <b>metadata</b> <b>operations</b> agree on a global order for acquiring both explicit locks and implicit locks on subtrees in the hierarchy. We use transactions with pessimistic concurrency control to ensure the safety and progress of <b>metadata</b> <b>operations.</b> Finally, we show how to improve performance of our solution by introducing a snapshotting mechanism at NameNodes that minimizes the number of roundtrips to the database...|$|R
40|$|This poster investigates {{problems}} and solutions for searching the Web {{for geospatial data}} produced by various levels of government. Currently, geospatial data to meet particular needs are difficult to find. We suggest using Internet DBMS and Semantic Web technology to improve search techniques over metadata by using context-oriented search phrases, full query expressions, and semantic mappings. Furthermore, automatic searching for data is possible using an Internet DBMS that ranges over Web <b>published</b> <b>metadata</b> and consults semantic mapping expressions of models and terms...|$|R
40|$|Abstract Scalability of {{the data}} access {{architecture}} in the Semantic Web {{is dependent on the}} establishment of caching mechanisms to take the load off of servers. Unfortunately, there is a chicken and egg problem here: Research, implementation, and evaluation of caching infrastructure is uninteresting as long as data providers do not <b>publish</b> relevant <b>metadata.</b> And <b>publishing</b> <b>metadata</b> is useless {{as long as there is}} no infrastructure that uses it. We show by means of a survey of live RDF data sources that caching metadata is prevalent enough already to be used in some cases. On the other hand, they are not commonly used even on relatively static data, and when they are given, they are very conservatively set. We point out future directions and give recommendations for the enhanced use of caching in the Semantic Web. ...|$|R
40|$|The {{currently}} established formats for how a Web site can <b>publish</b> <b>metadata</b> about a site’s pages, the robots. txt {{file and}} sitemaps, {{focus on how}} to provide information to crawlers about where to not go and where {{to go on a}} site. This is sufficient as input for crawlers, but does not allow Web sites to <b>publish</b> richer <b>metadata</b> about their site’s structure, such as the navigational structure. This paper looks at the availability of Web site metadata on today’s Web in terms of available information resources and quantitative aspects of their contents. Such an analysis of the available Web site metadata not only makes it easier to understand what data is available today; it also serves as the foundation for investigating what kind of information retrieval processes could be driven by that data, and what additional dat...|$|R
50|$|For {{software}} {{agents to}} work together efficiently they must share semantics of their data elements. This {{can be done by}} having computer systems <b>publish</b> their <b>metadata.</b>|$|R
40|$|This paper {{illustrates}} {{some issues}} and use cases identified during {{the design and}} implementation of GeoDCAT-AP, a metadata profile aiming to provide a representation of geospatial metadata compliant with the DCAT application profile for European data portals (DCAT-AP). In particular, the paper focuses on those issues that may have a possible relevance also outside the geospatial domain, covering topics concerning <b>metadata</b> profile-based negotiation, <b>publishing</b> <b>metadata</b> on the Web, representing API-based data access in metadata, and approaches to modelling data quality. JRC. B. 6 -Digital Econom...|$|R
40|$|Abstract. To {{facilitate}} data {{search and}} retrieval, a geographic data repository <b>publishes</b> <b>metadata</b> about the data resources it houses. Metadata generation is, however, a time consuming and error prone activity. Exploring unique characteristics of geographic objects, we first discuss {{a strategy that}} combines federations of gazetteers, thesauri and catalogs to harvest the information required for metadata annotations. Then, we propose a software architecture, ISO 19115 : 2003 compliant, for automated geographic metadata annotation generation. Finally, we describe the GeoCatalog tool, an implementation of the proposed architecture that demonstrates the viability of our approach. ...|$|R
40|$|The {{metadata}} {{service of the}} Ursa Minor distributed storage system scales metadata throughput as metadata servers are added. While doing so, it correctly handles <b>metadata</b> <b>operations</b> that involve items served by different metadata servers, consistently and atomically updating the items. Unlike previous systems, it does so by reusing existing metadata migration functionality to avoid complex distributed transaction protocols. It also assigns item IDs to minimize the occurrence of multiserver operations. Ursa Minor’s approach allows one to implement a desired feature with less complexity than alternative methods and with minimal performance penalty (under 1 % in non-pathological cases). ...|$|R
40|$|Striping is a {{technique}} that distributes file content over multiple storage servers and thereby enables parallel ac-cess. In {{order to be able}} to provide a consistent view across file data and <b>metadata</b> <b>operations,</b> the file system has to track the layout of the file and know where the file ends and where it contains gaps. In this paper, we present a light-weight protocol for maintaining a consis-tent notion of a file’s layout that provides POSIX seman-tics without restricting concurrent access to the file. In an evaluation, we show that the protocol scales and elicit its corner cases. ...|$|R
5000|$|Vendors that <b>publish</b> their <b>metadata</b> risk {{customers}} creating {{tools that}} could allow their customers to export their data from computer systems, therefore {{making it easier}} to migrate off of a vendor's system ...|$|R
