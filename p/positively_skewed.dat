631|29|Public
2500|$|For a large {{class of}} {{unimodal}} distributions that are <b>positively</b> <b>skewed</b> the mode, median and mean fall in that order. Conversely {{for a large}} class of unimodal distributions that are negatively skewed the mean {{is less than the}} median which in turn is less than the mode. In symbols for these <b>positively</b> <b>skewed</b> unimodal distributions ...|$|E
2500|$|... (α = 2, β > 2) The {{distribution}} is unimodal, <b>positively</b> <b>skewed,</b> right-tailed, with one inflection point, located {{to the right}} of the mode: ...|$|E
2500|$|... (0 < α < 1, 1 < β < 2) The {{distribution}} has a mode at {{the left}} end x = 0 and it is <b>positively</b> <b>skewed,</b> right-tailed. There is one inflection point, located {{to the right of}} the mode: ...|$|E
40|$|A novel {{method to}} measure bidders’ costs (valuations) in {{descending}} (ascending) auctions is introduced. Based on two bounded rationality constraints bidders’ costs (valuations) are given an imperfect measurements interpretation that is robust to behavioral {{deviations from the}} traditional rationality assumptions. Theory provides no guidance as {{to the shape of}} the cost (valuation) distributions while empirical evidence suggests them to be <b>positively</b> <b>skew.</b> Consequently, a flexible distribution is employed in an imperfect measurements framework. An illustration of the proposed method on Swedish public procurement data is provided along with a comparison to a traditional Bayesian Nash Equilibrium approach. log-generalized gamma distribution; latent variable; maximum likelihood; prediction; public procuremen...|$|R
5000|$|Several challenges, however, {{exist in}} the {{monitoring}} of international labour standards. The ILO and other international organisations generally rely on self-reporting data from countries. Some analysts have questioned the quality and neutrality of these sources of data. For example, definitions of what constitutes unemployment vary {{from country to country}} making it difficult to compare data and to judge data quality. [...] Additionally, despite the large amount of data, there are still gaps in their coverage. Coverage is greatest in the urban environments and the formal sector. Conversely, gaps {{exist in the}} coverage of rural environments and informal sectors which can <b>positively</b> <b>skew</b> the labour statistics that countries report.|$|R
40|$|Maximum {{likelihood}} estimation in {{confirmatory factor}} analysis requires large sample sizes, normally distributed item responses, and reliable indicators of each latent construct, but these ideals are rarely met. We examine alternative strategies for dealing with non-normal data, particularly when the sample size is small. In two simulation studies, we systematically varied: the degree of non-normality; the sample size from 50 to 1000; the way of indicator formation, comparing items versus parcels; the parcelling strategy, evaluating uniformly <b>positively</b> <b>skews</b> and kurtosis parcels versus those with counterbalancing skews and kurtosis; and the estimation procedure, contrasting maximum likelihood and asymptotically distribution-free methods. We evaluated the convergence behaviour of solutions, {{as well as the}} systematic bias and variability of parameter estimates, and goodness of fit...|$|R
5000|$|For a large {{class of}} {{unimodal}} distributions that are <b>positively</b> <b>skewed</b> the mode, median and mean fall in that order. Conversely {{for a large}} class of unimodal distributions that are negatively skewed the mean {{is less than the}} median which in turn is less than the mode. In symbols for these <b>positively</b> <b>skewed</b> unimodal distributions ...|$|E
5000|$|... (α = 2, β > 2) The {{distribution}} is unimodal, <b>positively</b> <b>skewed,</b> right-tailed, with one inflection point, located {{to the right}} of the mode: ...|$|E
5000|$|... {{and there}} is at least one strict inequality. Equivalently, [...] dominates [...] in the third order if and only if [...] for all nondecreasing, concave utility {{functions}} [...] that are <b>positively</b> <b>skewed</b> (that is, have a positive third derivative throughout).|$|E
40|$|International audienceWe have {{analysed}} {{relative humidity}} statistics from measurements in cirrus clouds taken unintentionally during the Measurement of OZone by Airbus In-service airCraft project (MOZAIC). The {{shapes of the}} in-cloud humidity distributions change from nearly symmetric in relatively warm cirrus (warmer than - 40 °) to considerably <b>positively</b> <b>skew</b> (i. e. towards high humidities) in colder clouds. These results are in agreement to findings obtained recently from the INterhemispheric differences in Cirrus properties from Anthropogenic emissions (INCA) campaign (Ovarlez et al., 2002). We interprete the temperature dependence of {{the shapes of the}} humidity distributions as an effect of the length of time a cirrus cloud needs from formation to a mature equilibrium stage, where the humidity is close to saturation. The duration of this transitional period increases with decreasing temperature. Hence cold cirrus clouds are more often met in the transitional stage than warm clouds...|$|R
30|$|The {{retrospective}} {{nature of}} this study and the relatively small cohort coupled with an unequal proportion of examinations performed in stage IV disease are potential limitations as they may <b>positively</b> <b>skew</b> the results {{and contribute to the}} high estimation of the clinical impact of FDG PET-CT. Reporting of the FDG PET-CT examination with a lack of blinding to the result of the prior CECT was likely to influence the positive clinical impact of FDG PET-CT, but this methodology is reflective of real life, day-to-day practice. In addition, {{it could be argued that}} the median delay between CECT and subsequent FDG PET-CT examination of 32  days may have been sufficient time for new sites of disease to develop in the intervening period and become apparent on FDG PET-CT and thereby falsely result in a major clinical impact by detection of additional sites of disease. However, sub-group analysis showed similar figures for change in management with differing time intervals between FDG PET-CT and CECT examinations.|$|R
40|$|This paper revisits {{the impact}} of regret aversion on the {{behavior}} of the competitive firm under price uncertainty. We show that the firm optimally produces more (less) when regret aversion prevails if the random output price is <b>positively</b> (negatively) <b>skewed.</b> In this case, high (low) output prices {{are much more likely to}} be seen than low (high) output prices. To avoid regret, the firm is induced to raise (lower) its output optimal level. The skewness of the price distribution as such plays a pivotal role in determining how regret aversion affects the firm's production decision...|$|R
5000|$|... (0 < α < 1, 1 < β < 2) The {{distribution}} has a mode at {{the left}} end x = 0 and it is <b>positively</b> <b>skewed,</b> right-tailed. There is one inflection point, located {{to the right of}} the mode: ...|$|E
5000|$|The CAI is {{commonly}} used by paleontologists due to its ease of measurement and the abundance of Conodonta throughout marine carbonates of the Paleozoic. However, the organism disappears from the fossil record after the Triassic period, so the CAI is not available to analyze rocks younger than [...] Additionally, the index can be <b>positively</b> <b>skewed</b> in regions of hydrothermal alteration.|$|E
5000|$|Since , the {{probability}} {{left of the}} mode, and therefore right of the mode as well, can equal any value in (0,1) depending {{on the value of}} [...] Thus the skewed generalized t distribution can be highly skewed as well as symmetric. If , then the distribution is negatively skewed. If , then the distribution is <b>positively</b> <b>skewed.</b> If , then the distribution is symmetric.|$|E
50|$|GAMLSS is {{especially}} suited for modelling a leptokurtic or platykurtic and/or <b>positively</b> or negatively <b>skewed</b> response variable. For count type response variable data {{it deals with}} over-dispersion by using proper over-dispersed discrete distributions. Heterogeneity also is dealt with by modelling the scale or shape parameters using explanatory variables. There are several packages written in R related to GAMLSS models.|$|R
40|$|Decisions that {{consumers}} make often rest on evaluations of attributes, {{such as how}} large, expensive, good, or fattening an option seems. Extant {{research has demonstrated that}} these evaluations in turn depend upon the recently experienced distribution of attribute values (e. g., <b>positively</b> or negatively <b>skewed).</b> In many situations decisions rely on recalling the attribute values of each option, a process that has been neglected in much of the previous literature. In two experiments, participants learned attribute information for labeled stimuli presented within either a <b>positively</b> or negatively <b>skewed</b> distribution and then they recalled values from labels after approximately one minute. The results demonstrated effects that are inconsistent with predictions of the category-adjustment model (Duffy, Huttenlocher, Hedges & Crawford, 2010) that recalled values would shift toward the mean of the distribution of values presented. Instead, results were consistent with predictions of the comparison-induced distortion model (Choplin & Hummel, 2002) that remembered values would depend on the density of stimuli within the attribute range. Reasons for these results, alternative models, and implications for decision-making are discussed. % changed with to withi...|$|R
40|$|Abstract — Sediment {{samples from}} a {{sedimentary}} sequence {{on the bank}} of River Ero in Geregu are evaluated to decipher the temporal variability in textures of unit layers FL_G (lowermost) to FL_A (uppermost). Sand predominates in the sediment samples and varies between 0. 3 and 68. 57 %; particles with silt-size fraction vary between 3. 01 and 26. 31 %. Gravel size fraction is low and occurs only in sediment samples of Units E and B, and varies between 0. 2 and 4. 44 %. The studied sediment sequence is dominated by sand units that are very fine-grained (e. g. FL_G, FL_F, FL_D and FL_C) to fine texture (FL_E and FL_A); the only exception is Unit FL_B that is medium grain sand. This deduction corroborates the computed mean size which varies between 1. 309 Ф to 3. 293 Ф. Computed standard deviation varies between 0. 622 Ф and 2. 274 Ф which illustrates that the sediments are very poorly sorted to moderately well-sorted sand. The skewness of the sediment samples ranges between- 0. 235 to 0. 278, and this implies that the sedimentary units are negatively <b>skewed</b> to <b>positively</b> <b>skew.</b> Computed kurtosis varies between 0. 856 and 1. 84, and based on the kurtosis values, distribution of particle size of the sedimentary units is classed as leptokurtic (e. g. FL_G and FL_D), very leptokurtic (e. g. FL_C), mesokurtic (e. g. FL_F and FL_A) and platykurtic (e. g. FL_E and FL_B). Consequently, the studied sediments are inferred to have deposited in moderate low energy environment. Bivariant plots of skewness versus standard deviation (or sorting) and mean size versus standard deviation shows that the sediments are dominantly river sands. However, the linear discriminate functions show an overlap of shallow marine and fluvial conditions of deposition for the sequence of sediments. The energy condition of the transporting medium and that of the depositional environment is inferred to be the dominant contro...|$|R
50|$|It is {{customary}} {{to transform}} data logarithmically to fit symmetrical distributions (like the normal and logistic) to data obeying a distribution that is <b>positively</b> <b>skewed</b> (i.e. skew to the right, with mean > mode, {{and with a}} right hand tail that is longer than the left hand tail), see lognormal distribution and the loglogistic distribution. A similar effect {{can be achieved by}} taking the square root of the data.|$|E
50|$|Skewness in a data series may {{sometimes}} be observed not only graphically but by simple {{inspection of the}} values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of 50. We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, e.g. (40, 49, 50, 51). Similarly, {{we can make the}} sequence <b>positively</b> <b>skewed</b> by adding a value far above the mean, e.g. (49, 50, 51, 60).|$|E
5000|$|... {{demonstrated}} that without imposing any structure on the underlying forcing process, the model-free CBOE volatility index (VIX) does not measure market expectation of volatility {{but that of}} a linear moment-combination. Particularly, VIX undervalues (overvalues) volatility when market return {{is expected to be}} negatively (<b>positively)</b> <b>skewed.</b> Alternatively, they develop a model-free generalized volatility index (GVIX). With no diffusion assumption, GVIX is formulated directly from the definition of log-return variance, and VIX is a special case of the GVIX. Empirically, VIX generally understates the true volatility, and the estimation errors considerably enlarge during volatile markets. In addition, the spread between GVIX and VIX (GV-Spread) follows a mean-reverting process.|$|E
50|$|A guiding {{principle}} of GAMLSS {{is how to}} learn from data generated in many fields. In particular, the GAMLSS statistical framework enables flexible regression and smoothing models to be fitted to the data. The GAMLSS model assumes that the distribution of response variable has any parametric distribution which might be heavy or light-tailed, and <b>positively</b> or negatively <b>skewed.</b> In addition, all {{the parameters of the}} distribution (e.g., mean), scale (e.g., variance) and shape (skewness and kurtosis) can be modelled as linear, nonlinear or smooth functions of explanatory variables.|$|R
40|$|Tournament {{incentives}} schemes {{have been}} criticized for inducing excessive risktaking among financial market participants. In this paper we investigate how relative performance-based incentive schemes and status concerns for higher rank influence portfolio choice in laboratory experiments. We find that both underperformers and over-performers adapt their portfolios to their current relative performance, preferring either <b>positively</b> or negatively <b>skewed</b> assets, respectively. Most importantly, these results hold both when relative performance is instrumental for higher payoffs in a tournament and when it is only intrinsically motivating and not payout-relevant. We find no effects when no relative performance information is given...|$|R
40|$|A 30 -yr {{climatology}} of the snow-to-liquid-equivalent ratio (SLR) {{using the}} National Weather Service (NWS) Cooperative Summary of the Day (COOP) data is presented. Descriptive statistics are presented for 96 NWS county warning areas (CWAs), {{along with a}} discussion of selected histograms of interest. The results of the climatology indicate that a mean SLR value of 13 appears more appropriate for much of the country rather than the often-assumed value of 10, although considerable spatial variation in the mean exists. The distribution for the entire dataset exhibits positive skewness. Histograms for individual CWAs are both <b>positively</b> and negatively <b>skewed,</b> depending upon the variability of the in-cloud, subcloud, and ground conditions. 1...|$|R
50|$|Indoor {{water use}} {{includes}} water flows through {{fixtures and appliances}} inside the house. The average daily indoor water use per household (averaging 2.65 people in the North American sample) ranged from zero to 644 gphd (gallons per household per day) and averaged 138 gphd, with standard deviation of about 80 gphd) or 521 liters per day and standard deviation of 300 liters. The equivalent average use per person is 52.1 gpcd (gallons per capita per day) or 197 liters per capita per day. Because the distribution of indoor use in the sample of homes is <b>positively</b> <b>skewed,</b> a more appropriate {{measure of central tendency}} is the median, which is about 125 gphd (or 472 lphd). Toilet flushing is the largest indoor use of water, followed by flows through kitchen and bathroom faucets, showers, clothes washers, leaks, bathtubs, other/miscellaneous uses, and dishwashers. Since the late 1990s, total indoor use has decreased by 22 percent, primarily due to the improved water efficiency of clothes washers and toilets.|$|E
5000|$|A related {{class of}} ratios is [...] "income share" [...] - what {{percentage}} of national income a subpopulation accounts for. Taking the ratio of income share to subpopulation size corresponds to a ratio of mean subpopulation income relative to mean income. Because income distribution is generally <b>positively</b> <b>skewed,</b> mean is higher than median, so ratios to mean are lower than ratios to median. This is particularly used to measure that fraction of income accruing to top earners - top 10%, 1%, [...]1%, [...]01% (1 in 10, in 100, in 1,000, in 10,000), and also [...] "top 100" [...] earners or the like; in the US top 400 earners is [...]0002% of earners (2 in 1,000,0000) - to study concentration of income - wealth condensation, or rather income condensation. For example, in the chart at right, US income share of top earners was approximately constant from the mid-1950s to the mid-1980s, then increased from the mid-1980s through 2000s; this increased inequality {{was reflected in the}} Gini coefficient.|$|E
5000|$|Words were {{separated}} into groupings {{based on how}} many language families appeared to be cognate for the word. Among the 188 words, cognate groups ranged from 1 (no cognates) to 7 (all languages cognate) {{with a mean of}} 2.3 ± 1.1. The distribution of cognate class size was <b>positively</b> <b>skewed</b> − many more small groups than large ones − as predicted by their hypothesis of variant decay rates. [...] Words were then grouped by their generalized worldwide frequency of use, part of speech, and previously estimated rate of replacement. Cognate class size was positively correlated with estimated replacement rate (r=0.43, P<0.001). Generalized frequency combined with part of speech was also a strong predictor of class size (r=0.48, P<0.001). Pagel et al. conclude [...] "This result suggests that, consistent with their short estimated half-lives, infrequently used words typically do not exist long enough to be deeply ancestral, but that above the threshold frequency words gain greater stability, which then translates into larger cognate class sizes." ...|$|E
40|$|In Paper [I] we use data on Swedish public {{procurement}} auctions for internal regularcleaning service contracts to provide novel empirical evidence regarding green publicprocurement (GPP) {{and its effect}} on the potential suppliers’ decision to submit a bid andtheir probability of being qualified for supplier selection. We find only a weak effect onsupplier behavior which suggests that GPP does not live up to its political expectations. However, several environmental criteria appear to be associated with increased complexity,as indicated by the reduced probability of a bid being qualified in the postqualificationprocess. As such, GPP appears to have limited or no potential to function as an environmentalpolicy instrument. In Paper [II] the observation is made that empirical evaluations of the effect of policiestransmitted through {{public procurement}}s on bid sizes are made using linear regressionsor by more involved non-linear structural models. The aspiration is typically to determinea marginal effect. Here, I compare marginal effects generated under both types ofspecifications. I study how a political initiative to make firms less environmentally damagingimplemented through public procurement influences Swedish firms’ behavior. Thecollected evidence brings about a statistically as well as economically significant effect onfirms’ bids and costs. Paper [III] embarks by noting that auction theory suggests that as the number of bidders(competition) increases, the sizes of the participants’ bids decrease. An issue in theempirical literature on auctions is which measurement(s) of competition to use. Utilizinga dataset on public procurements containing measurements on both the actual and potentialnumber of bidders I find that a workhorse model of public procurements is bestfitted to data using only actual bidders as measurement for competition. Acknowledgingthat all measurements of competition may be erroneous, I propose an instrumental variableestimator that (given my data) brings about a competition effect bounded by thosegenerated by specifications using the actual and potential number of bidders, respectively. Also, some asymptotic results are provided for non-linear least squares estimatorsobtained from a dependent variable transformation model. Paper [VI] introduces a novel method to measure bidders’ costs (valuations) in descending(ascending) auctions. Based on two bounded rationality constraints bidders’costs (valuations) are given an imperfect measurements interpretation robust to behavioraldeviations from traditional rationality assumptions. Theory provides no guidanceas to the shape of the cost (valuation) distributions while empirical evidence suggeststhem to be <b>positively</b> <b>skew.</b> Consequently, a flexible distribution is employed in an imperfectmeasurements framework. An illustration of the proposed method on Swedishpublic procurement data is provided along with a comparison to a traditional BayesianNash Equilibrium approach...|$|R
40|$|Trembanis, Arthur C. The {{stock of}} the Mid-Atlantic Bight sea scallop fishery is {{assessed}} every year {{through the use of}} various dredging and imaging techniques. The sustainability of the fishery depends on the proper setting of the yearly catch limits based on the assessment of the preceding year. Within the past 10 years, digital image surveys have been explored as a potential method to supplement the yearly dredged based surveys. AUVs {{have been shown to be}} a successful platform for rapidly and accurately performing seafloor image surveys of benthic habitats. In 2011, a Teledyne-Gavia autonomous underwater vehicle (AUV) with a hull-mounted camera was used to non-invasively optically and acoustically image 313 km of the seafloor within the Mid-Atlantic Bight at a constant altitude of 2 m. Survey transects were completed at 24 open access ground locations and 3 additional locations within the Elephant Trunk Access Area. Trained image analysts, using a scallop counting and sizing algorithm developed for this stock assessment, were able to enumerate and size sea scallops within the collected 250, 000 seafloor images, finding that the region had an overall scallop density of 0. 027 scallops/m 2. Georeferenced data was tagged by the AUV inertial navigation system (INS) to every seafloor image, allowing for unprecedented meter scale spatial analysis of the sea scallop distribution. The relationship between image subsampling and the accuracy of the resulting scallop density was explored via simulations run on the image analysis results. Eight AUV transects were resurveyed by a New Bedford commercial scallop dredge for shell height calibration data and to calculate the harvest efficiency of the dredge (0. 60). Image analysis and backscatter data collected by the AUV’s 900 kHz side-scan sonar were used to classify seafloor substrate types. The surveyed scallop strata were classified as 98. 6 % sandy seafloor with the remaining 1. 4 % representing intermittent shell hash, mounds, and ripples. The side-scan backscatter data revealed other varied seafloor texture, including escarpments from scallop dredge trawling and wave created sorted bedforms. Seafloor dredge scar area measured from the side-scan backscatter data and National Oceanic and Atmospheric Administration (NOAA) vessel monitoring system (VMS) tracking data were used as a proxy for fishing effort. Increased dredging was found to <b>positively</b> <b>skew</b> shell height distributions. University of Delaware, Department of GeologyM. S...|$|R
30|$|When pooling {{the index}} scores to dichotomous {{values in the}} present study, the mBoP score {{was found to have}} {{decreased}} from 2 or 3 to 0 or 1 in 55 % of the samples. Although the complete absence of inflammation was difficult to achieve in most implants, significant and stable reductions in the parameters of inflammation were demonstrated in most sites up to 6  months after treatment with the chitosan brush. Nicotine interferes with the bleeding response in soft tissues and may, consequently, lead to false negatives for the BoP; therefore, to avoid <b>positively</b> <b>skewing</b> the results due to smoking, we decided to exclude current smokers from this study. According to the literature, bleeding on probing has low sensitivity as a predictor for active peri-implant disease because of the high frequency of false-positive responses, but it has a high level of specificity as no bleeding on probing indicates peri-implant health [38]. Due to the absence of perpendicular periodontal fibres in dental implants, a lighter probing force should be used than when probing the gingival crevice in teeth. Similarly, the standardization of the examiners’ technique is critical [39]. A pressure-sensitive periodontal probe was used to record PPD and mBoP. We also used a 3 -graded bleeding index to further distinguish between true disease and bleeding {{from the base of the}} pocket as the result of excessive pressure and rupture of the junctional epithelium. Scores of 1 and 0 and scores of 2 and 3 were pooled to create a more rigid, dichotomous score. This strengthens the positive results because significant differences were obtained when both the graded and dichotomous BoP scores were analysed. However, previous smokers were included but the outcomes in this patient group did not differ from finding in never smokers. Similarly, patients taking anticoagulants were excluded to avoid false-positive bleeding scores because of the increased bleeding response caused by the medication. Salvi and co-workers [40] studied the reduction in experimental peri-implant mucositis and revealed that 3  weeks of reinstituted plaque control did not yield pre-experimental levels of peri-implant health. While infection control was carefully installed prior to baseline, and the included implants were plaque free, we found significant reductions in the parameters of inflammation as early as 2  weeks after treatment with the chitosan brush. These results were stable up to 6  months after treatment, indicating a fast and stable response. More of the crown margins were exposed 2  weeks after debridement with the brush and the levels were thereafter stable. The more apical position the crown margins is most probably related to reduction in the soft tissue oedema from the inflammation.|$|R
5000|$|Frequency {{values are}} {{calculated}} first and {{without reference to}} experimental data. Imagine that 48 numbers were presented {{on the same page}} in order to get ratings of numerical magnitude along six category scales. If just the frequency principle was followed, these 48 numbers would be divided equally amongst the six available categories. Suppose, however, that stimuli are presented with varying frequencies. In research judging the sizes of squares (where 1 = very small, 5 = very large), suppose that a <b>positively</b> <b>skewed</b> stimulus set emerges such that in a block of 25 square presentations, 10 are judged as category 1, 7 as category 2, 4 as category 3, 2 as category 4 and 2 as category 5 rank. As Parducci (1995, p. 83) delineates: [...] "one half of the 10 presentations of the smallest size are rated 1, the other half, 2 - yielding a mean rating of 1.5. For the second size, five of its seven presentations are rated 3, the other two, 4 - for a mean rating of + (2X4)/7 = 3.29. For the third size, three of its four presentations are rated 4, the fourth, 5 - for a mean rating of + (1X5)/4 = 4.25. Both presentations of each of the two largest sizes are rated 5." ...|$|E
5000|$|Finally, {{in chapter}} 11, Sterelny {{discusses}} the [...] "evolutionary escalator", or the tendency over time {{for life on}} earth to show a progressive increase both in complexity and adaptivity. While Gould does not outright reject this, he thinks it is a misleading {{way to think about}} the history of life. As above, with the example of horses, Gould argues that there has been no directional trend, but rather, a massive extinction in the horse lineage, with the surviving remnants happening to be largish grazers. So the appearance of a trend is generated by a reduction in heterogeneity. [...] "A trend which is hostage to one switch between life and death is no trend at all." [...] (p. 146) At the scale of complexity, the same applies. [...] "What we think of as a progressive increase in complexity is a change in the difference between the least and the most complex organism. It is a change in the spread of complexity." [...] (p. 146) Life starts in the simplest form that the constraints of chemistry and physics will allow, with bacteria probably close to that limit. [...] "So life starts at the minimum level of complexity. Since even now nearly everything that is alive is a bacterium, for the most part life has stayed that way." [...] (p. 146) But occasionally life builds a lineage that becomes more complex over time. There are no global evolutionary mechanisms that either prevent more complex organisms evolving from simpler ones, or that make it more likely to occur. Complexity tends to drift up because the point of life's origin is close to the physical lower bound. Such complex creatures are relatively less than bacteria, which still dominate life, but the difference between the simplest and most complex organisms tends to become greater over time. So the increased range is wholly undirected. Displayed as a frequency distribution curve or histogram, the shape would be skewed to the right (i.e. <b>positively</b> <b>skewed),</b> with the mode near the left. Over time, the range would increase as average complexity drifts upwards. But the mode would remain at left, with the curve spreading to the right, because there is a wall imposed by the laws of the physical sciences to the left, but not to the right.|$|E
30|$|We {{note that}} all the {{variables}} are <b>positively</b> <b>skewed</b> for the Islamic countries, while only gross domestic saving (GDS) is negatively skewed for non-Islamic countries. As all the other variables are <b>positively</b> <b>skewed,</b> they are skewed to the right. The values of kurtosis for both Islamic and non-Islamic countries are positive (platykurtic).|$|E
40|$|Possible {{effects of}} signal {{reception}} from different electrojet heights in a skewness of auroral coherent spectra are studied {{assuming that the}} &quot;inherent&quot; spectral line due to plasma turbulence is of type- 2 and symmetrical. For reasonable ionospheric parameters, the altitude integrated spectra {{are expected to be}} skewed negatively for positive mean Doppler shift, in agreement with radar observations at small aspect angles. However, the spectra could be <b>skewed</b> <b>positively</b> if the turbulent layer or the electron density profile is shifted to high altitudes of ~ 120 km. This change of spectral shape will not be observed experimentally if, at the same time, either the electron collision frequency is enhanced or the &quot;inherent&quot; spectral width is increased. Observational results are discussed in view of the predictions given...|$|R
40|$|It is {{well known}} that the {{inclusion}} of the threshold parameter in a lognormal distribution creates serious complications for parameter estimation; several parameterized schemes and global optimization procedures have been proposed to solve the problem in the maximum likelihood framework. A global Simulated Annealing optimization heuristic is proposed {{to solve the problem of}} maximum likelihood estimation in any parameterization scheme for the three-parameter lognormal distribution, as well as for the extended lognormal distribution. <b>Positively</b> and negatively <b>skewed</b> lognormal distributions are considered by introducing a one-parameter conditional estimation procedure in the classical parameterization for the three-parameter lognormal distribution, and a dual reparameterization is introduced for parameters estimation in the extended lognormal distribution. Simulated and real data are analyzed to test the efficiency of the proposed algorithm. ...|$|R
40|$|This paper {{introduces}} new {{and flexible}} classes of inefficiency distributions for stochastic frontier models. We consider both generalized gamma distributions and mixtures of generalized gamma distributions. These classes cover many interesting cases and accommodate both <b>positively</b> and negatively <b>skewed</b> composed error distributions. Bayesian methods allow for useful inference with carefully chosen prior distributions. We recommend a two-component mixture model where a sensible amount of structure is imposed through the prior {{to distinguish the}} components, which are given an economic interpretation. This setting allows for efficiencies to depend on firm characteristics, through the probability of belonging to either component. Issues of label-switching and separate identification of both the measurement and inefficiency errors are also examined. Inference methods through MCMC with partial centring are outlined and used to analyse both simulated and real data. An illustration using hospital cost data is discussed in some detail. ...|$|R
