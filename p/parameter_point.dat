87|1627|Public
25|$|With both {{of these}} strategies, a subset of {{statistics}} is selected from a large set of candidate statistics. Instead, the partial least squares regression approach uses information from all the candidate statistics, each being weighted appropriately. Recently, a method for constructing summaries in a semi-automatic manner has attained a considerable interest. This method {{is based on the}} observation that the optimal choice of summary statistics, when minimizing the quadratic loss of the <b>parameter</b> <b>point</b> estimates, can be obtained through the posterior mean of the parameters, which is approximated by performing a linear regression based on the simulated data.|$|E
2500|$|All ABC based methods {{approximate}} the likelihood function by simulations, {{the outcomes of}} which are compared with the observed data. More specifically, with the ABC rejection algorithm—the most basic form of ABC—a set of parameter points is first sampled from the prior distribution. Given a sampled <b>parameter</b> <b>point</b> , a data set [...] is then simulated under the statistical model [...] specified by [...] If the generated [...] is too different from the observed data , the sampled parameter value is discarded. In precise terms, [...] is accepted with tolerance [...] if: ...|$|E
5000|$|Since we {{now have}} <b>parameter</b> <b>point</b> estimates, [...] and , for the {{underlying}} distribution, {{we would like to}} find a point estimate [...] for the probability of success for event i. This is the weighted average of the event estimate [...] and [...] Given our point estimates for the prior, we may now plug in these values to find a point estimate for the posterior ...|$|E
5000|$|Step 4: The {{distance}} between the observed and simulated transition frequencies [...] is computed for all <b>parameter</b> <b>points</b> (Table 1, column 5). <b>Parameter</b> <b>points</b> for which the distance is smaller {{than or equal to}} [...] are accepted as approximate samples from the posterior (Table 1, column 6).|$|R
40|$|AbstractIn {{this paper}} {{we present a}} {{comprehensive}} analysis of the max-flow problem with n parametric capacities, and give the basis for an algorithm to solve it. In particular we give a method for finding the max-flow value {{as a function of}} the parameters, and max-flows for all <b>parameter</b> <b>points,</b> in terms of max-flow values to problems at certain key <b>parameter</b> <b>points.</b> In the problem with nonzero lower bounds on the arc flows, we derive a set of linear constraints whose solution set is identical to the set of all feasible <b>parameter</b> <b>points.</b> The intrinsic difficulty of the problem is compared with that of the general multiparametric linear programming problem, and thus light is shed on the difficulty of the latter problem, whose complexity is currently unknown...|$|R
50|$|Batting {{power was}} {{supposed}} to be amplified through the addition of <b>parameter</b> <b>points</b> in body strength and stamina. Stamina was never implemented.|$|R
5000|$|All ABC based methods {{approximate}} the likelihood function by simulations, {{the outcomes of}} which are compared with the observed data. More specifically, with the ABC rejection algorithm—the most basic form of ABC—a set of parameter points is first sampled from the prior distribution. Given a sampled <b>parameter</b> <b>point</b> , a data set [...] is then simulated under the statistical model [...] specified by [...] If the generated [...] is too different from the observed data , the sampled parameter value is discarded. In precise terms, [...] is accepted with tolerance [...] if: ...|$|E
50|$|With both {{of these}} strategies, a subset of {{statistics}} is selected from a large set of candidate statistics. Instead, the partial least squares regression approach uses information from all the candidate statistics, each being weighted appropriately. Recently, a method for constructing summaries in a semi-automatic manner has attained a considerable interest. This method {{is based on the}} observation that the optimal choice of summary statistics, when minimizing the quadratic loss of the <b>parameter</b> <b>point</b> estimates, can be obtained through the posterior mean of the parameters, which is approximated by performing a linear regression based on the simulated data.|$|E
5000|$|In {{order to}} compute the strain, the flat state is {{compared}} to the deformed state. (#1 & #2)In a standard measuring project, the flat state, the strain reference, is not captured optically but results from the theoretical point distance defined in the project parameters.As a default, Forming analysis system presumes an exactly regular initial pattern which is on one plane and for which the point distance is known. This is called the [...] "virtual reference stage" [...] and is marked with Stage 0 in italic letters in the software. All strain values refer to the adjusted computation <b>parameter</b> <b>Point</b> distance.The Forming analysis system software is also capable of analyzing several static deformation states (stages) within one project where each deformation stage can be set as strain reference any time. This procedure may be used, for example, for the deformation analysis of tubes.To allow for a full-field view of the strain, the software changes to the so-called grid mode (#3 & #4). This means that based on the center points of the measuring points a grid surface is created. Each grid line intersection point represents a 3D measuring point. The full-field color representation of the strain results from the 3D positions of these grid line intersection points. (#5 & #6) ...|$|E
2500|$|Step 2: Assuming {{nothing is}} known about , a uniform prior in the {{interval}} [...] is employed. The parameter [...] {{is assumed to be}} known and fixed to the data-generating value (...) , but could in general also be estimated from the observations. A number n of <b>parameter</b> <b>points</b> are drawn from the prior, and the model is simulated for each of the <b>parameter</b> <b>points</b> , which results in [...] sequences of simulated data. In this example, n=5, with each drawn parameter and simulated dataset recorded in Table 1, column 2-3. In practice, n would need to be much larger to obtain an appropriate approximation.|$|R
3000|$|..., {{and the two}} {{associated}} valley points, VP 1 _i and VP 2 _i, {{are identified}} using three-points sliding window method (Dumpala et al. 1982; Billauer 2012), the other five <b>parameter</b> <b>points</b> i.e., the half point at first half wave (HP 1 [...]...|$|R
30|$|With the {{estimated}} signal <b>parameters</b> to select <b>points</b> in the time-frequency domain, for signals of different frequency <b>parameters,</b> the <b>points</b> on respective time-frequency ridge {{were selected to}} construct their own SPTFD matrix.|$|R
40|$|In his 1953 paper Lucien Le Cam proved {{for regular}} {{univariate}} statistical models that sets of points of superefficiency have Lebesgue measure zero (in fact, these sets are even countable). Considering only computable estimators, {{it is possible}} to show that no computable <b>parameter</b> <b>point</b> can be a point of superefficiency. This strengthens Le Cam’s result to a dichotomy: either a <b>parameter</b> <b>point</b> θ can be computably estimated with zero error, or no computable estimator is more efficient at θ than the maximum likelihood estimator. ...|$|E
40|$|The {{standard}} F {{test for}} linear restrictions in regression is relevant {{as a criterion}} but fails to capture the notion of tradeoff between bias and variance. Average squared distance criteria yield operational tests that are more appropriate, depending upon objectives. In the present paper two alternative criteria are developed. The first allows testing of {{the hypothesis that the}} average squared distance of a restricted estimator from the <b>parameter</b> <b>point</b> in k space is less than the average squared distance of the unrestricted, ordinary least squares estimator from the same <b>parameter</b> <b>point.</b> The second sets up a test of betterness of the restricted estimator over the unrestricted estimator of E(Y/X), where betterness is again defined in average squared distance...|$|E
40|$|The {{optimality}} {{of existing}} and new <b>parameter</b> <b>point</b> estimators in classical and credibility statistical models can be proved, under zero-excess assumptions only, by protection methods in Hilbert spaces. Here we summarize the recent results. The proofs {{can be found}} in the forthcoming papers mentioned in the References...|$|E
40|$|We plot the {{two-dimensional}} {{projections of}} the parameter spaces of the cubic mappings. The projection of the <b>parameter</b> <b>points</b> that have non-totally disconnected Julia sets {{can be seen as}} a combination of Mandelbrot-like sets. The regularities of these projections with respect to parameters are explained using elementary analysis...|$|R
40|$|AbstractThe paper proves global indentification of {{the unique}} variances in factor analysis, for almost all <b>parameter</b> <b>points,</b> {{if the number of}} factors is smaller than the Ledermann bound. It also shows how to compute, using a closed-form representation, the second {{solution}} in the case of six variables and three factors...|$|R
40|$|This paper {{proposes a}} method called meshless parameterization, for {{parameterizing}} and triangulating "single patch" unorganized point sets. The points are mapped into a planar parameter domain by solving a sparse linear system. By making a standard triangulation of the <b>parameter</b> <b>points,</b> we obtain a corresponding triangulation {{of the original}} data set...|$|R
40|$|Optimality of estimators of a vector {{parameter}} {{in terms}} of the probability of the estimators being contained in suitable region(s) around the <b>parameter</b> <b>point</b> is defined. Conditions under which optimal estimators in the usual senses are also optimal in the above sense are investigated. Similar laws maximum probability estimator invariant orthonormal bases...|$|E
40|$|Here {{we review}} the possibilities, {{advantages}} and disadvantages of using magnetic material to form structures with one-sided fluxes (i. e., "Halbach arrays") for purpose of energy storage. These can form highly non-linear magnetic springs which can be widely adapted for specific cases, but an optimal <b>parameter</b> <b>point</b> exist for energy storage. QC 20160923 </p...|$|E
30|$|Several {{approaches}} for <b>parameter</b> <b>point</b> estimation were {{proposed in the}} literature but the maximum likelihood method is the most commonly employed. The maximum likelihood estimates (MLEs) enjoy desirable properties {{that can be used}} when constructing confidence intervals for the model parameters. Large sample theory for these estimates delivers simple approximations that work well in finite samples. The normal approximation for the MLEs in distribution theory is easily handled either analytically or numerically.|$|E
40|$|This paper compares two exact in {{probability}} confidence regions for {{the parameters}} of nonlinear regression in normal case. The employed tool is an old one, the power function of the dual test of a confidence region. This function depends upon two real variables, a standardized norm and a curvature measure between two <b>parameter</b> <b>points...</b>|$|R
40|$|The paper proves global indentification of {{the unique}} variances in factor analysis, for almost all <b>parameter</b> <b>points,</b> {{if the number of}} factors is smaller than the Ledermann bound. It also shows how to compute, using a closed-form representation, the second {{solution}} in the case of six variables and three factors. (C) 1997 Elsevier Science Inc...|$|R
2500|$|Step 5: The {{posterior}} distribution is approximated with the accepted <b>parameter</b> <b>points.</b> The {{posterior distribution}} {{should have a}} non-negligible probability for parameter values in a region around the true value of [...] in the system, if the data are sufficiently informative. In this example, the posterior probability mass is evenly split between the values 0.08 and 0.43.|$|R
40|$|In the Littlest Higgs {{model with}} T-parity, we study {{production}} processes of new gauge bosons {{at the international}} linear collider (ILC). Through Monte Carlo simulations of the production processes, we show that the heavy gauge boson masses can be determined very accurately at the ILC for a representative <b>parameter</b> <b>point</b> of the model. From the simulation result, we also discuss the determination of other model parameters at the ILC. Comment: 4 pages, 2 figures. Talk given at LCWS 08, Chicago, November 200...|$|E
40|$|We {{study the}} problem k-SET SPLITTING from a Fixed <b>Parameter</b> <b>point</b> of view. We give a linear kernel of 2 k {{elements}} and 2 k sets and improve {{on the current}} best running time for the problem, giving a O ∗ (4 k) algorithm. This is done by reducing the problem to a bipartite graph problem where we use crown decomposition to reduce the graph. We show that this result also gives a good kernel for Max Cut. Keywords: Algorithms, Graph Algorithms. ...|$|E
40|$|The general four <b>parameter</b> <b>point</b> {{interaction}} in one dimensional {{quantum mechanics is}} regulated. It allows the exact solution, but not the perturbative one. We conjecture that {{this is due to}} the interaction not being asymptotically free. We then propose a different breakup of unperturbed theory and interaction, which now is asymptotically free but leads to the same physics. The corresponding regulated potential can be solved both exactly and perturbatively, in agreement with the conjecture. Comment: 17 pages, no figures, Tex fil...|$|E
40|$|ABSTRACT. The study {{developed}} {{in this work}} is focused on constitutive modelling of saturated and unsaturated granular soils from the state <b>parameters</b> <b>point</b> of view. The Generalized Plasticity constitutive equation has been extended in order to reproduce stress-strain behaviour of granular soils with a single set of intrinsic model constants for different densities, confining pressures and saturation conditions. 1...|$|R
40|$|In this study, {{existing}} {{code for}} the tectonic loading process simulation at transcurrent plate boundaries have been parallelized on workstation clusters using MPI. Matrix assembling and inversion processes have been mainly opti-mized for parallel computing. 2 -way parallelization method {{has been developed}} for local operation both for <b>parameter</b> <b>points</b> and data/integral points. Good par-allel performance results are obtained...|$|R
40|$|International audienceThe study {{developed}} {{in this work}} is focused on constitutive modelling of saturated and unsaturated granular soils from the state <b>parameters</b> <b>point</b> of view. The Generalized Plasticity constitutive equation has been extended in order to reproduce stress-strain behaviour of granular soils with a single set of intrinsic model constants for different densities, confining pressures and saturation conditions...|$|R
40|$|In {{this paper}} we {{describe}} and present {{results on the}} <b>parameter</b> <b>point</b> estimation for the scale and threshold parameters of the Rayleigh distribution. Five estimating methods have been investigated, namely, the maximum likelihood, the method of moment, the probability weighted moments method, the least square method and the least absolute deviation method. Modified maximum likelihood estimators for the parameters are also proposed. Simulation {{studies have shown that}} the modified likelihood estimator outperforms the estimators obtained with the other methods except in the case of very small samples. ...|$|E
40|$|Point Set Surfaces define smooth {{surfaces}} {{from regular}} samples based on weighted averaging of the points. Because weighting is done {{based on a}} spatial scale <b>parameter,</b> <b>point</b> set surfaces apply basically only to regular samples. We suggest to attach individual weight functions to each sample {{rather than to the}} location in space. This extends Point Set Surfaces to irregular settings, including anisotropic sampling adjusting to the principal curvatures of the surface. In particular, we describe how to represent surfaces with ellipsoidal weight functions per sample. Details of deriving such a representation from typical inputs and computing points on the surface are discussed...|$|E
40|$|AbstractMany {{combinatorial}} optimization problems {{call for the}} optimization of a linear function over a certain polytope. Typically, these polytopes have an exponential number of facets. We explore the problem of finding small linear programming formulations when one may use any new variables and constraints. We show that expressing the matching and the Traveling Salesman Problem by a symmetric linear program requires exponential size. We relate the minimum size needed by a LP to express a polytope to a combinatorial <b>parameter,</b> <b>point</b> out some connections with communication complexity theory, and examine the vertex packing polytope for some classes of graphs...|$|E
5000|$|Step 5: The {{posterior}} distribution is approximated with the accepted <b>parameter</b> <b>points.</b> The {{posterior distribution}} {{should have a}} non-negligible probability for parameter values in a region around the true value of [...] in the system, if the data are sufficiently informative. In this example, the posterior probability mass is evenly split between the values 0.08 and 0.43.|$|R
40|$|ABSTRACT-This paper {{presents}} exhaustive {{review of}} various methods/techniques for incorporation of differential algebraic equations (DAE) model of FACTS controllers in multi-machine power system environments for enhancement of different operating parameters viewpoint such as damping, voltage stability, voltage security, loadability, active {{power and energy}} losses, power transfer capability, cost of generation and FACTS controllers, dynamic performance, and others <b>parameters</b> <b>point</b> of view. It also reviews various current techniques/methods for incorporation of FACTS controllers in power systems through all over world. Authors strongly believe that this survey article will be very much useful to the researchers for finding out the relevant references {{in the field of}} incorporation of FACTS controllers in power system environments for enhancement of damping of power oscillations, voltage stability, voltage security, loadability, active power and energy losses, power transfer capability, cost of generation and FACTS controllers, dynamic performance, and others <b>parameters</b> <b>point</b> of view...|$|R
40|$|Code for {{simulation}} of tectonic loading process at transcurrent plate boundaries was parallelized by MPI. Matrix assembly and inversion processes {{have been largely}} optimized for parallel computing, and 2 -way parallelization was developed for local operations for <b>parameter</b> <b>points</b> and data/integral points. Computations using this method on a Hitachi SR 2201 with up to 128 processors demonstrated good scalability. 1...|$|R
