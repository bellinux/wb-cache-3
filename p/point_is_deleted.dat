1|10000|Public
5000|$|McCallum and Avis {{were first}} {{to provide a}} correct {{algorithm}} to construct the convex hull of a simple polygon [...] in [...] time. The basic idea is very simple. The leftmost vertex is on the convex hull and we denote it [...] The second point {{is assumed to be}} a candidate convex hull vertex as well. At each step one looks at three consecutive vertices of the polygon, with two first ones tentatively assigned to the growing convex hull and the third one is a new unprocessed vertex of the polygon, say, we denote this as [...] If the angle is convex, then [...] and the whole triple is shifted by one vertex along the polygon. If the resulting angle is concave, then the middle <b>point</b> (...) <b>is</b> <b>deleted</b> and the test is repeated for the triple , etc. until we backtrack either to a convex angle or to point [...] After that the next (along the polygon) vertex is added to the triple to be tested, and the process repeats. However several previously published articles overlooked a possibility that deletion of a vertex from a polygon may result in a self-intersecting polygon, rendering further flow of the algorithm invalid. Fortunately, this case may also be handled efficiently. Later Tor and Middleditch (1984, [...] "Convex Decomposition of Simple Polygons") and independently Melkman (1985, [...] "Online Construction of the convex hull of a simple polyline") suggested a simpler approach with the same time complexity.|$|E
5000|$|... 1 <b>point</b> has <b>been</b> <b>deleted</b> {{because of}} the {{incidents}} happened by the Efes Pilsen Game. [...] (**) 1 <b>point</b> has <b>been</b> <b>deleted</b> {{because of the}} incidents happened by the Erdemirspor Game.|$|R
40|$|FIG. 1. Kinetic {{analysis}} ofthe effects ofdecapitation, ether and halothane on adrenal tyrosine hydroxylase {{activity at}} various substrate concentrations The results are means from five experiments ± S. E. M. All values were determined at 1 mrst 6 -MePtH 4. Six tyrosine concentrations were employed. For {{clarity of the}} figures the 0. 66 i and 1 zM <b>points</b> <b>were</b> <b>deleted</b> from the 5, V plot and the 100 5 M <b>point</b> <b>was</b> <b>deleted</b> from the 1 / 5, 1 /V plot. ...|$|R
5000|$|... #Subtitle level 3: Windows System Restore <b>points</b> may <b>be</b> <b>deleted</b> during defragmenting/optimizing ...|$|R
30|$|BVol% {{data were}} checked for outliers. One {{measurement}} <b>point</b> <b>was</b> <b>deleted</b> because it {{went against the}} trend for all other trees where BVol% decreased from the butt to 1.4  m. It also had the largest BVol% and produced a large residual if it was included in any regression models.|$|R
50|$|Mersin İdmanyurdu {{has become}} the only team {{in the history of}} Turkish first level {{football}} league, whose <b>points</b> <b>were</b> <b>deleted.</b> In 1980-81 season the team has drawn away from the field in Beşiktaş game. For that reason TFF decided to award the match to Beşiktaş by 3-0 and delete two extra points of MİY. MİY finished season with 21 points at 15th place, 6 points back of 14th team.|$|R
50|$|Running most defragmenters and optimizers {{can cause}} the Microsoft Shadow Copy service to delete some of the oldest restore points, even if the defragmenters/optimizers are built on Windows API. This is due to Shadow Copy keeping track of some {{movements}} of big files performed by the defragmenters/optimizers; when the total disk space used by shadow copies would exceed a specified threshold, older restore <b>points</b> <b>are</b> <b>deleted</b> until the limit is not exceeded.|$|R
30|$|Modify: In {{order to}} modify a data point over {{encrypted}} DISC, the CSP conducts an insert and a delete operation on the index at the server. Thus, the DO has {{to send the}} data <b>point</b> to <b>be</b> <b>deleted</b> and the new data <b>point</b> to <b>be</b> inserted. For the modify operation, the CSP needs to first locate where the <b>point</b> to <b>be</b> <b>deleted</b> lies in the index, then <b>delete</b> it. It <b>is</b> not feasible to insert the data point in the same location as the deleted point as their positions in the indexed tree may differ. Lastly, updates are propagated upwards till the root node of the tree.|$|R
3000|$|... b was {{included}} as a variable parameter in the fit. In {{order to reduce the}} impact of noise in the experimental data on χ 2, smoothed TACs and IFs were generated from the whole experimental datasets with robust locally weighted regression (LOWESS) smoothing as implemented in MATLAB. Furthermore, to assess the effects of limited degrees of freedom, every second data <b>point</b> <b>was</b> <b>deleted</b> from the experimental TACs. Modeling was then performed as described above with fixed or variable v [...]...|$|R
5000|$|... 1. Szentes 26, 2. MTK 25, 3. Csepel Autó 23, 4. Tatabánya 20, 5. Budai Spartacus 16, 6. Bp. VTSK 11, 7. Bp. Építők 9, 8. Hódmezővásárhelyi MTE 8, 9. Szolnoki Honvéd 6 <b>point.</b> SZEAC <b>were</b> <b>deleted.</b>|$|R
500|$|As {{reported}} on numerous {{sites on the}} internet (reference topic: AVG Turbo Mode System Restore Error), one of the Windows features, System Restore, is no longer accessible in this mode and provides an error code when attempting to access this feature requiring an AVG reset to [...] "Standard Mode" [...] to re-enable this feature. [...] Some have reported that prior restore <b>points</b> <b>are</b> <b>deleted</b> when [...] "Turbo Mode" [...] is activated; [...] However, this is not confirmed, and {{has been reported to}} AVG IT for correction for some time now.|$|R
50|$|The Bowyer-Watson {{algorithm}} is an incremental algorithm. It works by adding points, {{one at a}} time, to a valid Delaunay triangulation of {{a subset of the}} desired points. After every insertion, any triangles whose circumcircles contain the new <b>point</b> <b>are</b> <b>deleted,</b> leaving a star-shaped polygonal hole which is then re-triangulated using the new point. By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, although special degenerate cases exist where this goes up to O(N2).|$|R
30|$|False alarms are removed: if motion {{characteristics}} of successive subsequences are similar, those subsequences are merged {{and the change}} <b>point</b> between them <b>is</b> <b>deleted.</b>|$|R
40|$|One of the {{practical}} problems in engineering and science is the estimation of parameters for a given experimental data. It is also often required to update the estimates as new <b>points</b> <b>are</b> added to or as old <b>points</b> <b>are</b> <b>deleted</b> from the given data set. For data containing wild points, parameter estimation using approximation in the L 1 norm is usually recommended over other norms. It is shown that the updating of the the estimates is calculated with very little effort using parametric programming techniques, applied to an existing L 1 approximation algorithm which itself uses a dual simplex method. Thus an online adaptation for this algorithm is highly desirable. Numerical results and comments are given. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Plant {{differently}} colored {{points in}} the plane, then let random points ("Poisson rain") fall, and give each new point {{the color of the}} nearest existing point. Previous investigation and simulations strongly suggest that the colored regions converge (in some sense) to a random partition of the plane. We prove a weak version of this, showing that normalized empirical measures converge to Lebesgue measures on a random partition into measurable sets. Topological properties remain an open problem. In the course of the proof, which heavily exploits time-reversals, we encounter a novel self-similar process of coalescing planar partitions. In this process, sets A(z) in the partition are associated with Poisson random points z, and the dynamics <b>are</b> as follows. <b>Points</b> <b>are</b> <b>deleted</b> randomly at rate 1, when z <b>is</b> <b>deleted,</b> its set A(z) is adjoined to the set A(z^') of the nearest other point z^'. Comment: 47 page...|$|R
50|$|Denmark has {{a penalty}} point system that penalizes drivers with a klip ("cut/stamp") for certain traffic violations. The term klip {{refers to a}} klippekort ("punch card ticket"). If a driver with a non-probationary license accumulates three penalty points, then police conditionally suspend the driver's license. To get a new license, {{suspended}} drivers must pass both written and practical drivers examinations. Drivers who have been suspended and first-time drivers must avoid collecting two penalty points for a three-year probationary period; if the driver has not accumulated any penalty points, then the driver is allowed an extra penalty point so they can have three maximum. Penalty <b>points</b> <b>are</b> <b>deleted</b> from the police database three years after they were assessed. Police can also unconditionally ban people from driving.|$|R
5000|$|The Turbo Mode, {{accessible}} {{from the}} bottom of Start Center, gives system a performance boost by temporarily disabling some of Windows services and features of the user's choice, such as Windows Aero themes and visual effects. [...] As reported on numerous sites on the internet (reference topic: AVG Turbo Mode System Restore Error), one of the Windows features, System Restore, is no longer accessible in this mode and provides an error code when attempting to access this feature requiring an AVG reset to [...] "Standard Mode" [...] to re-enable this feature. Some have reported that prior restore <b>points</b> <b>are</b> <b>deleted</b> when [...] "Turbo Mode" [...] is activated; However, this is not confirmed, and has been reported to AVG IT for correction for some time now.|$|R
40|$|Abstract. We {{consider}} the following stochastic optimization problem first introduced by Chen et al. in [6]. We are given a vertex set of a random graph where each possible edge is present with probability pe. We do not know which edges are actually present unless we scan/probe an edge. However whenever we probe an edge and find it to be present, we are constrained to picking the edge and both its end <b>points</b> <b>are</b> <b>deleted</b> from the graph. We wish to find the maximum matching in this model. We compare our results against the optimal omniscient algorithm that knows {{the edges of the}} graph and present a 0. 573 factor algorithm using a novel sampling technique. We also prove that no algorithm can attain a factor better than 0. 896 in this model. ...|$|R
3000|$|Delete: To delete a data point, the Hilbert {{value of}} the data <b>point</b> to <b>be</b> <b>deleted</b> <b>is</b> sent in an {{encrypted}} format by the DO to the CSP. The Hilbert value (H) is encrypted using OPE, so that comparison operations can be conducted on the index. In the Hilbert R-tree deletion process, the entry with the Hilbert key value (i.e. leaf node, n [...]...|$|R
40|$|The Polar Diagram [1] {{of a set}} {{of points}} (i. e. sites) is a {{partition}} of the plane. It is a locus approach for problems processing angles. Also, Dynamic Polar Diagram problem is a problem in which some <b>points</b> can <b>be</b> added to or removed from the point set of the Polar Diagram. Sadeghi et al. [4] introduced this problem and solved it using an algorithm which is optimal in the case that some <b>points</b> <b>are</b> <b>deleted</b> from the set. But this algorithm is not optimal when some new <b>points</b> <b>are</b> inserted into the diagram. In this paper, we present an algorithm to solve the Dynamic Polar Diagram in optimal time in which we insert some new points into the diagram one by one. Our approach applies only on the regions that would be changed and solves the problem for each insertion in O(k + log n) time, in which 1 ≤ k ≤ n is the number of the sites which their regions would be changed. ...|$|R
40|$|Since {{the seminal}} article by Cook, {{the usual way}} to measure the {{influence}} of an observation in a statistical model <b>is</b> to <b>delete</b> the observation from the sample and compute a convenient norm {{of the change in}} the parameters or in the vector of forecasts. In this article we define a new way to measure the influence of an observation based on how this observation is being influenced {{by the rest of the}} data. More precisely, the new statistic we propose is defined as the squared norm of the vector of changes of the forecast of one observation when each of the sample <b>points</b> <b>are</b> <b>deleted</b> one by one. We prove that this new statistic has asymptotically a normal distribution and is able to detect a group of high leverage similar outliers that will be undetected by Cook’s statistic. We show in several examples that the proposed statistic is useful for detecting heterogeneity in regression models in large high-dimensional datasets...|$|R
40|$|AbstractSuppose {{a number}} of <b>points</b> <b>are</b> <b>deleted</b> from a sample of random vectors in Rd. The number of deleted points may depend on the sample sizen, and on any other sample information, {{provided}} only that it is bounded in probability asn→∞. In particular, “extremes” of the sample, however defined, may <b>be</b> <b>deleted.</b> We show that this operation {{has no effect on}} the asymptotic normality of the sample sum, {{in the sense that the}} sum of the <b>deleted</b> sample <b>is</b> asymptotically normal, after norming and centering, if and only if the sample sum itself is asymptotically normal with the same norming and centering as the <b>deleted</b> sum. That <b>is,</b> the sample must be drawn from a distribution in the domain of attraction of the multivariate normal distribution. The domain of attraction concept we employ uses general operator norming and centering, as developed by Hahn and Klass. We also show that random deletion has no effect on the “quadratic negligibility” of the sample. These are conditions that are important in the robust analysis of multivariate data and in regression problems, for example...|$|R
40|$|We define some linear spaces {{on the set}} of all proper subspaces of {{a triple}} system S(γ 2, 3,v). The {{connected}} components of these linear spaces are projective spaces of order 2 and punctured projective spaces of order 3, i. e. projective spaces of order 3 from which a <b>point</b> has <b>been</b> <b>deleted.</b> We show how these connected components can be used to find affine and projective factors in S(γ 2, 3,v). © 1987. SCOPUS: ar. kinfo:eu-repo/semantics/publishe...|$|R
25|$|After the ST-M {{came the}} ST-3, a {{substantial}} redesign in 1941 partly {{brought about by}} the unreliability of the Menasco engines fitted to STs to that point. The United States Army Air Corps (USAAC) had purchased several dozen ST-M variants under various designations and had Ryan Aeronautical re-engine most with Kinner R-440 radial engines. The USAAC found the modification to be beneficial and asked Ryan Aeronautical to design a variant with this engine as standard, and with airframe modifications considered desirable from in-service experience. The ST-3 that resulted featured a longer and more circular wider fuselage, this being suggested by the circular radial engine. Other changes included a revised rudder, balanced ailerons and elevators, and strengthened main landing gear with the legs spaced further apart. The streamlining spats covering the mainwheels, found on ST series aircraft to that <b>point,</b> <b>were</b> <b>deleted</b> as well. The ST-3 served as the basis for military versions ordered by the USAAC and the United States Navy (USN).|$|R
40|$|Suppose {{a number}} of <b>points</b> <b>are</b> <b>deleted</b> from a sample of random vectors in d. The number of deleted points may depend on the sample sizen, and on any other sample information, {{provided}} only that it is bounded in probability asn [...] >[infinity]. In particular, "extremes" of the sample, however defined, may <b>be</b> <b>deleted.</b> We show that this operation {{has no effect on}} the asymptotic normality of the sample sum, {{in the sense that the}} sum of the <b>deleted</b> sample <b>is</b> asymptotically normal, after norming and centering, if and only if the sample sum itself is asymptotically normal with the same norming and centering as the <b>deleted</b> sum. That <b>is,</b> the sample must be drawn from a distribution in the domain of attraction of the multivariate normal distribution. The domain of attraction concept we employ uses general operator norming and centering, as developed by Hahn and Klass. We also show that random deletion has no effect on the "quadratic negligibility" of the sample. These are conditions that are important in the robust analysis of multivariate data and in regression problems, for example. asymptotic normality random deletion of observations trimming extreme values quadratic negligibility operator norming sums and maxima of random variables sum of squares and products matrix covariance matrix...|$|R
40|$|The {{purpose of}} this study is to {{investigate}} the psychometric properties of scales with different missing data techniques. For this purpose 100 data sets were generated under different conditions of sample sizes (250, 500 and 1000) and number of items (10 and 15), respectively. Data <b>points</b> <b>were</b> <b>deleted</b> under missing completely at random, missing at random and missing not at random conditions by two, five and ten percent. Listwise deletion, similar response pattern imputation based on Euclidian distance, stochastic regression imputation, expectation – maximization algorithm and multiple imputation were carried out on incomplete data sets. Bias of Cronbach α, McDonald  and W coefficients were investigated for reliability estimates. Extracted variances and D 2 statistic obtained by principal component analysis and different indices obtained by confirmatory factor analysis are investigated for validity. Results show that listwise deletion, which is often applied as a default missing data technique, may cause serious problems. On the other hand expectation – maximization algorithm and multiple imputation generally outperformed but none of the techniques are the best for all condition...|$|R
40|$|We {{propose a}} {{procedure}} for computing a fast approximation to regression estimates {{based on the}} minimization of a robust scale. The procedure can be applied {{with a large number}} of independent variables where the usual algorithms require an unfeasible or extremely costly computer time. Also, it can be incorporated in any high-breakdown estimation method and may improve it with just little additional computer time. The procedure minimizes the robust scale over a set of tentative parameter vectors estimated by least squares after eliminating a set of possible outliers, which are obtained as follows. We represent each observation by the vector of changes of the least squares forecasts of the observation when each of the data <b>points</b> <b>is</b> <b>deleted.</b> Then we obtain the sets of possible outliers as the extreme points in the principal components of these vectors, or as the set of points with large residuals. The good performance of the procedure allows identication of multiple outliers, avoiding masking eects. We investigate the procedure’s eciency for robust estimation and power as an outlier detection tool in a large real dataset and in a simulation study. KEY WORDS: Masking; Outliers; Robust regression. 1...|$|R
50|$|It is not {{possible}} to create a permanent restore point. All restore <b>points</b> will eventually <b>be</b> <b>deleted</b> after the time specified in the RPLifeInterval registry setting is reached or earlier if allotted disk space is insufficient. Even if no user or software triggered restore <b>points</b> <b>are</b> generated allotted disk space is consumed by automatic restore points. Consequently, in systems with little space allocated, if a user does not notice a new problem within a few days, {{it may be too late}} to restore to a configuration from before the problem arose.|$|R
40|$|Haptic devices, {{such as the}} PHANTOM [1] (SensAble Technologies, Inc.) can be used {{to develop}} object {{interactions}} where various interaction states and state transitions are implemented through forces, rather than through menu selections, as in typical user interfaces. This parallels certain real-world interactions such as sliding an object over a plane or pressing hard to destroy (delete) something. We present a set of techniques that we call haptic state-surface interactions that are designed to make interactions with 3 D objects more fluid and natural. We develop the example of drawing a polyline on a curved or flat surface. Control <b>points</b> <b>are</b> selected by touching them and this enables them to be slid across the surface. Simply lifting up the stylus and breaking contact releases them. <b>Points</b> <b>are</b> <b>deleted</b> by pushing them through the surface. <b>Points</b> <b>are</b> cloned by applying force so that they “click-down”. We have also developed state-plane techniques that use pop-up orthogonal planes to allow for the positioning of points anywhere in 3 D space. We conducted two experiments to evaluate the state surface technique for the task of laying out a spline curve on a curved surface. The first experiment did not show any significant benefit over more conventional methods but lead to a redesign of the state surface interface. The second experiment showed the modified state surface interaction method to be superior in terms of interaction speed and user preferences to the alternatives...|$|R
40|$|A Kalman filter {{has been}} used to combine all {{publicly}} available, independently determined measurements of the Earth's orientation taken by the modern, space-geodetic techniques of very long baseline interferometry, satellite laser ranging, lunar laser ranging, and the global positioning system. Prior to combining the data, tidal terms were removed from the UT 1 measurements, outlying data <b>points</b> <b>were</b> <b>deleted,</b> series-specific corrections <b>were</b> applied for bias and rate, and the stated uncertainties of the measurements were adjusted by multiplying them by series-specific scale factors. Values for these bias- rate corrections and uncertainty scale factors were determined by an iterative, round-robin procedure wherein each data set is compared, in turn, to a combination of all other data sets. When applied to the measurements, the bias-rate corrections thus determined make the data sets agree with each other in bias and rate, and the uncertainty scale factors thus determined make the residual of each series (when differenced with a combination of all others) have a reduced chi-square of one. The corrected and adjusted series are then placed within an IERS reference frame by aligning them with the IERS Earth orientation series EOP (IERS) 90 C 04. The result of combining these corrected, adjusted and aligned series is designated SPCE 94 and spans October 6. 0, 1976 to January 27. 0, 1995 at daily intervals...|$|R
40|$|The Delaunay triangulations {{of a set}} of <b>points</b> <b>are</b> a {{class of}} triangulations which play an {{important}} role in a variety of different disciplines of science. Regular triangulations are a generalization of Delaunay triangulations that maintain both their relationship with convex hulls and with Voronoi diagrams. In regular triangulations, a real value, its weight, is assigned to each point. In this paper a simple data structure is presented that allows regular triangulations of sets of <b>points</b> to <b>be</b> dynamically updated, that <b>is,</b> new <b>points</b> can <b>be</b> incrementally inserted in the set and old <b>points</b> can <b>be</b> <b>deleted</b> from it. The algorithms we propose for insertion and deletion are based on a geometrical interpretation of the history data structure in one more dimension and use lifted flips as the unique topological operation. This results in rather simple and efficient algorithms. The algorithms have been implemented and experimental results are given. Postprint (published version...|$|R
5000|$|Junction point/directory junction: since Windows 2000, {{links to}} an {{absolute}} directory (\) {{on a local}} volume. Windows Server 2008 uses this configuration for [...] folder redirects. When a junction <b>point</b> <b>is</b> created with , <b>deleting</b> it using Windows Explorer will delete the targeted files immediately if using shift-delete (Windows 2000/XP/2003). The command [...] {{should not be used}} as it will delete all the files in the targeted directory. Deleting a junction <b>point</b> using Explorer <b>is</b> safe since Windows Vista.|$|R
40|$|We {{present an}} {{algorithm}} {{for maintaining the}} width of a planar point set dynamically, as <b>points</b> <b>are</b> inserted or <b>deleted.</b> Our algorithm takes time O(kn ǫ) per update, where k is the amount of change the update causes in the convex hull, n is the number of points in the set, and ǫ is any arbitrarily small constant. For incremental or decremental update sequences, the amortized time per update is O(n ǫ). ...|$|R
40|$|For reserving {{original}} {{sampling points}} {{to reduce the}} simulation runs, two general extension algorithms of Latin Hypercube Sampling (LHS) are proposed. The extension algorithms start with an original LHS of size m and construct a new LHS of size m+n that contains the original points as many as possible. In {{order to get a}} strict LHS of larger size, some original <b>points</b> might <b>be</b> <b>deleted.</b> The relationship of original sampling points in the new LHS structure is shown by a simple undirected acyclic graph. The basic general extension algorithm is proposed to reserve the most original points, but it costs too much time. Therefore, a general extension algorithm based on greedy algorithm is proposed to reduce the extension time, which cannot guarantee to contain the most original <b>points.</b> These algorithms <b>are</b> illustrated by an example and applied to evaluating the sample means to demonstrate the effectiveness...|$|R
40|$|We {{describe}} a geometrically-motivated technique for data classification. Given a finite {{set of points}} in Euclidean space, each classified according to some target classification, we use a hyperplane to separate off a set of points all having the same classification; these <b>points</b> <b>are</b> then <b>deleted</b> from the database and the procedure is iterated until no points remain. We explain how such an iterative `chopping procedure' leads to a type of decision list classification of the data points and in a classification of the data {{by means of a}} linear threshold artificial neural network with one hidden layer. In the case where the data <b>points</b> <b>are</b> all the 2 n vertices of the Boolean hypercube, the technique produces a neural network representation of Boolean functions differing from the obvious one based on a function's disjunctive normal formula. Introduction 3 1 Introduction It is a commonly-occuring problem to find an explanation of a set of points in Euclidean space, each of whi [...] ...|$|R
3000|$|Incremental min–max {{normalization}} In the beginning, an ascending numeric array {{is created}} {{from the data}} points of X with the algorithm of Quicksort, so x_min is the first element and x_max is the last one of the ordering array. When there is a new-coming data point, the oldest data <b>point</b> of X <b>is</b> <b>deleted</b> out of the array, and then the new data <b>point</b> <b>is</b> inserted into the array. The course of the deletion and insertion must preserve the ascending order of the array, so the algorithm of Binary search is used to find the element that needs deleting and the suitable position in the array to insert the new data <b>point.</b> As Quicksort <b>is</b> carried out once when the array of new-coming data <b>points</b> <b>is</b> full {{at the beginning of}} the course of the similarity search, and since then Binary search is invoked for every new-coming data point afterward, the time complexity of incremental min–max normalization is O([...] (n)).|$|R
