12|10000|Public
40|$|Visualization is the {{foundation}} for human to understand as human think and create in a graphic world. By nature, maintenance activity is a multi discipline approach, where maintenance engineer {{should be able to}} convey their idea to people with various backgrounds. In this situation, visualization is a powerful mode; it can act as a bridge to eliminate the knowledge gap within the group. Knight (2001) developed Jack-knife diagram for visualizing total downtime and factors influencing the failure (failure frequency and mean downtime) in a point estimation. As failure data is a probabilistic in nature, it is important to look not only point estimation but also interval estimation, so the <b>precision</b> <b>and</b> <b>uncertainty</b> of estimation can be identified. This paper proposes a way of visualizing total downtime and factors influencing the failure (failure frequency and mean downtime) in interval estimation. Case study from scaling machines is used for illustration. Godkänd; 2010; 20100823 (ysko...|$|E
40|$|Road safety, {{whatever}} the considered environment, {{relies heavily on}} the ability to detect and track moving objects from a moving point of view. In order to achieve such a detection, the vehicle's ego-motion must first be estimated and compensated. This issue is crucial to complete a fully autonomous vehicle; this is why several approaches have already been proposed. This study presents a method, based solely on visual information that implements such a process. Information from stereo-vision and motion is derived to extract the vehicle's ego-motion. Ego-motion extraction algorithm is thoroughly evaluated in terms of <b>precision</b> <b>and</b> <b>uncertainty.</b> Given those statistical attributes, a method for dynamic objects detection is presented. This method relies on 3 D image registration and residual displacement field evaluation. This method is then evaluated on several real and synthetic data sequences. It will be shown that it allows a reliable and early detection, even in hard cases (e. g. occlusions, [...] .). Given a few additional factors (detectable motion range),overall performances can be derived from visual odometry performances...|$|E
40|$|Multi-dimensional look up tables (LUTs) {{are widely}} {{employed}} for color transformations {{due to its}} high accuracy and general applicability. Using the LUT model generally involves the color measurement {{of a large number}} of samples. The <b>precision</b> <b>and</b> <b>uncertainty</b> of the color measurement will be mainly represented in the LUTs, and will affect the smoothness of the color transformation. This, in turn, strongly influences the quality of the reproduced color images. To achieve high quality color image reproduction, the color transformation is required to be relatively smooth. In this study, we have investigated the inherent characteristics of LUTs ’ transformation from color measurement and their effects on the quality of reproduced images. We propose an algorithm to evaluate the smoothness of 3 D LUT based color transformations quantitatively, which is based on the analysis of 3 D LUTs transformation from RGB to CIELAB and the second derivative of the differences between adjacent points in vertical and horizontal ramps of each LUT entry. The performance of the proposed algorithm was compared with a those proposed in two recent studies on smoothness, and a better performance is reached by the proposed method...|$|E
40|$|Experimental {{aerodynamic}} researchers require estimated <b>precision</b> <b>and</b> bias <b>uncertainties</b> of measured physical quantities, typically at 95 percent confidence levels. Uncertainties {{of final}} computed aerodynamic parameters are obtained by propagation of individual measurement uncertainties through the defining functional expressions. In this paper, rigorous mathematical techniques are extended to determine <b>precision</b> <b>and</b> bias <b>uncertainties</b> of any instrument-sensor system. Through this analysis, instrument uncertainties determined through calibration are now expressed as {{functions of the}} corresponding measurement for linear and nonlinear univariate and multivariate processes. Treatment of correlated measurement precision error is developed. During laboratory calibration, calibration standard uncertainties {{are assumed to be}} an order of magnitude less than those of the instrument being calibrated. Often calibration standards do not satisfy this assumption. This paper applies rigorous statistical methods for inclusion of calibration standard <b>uncertainty</b> <b>and</b> covariance due to the order of their application. The effects of mathematical modeling error on calibration bias uncertainty are quantified. The effects of experimental design on uncertainty are analyzed. The importance of replication is emphasized, techniques for estimation of both bias <b>and</b> <b>precision</b> <b>uncertainties</b> using replication are developed. Statistical tests for stationarity of calibration parameters over time are obtained...|$|R
40|$|Replace page 4 {{with the}} {{attached}} page, which contains a corrected version of Table 1. 1. The original Table 1. 1 mistakenly contained values of water absorption () wa l taken from Pope’s PhD thesis (Pope 1993), {{rather than the}} currently accepted values from Pope and Fry (1997), which were derived by additional analyses of Pope’s measurements to reduce the uncertainties of reported values. The attached (revised) version of Table 1. 1 (Err. 1) contains () wa l taken from Pope and Fry (1997) from 400 nm to 715 nm, with the numbers of significant figures adjusted to match the stated <b>precision</b> <b>and</b> <b>uncertainties</b> of the reported values at each wavelength. In the original version of Table 1. 1, the uniform listing at 4 decimal places truncated the actual values from 400 nm to 465 nm, <b>and</b> exceeded the <b>precisions</b> <b>and</b> <b>uncertainties</b> of the reported data (3 decimal places) at wavelengths> 640 nm. On further reviewing the remaining () wa l entries at other wavelengths in the original Table 1. 1, {{it was discovered that}} the listed values of () wa l at wavelengths> 720 nm, as derived from the complex refractive indices reported by Kou et al. (1993), were simply “nearest neighbor ” values, rather than interpolations to the stated wavelengths. These values were recomputed, interpolated to 5 nm intervals using a least squares cubic polynomial fit, and entered using the number of significant figures appropriate to the values reported by Kou et al. (1993). Kou et al. (1993) reported measured refractive index values, and 1 -standard deviation relative uncertainties () s l, at more than 60 wavelengths spanning 715 nm to 800 nm. The cubic polynomial fit used to interpolate the data i...|$|R
40|$|VALIDATION <b>AND</b> <b>UNCERTAINTY</b> ESTIMATION OF A METHOD BY HIGH PERFORMANCE LIQUID CHROMATOGRAPHY FOR THE ANALYSIS OF LYCOPENE AND beta-CAROTENE IN TOMATO PULP. A {{method to}} {{quantify}} lycopene and beta-carotene in freeze dried tomato pulp by high performance liquid chromatography (HLPC) was validated {{according to the}} criteria of selectivity, sensitivity, <b>precision</b> <b>and</b> accuracy. <b>and</b> <b>uncertainty</b> estimation of measurement was determined with data obtained in the validation. The validated method presented is selective in terms of analysis, {{and it had a}} good <b>precision</b> <b>and</b> accuracy. Detection limit for lycopene and beta-carotene was 4. 2 and 0. 23 mg 100 g(- 1), respectively. The estimation of expanded uncertainty (K = 2) for lycopene was 104 +/- 21 mg 100 g(- 1) and for beta-carotene was 6. 4 +/- 1. 5 mg 100 g(- 1) ...|$|R
40|$|Abstract. By {{employing}} regression methods minimizing predictive risk, we {{are usually}} looking for precise values {{which tends to}} their true response value. However, in some situations, {{it may be more}} reasonable to predict intervals rather than precise values. In this paper, we focus to find such intervals for the K-nearest neighbors (KNN) method with precise values for inputs and output. In KNN, the prediction intervals are usually built by considering the local probability distribution of the neighborhood. In situations where we do not dispose of enough data in the neighborhood to obtain statistically significant distributions, we would rather wish to build intervals which takes into account such distribution uncertainties. For this latter we suggest to use tolerance intervals to build the maximal specific possibility distribution that bounds each population quantiles of the true distribution (with a fixed confidence level) that might have generated our sample set. Next we propose a new interval regression method based on KNN which take advantage of our possibility distribution in order to choose, for each instance, the value of K which will be a good trade-off between <b>precision</b> <b>and</b> <b>uncertainty</b> due to the limited sample size. Finally we apply our method on an aircraft trajectory prediction problem...|$|E
30|$|Firstly, {{there are}} growing {{concerns}} {{with respect to}} uncertainties and margins of error associated with tourism climate indices. These uncertainties result from the index’s definition (weighting of parameters and choice of thresholds with reference to preference surveys) and calculation methods (detailed computation methods). This may limit the usefulness of such indices in long-term projections and climate change impact assessments. Indeed, it is still uncertain whether these margins of error, cumulated with other sources in the “uncertainty cascade” (natural climate variability, different global circulation models, different climate downscaling methods and different reference socio-economic scenarios) prevent users from drawing long-term conclusions as {{to the future of}} global tourism and the redistribution of tourism flows under climate change assumptions. Consequently, the quality, <b>precision</b> <b>and</b> <b>uncertainty</b> inherent in data sources used to calculate indices must be further examined so as to guide product users and producers. For instance, results may vary depending on the use of monthly, daily or tri-hourly data. Wind series, be they observed or modelled, are far less reliable than temperature series: should they be less heavily weighted? Depending on the grids produced by data sets or chosen for model results, results may not show the geography spread of climatic effects (breeze, altitude, etc.), even though tourism depends on local conditions. This paper illustrates these issues by assessing how differences in preference surveys affect the definition of tourism climate indices thresholds, in particular for thermal comfort. We then compare computation methods for France, showing the need for more precise detailed data sources and methods ensuring the comparability of results.|$|E
40|$|Bruker™ EM 27 /SUN {{instruments}} are commercial mobile solar-viewing near-IR spectrometers. They show promise for expanding the global density of atmospheric column measurements {{of greenhouse gases}} and are being marketed for such applications. They {{have been shown to}} measure the same variations of atmospheric gases within a day as the high-resolution spectrometers of the Total Carbon Column Observing Network (TCCON). However, there is little known about the long-term <b>precision</b> <b>and</b> <b>uncertainty</b> budgets of EM 27 /SUN measurements. In this study, which includes a comparison of 186 measurement days spanning 11 months, we note that atmospheric variations of X gas within a single day are well captured by these low-resolution instruments, but over several months, the measurements drift noticeably. We present comparisons between EM 27 /SUN instruments and the TCCON using GGG as the retrieval algorithm. In addition, we perform several tests to evaluate the robustness of the performance and determine the largest sources of errors from these spectrometers. We include comparisons of X CO 2, X CH 4, X CO, and X N 2 O. Specifically we note EM 27 /SUN biases for January 2015 of 0. 03, 0. 75, – 0. 12, and 2. 43 % for X CO 2, X CH 4, X CO, and X N 2 O respectively, with 1 σ running precisions of 0. 08 and 0. 06 % for X CO 2 and X CH 4 from measurements in Pasadena. We also identify significant error caused by nonlinear sensitivity when using an extended spectral range detector used to measure CO and N 2 O...|$|E
40|$|A {{method to}} {{quantify}} lycopene and &# 946;-carotene in freeze dried tomato pulp by high performance liquid chromatography (HLPC) was validated {{according to the}} criteria of selectivity, sensitivity, <b>precision</b> <b>and</b> accuracy, <b>and</b> <b>uncertainty</b> estimation of measurement was determined with data obtained in the validation. The validated method presented is selective in terms of analysis, {{and it had a}} good <b>precision</b> <b>and</b> accuracy. Detection limit for lycopene and &# 946;-carotene was 4. 2 and 0. 23 mg 100 g- 1, respectively. The estimation of expanded uncertainty (K = 2) for lycopene was 104 ± 21 mg 100 g- 1 and for &# 946;-carotene was 6. 4 ± 1. 5 mg 100 g- 1...|$|R
5000|$|There {{are many}} ways to help prevent or {{overcome}} the logjam of analysis paralysis. There may be many factors contributing to the cause. Lon Roberts breaks down the common definition of [...] "analysis paralysis" [...] into three possibly overlapping conditions of paralysis: analysis process, decision <b>precision,</b> <b>and</b> risk <b>uncertainty.</b> He uses this to give specific actions for each condition. Becky Kane and others give these following suggestions: ...|$|R
40|$|The {{performance}} of score-level fusion algorithms is often affected by conflicting decisions {{generated by the}} constituent matchers/classifiers. This paper describes a fusion algorithm that incorporates the likelihood ratio test statistic in a support vector machine (SVM) framework in order to classify match scores originating from multiple matchers. The proposed approach also {{takes into account the}} <b>precision</b> <b>and</b> <b>uncertainties</b> of individual matchers. The resulting fusion algorithm is used to mitigate the effect of covariate factors in face recognition by combining the match scores of linear appearance-based face recognition algorithms with their non-linear counterparts. Experimental results on a heterogeneous face database of 910 subjects suggest that the proposed fusion algorithm can significantly improve the verification {{performance of}} a face recognition system. Thus, the contribution of this work is two-fold: (a) the design of a novel fusion technique that incorporates the likelihood ratio test-statistic in a SVM fusion framework; and (b) the application of the technique to face recognition in order to mitigate the effect of covariate factors. 1...|$|R
40|$|Validity and {{reliability}} as scientific quality criteria {{have to be}} considered when using optical motion capture (OMC) for research purposes. Literature and standards recommend individual laboratory setup evaluation. However, system characteristics such as trueness, <b>precision</b> <b>and</b> <b>uncertainty</b> are often not addressed in scientific reports on 3 D human movement analysis. One reason may be the lack of simple and practical methods for evaluating accuracy parameters of OMC. A protocol was developed for investigating the accuracy of an OMC system (Vicon, volume 5. 5 × 1. 2 × 2. 0 m(3)) with standard laboratory equipment and by means of trueness and uncertainty of marker distances. The study investigated the effects of number of cameras (6, 8 and 10), measurement height (foot, knee and hip) and movement condition (static and dynamic) on accuracy. Number of cameras, height and movement condition affected system accuracy significantly. For lower body assessment during level walking, the most favorable setting (10 cameras, foot region) revealed mean trueness and uncertainty to be - 0. 08 and 0. 33 mm, respectively. Dynamic accuracy cannot be predicted based on static error assessments. Dynamic procedures have to be used instead. The significant influence of the number of cameras and the measurement location suggests that instrumental errors should be evaluated in a laboratory- and task-specific manner. The use of standard laboratory equipment makes the proposed procedure widely applicable and it supports the setup process of OCM by simple functional error assessment. Careful system configuration and thorough measurement process control are needed to produce high-quality data...|$|E
40|$|Cyclamate {{is widely}} used as intense {{sweetener}} in the European Union. The absence of a maximum limit {{for the use of}} cyclamate in tabletop sweeteners and the growing demand for this type of product highlights the importance of developing robust analytical methods for the determination of its content to understand if the consumption of tabletop sweeteners can {{have a negative impact on}} human health. The present work aimed at the optimisation and validation of an high-performance liquid chromatography (HPLC) analytical method for cyclamate determination in tabletop sweeteners based on the procedure of European Standard EN 12857. The validated method was then applied to the determination of this sweetener in different types of tabletop sweeteners (liquid, powder and tablets). Both standards and samples solutions were submitted to a derivatisation procedure which converted cyclamate to N,N-dichlorocyclohexylamine. The derivatised product was separated and quantified using a reversed-phase column, a mobile phase composed of water (20 %) and methanol (80 %), isocratic flow of 1 mlmin(- 1), and detection by ultraviolet spectrophotometry at a wavelength of 314 nm. The analytical method was internally validated according to the following validation parameters: working range, linearity, limits of detection and quantification, sensitivity, precision (repeatability and intermediate <b>precision),</b> <b>and</b> <b>uncertainty.</b> This method proved to be specific and selective for the determination of this sweetener, showing repeatability, RSDr 3 %, intermediate precision, RSDR 3. 3 %, and recovery rates from 92 % to 108 % for the different tabletop sweeteners. The method uncertainty was 9. 4 %. The concentration of cyclamate in the samples varied significantly, from 2. 9 % to 73. 9 %, which demonstrated that a possible excessive consumption of one of the analysed sweeteners can lead to exceeding the acceptable daily intake for cyclamate. info:eu-repo/semantics/publishedVersio...|$|E
40|$|In {{the context}} of mining, {{drilling}} {{is the process of}} making holes in the face and walls of underground mine rooms, to prepare those rooms for the subsequent operation, which is the charging process. Due {{to the nature of the}} task, drilling incurs a high number of drilling rig failures. Through a combination of a harsh environment (characterised by dust, high humidity, etc.), the operating context, and reliability and maintainability issues, drilling rigs are identified as a major contributor to unplanned downtime. The purpose of the research performed for this thesis has been to develop methods that can be used to identify the problems affecting drilling rig downtime and to identify the economic lifetime of drilling rigs. New models have been developed for calculating the optimum replacement time of drilling rigs. These models can also be used for other machines which have repairable or replaceable components. Based on an analysis performed in a case study, a life cycle cost (LCC) optimization model has been developed, taking the most important factors affecting the economic replacement time of drilling rigs into consideration. To this end, research literature studies, case studies, and simulation studies have been performed, interviews have been held, observations have been made and data have been collected. For the data analysis, theories and methodologies within reliability, maintainability, ergonomics and optimization have been combined with the best practices from the related industries. Firstly, this thesis analyses the downtime of the studied drilling rigs, with the <b>precision</b> <b>and</b> <b>uncertainty</b> of the estimation at a given confidence level, along with the factors influencing the failures. Secondly, the thesis identifies components that significantly contribute to the downtime and the reason for that downtime (maintainability and/or reliability problems). Based on the failure analysis, some minor suggestions have been made as to how to improve the critical components of the drilling rig. Thirdly, a new method is proposed that can help decision makers to identify the replacement time of reparable equipment from an economic point of view. Finally, the thesis proposes a method using the artificial neural network (ANN) for predicting the economic lifetime of drilling rigs through a series of basic weights and response functions. This ANN-based method can be made available to engineers without the use of complicated software. Most of the results are related to specific industrial and scientific challenges, such as planning for cost-effectiveness. The results of the case study are promising for the possibility of making a significant reduction in the LCC by optimizing the lifetime. The results have been verified through interaction with experienced practitioners from both the manufacturer and the mining company using the drilling rig in question. Godkänd; 2014; Bibliografisk uppgift: Hussan Al-Chalabi received a B. Sc. Eng. degree in mechanical engineering from Mosul University, Iraq, in 1994 and an M. Sc degree in mechanical engineering in thermal power from Mosul University, Iraq, in 2008. Then he joined the Department of Mechanical Engineering at Mosul University as a lecturer. In October 2010, he joined the Division of Operation, Maintenance and Acoustics at Lule°a University of Technology as a doctoral student.; 20141009 (hasham); Nedanstående person kommer att disputera för avläggande av teknologie doktorsexamen. Namn: Hussan Al-Chalabi Ämne: Drift och underhållsteknik/Operation and Maintenance Uppsats: Reliability and Life Cycle Cost Modelling of Mining Drilling Rigs Opponent: Professor emeritus Jan-Gunnar Persson, Institutionen för maskinkonstruktion, Kungliga tekniska högskolan, Stockholm Ordförande: Professor Jan Lundberg, Avd för drift, underhåll och akustik, Institutionen för samhällsbyggnad och naturresurser, Luleå tekniska universitet Tid: Fredag den 12 december 2014 kl 10. 00 Plats: F 1031, Luleå tekniska universite...|$|E
40|$|Optical atomic clocks {{represent}} the state-of-the-art in {{the frontier of}} modern measure-ment science. In this article we provide a detailed review {{on the development of}} optical atomic clocks that are based on trapped single ions and many neutral atoms. We discuss important technical ingredients for optical clocks, and we present measurement <b>precision</b> <b>and</b> systematic <b>uncertainty</b> associated with some of the best clocks to date. We conclude with an outlook on the exciting prospect for clock applications. ar X i...|$|R
40|$|Optical {{scatterometry}} {{has been}} {{proven to be a}} useful tool for the inspection and assessment of lithographic processes. The characteristic signature of the scattergram provides information about the surface relief profile and can be rapidly and non-invasively acquired. Currently, attention is being focused on the inversion of scatterometric data to determine the surface profile of the relief structure. To overcome the highly non-linear relationship between the properties of the diffracting structure and the diffraction measurements, we present a linearized solution based on a least-squared refinement method. This approach relies on the reliability of lithographic processes, which allows us to determine the departures of structural key parameters from a set of expected design values. This linearized inversion, as shown by the mathematical formalism, is independent of the scatterometric measurement configuration. Hence it can be applied whether, for example, the incident angle or wavelength of the light is varied to acquire either irradiance of phase information. This is validated in this paper by various examples handling the retrieval of three geometrical parameters (groove depth, line width, and sidewall angle) from reflectance data simulated using the rigorous coupled-wave theory (RCWT) for both angular and wavelength scans. An additional benefit of the proposed linearized solution is the ability to examine mathematically the significance of measurement errors. An analytical propagation of error is presented, connecting measurement noise to the soughed parameter <b>precisions</b> <b>and</b> <b>uncertainties.</b> We applied this formalism to th...|$|R
40|$|This paper mathematically {{transforms}} unobservable rational expectation equilibrium model parameters (information <b>precision</b> <b>and</b> supply <b>uncertainty)</b> into {{a single}} variable that is correlated with expected returns {{and that can be}} estimated with recently observed data. Our variable can be used to explain the cross section of returns in theoretical, numerical, and empirical analyses. Using Center for Research in Security Prices data, we show that a - 1 σ to + 1 σ change in our variable is associated with a 0. 31 % difference in average returns the following month (equaling 3. 78 % per annum). The results are statistically significant at the 1 % level. Our results remain economically and statistically significant after controlling for stocks' market capitalizations, book-to-market ratios, liquidities, and the probabilities of information-based trading. © 2012 Elsevier B. V...|$|R
40|$|Summary The thesis {{developed}} and improved methods for the integrated analysis {{of different types}} of fishery independent research surveys (trawl surveys, acoustic surveys, hydrographical surveys, and gillnet surveys) to study the distribution, density, abundance, migration and biological population dynamic parameters of marine fish species. The topics in the thesis addressed different combinations of trawl, hydro-acoustic, gillnet, and hydrographical data and application of different survey data analysis methods under consideration of factors influencing the survey catch and detection efficiency. Each topic was investigated in one of more case studies. One thesis topic has been to provide more precise estimates of fish distribution and density patterns from survey data (Chapter 2). The 1 st case study applied advanced statistical methods to Baltic trawl data and hydro-acoustic survey data in combination with survey sampled hydrographical data to estimate distri­bution and density patterns of juvenile 0 -group Baltic cod. These patterns were largely unknown. In the 2 nd case study new methodology was developed for analyzing trawl research survey data for Baltic cod and whiting including the corre­la­tion in distribution and density according to space, time, size, and species. The more precise density estimates improve the knowledge of the stock-recruitment relationship of Baltic cod and can improve the Baltic multi-species stock assessment. Furthermore, it will enable more precise marine management and spatial planning involving fish stocks and fisheries in the Baltic Sea. In context of Baltic cod stock assessment, the 3 rd case study developed a new method for inter-calibration of trawl survey CPUE data by fish size group exemplified by Baltic cod (and flounder) where the concept of disturbance by one trawl haul in relation to the next have been {{developed and}} quantified when calibrating new research survey trawl gears with the former ones. These results have been based on introduction of a new international ICES BITS trawl research survey design. A second topic was to improve and develop hydroacoustic research survey methods for more precise detection and discrimination of fish species according to fish size and orientation in the water (Chapter 3). Here, the 4 th case study focused on more precise acoustic target strength estimation of juvenile cod, while the 5 th case study has focused on acoustic discrimination of juvenile gadoid fish in particular juvenile Baltic cod. This enables more efficient research survey estimation of juvenile cod (gadoid) density patterns to be used in stock recruitment estimates and stock assessment. The third topic was to estimate more precisely fish mortality, maturity, and growth parameters for small forage fish species using research survey information (Chapter 4). Associated to this, the 5 th case study analysed these population dynamic parameters using trawl survey data taking into account spatial variation. This study provided more precise estimates and deeper understanding of Norway pout mortality, maturity, and growth dynamics. The more precise population dynamic parameters have been implemented in and improved the North Sea Norway pout stock assessment, management advice, management, and long term management plan evaluations. The fourth topic was to develop methodology to integrate hydroacoustic, gillnet, and hydro­gra­phi­cal research survey data to investigate pelagic fish migration patterns (Chapter 5). The methods were applied in the 6 th case study to evaluate Western Baltic herring feeding and spawning migra­tion based on distribution and density estimates in a narrow over-wintering area of the stock. The more precise information on migration patterns gives better possibility for acoustic monitoring of the full stock abundance in different areas and seasons og the year to be used in stock assessment and marine spatial planning. Also, it increases knowledge on biological interactions and mixing with other stocks and species.   In the final synthesis Chapter 6, the thesis reviews relevant analysis methods of research survey data and underlying data distributions, survey design and stratification, trawl survey inter-calibration and standardization, as well as estimation procedures and data processing methods in context of the obtained results and methods developed in the thesis. This is done with focus on survey <b>precision</b> <b>and</b> <b>uncertainty</b> (bias, sources of errors) for trawl and acoustic surveys and factors affecting it.        ...|$|E
40|$|This {{research}} {{evaluates the}} potential of thermal infrared (TIR) remote sensing to determine and continuously monitor the horizontal water temperature distribution of inland surface water bodies. Usually, monitoring temporal and spatial variability of surface water temperature takes place by measurement networks of in-situ gauges, but these networks are often limited by sparse sampling in both time and especially space. For these and other reasons (e. g. relatively cheap, easy, and fast) the use of remote sensing in water management studies and practices has increased. By remote sensing in the TIR spectrum the TIR radiation leaving from the top surface water layer (< 0. 1 mm) is measured, {{which can be used}} to determine the radiant water temperature distribution in the horizontal plane. This horizontal radiant water temperature distribution can be used as supplement to in-situ kinetic temperature measurements. It is however necessary to first evaluate the accuracy and precision of the remotely sensed water temperatures. Therefore, the goal {{of this study is to}} determine how and with what accuracy and precision TIR radiant water temperature measurements (Tremote) can be used as an approximation for the horizontal distribution of the kinetic water temperature, based on comparisons of Tremote with in-situ kinetic water temperature measurements (Tin-situ). The criteria used in this study to determine the accuracy and precision of Tremote is by means of comparison with Tin-situ, which are usually taken at a certain depth in the water column. Tremote represents a pixel value, which is thus compared with a point measurement of Tin-situ. The bias (Tremote – Tin-situ) statistics are indicative for the obtained accuracy and precision. In this study Landsat 7 ETM+ images measured in the thermal infrared spectrum (? = 8 - 14 ?m) are used for water temperature determination of Tremote (by means of the inverse of Planck’s Law), together with Landsat 7 ETM+ images measured in the visual and near infrared spectrum for water detection of inland surface water bodies. The effects of emissivity; atmospheric absorption, emission and scattering; and surface effects and thermal stratification are evaluated and, if possible, corrected for. Uncertainty contained by the Landsat 7 ETM+ instrument has been taken into account by means of applying a 95 % confidence interval over the obtained surface water body. Reduction of thermal pollution by land-originating TIR radiation of radiant water temperatures is well established by such a 95 % confidence interval water body. The correction for atmospheric circumstances took place by means of the web-based Atmospheric Correction Parameter Calculator, ACPC (see [URL] which makes use of the MODTRAN radiative transfer model. Obtained results have been verified by means of a newly derived atmospheric correction algorithm for Landsat 7 ETM+ TIR images, developed with use of the MODIS In-Scene Split Window Method. For clear-sky images, on which this study focuses, the uncertainty contained by the atmospheric correction is up to ± 0. 8 °C inland, which can increase up to ± 1. 5 °C near the coast. Coastal uncertainty of the atmospheric correction is larger because of larger coastal atmospheric gradients (mainly of water vapour), which is difficult to correct for. The emissivity of water approaches that of a black body (? = 1), but is usually 1 to 2 % lower. This causes a reduction of up to 1 °C of the established radiant water temperature, but introduces an uncertainty of up to ± 0. 5 °C. The surface effects and thermal stratification are influenced by many factors and processes which are difficult to address. The combined result of the surface effets and thermal stratification lead to an uncertainty in winter and summer of ± 1. 6 and ± 3. 2 °C, respectively. Overall, this study concludes the best procedure to approximate the horizontal kinetic water temperature distribution of inland surface waters with Landsat 7 ETM+ TIR images makes use of a 95 % Confidence Interval Water Mask, and an Emissivity and Atmospheric Correction. The accuracy and precision levels of the horizontal water temperature distribution display an average bias of 1. 5 °C with ? = 1. 5 °C and SE = 0. 1 °C. Tremote tends to nearly always over-predict Tin-situ. Using the 95 % Confidence Water Mask to avoid thermal pollution of water pixels by land (sub-pixel heterogeneity), rivers with a width less than 120 m cannot be well resolved from the ETM+ images anymore. Furthermore, no physical relation could be derived between Tremote and Tin-situ. The numerous and complex processes that together affect the measurement of Tremote and its agreement with Tin-situ, combined with the issue of the scale difference between Tremote and Tin-situ, make it difficult to derive a (physical) relation or formula that connects Tremote to Tin-situ. The seasonal influence, expressed by a difference between winter and summer, could be captured by means of the statistical analysis. During winter, Tremote over-predicts Tin-situ on average by 0. 8 ° with a bias spread of ? = 0. 8 °C and SE = 0. 2 °C. During summer, Tremote over-predicts Tin-situ on average by 1. 8 ° with a bias spread of ? = 1. 6 °C and SE = 0. 2 °C. The bias statistics of the obtained horizontal Tremote distribution and the statistical seasonal relation between Tremote and Tin-situ can be used as an approximation for the horizontal kinetic water temperature distribution. Based on the results in this study and the difficulty to establish a more direct relation between Tremote and Tin-situ, the proposed systematic correction becomes: in winter Tremote - 0. 8 and in summer Tremote – 1. 8. The bias spread statistics (? and SE) form a first and reasonable quantification for the <b>precision</b> <b>and</b> <b>uncertainty</b> contained by the obtained approximation. It is recommended to attempt to reduce the uncertainties contained by approximations obtained in this study by further research. Research towards a better Atmospheric Correction, which especially accounts for local surface conditions and the spatial variations in atmospheric circumstances, could mean a major improvement for the remotely sensed water temperature approximations. Other research to improve the approximations of horizontal kinetic water temperature distributions by Tremote is to assess the thermal stratification and the surface effects. To better understand the thermal water body processes and the skin effect, improving our insight in the relation between kinetic water temperature in the water column, kinetic water temperature at the surface (water depth 0 m) and the TIR radiant surface water temperature measured locally would help. This improved insight would also help to reduce the uncertainty of remote sensing measurements. It is also recommended to investigate and improve the operational abilities of remotely sensed water temperatures. A main practical constraint is the time interval by which images of the same location are generated by the Landsat 7 satellite: 16 days. Therefore, to operationalize space-borne remote sensing of water temperatures for daily water management practises it is highly recommended to include more satellites with a TIR spectral channel. Another practical recommendation is to optimize the time of ETM+ image acquisition and processing. With NASA currently requiring 1 - 3 days to process the images to L 1 T process level, it must be carefully decided for what purposes the water temperature information can be used. Water ManagementCivil Engineering and Geoscience...|$|E
40|$|High {{performance}} {{liquid chromatography}} (HPLC) coupled to an ultraviolet (UV), diode array or fluorescence detector (UV/DAD/FLD) {{has been used}} to set up an analytical procedure for the quantification of 16 EU priority polyaromatic hydrocarbons (PAHs) in smoke flavourings. The following parameters have been determined for the 16 EU priority PAHs: limit of detection, limit of quantification, <b>precision</b> (repeatability <b>and</b> intermediate <b>precision),</b> recovery <b>and</b> measurement <b>uncertainty,</b> using the concept of accuracy profiles. They were in close agreement with quality criteria described in the Commission Regulation (EC) no. 627 / 2006 concerning PAHs in smoke flavourings. Peer reviewe...|$|R
40|$|The crystal {{structure}} of the title compound, C 28 H 18 O 2, was originally determined by Ehrenberg [(1967 ▶). Acta Cryst. 22, 482 – 487] using intensity data obtained from Weissenberg photographs. The current determination provides a crystal and mol­ecular structure with a significantly higher <b>precision</b> <b>and</b> presents standard <b>uncertainties</b> on geometric parameters which are not available from the original work. The mol­ecule lies on a crystallographic twofold rotation axis which bis­ects the C—C bond [1. 603  (3)  Å] which joins the two anthracen- 9 (10 H) -one units...|$|R
40|$|Optimization is {{considered}} {{to be one of the}} pillars of statistical learning and also plays a major role in the design and development of intelligent systems such as search engines, recommender systems, and speech and image recognition software. Machine Learning is the study that gives the computers the ability to learn and also the ability to think without being explicitly programmed. A computer is said to learn from an experience with respect to a specified task and its performance related to that task. The machine learning algorithms are applied to the problems to reduce efforts. Machine learning algorithms are used for manipulating the data and predict the output for the new data with high <b>precision</b> <b>and</b> low <b>uncertainty.</b> The optimization algorithms are used to make rational decisions in an environment of <b>uncertainty</b> <b>and</b> imprecision. In this paper a methodology is presented to use the efficient optimization algorithm as an alternative for the gradient descent machine learning algorithm as an optimization algorithm...|$|R
40|$|A {{methodology}} {{is proposed}} {{for the evaluation of}} uncertainty in the in-flight determination of aircraft thrust, which provides error traceability to a national standards laboratory, and is independent of the procedure used to calculate or measure thrust in flight, thereby yielding a consistent means for the evaluation of measurement capabilities. Attention is given to the factors of measurement error, precision, bias, <b>uncertainty,</b> error estimation <b>and</b> classification, error propagation, ground testing, and the related problems of model bias error, model <b>precision</b> error, <b>and</b> the <b>uncertainty</b> limit...|$|R
40|$|With the {{majority}} of the world population residing in urban areas, attempts to monitor and mitigate greenhouse gas emissions must necessarily center on cities. However, existing carbon dioxide observation networks are ill-equipped to resolve the specific intra-city emission phenomena targeted by regulation. Here we describe the design and implementation of the BErkeley Atmospheric CO 2 Observation Network (BEACO 2 N), a distributed CO 2 monitoring instrument that utilizes low-cost technology to achieve unprecedented spatial density throughout and around the city of Oakland, California. We characterize the network in terms of four performance parameters – cost, reliability, <b>precision,</b> <b>and</b> systematic <b>uncertainty</b> – <b>and</b> find the BEACO 2 N approach to be sufficiently cost-effective and reliable while nonetheless providing high-quality atmospheric observations. First results from the initial installation successfully capture hourly, daily, and seasonal CO 2 signals relevant to urban environments on spatial scales that cannot be accurately represented by atmospheric transport models alone, demonstrating the utility of high-resolution surface networks in urban greenhouse gas monitoring efforts...|$|R
40|$|International audienceThe {{round robin}} "measurement of subwavelength diffractive elements" tackles the {{metrology}} {{problems related to}} the measurement of diffraction gratings by AFM. It aims at quantifying the absolute <b>precision</b> <b>and</b> the <b>uncertainty</b> of the measurement considering some features of such structures like the depth, the period, the fill factor and {{the shape of the}} profile. This round robin involved four partners within NEMO. Each partner has measured three different samples: one 2 D small depth grating, one 1 D small depth grating and one 1 D high aspect ratio grating. In order {{to get rid of the}} samples inhomogeneity, the measurements were performed exactly at the same location on each sample by all partners. This was achieved by using a multiscale resist pattern transferred on the gratings which precisely defined a 5 x 5 µm² area. The paper will sum up the experimental values obtained by all the partners, draw general conclusions and make suggestions to improve the accuracy of AFM measurements...|$|R
40|$|The high {{spatial and}} {{temporal}} variability of aerosols make networks capable of measuring their properties in near real time of high scientific interest. In this work we present and discuss results of an aerosol optical depth algorithm {{to be used in}} the European Brewer Network, which provides data in near real time of more than 30 spectrophotometers located from Tamanrasset (Algeria) to Kangerlussuaq (Greenland). Using data from the Brewer Intercomparison Campaigns in the years 2013 and 2015, and the period in between, plus comparisons with Cimel sunphotometers and UVPFR instruments, we check the <b>precision,</b> stability, <b>and</b> <b>uncertainty</b> of the Brewer AOD in the ultraviolet range from 300 to 320  nm. Our results show a precision better than 0. 01, an uncertainty of less than 0. 05, and a stability similar to that of the ozone measurements for well-maintained instruments. We also discuss future improvements to our algorithm with respect to the input data, their processing, and the characterization of the Brewer instruments for the measurement of aerosols...|$|R
40|$|One of the {{fundamental}} aspects of any physical experiment is the uncertainty or error limits of the measurement. Unfortunately, majority of published model experimental results do not come with the much needed error analysis. It is imperative that a fundamental <b>and</b> rigorous <b>uncertainty</b> assessment is carried out for all measurements, especially for measurements using a newly designed apparatus. This paper presents the <b>uncertainty</b> analysis methodology <b>and</b> results for a newly designed fully functional podded propulsor performance measurement apparatus. The measurements include propeller thrust, torque, rotation rate and advance speed as well as global forces and moments of a pod unit. The facility and measurement systems are briefly described, <b>and</b> detailed <b>uncertainty</b> assessment methodologies with examples for each measurement are provided with descriptions of bias <b>and</b> <b>precision</b> limits <b>and</b> total <b>uncertainties.</b> The generalized methodology {{can also be used}} for other relevant measurements. Peer reviewed: YesNRC publication: Ye...|$|R
30|$|In {{order to}} be {{compliant}} with ISO 14044, section 4.2. 3.6 (ISO 2006 a, b), LCI datasets must include a data quality description of its time-related, geographic and technological representativeness {{as well as of the}} <b>precision,</b> completeness, consistency <b>and</b> <b>uncertainty</b> of the information. Under this framework, some guidelines have been developed to address the DQRs. For instance, in the ILCD Handbook (European Commission JRC and Sustainability 2010 c), data quality scores rank the datasets based on six indicators (technological representativeness, geographical representativeness, time-related representativeness, completeness, precision/uncertainty, and methodological appropriateness and consistency) with data quality scores of 1 to 5 (high to low) assigned to each.|$|R
40|$|The {{classical}} Ensemble Kalman Filter (EnKF) {{is known}} to underestimate the prediction uncertainty resulting from model overfitting and estimation error. This can potentially lead to low forecast <b>precision</b> <b>and</b> an ensemble collapsing into a single realisation. In this paper we present alternative EnKF updating schemes based on shrinkage methods known from multivariate linear regression. These methods reduce the effects caused by collinear ensemble members, and have the same computational properties as the fastest EnKF algorithms previously suggested. In addition, the importance of model selection and validation for prediction purposes is investigated, and a model selection scheme based on Cross-Validation is introduced. The classical EnKF scheme is compared with the suggested procedures on two toy examples and one synthetic reservoir case study. Significant improvements are seen, {{both in terms of}} forecast <b>precision</b> <b>and</b> prediction <b>uncertainty</b> estimates. The Ensemble Kalman Filter (EnKF) is a Bayesian data assimilation method that in recent years has become popular when considering data assimilation for nonlinear spatio-temporal models (Evensen, 2007; Aanonsen et al., 2009). The EnKF is based on the classical Kalman Filter (KF) (Kalman, 1960). Assuming a Gaussian initial prior, with a linear and Gaussian forward and likelihood model, termed the Gauss-linear model, the KF provides an analytical solution for the posterio...|$|R
40|$|There {{are many}} {{parameters}} {{which are very}} difficult to calibrate in the threshold autoregressive prediction model for nonlinear time series. The threshold value, autoregressive coefficients, and the delay time are key parameters in the threshold autoregressive prediction model. To improve prediction <b>precision</b> <b>and</b> reduce the <b>uncertainties</b> in the determination of the above parameters, a new DNA (deoxyribonucleic acid) optimization threshold autoregressive prediction model (DNAOTARPM) is proposed by combining threshold autoregressive method and DNA optimization method. The above optimal parameters are selected by minimizing objective function. Real ice condition time series at Bohai are taken to validate the new method. The prediction results indicate that the new method can choose the above optimal parameters in prediction process. Compared with improved genetic algorithm threshold autoregressive prediction model (IGATARPM) and standard genetic algorithm threshold autoregressive prediction model (SGATARPM), DNAOTARPM has higher <b>precision</b> <b>and</b> faster convergence speed for predicting nonlinear ice condition time series...|$|R
40|$|The Total Irradiance Monitor (TIM) {{is a total}} solar {{irradiance}} (TSI) instrument designed to achieve a relative standard uncertainty (1 s accuracy) of 100 parts per million (ppm) <b>and</b> a <b>precision</b> <b>and</b> long-term <b>uncertainty</b> of 10 ppm/year. This instrument {{is one of four}} on the Solar Radiation and Climate Experiment (SORCE), a NASA/EOS satellite mission scheduled for launch in Nov. 2002. The TIM will report four TSI measurements d aily throughout SORCE’s goal 5 -year mission life. Instrument Description Electrical Substitution Radiometers (ESRs) measure incident radiant power. The four ambient-temperature TIM ESRs operate in pairs, with a DSP-controlled servo system continually balancing the temperature of an active ESR to its paired reference ESR. Incident sunlight passing through an open shutter <b>and</b> a <b>precision</b> aperture radiatively heats the active ESR. The replacement electrical heater power required by this ESR to maintain thermal balance as the shutter is cycled determines the incident {{solar irradiance}}. The relative standard uncertainty of this primary instrument is 100 ppm (1 s) with 2 ppm noise...|$|R
40|$|Laboratories working towards {{accreditation}} by the International Standards Organization (ISO) 15189 standard {{are required}} {{to demonstrate the validity}} of their analytical methods. The different guidelines set by various accreditation organizations make it difficult to pro-vide objective evidence that an in-house method is fit for the intended purpose. Besides, the required performance characteristics tests and acceptance criteria are not always de-tailed. The laboratory must choose the most suitable validation protocol and set the ac-ceptance criteria. Therefore, we propose a validation protocol to evaluate the performance of an in-house method. As an example, we validated the process for the detection and quantification of lead in whole blood by electrothermal absorption spectrometry. The fun-damental parameters tested were, selectivity, calibration model, <b>precision,</b> accuracy (<b>and</b> <b>uncertainty</b> of measurement), contamination, stability of the sample, reference interval, and analytical interference. We have developed a protocol that has been applied success-fully to quantify lead in whole blood by electrothermal atomic absorption spectrometry (ETAAS). In particular, our method is selective, linear, accurate, and precise, making it suitable for use in routine diagnostics...|$|R
40|$|This paper {{describes}} {{a method to}} determine the uncertainties of measured forces and moments from multi-component force balances used in wind tunnel tests. A multivariate regression technique is first employed to estimate the uncertainties of the six balance sensitivities and 156 interaction coefficients derived from established balance calibration procedures. These uncertainties are then employed to calculate the uncertainties of force-moment values computed from observed balance output readings obtained during tests. Confidence and prediction intervals are obtained for each computed force and moment as functions of the actual measurands. Techniques are discussed for separate estimation of balance bias <b>and</b> <b>precision</b> <b>uncertainties...</b>|$|R
40|$|This paper {{presents}} the calibration results <b>and</b> <b>uncertainty</b> {{analysis of a}} high-precision reference pressure measurement system currently used in wind tunnels at the NASA Langley Research Center (LaRC). Sensors, calibration standards, and measurement instruments are subject to errors due to aging, drift with time, environment effects, transportation, the mathematical model, the calibration experimental design, and other factors. Errors occur at every link {{in the chain of}} measurements and data reduction from the sensor to the final computed results. At each link of the chain, bias <b>and</b> <b>precision</b> <b>uncertainties</b> must be separately estimated for facility use, and are combined to produce overall calibration and prediction confidence intervals for the instrument, typically at a 95 % confidence level. The <b>uncertainty</b> analysis <b>and</b> calibration experimental designs used herein, based on techniques developed at LaRC, employ replicated experimental designs for efficiency, separate estimation of bias a [...] ...|$|R
5|$|Another Avogadro constant-based approach, {{known as}} the International Avogadro Coordination's Avogadro project, would define and {{delineate}} the kilogram as a 93.6mm diameter sphere of silicon atoms. Silicon was chosen because a commercial infrastructure with mature processes for creating defect-free, ultra-pure monocrystalline silicon already exists to service the semiconductor industry. To make a practical realization of the kilogram, a silicon boule (a rod-like, single-crystal ingot) would be produced. Its isotopic composition would be measured with a mass spectrometer to determine its average relative atomic mass. The boule would be cut, ground, and polished into spheres. The size of a select sphere would be measured using optical interferometry to an uncertainty of about 0.3nm on the radius—roughly a single atomic layer. The precise lattice spacing between the atoms in its crystal structure (≈192pm) would be measured using a scanning X-ray interferometer. This permits its atomic spacing to be determined with an uncertainty of only three parts per billion. With {{the size of the}} sphere, its average atomic mass, and its atomic spacing known, the required sphere diameter can be calculated with sufficient <b>precision</b> <b>and</b> low <b>uncertainty</b> to enable it to be finish-polished to a target mass of one kilogram.|$|R
