727|590|Public
25|$|In April 1997, Martha Palmer and Marc Light {{organized}} a workshop entitled Tagging with Lexical Semantics: Why, What, and How? {{in conjunction with}} the Conference on Applied Natural Language Processing. At the time, there was a clear recognition that manually annotated corpora had revolutionized other areas of NLP, such as <b>part-of-speech</b> <b>tagging</b> and parsing, and that corpus-driven approaches had the potential to revolutionize automatic semantic analysis as well. Kilgarriff recalled that there was “a high degree of consensus that the ﬁeld needed evaluation,” and several practical proposals by Resnik and Yarowsky kicked off a discussion that {{led to the creation of}} the Senseval evaluation exercises.|$|E
5000|$|RDRPOSTagger toolkit: Single Classification Ripple Down Rules for <b>Part-of-Speech</b> <b>tagging</b> ...|$|E
50|$|A lookup {{approach}} may use preliminary <b>part-of-speech</b> <b>tagging</b> to avoid overstemming.|$|E
5000|$|A {{manually}} verified <b>part-of-speech</b> <b>tagged</b> Quranic Arabic corpus.|$|R
30|$|<b>Part-of-speech</b> (POS) <b>tagging</b> is an {{important}} natural language processing (NLP) task, used as a preprocessing step in many applications. Its objective is to tag each token in a sentence with the corresponding <b>part-of-speech</b> <b>tag.</b>|$|R
40|$|AbstractRecurring {{sequences}} of words {{have long been}} considered as a signifier of different genres and registers by corpus linguists. The previous research mainly focused on lexical n-grams. Nevertheless, n-grams of other linguistic features, such as part-of-speech, have been less studied. The current study is expected to examine whether n-grams of <b>part-of-speech</b> <b>tags</b> extracted from a large corpus can be a discriminator of different genres. The results show that a strong correlation exists between the information about n-grams of <b>part-of-speech</b> <b>tags</b> and the genre of the text...|$|R
50|$|In Natural Language Processing, CLAWS is {{a program}} that {{performs}} <b>Part-of-speech</b> <b>tagging.</b>|$|E
5000|$|Sliding window based <b>part-of-speech</b> <b>tagging</b> {{is used to}} part-of-speech tag a text.|$|E
5000|$|The Sorani Kurdish Corpus: A One-Million-Word Corpus, with <b>part-of-speech</b> <b>tagging</b> (POS tagging), to be prepared.|$|E
3000|$|... {{belong to}} one of the <b>part-of-speech</b> <b>tags</b> (adjectives, adverbs, nouns, and verbs). For the hyperlinks part, all terms are used to build TDF. The matrix entries n(d [...]...|$|R
40|$|This paper {{presents}} {{our latest}} investigations on different features for factored language models for Code-Switching speech and {{their effect on}} automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be extracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as words, <b>part-of-speech</b> <b>tags,</b> Brown word clusters, open class words and clusters of open class word embeddings are explored. The experimental results reveal that Brown word clusters, <b>part-of-speech</b> <b>tags</b> and open-class words are the most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model containing Brown word clusters and <b>part-of-speech</b> <b>tags</b> and the model also including clusters of open class word embeddings yield the best mixed error rate results. In summary, the best language model can significantly reduce the perplexity on the SEAME evaluation set by up to 10. 8 % relative and the mixed error rate by up to 3. 4 % relative. Comment: IEEE/ACM Transactions on Audio, Speech, and Language Processing (Volume: 23, Issue: 3, March 2015...|$|R
50|$|A {{large corpus}} of <b>Part-Of-Speech</b> <b>tagged</b> {{sentences}} and an initial ontology with predeﬁned categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories.|$|R
5000|$|The Tajik Language Corpus: A One-Million-Word Corpus, with <b>part-of-speech</b> <b>tagging</b> (POS tagging), to be prepared; ...|$|E
50|$|A {{more recent}} {{development}} {{is using the}} structure regularization method for <b>part-of-speech</b> <b>tagging,</b> achieving 97.36% on the standard benchmark dataset.|$|E
50|$|In Computational Linguistics, he {{is known}} for {{pioneering}} the use of dynamic programming methods for <b>part-of-speech</b> <b>tagging</b> (DeRose 1988, 1990).|$|E
40|$|This toolkit {{comprises}} {{the tools and}} supporting scripts for unsupervised induction of dependency trees from raw texts or texts with already assigned <b>part-of-speech</b> <b>tags.</b> There are also scripts for simple machine translation based on unsupervised parsing and scripts for minimally supervised parsing into Universal-Dependencies style...|$|R
3000|$|... [...]. This biased term {{frequency}} {{is used in}} (5) for keyword selection. In {{the navigation}} experiments presented in the next section, proper names are detected based on the <b>part-of-speech</b> <b>tags</b> and a dictionary, where nouns with no definition in the dictionary are considered as proper names.|$|R
40|$|This paper {{describes}} {{a variety of}} methods for inserting phrase boundaries in text. The methods work by examining the likelihood of a phrase break occurring in a sequence of three <b>part-of-speech</b> <b>tags.</b> The paper explains this basic technique and desribes more sophisticaed variations using distance probabilities. 1...|$|R
5000|$|The Persian Word Bank (...) : A One-Hundred-Million-Word Corpus, with <b>part-of-speech</b> <b>tagging</b> (POS tagging), and A Frequency Dictionary of Persian, under preparation; ...|$|E
5000|$|Annotation {{consists}} of {{the application of a}} scheme to texts. Annotations may include structural markup, <b>part-of-speech</b> <b>tagging,</b> parsing, and numerous other representations.|$|E
5000|$|<b>Part-of-speech</b> <b>tagging.</b> Li et al. (2012) built {{multilingual}} POS-taggers {{for eight}} resource-poor languages {{on the basis}} of English Wiktionary and Hidden Markov Models.|$|E
40|$|Abstract — Reordering is {{important}} {{problem to be}} considered when translating between language pairs with different word orders. Myanmar is a verb final language and reordering is needed when it is translated into other languages which are different from Myanmar word order. In this paper, automatic reordering rule generation for Myanmar-English machine machine translation is presented. In order to generate reordering rules; Myanmar-English parallel tagged aligned corpus is firstly created. Then reordering rules are generated automatically by using the linguistic information from this parallel tagged aligned corpus. In this paper, function <b>tag</b> and <b>part-of-speech</b> <b>tag</b> reordering rule extraction algorithms are proposed to generate reordering rules automatically. These algorithms {{can be used for}} other language pairs which need reordering because these rules generation is only depend on <b>part-of-speech</b> <b>tags</b> and function tags...|$|R
40|$|We {{present a}} {{strictly}} lexical parsing model {{where all the}} parameters {{are based on the}} words. This model does not rely on <b>part-of-speech</b> <b>tags</b> or grammatical categories. It maximizes the conditional probability of the parse tree given the sentence. This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13. 6 % higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on <b>part-of-speech</b> <b>tags.</b> ...|$|R
40|$|This paper {{investigates the}} use of {{clustering}} techniques in word-sense classification, which identifies different contexts that a word was used with the same or similar sense. For simplicity, we have used the hierarchical clustering techniques: single- and complete-linkage, and we showed that {{the latter is a}} more suitable technique from our performance measurements (i. e. recall and precision) compared with manually grouping different contexts of similar meaning. We found that {{the use of}} <b>part-of-speech</b> <b>tags</b> and fixed-length context has better clustering performance than without <b>part-of-speech</b> <b>tags</b> and sentence context, respectively. The differences between manually identified groups of different contexts are measured in terms of recall and precision at about 80 %, which are not very different from the average recall and precision performance of complete-linkage clustering at 80 % and 75 %, respectively. 1...|$|R
50|$|In recent years, {{perceptron}} {{training has}} become {{popular in the}} field of natural language processing for such tasks as <b>part-of-speech</b> <b>tagging</b> and syntactic parsing (Collins, 2002).|$|E
5000|$|Typically, {{the process}} starts by extracting terms and {{concepts}} or noun phrases from plain text using linguistic processors such as <b>part-of-speech</b> <b>tagging</b> and phrase chunking. Then statistical or symbolic ...|$|E
5000|$|Weighted FSTs found {{applications}} in natural language processing, including machine translation, and in machine learning. An implementation for <b>part-of-speech</b> <b>tagging</b> {{can be found}} as one component of the [...] library.|$|E
50|$|The initial Brown Corpus {{had only}} the words themselves, plus a {{location}} identifier for each. Over the following several years <b>part-of-speech</b> <b>tags</b> were applied. The Greene and Rubin tagging program (see under part of speech tagging) helped considerably in this, but the high error rate meant that extensive manual proofreading was required.|$|R
40|$|Text {{classification}} is {{an important}} task in the legal domain. In fact, most of the legal information is stored as text in a quite unstructured format {{and it is important}} to be able to automatically classify these texts into a predefined set of concepts. Support Vector Machines (SVM), a machine learning al- gorithm, has shown to be a good classifier for text bases [Joachims, 2002]. In this paper, SVMs are applied to the classification of European Portuguese legal texts – the Por- tuguese Attorney General’s Office Decisions – and the rele- vance of linguistic information in this domain, namely lem- matisation and <b>part-of-speech</b> <b>tags,</b> is evaluated. The obtained results show that some linguistic information (namely, lemmatisation and the <b>part-of-speech</b> <b>tags)</b> can be successfully used to improve the classification results and, simultaneously, to decrease the number of features needed by the learning algorithm...|$|R
40|$|This corpus is a <b>part-of-speech</b> <b>tagged</b> {{version of}} Wallman, Jeff, Rowinski, Zach, Ngawang Trinley, Tomlinson, Chris, & Keutzer, Kurt. (2017). Collection of Tibetan etexts {{compiled}} by the Buddhist Digital Resource Center [Data set]. Zenodo. [URL] using the training data of Hill, Nathan W., & Garrett, Edward. (2017). A <b>part-of-speech</b> (POS) <b>tagged</b> corpus of Classical Tibetan [Data set]. Zenodo. [URL] Please note that the files are not post-processed or manually corrected and that {{a small number of}} files in the KarmaDelek directory were still annotated, although the original xml-input was corrupted already. using the memory based tagger of [URL]...|$|R
50|$|Hidden Markov {{models are}} {{especially}} {{known for their}} application in reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, <b>part-of-speech</b> <b>tagging,</b> musical score following, partial discharges and bioinformatics.|$|E
5000|$|MontyTagger: <b>Part-of-speech</b> <b>tagging</b> {{using the}} Penn Treebank tagset, enriched with [...] "Common Sense" [...] from the Open Mind Common Sense project. Exceeds {{accuracy}} of Brill94 tbl tagger using default training files ...|$|E
5000|$|The Brill tagger is an {{inductive}} {{method for}} <b>part-of-speech</b> <b>tagging.</b> It was described and invented by Eric Brill in his 1993 PhD thesis. It {{can be summarized}} as an [...] "error-driven transformation-based tagger". It is: ...|$|E
30|$|Open IE systems {{consider}} that every phrase between {{a pair of}} entities can denote a relation. This vision addresses the coverage limitation seen in traditional IE. However, it introduces {{a substantial amount of}} noise. In that way, open extractors improve precision by restricting relations to specific <b>part-of-speech</b> <b>tag</b> sequences that are intended to express true relations [1].|$|R
40|$|We {{present an}} {{interactive}} interface {{to explore the}} properties of intralingual and interlingual association measures. In conjunction, they can be employed for phraseme identification in word-aligned parallel corpora. The customizable component we built to visualize individual results is capable of showing <b>part-of-speech</b> <b>tags,</b> syntactic dependency relations and word alignments next to the tokens of two corresponding sentences...|$|R
40|$|This paper {{explores the}} {{relationship}} between various measures of unsupervised <b>part-of-speech</b> <b>tag</b> induction {{and the performance of}} both supervised and unsupervised parsing models trained on induced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. ...|$|R
