1|88|Public
40|$|We {{investigate}} {{the average number}} of moves made by Quick Select (a variant of Quick Sort for finding order statistics) to find an element with a randomly selected rank. This kind of grand average provides smoothing over all individual cases of a specific fixed order statistic. The variance of the number of moves involves intricate dependencies, and we only give reasonably tight bounds. 1 Quick Select. Quick Sort is a well known fast algorithm for data sorting. It was invented by Hoare (see [3]); see also [5] and [7]. Quick Sort is the default sorting scheme in some operating systems, such as UNIX. The algorithm is two-sided: It puts a pivot element in its correct position, and arranges the data in two groups relative to that pivot. Elements below the pivot go in one group, the rest are placed in the other group. The two groups are then handled recursively. The one-sided version (Quick Select) of the algorithm is popular for identifying order statistics. This one-sided version of Quick Sort to search for order statistics is also known as Hoare’s Find algorithm, which was first given in [2]. To find a certain order statistic, such as the first quartile, Quick Select proceeds with the <b>partition</b> <b>stage,</b> just as in Quick Sort, to place a pivot in its correct position, and creates the two groups on {{the two sides of the}} pivot. But then, the algorithm decides if the pivot is the sought order statistic or not. If it is, the algorithm terminates (announcing the pivot to be the sought order statistic), and if not it recursively pursues only a group on one side where the order statistic resides...|$|E
3000|$|... {{indicate}} {{zero and}} one probabilities of the received signal at time k and <b>stage</b> st. The <b>partitioning</b> <b>stage</b> {{is equal to}} [...]...|$|R
5000|$|In {{the image}} <b>partitioning</b> <b>stage,</b> the input picture (on RGB color space) {{is divided into}} 64 blocks to {{guarantee}} the invariance to resolution or scale. The inputs and outputs of this step are summarized in the following table: ...|$|R
5000|$|Partitioning : The <b>partitioning</b> <b>stage</b> of {{a design}} is {{intended}} to expose opportunities for parallel execution. Hence, {{the focus is on}} defining a large number of small tasks in order to yield what is termed a fine-grained decomposition of a problem.|$|R
50|$|From {{the early}} years of the reign of Empress Catherine the Great (1762 - 1796), Russia {{intensified}} its manipulation of Polish affairs. Prussia and Austria, the other powers surrounding the Republic, also took advantage of internal religious and political bickering. The neighboring states divided up the country in three <b>partition</b> <b>stages.</b> The third one in 1795 wiped Poland-Lithuania from the map of Europe.|$|R
40|$|The {{recently}} {{proposed and}} novel steel heat treatment {{process known as}} quenching and partitioning (Q&P) is studied, in particular, the <b>partitioning</b> <b>stage.</b> Characterization of the partially quenched microstructure prior to the <b>partitioning</b> <b>stage</b> is enabled by using a base composition containing a higher Mn concentration than for conventional Q&P steels, such that quenching to room temperature produces only partial decomposition of austenite to martensite. Increasing the carbon concentration of this base composition also enabled {{a study of the}} effect of steel carbon content, although at the higher carbon contents refrigeration was required to achieve marten site because of the reduced Ms temperatures. Increased carbon also allowed the effect of martensite morphology to be examined. Intercritical annealing of a conventional TRIP steel composition provided an alternative route towards increasing the carbon concentration of austenite prior to partitioning. Light optical microscopy, scanning electron microscopy, X-ray and neutron diffraction were used to characterise the evolution of microstructure. In particular, in-situ examination of the Q&P heat-treatment process, enabled by introducing a furnace into the neutron beam-line, gave real -time observations and analyses of partitioning. Lattice parameter measurements enabled calculation of the carbon content of retained austenite, providing evidence of carbon partitioning from martensite to untransformed austenite during the <b>partitioning</b> <b>stage.</b> Thus, the <b>partitioning</b> process was shown to be effective in thermal and mechanical stabilization of retained austenite. In addition, evidence was found for substantial carbon enrichment of the austenite phase, which might be expected to provide opportunity for new Q&P steel grades with enhanced propertiesEThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|International audienceIn this paper, {{we present}} an {{algorithm}} for partitioning any given 2 d domain into regions suitable for quadrilateral meshing. It {{can deal with}} multi-domain geometries with ease, {{and is able to}} preserve the symmetry of the domain. Moreover, this method keeps the number of singularities at the junctions of the regions to a minimum. Each part of the domain, being four-sided, can then be meshed using a structured method. The <b>partitioning</b> <b>stage</b> is achieved by solving a PDE constrained problem based on the geometric properties of the domain boundaries...|$|R
40|$|In this paper, we {{analyze the}} dual pivot Quicksort, {{a variant of}} the {{standard}} Quicksort algorithm, in which two pivots are used for the partitioning of the array. We are solving recurrences of the expected number of key comparisons and exchanges performed by the algorithm, obtaining the exact and asymptotic total average values contributing to its time complexity. Further, we compute the average number of <b>partitioning</b> <b>stages</b> and the variance of the number of key comparisons. In terms of mean values, dual pivot Quicksort {{does not appear to be}} faster than ordinary algorithm...|$|R
40|$|Summary. In this paper, {{we present}} an {{algorithm}} for partitioning any given 2 d domainintoregions suitableforquadrilateral meshing. Itcandealwithmulti-domain geometries with ease, {{and is able}} to preserve the symmetry of the domain. Moreover, this method keeps the number of singularities at the junctions of the regions to a minimum. Each part of the domain, being four-sided, can then be meshed using a structured method. The <b>partitioning</b> <b>stage</b> is achieved by solving a PDE constrained problem based on the geometric properties of the domain boundaries. Key words: meshgeneration; finiteelementmethods;partial differential equations, elliptic; quadrilateral; partitioning; multi-domain...|$|R
40|$|Abstract. Sleep is the {{important}} phenomenon in human’s life. About third time of human is spent on sleeping in which many important physical course happen and develop so that the research of sleep EEG gets more and more regards. The beginning of sleep research often starts from right <b>partition</b> by <b>stages.</b> Different sleep stages correspond with different brain states so that the sleep <b>partition</b> by <b>stages</b> has important meaning for the research of the sleep EEG. In this paper, a method is given for sleep segmentation using Lempel-Ziv complexity. From {{the result of the}} simulation, it can be drawn that the complexity measure can helpful distinguish the sleep stages so it plays an active role to find a reliable guideline for the automatic <b>partition</b> of sleep <b>stages</b> in sleeping periods in time...|$|R
30|$|One of the {{challenging}} {{problems of the}} HW/SW codesign is to perform an efficient HW/SW partitioning of the computational tasks. The goal of the <b>partitioning</b> <b>stage</b> is to find which computational tasks can be implemented in an efficient parallelized HW/SW architecture seeking for balanced area-time trade-offs between different admissible design solutions [18 – 20]. In this study, the iterative fixed-point POCS-DEDR regularized algorithm has been partitioned at the algorithmic level to minimize the overall signal processing (SP) time via transferring some required reconstructive SP functions from the SW to the HW. The {{solution to this problem}} requires, first, the definition of a partitioning model that meets all the specification requirements (functionality, goals and constraints).|$|R
40|$|AbstractMultiple Quickselect is an {{algorithm}} {{that uses}} the idea of Quicksort to search for several order statistics simultaneously. In order to improve the efficiency of Quicksort, one can use the median of 2 t+ 1 randomly chosen elements as pivot element in the <b>partitioning</b> <b>stage.</b> Such a median of 2 t+ 1 partition can also be applied to Multiple Quickselect {{to reduce the number}} of comparisons. Here we give an analysis of such Multiple Quickselect variants that use median of 2 t+ 1 partition and describe for these algorithms the asymptotic behaviour of the expected number of required comparisons to find p-order statistics in a data set of size n for n→∞ and fixed p...|$|R
40|$|Abstract. Computer aided hardware/software {{partitioning}} {{is one of}} the key {{challenges in}} hardware/software co-design. This paper describes a new approach to hardware/software partitioning for synchronous communication model. We transform the partitioning into a reachability problem of timed automata. By means of an optimal reachability algorithm, an optimal solution can be obtained in terms of limited resources in hardware. To relax the initial condition of the partitioning for optimization, two algorithms are designed to explore the dependency relations among processes in the sequential specification. Moreover, we propose a scheduling algorithm to improve the synchronous communication efficiency further after <b>partitioning</b> <b>stage.</b> Some experiments are conducted with model checker UPPAAL to show our approach is both effective and efficient...|$|R
40|$|In this paper, we analyse {{the dual}} pivot Quicksort, {{a variant of}} the {{standard}} Quicksort algorithm, in which two pivots are used for the partitioning of the array. We are solving recurrences of the expected number of key comparisons and exchanges performed by the algorithm, obtaining the exact and asymptotic total average values contributing to its time complexity. Further, we compute the average number of <b>partitioning</b> <b>stages</b> and the variance of the number of key comparisons. In terms of mean values, dual pivot Quicksort {{does not appear to be}} faster than ordinary algorithm. Comment: Post print of the article "Dual pivot Quicksort" published on 1 August of 2012 in the journal of Discrete Mathematics, Algorithms and Application...|$|R
50|$|Throughout the Soviet {{period the}} main theater hall {{was set up}} a single ballroom space. <b>Partitions</b> {{separating}} <b>stage,</b> orchestra pit and spectator areas were re-introduced during the controversial repairs of the 2000s, so the theater can once again be used in its original function.|$|R
50|$|After {{the image}} <b>partitioning</b> <b>stage,</b> a single {{representative}} color is selected from each block. Any method {{to select the}} representative color can be applied, but the standard recommends {{the use of the}} average of the pixel colors in a block as the corresponding representative color, since it is simpler and the description accuracy is sufficient in general. The selection results in a tiny image icon of size 8x8. The next figure shows this process. Note that {{in the image of the}} figure, the resolution of the original image has been maintained only in order to facilitate its representation.The inputs and outputs of this stage are summarized in the next table:Once the tiny image icon is obtained, the color space conversion between RGB and YCbCr is applied.|$|R
40|$|AbstractWith {{the growing}} {{interest}} toward Internet-based graphic applications, {{the design of}} a scalable mesh compression scheme has become a key issue. Using the multi-scale transformation theory introduced by Lounsbery et al. (1997) along with the parameterization techniques of Eck et al. (1995) provides an elegant theoretical framework for producing compact multi-scale representations of surfaces. However, this approach fails to provide good compression and geometric faithfulness in all cases. To solve this problem, we propose a three-step method enabling efficient scalable compression of arbitrary mesh with faithful representations at any level of detail: a <b>partitioning</b> <b>stage</b> along with a triangulation enable the production of a base mesh which preserves the geometry of the model. Then an adaptive parameterization is constructed over this base mesh...|$|R
5000|$|Scene 2 (Capriccio, Corale e Ciacona II): Marie has {{received}} a reproachful letter from Stolzius. She is reading it in tears when Desportes enters. He scornfully dictates to her a brusque reply. His flattery finally has the desired effect: his spot with Marie is won. In the room next door, Wesener's aged mother sings the folk song Rösel aus Hennegay with its prophetic line, Some day your cross will come to you. On a <b>partitioned</b> <b>stage</b> appear, on one side, Marie and Desportes as a couple engrossed in love play, and on the other, Stolzius and his mother, {{who is trying to}} convince her son that having broken off his engagement, the [...] "soldier's whore" [...] Marie was not worthy of him. But Stolzius defends her and swears revenge on Desportes.|$|R
40|$|The aim of {{this paper}} is to present an {{automatic}} generator of zonal models that makes it possible to free the user from the choice of the specific flows models that have to be implemented in the zonal model and from the <b>partitioning</b> <b>stage.</b> The dynamic simulation tool called “O-Zone ” is based on an advanced formulation of zonal models. It uses on a new way of sub-dividing the room that allows us to obtain a dynamic partitioning based on airflow patterns. The partitioning method chosen is the Octree method. It is a hierarchical representation of the space based on the successive and recursive subdivision of a cube in eight smaller cubes. NOMENCLATURE Ts Inlet air temperature [°C) ] qm_s Supply air mass flow [kg/s] dmij Mass flow rate (zone i to zone j) [kg/s] Cd Empirical coefficient [-...|$|R
40|$|Next-generation {{sequencing}} (NGS) platforms {{have presented}} unique {{challenges to the}} computing community. The large number of short reads characteristic of NGS data has increased the difficulty of assembling genomes without use of a reference sequence, a method known as de novo sequence assembly. Further complicating {{the problem is the}} recent interest in metagenomics, the sequencing of multi-genetic material from environmental samples. Specialized data structures, such as de Bruijn graphs and bloom filters, have been incorporated as the backbone of modern assembly software. But as the rapid growth in metagenomic data illustrates, the development of new data structures and algorithms must continue to keep pace. The goal of this project is to analyze and optimize the performance of these assembly algorithms, focusing specifically on the pre-processing and graph <b>partitioning</b> <b>stages.</b> Both memory usage and run-time optimizations are considered, and a range of computing platforms is targeted...|$|R
40|$|In {{reconfigurable}} systems, reconfiguration latency is a {{very important}} factor impact the system performance. In this paper, a framework is proposed that integrates the temporal partitioning and physical design phases to perform a static compilation process for reconfigurable computing systems. A temporal partitioning algorithm is proposed which attempts to decrease the time of reconfiguration on a partially reconfigurable hardware. This algorithm attempts to find similar single or pair of operations between subsequent partitions. Considering similar pairs instead of single nodes brings about less complexity for routing process. By using this technique, smaller reconfiguration bit-stream is obtained, which directly decreases the reconfiguration overhead time at the run-time. A complementary algorithm attempts to increase the similarity of subsequent partitions by searching for similar pairs and using a technique called dummy node insertion. An incremental physical design process based on similar configurations produced in the <b>partitioning</b> <b>stage</b> improves the metrics over iterations. 1...|$|R
40|$|This paper {{presents}} a technique for integrated partitioning and scheduling of hardware-software systems. The tool takes a task graph and area constraint as input and obtains a mapping and schedule {{such that the}} execution time is minimized. The algorithm differs from other approaches which either obtain the mapping during the <b>partitioning</b> <b>stage</b> [1] [2] or the scheduling stage [5]. We use an iterative approach where the partitioner assigns the mapping of {{only some of the}} tasks and the remaining tasks are assigned by the scheduler with an objective of minimizing the execution time. The technique takes both the time and area overheads due to inter-processor and intra-processor communication into account. The effectiveness of the approach is demonstrated by the experimental results. 1. Introduction Embedded systems typically have heterogeneous architectures. They consist of both off the shelf microprocessors and custom hardware units. Hardware-software (HW-SW) codesign process takes an ap [...] ...|$|R
40|$|We are {{interested}} in the problem of providing environments to parallel programmers which provide a good abstraction from the underlying machine architecture and yet still allow efficient execution on architectures with nonuniform memory locality. We do not suggest there might be a ready-made solution to this problem, or that any one solution will be applicable for all applications. For {{the purpose of this paper}} we are merely interested in outlining the tradeoffs, and explaining how they are dealt with in a number of existing parallel programming environments. Framework We shall use the terminology of Sarkar [Sarkar 1989] and others to describe the various phases involved in compiling a program for parallel execution. The first of these, detection of parallelism, involves deriving a maximally parallel description of the program in terms of a directed acyclic graph of tasks, satisfying precedence relations by communication on termination. In the second (<b>partitioning)</b> <b>stage,</b> task [...] ...|$|R
40|$|Abstract. It is {{well known}} that the {{performance}} of quicksort can be improved by selecting the median of a sample of elements as the pivot of each <b>partitioning</b> <b>stage.</b> For large samples the partitions are better, but the amount of additional comparisons and exchanges to find the median of the sample also increases. We show in this paper that the optimal sample size to minimize the average total cost of quicksort, {{as a function of the}} size n of the current subarray size, is a · √ n + o (√ n). We give a closed expression for a, which depends on the selection algorithm and the costs of elementary comparisons and exchanges. Moreover, we show that selecting the medians of the samples as pivots is not the best strategy when exchanges are much more expensive than comparisons. We also apply the same ideas and techniques to the analysis of quickselect and get similar results...|$|R
40|$|Understanding carbon {{redistribution}} in steels {{is crucial}} in developing advanced high strength steels. For instance, Quenching & Partitioning (Q&P) processes rely on the partitioning of carbon from martensite into austenite, where {{at the end of}} the heat treatment the carbon-enriched austenite shows higher stability at room temperature. Recent literature gives increasing evidence of carbide precipitation occurring during partitioning despite the addition of silicon, conventionally thought to suppress carbide precipitation. The aim of the present study is to gain insight into carbon-competing processes by applying a series of Q&P heat treatments, with particular focus on the <b>partitioning</b> <b>stage,</b> where the role of silicon in the stability of austenite is evaluated. Various characterisation techniques are combined in order to unveil the microstructural changes. While carbide precipitation does appear to occur in the presence of silicon, it is found that silicon plays an active role in the stabilisation of the austenite during the partitioning reaction. </p...|$|R
40|$|It is {{well known}} that the {{performance}} of quicksort can be substantially improved by selecting the median of a sample of three elements as the pivot of each <b>partitioning</b> <b>stage.</b> This variant is easily generalized to samples of size s = 2 k + 1. For large samples the partitions are better as the median of the sample makes a more accurate estimate of the median of the array to be sorted, but the amount of additional comparisons and exchanges to find the median of the sample also increases. We show that the optimal sample size to minimize the average total cost of quicksort (which includes both comparisons and exchanges) is s = a Δ p n + o(p n). We also give a closed expression for the constant factor a, which depends on the median-finding algorithm and the costs of elementary comparisons and exchanges. The result above holds in most situations, unless the cost of an exchange exceeds by far the cost of a comparison. In that particular case, it is better to select not the median of [...] ...|$|R
5000|$|A typical yearly meeting session has a {{presiding}} clerk, {{one or two}} recording {{clerks and}} a reading clerk on <b>stage.</b> <b>Partitioning</b> the work load with extra clerks lowers the stress level on the presiding clerk.|$|R
40|$|A {{process was}} {{developed}} for separating promethium from raixed fisBion product rare earths by continuous multistage conntercurrent extraction with 100 % tri-nbutylphosphste from nitric acid of 12 N or higher concentration. Distribution coefficients at 12 N acidity for aecdamium. promethium. and samarium are 0. 43. 0. 82, and 1. 55, respectively. Single-stage separation factors of 1. 9 between successive elements can be maintained throughout the system to give separations dependent only {{on the number of}} stages. Extracted values can be recovered from the organic solution by stripping with a smaller volume of dilute nitric acid. A flowsheet for purification of promethium includes one cycle for separation of promethium from neodymum and lighter elements and a secondycle for removal of samarium and heavier elements. Each cycle consists of a series of countercurrent <b>partitioning</b> <b>stages.</b> followed by stripping stages and an evaporator. With 20 stages in the first cycle and 34 stages in the second, a 90 % yield of promethium with a purity of 83 % can be obtained from a typical mixture of fission product rare earths, assuming essentially perfect mechanical efficiency. An increase to 34 stages in the first cycle would permit a 93 % yield of 99 % promethium. (auth...|$|R
40|$|Large {{multipliers}} {{are important}} for cryptographic applications because they need large keys. The ability to modify key lengths, for security reasons, suggests adaptability in multiplication bit-length. However, re configurability of multiplication is a difficult task, especially when bit-lengths are large, say over 500 bits. For fixed bit-lengths, much {{work has been done}} in the range of 32, 64 or even 128 bits for advanced microprocessors and DSPs. The objective of this work is to design large adaptable bit-length multipliers that can be employed in cryptographic systems. We present a multiplication scheme for higher radix multiplexer-based array multipliers and we suggest a parallelization of the scheme within a single FPGA based implementation. We also suggest a novel partition of the multiplier into folded pipeline stages such that each stage can be instantiated by reconfiguration from its preceding stage during the multiplication operation. The number of <b>partition</b> <b>stages</b> is flexible to meet the FPGA resource constraints. The rationale for pipeline folding is that the multiplier size may preclude a monolithic implementation within one FPGA chip. Using additional FPGAs reduces performance due to interchip communication. Results of large reconfigurable multipliers for 256 -bits and over implemented in Xilinx Virtex 4 are provided. © 2006 IEEE...|$|R
40|$|The {{occurrence}} of antibiotics {{in the environment}} has recently raised serious concerns regarding their potential threat to human health and aquatic ecosystem! A new magnetic nanocomposite, Fe 3 O 4 @C (Fe 3 O 4 coated with carbon), was synthesized, characterized, and then applied to remove five commonly-used sulfonamides (SAs) from water. Due to its combinational merits of the outer functionalized carbon shell and the inner magnetite core, Fe 3 O 4 @C exhibited a high adsorption affinity for selected SAs and a fast magnetic separability. The adsorption kinetics of SAs on Fe 3 O 4 @C could be expressed by the pseudo second-order model. The adsorption isotherms were fitted well with the Dual-mode model, revealing that the adsorption process consisted of an initial <b>partitioning</b> <b>stage</b> and a subsequent hole-filling stage. Solution pH exerted a strong impact on the adsorption process with the maximum removal efficiencies (74 % to 96 %) obtained at pH 4. 8 for all selected SAs. Electrostatic force and hydrogen bonding were two major driving forces for adsorption, and electron-donor-acceptor interactions may also make a certain contribution. Because the synthesized Fe 3 O 4 @C showed comprehensive advantages of high adsorptivity, fast magnetic separability, and prominent reusability, it has potential applications in water treatment...|$|R
40|$|Abstract—In {{this paper}} {{multistage}} trellis-coded vector quantization (MS-TCVQ) is {{developed as a}} constrained trellis sourcecoding technique. The performance of the two-stage TCVQ is studied for Gaussian sources. Issues of stage-by-stage design, output alphabet selection, and complexity are addressed with emphasis on selecting and <b>partitioning</b> the <b>stage</b> codebooks. For a given rate, MS-TCVQ achieves low encoding and storage complexity compared to TCVQ, and comparisons with samedimensional multistage vector quantization indicate a 0. 5 – 3 -dB improvement in signal-to-quantization-noise ratio. Index Terms — Data compression, residual quantizers, trelliscoded vector quantization (TCVQ), vector quantization...|$|R
40|$|This paper studies a {{repeated}} play {{of a family}} of games by resource-constrained players. To economize on reasoning resources, the family of games is partitioned into subsets of games which players do not distinguish. An example is constructed to show that when games are played a finite number of times, partitioning of the game set according to a coarse exogenously given partition might introduce new symmetric equilibrium payoffs which Pareto dominate best equilibrium outcomes with distinguished games. Moreover, these new equilibrium payoffs are also immune to evolutionary pressure at the <b>partition</b> selection <b>stage...</b>|$|R
40|$|We {{present in}} this paper a new interconnect-driven {{multilevel}} floorplanner, called interconnect-driven multilevelfloorplanning framework (IMF), to handle large-scale buildingmodule designs. Unlike the traditional multilevel framework that adopts the “Λ-shaped ” framework (inaccurately called the “V-cycle ” framework in the literature) : bottom-up coarsening followed by top-down uncoarsening, the IMF, in contrast, works in the “V-shaped” manner: top-down uncoarsening (partitioning) followed by bottom-up coarsening (merging). The top-down <b>partitioning</b> <b>stage</b> iteratively <b>partitions</b> the floorplan region based on min-cut bipartitioning with exact net-weight modeling {{to reduce the number}} of global interconnections and, thus, the total wirelength. Then, the bottom-up merging stage iteratively applies fixed-outline floorplanning using simulated annealing for all regions and merges two neighboring regions recursively. Experimental results show that the IMF obtains the best published fixed-outline floorplanning results with the smallest average wirelength for the Microelectronics Center of North Carolina/ Gigascale Systems Research Center benchmarks. In particular, IMF scales very well as the circuit size increases. The V-shaped multilevel framework outperforms the Λ-shaped one in the optimization of global circuit effects, such as interconnection and crosstalk optimization, since the V-shaped framework considers the global configuration first and then processes down to local ones level by level, and thus, the global effects can be handled at earlier stages. The V-shaped multilevel framework is general and, thus, can be readily applied to other problems...|$|R
40|$|In this paper, we {{describe}} the threading of H. 264 decoder on the multithreaded SandBlaster DSP. We present an efficient way of <b>partitioning</b> computation intensive <b>stages</b> such as macroblock decoding, inter and intra prediction and deblocking filter across threads. We also describe features to multithread an application using the Sandblaster tools chain...|$|R
40|$|Since the NULL Convention Logic (NCL) {{paradigm}} is delay-insensitive, NCL combinational circuits cannot be partitioned indiscriminately when pipelining, as can clocked circuits. Instead, these circuits must be <b>partitioned</b> into <b>stages,</b> such that each stage is inputcomplete {{with respect to}} all of its inputs and delayinsensitivity is maintained. Therefore the selected architecture for an NCL circuit may vary depending {{on whether or not}} the given circuit is to be pipelined. For example, the combinational circuit that has the shortest critical path may not necessarily result in optimal throughput when pipelined. This paper presents a method called Threshold Combinational Reduction for Pipelining (TCRP), t...|$|R
40|$|Quick sort is a sorting {{algorithm}} whose {{worst case}} running time is &# 952;(n 2) on an input array of n numbers. It {{is the best}} practical for sorting because it {{has the advantage of}} sorting in place. Problem statement: Behavior of quick sort is complex, we proposed in-place 2 m threads parallel heap sort algorithm which had advantage in sorting in place and had better performance than classical sequential quick sort in running time. Approach: The algorithm consisted of several stages, in first stage; it splits input data into two <b>partitions,</b> next <b>stages</b> it did the same <b>partitioning</b> for prior <b>stage</b> which had been spitted until 2 m partitions was reached equal to the number of available processors, finally it used heap sort to sort respectively ordered of non internally sorted partitions in parallel. Results: Results showed the speed of algorithm about double speed of classical Quick sort for a large input size. The number of comparisons needed was reduced significantly. Conclusion: In this study we had been proposed a sorting algorithm that uses less number of comparisons with respect to original quick sort that in turn requires less running time to sort the same input data...|$|R
