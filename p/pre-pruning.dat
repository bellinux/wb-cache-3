31|5|Public
2500|$|Pruning {{generally}} {{takes place}} once the leaves {{have fallen in}} November, and continues throughout the winter months. [...] Mechanized <b>pre-pruning</b> is carried out first, using a high-clearance tractor, and this cuts the time spent pruning manually by about a quarter. [...] The following pruning methods are permitted: ...|$|E
40|$|The Prism {{family of}} {{algorithms}} induces modular classification rules {{in contrast to}} the Top Down Induction of Decision Trees (TDIDT) approach which induces classification rules in the intermediate form of a tree structure. Both approaches achieve a comparable classification accuracy. However in some cases Prism outperforms TDIDT. For both approaches <b>pre-pruning</b> facilities have been developed in order to prevent the induced classifiers from overfitting on noisy datasets, by cutting rule terms or whole rules or by truncating decision trees according to certain metrics. There have been many <b>pre-pruning</b> mechanisms developed for the TDIDT approach, but for the Prism family the only existing <b>pre-pruning</b> facility is J-pruning. J-pruning not only works on Prism algorithms but also on TDIDT. Although {{it has been shown that}} J-pruning produces good results, this work points out that J-pruning does not use its full potential. The original J-pruning facility is examined and the use of a new <b>pre-pruning</b> facility, called Jmax-pruning, is proposed and evaluated empirically. A possible <b>pre-pruning</b> facility for TDIDT based on Jmax-pruning is also discussed...|$|E
40|$|<b>Pre-pruning</b> and Post-pruning are two {{standard}} {{techniques for}} handling noise in decision tree learning. <b>Pre-pruning</b> deals with noise during learning, while post-pruning addresses this problem after an overfitting {{theory has been}} learned. We first review several adaptations of pre- and post-pruning techniques for separate-and-conquer rule learning algorithms and discuss some fundamental problems. The primary goal {{of this paper is}} to show how to solve these problems with two new algorithms that combine and integrate pre- and post-pruning...|$|E
40|$|In this paper, {{we propose}} a 2 D based {{partition}} method for {{solving the problem}} of Ranking under Team Context(RTC) on datasets without a priori. We first map the data into 2 D space using its minimum and maximum value among all dimensions. Then we construct window queries with consideration of current team context. Besides, during the query mapping procedure, we can <b>pre-prune</b> some tuples which are not top ranked ones. This pre-classified step will defer processing those tuples and can save cost while providing solutions for the problem. Experiments show that our algorithm performs well especially on large datasets with correctness...|$|R
30|$|In this section, we will {{elaborate}} a three-phase extraction framework, {{which is}} comprised of modeling and preprocessing web pages to discover extraction characteristics held in an extensible characteristics container, identifying the extraction area based on the above characteristics, and refining the extraction decisions by semantics similarity. Before extraction, since HTML tags contribute greatly for web page structure except for making information visual, all web pages will be modeled into DOM trees for easy utilization of structural characteristics, and some redundant parts are also <b>pre-pruned</b> for simplifying later computation. Three kinds of characteristic, including visual punctuation mark characteristics, Web page structure characteristic and content semantics similarity, are defined and refined, which are then applied for the following extraction decisions.|$|R
40|$|In this paper, we ask if XML {{access control}} can be {{supported}} when underlying (XML or relational) storage {{system does not}} provide adequate security features, and propose three alternative solutions – primitive, pre-processing, and post-processing. Towards that scenario, in particular, we advocate a scalable and effective pre-processing approach, called QFilter. QFilter is based on Non-deterministic Finite Automata (NFA), and rewrites user’s queries such that parts violating access control rules are <b>pre-pruned.</b> Through analysis and experimental validation, we show that (1) QFilter guarantees that only permissible portion of data is returned to the authorized users, (2) such access controls can be efficiently enforced without relying on security features of underlying storage system, and (3) such independency makes QFilter capable of many emerging applications, such as in-network access control and access control outsourcing...|$|R
40|$|<b>Pre-Pruning</b> and Post-Pruning are two {{standard}} {{methods of}} dealing with noise in concept learning. <b>Pre-Pruning</b> methods are very efficient, while Post-Pruning methods typically are more accurate, but much slower, {{because they have to}} generate an overly specific concept description first. We have experimented with a variety of pruning methods, including two new methods that try to combine and integrate pre- and postpruning in order to achieve both accuracy and efficiency. This is verified with test series in a chess position classification task...|$|E
40|$|Abstract. This paper explores two {{simple and}} {{efficient}} <b>pre-pruning</b> {{strategies for the}} cost-sensitive decision tree algorithm to avoid overfitting. One is to limit the cost-sensitive decision trees {{to a depth of}} two. The other is to prune the trees with a pre-specified threshold. Empirical study shows that, compared to the error-based tree algorithm C 4. 5 and several other cost-sensitive tree algorithms, the new cost-sensitive decision trees with <b>pre-pruning</b> are more efficient and perform well on most UCI data sets. ...|$|E
40|$|<b>Pre-Pruning</b> and Post-Pruning are two {{standard}} {{methods of}} dealing with noise in decision tree learning. <b>Pre-Pruning</b> methods deal with noise during learning, while post-pruning methods try {{to address this problem}} after an overfitting theory has been learned. This paper shows how pre- and post-pruning algorithms can be used for separate-and-conquer rule learning algorithms. We discuss some fundamental problems and show how to solve them with two new algorithms that combine and integrate pre- and post-pruning. Keywords: Pruning, Noise Handling, Inductive Rule Learning, Inductive Logic Programmin...|$|E
40|$|Automatic {{traffic sign}} {{detection}} is challenging due {{to the complexity of}} scene images, and fast detection is required in real applications such as driver assistance systems. In this paper, we propose a fast {{traffic sign detection}} method based on a cascade method with saliency test and neighboring scale awareness. In the cascade method, feature maps of several channels are extracted efficiently using approximation techniques. Sliding windows are pruned hierarchically using coarse-to-fine classifiers and the correlation between neighboring scales. The cascade system has only one free parameter, while the multiple thresholds are selected by a data-driven approach. To further increase speed, we also use a novel saliency test based on mid-level features to <b>pre-prune</b> background windows. Experiments on two public traffic sign data sets show that the proposed method achieves competing performance and runs 27 times as fast as most of the state-of-the-art methods...|$|R
40|$|Post-harvest {{physiological}} {{deterioration of}} cassava roots {{is a serious}} limiting factor affecting storage over even short periods of time. The resulting black pigments render the roots inedible to both humans and animals. A {{better understanding of the}} causes of this problem and methods of control were the principal aims of the current investigation. Using a simple evaluation procedure the variation within and between cultivars as regards the susceptibility to deterioration was found to be considerable. Pruning plants prior to harvest was confirmed to induce a resistance to deterioration in susceptible cultivars as well as lead to some reduction in root starch content. This was found to continue for considerable periods after pruning (> 9 weeks). Ecosystem studies show that there is a large environmental component in the variation in deterioration susceptibility within one cultivar and that physiological stresses in general (climatic, biotic or edaphic) can lead to a reduction in susceptibility. Further data on harvests in Popayan support this. Biochemical studies provided evidence for the existance of a phenol (scopoletin) which fluorescences blue in UV light, appears in fresh tissue a few hours after harvest and rapidly accumulates. Applications of scopoletin to fresh tissue induce blue-black vessel pigmentation within 6 hours. Tissues from <b>pre-pruned</b> (resistant) plants do not accumulate scopoletin to the same extent but respond to the exogenous compound. An enzyme inhibitor presents scopoletin accumulation suggesting that its appearance is an enzymic process. Further investigation into a combination of cultural practices and appropriate storage techniques with regard to root quality should enable the goal of successful cassava root storage to be achieved...|$|R
30|$|It {{may not be}} {{computationally}} {{feasible to}} evaluate all proposed response plans, and a heuristic must be employed to prune the evaluation space. As discussed throughout this article, RFIA and ROIA targeted different impact dimensions whose performance criteria are contradictorily defined. We show throughout this article that their combination is highly beneficial and required. However, if both assessment perform a <b>pre-pruning</b> by their own standards, {{it is likely that}} disjoint subsets are evaluated. Therefore, a <b>pre-pruning</b> step must be based on a heuristic optimized against characteristics of RFIA and ROIA simultaneously, which is subject to future work.|$|E
40|$|<b>Pre-Pruning</b> and Post-Pruning are two {{standard}} {{methods of}} dealing with noise in concept learning. <b>Pre-Pruning</b> methods are very efficient, while Post-Pruning methods typically are more accurate, but much slower, {{because they have to}} generate an overly specific concept description first. We have experimented with a variety of pruning methods, including two new methods that try to combine and integrate pre- and postpruning in order to achieve both accuracy and efficiency. This is verified with test series in a chess position classification task. 1 Introduction Inductive Logic Programming (ILP) or Relational Learning has established itself as one of the major research areas in the field of Machine Learning [Muggleton, 1992, Lavrac and Dzeroski, 1993]. The ability to define concepts from data distributed in separate relational tables makes ILP methods particularly appropriate for learning from relational databases (see e. g. [Dzeroski and Lavrac, 1993]). However, data from real-world problem [...] ...|$|E
40|$|Post pruning of {{decision}} trees {{has been a}} successful approach in many real-world experiments, but over all possible concepts it does not bring any inherent improvement to an algorithm's performance. This work explores how a PAC-proven decision tree learning algorithm fares in comparison with two variants of the normal top-down induction {{of decision}} trees. The algorithm does not prune its hypothesis per se, {{but it can be}} understood to do <b>pre-pruning</b> of the evolving tree. We study a backtracking search algorithm, called Rank, for learning rank-minimal decision trees. Our experiments follow closely those performed by Schaffer [20]. They confirm the main findings of Schaffer: in learning concepts with simple description pruning works, for concepts with a complex description and when all concepts are equally likely pruning is injurious, rather than beneficial, to the average performance of the greedy topdown induction of decision trees. <b>Pre-pruning,</b> as a gentler technique, settles in the [...] ...|$|E
40|$|The Prism {{family of}} {{algorithms}} induces modular classification rules which, {{in contrast to}} decision tree induction algorithms, do not necessarily fit together into a decision tree structure. Classifiers induced by Prism algorithms achieve a comparable accuracy compared with decision trees {{and in some cases}} even outperform decision trees. Both kinds of algorithms tend to overfit on large and noisy datasets and this has {{led to the development of}} pruning methods. Pruning methods use various metrics to truncate decision trees or to eliminate whole rules or single rule terms from a Prism rule set. For decision trees many <b>pre-pruning</b> and postpruning methods exist, however for Prism algorithms only one <b>pre-pruning</b> method has been developed, J-pruning. Recent work with Prism algorithms examined J-pruning in the context of very large datasets and found that the current method does not use its full potential. This paper revisits the J-pruning method for the Prism family of algorithms and develops a new pruning method Jmax-pruning, discusses it in theoretical terms and evaluates it empirically...|$|E
40|$|Generating {{classification}} rules {{from data}} {{often leads to}} large sets of rules {{that need to be}} pruned. A new <b>pre-pruning</b> technique for rule induction is presented which applies instance reduction before rule induction. Training three rule classifiers on datasets that have been reduced earlier with instance reduction methods leads to a statistically significant lower number of generated rules, without adversely affecting the predictive performance. The search strategies used by the three algorithms vary in terms of both type (depth-first or beam search) and direction (general-to-specific or specific-to-general) ...|$|E
40|$|Conventional {{algorithms}} {{for decision}} tree induction use an attribute-value representation scheme for instances. This paper explores the empirical consequences of using set-valued attributes. This simple representational extension {{is shown to}} yield significant gains in speed and accuracy. To do so, the paper also describes an intuitive and practical version of <b>pre-pruning.</b> This method is shown to yield considerably better accuracy results when used as a pre-processor for numeric data. It is also shown to improve the accuracy for the second best classification option, which has valuable ramifications for post-processing...|$|E
40|$|The {{aim of this}} {{research}} was to verify the possibility of limiting yield and delaying grape sugar accumulation with postponement of finishing winter pruning after bud break in vines mechanically pre-pruned in February. In 2014 and 2015, in a spur-pruned Sangiovese vineyard, the finishing winter pruning treatments were carried out at BBCH 0 (buds still dormant), BBCH 15 and BBCH 55 (when the apical shoots on the canes retained on vines with mechanical <b>pre-pruning</b> treatment were 10 and 20 cm long, respectively). Contrary to BBCH 55 treatment, where the yield was drastically reduced (- 49...|$|E
40|$|In best-first {{top-down}} induction {{of decision}} trees, the best split is added in each step (e. g. the split that maximally reduces the Gini index). This {{is in contrast}} to the standard depth-first traversal of a tree. The resulting tree will be the same, just how it is built is different. The objective of this project is to investigate whether it is possible to determine an appropriate tree size on practical datasets by combining best-first decision tree growth with cross-validation-based selection of the number of expansions that are performed. <b>Pre-pruning,</b> post-pruning, CART-pruning can be performed this way to compare...|$|E
40|$|Root-pruning of Pinus radiata DDon {{seedlings}} {{resulted in}} {{a sharp increase in}} stomatal resistance and a concurrent drop in net photosynthesis. Eight days after root-pruning leaf water potential was restored to <b>pre-pruning</b> levels and within 12 days photosynthesis showed signs of recovery, accompanied by a decrease in stomatal resistance. Proliferation of new roots took place and thereafter the recovery process intensified. By day 32 photosynthesis was restored to about 60 % of the initial rate prior to root-pruning. Translocation of 14 C assimilate was restricted by root-pruning. An hypothesis concerning the physiological processes involved in root-pruning and hardening of nursery stock is discussed...|$|E
40|$|Abstract- A new one to {{many and}} {{many to many}} data linkage {{is based on a}} One-Class Clustering Tree (OCCT) which characterizes the {{entities}} that should be linked together. It is evaluated using datasets of Data leakage prevention, Recommender system and Fraud detection. The tree is built such that it is easy to understand and transform into Association rules. The Data Linkage is closely related to entity resolution problem and goal is to identify non-identical records and merge them into single representative record. Non-matching entities in certain domains can tend to fraudulent access. Knuth Morris pratt algorithm is used for fast pattern matching in strings. <b>Pre-Pruning</b> and Post-pruning are made in decision tree that reduce the time complexity of algorithm by reducing the size of tree...|$|E
40|$|The {{purpose of}} these trials was to {{evaluate}} possible effects on properties of grapes, particularly the physical and mechanical features, depending on the winter pruning system. The following pruning techniques were carried out: manual pruning (m); mechanical pruning (M); mechanical <b>pre-pruning</b> and subsequent manual finishing (Mm); mechanical <b>pre-pruning</b> and contemporary fast manual finishing, using a wagon facility with two operators equipped with pneumatic scissors (Mw). The trials were carried out on Sangiovese trained to spurred cordon. During the trials were measured: time and cost of pruning, quality of pruning and the vegetative-productive response of vines. During grape harvesting a consolidated analytical method of texture analysis was applied to evaluate the physical parameters of grapevine cultivar: pedicel detachment, skin perforation, skin thickness, grape features as hardness, cohesiveness, springiness. Analysis of working time showed that the manual pruning (m) determined a greater commitment of time, while the mechanized pruning (M) presented a time reduction of 95 %. The two mechanized pruning associated with manual finishing reduced the time of 21 % (Mm) and 69 % (Mw). The lowering cost is less evident but important anyway. Regarding the quality of pruning, {{the increase in the}} level of mechanization has produced an increase of spurs and buds density. It was also detected a higher percentage of damaged spurs and in wrong position. The increasing of mechanization levels of pruning also has produced smaller and sparser bunches with smaller berries. The study of mechanical properties of berries showed significant differences in the mechanical behaviours of the different pruning tests. The mechanized pruning presented higher values for the pedicel detachment, skin perforation and cohesiveness, while it gave lower values for thickness of skin and springiness. The results showed that mechanical pruning can modify properties of the berries which influence mechanical harvesting on vineyard...|$|E
40|$|Conventional {{algorithms}} {{for decision}} tree induction use an attribute-value representation scheme for instances. This paper explores the empirical consequences of using set-valued attributes. This simple representational extension, when {{used as a}} pre-processor for numeric data, is shown to yield significant gains in accuracy combined with attractive build times. It is also shown to improve the accuracy for the second best classification option, which has valuable ramifications for post-processing. To do so an intuitive and practical version of <b>pre-pruning</b> is employed. Moreover, {{the implementation of a}} simple pruning scheme serves as an example of pruning applicability over the resulted trees and also as an indication that the proposed discretization absorbs much of pruning potential. Finally, we construct several versions of the basic algorithm to examine the value of every component that comprises it. Keywords Decision trees, discretization, set-values, post-processing. ...|$|E
40|$|Decision tree pruning {{is useful}} in {{improving}} the generalization performance of decision trees. As opposed to explicit pruning in which nodes are removed from fully constructed decision trees, implicit pruning uses a stopping criteria to label a node as a leaf node when splitting it further would not result in acceptable improvement in performance. The stopping criteria is often also called the <b>pre-pruning</b> criteria and is typically based on the pattern instances available at node (i. e. local information). We propose a new criteria for prepruning based on a novel classifiability measure. The proposed criteria not only considers the number of pattern instances of different classes at a node (node purity) but also the spatial distribution of these instances to estimate the effect of further splitting the node. The algorithm and some experimental results are presented in this paper...|$|E
40|$|In {{this paper}} we will shortly review several pruning methods for {{relational}} learning algorithms and show {{how they are}} related to each other. We then report some experiments in several natural domains and try to analyse {{the performance of the}} algorithms in these domains in terms of runtime and accuracy. While some algorithms are clearly faster than others, no safe recommendation for achieving high accuracy can be given. 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on <b>pre-pruning</b> (Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994 b]) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow [Cohen, 1993]). More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994 c]) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, [...] ...|$|E
30|$|Except for {{presentation}} functions, HTML tags {{also provide}} valuable structural information for web pages, {{which are very}} helpful to improve extraction performance. The nested structure of HTML tags in web pages can be interpreted into a tree structure, which is named DOM model. We use Jsoup parser [30] to interpret web pages into DOM trees. DOM tree is the logical model of web pages, which allows web pages to be processed easily in memory. DOM model transforms each pair of HTML tags into a subtree, such as 〈DIV〉 and 〈 /DIV〉. A traversal operation can be exerted on DOM tree to access all branch and leaf nodes, and then output 〈path, content〉 pairs. In addition, we also use regular expressions to remove those unrelated tags and parts from web pages, such as subtrees covered by 〈script〉 and 〈 /script〉, which are not relevant to the required content. The <b>pre-pruning</b> will simplify the web pages, which is demonstrated in Fig.  2 b, c and d.|$|E
40|$|Most {{techniques}} for attribute selection in decision trees are biased towards attributes with many values, and several ad hoc solutions {{to this problem}} {{have appeared in the}} machine learning literature. Statistical tests for the existence of an association with a prespecified significance level provide a wellfounded basis for addressing the problem. However, many statistical tests are computed from a chi-squared distribution, which is only a valid approximation to the actual distribution in the large-sample case [...] -and this patently does not hold near the leaves of a decision tree. An exception is the class of permutation tests. We describe how permutation tests can be applied to this problem. We choose one such test for further exploration, and give a novel two-stage method for applying it to select attributes in a decision tree. Results on practical datasets compare favorably with other methods that also adopt a <b>pre-pruning</b> strategy. 1 Introduction Statistical tests provide a set of th [...] ...|$|E
40|$|Pruning is an {{effective}} method for dealing with noise in Machine Learning. Recently pruning algorithms, in particular Reduced Error Pruning, have also attracted interest {{in the field of}} Inductive Logic Programming. However, {{it has been shown that}} these methods can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. We introduce a new method which searches for good theories in a top-down fashion to get a better starting point for the pruning algorithm. Experiments show that this approach can significantly lower the complexity of the task as well as increase predictive accuracy. OEFAI-TR- 94 - 03 1 Introduction Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e. g. [Mingers, 1989] or [Esposito et al., 1993]). <b>Pre-pruning</b> [...] - heuristically deciding when to stop growing clauses and concep [...] ...|$|E
40|$|In a {{world where}} data is {{captured}} {{on a large scale}} the major challenge for data mining algorithms {{is to be able to}} scale up to large datasets. There are two main approaches to inducing classification rules, one is the divide and conquer approach, also known as the top down induction of decision trees; the other approach is called the separate and conquer approach. A considerable amount of work has been done on scaling up the divide and conquer approach. However, very little work has been conducted on scaling up the separate and conquer approach. In this work we describe a parallel framework that allows the parallelisation of a certain family of separate and conquer algorithms, the Prism family. Parallelisation helps the Prism family of algorithms to harvest additional computer resources in a network of computers in order to make the induction of classification rules scale better on large datasets. Our framework also incorporates a <b>pre-pruning</b> facility for parallel Prism algorithms...|$|E
40|$|This paper {{outlines}} {{some problems}} {{that may occur}} with Reduced Error Pruning in rule learning algorithms. In particular we show that pruning complete theories is incompatible with the separate-and-conquer learning strategy that is commonly used in propositional and relational rule learning systems. As a solution we propose to integrate pruning into learning and examine two algorithms, one that prunes at the clause level and one that prunes at the literal level. Experiments show that these methods are not only much more efficient, but also able to achieve small gains in accuracy by solving the outlined problem. Keywords: Rule Learning, Inductive Logic Programming, Pruning, Noise OEFAI-TR- 95 - 03 An Extended Abstract of this paper appeared in the Proceedings of the ECML- 95. 1 Introduction Most rule learning algorithms deal with noise in the data during learning, i. e. they employ <b>pre-pruning.</b> In relational learning systems such as Foil (Quinlan and Cameron-Jones 1993), mFoil (Dzeroski a [...] ...|$|E
40|$|In this paper, {{we present}} a {{comparison}} of five different approaches to extracting decision trees from diagnostic Bayesian nets, including an approach based on the dependency structure of the network itself. With this approach, attributes used in branching the decision tree are selected by a weighted information gain metric computed based upon an associated D-matrix. Using these trees, tests are recommended for setting evidence within the diagnostic Bayesian nets {{for use in a}} PHM application. We hypothesized that this approach would yield effective decision trees and test selection and greatly reduce the amount of evidence required for obtaining accurate classification with the associated Bayesian networks. The approach is compared against three alternatives to creating decision trees from probabilistic networks such as ID 3 using a dataset forward sampled from the network, KL-divergence, and maximum expected utility. In addition, the effects of using χ 2 statistics and probability measures for <b>pre-pruning</b> are examined. The results of our comparison indicate that our approach provides compact decision trees that lead to high accuracy classification with the Bayesian networks when compared to trees of similar size generated by the other methods, thus supporting our hypothesis. 1...|$|E
40|$|This paper {{provides}} {{an analysis of}} the behavior of separate-and-conquer or covering rule learning algorithms by visualizing their evaluation metrics and their dynamics in coverage space, a variant of ROC space. Our results show that most commonly used metrics, including accuracy, weighted relative accuracy, entropy, and Gini index, are equivalent to one of two fundamental prototypes: precision, which tries to optimize the area under the ROC curve for unknown costs, and a cost-weighted difference between covered positive and negative examples, which tries to find the optimal point under known or assumed costs. We also show that a straightforward generalization of the m-estimate trades off these two prototypes. Furthermore, our results show that stopping and filtering criteria like CN 2 's significance test focus on identifying significant deviations from random classification, which does not necessarily avoid overfitting. We also identify a problem with Foil's MDL-based encoding length restriction, which proves to be largely equivalent to a variable threshold on the recall of the rule. In general, we interpret these results as evidence that, contrary to common conception, <b>pre-pruning</b> heuristics are not very well understood and deserve more investigation...|$|E
40|$|In (Godin et al., 1995 a) we {{proposed}} an incremental conceptual clustering algorithm, derived from lattice theory (Godin et al., 1995 b), which is fast to compute (Mineau & Godin, 1995). This algorithm is especially useful {{when dealing with}} large data or knowledge bases, making classification structures available to large size applications like those found in industrial settings. However, {{in order to be}} applicable on large data sets, the analysis component of the algorithm had to be simplified: the thorough comparison of objects normally needed to fully justify the formation of classes had to be cut down. Of course, from less analysis results classes which carry less semantics, or which should not have been formed in the first place. Consequently, some classes are useless in terms of the information needs of the applications that will later on interact with the data. Pruning techniques are thus needed to eliminate these classes and simplify the classification structure. However, since these classification structures are huge, the pruning techniques themselves must be simple {{so that they can be}} applied in reasonable time on large classification structures. This paper presents three such techniques: one is based on the definition of constraints over the generalization language, the other two are based on discrimination metrics applied on links between classes or on the classes themselves. Because the first technique is applied before the classification structure is built, it is called a <b>pre-pruning</b> technique, while the other two are called postpruning techniques...|$|E
40|$|Since {{their first}} release in the 1970 s, {{relational}} databases have been routinely {{used to collect}} and organize real-world data [...] -from financial transactions, marketing surveys, to health informatics observations. Traditional data mining methods expect data {{in the form of}} a single table, thus resulting in an inability to deal with such relational repositories. Multirelational Data Mining, on the other hand, aims to discover useful patterns across multiple inter-connected relations in a relational database. To this end, this work focuses on how to build classification models for relational databases through multiple views (feature sets). This study developed four multiple view strategies for mining multirelational data. The thesis firstly introduces the Multi-View Relational Classification (MRC) framework, for constructing hypotheses from sets of attributes of the presented data. The MRC strategy distinguishes itself from existing multirelational mining algorithms by excluding the need to either transform multiple relations into a universal single table or to devise new techniques for direct relational learning. The MRC algorithm offers both predictive performance and efficiency gains over current relational models, when mining diverse relational databases. Secondly, the MRC-IM method extends the MRC approach in order to deal with skew-class multirelational data. Here, the number of examples from one class is much higher than the others and correctly classifying the underrepresented examples is of prime importance. The MRC-IM method offers performance gains over a current relational model not only against majority class instances, but also against underrepresented examples. While the MRC and MRC-IM methods construct an individual view using features within a sole relation, the third multi-view strategy formulated by this work, namely the MRC-Cross approach, enables the search and collection of relevant attributes across multiple relations when constructing individual views. Finally, we present the SESP technique for <b>pre-pruning</b> uninteresting relations of complex relational databases. Through identifying uninteresting views from the MRC framework, our SESP method creates a pruned structure, while minimizing predictive performance loss on the final classification model. The results of this study thus suggest that learning from multiple views sets a new direction for efficiently mining data in many relational forms, including relational databases, graphs and social networks...|$|E
40|$|This thesis {{presents}} {{results from}} two separate studies. First, {{the impact of}} bearer length on yield components within the canopy was investigated in season 2005 / 06, on a commercially-managed, mechanically-pruned vineyard of Vitis vinifera L. Cabernet Sauvignon in Coonawarra, South Australia. Pruning resulted in the retention of bearers with 1 - 7 nodes, with the weighted average bearer length being two nodes for the canopy. As bearers of one to five nodes in length were the most common, these were studied. Yield components (on a per shoot basis) were analysed according to the node position on the bearer at which the shoot arose. Both budburst and inflorescence number per node were highest at the distal node positions on each length bearer, even if the nodes were at the same positions {{from the base of}} the bearer and would normally be expected to have similar fertility. Budburst appeared to act by modifying inflorescence number per node based on the relative location of each node from the apex of the bearer. Shoots that arose from the most distal node positions had the highest flower number per inflorescence and berry number per bunch. Flower number per inflorescence was significantly higher on two-inflorescence shoots than single-inflorescence shoots. The relationship between bunch size and node position, unlike that between inflorescence number and node position, was dependent on bearer length. The relative size of the inflorescence appeared to be affected more so by the node pOSition at which the shoot occurred on the bearer, as opposed to the actual node position on the shoot at which the inflorescence occurred. There was a positive, non-linear relationship between average fruit yield per bearer and bearer length. Although yield was highest from the bearer with the highest node number (five nodes), there was no significant difference in yield per bearer for the bearers of three to five nodes in length. If average bearer length was increased from two to three nodes, the potential yield gain per bearer is estimated at 38 per cent. The second study presents results of correlations between bunch number and components of bunch weight (flower number and berry number) to investigate co-development of bunch number and bunch size. These data were collected from 4 vineyards in the Limestone Coast Zone of South Australia from Vilis vinifera L. Chardonnay, Shiraz and Cabemet Sauvignon during seasons 2002 / 03 to 2006 / 07. The significant correlations found between fertility and both bunch weight and flower number per inflorescence suggest that the same factors that affect bunch number in a particular season will also affect bunch size. When inflorescence primordia were initiated and differentiated under cool conditions, actual bunches per node and flowers per inflorescence were low. Differences in climate between the vineyard sites were found to be minimal and therefore did not strongly affect the magnitude of the yield components at the vineyard sites. Cultural practices at each vineyard site were sufficiently variable to affect fertility levels. Genotype is thought to determine the range of flowers per inflorescence that a variety can potentially carry, whereas actual flower number per inflorescence is thought to be determined by inflorescence primordium initiation and differentiation temperatures, as well as temperatures during budburst. Despite significant correlations between flower number per inflorescence and berry number per bunch, flower number per inflorescence preflowering for Cabemet Sauvignon, Shiraz and Chardonnay is inversely related to actual percentage fruit set. This is possibly a survival mechanism for the grapevine as it allows the vine to maximise yield each season without detriment to its longevity. Bunches per vine accounted for the majority of the seasonal variation in yield per vine. Fluctuations in bunch number per vine (and therefore yield) are likely to be reduced by varying the number of nodes retained per vine according to the relative fruitfulness per node present <b>pre-pruning.</b> This practice is therefore likely to result in the seasonal variation of berries per bunch becoming a stronger driver of yield. The commercial impacts of these studies are two-fold. Data presented will assist growers to understand the reasons for which their pruning regimes are affecting yield production and how these pruning regimes may be modified to achieve a target yield-particularly when growers are faced with seasons of low predicted fertility. In addition, data presented will allow growers to improve their crop forecasting accuracy, with a greater understanding of the link between bunch number and bunch size. In the current situation of oversupply in the wine industry, wineries are adopting a tough stance towards growers over-delivering on their grape contracts. Therefore, any assistance that can be provided to growers on improving accuracy of yield estimates will be beneficial both to the grower and winery. Thesis (M. Ag. Sc.) [...] University of Adelaide, School of Agriculture, Food and Wine, 200...|$|E

