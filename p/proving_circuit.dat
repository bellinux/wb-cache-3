21|184|Public
40|$|In {{the limited}} time available, {{it will not}} be {{possible}} to give a broad survey of the variety of algebraic techniques that have been used in <b>proving</b> <b>circuit</b> lower bounds. Instead, I will focus narrowly on a body of related results surrounding the complexity class ACC. In particular, I will cover the following:...|$|E
40|$|We survey recent {{developments}} {{in the study of}} probabilistic complexity classes. While the evidence seems to support the conjecture that probabilism can be deterministically simulated with relatively low overhead, i. e., that $P=BPP$, it also indicates that this may be a difficult question to resolve. In fact, proving that probabilistic algorithms have non-trivial deterministic simulations is basically equivalent to <b>proving</b> <b>circuit</b> lower bounds, either in the algebraic or Boolean models...|$|E
40|$|This paper coins {{the term}} "The Fusion Method" {{to a recent}} {{approach}} for <b>proving</b> <b>circuit</b> lower bounds. It describes the method, and surveys its achievements, potential and challenges. 1 Introduction In a recent paper, Karchmer [6] suggested an elegant way in which one can view {{at the same time}} both the "approximation method" of Razborov [13] and the "topological approach" of Sipser [15] for <b>proving</b> <b>circuit</b> lower bounds. In Karchmer's setting the lower bound prover shows that a given circuit C is too small for computing a given function f by contradiction, in the following way. She tries to combine (or 'fuse', as we propose calling it) correct accepting computations of inputs in f Γ 1 (1) by C into an incorrect accepting computation of an input in f Γ 1 (0). It turns out that this "Fusion Method" reduces the dynamic computation of f by C into a static combinatorial cover problem, which provides the lower bound. Moreover, different restrictions on how we can fuse computations [...] ...|$|E
40|$|We {{describe}} {{the results and}} status of a project aiming to provide a provably correct library of basic circuits. We use the theorem proving system PVS in order to <b>prove</b> <b>circuits</b> such as incrementers, adders, arithmetic units, multipliers, leading zero counters, shifters, and decoders. All specifications and proofs {{are available on the}} web. ...|$|R
5000|$|Nevanlinna Prize (1990) for {{introducing}} the [...] "approximation method" [...] in <b>proving</b> Boolean <b>circuit</b> lower bounds of some essential algorithmic problems, ...|$|R
40|$|We {{initiate}} {{a study of}} input-oblivious proof systems, and present a few preliminary results regarding such systems. Our results offer a perspective on {{the intersection of the}} non-uniform complexity class P/poly with uniform complexity classes such as N P and IP. In particular, we provide a uniform complexity formulation of the conjecture N P ⊂ P/poly and a uniform complexity characterization of the class IP ∩P/poly. These (and similar) results offer a perspective on the attempt to <b>prove</b> <b>circuit</b> lower bounds for complexity classes such as N P, PSPACE...|$|R
40|$|In circuit complexity, the {{polynomial}} {{method is}} a general approach to <b>proving</b> <b>circuit</b> lower bounds in restricted settings. One shows that functions computed by sufficiently restricted circuits are "correlated" in some way with a low-complexity polynomial, where complexity may be measured by the degree of the polynomial {{or the number of}} monomials. Then, results limiting the capabilities of low-complexity polynomials are extended to the restricted circuits. Old theorems proved by this method have recently found interesting applications to the design of algorithms for basic problems in the theory of computing. This paper surveys some of these applications, and gives a few new ones...|$|E
40|$|The {{congested}} clique {{model of}} distributed computing has been receiving attention {{as a model}} for densely connected distributed systems. While there has been significant progress on the side of upper bounds, we have very little in terms of lower bounds for the congested clique; indeed, it is now know that proving explicit congested clique lower bounds is as difficult as <b>proving</b> <b>circuit</b> lower bounds. In this work, we use traditional complexity-theoretic tools to build a clearer picture of the complexity landscape of the congested clique, proving non-constructive lower bounds and studying the relationships between natural problems. Peer reviewe...|$|E
40|$|We {{present an}} {{iterative}} approach to constructing pseudorandom generators, {{based on the}} repeated application of mild pseudorandom restrictions. We use this template to construct pseudorandom generators for combinatorial rectangles and read-once CNFs and a hitting set generator for width- 3 branching programs, all of which achieve near-optimal seed-length even in the low-error regime: We get seed-length O(log (n/epsilon)) for error epsilon. Previously, only constructions with seed-length O(^ 3 / 2 n) or O(^ 2 n) were known for these classes with polynomially small error. The (pseudo) random restrictions we use are milder than those typically used for <b>proving</b> <b>circuit</b> lower bounds in that we only set a constant fraction of the bits at a time. While such restrictions do not simplify the functions drastically, we show {{that they can be}} derandomized using small-bias spaces. Comment: To appear in FOCS 201...|$|E
50|$|Pairing-based {{cryptography}} has led {{to several}} cryptographic advancements. One of these advancements is more powerful and more efficient non-interactive zero-knowledge proofs. The seminal idea was to hide the values {{for the evaluation of}} the pairing in a commitment. Using different commitment schemes, this idea was used to build zero-knowledge proof systems under the sub-group hiding and under the decisional linear assumption. These proof systems <b>prove</b> <b>circuit</b> satisfiability, and thus by the Cook-Levin theorem allow proving membership for every language in NP. The size of the common reference string and the proofs is relatively small; however, transforming a statement into a boolean circuit incurs considerable overhead.|$|R
50|$|Circuit {{complexity}} {{goes back}} to Shannon (1949), who proved that almost all Boolean functions on n variables require circuits of size Θ(2n/n). Despite this fact, complexity theorists {{have not been able}} to <b>prove</b> superpolynomial <b>circuit</b> lower bounds for specific Boolean functions.|$|R
40|$|Abstract. An {{exponential}} {{lower bound}} {{on the circuit}} complexity of deciding the weak monadic second-order theory of one successor (WS 1 S) is <b>proved.</b> <b>Circuits</b> are built from binary operations, or 2 -input gates, which compute arbitrary Boolean functions. In particular, to decide the truth of logical formulas of length at most 610 in this second-order language requires a circuit containing at least 10 125 gates. So even if each gate were {{the size of a}} proton, the circuit would not fit in the known universe. This result and its proof, due to both authors, originally appeared in 1974 in the Ph. D. thesis of the first author. In this article, the proof is given, the result is put in historical perspective, and the result is extended to probabilistic circuits. ...|$|R
40|$|Razborov and Rudich {{identified}} {{an imposing}} barrier {{that stands in}} the way of progress toward the goal of proving superpolynomial lower bounds on circuit size. Their work on “natural proofs ” applies to a large class of arguments that have been used in complexity theory, and shows that no such argument can prove that a problem requires circuits of superpolynomial size, even for some very restricted classes of circuits (under reasonable cryptographic assumptions). This barrier is so daunting, that some researchers have decided to focus their attentions elsewhere. Yet the goal of <b>proving</b> <b>circuit</b> lower bounds is of such importance, that some in the community have proposed concrete strategies for surmounting the obstacle. This lecture will discuss some of these strategies, and will dwell at length on a recent approach proposed by Michal Koucky and the author...|$|E
40|$|We {{consider}} {{the problem of}} <b>proving</b> <b>circuit</b> lower bounds against the polynomial-time hierarchy. We give {{both positive and negative}} results. For the positive side, for any fixed integer k > 0, we give an explicit # 2 language, acceptable by a # +k), that requires circuit size > n. This provides a constructive version of an existence theorem of Kannan [Kan 82]. Our main theorem is on the negative side. We give evidence that it is infeasible to give relativizable proofs that any single language in the polynomial-time hierarchy requires super polynomial circuit size. Our proof techniques are based on the decision tree version of the Switching Lemma for constant depth circuits and Nisan-Wigderson pseudorandom generator. We also take this opportunity to publish some unpublshed older results of the first author on constant depth circuits, both straight lower bounds and inapproximability results based on decision tree type Switching Lemmas...|$|E
40|$|This work {{is devoted}} to explore the novel method of <b>proving</b> <b>circuit</b> lower bounds for the class NEXP by Ryan Williams. Williams is able to show two circuit lower bounds: A {{conditional}} lower bound which says that NEXP does not have polynomial size circuits if there exists better-than-trivial algorithms for CIRCUIT SAT and an inconditional lower bound which says that NEXP does not have polynomial size circuits of the class ACC^ 0. We put special emphasis on the first result by exposing, {{in as much as}} of a self-contained manner as possible, all the results from complexity theory that Williams use in his proof. In particular, the focus is put in an efficient reduction from non-deterministic computations to satisfiability of Boolean formulas. The second result is also studied, although not as thoroughly, and some pointers with regards to the relationship of Williams' method and the known complexity theory barriers are given...|$|E
40|$|AbstractThis note {{proves the}} {{existence}} of acyclic directed graphs of logarithmic depth, such that a superlinear number of input—output pairs remain connected after the removal of any sufficiently small linearly sized subset of the vertices. The technique {{can be used to}} prove the analogous, and asymptotically optimal, result for graphs of arbitrary depth, generalizing Schnitger's grate construction for graphs of large depth. Interest in this question relates to efforts to use graph theoretic methods to <b>prove</b> <b>circuit</b> complexity lower bounds for algebraic problems such as matrix multiplication. In particular, it establishes the optimality of Valiant's depth reduction technique as a method of reducing the number of connected input-output pairs. The proof uses Schnitger's grate construction, but also involves a lemma on expanding graphs which may be of independent interest...|$|R
40|$|The {{concept of}} {{designing}} for reliability {{will be introduced}} along with {{a brief overview of}} reliability, redundancy and traditional methods of fault tolerance is presented, as applied to current logic devices. The fundamentals of advanced circuit design and analysis techniques will be the primary focus. The introduction will cover the definitions of key device parameters and how analysis is used to <b>prove</b> <b>circuit</b> correctness. Basic design techniques such as synchronous vs asynchronous design, metastable state resolution time/arbiter design, and finite state machine structure/implementation will be reviewed. Advanced topics will be explored such as skew-tolerant circuit design, the use of triple-modular redundancy and circuit hazards, device transients and preventative circuit design, lock-up states in finite state machines generated by logic synthesizers, device transient characteristics, radiation mitigation techniques. worst-case analysis, the use of timing analyzer and simulators, and others. Case studies and lessons learned from spaceflight designs will be given as example...|$|R
40|$|Modeling and {{designing}} a trimless VCO requires a full {{understanding of the}} non-ideal nature of oscillator components and architectures. TRIMLESS voltage-controlled oscillators (VCOs) offer a practical alternative to conventional discrete VCO approaches that rely on tuning adjustments during production. The Colpitts style oscillator topology offers a <b>proven</b> <b>circuit</b> architecture for use in a trimless VCO design. A basic set of fundamental design equations can be derived for first-order oscillator design and selection of component values. Unfortunately, real-world components used to implement the trimless VCO are nonideal and alter the governing equations. The conclusion of this two-part article on trimless VCOs covers how actual circuit implementation departs from the ideal, offering an improved method for modeling, designing, and implementing trimless VCOs. In Part 1 (see Microwaves & RF, July 1999, p. 68), the Colpitts configuration (Fig. 7) was presented as the basis for a trimless VCO. The classic oscillator topology was described with a generalized set of equations to predict the fundamental oscillator behavior for the first-order design of the oscillator (i. e., component selection). The variation (error) in actual C...|$|R
40|$|We {{show that}} derandomizing Polynomial Identity Testing is, essentially, {{equivalent}} to <b>proving</b> <b>circuit</b> lower bounds for NEXP. More precisely, we prove {{that if one}} can test in polynomial time (or, even, nondeterministic subexponential time, infinitely often) whether a given arithmetic circuit over integers computes an identically zero polynomial, then ei-ther (i) NEXP ⊂ P/poly or (ii) Permanent is not computable by polynomial-size arithmetic circuits. We also prove a (par-tial) converse: If Permanent requires superpolynomial-size arithmetic circuits, then one can test in subexponential time whether a given arithmetic formula computes an identically zero polynomial. Since Polynomial Identity Testing is a coRP problem, we obtain the following corollary: If RP = P (or, even, coRP ⊆ ∩> 0 NTIME(2 n), infinitely often), then NEXP is not com-putable by polynomial-size arithmetic circuits. Thus, estab-lishing that RP = coRP or BPP = P would require proving superpolynomial lower bounds for Boolean or arithmetic circuits. We also show that any derandomization of RNC would yield new circuit lower bounds for a language in NEXP...|$|E
40|$|We {{consider}} the following question: given a two-argument boolean function f, represented as an N × N binary matrix, how hard {{is to determine the}} (deterministic) communication complexity of f? We address two aspects of this question. On the computational side, we prove that, under appropriate cryptographic assumptions (such as the intractability of factoring), the deterministic communication complexity of f is hard to approximate to within some constant. Under stronger (yet arguably reasonable) assumptions, we obtains even stronger hardness results that match the best known approximation. On the analytic side, we present a family of functions for which determining the communication complexity (or even obtaining non-trivial lower bounds on it) imply <b>proving</b> <b>circuit</b> lower bounds for some corresponding problems. Such connections between circuit complexity and communication complexity were known before (Karchmer-Wigderson 1988) only in the more involved context of relations (search problems) and not in the context of functions (decision problems). This result, in particular, may explain the difficulty of analyzing the communication complexity of certain functions such as the “clique vs. independent-set ” family of functions, introduced by Yannakakis (1988) ...|$|E
40|$|Abstract. Consider the {{following}} problem: Given an n × n matrix A and an input x, compute Ax. This problem has a simple algorithm which runs in time O(n 2). The question thus is: Is {{this is the}} best possible? Valiant showed ([12]) that if an n×n matrix A is rigid, then the smallest straight line program computing Ax is either super-linear size, or has super logarithmic depth. Roughly a matrix of rank n is said to be rigid, if to bring its rank down to n/ 2, one has to change at least n 1 +ɛ of its entries, for some ɛ> 0. After considerable effort by many researchers, the problem of finding an explicit rigid matrix (hence <b>proving</b> <b>circuit</b> lower bounds) remains elusive. This paper casts the problem of matrix rigidity in the language of algebraic geometry. As a first step, we provide the basic setup and prove some elementary results about this problem. This setting facilitates our understanding of the difficulty of determining the rigidity of an explicit matrix (like Hadamard). On the brighter side, we hope that tools from algebraic geometry might eventually help establish rigidity of explicit matrices...|$|E
40|$|The ART Series {{of three}} output DC-DC {{converters}} are {{designed specifically for}} use in the hostile radiation environments characteristic of space and weapon systems. The extremely high level of radiation tolerance inherent in the ART design is the culmination of extensive research, thorough analysis and testing and of careful component selection. Many of the <b>proven</b> <b>circuit</b> design features characterizing the International Rectifier standard product line were adapted for incorporation into the ART topology. Capable of uniformly high performance over long term exposures in radiation intense environments, this series sets the standard for distributed power systems demanding high performance and reliability. The ART converters are hermetically sealed in a rugged, low profile package utilizing copper core pins to minimize resistive DC losses. Long-term hermeticity is assured through use of parallel seam welded lid attachment along with International Rectifier’s rugged ceramic pin-to-package seal. Axial orientation of the leads facilitates preferred bulkhead mounting to the principal heat-dissipating surface. Total Dose> 100 krad (Si), 2 : 1 margin SEE Hardened to LET up to 83 Mev. cm 2 /m...|$|R
40|$|AbstractRazborov and Rudich {{have proved}} that, under a widely-believed {{hypothesis}} about pseudorandom number generators, there {{do not exist}} P/poly-computable Boolean function properties with density greater than 2 −poly(n) that exclude P/poly. This famous result {{is widely regarded as}} a serious barrier to proving strong lower bounds in circuit complexity theory, because virtually all Boolean function properties used in existing lower bound proofs have the stated complexity and density. In this paper, we show that under the same pseudorandomness hypothesis, there do exist nearly-linear-time-computable Boolean function properties with only slightly lower density (namely, 2 −q(n) for a quasi-polynomial function q) that not only exclude P/poly, but even separate NP from P/poly. Indeed, we introduce a simple, explicit property called discrimination that does so. We also prove unconditionally that there exist non-uniformly nearly-linear-time-computable Boolean function properties with this same density that exclude P/poly. Along the way we also note that by slightly strengthening Razborov and Rudichʼs argument, one can show that their “naturalization barrier” is actually a barrier to <b>proving</b> superquadratic <b>circuit</b> lower bounds, not just P/poly circuit lower bounds. It remains open whether there is a naturalization barrier to <b>proving</b> superlinear <b>circuit</b> lower bounds...|$|R
40|$|Most recent combinational {{equivalence}} checking {{techniques are}} based on exploiting circuit similarity. In this paper, we focus on circuits with no internal equivalent nodes or after internal equivalent nodes have been identified and merged. We present a new technique integrating Boolean Satisfiability and Binary Decision Diagrams. The proposed approach is capable of solving verification instances that neither of both techniques was capable to solve. The efficiency of the proposed approach is shown through its application on hard to <b>prove</b> industrial <b>circuits</b> and the ISCAS' 85 benchmark circuits...|$|R
40|$|We {{show that}} circuit lower bound proofs {{based on the}} method of random {{restrictions}} yield non-trivial compression algorithms for “easy ” Boolean functions from the corresponding circuit classes. The compression problem is defined as follows: given the truth table of an n-variate Boolean function f computable by some unknown small circuit from a known class of circuits, find in deterministic time poly(2 n) a circuit C (no restriction {{on the type of}} C) computing f so that the size of C is less than the trivial circuit size 2 n/n. We get non-trivial compression for functions computable by AC 0 circuits, (de Morgan) formulas, and (read-once) branching programs of the size for which the lower bounds for the corresponding circuit class are known. These compression algorithms rely on the structural characterizations of “easy ” functions, which are useful both for <b>proving</b> <b>circuit</b> lower bounds and for designing “meta-algorithms” (such as Circuit-SAT). For (de Morgan) formulas, such structural characterization is provided by the “shrinkage under random restrictions ” results [Sub 61, H̊as 98], strengthened to the “high-probability ” version by [San 10, IMZ 12, KR 12]. We give a new, simple proof of the “high-probability ” version of the shrinkage result for (de Morgan) formulas, with improved parameters...|$|E
40|$|The {{congested}} clique {{model of}} distributed computing has been receiving attention {{as a model}} for densely connected distributed systems. While there has been significant progress on the side of upper bounds, we have very little in terms of lower bounds for the congested clique; indeed, it is now know that proving explicit congested clique lower bounds is as difficult as <b>proving</b> <b>circuit</b> lower bounds. In this work, we use various more traditional complexity-theoretic tools to build a clearer picture of the complexity landscape of the congested clique: [...] Nondeterminism and beyond: We introduce the nondeterministic congested clique model (analogous to NP) and show that there is a natural canonical problem family that captures all problems solvable in constant time with nondeterministic algorithms. We further generalise these notions by introducing the constant-round decision hierarchy (analogous to the polynomial hierarchy). [...] Non-constructive lower bounds: We lift the prior non-uniform counting arguments to a general technique for proving non-constructive uniform lower bounds for the congested clique. In particular, we prove a time hierarchy theorem for the congested clique, showing that there are decision problems of essentially all complexities, both in the deterministic and nondeterministic settings. [...] Fine-grained complexity: We map out relationships between various natural problems in the congested clique model...|$|E
40|$|We {{observe that}} many {{important}} computational problems in NC 1 share a simple self-reducibility property. We then show that, for any problem A having this self-reducibility property, A has poly-nomial size TC 0 circuits if {{and only if}} it has TC 0 circuits of size n 1 + for every > 0 (counting the number of wires in a circuit as the size of the circuit). As an example of what this observation yields, consider the Boolean Formula Evaluation problem (BFE), which is complete for NC 1 and has the self-reducibility property. It follows from a lower bound of Impagliazzo, Paturi, and Saks, that BFE requires depth d TC 0 circuits of size n 1 +d. If one were able to improve this lower bound to show that there is some constant > 0 such that every TC 0 circuit family recognizing BFE has size n 1 +, then it would follow that TC 0 6 = NC 1. We show that proving lower bounds of the form n 1 + is not ruled out by the Natural Proof framework of Razborov and Rudich and hence there is currently no known barrier for separating classes such as ACC 0, TC 0 and NC 1 via existing “natural ” approaches to <b>proving</b> <b>circuit</b> lower bounds. We also show that problems with small uniform constant-depth circuits have algorithms that simultaneously have small space and time bounds. We then make use of known time-space tradeoff lower bounds to show that SAT requires uniform depth d TC 0 and AC 0 [6] circuits of size n 1 +c for some constant c depending on d. ...|$|E
40|$|Linear {{reciprocal}} resistive n-ports manifest convex (downward concave) {{property in}} view of their positive definite character. Several useful theorems for resistive n-ports are <b>proved</b> by electrical <b>circuit</b> considerations. The suggested line of physical reasoning via n-ports is potent for deriving a variety of matrix inequalities for positive definite operators...|$|R
5000|$|While {{the verbal}} {{description}} of circuitism has attracted interest, it has proven difficult to model mathematically. Initial efforts {{to model the}} monetary <b>circuit</b> <b>proved</b> problematic, with models exhibiting a number of unexpected and undesired properties - money disappearing immediately, for instance. These problem go by such names as: ...|$|R
40|$|AbstractWe <b>prove</b> that {{weighted}} <b>circuit</b> satisfiability for monotone or antimonotone circuits has no fixed-parameter tractable approximation algorithm {{with any}} approximation ratio function ρ, unless FPT≠W[1]. In particular, not having such an fpt-approximation algorithm implies that these problems have no polynomial-time approximation algorithms with ratio ρ(OPT) for any nontrivial function ρ...|$|R
40|$|In {{the last}} two lectures we proved two results on circuit lower bounds, namely P ARIT Y / ∈ AC 0, and that {{computing}} CLIQUEk,n requires superpolynomial size monotonic circuit. Recalling that relativizing proof technique is not sufficient for proving P = NP, one might wonder if {{there is also a}} limitation for <b>proving</b> <b>circuit</b> lower bound by those combinatorial techniques. Indeed, in this lecture, we are going to show that if a proof technique follows the “natural proof ” paradigm [2], then such technique could not separate NP from SIZE(n k). 2 The Natural Proof Paradigm Suppose we have a proof for f / ∈ C (e. g. SAT / ∈ SIZE(n k)), where C is a class of circuits. Often, the proof goes by showing that f does not satisfy some property P while every language in C does. Now, we say that a proof is a natural proof if it has the following three attributes: 1. Usefulness: It shows that f / ∈ P, but C ⊂ P, where P is a property/predicate 2. Constructivity: Given g: { 0, 1 } n → { 0, 1 }, we can decide whether g has property P in 2 O(n) time, i. e. in time polynomial of size of the truth table of g. 3. Largeness: At least 1 /n of all possible functions g: { 0, 1 } n → { 0, 1 } (there are 22 n are outside P of them) Here are two examples to illustrate the definition of a natural proof: Example 1 : P ARIT Y / ∈ AC 0 Recall that we proved this result by using the propert...|$|E
40|$|We {{present a}} general {{approach}} to the hoary problem of (im) <b>proving</b> <b>circuit</b> lower bounds. We define notions of hardness condensing and hardness extraction, in analogy to the corresponding notions from the computational theory of randomness. A hardness condenser is a procedure that takes in a Boolean function as input, {{as well as an}} advice string, and outputs a Boolean function on a smaller number of bits which has greater hardness when measured in terms of input length. A hardness extractor takes in a Boolean function as input, as well as an advice string, and outputs a Boolean function defined on a smaller number of bits which has close to maximum hardness. We prove several positive and negative results about these objects. First, we observe that hardness-based pseudo-random generators can be used to extract deterministic hardness from non-deterministic hardness. We derive several consequences of this observation. Among other results, we show that if E has exponential non-deterministic hardness, then E with linear advice has close to maximum deterministic hardness. We demonstrate a rare downward closure result: there is δ> 0 such that E with sub-exponential advice is contained in non-uniform space 2 δn if and only if there is k> 0 such that P with quadratic advice can be approximated in non-uniform space n k. Next, we consider limitations on natural models of hardness condensing and extraction. We show lower bounds on the length of the advice required for hardness condensing in a very general model of “relativizing ” condensers. We show that non-trivial black-box extraction of deterministic hardness from deterministic hardness is essentially impossible. Finally, we prove positive results on hardness condensing in certain special cases. We show how to condense hardness from a biased function without any advice, using a hashing technique. We also give a hardness condenser without advice from average-case hardness to worst-case hardness. Our technique involves a connection between hardness condensing and certain kinds of explicit constructions of covering codes...|$|E
40|$|Abstract. We {{show that}} circuit lower bound proofs {{based on the}} method of random {{restrictions}} yield non-trivial compression algorithms for “easy ” Boolean functions from the corresponding circuit classes. The compression problem is defined as follows: given the truth table of an n-variate Boolean function f computable by some unknown small circuit from a known class of circuits, find in deterministic time poly(2 n) a circuit C (no restriction {{on the type of}} C) computing f so that the size of C is less than the trivial circuit size 2 n/n. We get non-trivial compression for functions computable by AC 0 circuits, (de Morgan) formulas, and (read-once) branching programs of the size for which the lower bounds for the corresponding circuit class are known. These compression algorithms rely on the structural characterizations of “easy ” functions, which are useful both for <b>proving</b> <b>circuit</b> lower bounds and for designing “meta-algorithms ” (such as Circuit-SAT). For (de Morgan) formulas, such structural characterization is provided by the “shrinkage under random restrictions ” results by Subbotovskaya (1961) and H̊astad (1998), strengthened to the “high-probability ” ver-sion by Santhanam (2010), Impagliazzo, Meka & Zuckerman (2012 b), and Komargodski & Raz (2013). We give a new, simple proof of the “high-probability ” version of the shrinkage result for (de Morgan) for-mulas, with improved parameters. We use this shrinkage result to get both compression and #SAT algorithms for (de Morgan) formulas of size about n 2. We also use this shrinkage result to get an alternative proof of the result by Komargodski & Raz (2013) of the average-case lower bound against small (de Morgan) formulas. Finally, we show that the existence of any non-trivial compression al-gorithm for a circuit class C ⊆ P/poly would imply the circuit lower 2 Chen et al. bound NEXP 6 ⊆ C; a similar implication is independently proved also by Williams (2013). This complements the result by Williams (2010) that any non-trivial Circuit-SAT algorithm for a circuit class C would imply a superpolynomial lower bound against C for a language in NEXP...|$|E
40|$|Abstract – In this paper, {{we define}} the {{equivalence}} of logic circuits and examine their description in VHDL. We can describe logic circuit components as relations between {{input and output}} signals in VHDL and implement such circuits on FPGA. In this case, {{it is necessary to}} verify the equivalence of VHDL descriptions and the actual logic circuits constructed based on these descriptions. In a previous work, we proved the logical correctness of some circuits using the Mizar system. We also verified the equivalence of VHDL descriptions and the <b>proved</b> logic <b>circuits.</b> Keywords – formalized mathematics, gate, logic circuit, VHDL. 1...|$|R
40|$|Abstract. We {{consider}} constant depth circuits augmented {{with few}} modular, or more generally, arbitrary symmetric gates. We prove that cir-cuits augmented with o(log 2 n) symmetric gates must have size nΩ(log n) to compute a certain (complicated) function in ACC 0. This function is also {{hard on the}} average for circuits of size n logn aug-mented with o(log n) symmetric gates, {{and as a consequence}} we can get a pseudorandom generator for circuits of size m containing o(log m) symmetric gates. For a composite integer m having r distinct prime factors, we <b>prove</b> that <b>circuits</b> augmented with s MODm gates must have size...|$|R
40|$|We {{study the}} {{complexity}} of the following circuit minimization problem: given the truth table of a Boolean function f and a parameter s, decide whether f can be realized by a Boolean circuit of size at most s. We argue why this problem is unlikely to be in P (or even in P/poly) by giving a number of surprising consequences of such an assumption. We also argue that proving this problem to be NP-complete (if it is indeed true) would imply <b>proving</b> strong <b>circuit</b> lower bounds for the class DTIME(2 °('~)), which appears beyond the currently known techniques. 1...|$|R
