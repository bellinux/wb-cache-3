19|107|Public
25|$|Once the <b>pivot</b> <b>column</b> {{has been}} selected, {{the choice of}} pivot row is largely {{determined}} by the requirement that the resulting solution be feasible. First, only positive entries in the <b>pivot</b> <b>column</b> are considered since this guarantees {{that the value of}} the entering variable will be nonnegative. If there are no positive entries in the <b>pivot</b> <b>column</b> then the entering variable can take any nonnegative value with the solution remaining feasible. In this case the objective function is unbounded below and there is no minimum.|$|E
25|$|Since the {{entering}} variable will, in general, increase from 0 {{to a positive}} number, {{the value of the}} objective function will decrease if the derivative of the objective function with respect to this variable is negative. Equivalently, the value of the objective function is decreased if the <b>pivot</b> <b>column</b> is selected so that the corresponding entry in the objective row of the tableau is positive.|$|E
25|$|The {{geometrical}} {{operation of}} {{moving from a}} basic feasible solution to an adjacent basic feasible solution is implemented as a pivot operation. First, a nonzero pivot element is selected in a nonbasic column. The row containing this element is multiplied by its reciprocal to change this element to 1, and then multiples of the row {{are added to the}} other rows to change the other entries in the column to 0. The result is that, if the pivot element is in row r, then the column becomes the r-th column of the identity matrix. The variable for this column is now a basic variable, replacing the variable which corresponded to the r-th column of the identity matrix before the operation. In effect, the variable corresponding to the <b>pivot</b> <b>column</b> enters the set of basic variables and is called the entering variable, and the variable being replaced leaves the set of basic variables and is called the leaving variable. The tableau is still in canonical form but with the set of basic variables changed by one element.|$|E
2500|$|Columns 2, 3, and 4 can be {{selected}} as <b>pivot</b> <b>columns,</b> for this example column 4 is selected. The values of z {{resulting from the}} choice of rows 2 and 3 as pivot rows are 10/1=10 and 15/3=5 respectively. Of these the minimum is 5, so row 3 must be the pivot row. Performing the pivot produces ...|$|R
40|$|For {{least squares}} {{problems}} {{in which the}} rows of the coefficient matrix vary widely in norm, Householder QR factorization (without pivoting) has unsatisfactory backward stability properties. Powell and Reid showed in 1969 {{that the use of}} both row and <b>column</b> <b>pivoting</b> leads to a desirable row-wise backward error result. We give a reworked backward error analysis in modern notation and prove two new results. First, sorting the rows by decreasing ∞-norm {{at the start of the}} factorization obviates the need for row pivoting. Second, row-wise backward stability is obtained for only one of the two possible choices of sign in the Householder vector. Key words. Weighted least squares problem, Householder matrix, QR factorization, row <b>pivoting,</b> <b>column</b> <b>pivoting,</b> backward error analysis AMS subject classifications. 65 F 20, 65 G 05...|$|R
5000|$|Let [...] be an [...] {{permutation}} matrix {{such that}} [...] in block partitioned form, where the columns of [...] are the [...] <b>pivot</b> <b>columns</b> of [...] Every column of [...] is a linear {{combination of the}} columns of , {{so there is a}} matrix [...] such that , where the columns of [...] contain the coefficients of each of those linear combinations. So , [...] being the [...] identity matrix. We will show now that [...]|$|R
2500|$|Now select column 3 as a <b>pivot</b> <b>column,</b> {{for which}} row 3 {{must be the}} pivot row, to get ...|$|E
2500|$|Select column 5 as a <b>pivot</b> <b>column,</b> so {{the pivot}} row must be row 4, and the updated tableau is ...|$|E
2500|$|Next, {{the pivot}} row must be {{selected}} {{so that all}} the other basic variables remain positive. A calculation shows that this occurs when the resulting value of the entering variable is at a minimum. In other words, if the <b>pivot</b> <b>column</b> is c, then the pivot row r is chosen so that ...|$|E
5000|$|Transposing or <b>pivoting</b> (turning {{multiple}} <b>columns</b> into multiple rows or vice versa) ...|$|R
40|$|In the multifrontal method, each frontal matrix {{must hold}} {{all of its}} <b>pivot</b> rows and <b>columns</b> at one time. Moving data between frontal {{matrices}} is costly, but the method can handle arbitrary fill-reducing orderings. In the unifrontal method, <b>pivot</b> rows and <b>columns</b> are shifted out of the frontal matrix as the factorization proceeds. Data movement is simpler, but higher fill-in can result. We consider a hybrid unifrontal/multifrontal algorithm. Fill-reducing orderings can still be applied, but data movement is reduced by allowing <b>pivot</b> rows and <b>columns</b> to be shifted {{into and out of}} each frontal matrix (just as in the unifrontal method). Performance results on a Cray YMP supercomputer are presented...|$|R
5000|$|Determine which {{columns of}} the echelon form have <b>pivots.</b> The {{corresponding}} <b>columns</b> {{of the original}} matrix are {{a basis for the}} column space.|$|R
50|$|Once the <b>pivot</b> <b>column</b> {{has been}} selected, {{the choice of}} pivot row is largely {{determined}} by the requirement that the resulting solution be feasible. First, only positive entries in the <b>pivot</b> <b>column</b> are considered since this guarantees {{that the value of}} the entering variable will be nonnegative. If there are no positive entries in the <b>pivot</b> <b>column</b> then the entering variable can take any nonnegative value with the solution remaining feasible. In this case the objective function is unbounded below and there is no minimum.|$|E
5000|$|Now select column 3 as a <b>pivot</b> <b>column,</b> {{for which}} row 3 {{must be the}} pivot row, to get ...|$|E
5000|$|Select column 5 as a <b>pivot</b> <b>column,</b> so {{the pivot}} row must be row 4, and the updated tableau is ...|$|E
30|$|In both cases, the {{obtained}} over-determined {{systems are}} solved using the MATLAB’s ∖operator that makes use the QR factorization with a <b>column</b> <b>pivoting</b> method.|$|R
40|$|Three {{truncated}} QR {{methods are}} proposed for sinusoidal frequency estimation: (1) truncated QR without <b>column</b> <b>pivoting</b> (TQR), (2) truncated QR with preordered columns, and (3) truncated QR with <b>column</b> <b>pivoting.</b> It is {{demonstrated that the}} benefit of truncated SVD for high frequency resolution is achievable under the truncated QR approach with much lower computational cost. Other attractive features of the proposed methods include the ease of updating, which is difficult for the SVD method, and numerical stability. TQR methods thus offer efficient ways to identify sinusoidals closely clustered in frequencies under stationary and nonstationary conditions...|$|R
40|$|We present new normwise and componentwise {{perturbation}} {{analyses for}} the R factor of the QR factorization A = Q 1 R of an m Θ n matrix A with full column rank. The analyses more accurately reflect {{the sensitivity of}} the problem than previous normwise and componentwise results. The new condition numbers here are altered by any <b>column</b> <b>pivoting</b> used in AP = Q 1 R, and are bounded for a fixed n when the standard <b>column</b> <b>pivoting</b> strategy is used. Both numerical results and an analysis show that the standard method of pivoting is optimal in that it usually leads to a normwise condition number very close to its lower limit for any given A. It follows that the computed R will probably have greatest accuracy when we use the standard <b>column</b> <b>pivoting</b> strategy. Also we derive a practical estimate for the normwise condition number. Key words. QR factorization, perturbation analysis, pivoting AMS Subject Classifications: 65 F 05, 65 F 30, 65 G 05 1...|$|R
5000|$|Consider {{the matrix}} is in reduced echelon form.Then [...] is {{obtained}} {{by removing the}} third column of , the only one {{which is not a}} <b>pivot</b> <b>column,</b> and [...] by getting rid of the last row of zeroes, soIt is straightforward to check that ...|$|E
5000|$|Next, {{the pivot}} row must be {{selected}} {{so that all}} the other basic variables remain positive. A calculation shows that this occurs when the resulting value of the entering variable is at a minimum. In other words, if the <b>pivot</b> <b>column</b> is c, then the pivot row r is chosen so that ...|$|E
50|$|Since the {{entering}} variable will, in general, increase from 0 {{to a positive}} number, {{the value of the}} objective function will decrease if the derivative of the objective function with respect to this variable is negative. Equivalently, the value of the objective function is decreased if the <b>pivot</b> <b>column</b> is selected so that the corresponding entry in the objective row of the tableau is positive.|$|E
40|$|AbstractIn {{this paper}} we present an {{experimental}} comparison of several numerical tools for computing the numerical rank of dense matrices. The study includes the well-known SVD, the URV decomposition, and several rank-revealing QR factorizations: the QR factorization with <b>column</b> <b>pivoting,</b> and two QR factorizations with restricted <b>column</b> <b>pivoting.</b> Two different parallel programming methodologies are analyzed in our paper. First, we present block-partitioned algorithms for the URV decomposition and rank-revealing QR factorizations which provide efficient implementations on shared memory environments. Furthermore, we also present parallel distributed algorithms, {{based on the}} message-passing paradigm, for computing rank-revealing QR factorizations on multicomputers...|$|R
40|$|Abstract. A {{parallel}} {{algorithm is}} presented for the LU decomposition {{of a general}} sparse matrix on a distributed-memory MIMD multiprocessor with a square mesh communication network. In the algorithm, matrix elements are assigned to processors according to the grid distribution. Each processor represents the nonzero elements of its part of the matrix by a local, ordered, two-dimensional linked-list data structure. The complexity of important operations on this data structure and on several others is analysed. At {{each step of the}} algorithm, a parallel search for a set of m compatible pivot elements is performed. The Markowitz counts of the pivot elements are close to minimum, to preserve the sparsity of the matrix. The pivot elements also satisfy a threshold criterion, to ensure numerical stability. The compatibility of the m pivots enables the simultaneous elimination ofm pivot rows and m <b>pivot</b> <b>columns</b> in a rank-m update of the reduced matrix. Experimental results on a network of 400 transputers are presented for a set of test matrices from the Harwell-Boeing sparse matrix collection...|$|R
40|$|We propose three {{truncated}} QR {{methods for}} sinusoidal frequency cstiniation: (1) truncated QR without <b>column</b> <b>pivoting</b> (TQR); (2) truncated QR with pre-ordered columns (TQRR); and (3) tiuncated QR with <b>column</b> <b>pivoting</b> (TQRP). It is {{demonstrated that the}} benefit of truncated SVD (TSVD) for high frequency rcsolution is achievable under the truncated QR approach with much lower computational cost. Other attractive features of the proposed methods include the ease of updating, which is difficult for the SVD method, and numerical stability. Thus, the TQR nietliods offers efficient ways for identifying sinusoidals closedly clustered in frequencies under stationary and nonstationary conditions. Based on the FBLP model, computer simulations and comparisons are provided for different truncation methods under various SNR's. ...|$|R
50|$|The {{geometrical}} {{operation of}} {{moving from a}} basic feasible solution to an adjacent basic feasible solution is implemented as a pivot operation. First, a nonzero pivot element is selected in a nonbasic column. The row containing this element is multiplied by its reciprocal to change this element to 1, and then multiples of the row {{are added to the}} other rows to change the other entries in the column to 0. The result is that, if the pivot element is in row r, then the column becomes the r-th column of the identity matrix. The variable for this column is now a basic variable, replacing the variable which corresponded to the r-th column of the identity matrix before the operation. In effect, the variable corresponding to the <b>pivot</b> <b>column</b> enters the set of basic variables and is called the entering variable, and the variable being replaced leaves the set of basic variables and is called the leaving variable. The tableau is still in canonical form but with the set of basic variables changed by one element.|$|E
40|$|A {{variety of}} <b>pivot</b> <b>column</b> {{selection}} rules {{based upon the}} gradient criteria (including the steepest edge) have been explored to improve {{the efficiency of the}} primal simplex method. Simplex-like algorithms have been proposed imbedding the gradient direction (GD) which includes all variables whose increase or decrease leads to an improvement in the objective function. Recently a frame work has been developed in the simplex method to incorporate the reduced-gradient direction (RGD) consisting of only variables whose increase leads to an improvement in the objective function. In this paper, the results are extended to embed GD in the simplex method based on the concept of combining directions. Also mathematical properties related to combining directions as well as deleting a variable from all basic directions are presented...|$|E
40|$|With the {{development}} of the space shuttle launching facilities, it became mandatory to develop a shuttle rotating service structure to provide for the insertion and/or removal of payloads at the launch pads. The rotating service structure is a welded tubular steel space frame 189 feet high, 65 feet wide, and weighing 2100 tons. At the <b>pivot</b> <b>column</b> the structure is supported on a 30 inch diameter hemispherical bearing. At the opposite terminus the structure is supported on two truck assemblies each having eight 36 inch diameter double flanged wheels. The following features of the rotating service structure are discussed: (1) thermal expansion and contraction; (2) hurricane tie downs; (3) payload changeout room; (4) payload ground handling mechanism; (5) payload and orbiter access platforms; and (6) orbiter cargo bay access...|$|E
40|$|This paper {{proposes a}} new parallelization of the Primal and Dual Simplex {{algorithms}} for Linear Programming (LP) problems on massively parallel Single-Instruction Multiple-Data (SIMD) computers. The algorithms {{are based on}} the Steepest-Edge pivot selection method and the tableau representation of the constraint matrix. The initial canonical tableau is formed on an attached scalar host unit, and then partitioned into a rectangular grid of sub-matrices and distributed to the individual Processor Element (PE) memories. In the beginning of the parallel algorithm key portions of the simplex tableau are partially replicated and stored along with the sub-matrices on each one of the PEs. The SIMD simplex algorithm iteratively selects a pivot element and carries-out a simplex computation step until the optimal solution is found, or when unboundedness of the LP is established. The Steepest-Edge pivot selection technique utilizes information mainly from local replicas to search for the next pivot element. The <b>pivot</b> row and <b>column</b> are selectively broadcasted to the PEs before a pivot computation step, by efficiently utilizing the geometry of the toroidal mesh interconnection network. Every individual PE maintains locally and keeps consistent its replicas so that inter-processor communication due to data dependencies is further reduced. The presence of a pipelined inteconnection network, like the mesh network of MP- 1 and MP- 2 MasPar models allows the global reduction operations necessary in the selection of <b>pivot</b> <b>columns</b> and rows to be performed in time O(log nR + log nC), in (nR Θ nC) PE arrays. This particular combination of pivot selection, matrix representation, and selective data replication is shown to be highly efficient in the solution of linear prog [...] ...|$|R
40|$|We {{present the}} {{techniques}} of adaptive blocking and incremental condition estimation which {{we believe to be}} useful for the computation of common matrix decompositions in high-performance environments. We apply these new techniques to algorithms for computing the Householder QR factorization with and without pivoting on a coarse-grained distributed system. For reasons of portability, we use a pipelined scheme on a ring of processors as the basis of our algorithms. To take advantage of possible floating point hardware on each node we develop a blocked version of the pipelined Householder QR algorithm that employs the compact WY representation for products of Householder matrices. While a strategy involving blocks of fixed width leads to increased floating point utilization per node, it also leads to increased load imbalance. To reconcile this tradeoff we introduce a variable width block strategy based on a model of the critical path of the algorithm. The resulting adaptive blocking strategy provides for good floating point performance per node while maintaining overall load balance. Experimental results on the Intel iPSC hypercube show that the adaptive blocking strategy performs indeed better than any fixed width blocking strategy. In the second part of our thesis we develop methods for introducing pivoting into the distributed QR factorization algorithm. Incorporating the traditional <b>column</b> <b>pivoting</b> strategy in a straightforward manner introduces a global synchronization constraint which results in increased communication overhead. A strictly local pivoting scheme avoids the resulting loss in efficiency, but has to be monitored for reliability. To this end, we introduce an incremental condition estimator which allows us to update the estimate of the smallest singular value of an upper triangular matrix $R$ as new columns are added to $R$. The update requires only $O(n) $ flops an the storage of $O(n) $ words between successive steps. Experiments indicate that the incremental condition estimator is reliable despite its small computational cost. Using the incremental condition estimator we are then able to guard against the selection of troublesome <b>pivot</b> <b>columns</b> in our local pivoting scheme at little extra cost. Simulation results show that the resulting algorithm is about as reliable as the traditional QR factorization algorithm with <b>column</b> <b>pivoting...</b>|$|R
40|$|AbstractUsing QR-like {{decomposition}} with <b>column</b> <b>pivoting</b> {{and least}} squares techniques, we propose {{a new and}} efficient algorithm for solving symmetric generalized inverse eigenvalue problems, and give its locally quadratic convergence analysis. We also present some numerical experiments which illustrate the behaviour of our algorithm...|$|R
40|$|Abstract. A key {{technique}} for controlling numerical stability in sparse direct solvers is threshold partial pivoting. When selecting a pivot, the entire candidate <b>pivot</b> <b>column</b> below the diagonal must be up-to-date {{and must be}} scanned. If the factorization is parallelized across {{a large number of}} cores, communication latencies can be the dominant computational cost. In this paper, we propose two alternative pivoting strategies for sparse symmetric indefinite matrices of full rank that significantly reduce communication by compressing the necessary data into a small matrix {{that can be used to}} select pivots. Once pivots have been chosen, they can be applied in a communication-efficient fashion. For an n×p submatrix on P processors, we show our methods perform a factorization using O(logP) messages instead of the O(p logP) for threshold partial pivoting. The additional costs in terms of operations and communication bandwidth are relatively small. A stability proof is given and numerical results using a range of symmetric indefinite matrices arising from practical problems are used to demonstrate the practical robustness. Timing results on large random examples illustrate the potential speedup on current multicore machines...|$|E
40|$|ENGLISH: Simplex {{algorithm}} is a mathematical procedure for solving linear programming problems {{over and over}} {{with a way to}} test the angle points that meet the constraints to find the extreme point of the corner points that will maximize or minimize the objective function. Mathematical model of linear programming problems must first be modified in order to wake-matrix contains the identity of mathematics must be solved by using simplex algorithm. Building a slack variable is formed by bringing mathematics, surplus variables and variables in the form of artificial constraints that limit the need and requirement. In this case, the presence of an artificial variable as a variable which is zero at the optimal solution requires the use of a number M, ie {{a very large number of}} often called "Big M, the coefficient of artificial variables in the objective function. If the objective function is maximized, then - M is the coefficient of artificial variables. Conversely, if the objective function is minimized, then + M is the coefficient. Optimality condition: Entering Variable in maximizing (minimize) is a variable coefficient non basis the most negative (positive) in the equation destination z. Coefficient with the same value can be chosen arbitrarily. Optimum value is achieved if all the coefficients in the equation z non basic nonnegative (non positive). Feasibility conditions: either to maximize or minimize problem, the leaving variable is the current basic variables that have the smallest cut point (the minimum ratio in the denominator is strictly positive) to the variable entries. The same value can be chosen arbitrarily. Pivot variable can be determined using Gauss-Jordan elimination. This method begins by identifying the columns under variable was included as an entry field (trespassing colomn). Lines associated with the variable of the equation called the pivot and elements in the intersection between the entrance and common <b>pivot</b> <b>column</b> called pivot elements. Gauss-Jordan method to make a change on the basis of the use of two types of calculations: 1. pivot equation: new pivot equation= last pivot equation/ pivot element 2. all other equations including z new equation= (last equation-entering colomn coefisien) x new pivot equation Both types of calculations are basically looking for a new basic solution by substituting out the variables included in all equations, except in the pivot variable...|$|E
40|$|This paper gives {{perturbation}} {{analyses for}} Q 1 and R in the QR factorization A = Q 1 R, Q T 1 Q 1 = I, {{for a given}} real m Θ n matrix A of rank n. The analyses more accurately reflect {{the sensitivity of the}} problem than previous normwise results. The condition numbers here are altered by any <b>column</b> <b>pivoting</b> used in AP = Q 1 R, and the condition numbers for R are bounded for a fixed n when the standard <b>column</b> <b>pivoting</b> strategy is used. This strategy tends to improve the condition of Q 1, so the computed Q 1 and R will probably both have greatest accuracy when we use the standard <b>column</b> <b>pivoting</b> strategy. First order normwise perturbation analyses are given for both Q 1 and R. It is seen that the analysis for R may be approached in two ways [...] - a detailed "matrix [...] vector equation" analysis which provides tight bounds and resulting true condition numbers, which unfortunately are costly to compute and not very intuitive, and a perhaps simpler "matrix equation" analysis which provides results that are usually weaker but easier to interpret, and which allow efficient computation of a satisfactory estimate for the true condition number. Key Words. QR factorization, perturbation analysis, condition estimation, matrix equations, pivoting AMS Subject Classifications: 15 A 23, 65 F 35 1...|$|R
40|$|AbstractSeveral {{definitions}} of growth factors for Gaussian elimination are compared. Some new pivoting strategies, intermediate between partial pivoting and rook pivoting, are introduced. For random matrices, an approximation {{of the average}} normalized growth factor associated with several pivoting strategies is computed and analyzed. A stationary behavior of the expected growth factors of the new pivoting strategies is observed. Bounds for the growth factors of these pivoting strategies are provided. It is also shown that partial <b>pivoting</b> by <b>columns</b> produces small growth factors for matrices appearing in practical observations and for which the growth factors produced by partial pivoting are very large...|$|R
40|$|Abstract. This paper proposes an ortho-diffusion {{decomposition}} of graphs for estimating motion from image sequences. Orthonormal de-compositions of the adjacency matrix representations of image data are alternated with diffusions and data subsampling {{in order to}} robustly rep-resent image features using undirected graphs. Modified Gram-Schmidt with <b>pivoting</b> the <b>columns</b> algorithm is applied recursively for the or-thonormal decompositions at various scales. This processing produces a set of ortho-diffusion bases and residual diffusion wavelets at each image representation scale. The optical flow is estimated using the similarity in the ortho-diffusion bases space extracted from regions of two different image frames...|$|R
40|$|A new {{algorithm}} of Demmel et al. for computing {{the singular}} value decomposition (SVD) to high relative accuracy begins by computing a rank-revealing decomposition (RRD). Demmel et al. analyse {{the use of}} Gaussian elimination with complete pivoting (GECP) for computing the RRD. We investigate the use of QR factorization with complete <b>pivoting</b> (that is, <b>column</b> <b>pivoting</b> together with row sorting or row pivoting) {{as an alternative to}} GECP, since this leads to a faster SVD algorithm. We derive a new componentwise backward error result for Householder QR factorization and combine it with the theory of Demmel et al. to show that high relative accuracy in the computed SVD can be expected for matrices that are diagonal scalings of a well-conditioned matrix. An a posteriori error bound is derived that gives useful estimates of the relative accuracy of the computed singular values. Numerical experiments confirm the theoretical predictions...|$|R
