0|478|Public
40|$|A {{high speed}} MOS digital circuit {{simulation}} program PNAP- 1 (Parallel Network Analysis Program) implemented on a special-purpose parallel computer system LINKS- 1 is described. In LINKS- 1, 64 <b>node</b> <b>computers</b> are linked radially to a root computer {{through a very}} fast memory swapping unit. The task of the root computer is to control <b>node</b> <b>computer</b> and to store waveforms. Each <b>node</b> <b>computer</b> solves assigned subcircuit with standard circuit simulation techniques. PNAP- 1 adopts a block-type waveform relaxation technique as a vehicle of the parallel computation of required waveforms. The slow convergence or more serious non-convergence problem inherent to relaxation-based methods is alleviated by analyzing tightly coupled portions in direct manner in conjunction with exploiting the overall quasi-unidirectional property of MOS digital circuits. A number of experimental results show that PNAP- 1 can achieve a substantial speed improvement in the cost-effective analysis of VLSI circuits...|$|R
40|$|Distributed {{computer}} architectures are well {{accepted in the}} domain of real-time applications. To realize fault-tolerance, fail-silent <b>node</b> <b>computers</b> providing the same service can be clustered into Fault-Tolerant Units (FTUs). Each FTU provides a specified service as long as at least one of its <b>node</b> <b>computers</b> is operational. The communication between these FTUs has to be deterministic, reliable and timely, i. e. there must be a tight upper bound on {{the time it takes to}} send a message from one FTU to the other FTUs. This paper presents a communication system suitable for real-time applications that meets these requirements...|$|R
5000|$|Many {{companies}} issue typical laptops {{and only}} allow those specific computers to remotely connect. For example, the US Department of Defense only allows its remote computers to connect via VPN to its network (no direct Internet browsing) and uses two-factor authentication. [...] Some organizations use server-side tools to scan and/or validate the end <b>node's</b> <b>computer,</b> such as {{communicating with the}} node's TPM.|$|R
5000|$|Within a vast {{computer}} network, {{the individual}} computers {{on the periphery}} of the network, those that do not also connect other networks, and those that often connect transiently to one or more clouds are called end nodes. Typically, within the cloud computing construct, the individual user / customer computer that connects into one well-managed cloud is called an end <b>node.</b> Since these <b>computers</b> are a part of the network yet unmanaged by the cloud's host, they present significant risks to the entire cloud. This is called the End Node Problem. [...] There are several means to remedy this problem but all require instilling trust in the end <b>node</b> <b>computer.</b>|$|R
50|$|The {{components}} of a cluster are usually connected to each other through fast local area networks, with each <b>node</b> (<b>computer</b> used as a server) running its own instance of an operating system. In most circumstances, all of the nodes use the same hardware and the same operating system, although in some setups (i.e. using Open Source Cluster Application Resources (OSCAR)), different operating systems {{can be used on}} each computer, or different hardware.|$|R
40|$|A {{number of}} routing {{protocols}} {{have been proposed}} for mobile ad hoc networks. In this {{paper we propose a}} hybrid multiple zoning scheme minimizing the number of route request messages and nodes involved in the route discovery process by applying proactive routing between high power nodes and reactive routing between other <b>nodes.</b> <b>Computer</b> simulation using NS 2 reveals that the proposed approach can significantly lower routing overhead compared to previous schemes. Also, the improvement gets more significant as the number of nodes in the network increases...|$|R
40|$|Learning {{decision}} trees against {{very large}} amounts of data is not practical on single <b>node</b> <b>computers</b> due to the huge amount of calculations required by this process. Apache Hadoop is a large scale distributed computing platform that runs on commodity hardware clusters {{and can be used}} successfully for data mining task against very large datasets. This work presents a parallel decision tree learning algorithm expressed in MapReduce programming model that runs on Apache Hadoop platform and has a very good scalability with dataset size...|$|R
5000|$|At {{the center}} of each cluster was a star coupler, to which every <b>node</b> (<b>computer)</b> and data storage device in the cluster was {{connected}} by one or two pairs of CI cables. ("CI" [...] stands for Computer Interconnect.) Each pair of cables had a transmission rate of 70 megabits per second, a high speed for that era. Using two pairs gave an aggregate transmission rate of 140 megabits per second, with redundancy in case one cable failed; the star couplers also had redundant wiring for better availability.|$|R
30|$|Hadoop is an {{open source}} {{distributed}} computing platform developed by Apache Software Foundation. It is built to handle large data sets (or big data) on distributed systems (DS). DS {{is a group of}} connected computing <b>nodes</b> (<b>computers)</b> which may be geographically dispersed. By using Hadoop, the workload of a process is spread across the nodes of a DS. Hence, the completion time is reduced [20]. Sarade et al. [13] proposed the implementation of image processing in distributed computing by using Hadoop. They defined the workflow of the system to consist of the following steps: (i) images being uploaded into the Hadoop Distributed File System (HDFS) (ii) image processing through MapReduce (iii) presentation of results. The paper focused on the processing of satellite images particularly on image scaling, feature extraction and image recognition. Similar experimental setup was used in [20] where multiple sets of Hadoop cluster were formed to observe the execution time as the number of computing nodes changed. The results showed that a traditional single <b>node</b> <b>computer</b> took 1967  s to process 800 images while a Hadoop cluster containing 10 nodes took only 253  s. Besides reducing execution times through distributed computing, Hadoop also supports distributed machine learning through Apache Mahout [5].|$|R
40|$|Distributed {{integrated}} architectures in {{the automotive}} and avionic domain result in hardware cost reduction, dependability improvements, and improved coordination between application subsystems compared to federated systems. In {{order to support}} safety-critical application subsystems, a distributed integrated architecture needs to support fault-tolerance strategies that enable the continued operation of the system {{in the presence of}} failures. The basis for the implementation and validation of faulttolerance strategies are realistic fault assumptions, which are captured in a fault hypothesis. This paper describes a fault hypothesis for distributed integrated architectures, which takes into account the sharing of the communication and computational resources of a single distributed computer system among multiple application subsystems. Each <b>node</b> <b>computer</b> serves for the execution of multiple jobs. In analogy, the communication network interconnecting the <b>node</b> <b>computers</b> has to support message exchanges of more than one application subsystem. Using a generic system model of a distributed integrated architecture, we argue in favor of a differentiation of fault containment regions for hardware and software faults. Based on these fault containment regions, we discuss the failure modes, the failure rates, the maximum number of failures, and the recovery intervals. In particular, the fault hypothesis describes the assumptions concerning the respective frequencies of transient and permanent failures in consideration of recent semiconductor trends...|$|R
50|$|System {{information}} for <b>computer</b> <b>nodes</b> and clusters.|$|R
40|$|Consider a {{communication}} medium shared among {{a set of}} computer nodes; these <b>computer</b> <b>nodes</b> issue messages that are requested to be transmitted and they must finish their transmission before their respective deadlines. TDMA/SS is a protocol that solves this problem; it is {{a specific type of}} Time Division Multiple Access (TDMA) where a <b>computer</b> <b>node</b> is allowed to skip its time slot and then this time slot can be used by another <b>computer</b> <b>node.</b> We present an algorithm that computes exact queuing times for TDMA/SS in conjunction with Rate-Monotonic (RM) or Earliest- Deadline-First (EDF) ...|$|R
40|$|In this paper, we {{describe}} an effective method of using Self-Organizing Map (SOM) to group websites {{so as to}} eliminate or at least ease up slow speed, {{one of the fundamental}} problems, by using a MapReduce programming model. The proposed MapReduce SOM algorithm has been successfully applied to cluB, which is a typical SOM tool. Performance evaluation shows the proposed SOM algorithm took less time to complete computational processing (i. e. distributed computing) on large data sets in comparison with conventional algorithms, and performance improved by up to 20 percent with increasing <b>nodes</b> (<b>computers)</b> ...|$|R
30|$|To {{implement}} the proposed approach, MySQL database in Ubuntu has been used. Ubuntu is installed over VMware and all involved <b>nodes</b> (<b>computers)</b> are configured with heterogeneity. Factors of heterogeneity are allocated CPU power, allocated RAM and disk space and network bandwidth. To test and generate report python {{is used as}} scripting language. The test bed comprise of 90 nodes for the distributed multi-tenant data base. The processing capability ranges from 500 MHz to 2.4 GHz for processor, 500 MB to 1500 MB for RAM and 10 - 30 GB of free disk space as shown in Table 4.|$|R
40|$|Common {{techniques}} to realize fault-tolerance in distributed real-time systems are the replication of components (<b>node</b> <b>computers)</b> and the interconnection of these components by redundant communication channels. The {{communication between the}} components has to be reliable and predictable in its timing behavior. We describe a communication protocol which is suitable for synchronous distributed architectures. It provides reliable, deterministic, and timely multicast (one-to-many) communication between components {{which is based on}} sending of redundant broadcast messages. This paper presents the results of a study of the reliability of the communication taking into account failures in the communication network and components, as well as their on-line repair...|$|R
5000|$|A {{complex system}} is usually {{composed}} of many components and their interactions. Such a {{system can be}} represented by a network where nodes represent the components and links represent their interactions. for example, the INTERNET can be represented as a network composed of <b>nodes</b> (<b>computers)</b> and links (direct connections between computers). Its resilience to failures was studied using percolation theory in.Other examples are social networks, airline networks, biological networks and climate networks.Networks can also fail and recover spontaneously. For modeling this phenomenon see ref.Interacting complex systems can be modeled as networks of networks. For their breakdown and recovery properties see ...|$|R
5000|$|Over 650 <b>computer</b> <b>nodes</b> {{including}} specialist Macs and 7 discreet ICT rooms.|$|R
5000|$|Large scale {{combinatorial}} searches: Keys being candidate {{solutions to}} a problem and each key mapping to the <b>node,</b> or <b>computer,</b> {{that is responsible for}} evaluating them as a solution or not. e.g. Code Breaking ...|$|R
30|$|Traditionally, the {{networks}} were formed using homogenous <b>nodes</b> (<b>computers).</b> However, {{due to the}} emergence of 5 G and the Internet of Things (IoT) paradigms, heterogeneous networks comprising heterogeneous nodes such as sensors, phones, computers and satellites form, thereby providing various ubiquitous services. These wireless ad hoc networks are fully applicable in the field of the IoT. In the IoT, virtual objects, services, processes and devices are considered as nodes that are interconnected through the Internet. Soon, the IoT will amalgamate different technologies wirelessly in which ad hoc networks will play an integral part. Examples of such systems are smart cities, the Internet of connected vehicles (intelligent transportation system), etc. [4, 5].|$|R
30|$|In {{terms of}} {{execution}} times, Hadoop {{proved to be}} a reliable distributed computing platform. The execution times recorded in the experiment showed impressive improvements as the number of nodes increased. We achieved almost a speedup of six when a 19 -node cluster was in place compared to a single <b>node</b> <b>computer.</b> In general, we obtained a linear speedup as we added more computing nodes in the cluster. The overall processing times also showed exponential decrease across the clusters. Besides that, all cluster resources were utilized during feature extraction. The results provided a strong evidence that more resources can be put to use to further shorten the processing times.|$|R
40|$|In this paper, we {{consider}} a multihop {{wireless sensor network}} (WSN) with multiple relay nodes for each hop where the amplify-and-forward (AF) scheme is employed. We present a strategy to jointly design the linear receiver and the power allocation parameters via an alternating optimization approach that maximizes the sum rate of the WSN. We derive constrained maximum sum-rate (MSR) expressions along with an algorithm to compute the linear receiver and the power allocation parameters with the optimal complex amplification coefficients for each relay <b>node.</b> <b>Computer</b> simulations show good performance of our proposed methods in terms of sum rate compared to the method with equal power allocation. Comment: 3 figure...|$|R
50|$|The Tianhe-1A {{system is}} {{composed}} of 112 computer cabinets, 12 storage cabinets, 6 communications cabinets, and 8 I/O cabinets. Each computer cabinet {{is composed of}} four frames, with each frame containing eight blades, plus a 16-port switching board. Each blade is composed of two <b>computer</b> <b>nodes,</b> with each <b>computer</b> <b>node</b> containing two Xeon X5670 6-core processors and one Nvidia M2050 GPU processor. The system has 3584 total blades containing 7168 GPUs, and 14,336 CPUs, managed by the SLURM job scheduler. The total disk storage of the systems is 2 Petabytes implemented as a Lustre clustered file system, and the total memory size {{of the system is}} 262 Terabytes.|$|R
50|$|When {{the company}} {{launched}} and evolved its technology, {{there were no}} off-the-shelf computing systems and integrated software and sound cards. Consequently, all of the hardware from the company's main real-time CPU, all input and output cards, analog-to-digital and digital-to-analog cards {{and all of its}} memory cards were all developed internally, as well as all of the software, certainly a monumental task. The hardware and software of the company's real-time capability was used in other fields completely remote to music, such as the main Dartmouth College campus computing <b>node</b> <b>computers</b> for one of the USA's first campus-wide computing networks, and in medical data acquisition research projects.|$|R
40|$|We {{present a}} simple {{algorithm}} for constructing Hamiltonian cycles on trees together with additional edges between leaves, compute {{the number of}} such additional edges required for complete k-ary trees of given depth, show this number is minimal over all Hamiltonian cycles, and present an algorithm that determines the successor of each node in the cycle from the node index alone. We discuss the applications of these algorithms to parallel computing. 1 Introduction Many parallel algorithms need to transfer data in parallel from each <b>node</b> of the <b>computer</b> on which they are running to each other <b>node</b> of the <b>computer</b> in turn. Standard examples are algorithms to compute the cartesian product of two sets, each initially distributed over the <b>nodes</b> of the <b>computer,</b> or the join of two relations, each similarly initially distributed. Such algorithms must transfer data along a Hamiltonian cycle through the <b>nodes</b> of the <b>computer.</b> The questions then arise: which common interconnection topologies for [...] ...|$|R
30|$|We have {{developed}} a software-based coincidence processor that should have adequate performance to process list-mode singles data at near real-time provided that we have enough <b>computer</b> <b>nodes.</b> At a singles data rate of approximately 150 Mcps (equivalent to a 370 -MBq injection evenly distributed in an adult phantom), this can be achieved with 2 <b>computer</b> <b>nodes</b> running dual Intel Xeon E 5 - 2650 v 4 CPUs.|$|R
40|$|In most {{application}} {{scenarios of}} {{wireless sensor networks}} (WSN), sensor nodes are usually deployed randomly {{and do not have}} any knowledge about the network environment or even their ID's at the initial stage of their operations. In this paper, we address the clustering problems with a newly deployed multi-hop WSN where most existing clustering algorithms can hardly be used due to the absence of MAC link connections among the nodes. We propose an effective clustering algorithm based on a random contention model without the prior knowledge of the network and the ID's of <b>nodes.</b> <b>Computer</b> simulations have been used to show the effectiveness of the algorithm with a relatively low complexity if compared with existing schemes...|$|R
40|$|This paper {{discusses}} {{measures to}} make a distributed system based on the Time-Triggered Architecture resis-tant to arbitrary node failures. To achieve this, the pre-sented approach introduces a central guardian {{as part of the}} interconnection network. This guardian acts as a supervising unit to <b>node</b> <b>computers</b> by checking for fault hypothesis compliance at their respective network interfaces. By implementing appropriate algorithms the guardian is able to transform failure modes of nodes that cannot be tolerated by the fault hypothesis of the TTP/C protocol. This transformation ensures that – at the in-terface to correct nodes – even an arbitrarily faulty node will then be compliant to the fault hypothesis of the TTP/C communications protocol. ...|$|R
30|$|An author connects a <b>computer</b> <b>node</b> to {{multiple}} player nodes to create statement choices for a player.|$|R
30|$|An edge from a <b>computer</b> <b>node</b> to {{a player}} node (and vice-versa) depicts {{a flow of}} conversation.|$|R
5000|$|Computing {{facilities}} at the CGWA include a 41-node cluster [...] "Funes" [...] which was {{installed at the}} beginning of 2004, and an older cluster, [...] "Lobizon", with 96 <b>nodes.</b> These <b>computers</b> are used mainly for source modeling and for numerical relativity simulations.|$|R
30|$|Within a subject, {{an author}} {{develops}} {{a dialogue between}} a virtual character (<b>computer</b> <b>node)</b> and a player (player node).|$|R
50|$|Packet {{forwarding}} is the relaying of packets {{from one}} network segment to another by <b>nodes</b> in a <b>computer</b> network.|$|R
5000|$|For small {{transmission}} {{systems of}} about less than 10 nodes or buses, the Y matrix {{can be calculated}} manually. But for a realistic system with relatively large number of nodes or buses, say 1000 <b>nodes,</b> a <b>computer</b> program for computing Y is more practical to use.|$|R
5000|$|On-campus {{accommodation}} {{for girls}} is provided. All rooms {{are equipped with}} a bed, study table, <b>computer</b> <b>node</b> and an attached bath.|$|R
40|$|Distributed {{software}} systems enable enterprise-wide computing using business applications {{installed in}} different geographical areas. The complexity of distribution {{in this type}} of software highlights the significance of managing cooperation between distributed <b>nodes.</b> <b>Computer</b> supported cooperative work (cscw) transfers business requirements for cooperation to computer-based technical solutions. In such dynamic and distributed systems, when exceptional faults occur - cooperation is hard to achieve without enough knowledge about the context, which is called awareness. The dynamic nature of distributed software systems directs us to use agents for management purposes. In this paper, we propose a behavioral model of awareness that agents need to possess for effective fault management while they are engaged in cooperative work. The main contribution of this paper is presented through an example scenario in network management...|$|R
40|$|MPI (Message Passing Interface) is a {{proposed}} message passing standard {{for development of}} efficient and portable parallel programs. An implementation of MPI is presented and evaluated for the Meiko CS/ 2, a 64 <b>node</b> parallel <b>computer,</b> and a network of 8 SGI workstations connected by an ATM switch and Ethernet. 1...|$|R
