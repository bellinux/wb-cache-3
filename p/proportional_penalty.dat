2|35|Public
50|$|With {{these types}} of {{contracts}} thesupplier offers that it will buy back the remainingobsolete inventory at a discounted price. Thissupports the sharing of inventory risk between thepartners. A variation of this contract is thebackup agreement, where the customer gives apreliminary forecast and then makes an order less orequal to the forecasted quantity. If the order isless, it must also pay a <b>proportional</b> <b>penalty</b> for theremaining obsolete inventory. Buyback agreements arewidespread in the newspaper, book, CD and fashionindustries.|$|E
40|$|Guaranteed Minimum Withdrawal Benefits (GMWB) {{are popular}} riders in {{variable}} annuities with withdrawal guarantees. With withdrawals {{spread over the}} life of the annuities contract, the benefit promises to return the entire initial annuitization amount irrespective of the market performance of the underlying fund portfolio. Treating the dynamic withdrawal rate as the control variable, the earlier works on GMWB have considered the construction of a continuous singular stochastic control model and the numerical solution of the resulting pricing model. This paper presents a more detailed characterization of the pricing properties of the GMWB and performs a full mathematical analysis of the optimal dynamic withdrawal policies under the competing factors of time value of fund, optionality value provided by the guarantee and penalty charge on excessive withdrawal. When a <b>proportional</b> <b>penalty</b> charge is applied on any withdrawal amount, we can reduce the pricing formulation to an optimal stopping problem with lower and upper obstacles. We then derive the integral equations for the determination of a pair of optimal withdrawal boundaries. When a <b>proportional</b> <b>penalty</b> charge is applied on the amount that is above the contractual withdrawal rate, we manage to characterize the behavior of the optimal withdrawal boundaries that separate the domain of the pricing models into three regions: no withdrawal, continuous withdrawal at the contractual rate and an immediate withdrawal of a finite amount. Under certain limiting scenarios such as a high policy fund value, the time close to expiry, or a low value of guarantee account, we manage to obtain analytical approximate solution to the singular stochastic control model of dynamic withdrawals. (C) 2014 Elsevier B. V. All rights reserved...|$|E
5000|$|Providing for <b>proportional</b> {{criminal}} <b>penalties</b> to {{be applied}} to persons found guilty of trafficking in aggravating circumstances, including offences involving trafficking in children or offences committed or involving complicity by state officials ...|$|R
40|$|We present StarScan, a novel scan {{statistic}} for accurately detecting irregularly-shaped disease outbreaks. StarScan maximizes a penalized log-likelihood ratio statistic, {{allowing the}} radius around a central location to vary {{as a function}} of the angle and applying a <b>penalty</b> <b>proportional</b> to the total change in radius...|$|R
40|$|Government {{authority}} in China, while constitutionally organized as a unitary sovereign, is, in practice, a complex system of informal and formal divisions of authority between national, provincial, and local political actors. In {{the context of}} water pollution control, an issue of considerable interest in China, both central and subnational authorities have key roles. The incentives faced by some officials, however, are ill-aligned with environmental protection, predictably leading to inefficiently high levels of pollution. Recent changes in China 2 ̆ 7 s water pollution regime {{have the potential to}} create a more successful cooperative arrangement between the national and subnational governments. These reforms impose stronger economic and bureaucratic discipline on subnational authorities for environmental outcomes, yet preserve large degrees of discretion for achieving central targets. This approach maintains a largely decentralized system while helping to counteract {{some of the problems that}} have undermined China 2 ̆ 7 s water pollution efforts in the past. Although these reforms have strong potential, they can be improved with stronger environmental incentives for national officials, less intra-bureaucratic tension, expanded river basin planning, and experimentation with compensation mechanisms and trading to reduce regional disparities. In addition, information collection, the creation of more <b>proportional</b> <b>penalties</b> for non-compliant subnational actors, and an expanded role for cost-benefit analysis can help alleviate some of the shortfalls of the existing law...|$|R
40|$|Service level {{agreements}} (SLAs) {{are widely}} employed forms of performance‐based contracts in operations management. They compare performance {{during a period}} against a contracted service level and penalize outcomes exceeding some allowed deviation. SLAs {{have a number of}} design characteristics that need careful tuning to ensure that incentives are properly aligned. However, there is little theoretical research in this area. Using an example of an SLA for outsourcing inventory management, we make a number of recommendations. First it is preferable, if possible, that <b>penalties</b> be <b>proportional</b> to the underperformance rather than lump‐sum ones. This goes a long way towards mitigating strategic (“gaming”) behavior by the supplier. Second, it might be thought that giving “bonuses for good performance” rather than “penalties for bad performance” are essentially identical apart from the former being a more positive approach to management. This turns out to be incorrect in the case of large percentage service rate targets and that penalties will normally be preferred by the buying firm. Third, in order not to incorrectly penalize underperformance resulting purely from “noise” rather than supplier efforts, management might think it best to make allowed deviations from the target generous. Again intuition is not a helpful guide here: for <b>proportional</b> <b>penalties,</b> acceptable performance deviations should be close to the target. Although these results come from a particular inventory application, {{it is likely that the}} lessons are applicable to SLAs in general...|$|R
40|$|Conventional power {{distribution}} and control systems as employed in Apollo space vehicles and modern aircraft are reviewed, and arguments {{are presented in}} favor of applying some new techniques {{in the design of}} electric power systems for future manned spacecraft. Current Space Shuttle studies show that each pound of weight for supporting subsystems causes overall vehicle weight at launch to increase by 34. 5 lb, with <b>proportional</b> cost <b>penalty.</b> The presently available technology for an improved automatic or semiautomatic distribution and control system should be implemented. A systems approach based on the most current technology is discussed along with a unique method of system development, testing, and evaluation...|$|R
40|$|We {{consider}} the asymptotic behavior of regression estimators that minimize the residual sum of squares plus a <b>penalty</b> <b>proportional</b> to ∑ βjγ for some γ> 0. These estimators include the Lasso {{as a special}} case when γ = 1. Under appropriate conditions, we show that the limiting distribu-tions can have positive probability mass at 0 when the true value of the parameter is 0. We also consider asymptotics for “nearly singular ” designs...|$|R
40|$|We wish to {{estimate}} conditional density using Gaussian Mixture Regression model with logistic weights and means {{depending on the}} covariate. We aim at selecting the number of components of this model {{as well as the}} other parameters by a penalized maximum likelihood approach. We provide a lower bound on <b>penalty,</b> <b>proportional</b> up to a logarithmic term to the dimension of each model, that ensures an oracle inequality for our estimator. Our theoretical analysis is supported by some numerical experiments...|$|R
40|$|Abstract. We study {{convergence}} {{properties of}} a new nonlinear La-grangian method for nonconvex semidefinite programming. The con-vergence analysis shows that this method converges locally when the penalty parameter {{is less than a}} threshold and the error bound of so-lution is <b>proportional</b> to the <b>penalty</b> parameter under the constraint nondegeneracy condition, the strict complementarity condition and the strong second order sufficient conditions. The major tools used in the analysis include the second implicit function theorem and differentials of Löwner operators. 1...|$|R
40|$|AbstractThis paper {{analyzes}} {{the rate of}} local convergence of the Log-Sigmoid nonlinear Lagrange method for nonconvex nonlinear second-order cone programming. Under the componentwise strict complementarity condition, the constraint nondegeneracy condition and the second-order sufficient condition, we show that the sequence of iteration points generated by the proposed method locally converges to a local solution when the penalty parameter {{is less than a}} threshold and the error bound of solution is <b>proportional</b> to the <b>penalty</b> parameter. Finally, we report numerical results to show the efficiency of the method...|$|R
40|$|In this paper, we {{employ the}} Heston {{stochastic}} volatility model {{to describe the}} stock's volatility and apply the model to derive and analyze the optimal trading strategies for dealers in a security market. We also extend our study to option market making for options written on stocks {{in the presence of}} stochastic volatility. Mathematically, the problem is formulated as a stochastic optimal control problem and the controlled state process is the dealer's mark-to-market wealth. Dealers in the security market can optimally determine their ask and bid quotes on the underlying stocks or options continuously over time. Their objective is to maximize an expected profit from transactions with a <b>penalty</b> <b>proportional</b> to the variance of cumulative inventory cost...|$|R
40|$|This paper {{considers}} Arkin and Roundy's single machine weighted tardiness scheduling {{model with}} tardiness <b>penalties</b> <b>proportional</b> to the processing times. It presents a two-stage decomposition mechanism that {{proves to be}} powerful in solving the problem completely or reducing it to a much smaller problem. Three types of orderings of adjacent jobs are derived that {{play a crucial role}} in problem decomposition. The decomposition method solves 155 out of 320 test problems with job sizes ranging from 20 to 150. It reduces 163 unsolved problems to smaller subproblems with sizes not exceeding 25 jobs. The job sizes of the remaining two unsolved subproblems are 30 and 45. scheduling problem, precedence relation, proportional weights...|$|R
40|$|This paper {{describes}} a machine translation metric {{submitted to the}} WMT 14 Metrics Task. It is a simple modification of the standard BLEU metric using a monolin-gual alignment of reference and test sen-tences. The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the refer-ence sentence words {{with respect to the}} relative edit distance of the word prefixes and suffixes. The aligned words are in-cluded in the n-gram precision compu-tation with a <b>penalty</b> <b>proportional</b> to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measur-ing quality of translation into morphologi-cally richer languages. ...|$|R
40|$|We {{consider}} {{the problem of}} portfolio selection within the classical Markowitz mean-variance framework, reformulated as a constrained least-squares regression problem. We propose {{to add to the}} objective function a <b>penalty</b> <b>proportional</b> to the sum of the absolute values of the portfolio weights. This penalty regularizes (stabilizes) the optimization problem, encourages sparse portfolios (i. e. portfolios with only few active positions), and allows to account for transaction costs. Our approach recovers as special cases the no-short-positions portfolios, but does allow for short positions in limited number. We implement this methodology on two benchmark data sets constructed by Fama and French. Using only a modest amount of training data, we construct portfolios whose out-of-sample performance, as measured by Sharpe ratio, is consistently and significantly better than that of the naive evenly-weighted portfolio which constitutes, as shown in recent literature, a very tough benchmark. ...|$|R
40|$|Abstract. By {{means of}} a {{laboratory}} experiment, we study {{the impact of the}} endogenous adoption of a collective punishment mechanism within a one-shot binary trust game. The experiment comprises three games. In the first one, the only equilibrium strategy is not to trust, and not to reciprocate. In the second we exogenously introduce a sanctioning rule that imposes on untrustworthy second-movers a <b>penalty</b> <b>proportional</b> to the number of those who reciprocate trust. This generates a second equilibrium where everybody trusts and reciprocates. In the third game, the collective punishment mechanism is adopted through majority-voting. In line with the theory, we find that the exogenous introduction of the punishment mechanism significantly increases trustworthiness, {{and to a lesser extent}} also trust. However, in the third game the majority of subjects vote against it: subjects seem to be unable to endogenously adopt an institution which, when exogenously imposed, proves to be efficiency enhancing...|$|R
40|$|Fitness {{effects of}} {{mutations}} fall {{on a continuum}} ranging from lethal to deleterious to beneficial. The distribution of fitness effects (DFE) among random mutations is {{an essential component of}} every evolutionary model and a mathematical portrait of robustness. Recent experiments on five viral species all revealed a characteristic bimodal shaped DFE, featuring peaks at neutrality and lethality. However, the phenotypic causes underlying observed fitness effects are still unknown, and presumably thought to vary unpredictably from one mutation to another. By combining population genetics simulations with a simple biophysical protein folding model, we show that protein thermodynamic stability accounts for a large fraction of observed mutational effects. We assume that moderately destabilizing mutations inflict a fitness <b>penalty</b> <b>proportional</b> to the reduction in folded protein, which depends continuously on folding free energy (Δ G). Most mutations in our model affect fitness by altering Δ G, while, based on simple estimates, ≈ 10...|$|R
40|$|Cartel ringleaders {{can apply}} for amnesty in some jurisdictions (e. g., the E. U.), whereas in others they are {{excluded}} (e. g., the U. S.). This paper provides a survey of identified ringleaders in recent European cartel cases and explores theoretically the effect of ringleader exclusion on collusive prices. Our survey shows that (i) cartels often {{had more than one}} ringleader, (ii) the role of ringleaders was very diverse and (iii) ringleaders were typically the largest cartel members. Our theoretical analysis reveals that ringleader exclusion leads to higher prices when (iv) the joint profit maximum cannot be sustained under a nondiscriminatory leniency policy, (v) antitrust fines depend on individual cartel gains in a nonlinear fashion and (vi) the size distribution of members is sufficiently heterogeneous. These findings support the imposition of antitrust <b>penalties</b> <b>proportional</b> to firm size when ringleaders are excluded from the corporate leniency program...|$|R
40|$|We {{consider}} {{the problem of}} optimal load balancing in a server farm under overload conditions. A convex penalty minimization problem is studied to optimize queue overflow rates at the servers. We introduce {{a new class of}} α-fair penalty functions, and show that the cases of α = 0, 1, ∞ correspond to minimum sum <b>penalty,</b> <b>penalty</b> <b>proportional</b> fairness, and min-max fairness, respectively. These functions are useful to maximize the time to first buffer overflow and minimize the recovery time from temporary overload. In addition, we show that any policy that solves an overload minimization problem with strictly increasing penalty functions must be throughput optimal. A dynamic control policy is developed to solve the overload minimization problem in a stochastic setting. This policy generalizes the well-known join-the-shortest-queue (JSQ) policy and uses intelligent job tagging to optimize queue overflow rates without the knowledge of traffic arrival rates. © 2014 IFIP...|$|R
40|$|We {{formulate}} {{and study}} a multiobjective programming approach for production processes which implements suitable constraints on pollutant emissions. We consider two alternative optimization problems: (a) minimum pollution risk; (b) maximum expected return. For each pollutant, we define three different contamination levels: (a) the desirable or the target pollution level, (b) the alarm (warning or critical) level and (c) the maximum admissible (acceptable) level, and introduce <b>penalties</b> <b>proportional</b> to {{the amounts of}} pollutants that exceed these levels. The objective function of the minimum pollution risk problem is not smooth since it contains positive parts of some affine functions, resulting in mathematical difficulties, which can be solved by formulating an alternative linear programming model, which makes use of additional variables and has the same solutions as the initial problem. We investigate various particular cases and analyze a numerical example for a textile plant. Multiobjective programming Production planning Environmental constraints Pollutant emission...|$|R
40|$|Traditional Poster Session: Pulse Sequences & Reconstruction - Multi-Band MRI: no. 2411 PROPELLER MRI {{is widely}} used {{nowadays}} for motion correction. One major drawback of PROPELLER MRI is the long scan time. Previous studies have investigated in-plane acceleration (SENSE and GRAPPA) in PROPELLER 1, 5. However, multi-band (MB) simultaneous multi-slice acquisition, without SNR <b>penalty</b> <b>proportional</b> to square root of acceleration ratio, can be a more suitable solution for accelerating PROPELLER. In addition, MB acquisition combining PINS RF pulses 2 can reduce RF pulse power deposition, which is particularly useful for alleviating the SAR issue in FSE-PROPELLER at high field. A considerably low g-factor is possible in MB PROPELLER because its rotating phase encoding directions can consequently lead to a well-conditioned unwrapping problem. Another advantage of MB PROPELLER is that 3 D coil sensitivity maps (CSMs) can be directly estimated from the oversampled k-space center, without acquiring additional calibration data...|$|R
40|$|We {{present a}} new {{technique}} for extracting disk tissue from sagittal MR spine images. This technique uses linear Kalman filters to estimate initial conditions for a two-dimensional active contour model. We extend the contour model by computing an energy <b>penalty</b> <b>proportional</b> to the mismatch between a generic model and the patient specific model (using a chi-squared error measure). We compared the output of our system to human-guided manual segmentation. We performed 30 experiments (using a T 1 weighted 3 DFGRE pulse sequence), varying the number and location of disk boundary seedpoints as input to the Kalman filter. The output of the filter initialized the active contour models for the disks to be segmented. For each experiment, we measured the average RMS error between the computed and human-detected boundaries. In all cases, the errors were less than 0. 25 mm and the entire disc body could be extracted in under 10 minutes. Our results demonstrate that Kalman filters {{can be used to}} guide [...] ...|$|R
40|$|In this paper, we {{introduce}} the Vehicle Routing Problem with Flexible Time Windows (VRPFlexTW), in which vehicles {{are allowed to}} deviate from customer time windows by a given tolerance. This flexibility enables savings in the operational costs of carriers, since customers may be served {{before and after the}} earliest and latest time window bounds, respectively. However, as time window deviations are undesired from a customer service perspective, a <b>penalty</b> <b>proportional</b> to these deviations is accounted for in the objective function. We develop a solution procedure, in which feasible vehicle routes are constructed via a tabu search algorithm. Furthermore, we propose a linear programming model to handle the detailed scheduling of customer visits for given routes. We validate our solution procedure by a number of Vehicle Routing Problem with Time Windows (VRPTW) benchmark instances. We highlight the costs involved in integrating flexibility in time windows and underline the advantages of the VRPFlexTW, when compared to the VRPTW. © 2014 Elsevier Ltd...|$|R
40|$|We {{consider}} the optimality of the (s, S) policy for a periodic-review stochastic inventory problem with {{two types of}} shortage costs. The problem may arise in a rush-order application at a bank branch where the emergency provision costs during a foreign currency stockout are represented by <b>proportional</b> and lump-sum <b>penalties.</b> Aneja and Noori (1987) analyzed this problem and presented a set of conditions for the convexity of a particular function and made a claim about the K-convexity of another function to prove the optimality of the (s, S) policy. We show that because the function that is claimed to be K-convex is actually concave over a subset of its domain, Aneja and Noori's arguments cannot be used to prove the optimality of the (s, S) policy. However, we argue that Aneja and Noori's problem {{is equivalent to the}} typical lost-sales problem, and using this equivalence, we. nd a simple convexity condition that assures the optimality of the (s, S) policy. Stochastic Inventory Models, K-Convexity...|$|R
40|$|In this paper, we {{approach}} the gate sizing problem in VLSI circuits considering the variability of process parameters in the nano technology. We follow a penalty based approach in which violation of the timing/leakage constraints {{is associated with a}} <b>penalty</b> <b>proportional</b> to the degree of violation. We show that minimization of the expected value of this penalty can be optimally achieved due to inherent convexity of the expected penalty function without making any assumptions on the nature of variability and correlations. Such an approach is ideal in situations where the chips violating the timing and leakage constraints are sold at a loss (and not simply discarded). Comparision with state of the art sensitivity based approach demonstrate an improvement of 73. 1 % in the expected penalty/loss (also called cumulative yield loss) with an area overhead of 1. 8 %. Our approach also shows a speed-up of 2. 41 times. We also show that minimization of the cumulative yield loss also improves the traditional yield loss of the circuit with a 68. 86 % improvement from a sensitivity based approach...|$|R
40|$|This article {{addresses}} {{the problem of}} portfolio construction {{in the context of}} efficient hedge fund investmentsreplication. We propose a modification to the standard à la Sharpe ‘style analysis’ where we augment the objectivefunction with a <b>penalty</b> <b>proportional</b> to the sum of the absolute values of the replicating asset weights, i. e. the normof the asset weights vector. This penalty regularizes the optimization problem, with significant impacts on thestability of the resulting asset mix and the risk and return characteristics of the replicating portfolio. Our resultssuggest that the norm-constrained replicating portfolios exhibit significant correlations with their benchmarks, oftenhigher than 0. 9, have a fraction, i. e. about 1 / 2 to 2 / 3, of active positions relative to those determined through thestandard method, and are obtained with turnover which is in some instances about 1 / 4 of that for the standardmethod. Moreover, the extreme risk of the replicating portfolios obtained through the regularization method isalways lower than that exhibited by currently available commercial hedge fund investment replication products...|$|R
40|$|A model inverse design {{problem is}} used to {{investigate}} the effect of flow discontinuities on the optimization process. The optimization involves finding the cross-sectional area distribution of a duct that produces velocities that closely match a targeted velocity distribution. Quasi-one-dimensional flow theory is used, and the target is chosen to have a shock wave in its distribution. The objective function which quantifies {{the difference between the}} targeted and calculated velocity distributions may become non-smooth due to the interaction between the shock and the discretization of the flowfield. This paper offers two techniques to resolve the resulting problems for the optimization algorithms. The first, shock-fitting, involves careful integration of the objective function through the shock wave. The second, coordinate straining with shock penalty, uses a coordinate transformation to align the calculated shock with the target and then adds a <b>penalty</b> <b>proportional</b> to the square of the distance between the shocks. The techniques are tested using several popular sensitivity and optimization methods, including finite-differences, and direct and adjoint discrete sensitivity methods. Two optimization strategies, Gauss-Newton and sequential quadratic programming (SQP), are used to drive the objective function to a minimum...|$|R
40|$|This article {{presents}} a framework and develops a formulation {{to solve a}} path planning problem for multiple heterogeneous Unmanned Vehicles (UVs) with uncertain service times for each vehicle [...] target pair. The vehicles incur a <b>penalty</b> <b>proportional</b> to the duration of their total service time in excess of a preset constant. The vehicles differ in their motion constraints and are located at distinct depots {{at the start of}} the mission. The vehicles may also be equipped with disparate sensors. The objective is to find a tour for each vehicle that starts and ends at its respective depot such that every target is visited and serviced by some vehicle while minimizing the sum of the total travel distance and the expected penalty incurred by all the vehicles. We formulate the problem as a two-stage stochastic program with recourse, present the theoretical properties of the formulation and advantages of using such a formulation, as opposed to a deterministic expected value formulation, to solve the problem. Extensive numerical simulations also corroborate the effectiveness of the proposed approach. Comment: 8 pages, 2 figures, submitted to International Conference on Unmanned Aircraft Systems (ICUAS...|$|R
40|$|In this paper, {{we propose}} a new {{resource}} allocation framework for multimedia systems that perform multiple simultaneous video decoding tasks. We jointly consider the available system resources (e. g. processor cycles) and the video decoding task’s {{characteristics such as}} the sequence’s content, the bit-rate, and the GOP structure, {{in order to determine}} a fair and optimal resource allocation. To this end, we derive a quality-complexity model that determines the quality (in terms of PSNR) that a task can achieve given a certain system resource allocation. We use these quality-complexity models to determine a quality-fair and Pareto optimal resource allocation using the Kalai-Smorodinski Bargaining Solution (KSBS) from axiomatic bargaining theory. The KSBS explicitly considers the resulting multimedia quality when performing a resource allocation and distributes quality-domain <b>penalties</b> <b>proportional</b> to the difference between each video decoding task’s maximum and minimum quality requirements. We compare the KSBS to other fairness policies in the literature and find that because it explicitly considers multimedia quality it provides significantly fairer resource allocations in terms of the resulting PSNR compared to policies that operate solely in the resource domain. To weight the quality impact of the resource allocations to the different decoding tasks depending on application specific requirements or user preferences, we generalize the existing KSBS solution by introducing bargaining powers based on each video sequence’s motion and texture characteristics...|$|R
40|$|This paper {{presents}} an axiomatic characterization {{of a family}} of solutions to two-player quasi-linear social choice problems. In these problems the players select a single action from a set available to them. They may also transfer money between themselves. The solutions form a one-parameter family, where the parameter is a nonnegative number, t. The solutions can be interpreted as follows: Any efficient action can be selected. Based on this action, compute for each player a "best claim for compensation". A claim for compensation is the difference between the value of an alternative action and the selected efficient action, minus a <b>penalty</b> <b>proportional</b> {{to the extent to which}} the alternative action is inefficient. The coefficient of proportionality of this penalty is t. The best claim for compensation for a player is the maximum of this computed claim over all possible alternative actions. The solution, at the parameter value t, is to implement the chosen efficient action and make a monetary transfer equal to the average of these two best claims. The characterization relies on three main axioms. The paper {{presents an}}d justifies these axioms and compares them to related conditions used in other bargaining contexts. In Nash Bargaining Theory, the axioms analagous to these three are in conflict with each other. In contrast, in the quasi-linear social choice setting of this paper, all three conditions can be satisfied simultaneously. ...|$|R
40|$|The Markowitz {{mean-variance}} optimizing framework {{has served}} as the basis for modern portfolio theory for more than 50 years. However, efforts to translate this theoretical foundation into a viable portfolio construction algorithm have been plagued by technical difficulties stemming from the instability of the original optimization problem with respect to the available data. In this paper we address these issues of estimation error by regularizing the Markowitz objective function through the addition of a <b>penalty</b> <b>proportional</b> to the sum of the absolute values of the portfolio weights (l 1 penalty). This penalty stabilizes the optimization problem, encourages sparse portfolios, and facilitates treatment of transaction costs in a transparent way. We implement this methodology using the Fama and French 48 industry portfolios as our securities. Using only a modest amount of training data, we construct portfolios whose out-of-sample performance, as measured by Sharpe ratio, is consistently and significantly better than that of the naïve portfolio comprising equal investments in each available asset. In addition to their excellent performance, these portfolios have {{only a small number of}} active positions, a highly desirable attribute for real life applications. We conclude by discussing a collection of portfolio construction problems which can be naturally translated into optimizations involving l 1 penalties and which can thus be tackled by algorithms similar to those discussed here. Penalized Regression; Portfolio Choice; Sparse Portfolio...|$|R
40|$|Fitness {{effects of}} {{mutations}} fall {{on a continuum}} ranging from lethal to deleterious to beneficial. The distribution of fitness effects (DFE) among random mutations is {{an essential component of}} every evolutionary model and a mathematical portrait of robustness. Recent experiments on five viral species all revealed a characteristic bimodal-shaped DFE featuring peaks at neutrality and lethality. However, the phenotypic causes underlying observed fitness effects are still unknown and presumably, are thought to vary unpredictably from one mutation to another. By combining population genetics simulations with a simple biophysical protein folding model, we show that protein thermodynamic stability accounts for a large fraction of observed mutational effects. We assume that moderately destabilizing mutations inflict a fitness <b>penalty</b> <b>proportional</b> to the reduction in folded protein, which depends continuously on folding free energy (ΔG). Most mutations in our model affect fitness by altering ΔG, whereas based on simple estimates, ∼ 10 % abolish activity and are unconditionally lethal. Mutations pushing ΔG > 0 are also considered lethal. Contrary to neutral network theory, we find that, in mutation/selection/drift steady state, high mutation rates (m) lead to less stable proteins and a more dispersed DFE (i. e., less mutational robustness). Small population size (N) also decreases stability and robustness. In our model, a continuum of nonlethal mutations reduces fitness by ∼ 2 % on average, whereas ∼ 10 – 35 % of mutations are lethal depending on N and m. Compensatory mutations are common in small populations with high mutation rates. More broadly, we conclude that interplay between biophysical and population genetic forces shapes the DFE...|$|R
40|$|As {{wireless}} communication becomes an ever-more evolving and pervasive {{part of the}} existing world, system capacity and Quality of Service (QoS) provisioning are becoming more critically evident. In order to improve system capacity and QoS, it is mandatory that we pay closer attention to operational bandwidth efficiency issues. We address this issue for two operators' spectrum sharing in the same geographical area. We model and analyze interactions between the competitive operators coexisting in the same frequency band as a strategic noncooperative game, where the operators simultaneously share the spectrum dynamically as per their relative requirement. If resources are allocated in a conventional way (static orthogonal allocation), spectrum utilization becomes inefficient when there is load asymmetry between the operators and low inter-operator interference. Theoretically, operators can share resources in a cooperative manner, but pragmatically they are reluctant to reveal their network information to competitors. By using game theory, we design a distributed implementation, in which self-interested operators play strategies and contend for the spectrum resources in a noncooperative manner. We have proposed two game theoretic approaches in the thesis, one using a virtual carrier price; and the other based on a mutual history of favors. The former approach takes into account a <b>penalty</b> <b>proportional</b> to spectrum usage in its utility function, whereas in the latter, operators play strategies based on their history of interactions, i. e., how well the other behaved in the past. Finally, based on the simulations, we assess {{the performance of the}} proposed game theoretic approaches in comparison to existing conventional allocations. Comment: Master Thesis, Published on 25 / 8 / 2014, Aalto University ([URL]...|$|R
40|$|The P-splines of Eilers and Marx (1996) {{combine a}} B-spline basis with a {{discrete}} quadratic penalty {{on the basis}} coefficients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efficient computation, especially for Bayesian stochastic simulation; ii) {{it is possible to}} flexibly `mix-and-match' the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the B-spline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards <b>penalties</b> <b>proportional</b> to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is {{to point out that the}} three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efficient computation with tensor product smoothers of scattered data...|$|R
40|$|Reproduction is {{inherently}} risky, {{in part because}} genomic replication can introduce new mutations that are usually deleterious toward fitness. This risk is especially severe for organisms whose genomes replicate "semi-conservatively," e. g. viruses and bacteria, where no master copy of the genome is preserved. Lethal mutagenesis refers to extinction of populations due to an unbearably high mutation rate (U), and is important both theoretically and clinically, where drugs can extinguish pathogens by increasing their mutation rate. Previous theoretical models of lethal mutagenesis assume infinite population size (N). However, in addition to high U, small N can accelerate extinction by strengthening genetic drift and relaxing selection. Here, we examine how the time until extinction depends jointly on N and U. We first analytically compute the mean time until extinction (τ) in a simplistic model where all mutations are either lethal or neutral. The solution motivates the definition of two distinct regimes: a survival phase and an extinction phase, which differ dramatically in both how τ scales with N and in the coefficient of variation in time until extinction. Next, we perform stochastic population-genetics simulations on a realistic fitness landscape that both (i) features an epistatic distribution of fitness effects that agrees with experimental data on viruses and (ii) {{is based on the}} biophysics of protein folding. More specifically, we assume that mutations inflict fitness <b>penalties</b> <b>proportional</b> {{to the extent that they}} unfold proteins. We find that decreasing N can cause phase transition-like behavior from survival to extinction, which motivates the concept of "lethal isolation. " Furthermore, we find that lethal mutagenesis and lethal isolation interact synergistically, which may have clinical implications for treating infections. Broadly, we conclude that stably folded proteins are only possible in ecological settings that support sufficiently large populations...|$|R
40|$|Deterministic {{forecasts}} of wind production {{for the next}} 72 h at a single wind farm or at the regional level are among the main end-users requirement. However, for an optimal management of wind power production and distribution {{it is important to}} provide, together with a deterministic prediction, a probabilistic one. A deterministic forecast consists of a single value for each time in the future for the variable to be predicted, while probabilistic forecasting informs on probabilities for potential future events. This means providing information about uncertainty (i. e. a forecast of the PDF of power) in addition to the commonly provided single-valued power prediction. A significant probabilistic application is related to the trading of energy in day-ahead electricity markets. It has been shown that, when trading future wind energy production, using probabilistic wind power predictions can lead to higher benefits than those obtained by using deterministic forecasts alone. In fact, by using probabilistic forecasting it is possible to solve economic model equations trying to optimize the revenue for the producer depending, for example, on the specific penalties for forecast errors valid in that market. In this work we have applied a probabilistic wind power forecast systems based on the "analog ensemble" method for bidding wind energy during the day-ahead market {{in the case of a}} wind farm located in Italy. The actual hourly income for the plant is computed considering the actual selling energy prices and <b>penalties</b> <b>proportional</b> to the unbalancing, defined as the difference between the day-ahead offered energy and the actual production. The economic benefit of using a probabilistic approach for the day-ahead energy bidding are evaluated, resulting in an increase of 23 % of the annual income for a wind farm owner in the case of knowing "a priori" the future energy prices. The uncertainty on price forecasting partly reduces the economic benefit gained by using a probabilistic energy forecast system...|$|R
