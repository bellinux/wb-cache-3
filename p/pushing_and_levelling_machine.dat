0|10000|Public
50|$|The game {{follows the}} same {{gameplay}} format of previous games, {{in which the}} player uses Mallo to <b>push</b> <b>and</b> pull parts of the puzzle {{in order to reach}} the top. New to this game is the ability to stretch pieces out in addition to <b>pushing</b> <b>and</b> pulling. Some <b>levels</b> also contain hazards that might attack the player.|$|R
40|$|Analytic cost {{estimation}} {{is a valuable}} aid to assess the effect of various machine and mapping parameters on program performance. Cost estimation is either based on {{a model of the}} parallel algorithm or on a model of the actually generated machine code. Especially in case of a distributed-memory system the difference in abstraction is large. In this paper we study the trade-off between analytic {{cost estimation}} at high (program) <b>level</b> <b>and</b> low (<b>machine)</b> <b>level.</b> We show that, despite its high abstraction, program level cost estimation provides the best prediction quality. This approach is implemented in a cost estimation engine within a compiler for Spar, a parallel Java dialect...|$|R
40|$|Abstract- This paper {{emphasize}} on {{controlling the}} process variable parameters such as <b>level</b> <b>and</b> flow with real time implementation of gate control of {{hydroelectric power plant}} using Programmable Logic Controller. In this work, a programmable logic controller is used as an industrial computer {{playing the role of}} a control device <b>and</b> <b>push</b> buttons, <b>level</b> <b>and</b> flow sensors provide incoming signals to the control unit. The prototype model is provided with five levels in the upper tank <b>and</b> two <b>levels</b> in the lower tank and depending on the level sensor outputs the ladder logic is actuated. This work uses PLC of ALLEN BRADLEY MICROLOGIX 1200 inbuilt with 24 digital inputs and provides 16 potential free outputs to control the miniaturized process depicted in the work...|$|R
30|$|Visual Studio (VS) is Microsoft’s IDE for {{developing}} Window’s desktop applications, web services, web sites and web applications, based on C, C++, C#, VB.NET, F#, M. Python, Ruby, Javascript, HTML and CSS. Its editor offers code completion and code refactoring and its debugger {{can be used}} both at source <b>and</b> <b>machine</b> <b>level.</b> It also provides design tools for forms, web services, database schemas and classes.|$|R
50|$|Rising Star Industries was {{the primary}} American {{software}} vendor for the HASCI QX series. Their product line included the TPM-II and III operating system, Valdocs, a robust Basic language implementation, a graphics API library used {{by a variety of}} products which initially supported line drawing and fill functions and was later extended to support the QX-16 color boards, Z80 assembler, <b>and</b> low <b>level</b> Zapple <b>machine</b> code monitor which could be invoked from dip switch setting on the rear of the machine.|$|R
40|$|Abstract- Simulation {{is one of}} {{the most}} {{effective}} methods in the Design of Manufacturing Systems. Typical reasons for the simulation of a manufacturing system includes evaluating the capacity and equipment utilisation, identifying bottlenecks in the system, comparing the performance of alternative designs [1]. The potential benefits in using simulation are very large. Simulation can help users by contributing in design, in management and in the decision making of production systems. It is also able to model all kinds of company processes: physical, informational and decisional. Simulation models can be built at all hierarchical (operational, tactical, strategic) <b>and</b> detailing <b>levels</b> (<b>machine,</b> cell, shop, etc…). Moreover, the whole manufacturing system life cycle can be modelled and simulated (design, analysis, implementation, operation, etc…). This paper discusses case studies where modelling and simulation were used to evaluate and improve the performance of manufacturing systems. In all cases ProModel simulation package was used...|$|R
40|$|This paper {{presents}} a lexicographic approach and integer programming formulations for multi-objective, long-term production scheduling in make-to-order manufacturing environment. The problem {{objective is to}} assign customer orders for various product types to planning periods and to select machines for assignment in every period to complete all the orders with minimum number of tardy or early orders as a primary optimality criterion <b>and</b> to <b>level</b> <b>machine</b> assign-ments over a planning horizon as a secondary criterion. Some cutting constraints that make the integer programming formulations stronger are derived. The approach {{has been applied to}} optimize master production schedules in a flexible flowshop made up of several processing stages in series and an output buffer of limited capacity to hold completed products before deliv-ery to the customer. Each processing stage consists of identical, parallel machines. Numerical examples modeled after a real-world make-to-order flexible assembly line in the electronics in-dustry are provided and some computational results are reported. Keywords: Production scheduling, Flexible flowshop, Integer programming, Make-to-order manufacturing...|$|R
40|$|It is {{of utmost}} {{importance}} for a project such as T 4 ME to get a comprehensive and reliable overview of the projects and initiatives addressing similar topics. Mainly {{in order to establish}} relationships, build on previous achievement, and get a reliable and up-to-date view about the currentstate of the art. This report surveys ongoing and recent projects and initiatives at the national, EU <b>and</b> transnational <b>level</b> addressing <b>Machine</b> Translation, multilingual issues, language resources and technologies, or infrastructural issues at large. Focus is on Europe but relevant initia-tives outside Europe have been reviewed as well...|$|R
40|$|Optimization of multi {{criteria}} {{problems is}} a great need of producers to product precision parts with low costs. Many methods such as Taguchi and Response Surface Methodology have been employed for optimization of EDM process. However there are few researches involve the optimization of multi-response problem in EDM process. The attempt {{of this paper is}} to optimize multiple performance characteristics of EDM process using Grey relational analysis based on Taguchi orthogonal array. The response table and response graph for each <b>level</b> of the <b>machining</b> parameters is obtained <b>and</b> optimal <b>levels</b> of <b>machining</b> parameters including pulse on time, discharge current, discharge voltage and duty factor are found. The multiple performance characteristics including material removal rate, electrode wear ratio and surface roughness is considered...|$|R
5|$|The {{drive to}} capture wildlife, and {{particularly}} slow lorises, is increasingly dominated by demands from wealthy urban areas, replacing the subsistence hunting traditionally performed in poor rural areas. In {{the case of}} long-lived primates, such as the slow lorises, populations replenish themselves slowly. Slow lorises are particularly vulnerable {{because they tend to}} freeze when spotted. Lastly, increased access to new technologies, such as improved transport, guns, wire snares, and spotlights, have facilitated hunting <b>and</b> <b>pushed</b> extraction <b>levels</b> beyond the point of sustainability. These new factors threaten slow loris survival.|$|R
5000|$|The terms {{high-level}} and low-level {{are inherently}} relative. Some decades ago, the C language, and similar languages, were most often considered [...] "high-level", as it supported {{concepts such as}} expression evaluation, parameterised recursive functions, and data types and structures, while assembly language was considered [...] "low-level". Today, many programmers might refer to C as low-level, as it lacks a large runtime-system (no garbage collection, etc.), basically supports only scalar operations, and provides direct memory addressing. It, therefore, readily blends with assembly language <b>and</b> the <b>machine</b> <b>level</b> of CPUs <b>and</b> microcontrollers.|$|R
50|$|Digital {{equipment}} {{can generate}} distortions in audio signals {{that affect the}} way humans process sound. Many digital audio signals use the process of data-reduction to compress the amount of data needed to represent the signal. Such systems seek to optimize utility and achieve more with less. This sometimes leads to drop {{on the quality of}} the sound and creates an illusion that there is something missing from the audio. It has been observed that there is an intrinsic need, a compulsion, for greater loudness. Data-reduction systems may be a factor in this. This quest for greater loudness <b>and</b> <b>pushing</b> <b>levels</b> to the maximum may factor into listener fatigue.|$|R
40|$|CEBAF has {{recently}} upgraded its accelerator control sys-tem to use EPICS, a control system toolkit {{being developed by}} a collaboration among laboratories in the US and Europe. The migration to EPICS has taken place during a year of intense commissioning activity, with new and old control systems operating concurrently. Existing CAMAC hardware was pre-served by adding a CAMAC serial highway link to VME; newer hardware developments are now primarily in VME. Software is distributed among three tiers of computers: first, workstations and X terminals for operator interfaces <b>and</b> high <b>level</b> applications; second, VME single board computers for distributed access to hardware and for local control processing (complex sequences, limit checking, some process control); third, embedded processors where needed for faster closed loop operation. In some cases, multiple VME processors trans-parently access a single serial highway for improved perfor-mance. This system has demonstrated the ability to scale EPICS to controlling thousands of devices, including hundreds of embedded processors, with control distributed among doz-ens of VME processors executing more than 125, 000 EPICS database records. To deal with the large size of the control sys-tem, CEBAF has integrated an object oriented database, pro-viding data management capabilities for both low level I/O (calibration, alarm limits, etc.) <b>and</b> high <b>level</b> <b>machine</b> model-ling (optics properties, etc.). A new callable interface which is control system independent permits access to live EPICS data, data in other Unix processes, and data contained in the object oriented database (extensible to other sources) ...|$|R
40|$|This {{document}} {{describes an}} {{implementation of a}} symbolic simulator/debugger for the Systolic/Cellular Array Processor (SCAP), which is currently being built at Hughes Research Laboratories. The SCAP system is a parallel computer with 256 identical processing elements (PEs) connected using a mesh interconnection network in a 16 x 16 grid. Each PE features a two-bus internal architecture, with seven functional units, and four I/O ports used to communicate with its four neighboring PEs. All functional units operate on 32 -bit fixed point data. The reader is refered to [Przytula, 88] for {{a detailed description of}} SCAP 2 ̆ 7 s architecture, data representation format, <b>and</b> <b>machine</b> <b>level</b> operation of the system...|$|R
40|$|Image {{recognition}} and understanding {{is one of}} the most interesting fields of researches. Its main idea is to bridge the gap between the high level human image understanding <b>and</b> the low <b>level</b> <b>machine</b> image representation. Quite a lot of applications have been suggested in different fields like medicine, industry, robotics, satellite imagery and other applications. This paper proposes a new approach of traffic signs image {{recognition and}} understanding using computational intelligent techniques and the application of this approach on intelligent cars which can recognize the traffic signs and take a decision according to the signs it reads. Supervised machine learning has been selected since the algorithm does not need to classify the images but to identify their precise meaning. Different neural networks have been trained and used in this paper. The best neural network has been selected, which uses genetic algorithms in its training, and is known as evolutionary training neural network. Different image features have also been investigated and discussed. the best of these features, which fit the requirement of the suggested algorithm, have been selected...|$|R
40|$|Abstract: The paper {{considers}} {{the application of}} creative problem-solving (CPS) principles to the reduction of plant maintenance costs. A brief description of CPS principles and methods is rst given. Three case studies are then described that illustrate the use of CPS on problems at plant <b>and</b> <b>machine</b> <b>levels,</b> by different group compositions and on problems of different types. A review of experience {{of the use of}} CPS methods by maintenance engineers trained in their use is also given. It is concluded that CPS is a valuable approach that may be used effectively to identify causes of maintenance problems and to nd solutions to particular problems of diverse kinds that incur maintenance costs...|$|R
40|$|This {{presentation}} {{will introduce}} inspection guidelines for field staff to implement in inspecting projects with pedestrian facilities, including curb ramps and pedestrian signals. Review of new INDOT curb standards and requirements for accessible pedestrian signals will be included. These {{recent changes in}} standards will impact projects and thus {{it is imperative to}} advise field staff of what they should be looking for with regard to width and slopes on pedestrian access routes <b>and</b> pedestrian <b>push</b> buttons. Several issues have been noted in the field, including excessive cross slope and running slope, installation of fingertip-actuated <b>push</b> buttons, <b>and</b> substandard <b>level</b> landings <b>and</b> facility width. The process of determining technical infeasibility once the project is in construction will also be addressed. The session will be co-presented with INDOT construction staff...|$|R
40|$|Part 2 : Sustainable Supply ChainsInternational audienceModern {{manufacturing}} facilities waste various energy savings opportunities (ESO) and lack proper performance indicators to measure energy efficiency {{on the production}} line. The ESO is an opportunity window calculated from on-line production data, such as production count, machine downtime records, buffer <b>levels,</b> <b>and</b> <b>machine</b> idle status, allowing certain machines to be turned off for energy savings without negatively affecting throughput. New energy efficiency performance indicators are presented that use real time production data to identify the least energy efficiency machine on the line. The energy savings opportunity strategy utilizes the Energy Efficiency Performance Indicators (EEPI) to take the opportunity window for the least energy efficient machine at opportune times, allowing for improvements {{to be made to}} the machine, increasing the overall energy efficiency of the line...|$|R
40|$|Janez Funda, "Symbolic Simulator/Debugger for the Systolic/Cellular Array Processor",. January 1991. Symbolic Simulator/Debugger for the Systolic/Cellular Array Processor This {{document}} {{describes an}} {{implementation of a}} symbolic simulator/debugger for the Systolic/Cellular Array Processor (SCAP), which is currently being built at Hughes Research Laboratories. The SCAP system is a parallel computer with 256 identical processing elements (PEs) connected using a mesh interconnection network in a 16 x 16 grid. Each PE features a two-bus internal architecture, with seven functional units, and four I/O ports used to communicate with its four neighboring PEs. All functional units operate on 32 -bit fixed point data. The reader is refered to [Przytula, 88] for {{a detailed description of}} SCAP's architecture, data representation format, <b>and</b> <b>machine</b> <b>level</b> operation of the system...|$|R
40|$|In {{hierarchical}} {{production planning}} system, Aggregate Production Planning (APP) falls between the broad decisions of long-range planning and the highly specific and detailed short-range planning decisions. This study develops an interactive Multi-Objective Genetic Algorithm (MOGA) approach for solving the multi-product, multi-period aggregate production planning (APP) with forecasted demand, related operating costs, and capacity. The proposed approach attempts to minimize total costs {{with reference to}} inventory levels, labor <b>levels,</b> overtime, subcontracting <b>and</b> backordering <b>levels,</b> <b>and</b> labor, <b>machine</b> and warehouse capacity. Here several genetic algorithm parameters are considered for solving NP-hard problem (APP problem) and their relative comparisons are focused to choose the most auspicious combination for solving multiple objective problems. An industrial case demonstrates the feasibility of applying the proposed approach to real APP decision problems. Consequently, the proposed MOGA approach yields an efficient APP compromise solution for large-scale problems...|$|R
40|$|The actual gains {{achieved}} by replication are a complex {{function of the}} number of replicas, the placement of those replicas, the replication protocol, the nature of the transactions performed on the replicas, and the availability and performance characteristics of the machines and networks composing the system. This paper describes the design and implementation of the Replica Management System, which allows a programmer to specify the quality of service required for replica groups in terms of availability and performance. From the quality of service specification, information about the replication protocol to be used, and data about the characteristics of the underlying distributed system, the RMS computes an initial placement <b>and</b> replication <b>level.</b> As <b>machines</b> <b>and</b> communications systems are detected to have failed or recovered, or performance characteristics change, the RMS can be re [...] invoked to compute an updated mapping of replicas which preserves the desired quality of service. The [...] ...|$|R
40|$|Best {{practices}} frameworks such as ITIL {{provide a}} generic description of best practice {{processes that are}} intended {{to be followed by}} people. These processes are refined into more concrete steps before they are actionable. The refinement often is specific to the organization where the process is adopted, as well as people who are enacting the process. Modeling best practice processes is challenging. On one hand, these processes need a high-level, abstract representation. Current process modeling languages are too rigid for modeling them. On the other hand, automation of the enactment of these processes among people requires formal models. In this paper, we propose a framework for modeling best practice processes at three levels: user-level, formal process model <b>level</b> <b>and</b> <b>machine</b> representation <b>level</b> to support the collaborative and ad-hoc refinement of process models as well as the automation of their enactments. We also propose an approach to learn from the past enactments of processes to enable reuse of organizational domain knowledge...|$|R
40|$|Objectives: Interprofessionalism (IP) {{has emerged}} as a new {{movement}} in healthcare in response to workforce shortages, quality and safety issues and professional power dynamics. Stakeholders can push for IP (e. g. education providers to the health system) or pull (e. g. the health system to the education provider). Based on innovation theory, we hypothesized that there would be unequal forces within and across stakeholder domains which would work to facilitate or resist IP. The strongest pull pressures would be from the health system <b>and</b> services; <b>push</b> pressures for IP would come from government and higher education; with weaker <b>push</b> forces <b>and</b> <b>levels</b> of resistance, from protectionist professional bodies. Design, setting and participants:Our model was tested in a geographically bounded health jurisdiction. Information was gathered and analysed via individual (n = 99 participants) and group (n = 372 participants) interviews with stakeholders, and through document analysis. Results: The health system and services exerted the strongest pull in demanding IP. The strongest push factor was individual champions in positions of power. Professional bodies balanced their support of IP competencies with their role as advocates for their individual professions. A weak push factor came from government support for health workforce reform. Conclusions: Our hypothesis was supported, as were our predictions that the strongest pull would be from the providers <b>and</b> the strongest <b>push</b> from government <b>and</b> higher education. Our original model should be extended to account for contextual factors such as large-scale workplace and professional reform, which worked both for and against, IP. 8 page(s...|$|R
40|$|The {{following}} {{article describes}} and explains a technical term called Human Machine Interface. The HMI {{can be used}} in many different appliances, for instance in household devices to help a human communicate with these devices. However, this article considers the HMI for industrial needs, where a visual representation can be beneficial in many cases. When an industrial plant wants to apply the HMI to its manufacturing lines, a few changes must be made. When supervisory <b>and</b> <b>machine</b> <b>levels</b> are implemented to the manufacturing lines the HMI is ready for usage. In order to make the whole system function properly a set of rules and principles must be realized. Thus, this article can help the researchers and technicians accomplish perfect output from technical systems and not make common mistakes...|$|R
40|$|Part 3 : StrategyInternational audienceDetermination and {{comparison}} of the energy efficiency over all planning levels {{is one of the}} key challenges for small and mediums sized enterprises (SME). This is because there is a lack of transparency and information to identify the major energy consumers on plant level as well as on production <b>and</b> <b>machine</b> <b>level.</b> Therefore, there is a need to develop a holistic energy efficiency model which integrates all three planning levels supporting decision makers with the essential information. This paper presents first results from an ongoing benchmark study which is conducted with companies from Austria, Belgium and Germany. First results show, that there is a lack of information integration from the shop-floor to the planning levels which causes SMEs to fail their strategic energy efficiency targets...|$|R
40|$|In this paper, {{we present}} both {{simulation}} analysis and {{experimental study of}} a planar dynamic biped walking robot. The proposed control structure involves an equilibrium point controller at the local joint <b>level</b> <b>and</b> state <b>machines</b> at the interjoint level. The robot has actuated hip joints and knee joints as well as unactuated ankle joints with curved feet. We first show in simulation analysis that stable walking by this robot is possible with various gait patterns and {{a wide range of}} walking speed. Then, we implement the controller on a real biped robot, and show in real-time experiments that, by directly changing two controller parameters, the robot can change its walking gait/speed on the fly and can achieve a very fast walking speed. © 2009 IEEE...|$|R
40|$|Simulators for {{wireless}} sensor {{networks are}} a valuable tool for system development. However, current simulators can only simulate a single level of a system at once. This makes system development and evolution difficult since developers cannot use the same simulator for both high-level algorithm development and low-level development such as device-driver implementations. We propose cross-level simulation, a novel type of wireless sensor network simulation that enables holistic simultaneous simulation at different levels. We present an implementation of such a simulator, COOJA, a simulator for the Contiki sensor node operating system. COOJA allows for simultaneous simulation at the network level, the operating system <b>level,</b> <b>and</b> the <b>machine</b> code instruction set level. With COOJA, we show the feasibility of the cross-level simulation approach. 1...|$|R
50|$|An engineer's {{spirit level}} (or machinist's level) is {{generally}} used to <b>level</b> <b>machines,</b> {{although they may}} also be used to level large workpieces on machines such as planers. Using gravity as a reference and checking a machine's axis of travel at several points, the level is used to ensure the machine's axis is straight. A perfectly <b>level</b> <b>machine</b> does not actually need to be achieved, unless the particular manufacturing process requires it. Spirit levels are also used in building construction by carpenters and masons.|$|R
40|$|The first {{cross-sectional}} {{analyses are}} presented of {{a longitudinal study}} regarding the relationship between <b>pushing</b> <b>and</b> pulling and musculoskeletal disorders. Workers exposed to <b>pushing</b> <b>and</b> pulling and workers who had administrative tasks received a questionnaire. A significant association between shoulder complaints {{in the past year}} and high exposure to <b>pushing</b> <b>and</b> pulling was found...|$|R
40|$|A new {{material}} constitutive law is implemented in a 2 D {{finite element model}} to analyse the chip formation and shear localisation when machining titanium alloys. The numerical simulations use a commercial finite element software (FORGE 2005) able to solve complex thermo-mechanical problems. One of the main machining characteristics of titanium alloys is to produce segmented chips {{for a wide range}} of cutting speeds and feeds. The present study assumes that the chip segmentation is only induced by adiabatic shear banding, without material failure in the primary shear zone. The new developed model takes into account the influence of strain, strain rate and temperature on the flow stress and also introduces a strain softening effect. The tool chip friction is managed by a combined Coulomb–Tresca friction law. The influence of two different strain softening <b>levels</b> <b>and</b> <b>machining</b> parameters on the cutting forces and chip morphology has been studied. Chip morphology, cutting and feed forces predicted by numerical simulations are compared with experimental results...|$|R
40|$|In an infinite-horizon {{economy with}} {{matching}} frictions, I study the efficient assignment between workers of different skill <b>levels</b> <b>and</b> <b>machines</b> of different quality levels. Under some restrictions I {{show that the}} efficient allocation assigns a unique machine quality and market tightness to each skill, and that the assignment is saddle-path stable. The efficient assignment is not necessarily positively assortative and efficient wages do not necessarily increase with the skill level. Nevertheless, the social value of workers always increases with the skill level. I then show that the efficient allocation can be decentralized by a market mechanism, in which the firms direct workers' search by announcing and committing to the machine quality, the skill level they intend to hire for such machines, and the time-path of wages. Finally, I calibrate the model to the US data and examine how a skill-biased technological progress affects the assignment and inequality. (Copyright: Elsevier) Matching; Efficiency; Inequality; Skill Bias. ...|$|R
5000|$|..... LIL is, however, a failure. Its stiffest {{competition}} at Bell Labs {{is the language}} C, which is higher <b>level,</b> <b>and</b> <b>machine</b> independent. Every time it looked like C was too expensive to use for a particular project, LIL was considered. But almost every time, it proved easier (and more rewarding) to improve C, or its runtime support, or the hardware, than to invest time in yet another language. ... A machine independent language is always superior -- even for writing machine dependent code (it's easier to find trained programmers) -- {{so long as the}} overhead can be endured. It is clear now that writing straightforward code and then measuring it is the formula for the best end product. At worst there will be 5-15 per cent overhead, which is seldom critical. Once system writers become mature enough to recognize this basic truth, they gravitate naturally toward machine independent SILs. ... it looks like the little implementation language is an idea whose time as come -- and gone.|$|R
40|$|In today’s e-conomy {{the only}} chance for {{prosperity}} is to exploit optimally the emerging technologies based on which {{a new kind of}} infrastructure facilitates strategic partnerships among cyber-highway enabled participants. This paper merges the latest results obtained by the Holonic Manufacturing Systems (HMS) Consortium with the latest developed standards for platform interoperability released by the Foundation for Intelligent Physical Agents (FIPA) to propose a novel e-business model: the Holonic E-nterprise. Including the e-marketplace and e-factory as sub-models, this new paradigm links the three levels of a global collaborative organization (inter-enterprise, intra-enterprise <b>and</b> <b>machine</b> <b>level)</b> to build a web-centric ecosystem partnering in which the workflow is harmoniously managed. After clarifying the proposed concept we define a mapping between holons and agents, introducing the concept of mediator. We identify several patterns of holonic collaboration and throughout the paper identify their particularities at each level. The Holonic Enterprise extends both the HMS and FIPA models. On one side it extends the holonic manufacturing paradigm with one top level, the inter-enterprise one. On the other side it extends the multi-agent system (MAS) paradigm to the hardware (physical <b>machine)</b> <b>level...</b>|$|R
40|$|Although <b>pushing</b> <b>and</b> pulling is {{very common}} in {{occupational}} settings, this type of manual materials handling is less well studied than lifting and carrying. Several issues should be considered when obtaining exposure measures in epidemiological field studies on <b>pushing</b> <b>and</b> pulling. The {{purpose of this article}} is threefold: (i) to critically evaluate different methods to assess <b>push</b> <b>and</b> pull forces, (ii) to describe measures of exposure to <b>pushing</b> <b>and</b> pulling, and (iii) to consider measurement strategies for assessment of exposure to <b>pushing</b> <b>and</b> pulling. Firstly, information on the level of exerted forces with the accuracy needed for epidemiology can only be obtained from direct measurement methods. These methods are particularly required when <b>push</b> <b>and</b> pull tasks are biomechanically analysed, implying that also force direction and point of application relative to the worker have to be assessed. Secondly, to obtain a limited number of external exposure measures that reflect exposure to <b>pushing</b> <b>and</b> pulling over time, aggregation of various force measurements is suggested. Internal exposure measures and parameters corresponding to guidelines are also described. Thirdly, for truck drivers and refuse collectors a strategy of approximately five repeated measurements for each representative working situation is advised to obtain a reliable estimate of an individual's exposure to <b>pushing</b> <b>and</b> pulling. Relevance to industryAn overview is given of methods to assess forces accompanying <b>pushing</b> <b>and</b> pulling, which are very common activities in industry. Examples of exposure measures and measurement strategies in studies on adverse health effects of <b>pushing</b> <b>and</b> pulling are presented. Such studies should eventually stimulate prevention of work-related musculoskeletal disorders. Copyright (C) 1999 Elsevier Science B. V...|$|R
40|$|Objectives Low-back and {{shoulder}} complaints {{were examined in}} relation to self-reported and objectively assessed exposure to work-related <b>pushing</b> <b>and</b> pulling. Methods Workers from several companies (eg, nursing homes and flower auctions) with <b>pushing</b> <b>and</b> pulling tasks and, as reference, workers without physically demanding tasks were invited to participate. Altogether 829 workers initially received a questionnaire, of whom 459 responded both initially and after I year of follow-up. Initially, self-reported exposure to <b>pushing</b> <b>and</b> pulling was assessed by questionnaire. Furthermore, {{a representative sample of}} 131 workers was observed at work using TRAC (task recording and analysis on computer) to assess exposure to <b>pushing</b> <b>and</b> pulling objectively. For exposure to <b>pushing</b> <b>and</b> pulling, the workers were classified into a reference group and medium and high exposure groups. Initially and in the follow-up, the 12 -month prevalence of low-back {{and shoulder}} complaints was assessed. Complaints reported in the follow-up were separately used as dependent variables to calculate prevalence rate ratios (PR) in a Cox's proportional hazard regression analysis. Results The adjusted PR values were not significant for low-back complaints. For shoulder complaints, both the medium and high exposure groups showed significant adjusted PR values for self-reported exposure and observed duration of <b>pushing</b> <b>and</b> pulling when compared with the reference group (PR range 2. 18 - 4. 86). For the observed frequency of <b>pushing</b> <b>and</b> pulling, only the medium exposure group showed a significant PR, of 3. 95. Conclusions The findings suggest a rather strong relationship between <b>pushing</b> <b>and</b> pulling and shoulder complaints. The evidence for a relationship between <b>pushing</b> <b>and</b> pulling and low-back complaints is limite...|$|R
40|$|AbstractSun's Java {{architecture}} {{introduced a}} safe virtual machine (VM) {{in which an}} ensemble of software components developed independently could smoothly interoperate. The goal of Microsoft's Common Language Runtime (CLR) is to generalize this approach and allow components in many source languages to interoperate safely. CLR supports flexible interoperation by compiling various source languages into a common intermediate language and by using a unified type system. However, the type system in CLR (and Java VM) enforces only conventional type safety in an object-oriented system. Therefore, higher-level specifications (e. g., resource bounds, generalized access control, formal software protocols) cannot be enforced. Because conventional type systems are too inflexible for real applications, developers often bypass the type system, producing code that steps outside the managed part of the VM; such components cannot be verified. At Yale we have been developing typed common intermediate languages (named FLINT) that can support safely not only the standard object-oriented model, but also higher-order generic (polymorphic) programming and Java-style reflection (introspection). Unlike CLR, our type system is independent of any particular programming model, yet {{it is capable of}} expressing all valid propositions and proofs in higher-order predicate logic (so {{it can be used to}} capture and verify advanced program properties). The rich type system of FLINT makes it possible to typecheck both compiler intermediate code <b>and</b> low <b>level</b> <b>machine</b> code; this allows typechecking to take place at any phase of compilation, even after optimizations and register allocation. It also leads to a smaller and more extensible VM because low-level native routines that would otherwise be in VM can now be verified and moved into a certified library. This talk describes our vision of the FLINT system, outline our approach to its design, and survey the technologies that can be brought to support its implementation...|$|R
