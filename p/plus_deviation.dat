2|178|Public
40|$|INTRODUCTION Genetic {{evaluations}} {{based on}} lactation records have ignored {{differences in the}} shape of the lactation curve. Test day models account for lactation shape and offer more precision but use many more equations. A two-trait model with just yield and persistency could account for lactation shape with far fewer equations. Best prediction is a method to condense information from many test days into optimal lactation measures of yield and persistency. Computational shortcuts for reporting and analyzing test day data are provided. DERIVATION Daily yield can be modeled as management group mean or expected value <b>plus</b> <b>deviation</b> from that mean. The vector contains the expected values at each day of lactation for one trait. Vector t represents all 305 test day deviations for the trait, and vector t represents only measured m deviations. Vector t may be a subset of vector...|$|E
40|$|The {{masses of}} several recently-constructed {{rotating}} black holes in gauged supergravities, including the general such solution in minimal gauged supergravity in five dimensions, have until now been calculated only by integrating the first law of thermodynamics. In some respects {{it is more}} satisfactory to have a calculation of the mass that is based directly upon the integration of a conserved quantity derived from a symmetry principal. In this paper, we evaluate the masses for the newly-discovered rotating black holes using the conformal definition of Ashtekar, Magnon and Das (AMD), and show that the results agree with the earlier thermodynamic calculations. We also consider the Abbott-Deser (AD) approach, and show that this yields an identical answer for {{the mass of the}} general rotating black hole in five-dimensional minimal gauged supergravity. In other cases we encounter discrepancies when applying the AD procedure. We attribute these to ambiguities or pathologies of the chosen decomposition into background AdS metric plus deviations when scalar fields are present. The AMD approach, involving no decomposition into background <b>plus</b> <b>deviation,</b> is not subject to such complications. Finally, we also calculate the Euclidean action for the five-dimensional solution in minimal gauged supergravity, showing that it is consistent with the quantum statistical relation. Comment: Typos corrected and references update...|$|E
30|$|Class 4. Higher {{than the}} mean value <b>plus</b> the {{standard}} <b>deviation.</b>|$|R
30|$|Class 3. Lying {{between the}} mean value {{and the mean}} <b>plus</b> the {{standard}} <b>deviation.</b>|$|R
3000|$|Continuous {{variables}} were expressed as mean <b>plus</b> standard <b>deviation.</b> Categorical {{variables were}} described as number and percentage. The Wilcoxon signed-rank test was performed between the initial and follow-up echocardiographic measurements. We evaluated the difference in oxidative metabolism (k [...]...|$|R
40|$|A large {{deviation}} {{principle is}} derived for stochastic partial differential equations with slow-fast components. The result {{shows that the}} rate function is exactly that of the averaged equation <b>plus</b> the fluctuating <b>deviation</b> which is a stochastic partial differential equation with small Gaussian perturbation. This also confirms {{the effectiveness of the}} approximation of the averaged equation <b>plus</b> the fluctuating <b>deviation</b> to the slow-fast stochastic partial differential equations. Comment: 30 page...|$|R
30|$|All {{statistical}} {{analysis was performed}} with Prism software (GraphPad Prism). Statistical comparisons between two groups were evaluated by student’s t test. All the data are represented as mean <b>plus</b> standard <b>deviation</b> (SD). The differences were considered statistically significant when P value was less than 0.05.|$|R
30|$|The {{mean and}} {{standard}} deviation of normal serum samples were calculated and cut-off value was determined as the mean of normal controls <b>plus</b> two standard <b>deviations</b> (SD). Serum samples were considered positive if the mean optical densities were higher than cut-off point. For consideration as positive following the criteria of multiple-antigen ELISA, the serum tested specifically react with any antigen when the cut-off value was calculated from the mean optical densities <b>plus</b> three standard <b>deviations</b> (Zhang et al. 2009). Statistical analysis was performed with GraphPad Prism version 5.0.|$|R
50|$|The Fort Wayne Public Transportation Corporation (FWPTC), branded as Citilink, is {{the public}} {{transportation}} operator for the metro area of Fort Wayne and Allen County, Indiana. Transportation is provided Monday through Saturday on twelve fixed route lines <b>plus</b> two <b>deviation</b> routes. Citilink does not operate on Sundays and six holidays. Website is www.fwcitilink.com. Facebook Citilink (official). Real time information on RouteShout app.|$|R
40|$|Abstract: "A set of 12 charts are {{presented}} that relate Modified Mercalli intensity units to peak horizontal acceleration, velocity and duration for near field and far field locations, hard and soft sites, and sizes of earthquakes. Also shown is the mean, mean <b>plus</b> one standard <b>deviation,</b> mean <b>plus</b> two standard <b>deviations,</b> {{and the highest}} observed values. Ratios are provided of vertical to horizontal motions and predominant periods. These charts are for use with known fault sources and for floating earthquakes in zones {{where there are no}} identifiable causative faults. The procedure provides parameters for shaping time histories to be used for dynamic analyses. ...|$|R
50|$|A list of {{the opening}} dates in chronological order is here. This list shows the known opening dates for all line construction, {{including}} <b>deviations,</b> <b>plus</b> the known dates of other significant NCL related events etc.|$|R
25|$|Divergence Angle: The {{representative}} {{range of}} leeway angles for {{a category of}} leeway objects. It can be calculated by obtaining the net leeway angle over time for a specific leeway object’s drift trajectory, and then averaging again {{for a series of}} leeway drift trajectories of a number of leeway objects in a leeway category, to determine the mean leeway angle and standard deviation of the leeway angle for the category. Divergence angle is then calculated as twice the standard deviation of the leeway angle, or mean <b>plus</b> one standard <b>deviation</b> of the leeway angle, or mean <b>plus</b> two standard <b>deviations</b> of the leeway angle depending on the particular study.|$|R
40|$|This diploma {{thesis is}} {{concerned}} with the stress analysis of the total hip joint endoprosthesis ceramic head under oblique loading. Load was obtain from experimental in vivo measurement on three patients, whereas two of them was the same weight. As input variables were use contact forces, especially knee bend and standing on one leg. Furthermore they will be considered shape deflections from ideal cone, especially circle deviation, cone <b>deviation</b> and cone <b>plus</b> circle <b>deviation</b> together...|$|R
40|$|Artículo de publicación ISIThe {{propagation}} of elastic waves in polycrystals is revisited, {{with an emphasis}} on configurations relevant to the study of ice. Randomly oriented hexagonal single crystals are considered with specific, nonuniform, probability distributions for their major axis. Three typical textures or fabrics (i. e. preferred grain orientations) are studied in detail: one cluster fabric and two girdle fabrics, as found in ice recovered from deep ice cores. After computing the averaged elasticity tensor for the considered textures, wave propagation is studied using a wave equation with elastic constants c = c + δc that are equal to an average <b>plus</b> <b>deviations,</b> presumed small, from that average. This allows {{for the use of the}} Voigt average in the wave equation, and velocities are obtained solving the appropriate Christoffel equation. The velocity for vertical propagation, as appropriate to interpret sonic logging measurements, is analysed in more details. Our formulae are shown to be accurate at the 0. 5 % level and they provide a rationale for previous empirical fits to wave propagation velocities with a quantitative agreement at the 0. 07 – 0. 7 % level. We conclude that, within the formalism presented here, it is appropriate to use, with confidence, velocity measurements to characterize ice fabrics...|$|R
50|$|This {{is a list}} of the NCL opening {{dates in}} chronological order. This list shows the known opening dates for all line construction, {{including}} <b>deviations,</b> <b>plus</b> the known dates of other significant NCL related events. A list of the NCL opening dates by section is here.|$|R
3000|$|... 0.1 in {{predicting}} excess patient’s inspiratory effort determined as PTPes >  200  cmH 2 O × s × min− 1. This value {{was chosen as}} the upper value, i.e., mean value <b>plus</b> one standard <b>deviation,</b> tolerated by patients passing a successful spontaneous breathing trial [14]. A p value <  0.05 was considered as statistically significant.|$|R
30|$|GraphPad Prism {{software}} (San Diego, CA) {{was used}} for analysis of LIPS autoantibody data and for plotting values. For each test, LU were determined from the average {{of at least two}} separate measurements. Cut-off values were determined from the mean <b>plus</b> five standard <b>deviations</b> (SD) of the healthy volunteers and are indicated for each autoantibody test in the figures.|$|R
30|$|Serum {{antibody}} {{concentrations were}} defined as endpoint titers and were calculated as the reciprocal of the highest serum dilution producing an OD 450 above the cutoff value. The cutoff value was determined as the mean OD 450  nm of the corresponding dilution of control sera <b>plus</b> 3 standard <b>deviations.</b> A titer of ≥ 1000 was considered positive for the ELISA.|$|R
30|$|Collinearity {{tests and}} Spearman {{correlation}} coefficients {{were used to}} assess the multilinearity between the independent variables. For each specific DRG, the outliers of LOS were identified if an observed value for LOS was larger than the population mean <b>plus</b> 4 standard <b>deviations</b> according to the “ 68 - 95 - 99.7 rule” (68 - 95 - 99.7 rule 2012).|$|R
40|$|In pattern {{classification}} problems using a RBFNN classifier, {{the selection}} of the number of clusters and their corresponding centers influences the network's ability to generalize unseen data. In this paper, we evaluate different RBFNN architectures by a quantitative measure - RBFNN sensitivity measure, which is defined as the absolute expectation <b>plus</b> standard <b>deviation</b> of network output perturbations with respect to input perturbations. Numerical comparisons of a number of different RBFNN architectures are given using two of UCI datasets. The experiments show that the sensitivity measure would be correlated to the testing error for the unseen samples and simpler classification problem may have smaller sensitivity measure. Department of ComputingRefereed conference pape...|$|R
50|$|This {{is a list}} of the NCL opening {{dates in}} {{sectional}} order. This list shows the original opening date of each NCL section, ignoring any subsequent deviations etc. A list of the NCL opening dates by chronological order, including the known opening dates for all line construction, including <b>deviations,</b> <b>plus</b> the known dates of other significant NCL related events is here.|$|R
30|$|To {{comment on}} the general results at the {{profiles}} level, {{we were unable to}} use our approach to distinguish between students and employees (thus, natural and unnatural personae). This could be because the type of personae represented by individuals # 4 and # 5 is very similar to our natural profile template, or because there was too much variation in our sample used to define naturalness. To test this last hypothesis we adopted the approach mentioned in Step 1 that focused on defining and reflecting on the average penalty and the standard deviation for the initial participant sample. From this analysis, we found that at this level, there were six participants that had a penalty score higher than the mean penalty (0.0343) <b>plus</b> one standard <b>deviation</b> (0.0334) and one participant (of that six) with a score higher than the mean <b>plus</b> two standard <b>deviations</b> (unnatural). In some ways, these individuals could be considered as outliers whose profiles could have weakened the naturalness characterisation process.|$|R
40|$|Lower {{and upper}} bounds of the peak {{horizontal}} ground acceleration are obtained through {{the consideration of}} the peak accelerations of two horizontal orthogonal components of the ground motion. Since the bounded region is relatively narrow, it is proposed to use {{the mean of the}} upper and lower bound accelerations as the peak horizontal ground acceleration. This mean value is equal to 1. 20 times the larger peak acceleration of the two horizontal compo-nents of the ground motion. To support the proposed value, a simplified statis-tical analysis is also employed which results in a mean <b>plus</b> one standard <b>deviation</b> value for the peak horizontal ground acceleration of 1. 24 times the larger peak acceleration. These conclusions are also supported by comparison with 50 pairs of earthquake acceleration records. For the resultant acceleration, the 50 pairs of records yield a mean <b>plus</b> one standard <b>deviation</b> estimate of the order of 1. 20 times the larger peak acceleration...|$|R
30|$|The {{representation}} contains specific underlying overall trend, {{represented in}} red. The other two curves represent the <b>deviation,</b> <b>plus</b> (in green)/minus (in black), from the approximation signal. It {{can be observed}} that {{a large part of}} the traffic is contained between the green and black lines. The red line indicates an increase of the traffic in time, suggesting the possibility of saturation of BS 1.|$|R
30|$|To {{select the}} {{candidate}} hits {{on the final}} normalized path distances, the system employs two limits for peak picking. The first is a hard limit of a maximum number of peaks, which implies an average of 1 peak per 20 s of audio. The second is a threshold where only the peaks above the 90 % quantile of values above the mean <b>plus</b> standard <b>deviation</b> are selected. This guarantees that {{a small number of}} peaks is always chosen. Additionally, the peaks must be separated by a distance which is at least equal to the query length. The duration of the candidate hits in the utterance is also limited to between 0.5 and 1.9 {{times the size of the}} query. These figures were optimized on the development data.|$|R
30|$|The {{screening}} for thermostability was assessed {{based on the}} residual activity subsequent to the exposure to high temperatures. The supernatants were diluted (1 : 2000 in a total volume of 50 µl) in incubation mixture containing 25  mM KPi pH 8.0, 5  mM MgCl 2 and 100  mM glucuronic acid. Before incubation the initial activity was measured using an aliquot of 20  µl and adding 180  µl of reaction mixture (25  mM KPi pH  8.0 and 1  mM NAD+). Heat treatment was performed for 15  min at 58  °C in a PCR thermocycler. After cooling to 4  °C another aliquot of 20  µl {{was used to measure}} the residual activity. Variants showing a residual activity greater than the wildtype enzyme <b>plus</b> standard <b>deviation</b> were considered as hits.|$|R
30|$|Individual {{dim light}} {{melatonin}} onset (DLMO) values were calculated using {{the mean of}} the baseline concentration values <b>plus</b> two standard <b>deviations</b> of the mean. This concentration was used to calculate the timing of melatonin onset through a linear response function. Due to insufficient or contaminated samples, DLMO values were unable to be calculated for two ECTs and four LCTs. Cortisol peak was calculated as the time of the highest cortisol concentration recorded.|$|R
5000|$|The [...] "chart" [...] {{actually}} {{consists of}} a pair of charts: One to monitor the process standard deviation and another to monitor the process mean, as is done with the [...] and R and individuals control charts. The [...] and s chart plots the mean value for the quality characteristic across all units in the sample, , <b>plus</b> the standard <b>deviation</b> of the quality characteristic across all units in the sample as follows: ...|$|R
40|$|Keyframe {{animation}} is {{a common}} technique to generate animations of deformable characters and other soft bodies. With spline interpolation, however, {{it can be difficult}} to achieve secondary motion effects such as plausible dynamics when there are thousands of degrees of freedom to animate. Physical methods can provide more realism with less user effort, but it is challenging to apply them to quickly create specific animations that closely follow prescribed animator goals. We present a fast space-time optimization method to author physically based deformable object simulations that conform to animator-specified keyframes. We demonstrate our method with FEM deformable objects and mass-spring systems. Our method minimizes an objective function that penalizes the sum of keyframe <b>deviations</b> <b>plus</b> the <b>deviation</b> of the trajectory from physics. With existing methods, such minimizations operate in high dimensions, are slow, memory consuming, and prone to local minima. We demonstrate that significant computational speedups and robustness improvements can be achieved if the optimization problem is properly solved in a low-dimensional space. Selecting a low-dimensional space so that the intent of the animator is accommodated, and that at the same time space-time optimization is convergent and fast, is difficult. We present a method that generates a quality low-dimensional space using the given keyframes. It is then possible to find quality solutions to difficult space-time optimization problems robustly and in a manner of minutes. Singapore-MIT GAMBIT Game LabNational Science Foundation (U. S.) (Grant CCF- 0810888) Adobe SystemsPixar (Firm...|$|R
40|$|The {{author has}} {{identified}} the following significant results. Correlations for 55 segments were quite low, showing few values greater than. 6. Most of the correlations substantially increased {{when they were}} run for the breakdown of Assateague into eight coastal segments. Some of these correlations reflect strong and important relationships. The correlations between coastal orientation and the standard deviation of rate of shoreline erosion is. 93 at the. 01 level of significance. Other significant relationships were orientation and swash slope (. 84); standard deviation of erosion and subaerial beach slope a(-. 79); foredune height and subaerial beach width (-. 89); foredune height and mean <b>plus</b> standard <b>deviation</b> of erosion (-. 81); and rate of erosion over time and subaerial beach width (. 72). Low correlation was found between sand grain size and erosion and between sand grain size and orientation...|$|R
30|$|When {{performing}} a validation {{analysis of a}} population model, uncertainty should be addressed as discussed above in more detail {{to address the question}} to which extent methodological bias, sample size effects or other factors may cause uncertainty of the parameters used for model parameterisation (and consequently the model predictions) or validation. This can be done using either a quantitative or qualitative approach. While the model output can easily be quantified, the data with which the model results are compared for validation may not be available in sufficient detail to make a quantitative comparison possible. However, a qualitative tabular comparison (model prediction vs. field data) of the output of the model and the values observed in the field may also be appropriate, e.g. addressing uncertainty by showing ranges, means <b>plus</b> standard <b>deviations,</b> confidence intervals or percentiles from different studies and comparing such values with the results from the model.|$|R
40|$|This paper {{presents}} an inverse {{model of the}} radiation transfer processes occurring in the solar domain in vegetation plant canopies. It uses a gradient method to minimise the misfit between model simulation and observed radiant fluxes <b>plus</b> the <b>deviation</b> from prior information on the unknown model parameters. The second derivative of the misfit approximates uncertainty ranges for the estimated model parameters. In a second step, uncertainties are propagated from parameters to simulated radiant fluxes via the model's first derivative. All derivative information is provided by highly efficient code generated via automatic differentiation of the radiative transfer code. The paper further derives and evaluates an approach for avoiding secondary minima of the misfit. The approach exploits the smooth dependence of the solution on the observations, and relies on a data base of solutions for a discretised version of the observation space. JRC. H. 5 -Land Resources Managemen...|$|R
30|$|We {{propose that}} our method {{estimated}} engineering parameters within {{a range of}} <b>plus</b> one standard <b>deviation,</b> {{and this has been}} validated many times by comparing to past earthquakes. As stated in Hutchings et al. (2007), “The likelihood of an earthquake falling outside the plus-and-minus standard deviation values is thus about 30  %, and the likelihood of having an earthquake above the one standard deviation value is about 15  %.” The variation of both the Fourier amplitude and acceleration response spectra values for different stations is a factor of about 8 (Figs.  11, 14, 17).|$|R
40|$|Abstract—In {{response}} to the increasing variations in integrated-circuit manufacturing, the current trend is to create designs that take these variations into account statistically. In this paper, we quantify {{the difference between the}} statistical and deterministic optima of leakage power while making no assump-tions about the delay model. We develop a framework for deriving a theoretical upper bound on the suboptimality that is incurred by using the deterministic optimum as an approximation for the statistical optimum. We show that for the mean power measure, the deterministic optima is an excellent approximation, and for the mean <b>plus</b> standard <b>deviation</b> measures, the optimality gap increases as the amount of inter-die variation grows, for a suite of benchmark circuits in a 45 nm technology. For large variations, we show that there are excellent linear approximations {{that can be used to}} approximate the effects of variation. Therefore, the need to develop special statistical power optimization algorithms is questionable. Index Terms—Algorithms, gate sizing, optimization, physical design, statistical power. I...|$|R
40|$|In {{response}} to the increasing variations in integrated-circuit manufacturing, the current trend is to create designs that take these variations into account statistically. In this paper, we quantify {{the difference between the}} statistical and deterministic optima of leakage power while making no assumptions about the delay model. We develop a framework for deriving a theoretical upper bound on the suboptimality that is incurred by using the deterministic optimum as an approximation for the statistical optimum. We show that for the mean power measure, the deterministic optima is an excellent approximation, and for the mean <b>plus</b> standard <b>deviation</b> measures, the optimality gap increases as the amount of inter-die variation grows, for a suite of benchmark circuits in a 45 nm technology. For large variations, we show that there are excellent linear approximations {{that can be used to}} approximate the effects of variation. Therefore, the need to develop special statistical power optimization algorithms is questionable...|$|R
40|$|A {{mathematical}} {{model of the}} expected position errors encountered from LORAN-C during a non precision approach was formulated. From this, position error ellipses were generated that corresponded to two time difference correction schemes. One involved relaying corrections to the pilot just before he initiated the approach, and the other involved publishing time difference corrections in the instrument approach plates. It {{was found that the}} errors associated with both update scenarios were well within FAA AC 90 - 45 A accuracy standards for non precision approaches. The former scenario showed a significant improvement over the latter. Flight tests were conducted in a general aviation airplane carrying an equipment test bed designed to take data from a LORAN-C receiver and an ILS localizer receiver. The results of the flight tests show that the LORAN-C had a maximum error (average <b>plus</b> one standard <b>deviation)</b> of 1. 276 degrees deviation from the localizer path, and an average error (average <b>plus</b> one standard <b>deviation)</b> of. 648 degrees. It is concluded that LORAN-C is a suitable navigation system for non precision approaches and that time difference corrections made every eight weeks in the instrument approach plates will produce acceptable errors. June 1985 Also issued as an M. S. thesis, Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 1985 Includes bibliographical references (p. 272...|$|R
