48|206|Public
50|$|Separation {{logic is}} the basis of a number of tools for {{automatic}} and semi-automatic reasoning about programs, and is used in the Infer <b>program</b> <b>analyzer</b> currently deployed at Facebook.|$|E
5000|$|... = Visual Expert =Visual Expert is {{a static}} <b>program</b> <b>analyzer,</b> extracting design and {{technical}} information from software source code by reverse-engineering, used by programmers for software maintenance [...] , modernization or optimization.|$|E
50|$|Astrée is {{a static}} <b>program</b> <b>analyzer</b> that proves {{the absence of}} {{run-time}} errors in safety-critical embedded applications written or automatically generated in C. It {{is used in the}} Defense/Aerospace, Medical, Industrial Control, Electronic, Telecom/Datacom and Transportation industries. Astrée originates from the group of Patrick Cousot at CNRS/ENS and is developed and distributed by AbsInt under license from the CNRS/ENS.|$|E
5000|$|Bernd Bruegge, Tim Gottschalk, Bin Luo. A {{framework}} for dynamic <b>program</b> <b>analyzers.</b> In ACM SIGPLAN Notices, 28(10), 1993.|$|R
50|$|Frama-C {{stands for}} Framework for Modular Analysis of C programs. Frama-C {{is a set}} of {{interoperable}} <b>program</b> <b>analyzers</b> for C <b>programs.</b> Frama-C has been developed by Commissariat à l'Énergie Atomique et aux Énergies Alternatives (CEA-List) and Inria. Frama-C, as a static <b>analyzer,</b> inspects <b>programs</b> without executing them.|$|R
40|$|Several program {{analysis}} {{problems can}} be cast elegantly as a logic program. In this paper we show how recently-developed techniques for incremental evaluation of logic programs can be refined and used for deriving practical implementations of incremental <b>program</b> <b>analyzers.</b> Incremental <b>program</b> <b>analyzers</b> compute the changes to the analysis information due to small changes in the input program rather than re-analyzing the <b>program.</b> Demand-driven <b>analyzers</b> compute only the information requested by the client analysis/optimization. We describe a framework based on logic programming for implementing program analyses that combines incremental and demand driven techniques. We show {{the effectiveness of this}} approach by building a practical incremental and demanddriven context insensitive points-to analysis and evaluating this implementation for analyzing C programs with 10 - 70 K lines of code. Experiments show that our technique can compute the changes to analysis information due to small changes in the input program in, on the average, 6 % of {{the time it takes to}} reanalyze the program from scratch, and with little space overhead...|$|R
5000|$|Another {{important}} contribution of Paul Lazarsfeld was his advancement to media effects research {{that he was}} able to bring into fruition. Lazarsfeld’s “most important methodological contributions were the Lazarsfeld-Stanton <b>Program</b> <b>Analyzer</b> and focus group interviewing” according to Everett Rogers. The Lazarsfeld-Stanton <b>Program</b> <b>Analyzer,</b> or ‘Little Annie’ as it was called, provided audience members with a device that had a red button and a green button. When an experimental audience member viewed mediated content, they were able instantly communicate through the two buttons if what they witnessed was likable or it was not. [...] The second research method that was used in tandem with Little Annie was focus group interviewing. After using the tool and viewing the artifact, the participants of the study then filled out a questionnaire, and then discussed the content. [...] The tool was a boon because it allowed for broadcast content to be revised and also be rated for effectiveness. This tool was useful to truly measure audience analysis and reception of a message via a mediated channel. These tools produced both qualitative and quantitative data.|$|E
50|$|A third {{research}} project {{was that of}} listening habits. Because of this, a new method was developed to survey an audience - this was dubbed the Little Annie Project. The official name was the Stanton-Lazarsfeld <b>Program</b> <b>Analyzer.</b> This allowed one not {{only to find out}} if a listener liked the performance, but how they felt at any individual moment, through a dial which they would turn to express their preference (positive or negative). This has since become an essential tool in focus group research.|$|E
50|$|Terminator, a {{research}} project at Microsoft Research, is an automated <b>program</b> <b>analyzer</b> that aims to find whether a program can run infinitely (called a termination analysis). It supports nested loops and recursive functions, pointers and side-effects, and function-pointers as well as concurrent programs. Like all programs for termination analysis it tries to solve the halting problem for particular cases, since the general problem is undecidable. It provides a solution which is sound, meaning that when it states that a program does always terminate, the result is dependable.|$|E
40|$|In {{this paper}} we define a special class of graph rewrite systems for program analysis: edge {{addition}} rewrite systems (Ears). Ears {{can be applied}} to distributive data-flow frameworks over finite lattices [Hec 77] [RSH 94], as well as many other program analysis problems. We also present some techniques for optimized evaluation of Ears. They show that Ears are very well suited for generating efficient <b>program</b> <b>analyzers...</b>|$|R
40|$|In {{order to}} {{automatically}} infer the resource consumption of <b>programs,</b> <b>analyzers</b> track how data sizes change along a <b>program's</b> execution. Typically, <b>analyzers</b> measure {{the sizes of}} data by applying norms which are mappings from data to natural numbers that represent thesizes of the corresponding data. When norms are de_ned by taking typeinformation into account, they are named typed-norms. The main contributionof this paper is a transformational approach to resource analysiswith typed-norms. The analysis {{is based on a}} transformation of the programinto an intermediate abstract program in which each variable isabstracted with respect to all considered norms which are valid for itstype. We also sketch a simple analysis {{that can be used to}} automaticallyinfer the required, useful, typed-norms from programs...|$|R
40|$|AbstractOut of {{annotated}} programs proof carrying code systems {{construct and}} prove verification conditions that guarantee a given safety policy. The annotations {{may come from}} various <b>program</b> <b>analyzers</b> and must not be trusted as {{they need to be}} verified. A generic verification condition generator can be utilized such that a combination of annotations is verified incrementally. New annotations may be verified by using previously verified ones as trusted facts. We show how results from a trusted type analyzer may be combined with untrusted interval analysis to automatically verify that bytecode programs do not overflow. All trusted components are formalized and verified in Isabelle/HOL...|$|R
50|$|AbsInt is a 1998 {{spin-off}} {{from the}} Department for Programming Languages and Compilers at the Saarland University, where its founders {{had developed a}} generic and generative framework for binary-level static program analyzers and optimizers. An important component of this framework is the <b>Program</b> <b>Analyzer</b> Generator PAG, which allows to automatically generate static analyzers from a mathematical specification of the abstract domains and transfer functions. The first version of PAG was released in 1995. With PAG/WWW, a free academic version of PAG is available which has been used worldwide in numerous teaching courses.|$|E
40|$|Abstract. We verify three {{decision}} procedures on context-free grammars {{utilized in}} a <b>program</b> <b>analyzer</b> for a server-side programming language. One {{of the procedures}} decides inclusion between a context-free language and a regular language. The other two decide decision problems related to the well-formedness and validity of XML documents. From its formalization, we generate executable code for a balancedness checking procedure and incorporate it into an existing <b>program</b> <b>analyzer.</b> ...|$|E
40|$|A program {{analysis}} for documentation (PAD) written in FORTRAN has three steps: listing the variables, describing {{the structure and}} writing the program specifications. Technical notes on editing criteria for reviewing program documentation, technical notes for PAD, and FORTRAN <b>program</b> <b>analyzer</b> for documentation are appended...|$|E
40|$|Program {{analysis}} is mainly {{concerned with the}} design of <b>program</b> <b>analyzers</b> to automatically determine semantic properties of programs written in some <b>programming</b> language. Such <b>analyzers</b> take <b>programs</b> as input and output some useful information about their runtime behavior. This information debuggers [1], models-checkers [2], formal verifiers [13], etc. The difficulty of the task {{comes from the fact}} that programs are infinite states so that all interesting questions about program executions are undecidable. Hence the automatically produced information, although sound, must be incomplete. With the appearance of new computing paradigms, the scope of program analysis has been constantly broadening these last two decades. The term “program static analysis ” is therefore too restricted since the analysis problem appears as soon as one considers computer systems with states which evolve continuously or discretely over time, from term rewriting to communication protocols, critical embedded real-time systems and imag...|$|R
40|$|Abstract. In {{classifying}} malware, an open {{research question}} is how to combine similar extracted data from <b>program</b> <b>analyzers</b> {{in such a way that}} the advantages of the analyzers accrue and the errors are minimized. We propose an approach to fusing multiple program analysis outputs by abstracting the features to a common form and utilizing a disjoint union fusion function. The approach is evaluated in an experiment measuring classification accuracy on fused dynamic trace data on over 18, 000 mal-ware files. The results indicate that a näıve fusion approach can yield improvements over non-fused results, but the disjoint union fusion func-tion outperforms näıve union by a statistically significant amount in three of four classification methods applied. ...|$|R
40|$|Abstract. Parallel {{programming}} is rapidly gaining importance as a vector to develop high performance applications that exploit the improved capabilities of modern computer architectures. In consequence, {{there is a}} need to develop analysis and verification methods for parallel programs. Sequoia is a language designed to program parallel divide-and-conquer programs over a hierarchical, tree-structured, and explicitly managed memory. Using abstract interpretation, we develop a compositional proof system to analyze Sequoia programs and reason about them. Then, we show that common program optimizations transform provably correct Sequoia programs into provably correct Sequoia programs, provided the specification and the proof of the original program are strengthened by certifying analyzers, an extension of <b>program</b> <b>analyzers</b> that produce a derivation that the results of the analysis are correct. ...|$|R
30|$|The {{prototype}} {{provides a}} few primitives for programmers or <b>program</b> <b>analyzer</b> {{to mark the}} security-critical data load and store operations. We provide two case studies on different applications that show DTrace’s primitives are {{easy to use and}} effective to enhance the data integrity property in applications.|$|E
40|$|In this paper, we {{describe}} a system called C-Tutor, {{which is an}} intelligent tutoring system (ITS) for novice C programmers. A <b>program</b> <b>analyzer</b> {{is the most important}} part of the ITS for programming. The <b>program</b> <b>analyzer</b> of our system is a compound of a reverse engineering system and a didactical system. Since a novice program usually contains lots of bugs, information about the intentions of the programmer is inevitable to recognize a buggy program. In our approach, the intentions of a programmer are automatically extracted as a problem description from a sample program by a reverse engineering system called GOES (GOal Extraction System, [4]). Based on the problem description, students' programs are recognized by a didactical system called ExBug (Execution-guided deBugger, [15]). As a learning environment, Curriculum Network [16] constructs the knowledge base as genetic graphs to teach programming...|$|E
40|$|We have designedand {{implemented}} an interprocedural <b>program</b> <b>analyzer</b> generator, called system Z. Our goal is {{to automate}} the generation and management of semantics-based interprocedural program analysis {{for a wide range}} of target languages. System Z is based on the abstract interpretation framework. The input to system Z is a high-level specification of an abstract interpreter. The output is a C code for the specified interprocedural <b>program</b> <b>analyzer.</b> The system provides a high-level command set (called projection expressions) in which the user can tune the analysis in accuracy and cost. The user writes projection expressions for selected domains; system Z takes care of the remaining things so that the generated analyzer conducts an analysis over the projected domains, which will vary in cost and accuracy according to the projections. We demonstrate the system's capabilities by experiments with a set of generated analyzers which can analyze C, FORTRAN, and SCHEME programs...|$|E
40|$|This paper {{introduces}} {{a framework for}} rapid prototyping of object oriented programming languages and corresponding analysis tools. It is based on formal definitions of language features in rewrite logic, a simple and intuitive logic for concurrency with powerful tool support. A domain-specific front-end consisting of a notation and a technique, called K, allows for compact, modular, expressive and easy to understand and change definitions of language features. The framework is illustrated by first defining KOOL, an experimental concurrent object-oriented language with exceptions, and then by discussing the definition of JAVA. Generic rewrite logic tools, such as efficient rewrite engines and model checkers, {{can be used on}} language definitions and yield interpreters and corresponding formal <b>program</b> <b>analyzers</b> at no additional cost...|$|R
40|$|Abstract. We {{present a}} modular {{framework}} for building assembly-language <b>program</b> <b>analyzers</b> {{by using a}} pipeline of decompilers that gradually lift {{the level of the}} language to something appropriate for sourcelevel analysis tools. Each decompilation stage contains an abstract interpreter that encapsulates its findings about the program by translating the program into a higher-level intermediate language. For the hardest decompilation tasks a decompiler may request information from higherlevel stages in the pipeline. We provide evidence for the modularity of this framework through the implementation of multiple decompilation pipelines for both x 86 and MIPS assembly produced by gcc, gcj, and coolc (a compiler for a pedagogical mini-Java language) that share several low-level components. Finally, we discuss our experimental results that apply the BLAST model checker for C and the Cqual analyzer to decompiled assembly. ...|$|R
40|$|Abstract. In {{automatic}} software verification, we {{have observed}} a theoretical convergence of model checking and program analysis. In practice, however, model checkers are still mostly concerned with precision, e. g., {{the removal of}} spurious counterexamples; for this purpose they build and refine reachability trees. Lattice-based <b>program</b> <b>analyzers,</b> on the other hand, are primarily concerned with efficiency. We designed an algorithm and built a tool that can be configured to perform not only a purely tree-based or a purely lattice-based analysis, but offers many intermediate settings {{that have not been}} evaluated before. The algorithm and tool take one or more abstract interpreters, such as a predicate abstraction and a shape analysis, and configure their execution and interaction using several parameters. Our experiments show that such customization may lead to dramatic improvements in the precision-efficiency spectrum. ...|$|R
40|$|ASTRÉE is an {{abstract}} interpretation-based static <b>program</b> <b>analyzer</b> aiming at proving automatically {{the absence of}} run time errors in programs written in the C programming language. It has been applied with success to large embedded control-command safety critical realtime software generated automatically from synchronous specifications, producing a correctness proof for complex software without any false alarm {{in a few hours}} of computation...|$|E
40|$|This paper {{presents}} a constraint-based method for generating universally quantified loop invariants over array and scalar variables. Constraints are solved {{by means of}} an SMT solver, thus leveraging recent advances in SMT solving for the theory of non-linear arithmetic. The method has been implemented in a prototype of <b>program</b> <b>analyzer,</b> and a wide sample of examples illustrating its power is shown...|$|E
40|$|This paper proposes prepaging scheme using static program {{analysis}} {{to improve the}} predictability of execution behaviors of applications in embedded systems equipped with flash memory. We built a static <b>program</b> <b>analyzer</b> and the RTOS-based reference platform equipped with flash memory implementing the demand prepaging scheme proposed in this paper. Evaluation {{results show that the}} proposed prepaging scheme shows competitive execution times and improved memory consumption and prepaging hit ratio results compared to sequential prepaging...|$|E
50|$|During the Second World War, Thomas was {{involved}} in early computation systems and <b>programmed</b> the differential <b>analyzer</b> to calculate firing tables for the Navy.|$|R
40|$|Here 2 ̆ 7 s {{a logical}} {{approach}} to solving your herd 2 ̆ 7 s reproductive, nutritional, genetic and milk quality problems using the Dairy Herd <b>Analyzer</b> <b>program.</b> The Dairy Herd <b>Analyzer</b> <b>program</b> was developed at Kansas State University. The program uses {{information on the}} DHIA- 202 form to aid a producer in identifying areas {{that need to be}} addressed to improve productivity and overall herd management. The analysis specifically targets four management areas: reproduction, milk quality, nutrition and genetics...|$|R
40|$|This work is {{motivated}} {{by the fact that}} a “compact ” semantics for functional logic languages, which is essential for the development of efficacious semantics-based program manipulation tools (e. g. automatic <b>program</b> <b>analyzers</b> and debuggers), does not exist. The operational or the rewriting logic semantics that are most commonly considered in functional logic are unnecessarily oversized, as they contains many “semantically useless ” elements that can be retrieved from a smaller set of “basic” elements. Therefore, in this article, we present a compressed, goalindependent bottom-up fixpoint semantics that is correct and minimal w. r. t. answers computed for Curry expressions. We believe that the compactness of the semantics makes it particularly suitable for applications. Actually, our semantics can be finite whereas the big-step semantics is generally not, and even when both semantics are infinite, the fixpoint computation of our semantics produces fewe...|$|R
40|$|The {{development}} of the C-light project resulted {{in the application of}} new formalisms and implementation techniques which facilitate the verification process. The mixed axiomatic semantics proposes a choice between simplified and full-strength deduction rules depending on program objects and their properties. The LLVM infrastructure helps greatly in writing the C-light <b>program</b> <b>analyzer</b> and translator. The semantical labeling technique, proposed earlier, can now be safely kept in verification conditions during their proof. Two programs from the well-known verification benchmarks illustrate the applicability of the system. </p...|$|E
40|$|We {{report on}} a {{successful}} preliminary experience {{in the design and}} implementation of a special-purpose Abstract Interpretation based static <b>program</b> <b>analyzer</b> for the verification of safety critical embedded real-time software. The analyzer is both precise (zero false alarm in the considered experiment) and e#cient (less than one minute of analysis for 10, 000 lines of code). Even if it is based on a simple interval analysis, many features have been added to obtain the desired precision: expansion of small arrays, widening with several thresholds, loop unrolling, trace partitioning, relations between loop counters and other variables...|$|E
40|$|Software {{security}} {{has become an}} increasingly important issue for information and software system. Securevulnerabilities of software system may cause a company out of business and even destroy the social normaloperation. How to improve software security becomes a critical issue in software development process. Inthis paper, utilizing the static <b>program</b> <b>analyzer</b> and dynamic simulation analyzer to collect metrics,proposes an Analyzer-based Software Security Measurement (ASSM) model. Applying ASSM model, thesecure flaws of software system can be identified clearly. And, using a Rule-based Software SecurityImprovement (RSSI) operation to control and improve security defects and security vulnerability ofsoftware system. The security risk of software system can be reduced efficiently...|$|E
40|$|Logic <b>program</b> <b>analyzers</b> {{typically}} employ {{abstract interpretation}} and consist of two main components: an abstract domain and a generic iterative fixed-point computation or "engine". In earlier work, we presented a new engine {{based on an}} unfolding operation, and showed that it was uniformly more accurate than the standard engine. This unfolding engine however had a crucial limitation: {{it could not be}} generalized to constraint logic programming (CLP). In this paper, we develop a fundamentally different view of unfolding, based on a uniform treatment of constraints. The result is a family of analysis algorithms which is modular in the sense that CLP is modular over computation domains. Not only is the new engine more general, but it is also simpler and more flexible. In particular, it provides new opportunities for tuning efficiency/accuracy tradeoffs. At one end of the spectrum, the engine can be used to describe the standard engine; a [...] ...|$|R
40|$|Abstract: The norm ISO- 26262 aims at ascertaining the {{functional}} safety of Automo-tive Electric/Electronic Systems. It is {{not focused on}} purely functional system proper-ties, but also demands to exclude nonfunctional safety hazards in case they are critical for a correct functioning of the system. Examples are violations of timing constraints in real-time software and software crashes due to runtime errors or stack overflows. The ISO- 26262 ranks the static verification of program properties among the promi-nent goals of the software design and implementation phase. Static <b>program</b> <b>analyzers</b> are available that can prove the absence of certain non-functional programming errors, including those mentioned above. Static analyzers can be applied {{at different stages of}} the development process and can be used to complement or replace dynamic test methods. This article gives an overview of static program analysis techniques focusing on non-functional program properties, investigates the non-functional requirements o...|$|R
5000|$|CW Skimmer is a {{multi-channel}} Morse code (CW) decoder and <b>analyzer</b> <b>program</b> for Microsoft Windows. It {{was created}} by Alex Shovkoplyas, VE3NEA, and is marketed by Afreet Software, Inc.|$|R
