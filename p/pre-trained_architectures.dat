1|11|Public
40|$|In {{this paper}} we propose an {{implement}} a general convolutional neural network (CNN) building framework for designing real-time CNNs. We validate our models by creating a real-time vision system which accomplishes the tasks of face detection, gender classification and emotion classification simultaneously in one blended step using our proposed CNN architecture. After presenting {{the details of the}} training procedure setup we proceed to evaluate on standard benchmark sets. We report accuracies of 96 % in the IMDB gender dataset and 66 % in the FER- 2013 emotion dataset. Along with this we also introduced the very recent real-time enabled guided back-propagation visualization technique. Guided back-propagation uncovers the dynamics of the weight changes and evaluates the learned features. We argue that the careful implementation of modern CNN architectures, the use of the current regularization methods and the visualization of previously hidden features are necessary {{in order to reduce the}} gap between slow performances and real-time architectures. Our system has been validated by its deployment on a Care-O-bot 3 robot used during RoboCup@Home competitions. All our code, demos and <b>pre-trained</b> <b>architectures</b> have been released under an open-source license in our public repository. Comment: Submitted to ICRA 201...|$|E
30|$|Step 1 : Randomly {{select a}} batch of {{training}} data as input to the <b>pre-trained</b> <b>architecture.</b>|$|R
40|$|The {{ability to}} {{classify}} objects is fundamental for robots. Besides knowledge about their visual appearance, {{captured by the}} RGB channel, robots heavily need also depth information {{to make sense of}} the world. While the use of deep networks on RGB robot images has benefited from the plethora of results obtained on databases like ImageNet, using convnets on depth images requires mapping them into three dimensional channels. This transfer learning procedure makes them processable by <b>pre-trained</b> deep <b>architectures.</b> Current mappings are based on heuristic assumptions over preprocessing steps and on what depth properties should be most preserved, resulting often in cumbersome data visualizations, and in sub-optimal performance in terms of generality and recognition results. Here we take an alternative route and we attempt instead to learn an optimal colorization mapping for any given <b>pre-trained</b> <b>architecture,</b> using as training data a reference RGB-D database. We propose a deep network architecture, exploiting the residual paradigm, that learns how to map depth data to three channel images. A qualitative analysis of the images obtained with this approach clearly indicates that learning the optimal mapping preserves the richness of depth information better than current hand-crafted approaches. Experiments on the Washington, JHUIT- 50 and BigBIRD public benchmark databases, using CaffeNet, VGG 16, GoogleNet, and ResNet 50 clearly showcase the power of our approach, with gains in performance of up to 16 % compared to state of the art competitors on the depth channel only, leading to top performances when dealing with RGB-D dat...|$|R
40|$|We {{question}} the dominant role of real-world training {{images in the}} field of material classification by investigating whether synthesized data can generalise more effectively than real-world data. Experimental results on three challenging real-world material databases show that the best performing pre-trained convolutional neural network (CNN) architectures can achieve up to 91. 03 % mean average precision when classifying materials in cross-dataset scenarios. We demonstrate that synthesized data achieve an improvement on mean average precision when used as training data and in conjunction with <b>pre-trained</b> CNN <b>architectures,</b> which spans from ~ 5 % to ~ 19 % across three widely used material databases of real-world images. Comment: accepted for publication in VISAPP 2018. arXiv admin note: text overlap with arXiv: 1703. 0410...|$|R
30|$|Recently, {{there has}} been a trend in {{exploiting}} features generated by CNNs in computer vision, especially in the field of object recognition and detection [6]. A comprehensive evaluation further demonstrates the advantages of deep CNNs features with respect to shallow hand-crafted feature for image classification [22]. The advantage is that researchers will be free from mastering the knowledge of specific domains and the CNNs architecture can be used for many different domains, especially in visual systems with minor changes. CNN is a well-known architecture proposed by LeCun et al. [23] to recognize hand-written digits. Several research groups have recently shown that CNNs outperform classical approaches for object classification or detection that are based on hand-crafted features [8, 24]. The open-source software Caffe [25] provides <b>pre-trained</b> CNNs <b>architectures</b> for a variety of recognition tasks, which greatly reduces the difficulty in deploying and training CNNs for different tasks.|$|R
40|$|There {{has been}} {{fascinating}} work on creating artistic transformations of images by Gatys. This was revolutionary {{in how we}} can in some sense alter the 'style' of an image while generally preserving its 'content'. In our work, we present a method for creating new sounds using a similar approach, treating it as a style-transfer problem, starting from a random-noise input signal and iteratively using back-propagation to optimize the sound to conform to filter-outputs from a <b>pre-trained</b> neural <b>architecture</b> of interest. For demonstration, we investigate two different tasks, resulting in bandwidth expansion/compression, and timbral transfer from singing voice to musical instruments. A feature of our method is that a single architecture can generate these different audio-style-transfer types using {{the same set of}} parameters which otherwise require different complex hand-tuned diverse signal processing pipelines. Comment: Appeared in 31 st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA at the workshop for Machine Learning for Creativity and Desig...|$|R
40|$|We {{present a}} robust deep {{learning}} based 6 degrees-of-freedom (DoF) localization system for endoscopic capsule robots. Our system mainly focuses on localization of endoscopic capsule robots inside the GI tract using only visual information captured by a mono camera integrated to the robot. The proposed {{system is a}} 23 -layer {{deep convolutional neural network}} (CNN) that is capable to estimate the pose of the robot in real time using a standard CPU. The dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture, softness, and surface liquid properties so that the <b>pre-trained</b> CNN <b>architecture</b> can be transferred confidently into a real endoscopic scenario. An average error of 7 : 1 % and 3 : 4 % for translation and rotation has been obtained, respectively. The results accomplished from the experiments demonstrate that a CNN pre-trained with raw 2 D endoscopic images performs accurately inside the GI tract and is robust to various challenges posed by reflection distortions, lens imperfections, vignetting, noise, motion blur, low resolution, and lack of unique landmarks to track...|$|R
40|$|Object {{detection}} {{is crucial}} in the environment of autonomous driving and advance driver assistance systems for safely maneuvring vehicle in the urban traffic. Among the traffic participants we find pedestrians are the one who are most vulnerable and their safety is also crucial. Therefore, this work focuses on pedestrian detection in urban environment using the camera mounted on ego vehicle. The thesis aims at understanding and comparison of shallow and deep learning approaches for pedestrian detection, and two ensemble methods are proposed that combines the chosen deep and shallow method with the context-based classifier respectively. Firstly, an <b>pre-trained</b> deep <b>architecture</b> for object detection is combined with the context-based classifier. Whereas, in second method shallow approach is combined with context-based classifier. Further in the outlook of this work stereo data is used to minimize the detected false positives form the proposed ensemble deep approach. Prototyping of first proposed method is achieved using the CAFFE deep learning framework with Python interface, and the second shallow method is achieved using the well known computer vision library OpenCV with C++. The proposed method is trained, tested and evaluated on Caltech pedestrian dataset with diâ†µerent metri...|$|R
40|$|Real-time road-scene {{understanding}} is a challenging computer vision task with {{recent advances in}} convolutional neural networks (CNN) achieving results that notably surpass prior traditional feature driven approaches. Here, we take an existing CNN <b>architecture,</b> <b>pre-trained</b> for urban road-scene understanding, and retrain it towards the task of classifying off-road scenes, assessing the network performance within the training cycle. Within the paradigm of transfer learning we analyse the effects on CNN classification, by training and assessing varying levels of prior training on varying sub-sets of our off-road training data. For each of these configurations, we evaluate the network at multiple points during its training cycle, allowing us to analyse in depth exactly how the training process is affected by these variations. Finally, we compare this CNN to a more traditional approach using a feature-driven Support Vector Machine (SVM) classifier and demonstrate state-of-the-art results in this particularly challenging problem of off-road scene understanding...|$|R
40|$|We {{propose a}} new method that uses deep {{learning}} techniques {{to accelerate the}} popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two <b>pre-trained</b> deep convolutional <b>architectures.</b> More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i. e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i. e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems...|$|R
30|$|So far we {{have shown}} that {{representing}} the template frames by GMMs and using the local distance measures of LLR or KL significantly improved the accuracy performance over our HMM baselines, and the proposed methods are much {{more effective than the}} conventional template matching methods where the template frames use the original speech features. A question naturally arises is how would the proposed template-matching methods interact with an underlying acoustic model from which the GMMs are derived and the phone or word lattices are generated, and of particular interest is that as a baseline HMM system improves, whether the performance gain we have observed by the proposed template matching methods can still hold. This is a relevant issue since a baseline HMM system can be improved by using more advanced training methods and better features. Recently, a major advance has been made in using deep neural networks (DNNs) with many hidden layers for speech acoustic modeling, where the resulting DNNs learn a hierarchy of nonlinear feature detectors that can capture complex statistical patterns for speech data. For example, context-independent, <b>pre-trained</b> DNN/HMM hybrid <b>architectures</b> have achieved competitive performance in TIMIT phone recognition [38], context-dependent DNN/HMM has led to large improvements to several public domain large speech recognition tasks [39], and dumping features from deep convolutional neural networks to train GMM/HMM-based systems achieved higher accuracy performance than DNN/HMM hybrid architectures in several tasks [40].|$|R
40|$|We {{present a}} new local {{descriptor}} for 3 D shapes, directly applicable {{to a wide}} range of shape analysis problems such as point correspondences, semantic segmentation, affordance prediction, and shape-to-scan matching. The descriptor is produced by a convolutional network that is trained to embed geometrically and semantically similar points close to one another in descriptor space. The network processes surface neighborhoods around points on a shape that are captured at multiple scales by a succession of progressively zoomed out views, taken from carefully selected camera positions. We leverage two extremely large sources of data to train our network. First, since our network processes rendered views in the form of 2 D images, we repurpose <b>architectures</b> <b>pre-trained</b> on massive image datasets. Second, we automatically generate a synthetic dense point correspondence dataset by non-rigid alignment of corresponding shape parts in a large collection of segmented 3 D models. As a result of these design choices, our network effectively encodes multi-scale local context and fine-grained surface detail. Our network can be trained to produce either category-specific descriptors or more generic descriptors by learning from multiple shape categories. Once trained, at test time, the network extracts local descriptors for shapes without requiring any part segmentation as input. Our method can produce effective local descriptors even for shapes whose category is unknown or different from the ones used while training. We demonstrate through several experiments that our learned local descriptors are more discriminative compared to state of the art alternatives, and are effective in a variety of shape analysis applications...|$|R

