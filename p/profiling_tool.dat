278|753|Public
5000|$|... #Subtitle level 2: UN-Habitat's City Resilience <b>Profiling</b> <b>Tool</b> (CRPT) ...|$|E
5000|$|Specify a <b>profiling</b> <b>tool</b> for the development/component {{unit test}} {{environment}} ...|$|E
50|$|JProfiler is {{a commercially}} {{licensed}} Java <b>profiling</b> <b>tool</b> developed by ej-technologies GmbH, targeted at Java EE and Java SE applications.|$|E
50|$|Application <b>profiling</b> <b>tools.</b>|$|R
40|$|In-situ {{resistivity}} {{data obtained}} using a hydrostatic <b>profile</b> <b>tool</b> are compared with resistivity measurements obtained from {{soil and water}} samples in the laboratory. Resistivity was then compared with porosity measurements to demonstrate that Archie's law {{can be used in}} natural soft clay deposits. Archie's law is then used to convert in-situ bulk conductivity measurements made with a hydrostatic <b>profile</b> <b>tool</b> to po-rosity. Porosities estimated using the hydrostatic <b>profile</b> <b>tool</b> were found to be similar to the measured porosi-ties...|$|R
5000|$|Performance {{analysis}} (or <b>profiling</b> <b>tools)</b> {{that can}} help to highlight hot spots and resource usage ...|$|R
5000|$|Sequerome - A {{sequence}} <b>profiling</b> <b>tool</b> {{that links}} each BLAST record to the NCBI ORF enabling complete ORF {{analysis of a}} BLAST report.|$|E
5000|$|As the UN Agency for Human Settlements, UN-Habitat {{is working}} to support local governments and their {{stakeholders}} build urban resilience through the City Resilience <b>Profiling</b> <b>Tool</b> (CRPT). When applied, UN-Habitat's holistic approach to increasing resiliency results in local governments that {{are better able to}} ensure the wellbeing of citizens, protect development gains and maintain functionality in the face of hazards. The tool developed by UN-Habitat to support local governments achieve resilience is the City Resilience <b>Profiling</b> <b>Tool.</b> The Tool follows various stages and UN-Habitat supports cities to maximize the impact of CRPT implementation.|$|E
50|$|After {{translating}} TNSDL to C, any standard-compliant C compiler, linker, coverage {{measurement and}} <b>profiling</b> <b>tool</b> can be used. To make source-level debugging possible TNSDL puts line number {{references to the}} generated C code.|$|E
50|$|Most of the <b>profiling</b> <b>tools</b> {{available}} on the web today fall into this category. The user, upon visiting the site/tool, enters any relevant information like a keyword e.g. dystrophy, diabetes etc., or GenBank accession numbers, PDB ID. All the relevant hits by the search are presented in a format unique to each <b>tool’s</b> main focus. <b>Profiling</b> <b>tools</b> based on keyword searches are essentially search engines that are highly specialized for bioinformatics work, thereby eliminating a clutter of irrelevant or non-scholarly hits that might occur with a traditional search engine like Google. Most keyword-based <b>profiling</b> <b>tools</b> allow flexible types of keyword input, accession numbers from indexed databases as well as traditional keyword descriptors.|$|R
50|$|In August 2012, AfterCollege relaunched as a {{professional}} network for college students and recent graduates, offering <b>profile</b> <b>tools</b> and revamped job matching.|$|R
50|$|Prior to joining Google, {{he was at}} DEC/Compaq's Western Research Laboratory, {{where he}} worked on <b>profiling</b> <b>tools,</b> {{microprocessor}} architecture, and information retrieval.|$|R
50|$|Tcov, {{a source}} code {{coverage}} analysis and statement-by-statement <b>profiling</b> <b>tool,</b> {{comes as a}} standard utility. Tcov generates exact counts {{of the number of}} times each statement in a program is executed and annotates source code to add instrumentation.|$|E
50|$|In computing, OProfile is a {{system-wide}} statistical <b>profiling</b> <b>tool</b> for Linux. John Levon {{wrote it}} in 2001 for Linux kernel version 2.4 after his M.Sc. project; {{it consists of}} a kernel module, a user-space daemon and several user-space tools.|$|E
5000|$|... gcov {{creates a}} logfile called sourcefile.gcov which {{indicates}} {{how many times}} each line of a source file sourcefile.c has executed. This annotated source file {{can be used with}} gprof, another <b>profiling</b> <b>tool,</b> to extract timing information about the program.|$|E
50|$|In 2012 two IBM {{engineers}} recognized OProfile {{as one of}} the {{two most}} commonly used performance counter monitor <b>profiling</b> <b>tools</b> on Linux, alongside perf tool.|$|R
5000|$|Professional—Targeted at Symbian OS phone manufacturers, their partners, and application/middleware vendors {{working on}} {{demanding}} projects. Contained Developer features, system-level on-device debugging, and performance <b>profiling</b> <b>tools.</b>|$|R
50|$|Additionally, Mozilla Firefox {{implemented}} built-in WebGL tools {{starting with}} version 27 that allow editing vertex and fragment shaders. A {{number of other}} debugging and <b>profiling</b> <b>tools</b> have also emerged.|$|R
50|$|Rational Application Developer {{includes}} {{tools to}} improve code quality. A Java <b>profiling</b> <b>tool</b> helps to analyze an application's performance, memory usage, and threading problems. A software analysis tool identifies patterns and antipatterns in application code, and compares code to coding standards.|$|E
50|$|Gcov is {{a source}} code {{coverage}} analysis and statement-by-statement <b>profiling</b> <b>tool.</b> Gcov generates exact counts {{of the number of}} times each statement in a program is executed and annotates source code to add instrumentation. Gcov comes as a standard utility with the GNU Compiler Collection (GCC) suite.|$|E
5000|$|LPROF (or LCMS Profiler) is a GUI {{tool for}} {{producing}} profiles for cameras, scanners and monitors. LPROF was {{originally designed to}} demonstrate LittleCMS (LCMS) capabilities. In 2005, LPROF development was resumed by a new team, which produced its latest extended version of LPROF in 2006. LPROF remains the only fully GUI-based <b>profiling</b> <b>tool</b> for Linux.|$|E
50|$|The Graded Care <b>Profile</b> <b>Tool</b> is a {{practice}} tool which gives an objective {{measure of the}} quality of care in terms of a parent/carer's commitment. It was developed in the UK.|$|R
3000|$|From this investigation, it {{is found}} that the joint made by taper {{threaded}} pin <b>profiled</b> <b>tool</b> underwater cooling medium exhibited higher tensile properties of 345  MPa and joint efficiency of 76  %.|$|R
5000|$|A MLS {{facilitates}} {{the collection of}} subsurface head and chemical data over time, {{which is something that}} cannot be accomplished using [...] "one-time" [...] <b>profiling</b> <b>tools</b> like CPT or the Waterloo Profiler.|$|R
50|$|Tcov is {{a source}} code {{coverage}} analysis and statement-by-statement <b>profiling</b> <b>tool</b> for software written in Fortran, C and C++. Tcov generates exact counts {{of the number of}} times each statement in a program is executed and annotates source code to add instrumentation. It is a standard utility, provided free of cost with Sun Studio software under Sun Studio product license.|$|E
5000|$|Sequerome is {{a web-based}} Sequence <b>profiling</b> <b>tool</b> for {{integrating}} {{the results of}} a BLAST sequence-alignment report with external research tools and servers that perform advanced sequence manipulations, and allowing the user to record the steps of such an analysis. Sequerome is a web-based Java tool that acts as a front-end to BLAST queries and provides simplified access to web-distributed resources for protein and nucleic acid analysis.|$|E
50|$|Vector Fabrics' Pareon Profile is a {{predictive}} <b>profiling</b> <b>tool</b> {{based on}} dynamic analysis to explore opportunities and bottlenecks for parallel execution of C and C++ code. The product includes {{a model of}} the target platform (e.g. ARM Android) to predict the performance and power gains of a proposed code rewrite. It has been used a.o. to optimize Blink and Webkit, the engine underlying the Chrome browser, the Bullet Physics engine, the IdTech4 game engine underlying Doom 3, and a number of video codecs and image processing applications.|$|E
30|$|The macro {{features}} of the stir zone exhibit different material flow behaviour. From the macrograph, the stir zone {{can be divided into}} upper shoulder influenced region (SIR), middle pin influenced region (PIR) and lower vortex region (VOR). In both air and water cooling medium, the tunnel defects are observed in the advancing side-PIR of the joints fabricated using STC and TAC <b>profiled</b> <b>tools.</b> But the joints fabricated using THC and TTC <b>profiled</b> <b>tools</b> yielded defect free stir zones in both air and water cooling medium. The defective joints are not considered further analysis, and the defect-free THC and TTC joints alone are considered.|$|R
40|$|Abstract Recent {{years has}} seen the {{development}} of <b>profiling</b> <b>tools</b> for lazy functional language implementations. This paper {{presents the results of}} using a time profiler to profile the Glasgow Haskell compiler. So far as we know ghc is the only lazy functional language compiler to support source-level time profiling. The benefits of having such a tool can be spectacular, as this paper demonstrates. 1 Introduction As the use of lazy functional languages for applications programming has grown there has been a strong call for source-level <b>profiling</b> <b>tools</b> to aid the applications programmer in the identification of execution hot-spots and inefficiencies. Recent years has seen a significant response to this need with a number of <b>profiling</b> <b>tools</b> being proposed and developed [1, 6, 8, 9]. The Glasgow Haskell compiler, ghc, incorporates a profiling technique based on cost centres. These enable both the execution time and space consumption of the program to be profiled. The ideas were first presented in [9], but have undergone significant changes since then. We have now implemented a profiling semantics similar to that described in [1]...|$|R
50|$|Normally, {{purpose-built}} {{tools are}} used for data profiling to ease the process. The computation complexity increases when going from single column, to single table, to cross-table structural profiling. Therefore, performance is an evaluation criterion for <b>profiling</b> <b>tools.</b>|$|R
50|$|A typical {{sequence}} <b>profiling</b> <b>tool</b> carries {{this further}} {{by using an}} actual DNA, RNA, or protein sequence as an input and allows the user to visit different web-based analysis tools to obtain the information desired. Such tools are also commonly supplied with commercial laboratory equipment like gene sequencers or sometimes sold as software applications for molecular biology. In another public-database example, the BLAST sequence search report from NCBI provides a link from its alignment report to other relevant information in its own databases, if such specific information exists.|$|E
50|$|Each <b>profiling</b> <b>tool</b> {{has its own}} {{focus and}} area of interest. For example, the NCBI search engine Entrez segregates its hits by category, so that users looking for protein {{structure}} information can screen out sequences with no corresponding structure, while users interested in perusing the literature on a subject can view abstracts of papers published in scholarly journals without distraction from gene or sequence results. The Pubmed biosciences literature database is a popular tool for literature searches, though this service is nearly equaled with the more general Google Scholar.|$|E
50|$|A {{sequence}} <b>profiling</b> <b>tool</b> in bioinformatics {{is a type}} {{of software}} that presents information related to a genetic sequence, gene name, or keyword input. Such tools generally take a query such as a DNA, RNA, or protein sequence or ‘keyword’ and search one or more databases for information related to that sequence. Summaries and aggregate results are provided in standardized format describing the information that would otherwise have required visits to many smaller sites or direct literature searches to compile. Many sequence profiling tools are software portals or gateways that simplify the process of finding information about a query in the large and growing number of bioinformatics databases. The access to these kinds of tools is either web based or locally downloadable executables.|$|E
40|$|Automated persuasion is the Holy Grail of quantitatively biased {{data base}} designers. However, data base {{histories}} are, at best, probabilistic estimates of customer behavior {{and do not}} make use of more sophisticated qualitative motivational <b>profiling</b> <b>tools.</b> While usually absent from web designer thinking, qualitative motivational profiling can be integrated into data base designs. However, qualitative profiling would require that designers add to their repertoire a set of qualitative motivational <b>profiling</b> <b>tools.</b> Clearly the quantitative or qualitative tool must fit the task. This contemporary confusion is corrected by separating the marketing and market research tools into quantitative or qualitative applications according to the proper roles they play and the tasks they must engage...|$|R
40|$|This paper {{introduces}} {{the practice of}} assessing requirements for research data management (RDM) support in academic libraries, building on concepts of maturity, capability and readiness. An overview of existing RDM assessment methodologies, tools and instruments is presented, with institutional exemplars from the UK and the US. Drawing on consultations with the eScience community, we describe {{the development of the}} Community Capability Model Framework (CCMF), the derived capability factors and the CCM <b>Profile</b> <b>tool.</b> Finally, a Case Study for Agronomy research data is presented, showing how the CCM <b>Profile</b> <b>tool</b> can be applied to disciplinary research, to provide summaries and visualisations of data-intensive capability, which may inform planning for RDM support services in academic libraries...|$|R
40|$|International audienceWe {{measure and}} analyze the {{instruction}} level parallelism that condition the running-time performance of core numerical subroutines. We propose PerPI, a programmer oriented tool {{to fill the gap}} between high level algorithm analysis and machine dependent <b>profiling</b> <b>tools</b> and which provides reproducible results...|$|R
