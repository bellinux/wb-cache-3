0|10000|Public
40|$|An {{alternating}} <b>decision</b> <b>tree</b> is a {{model that}} generalizes <b>decision</b> <b>trees</b> and boosted <b>decision</b> <b>trees.</b> The main advantages of alternating <b>decision</b> <b>trees</b> over boosting regular <b>decision</b> <b>trees</b> are that they are easier to interpret for human experts {{and that they can}} be built ef- ciently. In this work, we introduce rst order alternating <b>decision</b> <b>trees</b> (FOADTs), an upgrade of alternating <b>decision</b> <b>trees</b> to rst order logic...|$|R
40|$|<b>Decision</b> <b>trees</b> are {{commonly}} used for classification. We propose to use <b>decision</b> <b>trees</b> not just for classification {{but also for the}} wider purpose of knowledge discovery, because visualizing the <b>decision</b> <b>tree</b> can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of <b>decision</b> <b>trees.</b> PaintingClass provides an intuitive layout and convenient navigation of the <b>decision</b> <b>tree.</b> PaintingClass also provides the user the means to interactively construct the <b>decision</b> <b>tree.</b> Each node in the <b>decision</b> <b>tree</b> is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a <b>decision</b> <b>tree</b> and explore the <b>decision</b> <b>tree</b> to gain additional knowledge...|$|R
40|$|The article {{presents}} {{the extension of}} a common <b>decision</b> <b>tree</b> concept to a multidimensional- vector- <b>decision</b> <b>tree</b> constructed {{with the help of}} evolutionary techniques. In contrary to the common <b>decision</b> <b>tree</b> the vector <b>decision</b> <b>tree</b> can make more than just one suggestion per input sample. It has the functionality of many separate <b>decision</b> <b>trees</b> acting on a same set of training data and answering different questions. Vector <b>decision</b> <b>tree</b> is therefore simple in its form, is easy to use and analyse and can express some relationships between decisions not visible before. To explore and test the possibilities of this concept we developed a software tool- DecRain- for building vector <b>decision</b> <b>trees</b> using the ideas of evolutionary computing. Generated vector <b>decision</b> <b>trees</b> showed good results in comparison to classical <b>decision</b> <b>trees.</b> The concept of vector <b>decision</b> <b>trees</b> can be safely and effectively used in any decision making process. Keywords...|$|R
40|$|An {{alternating}} <b>decision</b> <b>tree</b> is a {{model that}} generalizes <b>decision</b> <b>trees</b> and boosted <b>decision</b> <b>trees.</b> The main advantages of alternating <b>decision</b> <b>trees</b> over boosting regular <b>decision</b> <b>trees</b> are that they are easier to interpret for human experts {{and that they can}} be built efficiently. In this work, we introduce first order alternating <b>decision</b> <b>trees</b> (FOADTs), an upgrade of alternating <b>decision</b> <b>trees</b> to first order logic. We propose three important efficiency improvements, which make inducing FOADTs computationally feasible. Furthermore, we describe some issues with FOADTs that are specific to the first order setting. We conclude with some preliminary experiments. status: publishe...|$|R
30|$|VFDT ({{very fast}} <b>decision</b> <b>tree)</b> [6] and CVFDT (concept-adapting very fast <b>decision</b> <b>tree)</b> [7] are two {{classical}} and impactive algorithms in incremental <b>decision</b> <b>tree</b> algorithms.|$|R
40|$|Abstract- <b>Decision</b> <b>tree</b> {{classification}} {{technique is}} one of the most popular techniques in the emerging field of data mining. There are various methods for constructing <b>decision</b> <b>tree.</b> Induced <b>Decision</b> <b>tree</b> (ID 3) is the basic algorithm for constructing <b>decision</b> <b>trees.</b> After ID 3 various algorithms were proposed by different researchers and authors those are extensions of ID 3 algorithm. This paper contains a survey about the improved methods of ID 3 <b>decision</b> <b>tree</b> classification and those are FID 3 (fixed induced <b>decision</b> <b>tree)</b> and VPRSFID 3 (variable precision rough set fixed induced <b>decision</b> <b>tree).</b> In this short survey we will investigate which method is best among all the other methods...|$|R
30|$|The {{properties}} of a block-based <b>decision</b> <b>tree</b> {{are also the}} same as a pixel-based <b>decision</b> <b>tree</b> that we have just analyzed. Grana et al. [3] developed a <b>decision</b> <b>tree</b> based on a block-based scan mask as shown in Figure 2 that is an enhancement of the pixel-based scan mask shown in Figure 1. Hence, their <b>decision</b> <b>tree</b> performs very fast if the current 2 × 2 block consists of background pixels when compared with our proposed <b>decision</b> <b>tree</b> method. Our proposed <b>decision</b> <b>tree</b> is an extension of the pixel-based scan mask in Figure 3 to the block-based scan mask in Figure 4. Therefore, our <b>decision</b> <b>tree</b> performs very fast if the current 2 × 2 block is composed of foreground pixels when compared with Grana et al. [3]'s <b>decision</b> <b>tree.</b> This is the reason why the proposed <b>decision</b> <b>tree</b> performs faster than Grana et al. [3] for high density images.|$|R
40|$|The aim of {{the article}} is to analyse and {{thoroughly}} research the methods of construction of the <b>decision</b> <b>trees</b> that use <b>decision</b> <b>tree</b> learning with statement propositionalized attributes. Classical <b>decision</b> <b>tree</b> learning algorithms, as well as <b>decision</b> <b>tree</b> learning with propositionalized attributes have been observed. The article provides the detailed analysis {{of one of the}} methodologies on the importance of using the <b>decision</b> <b>trees</b> in knowledge presentation. The concept of ontology use is offered to develop classification systems of <b>decision</b> <b>trees.</b> The application of the methodology would allow improving the classification accuracy...|$|R
40|$|C 4. 5 {{algorithm}} {{is used to}} simplify <b>decision</b> <b>tree</b> from <b>decision</b> table by generating <b>decision</b> <b>tree</b> from an existing <b>decision</b> <b>tree.</b> With this algorithm, knowledge base in the decision table can be simplified. This research will build a consultation program using C 4. 5 algorithm which is called <b>decision</b> <b>tree</b> generator. <b>Decision</b> <b>tree</b> generator provides inference facility and a user interface for consultation. The user is required to build a knowledge base first, and the application will generate user interface automatically. There are two steps in <b>decision</b> <b>tree</b> generator: firstly, the application will build the <b>decision</b> <b>tree,</b> and after that the application will build the user interface for consultation session. The results of this research show that the <b>decision</b> <b>tree</b> generator can get goal and advice from tree exploration in consultation session...|$|R
40|$|Two <b>decision</b> <b>trees</b> {{are called}} <b>decision</b> {{equivalent}} if {{they represent the}} same function, i. e., they yield the same result for every possible input. We prove that given a <b>decision</b> <b>tree</b> and a number, to decide {{if there is a}} <b>decision</b> equivalent <b>decision</b> <b>tree</b> of size at most that number is NPcomplete. As a consequence, nding a <b>decision</b> <b>tree</b> of minimal size that is decision equivalent to a given <b>decision</b> <b>tree</b> is an NP-hard problem. This result diers from the well-known result of NP-hardness of nding a <b>decision</b> <b>tree</b> of minimal size that is consistent with a given training set. Instead our result is a basic result for <b>decision</b> <b>trees,</b> apart from the setting of inductive inference...|$|R
40|$|Two <b>decision</b> <b>trees</b> {{are called}} {{equivalent}} if {{they represent the}} same function, i. e., they yield the same result for every possible input. We prove that given a <b>decision</b> <b>tree</b> and a number, the problem of deciding {{if there is an}} equivalent <b>decision</b> <b>tree</b> of size at most that number is NP-complete. As a consequence, finding <b>decision</b> <b>tree</b> of minimal size that is decision equivalent to a given <b>decision</b> <b>tree</b> is an NP-complete problem...|$|R
40|$|Abstract: <b>Decision</b> <b>Tree</b> is a {{classification}} method used in Machine Learning and Data Mining. One major aim of {{a classification}} {{task is to}} improve its classification accuracy. In this paper, the experiments presume the induction of the different <b>Decision</b> <b>Trees</b> on four databases, using many attribute selection measures at the splitting of a <b>Decision</b> <b>Tree</b> node, the pruning of a <b>Decision</b> <b>Tree</b> using two pruning methods: confidence level pruning and pessimistic pruning method and finally, the <b>Decision</b> <b>Tree</b> execution on the test dataset to calculate the classification error rate of each <b>Decision</b> <b>Tree.</b> Copyright © 2007 Laviniu Aurelian Badulescu. All rights reserved...|$|R
40|$|We {{address the}} issue of {{compiling}} ML pattern matching to compact and efficient <b>decisions</b> <b>trees.</b> Traditionally, compilation to <b>decision</b> <b>trees</b> is optimized by (1) implementing <b>decision</b> <b>trees</b> as dags with maximal sharing; (2) guiding a simple compiler with heuristics. We first design new heuristics that are inspired by necessity, a concept from lazy pattern matching that we rephrase in terms of <b>decision</b> <b>tree</b> semantics. Thereby, we simplify previous semantic frameworks and demonstrate a straightforward connection between necessity and <b>decision</b> <b>tree</b> runtime efficiency. We complete our study by experiments, showing that optimizing compilation to <b>decision</b> <b>trees</b> is competitive with the optimizing match compile...|$|R
50|$|There are {{basically}} {{two different types}} of DT algorithms: one for inducing <b>decision</b> <b>trees</b> with only nominal attributes and another for inducing <b>decision</b> <b>trees</b> with both numeric and nominal attributes. This aspect of <b>decision</b> <b>tree</b> induction also carries to gene expression programming and there are two GEP algorithms for <b>decision</b> <b>tree</b> induction: the evolvable <b>decision</b> <b>trees</b> (EDT) algorithm for dealing exclusively with nominal attributes and the EDT-RNC (EDT with random numerical constants) for handling both nominal and numeric attributes.|$|R
40|$|Abstract. For the {{well-known}} concept of <b>decision</b> <b>trees</b> {{as it is}} used for inductive inference we study the natural concept of equivalence: two <b>decision</b> <b>trees</b> are equivalent {{if and only if}} they represent the same hypothesis. We present a simple e cient algorithm to establish whether two <b>decision</b> <b>trees</b> are equivalent or not. The complexity of this algorithm is bounded by the product of the sizes of both <b>decision</b> <b>trees.</b> The hypothesis represented by adecision tree is essentially a boolean function, just like a proposition. Although every boolean function can be represented in this way, we show that disjunctions and conjunctions of <b>decision</b> <b>trees</b> can not e ciently be represented as <b>decision</b> <b>trees,</b> and simply shaped propositions may require exponential size for representation as <b>decision</b> <b>trees.</b> ...|$|R
5000|$|Linear <b>decision</b> <b>trees,</b> {{just like}} the simple <b>decision</b> <b>trees,</b> make a {{branching}} decision based {{on a set of}} values as input. As opposed to binary <b>decision</b> <b>trees,</b> linear <b>decision</b> <b>trees</b> have three output branches. A linear function [...] is being tested and branching decisions are made based on the sign of the function (negative, positive, or 0).|$|R
40|$|Abstract:- Because {{the target}} domain of data mining using <b>decision</b> <b>trees</b> usually {{contains}} {{a lot of}} data, sampling is needed. But selecting proper samples for a given <b>decision</b> <b>tree</b> algorithm is not easy, because each <b>decision</b> <b>tree</b> algorithm has its own property in generating trees and selecting appropriate samples that represent given target data set well is difficult. As the size of samples grows, the size of generated <b>decision</b> <b>trees</b> grows with some improvement in error rates. But we cannot use larger and larger samples, because it’s not easy to understand large <b>decision</b> <b>trees</b> and data overfitting problem can happen. This paper suggests a progressive approach in determining a proper sample size to generate good <b>decision</b> <b>trees</b> with respect to generated tree size and accuracy. Experiments with two representative <b>decision</b> <b>tree</b> algorithms, CART and C 4. 5 show very promising results. Key-Words:- <b>decision</b> <b>trees,</b> proper sample size determination...|$|R
40|$|We {{study the}} {{relationships}} between the complexity of a task description and the minimal complexity of deterministic and nondeterministic <b>decision</b> <b>trees</b> solving this task. We investigate <b>decision</b> <b>trees</b> assuming a global approach i. e. arbitrary checks from a given check system can be used for constructing <b>decision</b> <b>trees.</b> Introduction <b>Decision</b> <b>trees</b> are widely used in different fields related to problem solving and knowledge representation. <b>Decision</b> <b>trees</b> over finite check systems are studied in such fields as test theory [3, 6, 7], theory of information systems and of rough sets [11, 15], theory of questionnaires [12], theory of decision tables [4], machine learning [14], and searching theory [1, 16]. <b>Decision</b> <b>trees</b> over infinite check systems have not been intensively investigated, with the exception of linear and algebraic <b>decision</b> <b>trees</b> [2, 5, 6] and some their generalizations [8]. Linear and algebraic <b>decision</b> <b>trees</b> are often used in computational geometry [13]. Furthermore in [...] ...|$|R
40|$|<b>Decision</b> <b>tree</b> {{classification}} {{techniques are}} currently gaining increasing impact {{especially in the}} light of the ongoing growth of data mining services. A central challenge for the <b>decision</b> <b>tree</b> classification is the identification of split rule and correct attributes. In this context, the article aims at presenting the current state of research on different techniques for classification using oblique <b>decision</b> <b>tree.</b> A variation to the traditional approach is the called oblique <b>decision</b> <b>tree</b> or multivariate <b>decision</b> <b>tree,</b> which allows multivariate tests in its non-terminal nodes. Univariate trees can only perform axis-parallel splits, whereas Oblique <b>decision</b> <b>trees</b> can model the decision boundaries that are oblique to attribute axis. The majority of these <b>decision</b> <b>tree</b> induction algorithms performs a top-down growing tree strategy and relay on an impuritybased measure for splitting nodes criteria. In this context, the article aims at presenting the current state of research on different techniques for Oblique <b>Decision</b> <b>Tree</b> classification. For this, the paper analyzes various traditional Multivariate and Oblique <b>Decision</b> <b>Tree</b> algorithms CART, OC 1 as well as standard SVM, GDT implementation...|$|R
40|$|AbstractIt is {{difficult}} for unvaried <b>decision</b> <b>tree</b> to reflect the relationship of attributes, multivariate <b>decision</b> <b>tree</b> can resolve this problem preferably, the former produces big tree, the latter gains simple tree but difficult to explain. Aim to these two features for <b>decision</b> <b>tree,</b> {{in this paper we}} advance a knowledge roughness based approach to hybrid <b>decision</b> <b>tree,</b> select lesser knowledge roughness as tested attribute to construct <b>decision</b> <b>tree.</b> As a result,we find this is a good approach with simple operation and higher efficiency...|$|R
30|$|After scrutinizing each <b>decision</b> <b>tree,</b> {{the most}} {{comprehensive}} one was selected and acknowledged as the base <b>decision</b> <b>tree.</b> Later, the tests {{not included in the}} base tree structure, but found in others were determined and were integrated to the base <b>decision</b> <b>tree.</b> As a result, the base <b>decision</b> <b>tree</b> was updated and modified to a more comprehensive one. In addition, the form of question items was reorganized in the modified and refined base <b>decision</b> <b>tree</b> and all questions were organized as yes–no questions.|$|R
40|$|<b>Decision</b> <b>tree</b> {{classification}} algorithms {{have significant}} potential for land cover mapping problems {{and have not}} been tested in detail by the remote sensing community relative to more conventional pattern recognition techniques such as maximum likelihood classification. In this paper, we present several types of <b>decision</b> <b>tree</b> classification algorithms arid evaluate them on three different remote sensing data sets. The <b>decision</b> <b>tree</b> classification algorithms tested include an univariate <b>decision</b> <b>tree,</b> a multivariate <b>decision</b> <b>tree,</b> and a hybrid <b>decision</b> <b>tree</b> capable of including several different types of classification algorithms within a single <b>decision</b> <b>tree</b> structure. Classification accuracies produced by each of these <b>decision</b> <b>tree</b> algorithms are compared with both maximum likelihood and linear discriminant function classifiers. Results from this analysis show that the <b>decision</b> <b>tree</b> algorithms consistently outperform the maximum likelihood and linear discriminant function classifiers in regard to classf — cation accuracy. In particular, the hybrid tree consistently produced the highest classification accuracies for the data sets tested. More generally, {{the results from this}} work show that <b>decision</b> <b>trees</b> have several advantages for remote sensing applications by virtue of their relatively simple, explicit, and intuitive classification structure. Further, <b>decision</b> <b>tree</b> algorithms are strictly nonparametric and, therefore, make no assumptions regarding the distribution of input data, and are flexible and robust with respect to nonlinear and noisy relations among input features and class labels...|$|R
40|$|<b>Decision</b> <b>trees</b> {{are one of}} {{the most}} {{powerful}} and commonly used supervised learning algorithms in the field of data mining. It is important that a <b>decision</b> <b>tree</b> performs accurately when employed on unseen data; therefore, evaluation methods are used to measure the predictive performance of a <b>decision</b> <b>tree</b> classifier. However, the predictive accuracy of a <b>decision</b> <b>tree</b> is also dependant on the evaluation method chosen since training and testing sets of <b>decision</b> <b>tree</b> models are selected according to the evaluation methods. The aim of this thesis was to study and understand how using different evaluation methods might have an impact on <b>decision</b> <b>tree</b> accuracies when they are applied to different <b>decision</b> <b>tree</b> algorithms. Consequently, comprehensive research was made on <b>decision</b> <b>trees</b> and evaluation methods. Additionally, an experiment was conducted using ten different datasets, five <b>decision</b> <b>tree</b> algorithms and five different evaluation methods in order to study the relationship between evaluation methods and <b>decision</b> <b>tree</b> accuracies. The <b>decision</b> <b>tree</b> inducers were tested with Leave-one-out, 5 -Fold Cross Validation, 10 -Fold Cross Validation, Holdout 50 split and Holdout 66 split evaluation methods. According to the results, cross validation methods were superior to holdout methods in overall. Moreover, Holdout 50 split has performed the poorest in most of the datasets. The possible reasons behind these results have also been discussed in the thesis...|$|R
40|$|Abstract- One {{important}} {{disadvantage of}} <b>decision</b> <b>tree</b> based inductive learning algorithms {{is that they}} use some irrelevant values to establish the <b>decision</b> <b>tree.</b> This causes the final rule set to be less general. To overcome with this problem the tree has to be pruned. In this article using the recently developed RULES inductive learning algorithm, pruning of a <b>decision</b> <b>tree</b> is explained. The <b>decision</b> <b>tree</b> is extracted for an example problem using the ID 3 algorithm and then is pruned using RULES. The results obtained before and after pruning are compared. This shows that the pruned <b>decision</b> <b>tree</b> is more general. Key Words- Pruning, <b>Decision</b> <b>Trees,</b> Inductive Learning. 1...|$|R
40|$|This paper {{discusses}} a {{basic design}} procedure for constructing <b>decision</b> <b>trees</b> from examples. We then discuss about some {{applications of the}} <b>decision</b> <b>trees.</b> <b>Decision</b> <b>trees</b> {{are the most important}} tool which is used in order to reach the final decision regarding any problem. The most important advantage of the <b>decision</b> <b>tree</b> {{can be seen in the}} multilevel-Decision making. <b>Decision</b> <b>Trees</b> have been applied in the field of software project request approval and also in predicting policy risk. In this paper we will see some examples from each of these applications to understand how <b>decision</b> <b>trees</b> are important tool for decision oriented fields. Keywords:- Multilevel decision, Dataset, Policy insurance, Insurance surrenders value...|$|R
40|$|AbstractBy proving {{exponential}} {{lower and}} polynomial upper bounds for parity <b>decision</b> <b>trees</b> and collecting similar bounds for nondeterministic and co-nondeterministic <b>decision</b> <b>trees,</b> the complexity classes related to polynomial-size deterministic, nondeterministic, co-nondeterministic, parity, and alternating <b>decision</b> <b>trees</b> are completely separated. Considering alternating <b>decision</b> <b>trees,</b> it is {{shown that the}} number of alternations between, say, ⋎-nodes and ⋏-nodes strongly influences their computational power...|$|R
5000|$|If {{the output}} of a <b>decision</b> <b>tree</b> is , for all , the <b>decision</b> <b>tree</b> is said to [...] "compute" [...] The depth of a tree is {{the maximum number of}} queries that can happen before a leaf is reached and a result obtained. '''''', the {{deterministic}} <b>decision</b> <b>tree</b> complexity of [...] is the smallest depth among all deterministic <b>decision</b> <b>trees</b> that compute [...]|$|R
3000|$|Compared {{to other}} algorithms, our {{proposed}} <b>decision</b> <b>tree</b> {{as shown in}} Figure 15, which is created from the proposed pixel-based scan mask in Figure 3, is a near-optimal <b>decision</b> <b>tree.</b> It performs approximately {{the same number of}} operations whether the pixel 'x' is a background or foreground. Hence, we can consider fundamental differences between our proposed <b>decision</b> <b>tree</b> (Figure 15) and the <b>decision</b> <b>tree</b> proposed by Wu et al. [6] (Figure 6). If the current pixel is background, Wu et al. [6]'s <b>decision</b> <b>tree</b> is going to check the pixel [...] "x" [...] only one time but our proposed <b>decision</b> <b>tree</b> usually checks it between 2 and 4 times. On the other hand, if the current pixel is foreground, Wu et al. [6]'s <b>decision</b> <b>tree</b> is going to check the pixel [...] "x" [...] between 2 and 5 times but our proposed <b>decision</b> <b>tree</b> will only check it between 2 and 4 times.|$|R
40|$|Abstract—This paper {{presents}} {{a survey of}} evolutionary algorithms designed for <b>decision</b> <b>tree</b> induction. In this context, most of the paper focuses on approaches that evolve <b>decision</b> <b>trees</b> as an alternate heuristics to the traditional top-down divideand-conquer approach. Additionally, we present some alternative methods that make use of evolutionary algorithms to improve particular components of <b>decision</b> <b>tree</b> classifiers. The paper original contributions are the following. First, it provides an upto-date overview that is fully focused on evolutionary algorithms and <b>decision</b> <b>trees</b> and does not concentrate on any specific evolutionary approach. Second, it provides a taxonomy which addresses works that evolve <b>decision</b> <b>trees</b> and works that design <b>decision</b> <b>tree</b> components using evolutionary algorithms. Finally, a number of references is provided that describe applications of evolutionary algorithms for <b>decision</b> <b>tree</b> induction in different domains. The paper ends by addressing some important issues and open questions that can be subject of future research. Index Terms—Evolutionary algorithms, <b>decision</b> <b>tree</b> induction, soft computing classification, regression. I...|$|R
40|$|For the {{well-known}} concept of <b>decision</b> <b>trees</b> {{as it is}} used for inductive inference we study the natural concept of equivalence: two <b>decision</b> <b>trees</b> are equivalent {{if and only if}} they represent the same hypothesis. We present a simple efficient algorithm to establish whether two <b>decision</b> <b>trees</b> are equivalent or not. The complexity of this algorithm is bounded by the product of the sizes of both <b>decision</b> <b>trees.</b> The hypothesis represented by a <b>decision</b> <b>tree</b> is essentially a boolean function, just like a proposition. Although every boolean function can be represented in this way, we show that disjunctions and conjunctions of <b>decision</b> <b>trees</b> can not efficiently be represented as <b>decision</b> <b>trees,</b> and simply shaped propositions may require exponential size for representation as <b>decision</b> <b>trees.</b> 1 Introduction The problem of inductive inference, or shortly induction, in machine learning ([7]) can be described as follows. Roughly speaking, a number of observations each having an outcome, has to [...] ...|$|R
40|$|AbstractThe parity <b>decision</b> <b>tree</b> model {{extends the}} <b>decision</b> <b>tree</b> model by {{allowing}} the computation of a parity function in one step. We prove that the deterministic parity <b>decision</b> <b>tree</b> complexity of any Boolean function is polynomially related to the non-deterministic complexity of the function or its complement. We also {{show that they are}} polynomially related to an analogue of the block sensitivity. We further study parity <b>decision</b> <b>trees</b> in their relations with an intermediate variant of the <b>decision</b> <b>trees,</b> as well as with communication complexity...|$|R
40|$|The {{application}} of boosting procedures to <b>decision</b> <b>tree</b> algorithms {{has been shown}} to produce very accurate classifiers. These classifiers are {{in the form of a}} majority vote over a number of <b>decision</b> <b>trees.</b> Unfortunately, these classifiers are often large, complex and difficult to interpret. This paper describes a new type of classification rule, the alternating <b>decision</b> <b>tree,</b> which is a generalization of <b>decision</b> <b>trees,</b> voted <b>decision</b> <b>trees</b> and voted <b>decision</b> stumps. At the same time classifiers of this type are relatively easy to interpret...|$|R
40|$|<b>Decision</b> <b>tree</b> {{method is}} a {{classification}} method {{that has been}} widely used for the solution of problems of classification. <b>Decision</b> <b>tree</b> classification provides a rapid and effective method. The approach has been proven <b>decision</b> <b>tree</b> method can be applied in various fields of life. Capability classification is indicated by the <b>decision</b> <b>tree</b> method is what encourages authors to use <b>decision</b> <b>tree</b> methods approach to measure the performance of civil servants. To build a <b>decision</b> <b>tree</b> induction algorithms used. In this study, the ID 3 algorithm method is used to construct a <b>decision</b> <b>tree.</b> Starting with the data collecting training samples and then measuring the entropy and information gain. Information Gain value {{will be used as}} the root of a <b>decision</b> <b>tree.</b> And translates it into a <b>decision</b> <b>tree</b> classification rules. The results show that the <b>decision</b> <b>tree</b> method is used to produce classification rules into groups employee performance Good and Bad. The resulting rules are used to measure the performance of employees and classifying employees into two groups above are constructed in an information system. Information system built to assist management in making more objective assessment process.     *) Penulis korespondensi: utje_caem@yahoo. com   Keywords : ID 3 Algorithm; Decision tree; Employee performanc...|$|R
5000|$|Smaller <b>decision</b> <b>trees</b> - C5.0 gets {{similar results}} to C4.5 with {{considerably}} smaller <b>decision</b> <b>trees.</b>|$|R
30|$|Five expert statisticians with {{experience}} in biostatistics from Dokuz Eylül University, Department of Statistics, and Ege University, Department of Biostatistics and Medical Informatics, gave their opinions during {{the development of the}} <b>decision</b> <b>tree</b> devised here. The unified <b>decision</b> <b>tree</b> structure, the questions and order of the questions in the <b>decision</b> <b>tree</b> were evaluated with these five experts. The experts in the panel were requested to examine each branch in the <b>decision</b> <b>tree</b> and to assess whether the tests recommended by the <b>decision</b> <b>tree</b> were compatible with the answers given.|$|R
30|$|<b>Decision</b> <b>tree</b> {{algorithms}} {{have several}} advantages. Since <b>decision</b> <b>trees</b> use an intuitive white box model, it easily predicts the target class of query data using Boolean logic. Especially, <b>decision</b> <b>trees</b> are simple {{to understand and}} interpret. Moreover, <b>decision</b> <b>tree</b> algorithms are robust and scalable for large data and they can train a classification model in reasonable processing time. Hence the <b>decision</b> <b>tree</b> model is effective not only to filter noisy PPI pairs from high-throughput experimental PPI data, but also to gain important insights that describe circumstantial evidences of PPIs.|$|R
