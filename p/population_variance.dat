368|388|Public
25|$|The <b>population</b> <b>{{variance}}</b> {{matches the}} variance of the generating probability distribution. In this sense, the concept of population can be extended to continuous random variables with infinite populations.|$|E
25|$|Often, {{since the}} <b>population</b> <b>variance</b> is an unknown parameter, it is {{estimated}} by the mean sum of squares; when this estimated value is used, {{the distribution of the}} sample mean is no longer a normal distribution but rather a Student's t distribution with n1 degrees of freedom.|$|E
25|$|Firstly, if the omniscient mean {{is unknown}} (and is {{computed}} as the sample mean), then the sample variance is a biased estimator: it underestimates the variance {{by a factor}} of (n−1) / n; correcting by this factor (dividing by n−1 instead of n) is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when n=1 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the <b>population</b> <b>variance.</b> If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.|$|E
50|$|This method {{also does}} not give exactly the nominal rate, but is {{generally}} not too far off. However, if the <b>population</b> <b>variances</b> are equal, or if the samples are rather small and the <b>population</b> <b>variances</b> can be assumed to be approximately equal, it is more accurate to use Student's t-test,.|$|R
5000|$|Under {{the null}} {{hypothesis}} of equal expectations, μ1 [...] μ2, {{the distribution of the}} Behrens-Fisher statistic T, which also depends on the variance ratio σ12/σ22, could now be approximated by Student's t distribution with these ν degrees of freedom. But this ν contains the <b>population</b> <b>variances</b> σi2, and these are unknown. The following estimate only replaces the <b>population</b> <b>variances</b> by the sample variances: ...|$|R
50|$|The {{assumption}} of homoscedasticity, {{also known as}} homogeneity of variance, assumes equality of <b>population</b> <b>variances.</b>|$|R
25|$|Many {{attempts}} have been made to search for one or more genes contributing to thrift. Modern tools of genome wide association studies have revealed many genes with small effects associated with obesity or type 2 diabetes but all of them together explain only between 1.4 and 10% of <b>population</b> <b>variance.</b> This leaves a large gap between the pregenomic and emerging genomic estimates of heritability of obesity and Type 2 diabetes: sometimes called the 'missing heritability'. The reasons for this discrepancy are not completely understood. A likely possibility is that the missing heritability is explained by rare variants of large effect that are found only in limited populations. These would be impossible to detect by standard whole genome sequencing approaches even with hundreds of thousands of participants. The extreme endpoint of this distribution are the so-called 'monogenic' obesities where most of the impact on body weight can be tied to a mutation in a single gene that runs in a single family. The classic example of such a genetic effect is the presence of mutations in the leptin gene.|$|E
500|$|A 2006 review {{identified}} ten {{other possible}} {{contributors to the}} recent increase of obesity: (1) insufficient sleep, (2) endocrine disruptors (environmental pollutants that interfere with lipid metabolism), (3) decreased variability in ambient temperature, (4) decreased rates of smoking, because smoking suppresses appetite, (5) increased use of medications that can cause weight gain (e.g., atypical antipsychotics), (6) proportional increases in ethnic and age groups that tend to be heavier, (7) pregnancy at a later age (which may cause susceptibility to obesity in children), (8) epigenetic risk factors passed on generationally, (9) natural selection for higher BMI, and (10) assortative mating leading to increased concentration of obesity risk factors (this would [...] {{increase the number of}} obese people by increasing <b>population</b> <b>variance</b> in weight). While there is substantial evidence supporting the influence of these mechanisms on the increased prevalence of obesity, the evidence is still inconclusive, and the authors state that these are probably less influential than the ones discussed in the previous paragraph.|$|E
2500|$|In general, the <b>{{population}}</b> <b>variance</b> of {{a finite}} population of size N with values x'i {{is given by}} ...|$|E
5000|$|... where S12 and S22 are {{the usual}} {{unbiased}} (Bessel-corrected) {{estimates of the}} two <b>population</b> <b>variances.</b>|$|R
50|$|Under {{a correct}} {{assumption}} of equal <b>population</b> <b>variances</b> a pooled estimate for σ is more precise.|$|R
5000|$|There is no {{assumption}} that the underlying <b>population</b> <b>variances</b> [...] are equal. This {{is known as the}} Behrens-Fisher problem.|$|R
2500|$|The <b>population</b> <b>variance</b> for a non-negative random {{variable}} {{can be expressed}} in terms of the cumulative distribution function F using ...|$|E
2500|$|If {{the random}} {{variable}} [...] represents samples {{generated by a}} continuous distribution with probability density function [...] then the <b>population</b> <b>variance</b> is given by ...|$|E
2500|$|The {{simplest}} estimators {{for population}} mean and <b>population</b> <b>variance</b> are simply {{the mean and}} variance of the sample, the sample mean and (uncorrected) sample variance – these are consistent estimators (they converge to the correct value {{as the number of}} samples increases), but can be improved. Estimating the <b>population</b> <b>variance</b> by taking the sample's variance is close to optimal in general, but can be improved in two [...] ways. Most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by n. However, using values other than n improves the estimator in various ways. Four common values for the denominator are n, n−1, n+1, and n−1.5: n is the simplest (<b>population</b> <b>variance</b> of the sample), n−1 eliminates bias, n+1 minimizes mean squared error for the normal distribution, and n−1.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.|$|E
40|$|Researchers {{are often}} {{interested}} in testing for the equivalence of <b>population</b> <b>variances.</b> Traditional difference-based procedures are appropriate {{to answer questions}} about differences in some statistic (e. g., variances, etc.). However, if a researcher is interested in evaluating the equivalence of <b>population</b> <b>variances,</b> it is more appropriate to use a procedure specifically designed to determine equivalence. A simulation study was used to compare newly developed equivalence-based tests to difference-based variance homogeneity tests under common data conditions. Results demonstrated that traditional difference-based tests assess equality of variances from the wrong perspective, and that the proposed Levene-Wellek-Welch test for equivalence of group variances using the absolute deviations from the median was the best performing test for detecting equivalence. An R function is provided in order to facilitate use of this test for equivalence of <b>population</b> <b>variances.</b> SSHR...|$|R
5000|$|Suppose it {{were known}} that the two <b>population</b> <b>variances</b> are equal, and samples of sizes n1 and n2 are taken from the two populations: ...|$|R
5000|$|However, in the Behrens-Fisher problem, the two <b>population</b> <b>variances</b> are {{not known}} to be equal, nor is their ratio known. Fisher {{considered}} the pivotal quantity ...|$|R
2500|$|For example, {{dividing}} the IQR by [...] (using the error function) {{makes it an}} unbiased, consistent estimator for the <b>population</b> <b>variance</b> if the data follow a normal distribution.|$|E
2500|$|If the {{biased sample}} {{variance}} (the second central {{moment of the}} sample, which is a downward-biased estimate of the <b>population</b> <b>variance)</b> is used to compute {{an estimate of the}} population's standard deviation, the result is ...|$|E
2500|$|Hence [...] {{gives an}} {{estimate}} of the <b>population</b> <b>variance</b> that is biased by a factor of [...] For this reason, [...] {{is referred to as the}} biased sample variance. Correcting for this bias yields the unbiased sample variance: ...|$|E
25|$|This {{is known}} as the Welch–Satterthwaite equation. The true {{distribution}} of the test statistic actually depends (slightly) on the two unknown <b>population</b> <b>variances</b> (see Behrens–Fisher problem).|$|R
50|$|Bartlett's test {{is used to}} {{test the}} null hypothesis, H0 that all k <b>population</b> <b>variances</b> are equal against the {{alternative}} that at least two are different.|$|R
50|$|This {{is known}} as the Welch-Satterthwaite equation. The true {{distribution}} of the test statistic actually depends (slightly) on the two unknown <b>population</b> <b>variances</b> (see Behrens-Fisher problem).|$|R
2500|$|... where dft is {{the degrees}} of freedom n– 1 of the {{estimate}} of the <b>population</b> <b>variance</b> of the dependent variable, and dfe is the {{degrees of freedom}} n – p – 1 of the estimate of the underlying population error variance.|$|E
2500|$|When {{estimating}} a scale parameter, such as {{when using}} an L-estimator as a robust measures of scale, such as to estimate the <b>population</b> <b>variance</b> or population standard deviation, one generally must multiply by a scale factor {{to make it an}} unbiased consistent estimator; see [...]|$|E
2500|$|... which {{differs from}} Z {{in that the}} exact {{standard}} deviation σ is replaced by the random variable S'n, has a Student's t-distribution as defined above. Notice that the unknown <b>population</b> <b>variance</b> σ2 {{does not appear in}} T, since it was in both the numerator and the denominator, so it canceled. Gosset intuitively obtained the probability density function stated above, with [...] equal to n − 1, and Fisher proved it in 1925.|$|E
50|$|Under the {{assumption}} of equal <b>population</b> <b>variances,</b> the pooled sample variance provides a higher precision estimate of variance than the individual sample variances. This higher precision can lead to increased statistical power when used in statistical tests that compare the populations, such as the t-test.|$|R
40|$|A common {{experimental}} {{design for the}} problem of comparing two means from a normal distribution assumes knowledge of {{the ratio of the}} <b>population</b> <b>variances.</b> The optimal sampling ra-tio is proportional to the square root of this quantity. This article demonstrates that a misspecication of the ratio of the popula-tion variances can cause a substantial loss in power of the cor-responding tests. As a robust alternative, a maximin approach is used to construct designs, which are efcient, whenever the experimenter is able to specify a specic region for the ratio of the <b>population</b> <b>variances.</b> The advantages of the robust designs for inference in the Behrens-Fisher problem are illustrated in a simulation study and an application to the design of experiment for bioassay is presented...|$|R
2500|$|... where [...] and [...] are {{the sample}} variances of the {{estimated}} residuals and the dependent variable respectively, which {{can be seen as}} biased estimates of the <b>population</b> <b>variances</b> of the errors and of the dependent variable. These estimates are replaced by statistically unbiased versions: [...] and [...]|$|R
2500|$|As {{explained}} above, while s2 is an {{unbiased estimator}} for the <b>population</b> <b>variance,</b> s {{is still a}} biased estimator for the population standard deviation, though markedly less biased than the uncorrected sample standard deviation. This estimator is commonly used and generally known simply as the [...] "sample standard deviation". The bias may still be large for small samples (N less than 10). As sample size increases, the amount of bias decreases. We obtain more information {{and the difference between}} [...] and [...] becomes smaller.|$|E
2500|$|Secondly, {{the sample}} {{variance}} does not generally minimize {{mean squared error}} between sample variance and <b>population</b> <b>variance.</b> Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see [...] ), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than n−1), and is a simple example of a shrinkage estimator: one [...] "shrinks" [...] the unbiased estimator towards zero. For the normal distribution, dividing by n+1 (instead of n−1 or n) minimizes mean squared error. The resulting estimator is biased, however, and {{is known as the}} biased sample variation.|$|E
5000|$|Suppose X1, ..., Xn are {{independent}} and identically distributed (i.i.d.) random variables with expectation μ and <b>population</b> <b>variance</b> σ2. If the sample mean, uncorrected sample variance, and <b>population</b> <b>variance</b> {{are defined as}} ...|$|E
2500|$|This test, {{also known}} as Welch's t-test, is used only when the two <b>population</b> <b>variances</b> are not assumed to be equal (the two sample sizes {{may or may not}} be equal) and hence must be {{estimated}} separately. The t statistic to test whether the population means are different is calculated as: ...|$|R
50|$|The sample {{standard}} deviations for the two samples are approximately 0.05 and 0.11, respectively. For such small samples, a test of equality between the two <b>population</b> <b>variances</b> would not be very powerful. Since the sample sizes are equal, the two forms of the two-sample t-test will perform similarly in this example.|$|R
5000|$|This test, {{also known}} as Welch's t-test, is used only when the two <b>population</b> <b>variances</b> are not assumed to be equal (the two sample sizes {{may or may not}} be equal) and hence must be {{estimated}} separately. The t statistic to test whether the population means are different is calculated as: ...|$|R
