197|757|Public
5000|$|A {{similar study}} {{was done in}} [...] where theauthors carry out {{experimental}} evaluation of a few uncalibrated visual servosystems that were popular in the 90’s. The major outcome was the experimental evidence {{of the effectiveness of}} visual servo control over conventionalcontrol methods.Kyrki et al. analyze servoing errors for position based and 2-1/2-Dvisual servoing. The technique involves determining the error in extractingimage position and propagating it to pose estimation and servoing control.Points from the image are mapped to points in the world a priori to obtain a mapping (which is basically the homography, although not explicitly statedin the paper). This mapping is broken down to pure rotations and translations. Pose estimation is performed using standard technique from ComputerVision. Pixel errors are transformed to the pose. These are propagating tothe controller. An observation from the analysis shows that errors in theimage plane are proportional to the depth and error in the depth-axis isproportional to square of depth.Measurement errors in visual servoing have been looked into extensively.Most error functions relate to two aspects of visual servoing. One beingsteady state error (once servoed) and two on the stability of the controlloop. Other servoing errors that have been of interest are those that arisefrom pose estimation and camera calibration. In, the authors extend thework done in [...] by considering global stability in the presence of intrinsicand extrinsic calibration errors. provides an approach to bound the taskfunction tracking error. In, the authors use teaching-by-showing visualservoing technique. Where the desired pose is known a priori and the robotis moved from a given pose. The main aim of the paper is to determine theupper bound on the positioning error due to image noise using a convex-optimization technique. provides a discussion on stability analysis with respect the uncertaintyin depth estimates. The authors conclude the paper with the observation thatfor unknown target geometry a more accurate depth estimate is required inorder to limit the error.Many of the visual servoing techniques [...] implicitly assume thatonly one object is present in the image and the relevant feature for trackingalong with the area of the object are available. Most techniques require eithera partial <b>pose</b> <b>estimate</b> or a precise depth estimate of the current and desiredpose.|$|E
40|$|This paper {{presents}} a head pose and facial feature estimation technique that works {{over a wide}} range of pose variations without a priori knowledge of the appearance of the face. Using simple LK trackers, head pose is estimated by Levenberg-Marquardt (LM) pose estimation using the feature tracking as constraints. Factored sampling and RANSAC are employed to both provide a robust <b>pose</b> <b>estimate</b> and identify tracker drift by constraining outliers in the estimation process. The system provides both a head <b>pose</b> <b>estimate</b> and the position of facial features and is capable of tracking {{over a wide range}} of head poses. 1...|$|E
40|$|Achieving a robust, {{accurately}} scaled <b>pose</b> <b>estimate</b> in long-range stereo presents significant challenges. For large scene depths, triangulation from {{a single}} stereo pair is inadequate and noisy. Additionally, vibration and flexible rigs in airborne applications mean accurate calibrations are often compromised. This paper presents a technique for accurately initializing a long-range stereo VO algorithm at large scene depth, with accurate scale, without explicitly computing structure from rigidly fixed camera pairs. By performing a monocular <b>pose</b> <b>estimate</b> over a window of frames {{from a single}} camera, followed by adding the secondary camera frames in a modified bundle adjustment, an accurate, metrically scaled <b>pose</b> <b>estimate</b> can be found. To achieve this {{the scale of the}} stereo pair is included in the optimization as an additional parameter. Results are presented both on simulated and field gathered data from a fixed-wing UAV flying at significant altitude, where the epipolar geometry is inaccurate due to structural deformation and triangulation from a single pair is insufficient. Comparisons are made with more conventional VO techniques where the scale is not explicitly optimized, and demonstrated over repeated trials to indicate robustness...|$|E
40|$|We {{address the}} task of {{estimating}} 3 D human poses from monocular camera sequences. Many works make use of multiple consecutive frames for the estimation of a 3 D pose in a frame. Although such an approach should ease the pose estimation task substantially since multiple consecutive frames allow to solve for 2 D projection ambiguities in principle, {{it has not yet}} been investigated systematically how much we can improve the 3 D <b>pose</b> <b>estimates</b> when using multiple consecutive frames opposed to single frame information. In this paper we analyze the difference in quality of 3 D <b>pose</b> <b>estimates</b> based on different numbers of consecutive frames from which 2 D <b>pose</b> <b>estimates</b> are available. We validate the use of temporal information on two major different approaches for human pose estimation - modeling and learning approaches. The results of our experiments show that both learning and modeling approaches benefit from using multiple frames opposed to single frame input but that the benefit is small when the 2 D <b>pose</b> <b>estimates</b> show a high quality in terms of precision...|$|R
40|$|Abstract—Human pose {{estimation}} using uncalibrated monocular visual inputs {{alone is}} a challenging problem {{for both the}} computer vision and robotics communities. From the robotics perspective, the challenge {{here is one of}} pose estimation of a multiply-articulated system of bodies using a single nonspecialized environmental sensor (the camera) and thereby, creating low-order surrogate computational models for analysis and control. In this work, we propose a technique for estimating the lowerlimb dynamics of a human solely based on captured behavior using an uncalibrated monocular video camera. We leverage our previously developed framework for human pose estimation to (i) deduce the correct sequence of temporally coherent gap-filled <b>pose</b> <b>estimates,</b> (ii) estimate physical parameters, employing a dynamics model incorporating the anthropometric constraints, and (iii) filter out the optimized gap-filled <b>pose</b> <b>estimates,</b> using an Unscented Kalman Filter (UKF) with the estimated dynamicallyequivalent human dynamics model. We test the framework on videos from the publicly available DARPA Mind’s Eye Year 1 corpus [8]. The combined estimation and filtering framework not only results in more accurate physically plausible <b>pose</b> <b>estimates,</b> but also provides <b>pose</b> <b>estimates</b> for frames, where the original human pose estimation framework failed to provide one. I...|$|R
40|$|A {{system for}} 3 -D {{reconstruction}} of a rigid object from monocular video sequences is introduced. Initially an object <b>pose</b> is <b>estimated</b> in each image by locating similar (unknown) texture assuming flat depth map for all images. Shape-from-silhouette [1] is then applied {{to construct a}} 3 -D model {{which is used to}} obtain better <b>pose</b> <b>estimates</b> using a model-based method. Before repeating the process by building a new 3 -D model, <b>pose</b> <b>estimates</b> are adjusted to reduce error by maximizing a quality measure for shape-fromsilhouette volume reconstruction. Translation of the object in the input sequence is compensated in two stages. The volume feedback is terminated when the updates in <b>pose</b> <b>estimates</b> become small. The final output is a pose index (the last set of <b>pose</b> <b>estimates)</b> and a 3 -D model of the object. Good performance of the system is shown by experiments on a real video sequence of a human head. Our method has the following advantages: 1. No model is asssumed for the objest. 2. Feature points are neither detected nor tracked, thus no problematic feature matching or lengthy point tracking are required. 3. The method generates a high level pose index for the input images, these can be used for content-based retrieval. Our method can also be applied to 3 -D object tracking in video. 1...|$|R
40|$|Head pose is an {{important}} indicator of a person’s focus of attention. Also, head pose estimation {{can be used as}} the front-end analysis for multi-view face analysis. For example, face recognition and identification algorithms are usually view dependent. Pose classification can help such face recognition systems to select the best view model. Subspace analysis has been widely used for head pose estimation. However, such techniques are usually sensitive to data alignment and background noise. In this paper a two-stage approach is proposed to address this issue by combining the subspace analysis together with the topography method. The first stage is based on the subspace analysis of Gabor wavelets responses. Different subspace techniques were compared for better exploring the underlying data structure. Nearest prototype matching with Euclidean distance was used to get the <b>pose</b> <b>estimate.</b> The single <b>pose</b> <b>estimate</b> was relaxed to a subset of poses around it to incorporate certain tolerance to data alignment and background noise. In the second stage, the <b>pose</b> <b>estimate</b> is refined by analyzing finer geometrical structure details captured by bunch graphs. This coarse-to-fine framework was evaluated with a large data set. We examined 86 poses, with the pan angle spanning from − 90 ◦ to 90 ◦ and the tilt angle spanning from − 60 ◦ to 45 ◦. The experimental results indicate that the integrated approach has a remarkably better performance than using subspace analysis alone...|$|E
40|$|This paper {{presents}} a novel {{application of the}} Visual Servoing Platform’s (ViSP) for pose estimation in indoor and GPS-denied outdoor environments. Our proposed solution integrates the trajectory solution from RGBD-SLAM into ViSP’s pose estimation process. Li-Chee-Ming and Armenakis (2015) explored the application of ViSP in mapping large outdoor environments, and tracking larger objects (i. e., building models). Their experiments revealed that tracking was often lost {{due to a lack}} of model features in the camera’s field of view, and also because of rapid camera motion. Further, the <b>pose</b> <b>estimate</b> was often biased due to incorrect feature matches. This work proposes a solution to improve ViSP’s pose estimation performance, aiming specifically to reduce the frequency of tracking losses and reduce the biases present in the <b>pose</b> <b>estimate.</b> This paper explores the integration of ViSP with RGB-D SLAM. We discuss the performance of the combined tracker in mapping indoor environments and tracking 3 D wireframe indoor building models, and present preliminary results from our experiments...|$|E
40|$|This paper {{discusses}} {{the development and}} {{construction of a new}} system for robust, obscured object recognition by means of Partial Evidence Reconstruction From Object Restricted Measures (PERFORM). This new approach employs a partial evidence accrual approach to form both an object identity metric and an object <b>pose</b> <b>estimate.</b> The partial evidence information is obtained by applying several instances of the authors' Linear Signal Decomposition/Direction of Arrival (LSD/DOA) pose estimation technique. LSD/DOA is a means for estimating object pose for possibly articulated objects with multiple degrees of pose freedom that avoids the use of search mechanisms and template matching. Each instance of application of the LSD/DOA system results in a <b>pose</b> <b>estimate</b> and match metric aimed at recognition of a portion of a desired target. Each such partial object recognizer is formed {{in such a way as}} to be exposed to no clutter input when positioned over the target component of interest when no obscur [...] ...|$|E
40|$|We {{explore the}} {{potential}} of variance matrices to represent not just statistical error on object <b>pose</b> <b>estimates</b> but also partially constrained degrees of freedom. Using an iterated extended Kalman filter as an estimation tool, we generate, combine and predict partially constrained <b>pose</b> <b>estimates</b> from 3 D range data. We find that partial constraints on the translation component of pose which occur frequently in practice are handled well by the method. However, coupled partial constraints between rotation and translation are, in general, non-linear and cannot be represented by this method. ...|$|R
40|$|We {{explore the}} {{potential}} of variance matrices to represent not just statistical error on object <b>pose</b> <b>estimates</b> but also partially constrained degrees of freedom. Using an iterated extended Kalman filter as an estimation tool, we generate, combine and predict partially constrained <b>pose</b> <b>estimates</b> from 3 D range data. We find that partial constraints on the translation component of pose which occur frequently in practice are handled well by the method. One key advantage of the method {{is that it allows}} simultaneous representation of both lack of knowledge, weak constraints, a priori position constraints and statistical error in a framework that allows incremental reasoning...|$|R
40|$|We {{propose a}} {{generative}} statistical approach to human motion modeling and tracking that utilizes probabilistic latent semantic (PLSA) models {{to describe the}} mapping of image features to 3 D human <b>pose</b> <b>estimates.</b> PLSA has been successfully used to model the co-occurrence of dyadic data on problems such as image annotation where image features are mapped to word categories via latent variable semantics. We apply the PLSA approach to motion tracking by extending it to a sequential setting where the latent variables describe intrinsic motion semantics linking human figure appearance to 3 D <b>pose</b> <b>estimates.</b> This approach {{is in contrast to}} many current methods that directly learn the often high-dimensional image-to-pose mappings and utilize subspace projections as a constraint on the pose space alone. As a consequence, such mappings may often exhibit increased computational complexity and insufficient generalization performance. We demonstrate the utility of the proposed model on the synthetic dataset and the task of 3 D human motion tracking in monocular image sequences with arbitrary camera views. Our experiments show that the DPLSA approach can produce accurate <b>pose</b> <b>estimates</b> {{at a fraction of the}} computational cost of alternative subspace tracking methods. 1...|$|R
40|$|Abstract — Our work {{presents}} {{solutions to}} two related vexing problems in feature-based localization of ground targets in Unmanned Aerial Vehicle (UAV) images: (i) A good initial {{guess at the}} <b>pose</b> <b>estimate</b> that would speed up the convergence to the final <b>pose</b> <b>estimate</b> for each image frame in a video sequence; and (ii) Time-bounded estimation {{of the position of}} the ground target. We address both these problems {{within the framework of the}} Iterative Closest Point (ICP) algorithm that now has a rich tradition of usage in computer vision and robotics applications. We solve the first of the two problems by frame-to-frame propagation of the computed pose estimates for the purpose of the initializations needed by ICP. The second problem is solved by terminating the iterative estimation process at the expiration of the available time for each image frame. We show that when frame-to-frame homography is factored into the iterative calculations, the accuracy of the position calculated at the time of bailing out of the iterations is nearly always sufficient for the goals of UAV vision. I...|$|E
40|$|Abstract- We {{show that}} {{landmark}} based localisation (LBL) and Lowe's model-based localisation (MBL) are complementary in that LBL provides a pose initialisation to MBL, {{which is a}} necessary input to the algorithm, and MBL can then refine that pose to a give more accurate <b>pose</b> <b>estimate</b> than LBL alone can provide. For LBL, we extend Betke and Gurvit's method, such {{that it can be}} used with standard perspective cameras (their original proposal was for omnidirectional cameras) {{in order to get a}} useful initial value as an input to Lowe's method. Intensive experiments have been carried out to analyse how camera parameters (intrinsic and extrinsic) affect the LBL position and orientation errors in the initial <b>pose</b> <b>estimate.</b> In error propagation experiments, we show that the position and orientation of a robot are sensitive to focal length and errors in imaged feature positions respectively. In the MBL pose refinement phase, we find that MBL is able to refine the position estimate, but the error in orientation estimate remains the same. Index Terms- Mobile robots, localization, visual landmarks, navigation...|$|E
40|$|This paper {{proposes a}} {{two-level}} 3 D human pose tracking method {{for a specific}} action captured by several cameras. The generation of pose estimates relies on fitting a 3 D articulated model on a Visual Hull generated from the input images. First, an initial <b>pose</b> <b>estimate</b> is constrained by a low dimensional manifold learnt by Temporal Laplacian Eigenmaps. Then, an improved global pose is calculated by refining individual limb poses. The validation of our method uses a public standard dataset and demonstrates its accurate and computational efficiency...|$|E
40|$|In {{this paper}} {{we present a}} {{real-time}} 3 D object tracking algorithm based on edges and using a single pre-calibrated camera. During the tracking process, the algorithm is continuously projecting the 3 D model to the current frame by using the <b>pose</b> <b>estimated</b> in the previous frame. Once projected, some control points are generated along the visible edges of the object. The next <b>pose</b> is <b>estimated</b> by minimizing the distances between the control points and the edges detected in the image. 1...|$|R
30|$|In the {{recognition}} stage, images {{are created by}} photographing users at certain intervals. A user's <b>poses</b> are <b>estimated</b> by comparing the photographed images with the pose decision tree. The <b>estimated</b> <b>poses</b> are then transmitted to other users through the network. In the reconstruction stage, a user's presence is expressed in a virtual human by considering the <b>estimated</b> <b>pose.</b>|$|R
30|$|An {{important}} trend can {{be observed}} in the results of both methods. Rotational error decreases with increasing segment length. We may conclude from this that there is some noise present on the immediate <b>poses</b> <b>estimated</b> by both methods, which averages to zero over many estimations.|$|R
40|$|This work {{presents}} {{some different}} approaches {{to the design of}} navigation systems for off-road terrain. The standard navigation methods are presented and their limitations in unstructured environments discussed. The concept of Simultaneous Localization and Map building is presented with outdoor navigation results in a variety of off-road unstructured environments. A batch data association method is introduced to determine data associations in a manner decoupled from the vehicle <b>pose</b> <b>estimate.</b> Finally, an observation-based dead-reckoning procedure is demonstrated that produces vehicle motion estimates of significantly better quality than odometry. ...|$|E
40|$|The ICP (Iterative Closest Point) {{algorithm}} is {{the de facto}} standard for geometric alignment of threedimensional models when an initial relative <b>pose</b> <b>estimate</b> is available. The basis of ICP is the search for closest points. Since the development of ICP, k-d trees {{have been used to}} accelerate the search. This paper presents a novel search procedure, namely cached k-d trees, exploiting iterative behavior of the ICP algorithm. It results in a significant speedup of about 50 % as we show in an evaluation using different data sets. ...|$|E
40|$|Monocular {{information}} from a gripper-mounted camera is used to servo the robot gripper to grasp a cylinder. The fundamental concept for rapid pose estimation {{is to reduce the}} amount of information that needs to be processed during each vision update interval. The grasping procedure is divided into four phases: learn, recognition, alignment, and approach. In the learn phase, a cylinder is placed in the gripper and the <b>pose</b> <b>estimate</b> is stored and later used as the servo target. This is performed once as a calibration step. The recognition phase verifies the presence of a cylinder in the camera field of view. An initial <b>pose</b> <b>estimate</b> is computed and uncluttered scan regions are selected. The radius of the cylinder is estimated by moving the robot a fixed distance toward the cylinder and observing the change in the image. The alignment phase processes only the scan regions obtained previously. Rapid pose estimates are used to align the robot with the cylinder at a fixed distance from it. The relative motion of the cylinder is used to generate an extrapolated pose-based trajectory for the robot controller. The approach phase guides the robot gripper to a grasping position. The cylinder can be grasped with a minimal reaction force and torque when only rough global pose information is initially available...|$|E
40|$|We {{describe}} an approach for automatically registering color images with 3 D laser scanned models. We use the chisquare statistic to compare color images to polygonal models texture mapped with acquired laser reflectance values. In complicated scenes {{we find that}} the chi-square test is not robust enough to permit an automatic global registration approach. Therefore, we introduce two techniques for obtaining initial <b>pose</b> <b>estimates</b> that correspond to a coarse alignment of the data. The first method is based on rigidly attaching a camera to a laser scanner and the second utilizes object tracking to decouple these imaging devices. The <b>pose</b> <b>estimates</b> serve as an initial guess for our optimization method, which maximizes the chi-square statistic over a local space of transformations in order to automatically determine the proper alignment. 1...|$|R
40|$|We {{address the}} problem of {{articulated}} 2 D human pose estimation in natural images. In previous work the Pictorial Structure Model [1] approach has proven particularly successful, however the accuracy of resulting <b>pose</b> <b>estimates</b> has been limited by the use of simple representations of limb appearance. We propose strong discriminatively trained limb detector...|$|R
40|$|Machine {{learning}} techniques, namely convolutional {{neural networks}} (CNN) and regression forests, have recently shown great promise in performing 6 -DoF localization of monocular images. However, {{in most cases}} image-sequences, rather only single images, are readily available. To this extent, none of the proposed learning-based approaches exploit the valuable constraint of temporal smoothness, often leading to situations where the per-frame error {{is larger than the}} camera motion. In this paper we propose a recurrent model for performing 6 -DoF localization of video-clips. We find that, even by considering only short sequences (20 frames), the <b>pose</b> <b>estimates</b> are smoothed and the localization error can be drastically reduced. Finally, we consider means of obtaining probabilistic <b>pose</b> <b>estimates</b> from our model. We evaluate our method on openly-available real-world autonomous driving and indoor localization datasets. Comment: To appear at CVPR 201...|$|R
40|$|Abstract—An Extended Kalman Filter-based {{algorithm}} for the localization {{of a team}} of robots {{is described}} in this paper. The distributed EKF localization scheme is straightforward in that the individual robots maintain a <b>pose</b> <b>estimate</b> using EKFs that are local to every robot. We then show how these results can be extended to perform heterogeneous cooperative localization in the absence or degradation of absolute sensors aboard the team members. The proposed algorithms are implemented using field data obtained from a team of ATRV-Mini robots traversing on uneven outdoor terrain...|$|E
40|$|The ICP {{algorithm}} and its derivatives is the {{de facto}} standard for registration of 3 D range-finder scans today. This paper presents a quantitative comparison between ICP and 3 D NDT, a novel approach based on the normal distributions transform. The new method addresses two of the main problems of ICP: {{the fact that it}} does not make use of the local surface shape and the computationally demanding nearest-neighbour search. The results show that 3 D NDT produces accurate results much faster, though it is more sensitive to error in the initial <b>pose</b> <b>estimate...</b>|$|E
40|$|Most current {{navigation}} algorithms {{in mobile}} robotics produce 2 D maps from {{data provided by}} 2 D sensors. In large part {{this is due to}} the availability of suitable 3 D sensors and difficulties of managing the large amount of data supplied by 3 D sensors. This paper presents a novel, multi-resolution algorithm that aligns 3 D range data stored in occupied voxel lists so as to facilitate the construction of 3 D maps. Multi-resolution occupied voxel lists (MROL) are voxel based data structures that efficiently represent 3 D scan and map information. The process described in this research can align a sequence of scans to produce maps and localise a range sensor within a prior global map. An office environment (200 square metres) is mapped in 3 D at 0. 02 m resolution, resulting in a 200, 000 voxel occupied voxel list. Global localisation within this map, with no prior <b>pose</b> <b>estimate,</b> is completed in 5 seconds on a 2 GHz processor. The MROL based sequential scan matching is compared to a standard iterative closest point (ICP) implementation with an error in the initial <b>pose</b> <b>estimate</b> of plus or minus 1 m and 90 degrees. MROL correctly scan matches 94 % of scans to within 0. 1 m as opposed to ICP with 30 % within 0. 1 m. © 2009 Springer Science+Business Media, LLC...|$|E
40|$|Registration of 3 D knee implant {{components}} to singleplane X-ray image sequences provides insight into implanted knee kinematics. In this paper a Maximum Likelihood approach is proposed to align the pose-related occluding contour {{of an object}} with edge segments extracted from a single-plane X-ray image. This leads to an Expectation Maximization algorithm which simultaneously determines the object’s <b>pose,</b> <b>estimates</b> point correspondences and rejects outlier points from the registration process. Considering (nearly) planar-symmetrical objects, the method is extended in order to simultaneously estimate two symmetrical object poses which both align the corresponding occluding contours with 2 D edge information. The algorithm’s capacity to generate accurate <b>pose</b> <b>estimates</b> {{and the necessity of}} determining both symmetrical poses when aligning (nearly) planar-symmetrical objects will be demonstrated in the context of automated registration of knee implant {{components to}} simulated and real single-plane X-ray images. 1...|$|R
30|$|The <b>pose</b> estimator, which <b>estimates</b> <b>poses</b> {{with the}} {{extracted}} silhouette in the recognition stage, must recognize multiple poses {{in real time}} in a mobile environment. However, the time to <b>estimate</b> <b>poses</b> increases {{with the number of}} poses because the number of comparisons also increases. To solve this problem, we propose a pose decision tree.|$|R
40|$|This paper {{reports the}} {{application}} of a novel shape-from-shading technique to <b>estimating</b> facial <b>pose.</b> The shape-from-shading algorithm uses a new ge-ometric technique for solving the image irradiance equation together with curvature consistency constraints. Orientation histograms extracted from the the needle maps delivered by the new shape-from-shading algorithm are used to <b>estimate</b> facial <b>pose.</b> We present a simple model of how the histogram bin-contents transform under rotation of the head. The parameters of this model are the head <b>pose</b> angles. We <b>estimate</b> <b>pose</b> by searching for the rotation angles which maximise the correlation between transformed histograms. A sensitivity analysis reveals that the methods can deliver <b>pose</b> <b>estimates</b> that are accurate to within a few degrees. ...|$|R
40|$|Abstract Most current {{navigation}} algorithms {{in mobile}} ro-botics produce 2 D maps from {{data provided by}} 2 D sensors. In large part {{this is due to}} the availability of suitable 3 D sensors and difficulties of managing the large amount of data supplied by 3 D sensors. This paper presents a novel, multi-resolution algorithm that aligns 3 D range data stored in occupied voxel lists so as to facilitate the construction of 3 D maps. Multi-resolution occupied voxel lists (MROL) are voxel based data structures that efficiently represent 3 D scan and map information. The process described in this research can align a sequence of scans to produce maps and localise a range sensor within a prior global map. An office environ-ment (200 square metres) is mapped in 3 D at 0. 02 m resolu-tion, resulting in a 200, 000 voxel occupied voxel list. Global localisation within this map, with no prior <b>pose</b> <b>estimate,</b> is completed in 5 seconds on a 2 GHz processor. The MROL based sequential scan matching is compared to a standard iterative closest point (ICP) implementation with an error in the initial <b>pose</b> <b>estimate</b> of plus or minus 1 m and 90 degrees. MROL correctly scan matches 94 % of scans to within 0. 1 m as opposed to ICP with 30 % within 0. 1 m...|$|E
40|$|Abstract. A {{system is}} {{introduced}} which can perform automatic, real-time {{estimation of the}} pose of a mobile robot moving in a known environment – {{in this case a}} ship section. In the system a <b>pose</b> <b>estimate</b> is generated continuously while the robot is moving based on the combination of visual sensing information and CAD-information. To reach an acceptable performance of the overall system a special emphasis is placed on robust feature detection and real-time capabilities. The update cycle time is 120 ms but this could be enhanced. Experiments show that the approach is feasible and reaches the demanded positioning accuracy’s...|$|E
40|$|Abstract—A novel {{method is}} {{developed}} {{to obtain a}} refined estimate of relative position and orientation (pose) from two views captured by a calibrated monocular camera. Due to the typically large number of matched pairs of feature points available, many estimates of the pose are possible by taking minimal sets of feature points. Among such estimates, the proposed method selects a subset that has low estimation error, and averages them in an appropriate manner to provide a refined estimate. Preliminary experiments on the image data show that the proposed method provides a more accurate <b>pose</b> <b>estimate</b> than refinement using least squares method. I...|$|E
40|$|A {{method for}} finding correspondences between widely {{separated}} views is presented. The proposed solution consists in estimating the local perspective distortion between the neighborhoods of junctions. To this end, a formulation is proposed, {{based on a}} constrained minimization involving an estimated fundamental matrix. An application is also proposed to fundamental matrix recovery using crude camera <b>pose</b> <b>estimates.</b> ...|$|R
40|$|We {{describe}} a linear-time algorithm that recovers absolute camera orientations and positions, along with uncertainty estimates, for networks of terrestrial image nodes spanning hundreds of meters in outdoor urban scenes. The algorithm produces <b>pose</b> <b>estimates</b> globally consistent to roughly 0. 1 # (2 milliradians) and 5 centimeters on average, or about four pixels of epipolar alignment...|$|R
40|$|This paper compares {{alternative}} approaches to pose estimation using visual cues from the environment. We examine approaches that derive <b>pose</b> <b>estimates</b> from global image properties, such as {{principal components analysis}} (PCA) versus from local image properties, {{commonly referred to as}} landmarks. We also consider the failure-modes of the different methods. Our work is validated with experimental results. ...|$|R
