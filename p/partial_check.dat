19|105|Public
6000|$|... "We risk it," [...] {{answered}} Barker, with {{a perfect}} placidity. [...] "Suppose he is a tyrant--he is still a check on a hundred tyrants. Suppose he is a cynic, it is to his interest to govern well. Suppose he is a criminal--by removing poverty and substituting power, we put a check on his criminality. In short, by substituting despotism we have put a total check on one criminal and a <b>partial</b> <b>check</b> on all the rest." ...|$|E
5000|$|Each {{financial}} transaction is recorded {{in at least}} two different nominal ledger accounts within the financial accounting system, so that the total debits equals the total credits in the General Ledger, i.e. the accounts balance. This is a <b>partial</b> <b>check</b> that each and every transaction has been correctly recorded. The transaction is recorded as a [...] "debit entry" [...] (Dr) in one account, and a [...] "credit entry" [...] (Cr) in a second account. The debit entry will be recorded on the debit side (left-hand side) of a General ledger account, and the credit entry will be recorded on the credit side (right-hand side) of a General ledger account. If the total of the entries on the debit side of one account is greater than the total on the credit side of the same nominal account, that account is said to have a debit balance.|$|E
50|$|As a <b>partial</b> <b>check</b> {{that the}} posting process was done correctly, a working {{document}} called an unadjusted trial balance is created. In its simplest form, {{this is a}} three-column list. Column One contains {{the names of those}} accounts in the ledger which have a non-zero balance. If an account has a debit balance, the balance amount is copied into Column Two (the debit column); if an account has a credit balance, the amount is copied into Column Three (the credit column). The debit column is then totalled, and then the credit column is totalled. The two totals must agree—which is not by chance—because under the double-entry rules, whenever there is a posting, the debits of the posting equal the credits of the posting. If the two totals do not agree, an error has been made, either in the journals or during the posting process. The error must be located and rectified, and the totals of the debit column and the credit column recalculated to check for agreement before any further processing can take place.|$|E
5000|$|In 1948 Norway's cabinet {{decided that}} the brigade's {{soldiers}} were to be issued condoms. 400,000 signatures of protest were collected, and Hauge asked advice from Trygve Bratteli {{about the possibility of}} having Labour Party employees do <b>partial</b> <b>checks</b> of the lists, and [...] "Should it be done?". (Municipal elections were scheduled for later in 1948, and Hauge thought that the signatures might harm the election campaign of the Labour Party.) The resulting advice was that the <b>partial</b> <b>checks</b> could be done, but that it was not advisable. (Even with condoms being issued, the military later found that [...] "five to six percent" [...] of the brigade's soldiers contracted STDs.) ...|$|R
5000|$|On 14 May 1946 {{parliament}} {{decided to}} establish a brigade of 4000 soldiers to be stationed in West-Germany—in line with the ministry's proposal—Norwegian Brigade Group in Germany. (In 1948 the cabinet dedided that the brigade's soldiers were to be issued condoms. 400 000 signatures of protest were collected, and Hauge asked advice from Trygve Bratteli {{about the possibility of}} having Labour Party employees do <b>partial</b> <b>checks</b> of the lists, and [...] "Should it be done?". (Municipal elections were scheduled for later in 1948, and Hauge thought that the signatures might harm the election campaign of the Labour Party.) The resulting advice was that the <b>partial</b> <b>checks</b> could be done, but that it was not advisable.|$|R
40|$|Abstract. Recently, David Chaum {{proposed}} an electronic voting scheme that combines visual cryptography and digital processing. It {{was designed to}} meet not only mathematical security standards, but also {{to be accepted by}} voters that do not trust electronic devices. In this scheme mix-servers are used to guarantee anonymity of the votes in the counting process. The mix-servers are operated by different parties, so an evidence of their correct operation is necessary. For this purpose the protocol uses randomized <b>partial</b> <b>checking</b> of Jakobsson et al., where some randomly selected connections between the (encoded) inputs and outputs of a mix-server are revealed. This leaks some information about the ballots, even if intuitively this information cannot be used for any efficient attack. We provide a rigorous stochastic analysis of how much information is revealed by randomized <b>partial</b> <b>checking</b> in the Chaum’s protocol. We estimate how many mix-servers are necessary for a fair security level. Namely, we consider probability distribution of the permutations linking the encoded votes with the decoded votes given the information revealed by randomized <b>partial</b> <b>checking.</b> We show that the variation distance between this distribution and the uniform distribution is O � � 1 already for a constant number of mix-servers (n is the number of vot...|$|R
40|$|Unprecedented street protests {{brought down}} Mubarak’s {{government}} and ushered {{in an era}} of competition between three rival political groups in Egypt. Using daily variation in the number of protesters, we document that more intense protests are associated with lower stock market valuations for firms connected to the group currently in power relative to non-connected firms, but have no impact on the relative valuations of firms connected to rival groups. These results suggest that street protests serve as a <b>partial</b> <b>check</b> on political rent-seeking. General discontent expressed on Twitter predicts protests but has no direct effect on valuations...|$|E
40|$|INTRODUCTION The {{purpose of}} this note is to review the report of Accent/HCG (1994), {{referred}} to here as AHCG, and other sources, and make recommendations regarding future official Values of Time for road commercial vehicles. This note starts by discussing current DTLR practice, {{as set out in}} its Transport Economics Note (TEN). Section 3 presents a digest of the AHCG findings. Section 4 looks at the findings of other studies. Although these are very mixed, carried out for a variety of purposes and presented in a variety of forms, they can serve as a <b>partial</b> <b>check</b> on the AHCG work. Section 5 presents interim conclusions...|$|E
40|$|Abstract In the {{framework}} of the standardization and laboratory activities concerning the calibration of hardness testers, in particular for indirect verification there are some questions to be answered to decide an efficient metrological confirmation strategy: a) how many points on each scale shall be checked? b) How many levels of the indentation scale shall be checked? c) How many levels of forces shall be checked? d) Is the correlation between indirect results and direct results strong enough to allow a <b>partial</b> <b>check</b> of the hardness scales of the tester? A number of tests have been made and the results have been statistically elaborated to give an answer to these questions...|$|E
40|$|We {{propose a}} new {{technique}} for making mix nets robust, called randomized <b>partial</b> <b>checking</b> (RPC). The basic idea is that rather than providing a proof of completely correct operation, each server provides strong evidence of its correct operation by revealing a pseudo-randomly selected subset of its input/output relations...|$|R
40|$|We {{propose a}} new {{technique}} for making mix nets robust, called randomized <b>partial</b> <b>checking</b> (RPC). The basic idea is that rather than providing a proof of completely correct operation, each server provides strong evidence of its correct operation by revealing a pseudorandomly selected subset of its input/output relations. Randomized <b>partial</b> <b>checking</b> is exceptionally e#cient compared to previous proposals for providing robustness; the evidence provided at each layer is shorter than the output of that layer, and producing the evidence is easier than doing the mixing. It works with mix nets based on any encryption scheme (i. e., on public-key alone, and on hybrid schemes using public-key/symmetric-key combinations). It also works both with Chaumian mix nets where the messages are successively encrypted with each servers' key, and with mix nets based on a single public key with randomized re-encryption at each layer...|$|R
30|$|Furthermore, when medical {{reports are}} lost or damaged, {{it is almost}} {{impossible}} to retrieve the lost information. Unlike a computerized system which can perform routine <b>partial</b> <b>checks,</b> human errors incurred in filling the medical forms and reports are more difficult to be detected by the system, since the report must be seen by another person before the abnormally can be discovered.|$|R
40|$|Four astronomical {{measures}} {{of changes in}} the length of day obtained in 1979 have been shown to exhibit the same, approximately 50 -day fluctuation. To find whether this fluctuation was persistent, and of meteorological origin, lunar laser ranging observations and wind data deduced from sources distributed over the globe were analyzed. A high degree of correlation was found between the two sets of data. It is implied that the 50 -day period fluctuations in length of day are real and related to meteorological effects. Observed changes in length of day can provide a constraint for models for atmospheric flow, and a <b>partial</b> <b>check</b> for global analyses of such motions...|$|E
40|$|In the {{framework}} of the standardization and laboratory activities concerning the calibration of hardness testers, in particular for indirect verification there are some questions to be answered to decide an efficient metrological confirmation strategy: a) how many points on each scale shall be checked? b) How many levels of the indentation scale shall be checked? c) How many levels of forces shall be checked? d) Is the correlation between indirect results and direct results strong enough to allow a <b>partial</b> <b>check</b> of the hardness scales of the tester? A number of tests have been made and the results have been statistically elaborated to give an answer to these questions...|$|E
40|$|The resolvable 2 -(14, 7, 12) {{designs are}} {{classified}} {{in a computer}} search: there are 1, 363, 486 such designs, 1, 360, 800 of which have trivial full automorphism group. Since every resolvable 2 -(14, 7, 12) design is also a resolvable 3 -(14, 7, 5) design and vice versa, the latter designs are simultaneously classified. The computer search utilizes {{the fact that these}} designs are equivalent to certain binary equidistant codes, and the classification is carried out with an orderly algorithm that constructs the designs point by point. As a <b>partial</b> <b>check,</b> a subset of these designs is constructed with an alternative approach by forming the designs one parallel class at a time...|$|E
40|$|The VALG {{project is}} {{introducing}} e-voting to municipal and county elections to Norway. Part of the e-voting {{system is a}} mix-net {{along the lines of}} Puiggalí et al. - a mix-net which can be efficiently verified by combining the benefits of optimistic mixing and randomized <b>partial</b> <b>checking.</b> This paper investigates their mix-net and proposes a verification method which improves both efficiency and privacy compared to Puiggalí et al...|$|R
40|$|Abstract. The VALG {{project is}} {{introducing}} evoting for municipal and county elections to Norway. Part of the evoting {{system is a}} mix net {{along the lines of}} Puiggaĺı et al. – a mix net which can be efficiently verified by combining the benefits of optimistic mixing and randomized <b>partial</b> <b>checking.</b> This paper investigates their mix net and proposes a verification method which improves both efficiency and privacy compared to Puiggaĺı et al [...] ...|$|R
40|$|The work {{describes}} the procedures used by auditing companies {{when they have}} to give their opinion, based only in <b>partial</b> <b>checking</b> of the accounting books. A comparative analysis of the procedures used by companies operating in Santiago (Chile) and Rio de Janeiro (Brasil) is deve loped. The study is structured in a manner that permit to address the following questions : Which are the sampling techniques used by auditing firms in Chile and Brasil? - Which are the problems faced by using these techniques? 11 there is any; What are the main differences between the techniques used by both type 01 firms? lhe bibliographic review summarizes the sources that 6 upport the use 01 <b>partial</b> <b>checking</b> as an auditing procedure. The methodology described after in to be used and the reason for its use are this parto The results of the survey applied to companies are also shown. A 1 ter, the data analysis is presented, making possible a comparison of the sampling techniques used by the auditing 1 irms from both countries. Finally, based on the results, the conclusions and comments are presented. Some curricula or professional career considerations for auditor s are a lso presented in this part...|$|R
40|$|Starting from a {{microscopic}} tight-binding model and using second order perturbation theory, we derive explicit expressions for the intrinsic and Rashba spin-orbit interaction induced {{gaps in the}} Dirac-like low-energy band structure of an isolated graphene sheet. The Rashba interaction parameter is first order in the atomic carbon spin-orbit coupling strength ξ and first order in the external electric field E perpendicular to the graphene plane, whereas the intrinsic spin-orbit interaction which survives at E= 0 is second order in ξ. The spin-orbit terms in the low-energy effective Hamiltonian have the form proposed recently by Kane and Mele. Ab initio electronic structure calculations were performed as a <b>partial</b> <b>check</b> on {{the validity of the}} tight-binding model. Comment: 5 pages, 2 figures; typos corrected, references update...|$|E
40|$|In {{a series}} of two papers we will generalize the {{extended}} Sugawara construction to the superalgebra SU(M+ 1 ‖N+ 1). This first part contains ground work which we need to construct examples of closed extended algebras. We develop the tensor analysis for contractions of super d and f symbols, and we present a free-field representation of the super Kac-Moody currents at level 1 in terms of spin- 0 fields. This is obtained from the standard spin- 1 / 2 conjugate free-field representation by bosonization. Because of the mixed statistics, some of these conjugate pairs are of the βγ type-similar to the superghosts appearing in Neveu-Schwarz-Ramond strings. The <b>partial</b> <b>check</b> on bosonization we carry out, in which we compare the torus partition functions before and after bosonization, is also of interest in that context...|$|E
40|$|Abstract: We {{discuss the}} {{spectrum}} of a string propagating on η-deformed AdS 5 × S 5 by treating its world-sheet theory as an integrable quantum field theory. The exact S-matrix of this field theory is given by a q-deformation of the AdS 5 × S 5 world-sheet S-matrix with real deformation parameter. By considering mirror (double Wick-rotated) versions of these world-sheet theories we give the Thermodynamic Bethe Ansatz description of their exact finite size spectra. Interestingly, this class of models maps onto itself under the mirror transformation. At {{the level of the}} string this appears to say that the light-cone world-sheet theories of strings on particular pairs of backgrounds are related by a double Wick rotation, a feature we call ‘mirror duality’. We provide a <b>partial</b> <b>check</b> of these statements {{at the level of the}} sigma model by considering reduced actions and their corresponding (mirror) giant magnon solutions. 1 Correspondent fellow at Steklov Mathematical Institute, Moscow. ar X i...|$|E
30|$|The paleointensity {{method is}} either the Thellier method (Thellier and Thellier, 1959; Coe, 1967) with <b>partial</b> TRM <b>checks</b> (accounting for 60 records), or the Shaw method {{with or without}} {{correction}} (19 records).|$|R
40|$|We study mix-nets with {{randomized}} <b>partial</b> <b>checking</b> (RPC) {{as proposed}} by Jakobsson, Juels, and Rivest (2002). RPC is a technique to verify the correctness of an execution both for Chaumian and homomorphic mix-nets. The idea is to relax the correctness and privacy requirements to achieve a more efficient mix-net. We identify serious issues in the original description of mix-nets with RPC and show how to exploit these to break both correctness and privacy, both for Chaumian and homomorphic mix-nets. Our attacks are practical and applicable to real world mix-net implementations, e. g., the Civitas and the Scantegrity voting systems. ...|$|R
50|$|In July 2016, Herbalife {{agreed to}} change its {{business}} model and pay $200 million in a settlement with the FTC. <b>Partial</b> refund <b>checks</b> were mailed to roughly 350,000 Herbalife distributors in January 2017.|$|R
40|$|Two {{experiments}} {{concerned with}} the communication of feelings in deceptive and nondeceptive messages are reported. The studies examined how access to different channels (face, body, speech) influenced the accuracy of receivers in detecting (1) senders' attempts to deceive and (2) senders' underlying affect. Also of interest were the interrelations among different receiving abilities. In Experiment 1, receivers viewed a videotape in which each of two senders described acquaintances in deceptive and nondeceptive ways. The vision (face vs. body) and sound (on vs. off) channels were manipulated orthogonally. In Experiment 2, as a <b>partial</b> <b>check</b> on {{the generalizability of the}} results of Experiment 1, two additional senders were used, and the sound manipulation was dropped. In both studies, deception was detected better from body than from face cues, but only when the deception involved making a positive description of a disliked person. In Experiment 1 the presence of speech cues resulted in superior detection of deception and also enhanced the recognition of genuine affect in nondeceptive descriptions, {{while at the same time}} impairing the recognition of such affect in deceptive communications. In both studies there was an inverse relationship between accuracy in detecting underlying affect and accuracy in recognizing genuine affect. The results are discussed in relation to other findings in this area...|$|E
40|$|The quantum {{corrections}} {{to black}} hole entropy, variously defined, suffer quadratic divergences {{reminiscent of the}} ones found in the renormalization of the gravitational coupling constant (Newton constant). We consider the suggestion, due to Susskind and Uglum, that these divergences are proportional, and attempt to clarify its precise meaning. Using a Euclidean formulation the proportionality is a fairly immediate consequence of basic principles [...] a low-energy theorem. Thus in this framework renormalizing the Newton constant renders the entropy finite, and equal to its semiclassical value. As a <b>partial</b> <b>check</b> on our formal arguments we compare the one loop determinants, calculated using heat kernel regularization. An alternative definition of black hole entropy relates it to behavior at conical singularities in two dimensions, and thus to a suitable definition of geometric entropy. Geometric entropy permits the same renormalization, {{but it does not}} yield an intrinsically positive quantity. For scalar fields geometric entropy is subtly sensitive to curved space couplings, even in the limit of flat space. Fermions and gauge fields are considered as well. Their functional determinants are closely related to the determinants for non-minimally coupled scalar fields with specific values for the curvature coupling, and pose no further difficulties. Comment: 23 pages, uses phyzz...|$|E
40|$|A {{fundamental}} problem in estuarine microbiology studies is the accurate {{determination of the}} density {{in the water column}} of both free-living bacteria and those attached to suspended particulate matter. When a water sample is filtered and the filter is viewed by epifluorescence microscopy, counts can be made of the numbers of bacteria which are seen on the filter background (free-living) and those which appear to lie on sediment particles (both free-living and attached). With only the additional knowledge of the proportion of the filter area covered by particles (a quantity that is straightforwardly determined by stereological point counting), results from geometric probability were used to determine the expected number of bacteria which are hidden by particles and hence to provide an estimation scheme for the true densities of free-living and attached bacteria. Variance equations based on a Taylor series are given, and a <b>partial</b> <b>check</b> of the method is attempted with controlled mixtures of bacteria and sediment. An alternative procedure is also proposed, in which the natural attached/free-living ratio is altered by an intervention experiment, allowing an estimation which is less model dependent but more labor intensive. Both methods are applied to a series of samples from the Tamar estuary, United Kingdom, taken in April 1985. A notable conclusion is that there are always more free-living than attached bacteria in the water column throughout the estuary...|$|E
50|$|In {{symbolic}} model <b>checking,</b> <b>partial</b> order reduction can {{be achieved}} by adding more constraints (guard strengthening).Further applications of partial order reduction involve automated planning.|$|R
40|$|We {{propose a}} formal {{framework}} to model an automated adaptation protocol based on Quantitative <b>Partial</b> Model <b>Checking</b> (QPMC). An agent seeks the collaboration {{of a second}} agent to satisfy some (fixed) condition on the actions to be executed. The provided protocol allows the two agents to automatically agree by iteratively applying QPMC...|$|R
40|$|In this thesis, {{we present}} two paper-based {{electronic}} voting systems Prêt-à-Voter and Demos. We describe {{these in the}} same systematic way with new examples. Furthermore, we implement RSA cryptosystem in Prêt-à-Voter. Then, we propose an informal analysis of what is required both in practice and in the technical part of Prêt-à-Voter. We present randomized <b>partial</b> <b>checking</b> during mix-net and emphasize issues surrounding this component based on Pfitzmann attack and duplicate a vote attack. We discuss {{the size of a}} ciphertext in Prêt-à-Voter and explain the difficulty of proving permutation and randomness in the system. Finally, we discuss the concept of privacy based on the privacy game and illustrate with attacks how the privacy can be broken in both Prêt-à-Voter and Demos. For a general analysis of these two voting systems this thesis should be read together with the thesis of Anna Vederhus...|$|R
40|$|Analysts {{who work}} {{frequently}} with linear models using the SAS System ® often use the LSMEANS statement in PROC GLM or PROC MIXED to obtain “adjusted means. ” The {{absence of a}} clear analogue to these least-squares means for nonlinear or generalized linear models can be an obstacle for both the analyst and the eventual audience of the analysis. This paper shows how to use PROC IML to estimate standard errors of CLASS variable effects from a logit model and from a linear model on log-transformed data. The macro used is designed for logit or log-transformed linear models used {{in the analysis of}} health care claims data but it is easily adapted to other contexts. The log-transformation example involves a linear model on the logarithm of a continuous variable (health care costs) for which the effect of interest was the difference in cost, not the difference in the logarithm of cost. The retransformation of log(cost) back to the original units incorporates Duan’s (JASA 1983) “smearing estimate ” to compensate for the bias that usually arises in the retransformation. A <b>partial</b> <b>check</b> on the correctness of the PROC IML code can be obtained by comparing results from PROC IML with results from PROC GLM or PROC MIXED in the linear case. Once the principles are understood and the basic PROC IML manipulations are coded, it is easy to adapt the method to other studies and to other models...|$|E
40|$|Cohort reconstructions (CR) {{currently}} {{applied in}} Pacific salmon management estimate temporally variant exploitation, maturation, and juvenile natural mortality rates but require an assumed (typically invariant) adult natural mortality rate (dA), resulting in unknown biases {{in the remaining}} vital rates. We explored the sensitivity of CR results to misspecification of the mean and/or variability of dA, {{as well as the}} potential to estimate dA directly using models that assumed separable year and age/cohort effects on vital rates (separable cohort reconstruction, SCR). For CR, given the commonly assumed dA = 0. 2, the error (RMSE) in estimated vital rates is generally small (≤ 0. 05) when annual values of dA are low to moderate (≤ 0. 4). The greatest absolute errors are in maturation rates, with large relative error in the juvenile survival rate. The ability of CR estimates to track temporal trends in the juvenile natural mortality rate is adequate (Pearson's correlation coefficient > 0. 75) except for high dA (≥ 0. 6) and high variability (CV > 0. 35). The alternative SCR models allowing estimation of time-varying dA by assuming additive effects in natural mortality, fishing mortality, and/or maturation rates did not outperform CR across all simulated scenarios, and are less accurate when additivity assumptions are violated. Nevertheless an SCR model assuming additive effects on fishing and natural (juvenile and adult) mortality rates led to nearly unbiased estimates of all quantities estimated using CR, along with borderline acceptable estimates of the mean dA under multiple sets of conditions conducive to CR. Adding an assumption of additive effects on the maturation rates allowed nearly unbiased estimates of the mean dA as well. The SCR models performed slightly better than CR when the vital rates covaried as assumed. These separable models could serve as a <b>partial</b> <b>check</b> on the validity of CR assumptions about the adult natural mortality rate, or even a preferred alternative if there is strong reason to believe the vital rates, including juvenile and adult natural mortality rates, covary strongly across years or age classes as assumed...|$|E
40|$|The study commences with {{a review}} of the many {{isolated}} suggestions advanced in support of a Marcan source under lying part of Acts. Whilst these are seen to have little coherence, the opposite theory recently propounded by Parker that Acts is ignorant of the Marcan Gospel is also found to he wanting; in probability. Following a brief general survey of prevalent attitudes to source criticism of Acts today, it is demonstrated how the present study has a certain advantage in this field, by being able to provide some objective control on the evidences for in knowing something of Mark's own language and method, and Luke’s treatment of it, we have some guidance as to the nature of one source in Acts, had Mark ever been used in the formation of Acts by Luke. As a <b>partial</b> <b>check</b> against a 'freak’ result, vocabulary of Matthew and John is also tested: this in effect heightening the connections between Mark and part of Acts. Armed with a knowledge of Mark's distinctive vocabulary, the thesis, develops the two major issues involved: firstly, does Mark's Gospel bear any evidence that its author intended to continue with an 'Acts' of any description? – after examination of key passages this possibility is left open. Secondly, assuming the hypothesis, the text of those passages in Acts which appear from statistical evidence to most possibly have Marcan affinities are analysed in detail, using the material gained from the examination of Marcan language as the basis for all discussion. At the same time the author's own attitudes to his material has constantly to be evaluated, and although the final conclusion remains necessarily speculative, the probability of a Marcan source underlying at least Ac. 3 : 1 - 11, 10 : 9 - 16 and 12 : 5 - 10 seems unavoidable. The work concludes with three Appendices, a Bibliography and an Index...|$|E
40|$|In the TACAS paper, it is {{proved that}} natural {{projection}} reduces to <b>partial</b> model <b>checking</b> and, when {{cast in a}} common setting, the two are equivalent. In addition, there it was presented a quotienting algorithm and introduced a tool for the <b>partial</b> model <b>checking</b> of finite-state systems {{that can be used}} as an alternative to natural projection. In connection with the TACAS paper, we present here the tool suite for the partial evaluation, called PESTS (Partial Evaluator of Simple Transition Systems), used there. We applied the prototype to some case studies. In particular, PESTS can be used to address the following different problems: 1. reducing the verification of a parallel composition to that of a single component; 2. synthesizing a submodule that respects a global specification: Submodule Construction Problem (SCP); 3. synthesizing a controller for a given component: Controller Synthesis Problem (CSP) ...|$|R
40|$|AbstractWe analyze a <b>partial</b> type <b>checking</b> {{algorithm}} for the inconsistent domain-free pure type system Type:Type (λ⁎). We {{show that}} the algorithm is sound and partially complete using a coinductive specification of algorithmic equality. This entails that the algorithm will only diverge due {{to the presence of}} diverging computations, in particular it will terminate for all typeable terms...|$|R
40|$|Abstract—In {{previous}} work, we {{have studied}} some noninterference properties for information flow analysis in computer systems on classic (possibilistic) labeled transition systems. In this paper, {{some of these}} properties, notably bisimulation-based nondeducibility on compositions (BNDC), are reformulated in a real-time setting. This is done by first enhancing the Security Process Algebra proposed {{by two of the}} authors with some extra constructs to model real-time systems (in a discrete time setting), and then by studying the natural extension of these properties in this enriched setting. We prove essentially the same results known for the untimed case: ordering relation among properties, compositionality aspects, <b>partial</b> model <b>checking</b> techniques. Finally, we illustrate the approach through two case studies, where in both cases the untimed specification is secure, while the timed specification may show up interesting timing covert channels. Index Terms—Bisimulation, information flow security, <b>partial</b> mode <b>checking,</b> process algebra, real-time systems, timing covert channels. I...|$|R
