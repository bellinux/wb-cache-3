6890|1723|Public
5|$|In April 1958, S. Gill (Ferranti) {{discussed}} {{parallel programming}} {{and the need}} for branching and waiting. Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time. Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch. In 1967, Amdahl and Slotnick published a debate about the feasibility of <b>parallel</b> <b>processing</b> at American Federation of Information Processing Societies Conference. It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.|$|E
5|$|In November 2006, first-generation {{symmetric}} multiprocessing (SMP) clients were publicly released for open beta testing, {{referred to as}} SMP1. These clients used Message Passing Interface (MPI) communication protocols for <b>parallel</b> <b>processing,</b> as {{at that time the}} GROMACS cores were not designed to be used with multiple threads. This was the first time a distributed computing project had used MPI. Although the clients performed well in Unix-based operating systems such as Linux and macOS, they were troublesome under Windows. On January24, 2010, SMP2, the second generation of the SMP clients and the successor to SMP1, was released as an open beta and replaced the complex MPI with a more reliable thread-based implementation.|$|E
5|$|Initially, Titan used Jaguar's 10 PB of Lustre storage with a {{transfer}} speed of 240 GB/s, but in April 2013, the storage was upgraded to 40 PB with {{a transfer}} rate of 1.4 TB/s. GPUs {{were selected for}} their vastly higher <b>parallel</b> <b>processing</b> efficiency over CPUs. Although the GPUs have a slower clock speed than the CPUs, each GPU contains 2,688 CUDA cores at 732 MHz, resulting in a faster overall system. Consequently, the CPUs' cores are used to allocate tasks to the GPUs rather than directly processing the data as in conventional supercomputers.|$|E
40|$|At present, <b>parallel</b> signal <b>processing</b> tasks {{is applied}} {{more and more}} widely. Traditional way of {{development}} of <b>parallel</b> signal <b>processing</b> tasks is custom development. This development way needs manual programming, the development cycle is long, and the platform portability is poor. The <b>parallel</b> signal <b>processing</b> software discussed in this paper will make revolutionary {{change in the way}} of development of <b>parallel</b> signal <b>processing</b> tasks. The goal of <b>parallel</b> signal <b>processing</b> tasks is to achieve fully graphical and modularized development under an integrated development software environment. The software can automatically generate codes. Besides, the software provides the function of predicting and evaluating the performance. Furthermore, the software has generalization of hardware platform. © 2015 Taylor & Francis Group, London...|$|R
40|$|Abstract—Cloud Computing has {{became an}} {{emerging}} {{area in the}} recent areas. It extends services like SaaS, PaaS and IaaS. One of the complicated applications in Iaas is <b>parallel</b> data <b>processing.</b> <b>Parallel</b> data <b>processing</b> is needed to integrate and deploy their programs in customer premises. However, the processing frameworks which are currently used have been designed for static, homogeneous cluster setups and disregard the particular nature of a cloud. Results into the allocated resources may not be sufficient for the large scale jobs and unnecessarily increase processing time and cost. In this paper we discuss the opportunities and challenges for efficient <b>parallel</b> data <b>processing</b> in clouds and a novel framework is designed for the <b>parallel</b> data <b>processing...</b>|$|R
40|$|This thesis {{contains}} basic theoretical {{information about}} image <b>processing,</b> <b>parallel</b> data <b>processing</b> {{and information about}} CUDA standard in detail, also desribes aplicability of CUDA in <b>parallel</b> data <b>processing.</b> Testing application compares speed of the image processing in serial CPU application and GPU parallel application, and describes basic methods of parallel programming in the CUDA platform...|$|R
5|$|Uncharted {{uses the}} Cell {{microprocessor}} to generate dozens of layered character animations to portray realistic expressions and fluid movements, which allow for responsive player control. The PlayStation 3's graphics processing unit, the RSX Reality Synthesizer, employed several functions to provide graphical details that helped immerse the player {{into the game}} world: lighting models, pixel shaders, dynamic real-time shadowing, and advanced water simulation. The new hardware allowed for processes which the team had never used in PlayStation 2 game development and required them to quickly familiarize with the new techniques; for example, <b>parallel</b> <b>processing</b> and pixel shaders. While Blu-ray afforded greater storage space, the team became concerned with running out of room several times—Uncharted used more and bigger textures than previous games, and included several languages on the disc. Gameplay elements requiring motion sensing, such as throwing grenades and walking across beams, were implemented {{to take advantage of}} the Sixaxis controller. A new PlayStation 3 controller, the DualShock 3, was unveiled at the 2007 Tokyo Game Show, and featured force feedback vibration. Uncharted was also on display at the show with demonstrations that implemented limited support for vibration.|$|E
25|$|ALGOL 68 {{supports}} programming of <b>parallel</b> <b>processing.</b> Using the keyword par, a collateral clause {{is converted}} to a parallel clause, where the synchronisation of actions is controlled using semaphores. In A68G the parallel actions are mapped to threads when available on the hosting operating system. In A68S a different paradigm of <b>parallel</b> <b>processing</b> was implemented (see below).|$|E
25|$|Magnussen, S., Greenlee, M.W., & Thomas, J.P. (1996). <b>Parallel</b> <b>processing</b> {{in visual}} {{short-term}} memory. Journal of Experimental Psychology: Human Perception and Performance, 22(1), 202–212.|$|E
40|$|We {{propose the}} Fuce (FUsion of Comunication and Execution) architecture. The Fuce {{processor}} {{of the architecture}} executes all processing composed of fine-grain 2 ̆ 2 uninterruptible 2 ̆ 2 threads. The multiple thread execution model of the architecture takes the continuation-based multithreading model. The execution model takes dependency among threads as the continuation instruction. The <b>parallel</b> I/O <b>processing</b> with the execution model divides I/O processing into fine-grain threads, and associates in Chip Multiprocessor as processor cores and fine-grain threads. In this paper, the <b>parallel</b> I/O <b>processing</b> in the continuation-based multithreading compares the <b>parallel</b> I/O <b>processing</b> with interrupt. This paper shows the <b>parallel</b> I/O <b>processing</b> performance is almost equal, and the response time in the continuation-based multithreading can be shortened...|$|R
5000|$|... "IEEE An Effective Scheduling Algorithm for <b>Parallel.</b> Transaction <b>Processing</b> Systems", ...|$|R
5000|$|... {{multi-core}} <b>parallel</b> image <b>processing</b> (when {{not using}} GPU and frame-dropping is disabled) ...|$|R
25|$|Analytics Platform System (APS): Formerly Parallel Data Warehouse (PDW) A massively <b>parallel</b> <b>processing</b> (MPP) SQL Server {{appliance}} {{optimized for}} large-scale data warehousing such {{as hundreds of}} terabytes.|$|E
25|$|These early {{architectures}} introduced <b>parallel</b> <b>processing</b> at {{the processor}} level, with innovations such as vector processing, {{in which the}} processor can perform several operations during one clock cycle, {{rather than having to}} wait for successive cycles.|$|E
25|$|Japan's {{entry into}} {{supercomputing}} {{began in the}} early 1980s. In 1982, Osaka University's LINKS-1 Computer Graphics System used a massively <b>parallel</b> <b>processing</b> architecture, with 514 microprocessors, including 257 Zilog Z8001 control processors and 257 iAPX 86/20 floating-point processors. It was mainly used for rendering realistic 3D computer graphics. It was the world's most powerful computer, as of 1984.|$|E
50|$|Tibero RDBMS {{provides}} distributed database links, data replication, database clustering(Tibero Active Cluster or TAC) {{which is}} similar to Oracle RAC., <b>parallel</b> query <b>processing,</b> and query optimizer. It conforms with SQL standard specifications and development interfaces and guarantees high compatibility with other types of databases.Other features include; Row-level locking, multi-version concurrency control, <b>Parallel</b> query <b>processing,</b> and partition table support.|$|R
40|$|A <b>parallel</b> block <b>processing</b> for remote sensed {{images for}} {{classification}} problem {{is presented in}} this paper. Due to increase in computational time for processing the remote sensing images for pixel dimension more than 1000 × 1000. Block processing approach is applied for an image in parallel by distributing the task among the cores. K -means {{is one of the}} widely used clustering method for analyzing features in images. Hence it is considered for the <b>parallel</b> block <b>processing</b> approach. The <b>parallel</b> Block <b>Processing</b> approach was implemented using Matlab 2014 a programming environment. The experiment is carried out on data sets comprising of 200 samples of high resolution orthoimagery satellite images. The result obtained from <b>parallel</b> block <b>processing</b> approach lead to efficient usage of hardware resources, depletion in time compared to sequential K -means algorithm. Results are acceptable and this approach can be applied for image processing operations...|$|R
40|$|Many image {{processing}} tasks exhibit {{a high degree}} of data locality and parallelism and map quite readily to specialized massively parallel computing hardware. However, as distributed memory machines are becoming a viable and economical parallel computing resource, {{it is important to understand}} how to use these environments for <b>parallel</b> image <b>processing</b> as well. In this paper we discuss our implementation of a <b>parallel</b> image <b>processing</b> software library (the <b>Parallel</b> Image <b>Processing</b> Toolkit). The library is easily extensible and hides the parallelism from the user. Inside the Toolkit, a message-passing model of parallelism is designed around the Message Passing Interface (MPI) standard. Experimental results are presented to demonstrate the parallel speedup obtained with the <b>Parallel</b> Image <b>Processing</b> Toolkit in a typical workstation cluster over a wide variety of {{image processing}} tasks. We also discuss load balancing and the potential for parallelizing portions of image processing tasks [...] ...|$|R
25|$|ALGOL 68R(R) from RRE was {{the first}} ALGOL 68 subset implementation, running on the ICL 1900. Based on the {{original}} language, the main subset restrictions were definition before use and no <b>parallel</b> <b>processing.</b> This compiler was popular in UK universities in the 1970s, where many computer science students learnt ALGOL 68 as their first programming language; the compiler was renowned for good error messages.|$|E
25|$|From 1998 to 2005, {{he was a}} {{professor}} and Regents' Lecturer at The University of New Mexico. In 2005, Bader moved to Georgia Tech, {{where he is now}} a Full Professor. He has served on numerous conference program committees related to <b>parallel</b> <b>processing,</b> edited numerous journals, published numerous articles, and is a Fellow of the IEEE, Fellow of the AAAS, and Member of the ACM.|$|E
25|$|In ALGOL 68S(S) from Carnegie Mellon University {{the power}} of <b>parallel</b> <b>processing</b> was {{improved}} by adding an orthogonal extension, eventing. Any variable declaration containing keyword event made assignments to this variable eligible for parallel evaluation, i.e. the right hand side {{was made into a}} procedure which was moved to one of the processors of the C.mmp multiprocessor system. Accesses to such variables were delayed after termination of the assignment.|$|E
40|$|AbstractA <b>parallel</b> block <b>processing</b> for remote sensed {{images for}} {{classification}} problem {{is presented in}} this paper. Due to increase in computational time for processing the remote sensing images for pixel dimension more than 1000 × 1000. Block processing approach is applied for an image in parallel by distributing the task among the cores. K -means {{is one of the}} widely used clustering method for analyzing features in images. Hence it is considered for the <b>parallel</b> block <b>processing</b> approach. The <b>parallel</b> Block <b>Processing</b> approach was implemented using Matlab 2014 a programming environment. The experiment is carried out on data sets comprising of 200 samples of high resolution orthoimagery satellite images. The result obtained from <b>parallel</b> block <b>processing</b> approach lead to efficient usage of hardware resources, depletion in time compared to sequential K -means algorithm. Results are acceptable and this approach can be applied for image processing operations...|$|R
40|$|AbstractIn {{recent years}} large-set <b>parallel</b> data <b>processing</b> has gained quantum {{as one of}} the {{predominant}} applications of Infrastructure-as-a-Service (IaaS) clouds. Data processing frameworks like Google's MapReduce and its open source implementation Hadoop, Microsoft's Dryad and so on are currently in use for <b>parallel</b> data <b>processing</b> in cloud-based companies. But the problem with them is that they are designed for homogeneous environments like clusters and disregard the dynamic and heterogeneous nature of a cloud. As a result, allocation and de-allocation of compute nodes at runtime is ineffective thereby increasing processing time and cost. In this paper we present our approach towards <b>parallel</b> data <b>processing</b> exploiting dynamic resource allocation in IaaS clouds. Our architecture ensures <b>parallel</b> data <b>processing</b> using Directed Acyclic task graph. To reduce the latency and to improve throughput, load balancing is introduced in the architecture. Incoming jobs are divided into tasks that are assigned to different types of virtual machines that are dynamically instantiated and terminated during job execution...|$|R
5000|$|Sequential {{function}} chart (SFC), has elements to organize programs for sequential and <b>parallel</b> control <b>processing.</b>|$|R
25|$|Additional {{subcortical}} projections {{are shared}} between the striatum, particularly ventral reward-related areas,. Connectivity with thalamic and periaqueductal grey areas further suggests {{a role for}} the orbitofrontal cortex in both inhibitory and excitatory regulation of autonomic function. <b>Parallel</b> <b>processing</b> loops in connectivity between cortico-striatal networks seem {{to be involved in}} the processing of goal-directed and habitual action, whereas cortico-limbic connectivity seems to be of prime importance for action selection, implicating the basolateral amygdala, and the integration of information into behavioral output.|$|E
25|$|Typical {{algorithms}} {{that are}} exact and yet run in sub-linear time use <b>parallel</b> <b>processing</b> (as the NC1 matrix determinant calculation does), non-classical processing (as Grover's search does), or alternatively have guaranteed assumptions on the input structure (as the logarithmic time binary search and many tree maintenance algorithms do). However, formal languages {{such as the}} set of all strings that have a 1-bit in the position indicated by the first log(n) bits of the string may depend on every bit of the input and yet be computable in sub-linear time.|$|E
25|$|During August 2008, Hyneman and Savage {{appeared}} {{on the stage of}} NVISION 08, an event sponsored by Nvidia, having been asked by Nvidia's Creative Director, David Wright, to provide a visual demonstration {{of the power of the}} GPU vs a CPU. They did this by creating an image of the Mona Lisa with a giant <b>parallel</b> <b>processing</b> paintball gun, setting a world record for largest paintball gun in the process. An encore of the demonstration was given at YouTube Live featuring Hyneman standing in the path of the paintballs wearing a suit of armor.|$|E
40|$|Abstract: System {{developments}} {{and research on}} <b>parallel</b> query <b>processing</b> have concentrated either on "Shared Everything" or "Shared Nothing " architectures so far. While there are several commercial DBMS based on the "Shared Disk " alternative, this architecture has received very little attention with respect to <b>parallel</b> query <b>processing.</b> A comparison between Shared Disk and Shared Nothing reveals many potential benefits for Shared Disk with respect to <b>parallel</b> query <b>processing.</b> In particular, Shared Disk supports more flexible control over the communication overhead for intratransaction parallelism, and a higher potential for dynamic load balancing and efficient processing of mixed OLTP/ query workloads. We also sketch necessary extensions for transaction management (concurrency/coherency control, logging/recovery) to support intra-transaction parallelism in the Shared Disk environment. ...|$|R
50|$|Apache Flink, an {{open-source}} <b>parallel</b> data <b>processing</b> platform has implemented PACTs. Flink {{allows users}} to specify user functions with annotations.|$|R
50|$|Pulveriser: A {{multi-purpose}} {{piece of}} virtual hardware that combines compression, distortion, filters, tremolo, <b>parallel</b> signal <b>processing</b> and an envelope follower.|$|R
25|$|In 1980 Sassenrath {{graduated}} from the University of California, Davis with a B.S. in EECS (electrical engineering and computer science). During his studies he became interested in operating systems, <b>parallel</b> <b>processing,</b> programming languages, and neurophysiology. He was a teaching assistant for graduate computer language courses and a research assistant in neuroscience and behavioral biology. His uncle, Dr. Julius Sassenrath, headed the educational psychology department at UC Davis, and his aunt, Dr. Ethel Sassenrath, {{was one of the}} original researchers of THC at the California National Primate Research Center.|$|E
25|$|Xorro {{is a new}} slot {{using an}} {{industry}} standard PCIe ×8 form factor to give access to the 'Xena' I/O. This will be the route to Xena's 64 I/O lines, which are dynamically configurable as input, output, or bidirectional. 'Xorro' will allow bridging Xena to external hardware for control purposes, to internal systems, or to other XCore processors. This last point is worth more exploration; XCore is a <b>parallel</b> <b>processing</b> architecture; more XCores can be chained together if more computing power is required.. Reference boards have been made with up to 256 cores, offering a theoretical 102400 MIPS.|$|E
25|$|In 2009 REvolution Computing {{accepted}} {{nine million}} dollars in venture capital from Intel and North Bridge Venture Partners, a private equity firm. Intel had previously supported REvolution Computing with venture capital in 2008. A number of Intel employees also joined Revolution Analytics as employees or as advisors. Concurrently, the company changed their name to Revolution Analytics and invited Norman Nie, founder of SPSS, to serve as CEO. This change in management corresponded with a movement toward building a more complete set of software for commercial users; prior to 2009 Revolution had been focused on building <b>parallel</b> <b>processing</b> functionality into the then mostly single threaded R. David Rich replaced Norman Nie as CEO in February 2012.|$|E
40|$|Environments for {{coupling}} symbolic computation with <b>parallel</b> numeric <b>processing</b> are demonstrated using Mathematica and an nCUBE 2 multiple-instruction-thread-multiple-data-stream computer. The so-called "math³" environments provide programming models that allow distributed symbolic computing, {{as well as}} a bridge between sequential symbolic computing and distributed numerical computation. Scientific visualization, one strength of Mathematica and a classical weakness of parallel machines, is showcased as a feature of the math³ environments. Applications that demand <b>parallel</b> symbolic <b>processing</b> also are discussed...|$|R
40|$|Networks of {{workstations}} (NOWs) are cost-effective {{alternatives to}} multiprocessor systems. Recently, NOWs {{have been proposed}} for <b>parallel</b> query <b>processing.</b> This paper explores the various architectures for using NOWs for <b>parallel</b> query <b>processing.</b> We start our discussion with two basic architectures: centralized and distributed. We identify {{the advantages and disadvantages}} associated with these architectures. Then we propose a hierarchical architecture that inherits the merits of the centralized and distributed architectures while minimizing the associated pitfalls. We also discuss the performance and research issues that require attention in all three architectures. Key words: <b>Parallel</b> query <b>processing,</b> Networks of workstations, Hierarchical architecture, Load sharing, Processor allocation, Databases 1 Introduction Relational databases have been the primary source of information repository for a very long time, and the support for querying this stored information is known as databa [...] ...|$|R
40|$|AgentSphere is a {{platform}} for an autonomous <b>parallel</b> distributed <b>processing</b> system based on strong migration mobile agent system. In general, {{it is very difficult}} to manage load balance and effectiveness of total <b>processing</b> performance on <b>parallel</b> distributed <b>processing</b> system. Our approach is that mobile agents help us to perform effective <b>parallel</b> distributed <b>processing</b> by agents' autonomy. In our conventional AgentSphere, one has to describe all of agent’s code necessary to functions of it by oneself. In this paper, we propose a mechanism for creating a new agent by using various functional agents, which have been prepared on the AgentSphere network a priori. According to this mechanism, when a user describe an agent's source code, one can write it easily and effectively without writing all of its code by means of selecting existing functional agents and delegating them to process the parts of functions...|$|R
