3477|4866|Public
5|$|The crucial {{property}} {{turns out}} to be treewidth, a measure of how tree-like the graph is. For a graph G of treewidth at most k and a graph H, the homomorphism problem can be solved in time |V(H)|O(k) with a standard dynamic <b>programming</b> <b>approach.</b> In fact, it is enough to assume that the core of G has treewidth at most k. This holds even if the core is not known.|$|E
25|$|Links to the MAPLE {{implementation}} of the dynamic <b>programming</b> <b>approach</b> may be found among the external links.|$|E
25|$|Several {{algorithms}} {{are available}} to solve knapsack problems, based on dynamic <b>programming</b> <b>approach,</b> branch and bound approach or hybridizations of both approaches.|$|E
40|$|In this paper, {{we present}} a new design-space {{exploration}} approach which we call the symbolic <b>program</b> <b>approach.</b> The symbolic <b>program</b> <b>approach</b> is based on both the trace driven approach and the control data ow graph approach. As expected, the trajectory of the symbolic <b>program</b> <b>approach</b> appears somewhere in-between the two extremes mentioned above. Thus, it leads to the shorter simulation time while it can still give fairly accurate performance numbers. Moreover, it produces results that can be readily taken as input for further design...|$|R
30|$|This {{is clearly}} a multi-choice {{stochastic}} bi-level programming problem. The problem cannot be solved without using multi-choice programming and stochastic <b>programming</b> <b>approaches.</b>|$|R
5000|$|Antioch University Seattle has master's and PhD level <b>programs</b> <b>approaching</b> {{environmental}} policy and sustainability issues from both a social science and natural science perspective.|$|R
25|$|This dynamic <b>programming</b> <b>approach</b> {{is used in}} machine {{learning}} via the junction tree algorithm for belief propagation in graphs of bounded treewidth. It also {{plays a key role}} in algorithms for computing the treewidth and constructing tree decompositions: typically, such algorithms have a first step that approximates the treewidth, constructing a tree decomposition with this approximate width, and then a second step that performs dynamic programming in the approximate tree decomposition to compute the exact value of the treewidth.|$|E
25|$|The same dynamic {{programming}} method {{also can be}} applied to graphs with unbounded pathwidth, leading to algorithms that solve unparametrized graph problems in exponential time. For instance, combining this dynamic <b>programming</b> <b>approach</b> with the fact that cubic graphs have pathwidth n/6+o(n) shows that, in a cubic graph, the maximum independent set can be constructed in time O(2n/6+o(n)), faster than previous known methods. A similar approach leads to improved exponential-time algorithms for the maximum cut and minimum dominating set problems in cubic graphs, and for several other NP-hard optimization problems.|$|E
25|$|Generally, {{sequence}} alignment means {{constructing a}} string from {{two or more}} given strings with the greatest similarity by adding, deleting letters, or adding a space for each string. The multiple sequence alignment problem is generally based on pairwise sequence alignment and currently, for pairwise sequence alignment problem, biologists can use dynamic <b>programming</b> <b>approach</b> to obtain its optimal solution. However, the multiple sequence alignment problem {{is still one of}} the intractable problems in bioinformatics, because finding the optimal solution of multiple sequence alignment has been proved as a NP-complete problem so that only approximate optimal solution can be obtained.|$|E
5000|$|The <b>program's</b> <b>approach</b> {{combines}} {{traditional classroom}} and laboratory training with specialized experiences such as visits from successful scientists, unique internships, and various field research.|$|R
40|$|In {{the past}} year the Yucca Mountain Site Characterization Project has {{implemented}} a new <b>Program</b> <b>Approach</b> to the licensing process. The <b>Program</b> <b>Approach</b> suggests a step-wise approach to licensing in which the early phases will require less site information than previously planned and necessitate a lesser degree {{of confidence in the}} longer-term performance of the repository. Under the <b>Program</b> <b>Approach,</b> the thermal test program is divided into two principal phases: (1) short-term in situ tests (in the 1996 to 2000 time period) and laboratory thermal tests to obtain preclosure information, parameters, and data along with bounding information for postclosure performance; and (2) longer-term in situ tests to obtain additional data regarding postclosure performance. This effort necessitates a rethinking of the testing program because the amount of information needed for the initial licensing phase is less than previously planned. This document proposes a revised and consolidated in situ thermal test program (including supporting laboratory tests) that is structured {{to meet the needs of}} the <b>Program</b> <b>Approach.</b> A customer-supplier model is used to define the Project data needs. These data needs, along with other requirements, were then used to define a set of conceptual experiments that will provide the required data within the constraints of the <b>Program</b> <b>Approach</b> schedule. The conceptual thermal tests presented in this document represent a consolidation and update of previously defined tests that should result in a more efficient use of Project resources. This document focuses on defining the requirements and tests needed to satisfy the goal of a successful license application in 2001, should the site be found suitable...|$|R
40|$|Summary: The {{capacity}} development {{approach was}} proposed by UNDP and European donors {{as a new}} approach based upon the African aid failure in the 1980 s {{and the end of}} the Cold War instead of conventional aid approach. In this paper, capacity development approach comes into collision with institutional studies in social sciences on purpose to accelerate knowledge evolution. This paper presents a new perspective on the development process and on aid policy. It is named an approach of “capacity development and institutional change”. By using this new approach of “capacity develop-ment and institutional change”, capacity development is able to cover not only technical cooperation but also lending matters. Moreover, the <b>program</b> <b>approach</b> is realized into development strategy and aid policy. The <b>program</b> <b>approach</b> indicates criteria of selectivity and priority of allocation of development resources including aid resources. <b>Program</b> <b>approach</b> concentrates more on the policy making process or on the top down (upstream) approach. Furthermore, this paper shows the importance of field experiences, meaning the advantages of Japanese aid compared to European aid, especially with regards to making the <b>program</b> <b>approach</b> more e#ective. The micro (field experience) and macro (top down) loop is a critical factor for aid e#ectiveness. 1...|$|R
2500|$|The dynamic <b>programming</b> <b>approach</b> {{to solve}} this problem {{involves}} breaking it apart into a sequence of smaller decisions. To do so, we define a sequence of value functions , for [...] which represent the value of having any amount of capital [...] at each time [...] Note that , that is, there is (by assumption) no utility from having capital after death.|$|E
2500|$|While daytime viewership has rebounded {{from its}} 2005 lows, primetime ratings remain weak {{relative}} {{to those of}} other news networks and CNBC continues to try and rejuvenate its evening lineup. Along with developing new program formats such as Fast Money and Mad Money, the network operates a [...] "checkerboard" [...] <b>programming</b> <b>approach,</b> airing various genres of shows including documentaries, town-hall style discussions and repeats of some NBC output.|$|E
2500|$|Collective dominance: [...] The i-th item is [...] collectively {{dominated}} by J, written as , if the total weight of {{some combination of}} items in J is less than wi and their total value is greater than vi. [...] Formally, [...] and [...] for some , i.e[...] [...] Verifying this dominance is computationally hard, so {{it can only be}} used with a dynamic <b>programming</b> <b>approach.</b> [...] In fact, this is equivalent to solving a smaller knapsack decision problem where , , and the items are restricted to J.|$|E
5000|$|The key {{difference}} between program assurance and program quality management or audit, is that program assurance tends {{to look at}} the potential impact of the <b>program's</b> <b>approach.</b>|$|R
40|$|International audienceThese last years, the multi-agent domain has {{proposed}} several proposals {{for the development}} of decentralised and open systems given birth to agent-oriented, environment- oriented, interaction-oriented or organisation-oriented <b>programming</b> <b>approaches.</b> In this paper, instead of privileging one dimension, we propose a seamless integration of these <b>programming</b> <b>approaches</b> into what we call "multi-agent oriented programming". We discuss how this approach brings the full potential of multi-agent systems as a programming paradigm {{to be used in the}} development of Intelligent Environments. We illustrate this with some applications and discuss how it opens interesting perspectives for ambient computing...|$|R
40|$|The {{operation}} of aggregators of {{distributed energy resources}} (DER) is a highly complex task that is affected by numerous factors of uncertainty such as renewables injections, load levels and market conditions. However, traditional stochastic <b>programming</b> <b>approaches</b> neglect information around temporal dependency of the uncertain variables due to computational tractability limitations. This paper proposes a novel stochastic dual dynamic <b>programming</b> (SDDP) <b>approach</b> for the optimal {{operation of}} a DER aggregator. The traditional SDDP framework is extended to capture temporal dependency of the uncertain wind power output, through the integration of an n-order autoregressive (AR) model. This method is demonstrated to achieve a better trade-off between solution efficiency and computational time requirements compared to traditional stochastic <b>programming</b> <b>approaches</b> based {{on the use of}} scenario trees...|$|R
5000|$|For {{the case}} of two {{sequences}} of n and m elements, the running time of the dynamic <b>programming</b> <b>approach</b> is O(n × m). For an arbitrary number of input sequences, the dynamic <b>programming</b> <b>approach</b> gives a solution in ...|$|E
5000|$|The FAUST {{programming}} model {{combines a}} functional <b>programming</b> <b>approach</b> with a block diagram syntax: ...|$|E
5000|$|Vázquez Aldir {{described}} Imagen's <b>programming</b> <b>approach</b> as [...] "a family channel, with a {{focus on}} women".|$|E
5000|$|... #Caption: OV-101 Enterprise takes flight for {{the first}} time over Dryden Flight Research Facility, Edwards, California in 1977 as part of the Shuttle <b>program's</b> <b>Approach</b> and Landing Tests (ALT).|$|R
50|$|The {{motivation}} for aspect-oriented <b>programming</b> <b>approaches</b> {{stem from the}} problems caused by code scattering and tangling. The purpose of Aspect-Oriented Software Development is to provide systematic means to modularize crosscutting concerns.|$|R
5000|$|The first {{trigger for}} a trap-neuter-return program is when free-roaming cats or kittens {{are seen in}} need and/or not having been neutered. A TNR <b>program</b> <b>approaches</b> the {{situation}} using the following recommended steps: ...|$|R
50|$|Forthcoming. The <b>programming</b> <b>approach</b> and the {{dissolution}} of economics. A manifesto against determinism in the social sciences.|$|E
50|$|Links to the MAPLE {{implementation}} of the dynamic <b>programming</b> <b>approach</b> may be found among the external links.|$|E
50|$|<b>Programming</b> <b>Approach</b> to Computability (The Akm Series in Theoretical Computer Science) by A. J. Kfoury, et al.|$|E
5000|$|Embedded Systems Architectures Summary of {{general-purpose}} architectures (recall). Focus on the ARM architecture; The coprocessor concept; Multiprocessor fundamentals; GPU architectures: basics, <b>programming</b> <b>approaches.</b> Lab on ARM with {{interrupt handling}} {{and design of}} a device driver.|$|R
5000|$|WAVE {{provided}} professional development, curriculum, technical assistance, {{and other}} supports {{to schools and}} youth programs that implemented WAVE programs locally. WAVE offered five main <b>program</b> <b>approaches</b> that any school or organization can adapt and implement: ...|$|R
50|$|The NCAS is {{constantly}} looking {{to grow and}} expand {{when it comes to}} programs offered to its members. In addition, several national <b>programs</b> <b>approach</b> the NCAS looking to partner because of its great membership of colleges and universities across the country.|$|R
5000|$|Object-oriented {{programming}} (OOP) {{developed in}} the early 1960s, and became a dominant <b>programming</b> <b>approach</b> during the mid-1990s ...|$|E
50|$|Several {{algorithms}} {{are available}} to solve knapsack problems, based on dynamic <b>programming</b> <b>approach,</b> branch and bound approach or hybridizations of both approaches.|$|E
5000|$|Nussinov {{proposed}} the first dynamic <b>programming</b> <b>approach</b> for nucleic acid secondary structure prediction, {{this method is}} now known as the [...] "Nussinov Algorithm".|$|E
40|$|This article {{developed}} an approached model of congestion, based on relaxed combination of inputs, in stochastic {{data envelopment analysis}} (SDEA) with chance constrained <b>programming</b> <b>approaches.</b> Classic data envelopment analysis models with deterministic data {{have been used by}} many authors to identify congestion and estimate its levels; however, data envelopment analysis with stochastic data were rarely used to identify congestion. This article used chance constrained <b>programming</b> <b>approaches</b> to replace stochastic models with ‘‘deterministic equivalents”. This substitution leads us to non-linear problems that should be solved. Finally, the proposed method based on relaxed combination of inputs was used to identify congestion input in six Iranian hospital with one input and two outputs in the period of 2009 to 2012...|$|R
40|$|The {{purposes}} of this memorandum are to (1) summarize common systems for third party verification used in U. S. and international greenhouse gas (GHG) reporting programs, (2) summarize EPA regulatory <b>program</b> <b>approaches</b> for data verification, including the acid rain <b>program</b> <b>approach,</b> (3) discuss issues and considerations for EPA’s decision on whether the mandatory GHG reporting rule will require third party verification or rely on facility selfcertification of reports with (or without) EPA verification, (4) discuss cost differences between third party verification versus self-certification with EPA verification approaches, and (5) identify the types of data that {{would need to be}} reported if EPA were to verify the data rather than require third party verification. Attachments provide additional details...|$|R
40|$|Management and {{leadership}} Supporting vocational {{education and training}} providers in building capability for the future CONSORTIUM RESEARCH <b>PROGRAM</b> <b>Approaches</b> for sustaining and building management {{and leadership}} capability in VET providers NCVER Approaches for sustaining and building management and leadership capability in VET provider...|$|R
