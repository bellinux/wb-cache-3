0|17|Public
40|$|Web {{developers}} often want to repurpose interactive behaviors from third-party web pages, but {{struggle to}} locate the specific source code that implements the behavior. This task is chal-lenging because developers must find and connect all of the non-local interactions between event-based JavaScript code, declarative CSS styles, and web page content that combine to express the behavior. The Scry tool embodies {{a new approach to}} locating the code that implements interactive behaviors. A developer selects a page element; whenever the element changes, Scry cap-tures the rendering engine’s inputs (DOM, CSS) and outputs (screenshot) for the element. For any two captured element states, Scry can compute how the states differ and which lines of JavaScript code were responsible. Using Scry, a developer can locate an interactive behavior’s implementation by <b>picking</b> two <b>output</b> states; Scry indicates the JavaScript code directly responsible for their differences...|$|R
40|$|Abstract. Secure multiparty {{computation}} {{allows a}} group of distrusting parties to jointly compute a (possibly randomized) function of their inputs. However, it {{is often the case}} that the parties executing a computation try to solve a search problem, where one input may have a multitude of correct answers – such as when the parties compute a shortest path in a graph or find a solution to a set of linear equations. <b>Picking</b> one <b>output</b> arbitrarily from the solution set has significant implications on the privacy of the algorithm. Beimel et al. [STOC 2006] gave a minimal definition for private computation of search problems with focus on proving impossibility result. In this work we aim for stronger definitions of privacy for search problems that provide reasonable privacy. We give two alternative definitions and discuss their privacy guarantees. We also supply algorithmic machinery for designing such protocols for a broad selection of search problems. ...|$|R
50|$|The {{digital signal}} {{processing}} technology used in modern hybrids addresses the isolation requirement and implements ancillary functions.ISDN and VoIP telco connections theoretically have no need for hybrids. However, calls that have ISDN or VoIP on one end usually terminate to an analog line at the other, {{and so there is}} a significant source of echo from both the telco hybrid on the line card and the phone itself. Acoustic coupling, when the microphone <b>picks</b> up the <b>output</b> of the earpiece, is another potential source of echo. Electrical pickup between analog circuits (crosstalk) is yet another. Even low echo levels can be audible when there is a long delay, as is usually the case with VoIP.|$|R
40|$|The {{first four}} decades of {{computer}} technology are each characterized by {{a different approach to}} the way computers were used. In the 1950 s, programmers would reserve time on the computer and have the computer all to themselves while they were using it. In the 1960 s, batch processing carne about. People would submit their jobs which were queued for processing. They would be run one at a time and the owners would <b>pick</b> up their <b>output</b> later. Time-shanng became the way people used computers in the 1970 s so that users could share a computer under the illusion that they had it to themselves. The 1980 s are the decade of personal computing: people have their own dedicated machine on their desks...|$|R
30|$|A LabVIEW {{software}} controls spinning and translating the specimen, sets the furnace temperature through the thermocontroller, controls a direct current {{source for the}} solenoid coil, and obtains the signal from the fluxgate sensor. A digital input/output board CONTEC PIO 32 / 32 F(PCI), installed in a PCI bus of a personal computer, sends on/off signals to the spinning and translating motors, the solenoid coil, and a blower, whereas it receives signals from the photo-interrupters monitoring the position and angle of a specimen. The thermocontroller is operated through an RS 232 C port, and the current source is connected to a USB port. An analog input board, INTERFACE PCI- 3135, <b>picks</b> up the <b>output</b> voltage {{in the range of}} ± 10  V from the fluxgate sensor and converts it into a 16 -bit digital signal. Driver softwares that enable LabVIEW to communicate with the digital or analog boards are available from the respective manufacturer.|$|R
40|$|It {{is often}} {{important}} {{to be able to}} automatically detect `who spoke when' in audio data. The speaker diarisation task attempts to address this problem on Broadcast News data by defining an error rate which can be used to evaluate segmentations and their associated speaker labels. Many different methods exist to automatically generate such segmentations and it would be desirable if segmentations from different origins could be combined to produce a more accurate one. This paper introduces a cluster voting scheme which attempts to use information from more than one diarisation system to produce a new speaker segmentation with a lower diarisation error rate. The scheme first generates a set of possible segmentations which minimise a distance metric based on the diarisation error rate and then defines a method of <b>picking</b> the final <b>output</b> from this set. Experiments presented using two inputs confirm that the diarisation error rate can be reduced using this new method...|$|R
40|$|Neighborhood {{preservation}} from input {{space to}} output space {{is an essential}} element of selforganizing feature maps like the Kohonen-map. However, a measure for the preservation or violation of neighborhood relations, which is more systematic than just visual inspection of the map, was lacking. We show, that a topographic product P, first introduced in nonlinear dynamics, is an appropriate measure in this regard. It is sensitive to large scale violations of the neighborhood ordering, but does not account for neighborhood ordering distortions due to varying areal magnification factors. A vanishing value of the topographic product indicates a perfect neighborhood preservation, negative (positive) values indicate a too small (too large) output space dimensionality. In a simple example of maps from a 2 D input space onto 1 D, 2 D and 3 D output spaces we demonstrate how the topographic product <b>picks</b> the correct <b>output</b> space dimensionality. In a second example we map 19 D speech data onto various output spaces and find, that a 3 D output space (instead of 2 D) seems to be optimally suited to the data. This is in agreement with a recent speech recognition experiment on the same data set...|$|R
40|$|AbstractSemantic web {{offers a}} smarter web service which synchronizes and {{arranges}} {{all the data}} over web in a disciplined manner. In data mining over web, the accuracy of selecting necessary data according to user demand and <b>pick</b> them for <b>output</b> is considered as a major challenging task over the years. This paper proposes an approach to mapping data over the web 3. 0 through ontology and access the required data via an intelligent agent. The agent provides all the searched data related to user query from which user can find desired information. When the user does not have sufficient search parameter, knowledge can be perceived from the {{information provided by the}} agent. The derivation of such unknown knowledge from the existing can be achieved by semantic web mining. We present an intelligent agent-based web mining model where users’ query is being searched by following existing traditional way, e. g. by Google. The intelligent agent checks the searched data and derives only those are the semantically related to users search parameter. A work-in-progress case study of University Faculty Information presented to examine the effectiveness of the proposed model...|$|R
40|$|A humble {{technique}} is suggested to spot turbines and <b>pick</b> their rule <b>output</b> appraise charges and revenues and offer useful indications for Micro Hydro Power MHP plant project in current irrigation systems. This technique based on simple models and demanding a reduced number of input factors cool to survey in primary project stages {{has been used}} and confirmed in an existing irrigation system located in Calabria Italy. The consequences have emphasized {{that in the case}} study the lowest profitable turbine would yield 5 KW. A lower number of plants with higher output would create no particular monetary savings compared to a greater number of smaller turbines. Besides neither was the choice of increasing pipe diameter found to provide savings. In general a considerable potential from MHP operation has been revealed in current irrigation systems providing a return on investment higher than that provided by the Italian monetary market. Lastly MHP usefulness noticeably rises with total annual process time being on average 55 higher in a wet year eight months of electrical productionfour month of irrigation as opposed to a dry year six months of electrical productionsix months of irrigation...|$|R
40|$|In 2009, the Austrian economy {{contracted}} by 3. 4 {{percent in}} volume. Goods exports, manufacturing output {{and investment in}} business equipment fell particularly strongly, while private consumption proved resilient, edging up slightly. Driven by the expansionary monetary and fiscal policy stance adopted worldwide, the downward trend of activity turned around as of mid- 2009, with Austrian exports and industrial <b>output</b> <b>picking</b> up. The recovery is set to continue {{over the next few}} quarters, allowing real GDP to grow by 1. 5 percent in 2010. However, the still low capacity utilisation will hardly provide incentives for higher investment in the short run. In a first outlook on 2011, real GDP growth is projected at 1. 6 percent. The momentum of growth in 2010 and 2011 will be too weak to prevent unemployment from rising. According to national definitions, unemployment in 2011 will reach 8. 1 percent of the dependent labour force, the highest rate since 1953. The general government balance will markedly deteriorate under the impact of automatic stabilisers, notably those affecting revenues, and the budgetary cost of the fiscal stimulus programmes. For 2010, a deficit amounting to 5. 2 percent of GDP is expected. Economic Outlook; Austria...|$|R
40|$|The {{aim of this}} Bachelor Thesis is to {{introduce}} Data Envelopment Analysis method and to apply it to the secondary schools of Southeastern Estonia. DEA method is a mathematical method that calculates the efficiency by solving linear programming problem. This method is special because it can calculate efficiency of non-profit organizations like hospitals, libraries and administrative units. This paper consists of four parts. The first part is {{the introduction of the}} DEA method. The second part is about choosing the inputs and outputs. DEA method needs different inputs and outputs and to find the best parameters, we contacted the County Governments of the three Southeastern Estonian counties, Põlva, Valga and Võru, which schools we analyzed in the paper. Every county government proposed a different set of inputs and outputs and so we got three sets to use in the DEA method. The fourth set was conducted using the information found in the earlier studies from all over the world. The third part is application of the method to the secondary schools of Southeastern Estonia and the fourth part is results. We analyzed 20 schools. To apply the method on the sets we used Data Envelopment Analysis (Computer) Program (DEAP) which is a DOS-program from the nineties, but inspite of the age is a very good program. It makes applying the DEA method easy. It only needs two. txt files, which consist of instructions and data. The program has many choices {{when it comes to the}} type of DEA. User can <b>pick</b> from <b>output</b> or input oriented method, one-, two- or multistage method, Malmquist or cost DEA. Results of the analyze depend on the inputs and outputs. With one set there were only three efficient schools and in another there were ten. The mean value of efficiency varied from 0, 732 to 0, 950. This study can be used by the profesionals to analyze the educational system. The most important part of the analyze is to find the right inputs and outputs to describe the educational system...|$|R
40|$|A Python {{package for}} the {{detection}} and analysis of repeating and near-repeating seismicity. This release is mostly a feature release, with a few minor bug-fixes. For this release we add a subspace detector. The following is the change-log for this release: Bug-fix in plot_repicked removed where data were not normalized properly; Bug-fix in lag_calc where data were missing in the continuous data fixed (this led to incorrect <b>picks,</b> major bug!); <b>Output</b> cross-channel correlation sum in lag-calc output; Add id to DETECTION objects, {{which is consistent with}} the events within DETECTION objects and catalog output, and used in lag_calc to allow linking of detections to catalog events; Add lots of logging and error messages to lag-calc to ensure user understands limits; Add error to day-proc to ensure user is aware of risks of padding; Change utils. pre_processing. process to accept different length of data enforcement, not just full day (allow for overlap in processing, which might be useful for reducing day start and end effects); Bug-fix in mag_calc. amp_pick_event, broke loop if data were missing; Lots of docs adjustment to sort order of doc-strings and hyper-links; Allow multiple uses of the same channel in templates (e. g. you can now use a template with two windows from the same channel, such as a P and an S); Add evaluation mode filter to utils. catalog_utils. filter_picks; Update subspace plot to work when detector is not partitioned; Make tests run a little faster; Add pep 8 testing for all code...|$|R
30|$|The {{comparison}} between activation function and spike activity versus L-LFP is an approximation, {{based on a}} number of assumptions. First, the L-LFP is generated by multiple types of cellular current [17]. However, {{it is reasonable to assume}} that during the high levels of activity during seizures, the synaptic component will be the principal contributor [4, 6, 18]. In addition, a significant part of the non-synaptic sources of the L-LFP will be proportional to synaptic activity. In this context, it should be noted that such a relationship between synaptic activity and field potential has been the basis of many models of the electroencephalogram (EEG) as well, e.g. [19]. Next, we use the spike signal as a metric for network output while the multi-unit spike activity in a micro-electrode recording contains both input as well as output spikes of the local population. This is plausible since, due to geometry, the probability of <b>picking</b> up an <b>output</b> spike from an active neuron is much higher than recording from a thin afferent axon. Furthermore, if we assume the input spikes are proportional to the synaptic potentials they generate, they could only destroy the Gaussian-like result that we obtained in Fig.  1 C. Another significant fact is that we only found Gaussian-like functions as in Fig.  1 C within the epileptic core and not outside that area. This suggests that (inhibitory) cells reach depolarization block only within the core. Thus, although the relationship between L-LFP and multi-unit activity is not an exact measure of the population’s activation function, it is a reasonable proxy for it.|$|R
40|$|The {{demand for}} the {{concrete}} products is seasonal and huge stock is built in winter for dispatch in summer. As 1000 - 2000 products with different sizes, weights, handling and stacking requirements are involved, the process of deciding appropriate locations to stock the products and track them while loading into lorries for dispatch becomes complex. Stockyards in the precast concrete products industry are experiencing space congestion, and long vehicle waiting times for both the storage and retrieval of concrete products {{due to lack of}} a proper methodology to manage stockyard layouts and their operations. This paper describes an ongoing research that addresses the stockyard layout management problem through the development of an integrated simulation and visualisation model. The paper focuses {{on the development of the}} visualisation and simulation element of the stockyard management system "SimStock". The simulation model has been developed using ARENA/SIMAN, a general-purpose simulation language. The simulation model integrates production and forecast schedules, evaluates "what-if" scenarios with different layouts, products allocation to storage locations and order <b>picking</b> policies. The <b>output</b> of the simulation model is recorded in a database (Ms Access). The visualisation model was developed through integrating AutoCAD 2000 with the database of the simulation model such that the simulated layouts can be studied in greater details and validated in a simpler manner. The visualisation module is used to assist managers in designing stock layouts (one of the major inputs to the simulation model) and visualise the simulation process in 2 D (and 3 D) perspectives, and manage real time implementation of proposed stockyard solutions...|$|R
60|$|The Child cried inconsolably, {{and grew}} hollow-eyed, knock-kneed, spindling, and corykilverty {{in many other}} respects. The Millionaire smiled and tapped his coffers confidently. The <b>pick</b> of the <b>output</b> of the French and German toymakers was rushed by special {{delivery}} to the mansion; but Rachel refused to be comforted. She was weeping for her rag child, and was for a high protective tariff against all foreign foolishness. Then doctors with the finest bedside manners and stop-watches were called in. One by one they chattered futilely about peptomanganate of iron and sea voyages and hypophosphites until their stop-watches showed that Bill Rendered was under the wire for show or place. Then, as men, they advised that the rag-doll be found {{as soon as possible}} and restored to its mourning parent. The Child sniffed at therapeutics, chewed a thumb, and wailed for her Betsy. And all this time cablegrams were coming from Santa Claus saying that he would soon be here and enjoining us to show a true Christian spirit and let up on the pool-rooms and tontine policies and platoon systems long enough to give him a welcome. Everywhere the spirit of Christmas was diffusing itself. The banks were refusing loans, the pawn-brokers had doubled their gang of helpers, people bumped your shins on the streets with red sleds, Thomas and Jeremiah bubbled before you on the bars while you waited on one foot, holly-wreaths of hospitality were hung in windows of the stores, they who had 'em were getting their furs. You hardly knew which was the best bet in balls--three, high, moth, or snow. It was no time at which to lose the rag-doll or your heart.|$|R
40|$|Diplomska naloga nam prikaže enega izmed možnih načinov kalibracije pulznega oksimetra. Raziskali bomo možnost manipulacije svetlobe od izvora proti ponoru oksimetra. Opazujemo, kako se svetloba spreminja, ko prečka aktiven medij – prst. Uspešno zaključena kalibracija pomeni, da je signal, ki ga oddamo iz kalibratorja, podoben pravemu signalu na fotopletizmografu v sprejemnem delu oksimetra, ko nanj pade svetloba iz medija. S pravilno oblikovanim signalom, ter znanimi vrednostmi AC ter DC komponente signalov rdeče in infrardeče svetlobe, lahko ustvarimo tabelo ter iz te tabele izračunamo oksigenacijo krvi. Poleg oksigenacije lahko preverimo tudi natančnost meritve srčnega utripa, saj signal moduliramo tako frekvenčno kot amplitudno. Ves nadzor nad intenziteto svetlobe imamo preko računalnika v programu ArduinoIDE. Spoznamo, da je kalibracija možna, vendar je močno odvisna od karakteristik sprejemnega dela individualnega oksimetra. Reading {{through this}} thesis, {{you will be}} shown a few {{possible}} calibrations of pulse oximeter. We will focus on manipulation of the light source {{in a way in}} which the light coming out of calibrators output will create a signal that will replicate tissues response. It is crucial to <b>pick</b> the right <b>output</b> LED of the calibrator in order to satisfy the needs for an accurate calibration. Therefore LED should emit wavelengths in the bandwidth of the oximeters receiver. The calibrator is ready to be programmed to a certain amplitude and frequency to replace tissues response. We will be controlling the amplitude and the frequency of the calibrators LEDs via personal computer inside the ArduinoIDE program. In our results we can see that this kind of calibration is possible, but is highly dependent on oximeters receiver circuit. Each has a different response. Making experiments and putting results into a table should provide a stable calibration...|$|R
40|$|A Python {{package for}} the {{detection}} and analysis of repeating and near-repeating seismicity. This release contain significant updates to the object-oriented API for matched-filter detection (while retaining the old API), alongside re-written, more efficient correlation internals, and a re-designed (more accurate and efficient) subspace detection module. The change log for this release is as follows: Increase test coverage (edge-cases) in template_gen; Fix bug in template_gen. extract_from_stack for duplicate channels in template; Increase coverage somewhat in bright_lights, remove non-parallel option (previously only used for debugging in development); Increase test coverage in lag_calc; Speed-up tests for brightness; Increase test coverage for match_filter including testing io of detections; Increase subspace test coverage for edge cases; Speed-up catalog_to_dd_tests; Lag-calc will pick S-picks on channels ending E, N, 1 and 2, change from only picking on E and N before; warning added to docs; Add full tests for pre-processing; Run tests in parallel on ci, speed-up tests dramatically; Rename singular-value decomposition functions (with depreciation warnings); Rename SVD_moments to lower-case and add depreciation warning; Increase test coverage in utils. mag_calc; Add Template, Tribe, Family, Party objects and rename DETECTION to Detection; Template objects maintain meta-data associated with their creation to stream-line processing of data (e. g. reduce chance of using the wrong filters). Template events have a detect method which takes unprocessed data and does the correct processing using the Template meta-data, and computes the matched-filter detections. Tribe objects are containers for multiple Templates. Tribe objects have a detect method which groups Templates with similar meta-data (processing information) and runs these templates in parallel through the matched-filter routine. Tribe. detect outputs a Party of Family objects. The Party object is a container for many Family objects. Family objects are containers for detections from the same Template. Family and Party objects have a lag_calc method which computes the cross-correlation pick-refinements. The upshot {{of this is that}} it is possible to, in one line, generate a Tribe of templates, compute their matched-filter detections, and generate cross-correlation <b>pick</b> refinements, which <b>output</b> Event objects, which can be written to a catalog: Tribe. construct(method, kwargs). detect(st, kwargs). lag_calc(**kwargs). write() Added 25 tests for these methods. Add parameters threshold_type and threshold_input to Detection class. Add support for legacy Detection objects via NaN and unset values. Removed support for obspy < 1. 0. 0 Update / correct doc-strings in template-gen functions when describing processing parameters. Add warning message when removing channels from continuous data in match_filter; Add min_snr option for template generation routines, if the signal-to-noise ratio is below a user-defined threshold, the channel will not be used. Stop enforcing two-channel template channel names. Fix bug in detection_multiplot which didn't allow streams with fewer traces than template; Update internals to custom C fftw-based correlation rather than openCV (Major change); OpenCV has been removed as a dependancy; eqcorrscan. core. match_filter. normxcorr 2 now calls a compiled C routine; Parallel workflows handled by openMP rather than Python Multiprocessing for matched-filter operations to allow better memory handling. It is worth noting that we tried re-writing using SciPy internals which led to a significant speed-up, but with high memory costs, we ended up going with this option, which was the more difficult option, because it allows effective use on SLURM managed systems where python multiprocessing results in un-real memory spikes (issue # 88) ...|$|R

