3|10000|Public
40|$|This paper {{introduces}} adaptation-aware {{editing and}} progressive update propagation, two novel mechanisms that enable authoring multimedia content and collaborative work on mobile devices. Adaptation-aware editing enables editing content that was adapted to reduce download {{time to the}} mobile device. Progressive update propagation reduces the time for propagating content generated at the mobile device by transmitting either {{a fraction of the}} modifications or transcoded versions thereof. With application-aware editing and progressive update propagation, an object present at a mobile device is characterized not only by a particular version, as in conventional replication, but also by a particular fidelity. We demonstrate that replication models can be extended to account for fidelity independently of the mechanisms used for concurrency control and consistency maintenance. As a result, the two techniques described in this paper can easily be added to any replication protocol, whether optimistic or pessimistic. We report on our experience implementing adaptation-aware editing and progressive update propagation. Experiments with two multimedia applications, an email reader and a <b>presentation</b> <b>software</b> <b>package,</b> show that both mechanisms can be added with modest programming effort and achieve substantial reductions in upload and download latencies...|$|E
40|$|A {{project to}} improve the Hanford Site's {{corrosion}} monitoring strategy was started in 1995. The project is designed to integrate EN-based corrosion monitoring into the site's corrosion monitoring strategy. In order to monitor multiple tanks, {{a major focus of}} this project has been to automate the data collection and analysis process. Data collection and analysis from the early EN corrosion monitoring equipment (241 -AZ- 101 and 241 -AN- 107) was primarily performed manually by a trained operator skilled in the analysis of EN data. Thousands of raw data files were collected, manually sorted and stored. Further statistical analysis of these files was performed by manually stripping out data from thousands of raw data files and calculating statistics in a spreadsheet format. Plotting and other graphical display analyses were performed by manually exporting data from the data files or spreadsheet into another plotting or <b>presentation</b> <b>software</b> <b>package.</b> In 1999, an Amulet/PRP system was procured and employed on the 241 -AN- 102 corrosion monitoring system. A duplicate system was purchased for use on the upcoming 241 -AN- 105 system. A third system has been procured and will eventually be used to upgrade the 241 -AN- 107 system. The Amulet software has greatly improved the automation of waste tank EN data analysis. In contrast with previous systems, the Amulet operator no longer has to manually collect, sort, store, and analyze thousands of raw EN data files. Amulet writes all data to a single database. Statistical analysis, uniform corrosion rate, and other derived parameters are automatically calculated in Amulet from the raw data while the raw data are being collected. Other improvements in plotting and presentation make inspection of the data a much quicker and relatively easy task. These and other improvements have greatly improved the speed at which EN data can be analyzed in addition to improving the quality of the final interpretation. The increase in data automation offered by the Amulet software is necessary if multiple tanks are to instrumented and analyzed at the Hanford Site. Although advances in the automation of data analysis have been great, Hanford EN data analysis still demands a highly trained corrosion expert. Neural networks could de-skill the post-data collection analysis procedure and broaden the range of users able to understand and interpret corrosion data. Ultimately, the ability to de-skill data the data analysis process will make or break the use of EN as a plant monitoring tool on a wide scale...|$|E
40|$|Abstract]: The Cartographer {{with the}} tools of today’s visualisation, interactivity, {{multimedia}} and presentation media all {{made possible by the}} computer, has the ability to transform the traditional paper map view into a three – dimensional visualization voyage of discovery. The general objective in this project was to develop an interactive map associated with tourism, using the cartographic tools of today using past data to develop a map product that will present the relevant material to a users associated with tourism, in an effective manner. The challenge defined by the objectives was to convey the images of the quarry at Blue Circle Southern Cement’s (BCSC) Waurn Ponds mine site to the user who was identified as, a sightseer, a student or a mine employee or a person or organisation involved in the tourism industry. The needs of the user were to see the dynamic ever changing terrain of the quarry, the processes of mineral extraction, the layers of geology all incorporated simply in an interactive multimedia display based on photo maps used in a way to aid the communication of information. These images were created with the 3 D visualization tools of Geographic information Systems (GIS) and generated from data associated with past land surveys, rejuvenated for this map and aerial photographs rectified to existing surfaces. The format of these 3 Dimensional (3 D) visualisation has included an aerial photograph draped over a digital terrain model and rotated to present an 3 D animation revealing an informative view of the site. A virtual 3 D fly through was created along the haul road system slicing through the reclaimed landscape of past mining site. To highlight the past a temporal display dating back over six years was produced with colour and 3 D enhanced landscapes in an effective display. The mining processes of the open cut quarry shows limestone removal and transportation to processing locations. The dumping of overburden that was stripped from above the limestone, this was all captured in digital video enriched by the sounds of powerful machines as they worked. The final result is an informative interactive map that is presented simply, well within the potential of PowerPoint; a friendly, widely available and versatile multimedia presentation package. This has allowed the focus to remain on the user while utilising large data sources and presenting advanced forms of 3 D visualisation. The results have shown how effective the tools that have been used from GIS have been {{with the use of the}} appropriate data to display selected information in the this interactive map. How versatile the ‘off the shelf’ <b>presentation</b> <b>software</b> <b>package</b> was in the acceptance of this information for presentation to the user in a simple format. Following the comparison with other map formats, it has allowed the mind to enquire what level of interactive map could be produce if a fully integrated authoring system package was available via the internet and the possible effect this would have on the map and more importantly the user. ...|$|E
40|$|Terrain {{rendering}} {{is widely}} used in industry and research. GIS <b>software</b> <b>packages</b> as well as navigation systems make use of ter- rain rendering to visualize terrain information. Recent trends in research show that scientific terrain visualization is shifting {{more and more to}} an interactive analysis tool. This allows domain spe- cific users to perform visual analysis tasks within an interactive visual environment. Visual analysis tools are <b>software</b> <b>package</b> act- ing as a toolbox and providing functionality to support the work of domain specific users such as data exploration, data analysis and data <b>presentation.</b> Such <b>software</b> <b>packages</b> still suffer from limita- tions such as restricted or imprecise data and problems with large data handling. These challenges will also be at the core of research in scientific terrain visualization in the near future. In this paper we describe some open challenges for scientific terrain visualization in the acquisition, processing and rendering of terrain related geo- spatial information as well as new methods which could be used to address these challenges...|$|R
40|$|Laser {{pointers}} {{are completely}} safe when properly {{used as a}} visual or instructional aid. However, they can cause serious eye damage when used improperly. There have been enough documented injuries from laser pointers to trigger a warning from the Food and Drug Administration (FDA and Safety Notification). Before deciding to use a laser pointer, presenters are reminded to consider alternate methods of calling attention to specific items. Most <b>presentation</b> <b>software</b> <b>packages</b> offer screen pointer options with the same features, but without the laser hazard. Selecting a Safe Laser Pointer 1) Choose low power lasers (Class 2 or 3 R, formerly 3 a) Whenever possible, select a Class 2 laser pointer because of the lower risk of eye damage. If a more powerful laser is required, presenters may use a Class 3 R laser {{with the understanding that}} greater responsibility and care are required. Laser devices with other laser classes are inappropriate for use as a laser pointer (see Identifying Laser Characteristics below). 2) Choose red-orange lasers (633 to 650 nm wavelength, choose closer to 635 nm) Early diode laser pointers were dark red (670 nm wavelength) and difficult to see due to the eye’s lower response to red. Newer red-orange lasers are significantly brighter than their dark red predecessors using the same or even less power. See below for available products. Green and blue laser pointers are available, however each has serious drawbacks 1. Initially, green will appear brighter than red. However, green may actually be too bright and has been found to leave a distracting after-image on the retina, making it difficult to concentrate on the presentation. Safety concerns have been raised about photo-biological effects from blue light laser pointers (400 - 500 nm) and they should be avoided. 3) On/Off Switch Ensure that the on switch is a momentary contact type that is designed to shut off the pointer when released. Use of a laser pointed with a locking device to keep the laser beam on is prohibited. Each label includes: the starburst insignia, laser classification, maximu...|$|R
40|$|The LCG Applications Area at CERN {{provides}} {{basic software}} components for the LHC experiments such as ROOT, POOL, COOL which are developed in house {{and also a}} set of "external" <b>software</b> <b>packages</b> (70) which are needed in addition such as Python, Boost, Qt, CLHEP, etc. These packages target many different areas of HEP computing such as data persistency, math, simulation, grid computing, databases, graphics, etc. Other packages provide tools for documentation, debugging, scripting languages and compilers. All these packages are provided in a consistent manner on different compilers, architectures and operating systems. The Software Process and Infrastructure project (SPI) [1] {{is responsible for the}} continous testing, coordination, release and deployment of these <b>software</b> <b>packages.</b> The main driving force for the actions carried out by SPI are the needs of the LHC experiments, but also other HEP experiments could profit from the set of consistent libraries provided and receive a stable and well tested foundation to build their experiment <b>software</b> frameworks. This <b>presentation</b> will first provide {{a brief description of the}} tools and services provided for the coordination, testing, release, deployment and <b>presentation</b> of LCG/AA <b>software</b> <b>packages</b> and then focus on a second set of tools provided for outside LHC experiments to deploy a stable set of HEP related <b>software</b> <b>packages</b> both as binary distribution or from source...|$|R
40|$|A large {{national}} research project in Australia {{over the past}} seven years has focused on improving the quality of grains for feeding to livestock. A major expectation of the grain and livestock industries from this project is the adoption of rapid and objective analytical tests for relevant quality criteria of grains, preferably at the point of delivery. NIR spectroscopy obviously has a key role, and calibrations have been derived, or attempted, for 52 different chemical and physical properties, several in vitro or in sacco tests, and in vivo digestibility or energy content for ruminants [1 - 3], pigs and poultry. Final calibrations were developed using over 100 grains, either whole or ground, which were scanned on laboratory-based grating monochromators, namely Foss-NIRSystems models 6500 or 5000 instruments. However, the industry has requested that the appropriate NIR spectroscopy testing procedures be made available on simpler, cheaper and as such, often older, spectrometers. This presents a real challenge for calibration transfer, given the differences in optical configuration, spectral range, sample <b>presentation</b> and <b>software</b> <b>packages</b> among the instrument types. Some instruments also target specific applications and constituents. Due to the nature of many of the tests of interest in this project, it was a difficult task to make direct and fair comparisons between NIR spectrometers in the market-place. The {{purpose of this study was}} to compare the accuracy of some NIR spectrometers for estimation of selected indicators of feed grain quality, using whole grain samples...|$|R
50|$|Caligari Corporation {{was founded}} in 1985 by Roman Ormandy. A {{prototype}} 3D video animation package for the Amiga Computer, {{which led to the}} incorporation of Octree Software in 1986. From 1988 to 1992, Octree released several <b>software</b> <b>packages</b> including Caligari1, Caligari2, Caligari Broadcast, and Caligari 24. Caligari wanted to provide inexpensive yet professional industrial video and corporate <b>presentation</b> <b>software.</b> In 1993, Octree Software moved from New York to California and became known as Caligari Corporation. In 1994 trueSpace 1.0 was introduced on the Windows platform. In early 2008, the company was acquired by Microsoft.|$|R
40|$|Purpose: The loss of raw {{materials}} in wood cutting industries has reached high proportions (30 to 36 % of volume yield, Ngolle Ngolle, 2009). The {{purpose of this paper}} is to solve the problem of optimising the production on the basis of the commercial value of the cuts. Design/methodology/approach: In order to tackle this problem, we started with the generalities on the exploitation and the primary conversion of wood in Cameroon. After that, we studied the various methods of cutting and the different products obtained. We then proceeded with the formulation of the log cutting optimisation problem based on a real shape model of the logs, objective of research works presented in (Danwé, Bindzi, Meva’a & Nola, 2011). We finally completed our work with the design and the <b>presentation</b> of a <b>software</b> <b>package</b> called cutting optimiser. Findings: We start by having some knowledge on the geometry of the logs, cutting strategies and classification of cuts. This classification enables us to determine the quality and quantity of the production, and to estimate the commercial value of the log. The solution to this problem then led to the design of a <b>software</b> <b>package.</b> Research limitations/implications: In this paper, the optimisation problem concerns problems where the objective function is non-explicit, the variables discreet, and the constraints non-explicit. Practical implications: The solution to this problem then led to the design of a <b>software</b> <b>package</b> to be used as a cutting optimiser. The automation of the cutting operation leads to an accelerated work and an increase in the volume of the cuts produced daily. Originality/value: This research is among the few to solve discrete optimization problems with constraints. Some constraints concerning the mechanical characteristics of the logs are taken into account. The constraints can equally be non-explicit. Moreover the market standards impose technological constraints which render the problem of optimisation even more complex...|$|R
50|$|The {{company was}} founded in 1985 by Roman Ormandy. A {{prototype}} 3D video animation package for the Amiga Computer led to the incorporation of Octree Software in 1986. From 1988 to 1992, Octree released several <b>software</b> <b>packages</b> including Caligari1, Caligari2, Caligari Broadcast, and Caligari 24. Caligari wanted to provide inexpensive yet professional, industrial video and corporate <b>presentation</b> <b>software.</b> In 1993 Octree Software moved from New York to California and became known as Caligari Corporation. In 1994 trueSpace 1.0 was introduced on the Windows platform.In early 2008, the company was acquired by Microsoft and trueSpace 7.6 was released for free.|$|R
40|$|Detection of extra-solar planets {{has been}} a very popular topic with the general public for years. Considerable media {{coverage}} of recent detections (currently at about 50) has only heightened the interest in the topic. School children are particularly interested in learning about recent astronomical discoveries. Scientists have the knowledge and responsibility to present this information in both an understandable and interesting format. Most classrooms and homes are now connected to the internet, which can be utilized to provide more than a traditional 'flat' <b>presentation.</b> An interactive <b>software</b> <b>package</b> on planet detection has been developed. The major topics include: " 1996 - The Break Through Year In Planet Detection"; "What Determines If A Planet Is Habitable?"; "How Can We Find Other Planets (Search Methods) "; "All About the Kepler Mission: How To Find Terrestrial Planets"; and "A Planet Detection Simulator". Using the simulator, the student records simulated observations and then analyzes and interprets the data within the program. One can determine the orbit and planet size, the planet's temperature and surface gravity, and finally determine if the planet is habitable. Originally developed for the Macintosh, a web based browser version is being developed...|$|R
5000|$|A {{number of}} <b>software</b> <b>packages</b> use the SWMM5 engine, {{including}} many commercial <b>software</b> <b>packages.</b> Some of these <b>software</b> <b>packages</b> include: ...|$|R
50|$|There {{are many}} <b>software</b> <b>packages</b> {{available}} to merge text and images into VDP print files. Some are stand-alone <b>software</b> <b>packages,</b> however {{most of the}} advanced VDP <b>software</b> <b>packages</b> are actually plug-in modules for one or more publishing <b>software</b> <b>packages</b> such as Adobe Creative Suite.|$|R
5000|$|<b>Software</b> <b>packaging</b> formats {{are used}} to create <b>software</b> <b>packages</b> that may be self-installing files.|$|R
5000|$|With {{extraction}} lossy data conversion, <b>software</b> <b>packages</b> take content stored by {{a different}} <b>software</b> <b>package</b> and extract out the content to the desired format. This may allow data to be extracted in a format not recognized by the original <b>software</b> <b>package.</b>|$|R
40|$|Numerous <b>software</b> <b>packages</b> {{are being}} used and updated {{regularly}} on most computer systems. Installing all these <b>software</b> <b>packages</b> is a formidable task because each one has a different procedure for compiling or for placing the files required at run time. The LUDE (Logithèque Universitaire Distribuée et Extensible) software library is an organization for installing <b>software</b> <b>packages,</b> a set of tools to install and uninstall <b>software</b> <b>packages</b> and browse their documentation, {{and a number of}} FTP servers offering over 100 pre-installed freely redistributable <b>software</b> <b>packages.</b> It offers functionality and flexibility not available i...|$|R
40|$|This paper {{presents}} an integrated rule-based and case-based reasoning approach for evaluation and {{selection of the}} <b>software</b> <b>packages.</b> Rule-based reasoning is used to (i) store knowledge about the software evaluation criteria (ii) guide user to capture user needs of the <b>software</b> <b>package.</b> Case-based reasoning is used to (i) determine the fit between candidate <b>software</b> <b>packages</b> and user needs of the <b>software</b> <b>package</b> (ii) rank the candidate <b>software</b> <b>packages</b> according to their score. We have implemented this approach and performed usability test to verify functionality, efficiency and convenience of this approach...|$|R
40|$|Teachers using {{traditional}} lecture method {{find it difficult}} to communicate the concept of chemical bonding to students; and students {{find it difficult to}} learn the concept. The trend in the 21 st century learning is the use of computer and <b>software</b> <b>packages</b> to facilitate teaching-learning process. This study set out to develop and validate a <b>software</b> <b>package</b> for teaching chemical bonding in secondary schools. The study produced chemical bonding instructional <b>software</b> <b>package</b> (CBISP), adopting the procedure suggested in FTCESP-model for teacher-made computer educational <b>software</b> <b>package.</b> It also produced an instrument for validation of the <b>software</b> <b>package.</b> The internal consistency (α) of the Chemical Bonding Instructional <b>Software</b> <b>Package</b> (CBISP) has a value of 0. 781, obtained by Kendall's Coefficient of Concordance method used in determining it. The author asserts that the procedure adopted in the development and validation of the CBISP is a veritable way of ensuring sustainable supply of relevant <b>software</b> <b>packages</b> in the school system...|$|R
40|$|Publication of {{this article}} was funded by the Stellenbosch University Open Access Fund. The {{original}} publication is available at [URL] factors exist that may contribute to the unsuccessful completion of application <b>software</b> <b>package</b> implementation projects. The most significant contributor to application <b>software</b> <b>package</b> project failure lies in the misalignment of the organisation’s business processes with the functionality of the application <b>software</b> <b>package.</b> While various IT control frameworks that may assist in the implementation of application <b>software</b> <b>packages</b> are available, the question arises why industry still reports that the success rate of application <b>software</b> <b>package</b> implementation projects remains low. The {{purpose of this study was}} to examine the extent to which the Projects in Controlled Environment (PRINCE 2) framework assists in the alignment of the organisation’s business processes with the functionality provided by the application <b>software</b> <b>package</b> implemented. This study investigated whether PRINCE 2 addresses all the reasons for project failure. It identifies the shortcomings and weaknesses in PRINCE 2 which may contribute to the misalignment between the business processes of the organisation and the functionality of the application <b>software</b> <b>package</b> implemented. The study recommends how these weaknesses identified in PRINCE 2 can be addressed. By taking these recommendations into account when using PRINCE 2 to implement application <b>software</b> <b>packages,</b> proper alignment between the organisation’s business processes and the functionality of the application <b>software</b> <b>package</b> may be achieved. This results in a more successful application <b>software</b> <b>package</b> implementation. Stellenbosch UniversityPublishers' versio...|$|R
40|$|Software defect {{prediction}} {{is the process}} of locating defective modules in software. Software quality may be a field of study and apply that describes the fascinating attributes of <b>software</b> <b>package</b> product. The performance should be excellent with none defects. Software quality metrics are a set of <b>software</b> <b>package</b> metrics that target the standard aspects of the product, process, and project. The <b>software</b> <b>package</b> {{defect prediction}} model helps in early detection of defects and contributes to their economical removal and manufacturing a top quality <b>software</b> <b>package</b> supported many metrics. The most objective of paper is to assist developers determine defects supported existing <b>software</b> <b>package</b> metrics victimization data mining techniques and thereby improve the <b>software</b> <b>package</b> quality. In this paper, role of various classification techniques in software defect prediction process are analyzed...|$|R
40|$|An {{application}} <b>software</b> <b>package</b> implementation is {{a complex}} endeavour, and as such it requires the proper understanding, evaluation and redefining of the current business processes {{to ensure that the}} implementation delivers on the objectives set {{at the start of the}} project. Numerous factors exist that may contribute to the unsuccessful implementation of application <b>software</b> <b>packages.</b> However, the most significant contributor to the failure of an application <b>software</b> <b>package</b> implementation lies in the misalignment of the organisation’s business processes with the functionality of the application <b>software</b> <b>package.</b> Misalignment is attributed to a gap that exists between the business processes of an organisation and what functionality the application <b>software</b> <b>package</b> has to offer to translate the business processes of an organisation into digital form when implementing and configuring an application <b>software</b> <b>package.</b> This gap is commonly referred to as the information technology (IT) gap. This study proposes to define and discuss the IT gap. Furthermore this study will make recommendations for aligning the business processes with the functionality of the application <b>software</b> <b>package</b> (addressing the IT gap). The end result of adopting these recommendations will be more successful application <b>software</b> <b>package</b> implementations...|$|R
25|$|One of the {{elements}} of the package will be the <b>software</b> <b>package.</b> The <b>software</b> <b>package</b> is a package in itself, because it consists of the different software components that together form the product. In contrast with the overall <b>package,</b> the <b>software</b> <b>package</b> is always a technical package in which all the files needed are combined in order to run the software product. Another concept of the <b>software</b> <b>package</b> is the version. This keeps track of the modifications made to the software product. By relating it to the <b>software</b> <b>package</b> the vendor and the customer are able {{to keep track of the}} functionality and properties of the product the customer is using.|$|R
30|$|The {{following}} procedure {{requires a}} computer running Microsoft Windows and our symmetry-mismatch reconstruction <b>software</b> <b>package.</b> Unpacking the <b>software</b> <b>package</b> to a folder and setting {{a path to}} this folder in the Windows system are required before using the <b>software</b> <b>package.</b> This procedure assumes that users have experiences in cryo-EM image processing and single particle reconstruction.|$|R
40|$|A {{statistical}} {{user interface}} is an interface between a human user and a statistical <b>software</b> <b>package.</b> Whenever {{we use a}} statistical <b>software</b> <b>package</b> we want to solve a specific statistical problem. But very often at {{first it is necessary}} to learn specific things about the <b>software</b> <b>package.</b> Everyone of us knows about the ?religious wars? concerning the question which statistical software package/method is the best for a certain task; see Marron (1996) and Cleveland and Loader (1996) and related internet discussions. Experienced statisticians use a bunch of different statistical <b>software</b> <b>packages</b> rather than a single one; although all of the major companies (at least the marketing departments) tell us that we only need their <b>software</b> <b>package.</b> [...] ...|$|R
40|$|This work {{gives an}} {{overview}} of the overall equipment efficiency (OEE) and root cause analysis (RCA) methods. There are summarized parameters necessary to calculate and implement these methods. There has been {{an overview of}} plywood production line gluing process. This work describes overall equipment efficiency and root cause analysis implementation of “Latvijas Finieris” plywood production gluing line, data collection options using Wonderware System Platform, SQL <b>software</b> <b>packages.</b> Data analysis is implemented in several ways: graphs data analyzed with Wonderware Historian <b>software</b> <b>package,</b> logical data processing and calculation implemented through Wonderware System Platform and Wonderware MES <b>software</b> <b>packages,</b> graphical representation is realized with Wonderware Intouch <b>software</b> <b>package.</b> End-user data are available through Wonderware Information Studio <b>software</b> <b>package...</b>|$|R
40|$|Evaluating and {{selecting}} <b>software</b> <b>packages</b> {{that meet}} an organization's requirements {{is a difficult}} software engineering process. Selection of a wrong <b>software</b> <b>package</b> can {{turn out to be}} costly and adversely affect business processes. The aim {{of this paper is to}} provide a basis to improve the process of evaluation and selection of the <b>software</b> <b>packages.</b> This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting <b>software</b> <b>packages,</b> <b>software</b> evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating <b>software</b> <b>packages.</b> The key findings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the <b>software</b> <b>packages,</b> (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection. (C) 200...|$|R
30|$|All {{numerical}} simulations and bifurcation diagrams (both one- and two-parameter) {{are constructed}} using the XPPAUT <b>software</b> <b>package</b> [26], using the Runge-Kutta integration method, and computer codes can be downloaded from the following website: [URL] The surface in Figure 9 was constructed using the AUTO <b>software</b> <b>package</b> [27]. All graphics were produced with the <b>software</b> <b>package</b> MATLAB.|$|R
30|$|This section {{presents}} the hardware design, software design, and choice technologies. The hardware {{is an open}} hardware and the <b>software</b> <b>packages</b> used are FLOSS (Free/Libre/Open Source <b>Software)</b> <b>packages.</b>|$|R
50|$|Dependency hell is a colloquial {{term for}} the {{frustration}} of some software users who have installed <b>software</b> <b>packages</b> which have dependencies on specific versions of other <b>software</b> <b>packages.</b>|$|R
40|$|In surveying, volume calculating is {{commonly}} used. Frequently we use cross-section method. Usually terestrical method {{is used to}} determine characteristic points of crosssections. Observations we make on field need to be processed in <b>software</b> <b>packages</b> in order to calculate volume of an object. The calculated volumes may {{vary depending on the}} density of measured cross-sections and selection of <b>software</b> <b>package.</b> In this thesis, we compared the course of processing in <b>software</b> <b>packages</b> and volume differences, which we obtained using different <b>software</b> <b>packages</b> and different density of measured cross-sections...|$|R
5000|$|A large {{proportion}} of free statistical <b>software</b> <b>packages,</b> however, are from individuals. Some of these <b>software</b> <b>packages</b> from individuals include Easyreg, MicrOsiris, [...] OpenStat, PSPP, SOFA, Zelig. and DataMelt ...|$|R
5000|$|Deltek Costpoint is {{a project}} {{accounting}} <b>software</b> <b>package</b> sold by Deltek. Costpoint is the [...] "high-end" [...] of the three main project accounting <b>software</b> <b>packages</b> sold by the company.|$|R
40|$|Abstract — MATLAB is {{at present}} {{among the best}} {{available}} technique for image processing. Medical images after digitalized processed can help {{reducing the number of}} false positives and they assist medical officers in deciding between follow-up and biopsy. This paper gives a survey of image processing algorithms that have been developed for detection of masses and segmentation techniques. 35 students from university campus participated in the Development of Biomedical Image Processing <b>Software</b> <b>Package</b> for New Learners Survey investigating the use of <b>software</b> <b>package</b> for processing and editing image. Composed of 19 questions, the survey built a comprehensive picture of the <b>software</b> <b>package,</b> programming language, workflow of the tool and captured the attitudes of the respondents. The result of this study shows that MATLAB is among the famous <b>software</b> <b>package</b> and this result is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed <b>software</b> <b>package.</b> Keywords- MATLAB; image processing; image editting; <b>software</b> <b>package.</b> 1...|$|R
40|$|The {{purpose of}} the study was to assess the {{reliability}} of (semi-) automatic left ventricular (LV) function measurements using three different <b>software</b> <b>packages</b> on the same dual-source computed tomography (DSCT) datasets and to compare agreement among the <b>software</b> <b>packages.</b> Forty consecutive patients, undergoing cardiac DSCT were included (31 men, mean age 58 +/- 14 years). LV function analysis was performed with all three <b>software</b> <b>packages.</b> ANOVA testing was used to determine the difference among the repeated measurements and the difference among the <b>software</b> <b>packages.</b> Bland-Altman plots were computed to describe the agreement among the <b>software</b> <b>packages.</b> No significant difference was found among the repeated measurements. In the comparison of the three <b>software</b> <b>packages,</b> a significant difference was observed when measurements were used with minimal user interaction. When end-diastolic and end-systolic phases were manually set, there was no overall significant difference, but in 12. 5 % of patients a large (> 10 %) difference in LVEF was found. All three <b>software</b> <b>packages</b> have good intraobserver variability, but the results of the three packages were significantly different. For clinical use, one should be aware of the clinical impact of possible segmentation flaws when (semi-) automatic LV function assessment is used...|$|R
5000|$|... eXpressDSP is a <b>software</b> <b>package</b> {{produced}} by Texas Instruments. This <b>software</b> <b>package</b> is {{a suite of}} tools used to develop applications on Texas Instruments Digital Signal Processing line of chips.|$|R
30|$|Statistically {{significant}} differences in SUV measurements, between the two <b>software</b> <b>packages,</b> ranging from 9 to 21.8 % were found depending on ROI location. In 79 % (n[*]=[*] 23) of the ROI locations, {{the differences between the}} SUV measurements from each <b>software</b> <b>package</b> were found to be statistically significant. The time taken to perform the analyses and export data from the <b>software</b> <b>packages</b> also varied considerably.|$|R
