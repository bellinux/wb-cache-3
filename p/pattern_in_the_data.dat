109|10000|Public
2500|$|Hyman also {{reviewed}} the autoganzfeld experiments and discovered a <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> that implied a visual cue {{may have taken}} place: ...|$|E
2500|$|A 1989 {{study found}} {{a very high}} number of {{partners}} (over 100) to be present though rare among homosexual males. General Social Survey data indicates that the distribution of partner numbers among {{men who have sex}} exclusively with men and men who have sex exclusively with women is similar, but that differences appear in the proportion of those with very high number of partners, which is larger among gay men, but that in any case makes up a small minority for both groups. OkCupid discovered a similar <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> collected from its vast number of users, published in 2010: the median number of self-reported lifetime sexual partners for both gay and straight men was six; however, a small minority of gay men (2%) were having a disproportionate share of all self-reported gay sex (23%). According to updated OkCupid data published in 2014, gay male users self-reported a lower median of lifetime sex partners than straight male users: four for gay men and five for straight men. A 2007 study reported that two large population surveys found [...] "the majority of gay men had similar numbers of unprotected sexual partners annually as straight men and women." ...|$|E
5000|$|Hyman also {{reviewed}} the autoganzfeld experiments and discovered a <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> that implied a visual cue {{may have taken}} place: ...|$|E
5000|$|... #Caption: Columns {{and rows}} are moved around until a {{diagonal}} pattern appears, thereby {{making it easy}} to see <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b>|$|R
50|$|The {{techniques}} outlined {{have been}} designed to help visualise and explore <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> <b>in</b> order to facilitate the revelation of these three features.|$|R
40|$|Data mining {{has great}} {{potential}} <b>in</b> <b>the</b> fields of tea cultivation and tea industry of Assam for exploring <b>the</b> hidden <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> sets of <b>the</b> domain. These patterns can be utilized for tea cultivation analysis. However, <b>the</b> available raw <b>data</b> are widely distributed, heterogeneous in nature, and voluminous. These data {{need to be}} collected in an organized form. This collected data can be then integrated to form an information system. Data mining technology provides a user-oriented approach to novel and hidden <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b> Data mining and statistics both strive towards discovering <b>patterns</b> and structures <b>in</b> data. Statistics deals with heterogeneous numbers only,where data mining deals with heterogeneous fields...|$|R
50|$|If all {{the bits}} are 1, then people infer {{that there is}} a bias in the coin and that it is more likely also that the next bit is 1 also. This is {{described}} as learning from, or detecting a <b>pattern</b> <b>in</b> <b>the</b> <b>data.</b>|$|E
50|$|Approaches towards {{clustering}} in axis-parallel or arbitrarily oriented affine subspaces {{differ in}} how they interpret the overall goal, which is finding clusters in data with high dimensionality. An overall different approach is to find clusters based on <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> matrix, {{often referred to as}} biclustering, which is a technique frequently utilized in bioinformatics.|$|E
50|$|Overfitting {{is where}} the model matches the random noise and not the <b>pattern</b> <b>in</b> <b>the</b> <b>data.</b> For example, take the {{situation}} where a curve is fitted {{to a set of}} points. If polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviances from the fitted curve will be smaller. Smaller information length means more probable.|$|E
5000|$|The matrix can {{represent}} {{a large number}} of system elements and their relationships in a compact way that highlights important <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> (such as feedback loops and modules).|$|R
50|$|Ray Solomonoff {{developed}} algorithmic probability {{which gave}} {{an explanation for}} what randomness is and how <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> may be represented by computer programs, that give shorter representations of <b>the</b> <b>data</b> circa 1964.|$|R
50|$|Diagonalization is {{the process}} of re-ordering the rows and columns of tables and charts so that <b>the</b> <b>data</b> forms an {{approximately}} diagonal line. This makes it easier for people to see <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b>|$|R
50|$|Possibilities {{of sensory}} leakage in the Ganzfeld {{experiments}} included the receivers hearing {{what was going}} on in the sender's room next door as the rooms were not soundproof and the sender's fingerprints to be visible on the target object for the receiver to see. Hyman reviewed the autoganzfeld experiments and discovered a <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> that implied a visual cue may have taken place. Hyman wrote the autoganzfeld experiments were flawed because they did not preclude the possibility of sensory leakage.|$|E
5000|$|Similarly, bit {{stuffing}} replaces these start and end marks with flag {{consisting of a}} special bit pattern (e.g. a 0, six 1 bits and a 0). Occurrences of this bit <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> to be transmitted are avoided by inserting a bit. To use the example where the flag is 01111110, a 0 is inserted after 5 consecutive 1's in the data stream. The flags and the inserted 0's are removed at the receiving end. This makes for arbitrary long frames and easy synchronization for the recipient. Note that this stuffed bit is added even if the following data bit is 0, {{which could not be}} mistaken for a sync sequence, so that the receiver can unambiguously distinguish stuffed bits from normal bits.|$|E
5000|$|Functional {{programming}} data structures {{often use}} polymorphic recursion to simplify type error checks and solve problems with nasty [...] "middle" [...] temporary solutions that devour memory in more traditional data {{structures such as}} trees. In the two citations that follow, Okasaki (pp. 144-146) gives a CONS example in Haskell wherein the polymorphic type system automatically flags programmer errors. The recursive aspect is that the type definition assures that the outermost constructor has a single element, the second a pair, the third a pair of pairs, etc. recursively, setting an automatic error finding <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> type. Roberts (p. 171) gives a related example in Java, using a Class to represent a stack frame. The example given is {{a solution to the}} Tower of Hanoi problem wherein a stack simulates polymorphic recursion with a beginning, temporary and ending nested stack substitution structure.|$|E
40|$|We {{propose a}} {{discussion}} index model (Stock and Watson, 2002) to fore- cast electricity demand {{for one hour}} to one week ahead. The model is particularly useful as it captures complicated seasonal <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b> <b>The</b> forecast performance of the proposed method is illustrated with a simulated real-time experiment for <b>data</b> from <b>the</b> Pennsylvania- New Jersey-Maryland Interchange...|$|R
40|$|Medical {{data mining}} has great {{potential}} for exploring <b>the</b> hidden <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> sets of <b>the</b> medical domain. These patterns can be utilized for clinical diagnosis. However, the available raw medical data are widely distributed, heterogeneous in nature, and voluminous. These data need to be collected in an organized form. This collected data can be then integrated to form a hospital information system. Data mining technology provides a user-oriented approach to novel and hidden <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b> Data mining and statistics both strive towards discovering <b>patterns</b> and structures <b>in</b> data. Statistics deals with heterogeneous numbers only, whereas data mining deals with heterogeneous fields. We identify a few areas of healthcare where these techniques {{can be applied to}} healthcare databases for knowledge discovery. In this paper we briefly examine <b>the</b> impact of <b>data</b> mining techniques, including artificial neural networks, on medical diagnostics...|$|R
50|$|DCIM {{providers}} are increasingly linking with {{computational fluid dynamics}} providers to predict complex airflow <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> center. <b>The</b> CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.|$|R
5000|$|Testing a {{hypothesis}} {{suggested by the}} data can very easily result in false positives (type I errors). If one looks long enough and in enough different places, eventually data can be found to support any hypothesis. Yet, these positive data do not by themselves constitute evidence that the hypothesis is correct. The negative test data that were thrown out are just as important, because they give one {{an idea of how}} common the positive results are compared to chance. Running an experiment, seeing a <b>pattern</b> <b>in</b> <b>the</b> <b>data,</b> proposing {{a hypothesis}} from that pattern, then using the same experimental data as evidence for the new hypothesis is extremely suspect, because data from all other experiments, completed or potential, has essentially been [...] "thrown out" [...] by choosing to look only at the experiments that suggested the new hypothesis in the first place.|$|E
50|$|In step three an {{existing}} clustering algorithm {{is used to}} cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their CF vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution <b>pattern</b> <b>in</b> <b>the</b> <b>data.</b> However there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain {{a new set of}} clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.|$|E
5000|$|In {{the design}} and {{analysis}} of experiments, post hoc analysis (from Latin post hoc, [...] "after this") consists {{of looking at the}} data—after the experiment has concluded—for patterns that were not specified a priori. It is sometimes called data dredging by critics to evoke the sense that the more one looks the more likely something will be found. More subtly, each time a <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> is considered, a statistical test is effectively performed. This greatly inflates the total number of statistical tests and necessitates the use of multiple testing procedures to compensate. However, this is difficult to do precisely and in fact most results of post hoc analyses are reported as they are with unadjusted p-values. These p-values must be interpreted {{in light of the fact}} that they are a small and selected subset of a potentially large group of p-values. Results of post hoc analyses should be explicitly labeled as such in reports and publications to avoid misleading readers.|$|E
5000|$|PNDA is a {{platform}} for scalable network analytics, rounding up data from [...] "multiple sources on a network and works with Apache Spark to crunch <b>the</b> numbers <b>in</b> order to find useful <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> more effectively." ...|$|R
50|$|Rule {{induction}} {{is an area}} {{of machine}} learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of <b>the</b> <b>data,</b> or merely represent local <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b>|$|R
40|$|A {{methodology}} for {{the determination of}} wafer temperature in Molecular Beam Epitaxy via diffuse reflectance measurements is developed. Approximate physical principles are not used, instead, <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> (reflectance versus wavelength) are exploited via wavelet decomposition and Principal Component Analysis...|$|R
5000|$|A 1989 {{study found}} {{a very high}} number of {{partners}} (over 100) to be present though rare among homosexual males. General Social Survey data indicates that the distribution of partner numbers among {{men who have sex}} exclusively with men and men who have sex exclusively with women is similar, but that differences appear in the proportion of those with very high number of partners, which is larger among gay men, but that in any case makes up a small minority for both groups. OkCupid discovered a similar <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> collected from its vast number of users, published in 2010: the median number of self-reported lifetime sexual partners for both gay and straight men was six; however, a small minority of gay men (2%) were having a disproportionate share of all self-reported gay sex (23%). According to updated OkCupid data published in 2014, gay male users self-reported a lower median of lifetime sex partners than straight male users: four for gay men and five for straight men. A 2007 study reported that two large population surveys found [...] "the majority of gay men had similar numbers of unprotected sexual partners annually as straight men and women." ...|$|E
40|$|Neural {{networks}} {{have the ability}} to recognize the hidden <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> and accordingly estimate the output values. Provision of model-free solutions, data error tolerance, built in dynamism and lack of any exogenous input requirement makes the neural network attractive. A neural network is an information processing system modeled on th...|$|E
30|$|PCA can {{be applied}} to rule out outliers, to reduce our dataset (which can ease our work greatly in cases of big, complex datasets) and to build models that {{describe}} the behavior of a physical or chemical system and reveal any <b>pattern</b> <b>in</b> <b>the</b> <b>data.</b> The models can be used for predictions when we introduce new data (new samples measured in the same way).|$|E
25|$|Phenomenological models: distill the {{functional}} and distributional shapes from observed <b>patterns</b> <b>in</b> <b>the</b> <b>data,</b> or researchers decide on functions and distribution that are {{flexible enough to}} match the patterns they or others (field or experimental ecologists) have found <b>in</b> <b>the</b> field or through experimentation.|$|R
50|$|Kendall and Smith's {{original}} four {{tests were}} hypothesis tests, which took as their null hypothesis {{the idea that}} each number in a given random sequence had an equal chance of occurring, and that various other <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> should be also distributed equiprobably.|$|R
30|$|Stage I - (Linear and {{nonlinear}} modeling): Similar to {{the previous}} section, to capture the linear and nonlinear <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> for <b>the</b> S&P time series, the ARIMA (1,[*] 0,[*] 0) and MLP models with three input, three hidden, and one output neuron are designed.|$|R
30|$|We {{observe that}} in both {{networks}} clustering decreases with age for young users, then {{it starts to}} increase again. This general trend is not aligned with the one expected according to the null model indicating a marked age <b>pattern</b> <b>in</b> <b>the</b> <b>data.</b> The strongest deviations are found for young teenagers, whose tendency to form dense groups is much larger than expected, and for users over 20, who on the contrary exhibit sparse relationships.|$|E
30|$|The {{results show}} that a larger portion of the {{high-level}} term is required to get correct classification {{when there is a}} complex-formed and well-defined <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> set. In this case, we also show that traditional classification algorithms are unable to identify those data patterns. Moreover, computer simulations on real-world data sets show that HL-KAOG and support vector machines provide similar results and they outperform well-known techniques, such as decision trees and K-nearest neighbors.|$|E
40|$|We {{describe}} hashing of {{data bases}} {{as a problem}} of information and coding theory. It is shown that the triangle inequality for the Hamming distances between binary vectors may essentially decrease the computational efforts of {{a search for a}} <b>pattern</b> <b>in</b> <b>the</b> <b>data</b> base. Introduction of the Lee distance in the space, which consists of the Hamming distances, leads to a new metric space where the triangle inequality can be effectively used...|$|E
30|$|The {{method of}} Pattern Matching (PM) or Pattern Imitation [8] {{presented}} in this paper can be ascribed to local modeling; it avoids the assumptions normally required <b>in</b> <b>the</b> context of time-series modeling (e.g. stationarity and/or Gaussian condition) and is based on the imitation of <b>the</b> past <b>patterns</b> <b>in</b> <b>the</b> <b>data</b> history.|$|R
40|$|Econometric Methodology {{is based}} on logical positivist principles. Since logical {{positivism}} has collapsed, {{it is necessary to}} re-think these foundations. We show that positivist methodology has led econometricians to a meaningless search for <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b> An alternative methodology which relates observed patterns to real causal structures is proposed...|$|R
50|$|Time-to-event studies {{must have}} {{sufficiently}} long follow-up durations to capture enough events to reveal meaningful <b>patterns</b> <b>in</b> <b>the</b> <b>data.</b> A short follow-up duration {{is appropriate for}} studying very severe cancers with poor prognoses, whereas a long follow-up duration is better suited to studying less-severe disease, or participants with good prognoses.|$|R
