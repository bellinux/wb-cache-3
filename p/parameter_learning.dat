618|1804|Public
50|$|Obviously the <b>parameter</b> <b>learning</b> {{for simple}} {{distributions}} {{is a very}} well studied field that is called statistical estimation {{and there is a}} very long bibliography on different estimators for different kinds of simple known distributions. But distributions learning theory deals with learning class of distributions that have more complicated description.|$|E
5000|$|In some {{settings}} {{the class}} of distributions [...] is a class with well known distributions which can be described by set a set of parameters. For instance [...] could be {{the class of}} all the Gaussian distributions [...] In this case the algorithm [...] {{should be able to}} estimate the parameters [...] In this case [...] is called <b>parameter</b> <b>learning</b> algorithm.|$|E
50|$|The <b>parameter</b> <b>learning</b> task in HMMs is to find, {{given an}} output {{sequence}} or {{a set of}} such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of {{the parameters of the}} HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum-Welch algorithm or the Baldi-Chauvin algorithm. The Baum-Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability. Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g. Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.|$|E
3000|$|..., the <b>parameters</b> <b>learned</b> {{separately}} by {{two different}} algorithms and two different objective functions.|$|R
3000|$|... b, the {{proposed}} method and “without ANNS” used the <b>parameters</b> <b>learned</b> with the 482 subjects. Figure 3 [...]...|$|R
30|$|Artificial {{neural network}} {{is capable of}} {{relating}} the input and output <b>parameters,</b> <b>learning</b> from examples through iteration [54]. The main edge is its nonlinearity and adaptability, providing accurate prediction with uncertain data [55].|$|R
40|$|Bayesian network {{classifiers}} {{have been}} widely used for classification problems. Given a fixed Bayesian network structure, parameters learning can take two different approaches: generative and discriminative learning. While generative <b>parameter</b> <b>learning</b> is more efficient, discriminative <b>parameter</b> <b>learning</b> is more effective. In this paper, we propose a simple, efficient, and effective discriminative <b>parameter</b> <b>learning</b> method, called Discriminative Frequency Estimate (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art discriminative <b>parameter</b> <b>learning</b> method ELR in accuracy, but is significantly more efficient. 1...|$|E
30|$|In this work, {{the actual}} <b>parameter</b> <b>learning</b> is {{performed}} with the widely used least squares (LS) estimation. For brevity, the <b>parameter</b> <b>learning</b> and digital cancellation procedure is here outlined only for the ith receiver, since the procedure is identical for all the receivers.|$|E
30|$|Gaussian mixture model (GMM) {{has been}} widely used in {{different}} areas, e.g., clustering, image segmentation (Zhang et al. [2001]), speaker identification (Reynolds [1995]), document classification (Nigam et al. [2000]), market analysis (Chiu and Xu [2001]), etc. Learning a GMM consists of <b>parameter</b> <b>learning</b> for estimating all unknown parameters and model selection for determining the number of Gaussian components k. <b>Parameter</b> <b>learning</b> is usually implemented under the maximum likelihood principle by an expectation-maximization (EM) algorithm (Redner and Walker [1984]). A conventional model selection approach is featured by a two-stage implementation, which suffers from a huge computation because it requires <b>parameter</b> <b>learning</b> for each candidate GMM. Moreover, <b>parameter</b> <b>learning</b> will become less reliable as k becomes larger, which implies more free parameters.|$|E
50|$|Together, these {{properties}} allow CNNs {{to achieve}} better generalization on vision problems. Weight sharing dramatically reduces {{the number of}} free <b>parameters</b> <b>learned,</b> thus lowering the memory requirements for running the network. Decreasing the memory footprint allows the training of larger, more powerful networks.|$|R
3000|$|... : These methods {{differ from}} {{unsupervised}} methods {{due to the}} fact that they label the classes to be discriminated. Unlike unsupervised methods, there are two important phases. The ‘training phase’ is considered as a passive modeling stage, which uses a ‘training data set’ (which is labeled) to find the patterns in the data. The model <b>parameters</b> <b>learned</b> during the training phase are stored for further validation. The second phase, called the ‘prediction phase’ (testing phase) is the active stage where the unseen data (the data which were not part of the training set) are validated using the model <b>parameters</b> <b>learned</b> in the first phase, using for instance Discriminant Analysis (DA), Multiple Linear Regression (MLR), Principal Component Regression (PCR), Partial Least Squares (PLS), Support Vector Machines (SVM).|$|R
50|$|Their <b>parameters</b> are <b>learned</b> {{using the}} EM algorithm.|$|R
40|$|We {{review the}} {{challenges}} of Bayesian network learning, especially <b>parameter</b> <b>learning,</b> and specify the problem of learning with sparse data. We explain {{how it is possible}} to incorporate both qualitative knowledge and data with a multinomial <b>parameter</b> <b>learning</b> method to achieve more accurate predictions with sparse data...|$|E
40|$|Abstract. We {{investigate}} {{maximum likelihood}} <b>parameter</b> <b>learning</b> in Conditional Random Fields (CRF) and present an empirical study of pseudo-likelihood (PL) based approximations of the parameter likelihood gradient. We show, {{as opposed to}} [1][2], that these <b>parameter</b> <b>learning</b> methods can be improved and evaluate the resulting performance employing different inference techniques. We show that the approximation based on penalized pseudo-likelihood (PPL) {{in combination with the}} Maximum A Posteriori (MAP) inference yields results comparable to other state of the art approaches, while providing low complexity and advantages to formulating <b>parameter</b> <b>learning</b> as a convex optimization problem. Eventually, we demonstrate applicability on the task of detecting man-made structures in natural images...|$|E
40|$|We {{present an}} {{efficient}} method for statistical <b>parameter</b> <b>learning</b> {{of a certain}} class of symbolic-statistical models (called PRISM programs) including hidden Markov models (HMMs). To learn the parameters, we adopt the EM algorithm, an iterative method for maximum likelihood estimation. For the efficient <b>parameter</b> <b>learning,</b> we first introduce a specialized data structure for explanations for each observation, and then apply a graph-based EM algorithm...|$|E
40|$|<b>Learned</b> <b>parameters</b> and {{resulting}} segmentation {{corresponding to the}} analyses shown in the Segway 2. 0 application note. Directory structure: GMM (datasets corresponding to the mixture of Gaussians analysis) 	 1 -component 	 		traindir/ 		 			log/ (training log likelihood progression) 			params/ (<b>learned</b> <b>parameters)</b> 		 		 		identifydir/ 		 			segway. bed. gz (segmentation) 		 		 	 	 	 3 -component 	 		traindir/ 		 			log/ (training log likelihood progression) 			params/ (<b>learned</b> <b>parameters)</b> 		 		 		identifydir/ 		 			segway. bed. gz (segmentation) 		 		 	 	 minibatch-fixed (datasets corresponding to the minibatch learning analysis) 	fixed/ 	 		traindir/ 		 			log/ (training and validation log likelihood progression) 			params/ (<b>learned</b> <b>parameters)</b> 		 		 	 	 	minibatch/ 	 		traindir/ 		 			log/ (training and validation log likelihood progression) 			params/ (<b>learned</b> <b>parameters)</b> 		 		 	 	 TSS_prediction (datasets corresponding to the TSS prediction analysis) (where k=component number= 1 - 5, n=random start number= 1 - 10) 	outputs_[date]_k/ 	 		traindir/ 		 			log/ (training and validation log likelihood progression) 			params/ (<b>learned</b> <b>parameters)</b> 		 		 		identifydir_n/ 		 			segway. bed. gz (segmentation...|$|R
40|$|This Bachelor’s thesis {{deals with}} {{self-organizing}} networks and its learning mechanism. The activation, adaptation {{and application of}} Kohonen network are discussed in this thesis. The program Kohonen neural network is described. The practical part of this work analyzes effect of <b>learning</b> <b>parameters</b> choice on final state of Kohonen network and how do this <b>learning</b> <b>parameters</b> affect <b>learning</b> process. The effect of weight vector initialization on the final best-matching neuron “position” is analyzed...|$|R
3000|$|... {{tasks of}} target c {{is assumed to}} have an {{independent}} state-transition statistics, but the state-dependent observation statistics are shared across these tasks, i.e., the observation <b>parameters</b> are <b>learned</b> via all aspect-frames; while in the STL TSB-HMM, each multi-aspect frame of target c is learned separately, therefore, each target-aspect frame builds its own model and the corresponding <b>parameters</b> are <b>learned</b> just via this aspect-frame.|$|R
40|$|In this work, we {{investigated}} {{the potential of}} a recently proposed <b>parameter</b> <b>learning</b> algorithm for Conditional Random Fields (CRFs). Parameters of a pairwise CRF are estimated via a stochastic subgradient descent of a max-margin learning problem. We compared the performance of our brain tumor segmentation method using <b>parameter</b> <b>learning</b> to a version using hand-tuned parameters. Preliminary results on {{a subset of the}} BRATS 2015 training set show that <b>parameter</b> <b>learning</b> leads to comparable or even improved performance. In addition, we also performed experiments to study the impact of the composition of training data on the final segmentation performance. We found that models trained on mixed data sets achieve reasonable performance compared to models trained on stratified data...|$|E
40|$|In this report, {{we address}} the problem of <b>parameter</b> <b>learning</b> for belief {{networks}} with fixed structure based on empirical observations. Both complete and incomplete (data) observations are included. Given complete data, we describe the simple problem of single <b>parameter</b> <b>learning</b> for intuition and then expand to belief networks under appropriate system decomposition. If the observations are incomplete, we first estimate the "missing" observations and treat them as though they are "real" observations, based on which the <b>parameter</b> <b>learning</b> can be executed as in complete data case. We derive a uniform algorithm based on this idea for incomplete data case and present the convergence and optimality properties. Such an algorithm is suitable trivially under complete observations...|$|E
40|$|ProbLog is a state-of-art {{combination}} of logic programming and probabilities; in particular ProbLog offers <b>parameter</b> <b>learning</b> through {{a variant of}} the EM algorithm. However, the resulting learning algorithm is rather slow, even when the data are complete. In this short paper we offer some insights that lead to orders of magnitude improvements in ProbLog's <b>parameter</b> <b>learning</b> speed with complete data. Comment: StarAI - International Workshop on Statistical Relational A...|$|E
50|$|The {{original}} RW model cannot {{account for}} this effect. But the revised model can: In Phase 2, stimulus B is indirectly activated through within-compound-association with A. But instead of a positive <b>learning</b> <b>parameter</b> (usually called alpha) when physically present, during Phase 2, B has a negative <b>learning</b> <b>parameter.</b> Thus during the second phase, B's associative strength declines whereas A's value increases because of its positive <b>learning</b> <b>parameter.</b>|$|R
40|$|In {{this paper}} we propose an {{artificial}} immune system in which learning automata are used to adaptively determine the values of its <b>parameters.</b> <b>Learning</b> automata are used for altering the shape of receptor portion of antibodies to better complementarily match the confronted antigen. In order to show {{the effectiveness of the}} proposed artificial immune computer experiments have been conducted. The result of experimentations confirms the effectiveness of the proposed model. 1...|$|R
40|$|The non-stationary {{nature of}} image {{characteristics}} calls for adaptive processing, {{based on the}} local image content. We propose a simple and flexible method to learn local tuning of parameters in adaptive image processing: we extract simple local features from an image and learn the relation between these features and the optimal filtering <b>parameters.</b> <b>Learning</b> is performed by optimizing a user defined cost function (any image quality metric) on a training set. We apply our method to three classical problems (denoising, demosaicing and deblurring) and we show {{the effectiveness of the}} <b>learned</b> <b>parameter</b> modulation strategies. We also show that these strategies are consistent with theoretical results from the literature...|$|R
30|$|To {{verify the}} {{validity}} of the CPT retrieval algorithm, we undertake a comparative analysis with another method for <b>parameter</b> <b>learning.</b>|$|E
3000|$|However, a {{two-stage}} implementation {{suffers from}} a huge computation because it requires <b>parameter</b> <b>learning</b> for each candidate. Also, estimating θ [...]...|$|E
3000|$|... 2 = 1.7 are empirically set coefficients. A more principled {{way would}} be to obtain these {{parameters}} through a <b>parameter</b> <b>learning</b> method.|$|E
30|$|Cai et al. [8] {{proposed}} a combined hierarchical reinforcement learning method for multi-robot cooperation in completely unknown environments. This method {{is a result}} of the integration of options with the MAXQ hierarchical reinforcement learning method. The MAXQ method is used to identify the problem hierarchy. The proposed method obtains all the required <b>learning</b> <b>parameters</b> through <b>learning</b> without any need for an explicit environment model. The cooperation strategy is then built based on the <b>learned</b> <b>parameters.</b> In this method, multiple simulations are required to build the problem hierarchy which is a time consuming process.|$|R
40|$|In {{this work}} we present {{research}} on three topics which {{have implications for}} future robotic applications. Couched in learning from human provided examples, we study how robots can demonstrate learning curves akin to those observed for human students. Specifically we show how the <b>parameters</b> of robot <b>learning</b> curves relate to those <b>parameters</b> from <b>learning</b> curves gen-erated by human students. Next we show how these <b>parameters</b> and <b>learning</b> process they represent are af-fected {{by the quality of}} instruction provided. Finally, we present a method to generate an estimate of the robot learning curve. This method is of merit since it is based on properties of interaction that can be extracted as learning occurs...|$|R
3000|$|... } is {{the initial}} state {{probability}} vector [33]. The <b>parameters</b> are <b>learned</b> from training data using Baum-Welch method. This {{is done for}} each class separately.|$|R
40|$|In {{this paper}} we develop a general simulation-based {{approach}} to filtering and sequential <b>parameter</b> <b>learning.</b> We {{begin with an}} algorithm for filtering in a general dynamic state space model and then extend this to incorporate sequential <b>parameter</b> <b>learning.</b> The key idea is to express the filtering distribution as a mixture of lag-smoothing distributions and to implement this sequentially. Our approach {{has a number of}} advantages over current methodologies. First, it allows for sequential parmeter learning where sequential importance sampling approaches have difficulties. Secon...|$|E
30|$|In PPNews, the {{structure}} of BN is fixed and the data is complete. The news BNs are revised based on <b>parameter</b> <b>learning</b> [49] in super peers.|$|E
40|$|Abstract—Aiming at the incompleteness and {{uncertainty}} of information existing in power system fault diagnosis, a new fault diagnosis approach based on Bayesian network is proposed in this paper. Through the Bayesian network of structure learning and <b>parameter</b> <b>learning,</b> a power system fault diagnosis model based on Bayesian network has been proposed. Conditional probability table describes the connection degree between various factors in quantity. Diagnostic results of instance proved the effectiveness and {{superiority of the}} proposed method. Keywords-power system; fault diagnosis; Bayesian network; structure learning; <b>parameter</b> <b>learning.</b> I...|$|E
40|$|AbstractAn {{artificial}} {{neural network}} (ANN) based current controller for a high voltage direct current (HVDC) transmission link, composed of an ANN trained off-line in parallel with a robust PI controller, is described in this paper. Different ANN architectures are investigated for this ANN controller. Comparisons between the responses obtained with the PI and ANN controllers for the rectifier of a HVDC transmission system are made for various system ANN <b>parameters</b> (<b>learning</b> rate and momentum term) contingencies and it is shown that the later has many attractive features...|$|R
30|$|Here λ is the <b>learning</b> <b>parameter</b> {{which means}} that small values of λ enable the player to choose the optimal {{strategy}} more accurately. For our model, we define λ= 1 -β, where β is the <b>learning</b> <b>parameter</b> used for the game model of SICA (see Section 3).|$|R
40|$|Abstract: We {{developed}} a pedestrian classifier using GFB(Gabor Filter Bank) -based feature extraction and SVM(Support Vector Machine). Because the SVM uses RBF(Radial Basis Function) and is applied for nonseparable data, <b>learning</b> <b>parameters</b> should be optimized. This paper proposes GA(Ganetic Algorithm) -based optimization of SVM <b>learning</b> <b>parameters.</b> 1...|$|R
