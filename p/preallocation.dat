33|1|Public
2500|$|... do {{not exceed}} 80 characters. This {{probably}} was {{to allow for}} <b>preallocation</b> of fixed line sizes in software: at the time most users relied on DEC VT (or compatible) terminals which could display 80 or 132 characters per line. Most people preferred the bigger font in 80-character modes and so it became the recommended fashion to use 80 characters or less (often 70) ...|$|E
50|$|A {{sequence}} in FASTA format is {{represented as a}} series of lines, each of which should be no longer than 120 characters and usuallydo not exceed 80 characters. This probably was to allow for <b>preallocation</b> of fixed line sizes in software: at the time most users relied on DEC VT (or compatible) terminals which could display 80 or 132 characters per line. Most people preferred the bigger font in 80-character modes and so it became the recommended fashion to use 80 characters or less (often 70)in FASTA lines. Also, the width of a standard printed page is 70 to 80 characters (depending on the font).|$|E
30|$|The first {{improvement}} is the block <b>preallocation</b> cache, which is hinted by the <b>preallocation</b> feature of Ext 2. Ext 2 preallocates blocks {{to reduce the}} overhead to search a clear bit in the free block bitmap that manages free blocks. It uses a preallocated block for the subsequent block allocation, and avoids searching a free block for each block request. If bitwise operations can be totally avoided, the overhead can be further reduced.|$|E
30|$|Cloud {{computing}} provides IT {{services to}} clients {{without reference to}} the infrastructure that hosts the services [6]. The cloud is a generic platform for different business types, from small-scale to very-large scale. The upfront cost is minimal as the infrastructure is provided by the cloud operator, who rents resources to cloud clients. The elasticity of clouds permits business grows without long-term planning and resource <b>preallocations.</b> A pay-per-use business model on top of a virtualized computing resource pool enables resource sharing and on-demand resource provisioning. Computing resources (hardware and software) can then be dynamically allocated and efficiently used, ensuring faster amortization (CAPEX) and better scalability as well as savings in power consumption, security, maintenance and software licensing, among others (OPEX).|$|R
30|$|One of {{the reasons}} why Ext 2 {{performs}} better for block allocation than PRAMFS can be because of its block <b>preallocation</b> feature. When a new block is allocated, Ext 2 internally preallocates several blocks adjacent to the block just allocated. Such <b>preallocation</b> helps to make the blocks of a file more contiguous and also to make the subsequent block allocation much more efficient; thus, Ext 2 can perform better for block allocation than PRAMFS.|$|E
40|$|This paper {{presents}} a token-based channel access protocol for wavelength division multiplexed optically interconnected multiprocessors. Our empirical study of access protocols based on slotted time division multiplexed data and control channels reveals that such protocols typically suffer from excessive slot synchronizationlatency due to static slot <b>preallocation.</b> The proposed token-based time division multipleaccess protocol minimizes latency by allowing dynamic allocation of slots to use channels efficiently. Simulation {{results indicate that}} the proposed scheme can significantly increase the performance of protocols based on <b>preallocation</b> and those based on preallocation-controlled reservation of multiple channels. ...|$|E
30|$|The block <b>preallocation</b> cache is a {{linked list}} of the free blocks preallocated from a file system. In {{response}} to a block allocation request, a block is taken out from the linked list instead of searching a clear bit in the free block bitmap. When the list is empty, 64 blocks are allocated together from a file system and added to the list. Searching 64 free blocks in the bitmap can avoid bitwise operations. Since our target CPU is x 86 _ 64, its word size is 64 -bit. By considering the bitmap as an array of 64 -bit elements, searching 64 free blocks is simply searching an array element of which content is 0. When a block is freed, it is returned to the block <b>preallocation</b> cache. If there is an enough number of blocks in the cache, the freed block is returned to a file system.|$|E
40|$|Different {{companies}} {{sharing the}} same cloud infrastructure often prefer to run their virtual machines (VMs) in isolation, i. e., one VM per physical machine (PM) core, due to security and efficiency concerns. To accommodate load spikes, e. g., those caused by flash-crowds, each service is allocated more machines than necessary for its instantaneous load. However, flash-crowds of different hosted services are not correlated, so at any given time, only {{a subset of the}} machines are used. We present here the concept of <b>preallocation</b> — having a single physical machine ready to quickly run one of a few possible VMs, without ever running more than one at a given time. The preallocated VMs are initialized and then paused by the hypervisor. We suggest a greedy <b>preallocation</b> strategy, and evaluate it by simulation, using workloads based on previous analyses of flash-crowds. We observe a reduction of 35 - 50 % in number of PMs used compared with classical dynamic allocation. This means that a datacenter can provide per-service isolation with 35 %- 50 % fewer PMs. ...|$|E
40|$|Although Moore’s law {{ensures the}} {{increase}} in computational power, IO performance appears to be left behind. This minimizes the benefits gained from increased computational power. Processors have to idle {{for a long time}} waiting for IO. Another factor that slows the IO communication is the increased parallelism required in today’s computations. Most modern processing units are built from multiple weak cores. Since IO has a low parallelism the weak cores will decrease system performance. Furthermore to avoid added delay of external storage, future High Performance Computing (HPC) systems will employ Active Storage Fabrics (ASF). These embed storage directly into large HPC systems. Single HPC node IO performance will therefore require optimization. This can only be achieved with a full understanding of the IO stack operations. The analysis of the IO stack under the new conditions of multi-core and massive parallelism leads to some important conclusions. The IO stack is generally built for single devices and is heavily optimized for HDD. Two main optimization approaches are taken. The first is optimizing the IO stack to accommodate parallelism. Conclusions on IO analysis shows that a design based on several parallel operating storage devices is the best approach for parallelism in the IO stack. A parallel IO device with unified storage space is introduced. The unified storage space allows for optimal function division among resources for both read and write. The design also avoids large parallel file systems overhead by using limited changes to a conventional file system. Furthermore the interface of the IO stack is not changed by the design. This is a rather important restriction to avoid application rewrite. The implementation of such a design is shown to result in an increase in performance. The second approach is Optimizing the IO stack for Solid State Drives (SSD). The optimization for the new storage technology demanded further analysis. These show that the IO stack requires revision on many levels for optimal accommodation of SSD. File system <b>preallocation</b> of free blocks is used as an example. <b>Preallocation</b> is important for data contingency on HDD. However due to fast random access of SSD <b>preallocation</b> represents an overhead. By careful analysis to the block allocation algorithms, <b>preallocation</b> is removed. As an additional optimization approach IO compression is suggested for future work. It can utilize idle cores during an IO transaction to perform on the fly IO data compression...|$|E
40|$|The channel {{allocation}} {{is one of}} the key {{problems in}} the design of the wireless network, as it greatly influences the throughput and performance of the network. Firstly we introduce the channel propagation model, which is able to accurately reflect the signal attenuation, the network interference and other factors in the actual environment. Based on this work, we put forward the channel allocation algorithm named LRCAA, which includes two parts: the <b>preallocation</b> stage and the dynamic optimization stage. The LRCAA is verified by the experiment, which shows it can converge to a stable optimal point and obtain a better network performance...|$|E
40|$|During {{the early}} stage of {{reusable}} launch vehicle (RLV) reentry flight, reaction control system (RCS) is the major attitude control device. RCS, which is {{much different from the}} atmospheric steer’s control, requires a well designed control allocation system to fit the attitude control in high altitude. In this paper, an indexed control method was proposed for RCS <b>preallocation,</b> a 0 - 1 integer programming algorithm was designed for RCS allocation controller, and then this RCS scheme’s effect was analyzed. Based on the specified flight mission simulation, the results show that the control system is satisfied. Moreover, several comparisons between the attitude control effect and RCS relevant parameters were studied...|$|E
40|$|This paper {{addresses}} {{the issues of}} distributed dynamic resource allocation (DRA) in the downlink of a SDMA broadband wireless packet network with multiple access points and adaptive antennas. Packet access and downlink beamforming make the intercell interference hard to predict, and {{how to handle it}} in order to efficiently perform slot and power allocation is still an open issue. We propose here new DRA strategies for efficient allocation in a distributed environment, which jointly consider spatial compatibility of users and an intercell interference estimation based on a fraction of the worst case interference measurements. The latter is independent of the actual allocation. We consider a low mobility TDD system with synchronized base stations and we assume perfect channel knowledge. The algorithms which are introduced and compared are the distributed max-min fit (DMMF) and the distributed reverse fit (DRF), which are based on the max-min fit criterion, the DMMF combined with the novel concept of nulls <b>preallocation,</b> where each AP reserves some beamforming nulls for the most interfered users of neighboring cells, and the power shaping technique (PS-DRA for SDMA), which efficiently exploits static power <b>preallocation</b> on time slots. The numerical results show that our techniques are able to significantly reduce the gap between a greedy centralized strategy that fully coordinates all the access ports and the baseline case of random distributed allocation, previously proposed for packet access. However, distributed power control in a packet switched environment with downlink beamforming still remains a hard task: nevertheless, PS-DRA is an efficient solution for joint slot-power allocatio...|$|E
40|$|This is {{a machine}} {{learning}} application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC's computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply {{natural language processing}} (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log [...] -without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features <b>preallocation</b> for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semi-supervised learning and prediction methods for other critical events such as cyberattack detection. Comment: 8 page...|$|E
40|$|Rights to {{individual}} papers {{remain with the}} author or the author's employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must {{be included in the}} reproduced paper. USENIX acknowledges all trademarks herein. Planned Extensions to the Linux Ext 2 /Ext 3 Filesystem The ext 2 filesystem was designed with the goal of expandability while maintaining compatibility. This paper describes ways in which advanced filesystem features can be added to the ext 2 filesystem while retaining forwards and backwards compatibility as much as possible. Some of the filesystem extensions that are discussed include directory indexing, online resizing, an expanded inode, extended attributes and access control lists support, extensible inode tables, extent maps, and <b>preallocation.</b> ...|$|E
40|$|This paper {{proposes a}} novel {{distributed}} {{dynamic resource allocation}} (DRA) algorithm for the downlink of a SDMA broadband wireless packet network with multiple access ports and adaptive antennas. With the new scheme, each access port (AP) independently preallocates some beamforming s toward the most interfered users of the neighbor APs and broadcasts this information to the other APs. Then all APs independently and distributedly perform the allocation of their users using the distributed max-min fit (DMMF) scheme. Simulation results show a significant improvement over the DMMF without s <b>preallocation</b> and a significantly reduced gap between the DMMF and the centralized max-min fit (CMMF) algorithm, which performs the allocation by fully coordinating all the access ports. As a baseline case, {{the performance of a}} random slot allocation algorithm is also reported...|$|E
40|$|From {{the first}} {{recorded}} influenza pandemic in 1890, {{there have been}} new strains of influenza which have caused pandemics approximately every 30 years including recent {{events such as the}} H 5 N 1 Avian 'Flu pandemic and the 2009 H 1 N 1 Swine 'Flu pandemic. Although the 2009 pandemic was mild in nature, if events of the past are any indication then control of future pandemics is of utmost importance. Vaccination is commonly looked at to help control the spread of a pandemic, however, vaccinations are strain-specific. While developing a new vaccine is possible, the World Health Organisation estimates that this process would take four to five months. This means that vaccination cannot be used to help control the spread of influenza early on in a pandemic. An alternative are antivirals which are not strain-specific, meaning that they can potentially be used to help control the spread of influenza early on in a pandemic. Antivirals are, however, not as effective at reducing the spread of disease when compared to vaccination. In the 2009 Swine 'Flu pandemic, many countries worldwide utilised antiviral medication, with the aim to assist in controlling the spread of influenza. The most common method in which these antivirals were utilised we refer to as dynamic allocation. In dynamic allocation, when the first person in a household experiences influenza-like symptoms, they report to a health professional. Then, a sample is sent for laboratory testing. If the individual is confirmed to have influenza, the entire household is allocated a course of antivirals and every member of the household begins taking them. The potential weakness in this strategy is the delay between becoming infectious and a household receiving antivirals. We consider an alternative antiviral allocation scheme which we call <b>preallocation.</b> In a <b>preallocation</b> scheme, instead of waiting for antivirals to be delivered after the first confirmed infection, {{as is the case with}} dynamic allocation, the antivirals are delivered to households at the beginning of the pandemic. When the first person experiences symptoms, they contact a health professional via a telephone hotline. The professional then decides if it is likely that the individual has influenza. If the individual is likely to have influenza then the entire household starts taking antivirals immediately, just as is the case in dynamic allocation. The advantage of this scheme is that the delay is essentially zero, but there is the potential for the antivirals to be wasted in at least two ways. First, this type of identification of infection is clearly less precise than laboratory testing. Second, it is possible that antivirals will be preallocated to a household who will never experience infection and so those antivirals will essentially be wasted. It is this tradeoff that is the focus of this thesis. The stochastic households epidemic model which is detailed and developed in this work incorporates the household structure of a general population. This allows us to incorporate the stronger mixing of individuals who share a household compared to individuals in the general population, as well as the fact that antivirals are allocated to an entire household when infection is first detected. To analyse this model, we develop two approximations: (i) A branching process approximation, and (ii) a deterministic approximation, that assist us in calculating quantities associated with a pandemic. The branching process is very fast to compute, but due to required assumptions in the derivation, it is only able to describe the early stages of the pandemic. The branching process is able to rapidly compute quantities such as the Malthusian parameter, r, and the household reproductive ratio, R* [* subscript], but is unable to calculate quantities such as the final epidemic size, that is, the total number of people infected over the course of the pandemic [...] The deterministic approximation does not allow for as rapid evaluation as the branching process approximation, but is able to approximately reproduce the entire expected pandemic curve, giving access to quantities such as the expected final epidemic size. Both of these approximations are fast to compute so we can explore a range of parameters and compare the two allocation schemes - dynamic allocation and <b>preallocation.</b> We show that <b>preallocation</b> of antivirals often leads to a smaller final epidemic size than dynamic allocation for a severe pandemic outbreak, while a dynamic allocation scheme often gives a lower Malthusian parameter, r, and household reproductive ratio, R* [* subscript]. We provide a justification for this behaviour and demonstrate that the results are relatively robust across the parameters controlling the pandemic. We also consider a number of extensions to the deterministic approximation such as the incorrect use of antivirals, a hybrid allocation scheme, and the production of antivirals during the pandemic. Under these extensions, the general behaviour of the two schemes - <b>preallocation</b> yielding a lower final epidemic size but dynamic allocation yielding superior early-time quantities - is unchanged. Thesis (M. Phil.) [...] University of Adelaide, School of Mathematical Sciences, 201...|$|E
40|$|The report {{describes}} {{the process of}} designing, building and evaluating a Simula compiler based directly on the multi-platform optimization and code generation framework of the GNU Compiler Collection (GCC). Utilization of this framework, known as the GCC back-end, enables this Simula implementation to generate good-quality assembler code for the variety of machine platforms that are supported by GCC. The interface provided by the GCC back-end is more flexible than the C language and therefore provides certain advantages when expressing the semantics of Simula to the low-level optimization machinery of the GCC back-end. Accommodation of Simula's coroutine feature poses a particular challenges on the GCC back-end {{with respect to the}} heap-based activation records that become necessary. Extensive <b>preallocation</b> of activation records reduces the number of heap allocations and improves run-time performance at the expense of memory parsimony...|$|E
40|$|New {{techniques}} {{in the implementation}} of out-of-band control in ATM networks are causing both industry and research laboratories to look again at the whole question of ATM signalling. These techniques devolve the control from the network devices into a higher level distributed processing environment, resulting in simpler network devices and more flexible control architectures. This paper takes this idea one stage further and suggests that at least in some cases, the only place in which control can be exerted without inhibiting applications is within the applications themselves. We call the combination of an application defined control policy and a network connection a connection closure. 1 INTRODUCTION A network control architecture is a set of policies and algorithms used to permit network devices to pass information between geographically separated end systems. Control architectures can be loosely divided into two groups: those that do not require the <b>preallocation</b> of resources to ap [...] ...|$|E
40|$|One of {{the major}} {{challenges}} of cloud computing is the management of request-response coupling and optimal allocation strategies of computational resources for {{the various types of}} service requests. In the normal situations the intelligence required to classify the nature and order of the request using standard methods is insufficient because the arrival of request is at a random fashion and it is meant for multiple resources with different priority order and variety. Hence, it becomes absolutely essential that we identify the trends of different request streams in every category by auto classifications and organize <b>preallocation</b> strategies in a predictive way. It calls for designs of intelligent modes of interaction between the client request and cloud computing resource manager. This paper discusses about the corresponding scheme using Adaptive Resonance Theory- 2. Comment: pages 11, figures 3, International Journal on Cloud Computing: Services and Architecture(IJCCSA),Vol. 1, No. 2, August 201...|$|E
40|$|Modern {{switches}} and routers often use dynamic RAM (DRAM) {{in order to}} provide large buffer space. For advanced quality of service (QoS), per-flow queueing is desirable. We study the architecture of a queue manager for many thousands of queues at OC- 192 (10 Gbps) line rate. It forms the core of the "datapath" chip in an efficient chip partitioning for the line cards of {{switches and}} routers that we propose. To effectively deal with bank conflicts in the DRAM buffer, we use pipelining and out-of-order execution techniques, like the ones originating in the supercomputers of the 60 's. To avoid off-chip SRAM, we maintain the pointers in the DRAM, using free buffer <b>preallocation</b> and free list bypassing. We have described our architecture using behavioral Verilog (a Hardware Description Language), at the clock-cycle accuracy level, assuming Rambus DRAM. We estimate the complexity of the queue manager at roughly 60 thousand gates, 80 thousand flip-flops, and 4180 Kbits of on-chip SRAM, for 64 K flows. I...|$|E
40|$|Abstract. This paper {{deals with}} the problem of statically {{inferring}} the shape of an array in languages such as MATLAB. Inferring an array’s shape is desirable because it empowers better compilation and interpre-tation; specifically, knowing an array’s shape could permit reductions in the number of run-time array conformability checks, enable memory <b>preallocation</b> optimizations, and facilitate the in-lining of “scalarized” code. This paper describes how the shape of a MATLAB expression can be determined statically, based on a methodology of systematic matrix formulations. The approach capitalizes on the algebraic properties that underlie MATLAB’s shape semantics and exactly captures the shape that the MATLAB expression assumes at run time. Some of the highlights of the approach are its applicability to a large class of MATLAB functions and its uniformity. Our methods are compared with the previous shadow variable scheme, and we show how the algebraic view allows inferences not deduced by the traditional approach. ...|$|E
40|$|This work is {{dedicated}} to resolve the Journaling of Journal Anomaly in Android IO stack. We orchestrate SQLite and EXT 4 filesystem so that SQLite’s file-backed journaling activity can dispense with the expensive filesystem intervention, the journaling, without compromising the file integrity under unexpected filesystem failure. In storing the logs, we exploit the direct IO to suppress the filesystem interference. This work consists of three key ingredients: (i) <b>Preallocation</b> with Explicit Journaling, (ii) Header Embedding, and (iii) Group Synchronization. <b>Preallocation</b> with Explicit Journaling eliminates the filesystem journaling properly protecting the file metadata against the unexpected system crash. We redesign the SQLite B-tree structure with Header Embedding to make it direct IO compatible and block IO friendly. With Group Synch, we minimize the synchronization overhead of direct IO and make the SQLite operation NAND Flash friendly. Combining the three technical ingredients, we develop a new journal mode in SQLite, the WALDIO. We implement it on the commercially available smartphone. WALDIO mode achieves 5. 1 × performance (insert/sec) against WAL mode which is the fastest journaling mode in SQLite. It yields 2. 7 × performance (inserts/sec) against the LS-MVBT, the fastest SQLite journaling mode known to public. WALDIO mode achieves 7. 4 × performance (insert/sec) against WAL mode when it is relieved from the overhead of explicitly synchronizing individual log-commit operations. WALDIO mode reduces the IO volume to 1 / 6 compared against the WAL mode. We {{would like to thank}} the anonymous reviewers for their insightful comments and feedback. Special thanks go to our shepherd Theodore Ts’o whose constructive comment and advice have made our work further mature and rigorous. We also would like to thank Yongseok Jo at EFTECH, Dongjun Shin, Seunghwan Hyun, Dongil Park and Heegyu Kim at Samsung Electronics for their advice on revising this paper. Finally, we like to thank our colleague Seongjin Lee for his help in preparing the manuscript. This work is sponsored by IT R&D program from MKE/KEIT (No. 10041608, Embedded system Software for New-memory based Smart Device) and by ICT R&D program of MSIP/IITP (No. 1 I 2221 - 14 - 1005) ...|$|E
40|$|One of {{the main}} hurdles that array-based {{languages}} such as MATLAB and APL pose to compilation {{is the lack of}} an explicit declaration for an array’s shape. In programs written using these languages, the attributes of intrinsic type and shape for a variable are implicitly characterized by the defining expression for that variable. In addition, these attributes are allowed to change on the fly. On account of these complications, and others such as the dynamic binding of storage to names, interpreters are typically used to cope with their translation. In this report, we address the problem of statically inferring the shape of an array in languages such as MATLAB. Inferred shapes are desirable from the standpoint of both program compilation and efficient interpretation because static knowledge of an array’s shape could permit reductions in the number of run-time array conformability checks, enable memory <b>preallocation</b> optimizations, and facilitate efficient translations to “scalar ” target languages such as C. This report presents a framework for statically describing the shape of a MATLA...|$|E
40|$|As {{computing}} and {{embedded systems}} evolve towards highly parallel multiprocessors, major {{research and development}} efforts are being focused on network interface (NI) architectures that enable efficient interprocessor communication (IPC). This thesis is focused on NI architecture, prototyping and design issues for cluster and chip multiprocessors. This work includes {{the development of a}} NI queue manager, key NI design issues with respect to IPC and a proposed NI design well suited to chip multiprocessors. The first part of this thesis presents the architecture design and implementation of a NI queue manager that supports Virtual Output Queuing, Variable-Size Multi-Packet Segmentation and QFC flow control. To increase the available network buffer space VOQs can migrate to external memory in the form of memory blocks connected in linked-lists. Free-List Bypass and Free Block <b>Preallocation</b> optimization techniques are employed to minimize the required number of accesses to external memory and achieve higher bandwidth. In addition, a novel packet processing mechanism that converts arbitrary traffic segments into autonomous network packets was implemented. Detailed FPGA hardware cost results ar...|$|E
40|$|Conventional compilers are {{designed}} for producing highly optimized code without paying much attention to compile time. The design goals of Java just-in-time compilers are different: produce fast code at the smallest possible compile time. In this article we present a very fast algorithm for translating JavaVM byte code to high quality machine code for RISC processors. This algorithm handles combines instructions, does copy elimination and coalescing and does register allocation. It comprises three passes: basic block determination, stack analysis and register <b>preallocation,</b> final register allocation and machine code generation. This algorithm replaces an older one in the CACAO JavaVM implementation reducing the compile time {{by a factor of}} seven and producing slightly faster machine code. The speedup comes mainly from following simplifications: fixed assignment of registers at basic block boundaries, simple register allocator, better exception handling, better memory management and fine tuning the implementation. The CACAO system is currently faster than every JavaVM implementation for the Alpha processor and generates machine code for all used methods of the javac compiler and its libraries in 60 milliseconds on an Alpha workstation. ...|$|E
40|$|We {{discuss the}} issues {{involved}} in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. One way to implement MPI-IO portably is to implement {{it on top of}} the basic Unix I/O functions (open, lseek, read, write, and close), which are themselves portable. We argue that this approach has limitations in both functionality and performance. We instead advocate an implementation approach that combines a large portion of portable code and a small portion of code that is optimized separately for different machines and file systems. We have used such an approach to develop a high-performance, portable MPI-IO implementation, called ROMIO. In addition to basic I/O functionality, we consider the issues of supporting other MPI-IO features, such as 64 -bit file sizes, noncontiguous accesses, collective I/O, asynchronous I/O, consistency and atomicity semantics, user-supplied hints, shared file pointers, portable data representation, and file <b>preallocation.</b> We describe how we implemented each of these features on various machines and file systems. The machines we consider are the HP Exemplar...|$|E
40|$|The Intel Concurrent File System (CFS) for the iPSC/ 2 {{hypercube}} {{is one of}} {{the first}} production file systems to utilize the dechtstering of large files across numbers of disks to improve I/O performance. The CFS also makes use of dedicated I/O nodes, operating asynchronously, which provide tile caching and prefetching. Processing of I/O requests is distributed between the compute node that initiates the request and the IfO nodes that service the request. The effects of the various design decisions in the Intel CFS are dlffictdt to determine without measurements of an actual system. We present performance measurements of the CFS for a hypercube with 32 compute nodes and four 1 / 0 nodes (four disks). Measurement of read/write rates for one compute node to one 1 / 0 node, one compute node to multiple I/O nodes, and multiple compute nodes to multiple 1 / 0 nodes form the basis for the study. Additional measurements show the effects of different buffer sizes, caching, prefetching, and file <b>preallocation</b> on system performance. 1...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. The coordination of multifunction phased array radars across networked platforms can enable superior functionality and battle space awareness. This thesis formulates and solves {{a number of}} optimization models and heuristic algorithms to analyze and prescribe radar-to-target assignments and schedules. One optimization model uses full target information to provide a best case assessment of {{the ability of a}} given set of radar platforms to track a collection of targets. A modified version of this model determines the impact on these results if targets coordinate their maneuvers in order to overwhelm the radar system. We then consider the more realistic scenario in which the planner's knowledge is imperfect and describe approaches for allocating radar assets to targets in that setting. The first such approach extends an existing two-dimensional geographic allocation method to three dimensions. This allows for an allocation of the operating space to radar assets and can serve as a <b>preallocation</b> heuristic for more sophisticated assignment algorithms. Moreover, because the existing method does not account for transfers in tasks between geographical areas, this thesis models the additional workload involved in performing handoffs of targets between radars. Lieutenant, United States Nav...|$|E
40|$|We {{consider}} distributed dynamic slot {{and power}} allocation for the downlink of a TD/SDMA broadband wireless packet network with multiple access ports and adaptive antennas. The still open issue for packet multicell SDMA {{is how to}} manage the intercell interference, which {{is very difficult to}} predict in an uncoordinated environment, due to packet access and downlink beamforming. For this reason, doing distributed allocation efficiently as well as improving the performance by means of power control results in a very hard task. We propose a greedy SDMA algorithm exploiting the power shaping technique, which is based on a static <b>preallocation</b> of the transmit power to each slot of the frame. This permits to obtain a partial predictability of intercell interference, allowing different levels of estimated intercell interference and available power for each slot. We show that our greedy SDMA algorithm with power shaping increases system capacity with respect to the same algorithm without power shaping and reduces the performance gap with respect to a greedy centralized strategy, thus limiting the need of coordination among cells. Both centralized and distributed algorithms are compared, as reference, to the baseline case of random allocation, previously proposed for packet access...|$|E
40|$|Abstract—The modern {{file system}} is still {{implemented}} in the kernel, and is statically linked with other kernel components. This architecture has brought performance and efficient integration with memory management. However kernel development is slow and modern storage systems must support an array of features, including distribution across a network, tagging, searching, deduplication, checksumming, snap-shotting, file <b>preallocation,</b> real time I/O guarantees for media, and more. To move complex components into user-level however will require an efficient mechanism for handling page faulting and zero-copy caching, write ordering, synchronous flushes, interaction with the kernel page write-back thread, and secure shared memory. We implement such a system, and experiment with a user-level object store built on top. Our object store is a complete re-design of the traditional storage stack and demonstrates the efficiency of our technique, and the flexibility it grants to user-level storage systems. Our current prototype file system incurs between a 1 % and 6 % overhead on the default native file system EXT 3 for in-cache system workloads. Where the native kernel file system design has traditionally found its primary motivation. For update and insert intensive metadata workloads that are out-of-cache, we perform 39 times better than the native EXT 3 file system, while still performing only 2 times worse on out-of-cache random lookups. I...|$|E
40|$|Ifie cascade {{correlation}} based {{neural network}} learning algorithm has drwwvt {{a lot of}} attention because of its enhanced learning capability. It overcomes significant drawbacks of error backpropagalion (EBP) in that (1) it is no longer constrained 10 a fixed architecture via a <b>preallocation</b> of the number of hidden units, (2) it features seiec(ive weigh ([raining as opposed to EBP’s global weight training. In addition, jiJom a hardware implementation perspective, nel works based on the cascade correlation algorithm require sign ijcantly less complex synaptic weight circuitry than those reqliired by EBP. We present a rnathernalical analysis for a new scheme termed Cascade Error Projection (CEP) and show tha ((he same is also applicable to Cascade Correlation. In CEP, it is shown that there exis!s, al least, a non zero set of weights which is calculaledfiom afine space and that convergence is assured because the net work salisfies the Liapunov criteria, iti added hidden units domain rather than in the time domain. The CEP technique is faster to execute because part of the weigh(s are dcterrnin is[ical[y obtained, and the learning of weights fiorn the inputs to each added hidden unit is perjorrned as a single layer percepiron learning with previously obtained weights frozen. In addition, Ihc initial weights s[arl oul with a zero value for e~’ery newly added unit, and a single hidden unit i...|$|E
40|$|Recently, {{pandemic}} {{response has}} involved {{the use of}} antivirals. These antivirals are often allocated to households dynamically throughout the pandemic with the aim being to retard the spread of infection. A drawback {{of this is that}} there is a delay until infection is confirmed and antivirals are delivered. Here an alternative allocation scheme is considered, where antivirals are instead preallocated to households at the start of a pandemic, thus reducing this delay. To compare these two schemes, a deterministic approximation to a novel stochastic household model is derived, which allows efficient computation of key quantities such as the expected epidemic final size, expected early growth rate, expected peak size and expected peak time of the epidemic. It is found that the theoretical best choice of allocation scheme depends on strain transmissibility, the delay in delivering antivirals under a dynamic allocation scheme and the stockpile size. A broad summary is that for realistic stockpile sizes, a dynamic allocation scheme is superior with the important exception of the epidemic final size under a severe pandemic scenario. Our results, viewed in conjunction with the practical considerations of implementing a <b>preallocation</b> scheme, support a focus on attempting to reduce the delay in delivering antivirals under a dynamic allocation scheme during a future pandemic. Michael Lydeamore, Nigel Bean, Andrew J. Black, Joshua V. Ros...|$|E
40|$|Abstract With current {{estimates}} {{of data to}} be managed and madeavailable increasing at 60 % per annum, disk space utilization is becoming a critical performance issue for highend users, including {{but not limited to}} IT solutions, Storage Area Networks and Virtual Machine environment. Wepropose Dynamic Allocation of Disk Area (DADA), a disk management framework that performs on-demand diskarea allocation on the basis of user access patterns. Results show that our service can reduce disk space uti-lization upto 20 times when compared to traditional static allocation policies. Also it is shown that using certain <b>preallocation</b> schemes, minimize the overhead incurred due to on demand allocation. Our experiments indicate thatthere is a tradeoff between disk utilization and the runtime performance of the programs. We test our schemeon two microbenchmarks with varied properties and show that our framework performs considerably better for nonI/O intensive applications. Also we perform experiments on HP disk traces to study the efficacy of our pre alloca-tion mechanisms. 1 Introduction Information technology is the lifeblood of any busi-ness, especially today when organization performance depends on information on demand. Business accountabilityhinges on it, laws and regulations mandate it, customers demand it, and effective business processes rely on it. Butas utterly valuable as information on demand has become, it has become more costly to store, maintain and protect. Storage is no longer an afterthought. Too much isat stake. Companies are searching for more ways to efficiently manage expanding volumes of data. Also, the in-creasing complexity of managing large numbers of storage devices and vast amounts of data is driving greaterbusiness value into software and services. With current {{estimates of}} data to be managed and made available in-creasing at 60 percent per annum [9], disk space utilization is fast becoming a major concern for high endusers. Scalability and manageability have become major issues in data storage with solutions pointing towardssmart placement of data on the disk and flexible data movement within storage devices [5]. As storage takesprecedence, three major initiatives have emerged...|$|E
40|$|There are {{a variety}} of driver distractions that {{negatively}} affect driver workload and performance. These distractions range attempting to light a cigarette, and putting on make-up, to eating or drinking, tuning the radio, using a cellular phone, or using an in-vehicle navigation system. Of particular present interest are the distracting effects of telematic devices, which include traffic information systems, telecommunication, intelligent aid and control, and navigational systems. These devices can now be found on-board various types of U. S. and foreign automobiles. Despite having many potential benefits, there are also several behavioral problems resulting from poor use of these devices. The present research was designed to investigate the deleterious effects of telematics on driver performance. It was hypothesized that all the telematic systems used in this study would degrade driver performance and increase workload. A mixed-model factorial design (2 x 3) was used, with telematics being a between-subject factor and allocation phase a within-subject factor (repeated measures). All participants were required to drive three, four-minute simulated (pre, during, and post) allocation phases. In the <b>preallocation</b> phase, participants were required to drive while performing a secondary counting task, (counting and responding to a series of randomly presented visual signals). During the allocation phase, participants were required to drive and perform the secondary counting task while either talking on the phone or tuning a radio (distractibility task). In the post-allocation phase, participants were required to drive while performing the secondary counting task. Data from the counting task (number of correct, wrong, and misses) and driving errors (collisions, crossing the median, leaving the road, maintaining the speed limit, and lane deviations) were recorded and statistically analyzed. Thirty-four participants (nine males and 25 females) from the University of Central Florida participated in this study. A series of analyses of variance (ANOVA) were conducted to test for the effects of telematics and workload on each of the dependent measures. A significant main effect of phase on lane deviations was observed, F(2, 64) = 10. 58, p 3 ̆c. 001, indicating that more lane deviations were made during the cell phone and radio tuning use (M = 9. 14) than during both of the pre-allocation (M = 4. 14) and post-allocation (M = 5. 88) phases. ANOVA also yielded a significant main effect of phase on crossing the median, F(3, 68) = 4. 63, p 3 ̆c. 05, indicating that more crossings were made during the allocation phase (M = 5. 05) than during the pre-allocation (M = 3. 05) and post-allocation (M = 4. 47) phases. Similarly, the results also showed a significant effect of phase on the distraction task performance, F(2, 64) = 5. 70, p 3 ̆c. 01, indicating that more errors were made during the allocation phase (M = 6. 50) than during the pre-allocation (M = 4. 50) and the post-allocation (M = 3. 38) phases. The present findings indicate that both cellular phone and radio systems are capacity demanding. The counting task results demonstrate the increased level of workload associated with these telematic devices. In addition, driving performance errors were also higher for both the cellular phone and the radio systems. Our findings suggest the need to regulate the use of such devices in order to avoid overloading the driver’s attentional spare capacity...|$|E

