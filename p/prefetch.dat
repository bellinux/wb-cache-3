997|1876|Public
25|$|Another {{benefit is}} its <b>prefetch</b> buffer, which is 8-burst-deep. In contrast, the <b>prefetch</b> buffer of DDR2 is 4-burst-deep, and the <b>prefetch</b> buffer of DDR is 2-burst-deep. This {{advantage}} is an enabling technology in DDR3's transfer speed.|$|E
25|$|Multiple {{independent}} <b>prefetch</b> streams {{with automatic}} length and stride detection.|$|E
25|$|Abort mode: A {{privileged mode}} that is entered {{whenever}} a <b>prefetch</b> abort or data abort exception occurs.|$|E
40|$|Instruction cache misses can {{severely}} {{limit the}} performance of both superscalar processors and high speed sequential machines. Instruction cache <b>prefetching</b> attempts to prevent misses, or at least reduce their cost, by bringing lines into the instruction cache before they are accessed by the CPU fetch unit. There have been several algorithms proposed to do this, most notably next-line <b>prefetching</b> and table-based target <b>prefetching</b> schemes. A new scheme called wrong-path <b>prefetching</b> is proposed which combines next-line <b>prefetching</b> and target-always <b>prefetching.</b> Surprisingly, {{a large part of}} its performance is based upon <b>prefetching</b> the not-taken path of conditional branches. Not only does wrong-path <b>prefetching</b> achieve higher performance than next-line or table-based <b>prefetching</b> schemes, the amount of additional hardware required is roughly the same as next-line and considerable less than table-based implementations. When compared with no <b>prefetching,</b> wrong-path <b>prefetching</b> can reduce t [...] ...|$|R
40|$|In {{this report}} we compare the {{performance}} of a <b>prefetching</b> page server system with a <b>prefetching</b> object server system. We simulate the object access pattern by assigning transition probabilities to the object relationships. According to the transition probabilities we compute the access probability of pages and objects. We designed several <b>prefetching</b> techniques for a <b>prefetching</b> page server and a <b>prefetching</b> object server. We compared {{the performance of}} the <b>prefetching</b> techniques in a simulation...|$|R
40|$|Data <b>prefetching</b> is an {{effective}} technique to hide memory latency and thus bridge the increasing processor-memory performance gap. Our previous work presents guided region <b>prefetching</b> (GRP), a hardware/software cooperative <b>prefetching</b> technique which cost-effectively tolerates L 2 latencies. The compiler hints improve L 2 <b>prefetching</b> accuracy and reduce bus bandwidth consumption compared to hardware only <b>prefetching.</b> However, some useless <b>prefetches</b> remain to degrade memory performance. This paper first explores a more aggressive GRP <b>prefetching</b> scheme which pushes L 2 <b>prefetches</b> into the L 1, similar to the IBM Power 4 and 5 cache designs. This approach yields some additional performance improvements. This work then combines GRP with evict-me, a compilerassisted cache replacement policy. This combination can reduce cache pollution introduced by useless <b>prefetches</b> and further improve memory system performance. ...|$|R
25|$|DDR SDRAM employs <b>prefetch</b> {{architecture}} {{to allow}} {{quick and easy}} access to multiple data words located on a common physical row in the memory.|$|E
25|$|The {{drawback}} of {{the older}} fast column access method was that a new column address had to be sent for each additional dataword on the row. The address bus had to operate at the same frequency as the data bus. <b>Prefetch</b> architecture simplifies this process by allowing a single address request to result in multiple data words.|$|E
25|$|Memory {{manufacturers}} {{stated that}} it was impractical to mass-produce DDR1 memory with effective transfer rates in excess of 400MHz (i.e. 400 MT/s and 200MHz external clock) due to internal speed limitations. DDR2 picks up where DDR1 leaves off, utilizing internal clock rates similar to DDR1, but is available at effective transfer rates of 400MHz and higher. DDR3 advances extended the ability to preserve internal clock rates while providing higher effective transfer rates by again doubling the <b>prefetch</b> depth.|$|E
40|$|Conventional <b>prefetching</b> schemes regard {{prediction}} accuracy {{as important}} because useless data <b>prefetched</b> by a faulty prediction may pollute the cache. If <b>prefetching</b> requires considerably low read cost but the prediction is not accurate, it {{may or may}} not be beneficial depending on the situation. However, the problem of low prediction accuracy can be dramatically reduced if we efficiently manage <b>prefetched</b> data by considering the total hit rate for both <b>prefetched</b> data and cached data. To achieve this goal, we propose an adaptive strip <b>prefetching</b> (ASP) scheme, which provides low <b>prefetching</b> cost and evicts <b>prefetched</b> data at the proper time by using differential feedback that maximizes the hit rate of both <b>prefetched</b> data and cached data in a given cache management scheme. Additionally, ASP controls <b>prefetching</b> by using an online disk simulation that investigates whether <b>prefetching</b> is beneficial for the current workloads and stops <b>prefetching</b> if it is not. Finally, ASP provides methods that resolve both independency loss and parallelism loss that may arise in striped disk arrays. We implemented a kernel module in Linux version 2. 6. 18 as a RAID- 5 driver with our scheme, which significantly outperforms the sequential <b>prefetching</b> of Linux from several times to an order of magnitude in a variety of realistic workloads. ...|$|R
40|$|This paper studies {{hardware}} <b>prefetching</b> for second-level (L 2) caches. Previous work on <b>prefetching</b> {{has been}} extensive but largely directed at primary caches. In some cases only L 2 <b>prefetching</b> is possible or is more appropriate. By studying L 2 <b>prefetching</b> characteristics {{we show that}} existing stride-directed methods [1, 8] for L 1 caches do not work as well in L 2 caches. We propose a new stride-detection mechanism for L 2 <b>prefetching</b> and combine it with stream buffers used in [16]. Our evaluation shows that this new <b>prefetching</b> scheme {{is more effective than}} stream buffer <b>prefetching</b> particularly for applications with long-stride accesses. Finally, we evaluate an L 2 cache <b>prefetching</b> organization which combines a small L 2 cache with our stridedirected <b>prefetching</b> scheme. Our results show that this system performs significantly better than stream buffer <b>prefetching</b> or a larger non-prefetching L 2 cache without suffering from {{a significant increase in the}} memory traffic...|$|R
40|$|We {{study the}} {{efficiency}} of previously proposed stride and sequential <b>prefetching</b> [...] -two promising hardware-based <b>prefetching</b> schemes to reduce readmiss penalties in shared-memory multiprocessors. Although stride accesses dominate in four out of six of the applications we study, we find that sequential <b>prefetching</b> does as well as and in same cases even better than stride <b>prefetching</b> for five applications. This is because (i) most strides are shorter than the block size (we assume 32 byte blocks), which means that sequential <b>prefetching</b> is as effective for these stride accesses, and (ii) sequential <b>prefetching</b> also exploits the locality of read misses with non-stride accesses. However, since stride <b>prefetching</b> in general results in fewer useless <b>prefetches,</b> it offers the extra advantage of consuming less memory-system bandwidth. Corresponding author: Fredrik Dahlgren Keywords: Hardware-Controlled <b>Prefetching,</b> Latency Tolerance, Performance Evaluation, Relaxed Memory Consiste [...] ...|$|R
25|$|MIPS IV is {{the fourth}} version of the architecture. It is a superset of MIPS III and is {{compatible}} with all existing versions of MIPS. MIPS IV was designed to mainly improve floating-point (FP) performance. To improve access to operands, an indexed addressing mode (base + index, both sourced from GPRs) for FP loads and stores was added, as were <b>prefetch</b> instructions for performing memory prefetching and specifying cache hints (these supported both the base + offset and base + index addressing modes).|$|E
25|$|Internet Explorer 11 is {{featured}} in a Windows 8.1 update which was released on October 17, 2013. It includes an incomplete mechanism for syncing tabs. It is a major update to its developer tools, enhanced scaling for high DPI screens, HTML5 prerender and <b>prefetch,</b> hardware-accelerated JPEG decoding, closed captioning, HTML5 full screen, and is the first Internet Explorer to support WebGL and Google's protocol SPDY (starting at v3). This version of IE has features dedicated to Windows 8.1, including cryptography (WebCrypto), adaptive bitrate streaming (Media Source Extensions) and Encrypted Media Extensions.|$|E
25|$|Palomino was {{the first}} K7 core to include the full SSE {{instruction}} set from the Intel Pentium III, as well as AMD's 3DNow! Professional. It is roughly 10% faster than Thunderbird at the same clock speed, {{thanks in part to}} the new SIMD functionality and to several additional improvements. The core has enhancements to the K7's TLB architecture and added a hardware data <b>prefetch</b> mechanism to take better advantage of available memory bandwidth. Palomino was also the first socketed Athlon officially supporting dual processing, with chips certified for that purpose branded as the Athlon MP. According to articles posted on HardwareZone, it was possible to mod the Athlon XP to function as an MP by connecting some fuses on the OPGA, although results varied with the motherboard used.|$|E
40|$|CONTENTS INTRODUCTION [...] . 2 1. PREFETCHING TECHNIQUES [...] 3 1. 1. Software <b>Prefetching</b> [...] . 3 1. 2. Harware <b>Prefetching</b> [...] 4 1. 2. 1. Sequential <b>Prefetching</b> [...] 4 1. 2. 2. <b>Prefetching</b> with Arbitrary Strides [...] . 5 1. 3. Reference Prediction Table (RPT) [...] ...|$|R
40|$|Existing <b>prefetching</b> {{techniques}} rely on server-based, proxybased, or client-based reference access information. Although Web servers {{may provide}} accurate access information, our {{studies show that}} significant communication overhead can be involved by sending unnecessary reference information to clients or/and proxy servers. Our study also shows that prediction accuracy of proxy-based <b>prefetching</b> can be significantly limited without input of Web servers. We propose a coordinated proxy-server <b>prefetching</b> technique that adaptively utilizes the reference information and coordinates <b>prefetching</b> activities at both proxy and web servers. We have carefully considered the trade-off between the accuracy of server information and {{the high cost of}} obtaining the information in <b>prefetching</b> method designs. In our design, the reference access information stored in proxy servers will be the main source serving data <b>prefetching</b> for groups of clients, each of whom shares the common surfing interests. The access information in the web server will be used to serve data <b>prefetching</b> only for data objects that are not qualified for proxy-based <b>prefetching.</b> Conducting trace-driven simulations, we have quantified the limits of both server-based and proxy-based <b>prefetching,</b> and evaluated the proposed coordinated proxy-server <b>prefetching</b> technique to show its effectiveness. We show that both hit ratios and byte hit ratios contributed from coordinated proxy-server <b>prefetching</b> are are up to 88 % higher than that from proxy-based <b>prefetching,</b> and they are comparable to the ratios from serverbased <b>prefetching</b> with a difference of 5 %...|$|R
40|$|This paper studies {{hardware}} <b>prefetching</b> for second-level (L 2) caches. Previous work on <b>prefetching</b> {{has been}} extensive but largely directed at primary caches. In some cases only L 2 <b>prefetching</b> is possible or is more appropriate. We concentrate on stride-directed <b>prefetching</b> and study stream buffers and L 2 cache <b>prefetching.</b> We show that proposed stride-directed organizations/prefetching algorithms {{do not work}} as well in L 2 caches and describe a new stride-detection mechanism. We study an L 2 cache <b>prefetching</b> organization which combines a small L 2 cache with our stride-directed <b>prefetching</b> algorithm. We compare our system with stream buffer <b>prefetching</b> and other stride-detection algorithms. Our simulation results show this system to perform significantly better than stream buffer <b>prefetching</b> or a larger non-prefetching L 2 cache without suffering from {{a significant increase in}} the memory traffic. 1 Introduction Advances in processor architecture and its implementation technol [...] ...|$|R
25|$|The {{performance}} {{increase of}} the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was {{partly due to the}} non-multiplexed address and data buses, but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286, while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the <b>prefetch</b> of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.|$|E
2500|$|The DDR4 SDRAM uses a 8n <b>prefetch</b> {{architecture}} {{to achieve}} high-speed operation. The 8n <b>prefetch</b> architecture {{is combined with}} ...|$|E
2500|$|DDR4 {{will not}} double the {{internal}} <b>prefetch</b> width again, but {{will use the}} same 8n <b>prefetch</b> as DDR3. [...] Thus, {{it will be necessary}} to interleave reads from several banks to keep the data bus busy.|$|E
5000|$|Data <b>prefetching</b> fetches data {{before it}} is needed. Because data access {{patterns}} show less regularity than instruction patterns, accurate data <b>prefetching</b> is generally more challenging than instruction <b>prefetching.</b>|$|R
40|$|Software-controlled data <b>prefetching</b> {{offers the}} {{potential}} for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While <b>prefetching</b> has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compilerbased <b>prefetching</b> for pointer-based applications [...] -in particular, those containing recursive data structures. We identify the fundamental problem in <b>prefetching</b> pointer-based data structures and propose a guideline for devising successful <b>prefetching</b> schemes. Based on this guideline, we design three <b>prefetching</b> schemes, we automate the most widely applicable scheme (greedy <b>prefetching)</b> in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R 10000. Our results demonstrate that compiler-inserted <b>prefetching</b> can significantly improve the execution [...] ...|$|R
40|$|A key {{obstacle}} to achieving high performance on software distributed shared memory (DSM) systems is their high memory latencies. Software-controlled <b>prefetching</b> tolerates memory latency by overlapping computation with communication. This thesis proposes and evaluates an implementation of software-controlled non-binding <b>prefetching</b> on a software DSM called TreadMarks. With programmer-inserted <b>prefetching,</b> {{all of our}} applications achieve better performance. The overall speedup ranges from 4 % to 29 %. In addition, we observe that the performance of compiler-inserted <b>prefetching</b> matches that of programmer-inserted <b>prefetching</b> in a few cases. We also investigate <b>prefetching</b> with runtime information. Although using dynamic information to issue <b>prefetches</b> can overcome some {{of the limitations of}} statically inserted <b>prefetches,</b> the overheads of this approach often more than o set any gain in memory performance. Finally, we evaluate the combined e ects of <b>prefetching</b> and multithreading on application performance. In several cases, the combined approach outperforms either technique alone, but the overall results are mixed...|$|R
2500|$|Each {{generation}} of SDRAM {{has a different}} <b>prefetch</b> buffer size: ...|$|E
2500|$|DDR SDRAM's <b>prefetch</b> {{buffer size}} is 2n (two datawords per memory access) ...|$|E
2500|$|DDR2 SDRAM's <b>prefetch</b> {{buffer size}} is 4n (four datawords per memory access) ...|$|E
40|$|With the {{development}} of active proxy, the functions of a proxy have been enhanced beyond simply storing Web contents. Web <b>prefetching</b> activity in proxy is such an example to reduce client-perceived latency. In this paper, we propose a coordinated proxy [...] server <b>prefetching</b> technique that adaptively utilizes the access information and coordinates <b>prefetching</b> activities at both proxy and Web servers. In our design, the access information stored in proxies will be the main source serving data <b>prefetching</b> for groups of clients sharing common surfing interests. The access information in the Web server {{will be used to}} serve data <b>prefetching</b> only for data objects that are not qualified for proxybased <b>prefetching.</b> Conducting trace-driven simulations, we show that both hit ratios and byte hit ratios contributed from coordinated proxy-server <b>prefetching</b> are 30 - 75 % higher than other <b>prefetching</b> schemes, and they are comparable to the ratios from a proxyless server-based <b>prefetching</b> that is able to observe every access to the server...|$|R
40|$|<b>Prefetching</b> {{has been}} shown to be an {{effective}} technique for reducing user perceived latency in distributed systems. In this paper we show that even when <b>prefetching</b> adds no extra traffic to the network, it can have serious negative performance effects. Straightforward approaches to <b>prefetching</b> increase the burstiness of individual sources, leading to increased average queue sizes in network switches. However, we also show that applications can avoid the undesirable queueing e ects of <b>prefetching.</b> In fact, we show that applications employing <b>prefetching</b> can signi cantly improve network performance, to a level much better than that obtained without any <b>prefetching</b> at all. This is because <b>prefetching</b> offers increased opportunities for traffic shaping that are not available in the absence of <b>prefetching.</b> Using a simple transport rate control mechanism, a <b>prefetching</b> application can modify its behavior from a distinctly ON/OFF entity to one whose data transfer rate changes less abruptly, while still delivering all data in advance of the user's actual requests...|$|R
40|$|Although data <b>prefetching</b> {{algorithms}} {{have been}} extensively studied for years, {{there is no}} counterpart research done for metadata access performance. Existing data <b>prefetching</b> algorithms, either lack of emphasis on group <b>prefetching,</b> or bearing {{a high level of}} computational complexity, do not work well with metadata <b>prefetching</b> cases. Therefore, an efficient, accurate, and distributed metadata-oriented <b>prefetching</b> scheme is critical to leverage the overall performance in large distributed storage systems. In this paper, we present a novel weighted-graph-based <b>prefetching</b> technique, built on both direct and indirect successor relationship, to reap performance benefit from <b>prefetching</b> specifically for clustered metadata servers, an arrangement envisioned necessary for petabyte-scale distributed storage systems. Extensive trace-driven simulations show that by adopting our new metadata <b>prefetching</b> algorithm, the miss rate for metadata accesses on the client site can be effectively reduced, while the average response time of metadata operations can be dramatically cut by up to 67 percent, compared with legacy LRU caching algorithm and existing state-of-the-art <b>prefetching</b> algorithms...|$|R
2500|$|DDR3 SDRAM's <b>prefetch</b> {{buffer size}} is 8n (eight datawords per memory access) ...|$|E
2500|$|T â€“ {{support for}} <b>prefetch</b> with modify intent {{to improve the}} {{performance}} of the first attempt to acquire a lock ...|$|E
2500|$|The {{above are}} the JEDEC-standardized commands. [...] Earlier chips {{did not support}} the dummy channel or pair <b>prefetch,</b> and used a {{different}} encoding for precharge.|$|E
40|$|The {{benefits}} of Web cache <b>prefetching</b> are well understood, and so <b>prefetching</b> has been implemented {{in a number}} of commercial products. This paper argues that the current support for <b>prefetching</b> in HTTP/ 1. 1 is insufficient because <b>prefetching</b> with GET is not good. Existing <b>prefetching</b> implementations can cause problems with undesirable side-effects and server abuse, and the potential for these problems may thwart additional <b>prefetching</b> development and deployment. We make some initial suggestions of extensions to HTTP that would allow for safe <b>prefetching,</b> reduced server abuse, and differentiated Web server quality of service. It is our hope that this paper will restart a dialog on these issues that will move in time into a standards development process...|$|R
40|$|I/O <b>prefetching</b> {{has been}} {{employed}} {{in the past as}} one of the mech- anisms to hide large disk latencies. However, I/O <b>prefetching</b> in parallel applications is problematic when multiple CPUs share the same set of disks due to the possibility that <b>prefetches</b> from different CPUs can interact on shared memory caches in the I/O nodes in complex and unpredictable ways. In this paper, we (i) quantify the impact of compiler-directed I/O <b>prefetching</b> - developed originally in the context of sequential execution - on shared caches at I/O nodes. The experimental data collected shows that while I/O <b>prefetching</b> brings benefits, its effectiveness reduces significantly as the number of CPUs is increased; (ii) identify inter-CPU misses due to harmful <b>prefetches</b> as one of the main sources for this re- duction in performance with the increased number of CPUs; and (iii) propose and experimentally evaluate a profiler and compiler assisted adaptive I/O <b>prefetching</b> scheme targeting shared storage caches. The proposed scheme obtains inter-thread data sharing information using profiling and, based on the captured data sharing patterns, divides the threads into clusters and assigns a separate (customized) I/O prefetcher thread for each cluster. In our approach, the compiler generates the I/O <b>prefetching</b> threads automatically. We implemented this new I/O <b>prefetching</b> scheme using a compiler and the PVFS file system running on Linux, and the empirical data collected clearly underline the importance of adapting I/O <b>prefetching</b> based on program phases. Specifically, our pro- posed scheme improves performance, on average, by 19. 9 %, 11. 9 % and [URL] over the cases without I/O <b>prefetching,</b> with independent I/O <b>prefetching</b> (each CPU is performing compiler-directed I/O <b>prefetching</b> independently), and with one CPU <b>prefetching</b> (one CPU is reserved for <b>prefetching</b> on behalf of others), respectively, when 8 CPUs are used. Copyright 2008 ACM...|$|R
40|$|Traditional {{software}} controlled data cache <b>prefetching</b> {{is often}} ineffective {{due to the}} lack of runtime cache miss and miss address information. To overcome this limitation, we implement runtime data cache <b>prefetching</b> in the dynamic optimization system ADORE (ADaptive Object code REoptimization). Its performance has been compared with static software <b>prefetching</b> on the SPEC 2000 benchmark suite. Runtime cache <b>prefetching</b> shows better performance. On an Itanium 2 based Linux workstation, it can increase performance by more than 20 % over static <b>prefetching</b> on some benchmarks. For benchmarks that do not benefit from <b>prefetching,</b> the runtime optimization system adds only 1 %- 2 % overhead. We have also collected cache miss profiles to guide static data cache <b>prefetching</b> in the ORC R #compiler. With that information the compiler can effectively avoid generating <b>prefetches</b> for loops that hit well in the data cache...|$|R
