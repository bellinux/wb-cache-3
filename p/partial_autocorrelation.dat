199|51|Public
25|$|Finding {{appropriate}} {{values of}} p and q in the ARMA(p,q) {{model can be}} facilitated by plotting the <b>partial</b> <b>autocorrelation</b> functions for an estimate of p, and likewise using the autocorrelation functions for an estimate of q. Further information can be gleaned by considering the same functions for the residuals of a model fitted with an initial selection of p and q.|$|E
50|$|For higher-order autoregressive processes, {{the sample}} {{autocorrelation}} {{needs to be}} supplemented with a <b>partial</b> <b>autocorrelation</b> plot. The <b>partial</b> <b>autocorrelation</b> of an AR(p) process becomes zero at lag p + 1 and greater, so we examine the sample <b>partial</b> <b>autocorrelation</b> function {{to see if there}} is evidence of a departure from zero. This is usually determined by placing a 95% confidence interval on the sample <b>partial</b> <b>autocorrelation</b> plot (most software programs that generate sample autocorrelation plots also plot this confidence interval). If the software program does not generate the confidence band, it is approximately , with N denoting the sample size.|$|E
50|$|There are {{algorithms}} for {{estimating the}} <b>partial</b> <b>autocorrelation</b> {{based on the}} sample autocorrelations (Box, Jenkins, and Reinsel 2008 and Brockwell and Davis, 2009). These algorithms derive from the exact theoretical relation between the <b>partial</b> <b>autocorrelation</b> function and the autocorrelation function.|$|E
40|$|We {{compare the}} {{performance}} of the inverse and ordinary (<b>partial)</b> <b>autocorrelations</b> for time series model identification. It is found that, both in terms of Bahadur's slope and Pitman's asymptotic relative efficiency, the inverse <b>partial</b> <b>autocorrelations</b> are more efficient than the ordinary autocorrelations for identification of moving-average models. By duality, the <b>partial</b> <b>autocorrelations</b> turn out to be more powerful than the inverse autocorrelations to identify autoregressive models. Numerical experiments on both simulated and real data sets are presented to highlight the theoretical results. Copyright 2006 The Authors Journal compilation 2006 Blackwell Publishing Ltd. ...|$|R
40|$|We {{develop a}} {{technique}} for derivation of the asymptotic joint {{distribution of the}} sample <b>partial</b> <b>autocorrelations</b> of a process, given the corresponding distribution of sample autocorrelations. No assumption of asymptotic normality is needed. The underlying process need not be stationary. The technique is demonstrated through a detailed study of ARMA (1, 1) -like processes, but is applicable to other models. The results extend those of Mills and Seneta (1989) for the AR(1) -like case. The study is motivated by the known relationships and properties, especially is the classical AR(p) case, of population and sample <b>partial</b> <b>autocorrelations.</b> Sample <b>autocorrelations</b> Sample <b>partial</b> <b>autocorrelations</b> Autoregressive moving-average processes Non-stationarity...|$|R
40|$|This article {{proposes a}} new {{portmanteau}} test based on sample <b>partial</b> <b>autocorrelations.</b> The test statistic is asymptotically χ 2 under {{the null hypothesis}} of randomness. Simulation {{results indicate that the}} proposed test, which utilizes Anderson's mean and variance formulae of sample <b>partial</b> <b>autocorrelations,</b> outperforms the Ljung-Box test in terms of controlling test size and minimizing dispersion bias. ...|$|R
5000|$|Autocorrelation {{function}} (ACF) and <b>partial</b> <b>autocorrelation</b> function (PACF).|$|E
5000|$|... #Subtitle level 4: Autocorrelation and <b>partial</b> <b>autocorrelation</b> plots ...|$|E
50|$|The sample <b>partial</b> <b>autocorrelation</b> {{function}} {{is generally not}} helpful for identifying {{the order of the}} moving average process.|$|E
40|$|AbstractWe {{study the}} role of <b>partial</b> <b>autocorrelations</b> in the reparameterization and parsimonious {{modeling}} of a covariance matrix. The work is motivated by and tries to mimic the phenomenal success of the <b>partial</b> <b>autocorrelations</b> function (PACF) in model formulation, removing the positive-definiteness constraint on the autocorrelation function of a stationary time series and in reparameterizing the stationarity-invertibility domain of ARMA models. It turns out that once an order is fixed among the variables of a general random vector, then the above properties continue to hold and follow from establishing a one-to-one correspondence between a correlation matrix and its associated matrix of <b>partial</b> <b>autocorrelations.</b> Connections between the latter and {{the parameters of the}} modified Cholesky decomposition of a covariance matrix are discussed. Graphical tools similar to partial correlograms for model formulation and various priors based on the <b>partial</b> <b>autocorrelations</b> are proposed. We develop frequentist/Bayesian procedures for modelling correlation matrices, illustrate them using a real dataset, and explore their properties via simulations...|$|R
30|$|Identification of the {{potential}} models {{by looking at the}} sample <b>autocorrelations</b> and the <b>partial</b> <b>autocorrelations.</b>|$|R
40|$|AbstractA limit theorem is {{developed}} for sample <b>partial</b> <b>autocorrelations,</b> when the vector {N 12 (R(k) −mk), k= 1,…,H} converges in distribution, the R(k) being sample autocorrelations from a not-necessarily stationary process. The result {{is used to}} develop a Quenouille-type goodness-of-fit test based on sample <b>partial</b> <b>autocorrelations</b> for the simple branching process with immigration. This is compared with a test of Venkataraman (1982); and both are applied to historical data...|$|R
50|$|<b>Partial</b> <b>autocorrelation</b> plots (Box and Jenkins, Chapter 3.2, 2008) are a {{commonly}} used tool for identifying {{the order of}} an autoregressive model. The <b>partial</b> <b>autocorrelation</b> of an AR(p) process is zero at lag p + 1 and greater. If the sample autocorrelation plot indicates that an AR model may be appropriate, then the sample <b>partial</b> <b>autocorrelation</b> plot is examined to help identify the order. One looks for the point on the plot where the partial autocorrelations for all higher lags are essentially zero. Placing on the plot {{an indication of the}} sampling uncertainty of the sample PACF is helpful for this purpose: this is usually constructed on the basis that the true value of the PACF, at any given positive lag, is zero. This can be formalised as described below.|$|E
5000|$|In {{time series}} analysis, the <b>partial</b> <b>autocorrelation</b> {{function}} (sometimes [...] "partial correlation function") {{of a time}} series is defined, for lag h, as ...|$|E
50|$|The sample {{autocorrelation}} {{plot and}} the sample <b>partial</b> <b>autocorrelation</b> plot are {{compared to the}} theoretical behavior of these plots when the order is known.|$|E
40|$|We {{study the}} role of <b>partial</b> <b>autocorrelations</b> in the reparameterization and parsimonious {{modeling}} of a covariance matrix. The work is motivated by and tries to mimic the phenomenal success of the <b>partial</b> <b>autocorrelations</b> function (PACF) in model formulation, removing the positive-definiteness constraint on the autocorrelation function of a stationary time series and in reparameterizing the stationarity-invertibility domain of ARMA models. It turns out that once an order is fixed among the variables of a general random vector, then the above properties continue to hold and follow from establishing a one-to-one correspondence between a correlation matrix and its associated matrix of <b>partial</b> <b>autocorrelations.</b> Connections between the latter and {{the parameters of the}} modified Cholesky decomposition of a covariance matrix are discussed. Graphical tools similar to partial correlograms for model formulation and various priors based on the <b>partial</b> <b>autocorrelations</b> are proposed. We develop frequentist/Bayesian procedures for modelling correlation matrices, illustrate them using a real dataset, and explore their properties via simulations. Autoregressive parameters Cholesky decomposition Positive-definiteness constraint Levinson-Durbin algorithm Prediction variances Uniform and reference priors Markov chain Monte Carlo...|$|R
40|$|One of the {{difficulties}} that arise in the statistical analysis of autoregressive schemes is the very complex nature of {{the domain of the}} regression parameters. In the present paper we study an alternative parametrization of autoregressive models of finite order, namely the parametrization by the <b>partial</b> <b>autocorrelations.</b> These are shown to vary freely from - 1 to + 1 and to be in a one-to-one, continuously differentiable correspondence with the regression parameters. Properties of the asymptotic normal distribution of the maximum likelihood estimates are discussed, and we present a new deduction of Quenouille's result on the asymptotic independence of some of the estimated <b>partial</b> <b>autocorrelations.</b> Autoregressive processes <b>partial</b> <b>autocorrelations</b> variation independent parametrization domain of regression parameters asymptotic distribution of estimates asymptotic independence of estimates conditional correlations...|$|R
40|$|This paper {{deals with}} {{detection}} algorithms for impulse- radio ultrawideband transmitted-reference systems. For binary modulation schemes, we propose a novel detector that operates on the <b>partial</b> <b>autocorrelations</b> of the received signal as computed over time intervals (bins) of size {{less than the}} symbol period. Assuming that the receiver has knowledge of the <b>partial</b> <b>autocorrelations</b> of the channel response over those bins, we derive the minimum error probability detector (MEPD). In doing so, we exploit some invariance properties of the detection problem under consideration. It {{turns out that the}} receiver adopting such a strategy is superior to a conventional TR system, the more so the smaller the bin size is. Also, we address the problem of estimating the <b>partial</b> <b>autocorrelations</b> of the channel response and propose a simple solution by exploiting a training sequence. The impact of the estimation errors on the receiver performance is shown to be marginal...|$|R
50|$|The <b>partial</b> <b>autocorrelation</b> of an AR(p) {{process is}} zero at lag p + 1 and greater, so the {{appropriate}} maximum lag {{is the one}} beyond which the partial autocorrelations are all zero.|$|E
50|$|Sometimes the {{autocorrelation}} function (ACF) and <b>partial</b> <b>autocorrelation</b> function (PACF) will suggest that an MA model {{would be a}} better model choice and sometimes both AR and MA terms should be used in the same model (see Box-Jenkins method#Identify p and q).|$|E
50|$|In {{time series}} analysis, the <b>partial</b> <b>autocorrelation</b> {{function}} (PACF) gives the partial correlation {{of a time}} series with its own lagged values, controlling for {{the values of the}} time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags.|$|E
30|$|This {{is to save}} {{space and}} to {{concentrate}} on items that are commented on. For example, the <b>partial</b> <b>autocorrelations</b> are never shown. An unedited output is available (Online Resource 4).|$|R
40|$|AbstractWe {{develop a}} {{technique}} for derivation of the asymptotic joint {{distribution of the}} sample <b>partial</b> <b>autocorrelations</b> of a process, given the corresponding distribution of sample autocorrelations. No assumption of asymptotic normality is needed. The underlying process need not be stationary. The technique is demonstrated through a detailed study of ARMA (1, 1) -like processes, but is applicable to other models. The results extend those of Mills and Seneta (1989) for the AR(1) -like case. The study is motivated by the known relationships and properties, especially is the classical AR(p) case, of population and sample <b>partial</b> <b>autocorrelations...</b>|$|R
40|$|AbstractOne of the {{difficulties}} that arise in the statistical analysis of autoregressive schemes is the very complex nature of {{the domain of the}} regression parameters. In the present paper we study an alternative parametrization of autoregressive models of finite order, namely the parametrization by the <b>partial</b> <b>autocorrelations.</b> These are shown to vary freely from − 1 to + 1 and to be in a one-to-one, continuously differentiable correspondence with the regression parameters. Properties of the asymptotic normal distribution of the maximum likelihood estimates are discussed, and we present a new deduction of Quenouille's result on the asymptotic independence of some of the estimated <b>partial</b> <b>autocorrelations...</b>|$|R
5000|$|Given a {{time series}} , the <b>partial</b> <b>{{autocorrelation}}</b> of lag k, denoted , is the autocorrelation between [...] and [...] with the linear dependence of [...] on [...] through [...] removed; equivalently, {{it is the}} autocorrelation between [...] and [...] that is not accounted for by lags 1 to k − 1, inclusive.|$|E
50|$|Model {{identification}} and model selection: {{making sure that}} the variables are stationary, identifying seasonality in the dependent series (seasonally differencing it if necessary), and using plots of the autocorrelation and <b>partial</b> <b>autocorrelation</b> functions of the dependent time series to decide which (if any) autoregressive or moving average component should be used in the model.|$|E
50|$|In practice, {{the sample}} {{autocorrelation}} and <b>partial</b> <b>autocorrelation</b> functions are random variables {{and do not}} give the same picture as the theoretical functions. This makes the model identification more difficult. In particular, mixed models can be particularly difficult to identify. Although experience is helpful, developing good models using these sample plots can involve much trial and error.|$|E
40|$|Structural vector autoregressions allow {{dependence}} among contemporaneous variables. If such {{models have}} a recursive structure, {{the relationships among}} the variables can be represented by directed acyclic graphs. The identification of these relationships for stationary series may be enabled by {{the examination of the}} conditional independence graph constructed from sample <b>partial</b> <b>autocorrelations</b> of the observed series. In this article, we extend this approach to the case when the series follows an I(1) vector autoregression. For such a model, estimated regression coefficients may have non-standard asymptotic distributions and in small samples this affects the distribution of sample <b>partial</b> <b>autocorrelations.</b> We show that, nevertheless, in large samples, exactly the same inference procedures may be applied as in the stationary case. Copyright 2008 The Authors. Journal compilation 2008 Blackwell Publishing Ltd...|$|R
40|$|AbstractIt {{is shown}} {{that for a}} data set from a {{branching}} process with immigration, where the offspring distribution is Bernoulli and the immigration distribution is Poisson, the normed sample <b>partial</b> <b>autocorrelations</b> are asymptotically independent. This makes possible a goodness-of-fit test of known (Quenouille) form. The underlying process is a classical model in statistical mechanics...|$|R
40|$|A Bayesian {{approach}} is presented for estimating nonparametrically an additive regression model with autocorrelated errors. Each of the potentially nonlinear components is modeled as a regression spline using many knots, while the errors are modeled {{by a high}} order stationary autoregressive process parameterized {{in terms of its}} <b>partial</b> <b>autocorrelations.</b> Significant knots and <b>partial</b> <b>autocorrelations</b> are selected using variable selection. All aspects of the model are estimated simultaneously using Markov chain Monte Carlo. It is shown empirically that the proposed approach works well on a number of simulated examples. 1 Introduction When a regression model is fitted to time series data the errors are often likely to be autocorrelated, such as in the problems tackled by Engle, Granger, Rice and Weiss (1986) and Harvey and Koopman (1993). Few approaches are currently available for estimating a regression model nonparametrically when the errors are autocorrelated, despite the fact that fai [...] ...|$|R
50|$|Finding {{appropriate}} {{values of}} p and q in the ARMA(p,q) {{model can be}} facilitated by plotting the <b>partial</b> <b>autocorrelation</b> functions for an estimate of p, and likewise using the autocorrelation functions for an estimate of q. Further information can be gleaned by considering the same functions for the residuals of a model fitted with an initial selection of p and q.|$|E
5000|$|Once {{stationarity}} and seasonality {{have been}} addressed, {{the next step}} is to identify the order (i.e. the p and q) of the autoregressive and moving average terms. Different authors have different approaches for identifying p and q. Brockwell and Davis (1991) state [...] "our prime criterion for model selection ARMA(p,q) models will be the AICc", i.e. the Akaike information criterion with correction. Other authors use the autocorrelation plot and the <b>partial</b> <b>autocorrelation</b> plot, described below.|$|E
50|$|Box and Jenkins {{proposed}} a three-stage methodology involving model identification, estimation and validation. The identification stage involves identifying if {{the series is}} stationary or not {{and the presence of}} seasonality by examining plots of the series, autocorrelation and <b>partial</b> <b>autocorrelation</b> functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.|$|E
40|$|Many {{parameters}} and positive-definiteness {{are two major}} obstacles in estimating and modelling a correlation matrix for longitudinal data. In addition, when longitudinal data is incomplete, incorrectly modelling the correlation matrix often results in bias in estimating mean regression parameters. In this paper, we introduce a flexible and parsimonious class of regression models for a covariance matrix parameterized using marginal variances and <b>partial</b> <b>autocorrelations.</b> The <b>partial</b> <b>autocorrelations</b> can freely vary in the interval (− 1, 1) while maintaining positive defi-niteness of the correlation matrix so the regression parameters in these models will have no con-straints. We propose a class of priors for the regression coefficients and examine the importance of correctly modeling the correlation structure on estimation of longitudinal (mean) trajectories {{and the performance of}} the DIC is choosing the correct correlation model via simulations. The regression approach is illustrated on data from a longitudinal clinical trial...|$|R
40|$|AbstractOur aim is {{to suggest}} ways of {{improving}} time-domain modelling, {{for the purpose of}} more effective forecasting, by better interpretation of the sample <b>autocorrelations</b> and <b>partial</b> <b>autocorrelations</b> obtained from raw time-series data. For this objective, we assume no specialist knowledge, as we start by surveying all those standard ideas of univariate analysis which are needed for the subsequent development of our thesis...|$|R
40|$|For {{a binary}} {{pseudorandom}} sequence {Si} with period N, the <b>partial</b> period <b>autocorrelation</b> function AS(τ, k, D) {{is defined by}} correlating {{the portion of the}} sequence within a window of size D, and start position k, with the portion in another window of the same size but starting τ steps later in the sequence. A distribution of possible <b>partial</b> period <b>autocorrelation</b> values is obtained by allowing the start position k to vary over all possible values 0 ≤ k < N. The expectation value is proportional to the periodic autocorrelation function AS(τ). In this paper the variance in the <b>partial</b> period <b>autocorrelation</b> values is estimated for a large class of binary pseudorandom sequences, the so-called “geometric sequences ”. An estimate is given for the minimum window size D which is needed in order to guarantee (with probability of error less than ɛ), that a signal has been synchronized, based on measurement of a single <b>partial</b> period <b>autocorrelation</b> value...|$|R
