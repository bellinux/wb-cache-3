4|10000|Public
50|$|The ideas {{presented}} above concretely expose two FOSD principles. The Principle of Uniformity {{states that}} all program artifacts are treated and modified {{in the same}} way. (This is evidenced by deltas for different artifact types above). The <b>Principle</b> <b>of</b> <b>Scalability</b> states all levels of abstractions are treated uniformly. (This {{gives rise to the}} hierarchical nesting of tuples above).|$|E
40|$|International audienceVideo Indexing {{technique}} {{is crucial in}} multimedia applications. In the case of HD (High Definition) Video, the <b>principle</b> <b>of</b> <b>scalability</b> is of great importance. The wavelet decomposition used in the MJPEG 2000 provides this property. In this paper, two descriptors for indexing of HD Video are presented. The first one {{is based on the}} GM (Global Motion) of the Video and the correlation matrix is used as similarity measure for matching. The second one is object-oriented and uses histograms in the wavelet domain as indexes; similarity measure is then based on histogram intersection. The complete process of extraction of objects of interest and to determine GM are also detailed...|$|E
40|$|Video {{indexing}} technique {{is crucial in}} multimedia applications. In the case of HD (High Definition) Video, the <b>principle</b> <b>of</b> <b>scalability</b> is of great importance. The wavelet decomposition used in the JPEG 2000 standard provides this property. In this paper, we propose a scalable descriptor based on objects. First, a scalable moving object extraction method is constructed. Using the wavelet data, it relies on {{the combination of a}} robust global motion estimation with a morphological color segmentation at a low spatial resolution. It is then refined using the scalable order of data. Second, a descriptor is built only on the objects found at the previous step. This descriptor is based on multiscale histograms of wavelet coefficients of moving object...|$|E
5000|$|The {{original}} {{implementation of}} AHEAD is the AHEAD Tool Suite and Jak language, which exhibits both the <b>Principles</b> <b>of</b> Uniformity and <b>Scalability.</b> Next-generation tools include CIDE ...|$|R
50|$|Power {{scaling of}} a laser is {{increasing}} its output power without changing the geometry, shape, or <b>principle</b> <b>of</b> operation. Power <b>scalability</b> {{is considered an}} important advantage in a laser design.|$|R
40|$|The {{complexity}} of transnational research programming and requisite large scale stakeholder engagement set a major managerial challenge how to prepare, run and evaluate such activities in an effective, efficient and appropriate {{as well as}} transparent, open and inclusive manner. To address such co-ordination challenges we specify dimensions of transnational, vertical, horizontal and temporal co-ordination and apply them to three cases on foresight processes in connection with transnational research programming. This provides some evidence on a potentially significant role for foresight in facilitating and integrating different functions of programming but also shows major challenges in foresight design and management, which we address by way of elaborating guiding foresight <b>principles</b> <b>of</b> <b>scalability,</b> modularity and flexibility. We also consider the potential role of foresight in Joint Programming in Europe and in transnational research programming elsewhere. JRC. J. 2 -Knowledge for Growt...|$|R
30|$|Since {{the body}} of {{knowledge}} {{in the field of}} vulnerability grows steadily and more and more research fields are intersecting in dealing with vulnerability and related concepts, such as resilience, the ontology should be able to keep pace with this development. This is captured by the <b>principle</b> <b>of</b> <b>scalability,</b> which refers to the fact that ontologies should be “… easily extendable to enable specialized domains to build upon more general ontologies already generated” (Raskin and Pan 2005, p. 1121). This principle was included in keeping a level of openness and awareness of related concepts (such as resilience), but also in the different fields of vulnerability assessment (disaster risk reduction, climate change, development studies) in the group discussions. Despite selecting key vulnerability assessments from as wide a range of disciplines as possible, the ontology proposed in VuWiki is shaped (1) by the selection of the initial 55 assessments to develop and test the ontology, which has a focus on social vulnerability assessments in the context of disaster risk reduction; and (2) by our interpretation of the literature and how the initial 45 assessments and the 10 test cases were used for developing the terminological structure of the ontology. To this extent, like in most taxonomic approaches in vulnerability research or social science in general, we will always encounter some difficulties in “classifying” all potential objects by a predefined set of categories and properties. Due to the <b>principle</b> <b>of</b> <b>scalability</b> and due to our selection and interpretation of the initial assessments, and also due to the principles of natural language and application independence, the ontology developed here may not satisfy the representation of concepts and their interrelations as used in the strict perspective or technical jargon of one discipline. Nevertheless, we expect that the four key questions that form the basic structure of the ontology (on the first level) capture the core dimensions and approaches in vulnerability assessment regardless of particular disciplinary backgrounds. At the same time, we expect that the ontology provides a conceptual foundation that can incorporate a range of additional dimensions and concepts in vulnerability, which currently are not considered, and that the ontology can be extended rather flexibly {{in a way that will}} not require a total revision of the existing structure.|$|E
40|$|Networking is {{undergoing}} a transformation throughout our industry. The need for scalable network control and automation shifts the focus from hardware driven products with ad hoc control to Software Defined Networks. This process is now well underway. In this paper, we adopt {{the perspective of the}} Promise Theory to examine the current and future states of networking technologies. The goal is to see beyond specific technologies, topologies and approaches and define principles. Promise Theory's bottom-up modeling has been applied to server management for many years and lends itself to <b>principles</b> <b>of</b> self-healing, <b>scalability</b> and robustness...|$|R
40|$|Manufacturing {{industries}} in the 21 st century will face unpredictable, high-frequency market changes driven by globalization. Consequently, industries have to shift to more costeffective production systems. Reconfigurability can provide costeffective and quick reaction to market changes. This paper discloses a new paradigm in Reconfigurable Machining Systems, RMS, that enables flexibility, by the <b>principle</b> <b>of</b> modularity, integrability, <b>scalability</b> and convertibility in an easy manner. The architecture, characteristics and advantages of the new Lego-like RMS are presented and discussed...|$|R
30|$|The JC:HEM {{pilot project}} {{commenced}} in September 2010, {{in partnership with}} Jesuit Refugee Service (JRS), following a needs assessment which documented a demand for tertiary education among the refugee population. JC:HEM was invited by JRS to serve refugees living in Dzaleka camp in Malawi, Kakuma camp in Kenya, and urban refugees living in Amman, Jordan. An original pilot site included Aleppo, Syria, but implementation at this site was postponed during the war. The pilot phase was guided by the <b>principles</b> <b>of</b> sustainability, <b>scalability,</b> and transferability, using technology to identify program structures in keeping with these principles (Jesuit Commons: Higher Education at the Margins [JC:HEM], 2010). After identifying needs for tertiary education amongst refugee populations, the JC:HEM program leveraged the increased availability of technology and improved internet connectivity across Africa to implement distance learning in the pilot sites (Dankova & Giner, 2011). The pilot phase of JC:HEM concluded in December 2013 and entered a 2 -year capacity-building phase. JC:HEM now operates under the name Jesuit Worldwide Learning (JWL, 2017).|$|R
40|$|RMS aims at {{providing}} sufficient flexibility by the <b>principle</b> <b>of</b> modularity, integrability, <b>scalability</b> in {{a shorter}} {{period of time}} while the other manufacturing system provides generalized flexibility designed for anticipation variation. Here we have presented the different components of manufacturing system, comparison of manufacturing system, their merits,demerits. capabilities,characteristics which plays an important role. In this thesis we have analyses design the automotive bumper system {{with the help of}} manufacturing system design decomposition and replaced the experimental testing for the impact test by finite element analysis technique. we can improve any system if we focus on the strategic design and systematic procedure. Thus RMS is an optimized system configuration and economic machining system that fits the customer requirements. ...|$|R
40|$|Assessors often {{diminish}} communicating risk to show {{a single}} category or color without providing a full context of the evaluation, basis, and assumptions behind the risk assessment. We attempt to remedy that by presenting an approach to communicate risk assessments more completely with {{a clearer understanding of}} these issues. First, we specify assessor should present necessary information as part of a standard risk assessment statement. This information is discussed in four groups: 1) the activity or a collection of activities being assessed, 2) the context of the assessment (who made it, when, with what scope, and how rigorously), 3) setting of the assessment (scenario, assumed conditions, timeframe, assumed choices, and mitigation measures), and 4) the resulting assessment. Second, we propose an approach to standardize the presentation of the actual assessment by applying the <b>principles</b> <b>of</b> simplicity, <b>scalability,</b> and consistency. The assessor needs to develop outcome-centric measures for key activities to provide a basis to assess the potential consequences, determine the success and failure points of the activity, and present the expected outcome for each scenario setting. We standardize the presentation of the risk assessments as categorical risks, such as colored ranges, by apportioning the expected consequences on the metric scales. We discuss combining assessments for a single activity and for an aggregate activity. The United States Air Force has implemented both our standard risk statement and our presentation approach...|$|R
40|$|Abstract. The Semantic Web {{brings a}} {{powerful}} set of concepts, standards {{and ideas that}} are already changing {{the shape of the}} Web. However, in order to put these notions into practice we need to translate them into code. That is why the broad notion of programming the Semantic Web is important, even if it remains challenging to provide the appropriate tools for developers to implement these ideas. In this work, we present our experience in a series of common Semantic Web programming tasks, and how we improve the developer experience and productivity by providing a simple Domain Specific Language (DSL) in Scala. We exemplify its use through fundamental tasks such as dealing with RDF data and ontologies, or performing reasoning and querying using JVM languages. In particular, we show how we can profit from the intrinsic extensibility of the Scala language to simplify these tasks using a custom DSL, {{at the same time that}} we introduce <b>principles</b> <b>of</b> reactivity and <b>scalability</b> in the code. ...|$|R
40|$|The {{purpose of}} this paper is to examine the praxis of the multi-dimensional {{components}} of H. Igor Ansoff¹s Strategic Management Systems and the efficacy of use of each of its components relative to the formulation and implementation of corporate level strategy in for-profit, not-for-profit, small and medium sized enterprises. &# 13; Based on empirically validated research and industry supported journals, substantial evidence endorse both an implicit and explicit acknowledgement of its applicability and it¹s value as a whole or in part providing increased financial performance for firms competing in turbulent environments. &# 13; There is a notable variance within industry and academia concerning the asymmetry of costs incurred relative to the benefits achieved of the successful implementation of Ansoffian Strategy within Small and Medium sized enterprises. Although the <b>principles</b> and processes <b>of</b> <b>scalability</b> are still conceptually at an embryonic stage, this paper will illuminate those features of Ansoff’s Strategic Management Systems and conditions for optimal use. &# 13; Finally, we will discuss Ansoff’s Strategic Success Paradigm and principles for use by Small and Medium Sized Enterprises, which when implemented have proven empirically to increase the firm’s probability of strategic success...|$|R
5000|$|<b>Principle</b> <b>of</b> Cheap Design and Redundancy: Pfeifer {{realized}} that implicit assumptions made by engineers often substantially influence a control architecture's complexity. This insight {{is reflected in}} discussions <b>of</b> the <b>scalability</b> problem in robotics. The internal processing needed for some bad architectures can grow {{out of proportion to}} new tasks needed of an agent.|$|R
40|$|The term {{scalability}} appears {{frequently in}} computing literature, {{but it is}} a term that is poorly defined and poorly understood. The lack of a clear, consistent and systematic treatment <b>of</b> <b>scalability</b> makes it difficult to evaluate claims <b>of</b> <b>scalability</b> and to compare claims from different sources. This paper presents a framework for precisely characterizing and analyzing the <b>scalability</b> <b>of</b> a software system. The framework treats scalability as a multi-criteria optimization problem and captures the dependency relationships that underlie typical notions <b>of</b> <b>scalability.</b> The paper presents the results of a case study in which the framework and analysis method were applied to a real-world system, demonstrating {{that it is possible to}} develop a precise, systematic characterization <b>of</b> <b>scalability</b> and to use the characterization to compare the <b>scalability</b> <b>of</b> alternative system designs. Categories and Subject Descriptors C. 4 [Computer Systems Organization]: Performance of Systems—design studies, measurement techniques, modelin...|$|R
40|$|The design <b>principle</b> <b>of</b> maximizing local {{autonomy}} {{except when it}} conflicts with global robustness {{has led to a}} scalable Internet with enormous heterogeneity of both applications and infrastructure. These properties have not been achieved in the mechanisms for specifying and enforcing security policies. The STRONGMAN (for Scalable TRust Of Next Generation MANagement) system [9], [10] offers three new approaches to <b>scalability,</b> applying the <b>principle</b> <b>of</b> local policy enforcement complying with global security policies. First is the use of a compliance checker to provide great {{local autonomy}} within the constraints of a global security policy. Second is a mechanism to compose policy rules into a coherent enforceable set, e. g., at the boundaries of two locally autonomous application domains. Third is the "lazy instantiation" of policies {{to reduce the amount of}} state that enforcement points need to maintain. In this paper, we focus on the issues <b>of</b> <b>scalability</b> and heterogeneity...|$|R
40|$|Abstract – We {{present a}} physics-based {{approach}} to the lo-calization of chemical sources with autonomous swarms. Robotic vehicles with short-range sensors use local pair-wise interactions to self-assemble into structured lattice for-mations, serving as distributed sensor and computational meshes. The robots use fluid flow information to navigate toward the chemical emitter. We develop a new search algo-rithm from first <b>principles</b> <b>of</b> fluid mechanics that outperforms the leading biomimetic competitors for the chemical source localization task. A validation <b>of</b> the <b>scalability</b> <b>of</b> our solu-tion via simulation of plume and vehicle dynamics is given...|$|R
40|$|Abstract — The design <b>principle</b> <b>of</b> maximizing local {{autonomy}} {{except when it}} conflicts with global robustness {{has led to a}} scalable Internet with enormous heterogeneity of both applications and infrastructure. These properties have not been achieved in the mechanisms for specifying and enforcing security policies. The STRONGMAN (for Scalable TRust Of Next Generation MANagement) system [9], [10] offers three new approaches to <b>scalability,</b> applying the <b>principle</b> <b>of</b> local policy enforcement complying with global security policies. First is the use of a compliance checker to provide great {{local autonomy}} within the constraints of a global security policy. Second is a mechanism to compose policy rules into a coherent enforceable set, e. g., at the boundaries of two locally autonomous application domains. Third is the “lazy instantiation ” of policies {{to reduce the amount of}} state that enforcement points need to maintain. In this paper, we focus on the issues <b>of</b> <b>scalability</b> and heterogeneity. I...|$|R
40|$|The term {{scalability}} appears {{frequently in}} computing literature, {{but it is}} a term that is poorly defined and poorly understood. It is an important attribute of computer systems that is frequently asserted but rarely validated in any meaningful, systematic way. The lack of a consistent, uniform and systematic treatment <b>of</b> <b>scalability</b> makes it difficult to identify and avoid scalability problems, clearly and objectively describe the <b>scalability</b> <b>of</b> software systems, evaluate claims <b>of</b> <b>scalability,</b> and compare claims from different sources. This thesis provides a definition <b>of</b> <b>scalability</b> and describes a systematic framework for the characterization and analysis <b>of</b> software systems <b>scalability.</b> The framework is comprised of a goal-oriented approach for describing, modeling and reasoning about scalability requirements, and an analysis technique that captures the dependency relationships that underlie typical notions <b>of</b> <b>scalability.</b> The framework is validated against a real-world data analysis system and is used to recast a number of examples taken from the computing literature and from industry in order to demonstrate its use across different application domains and system designs. ...|$|R
40|$|The word "scalability" {{is used in}} {{a variety}} of ways by {{different}} simulation communities. This paper describes some of the more common usages and presents a general, unifying definition <b>of</b> simulation <b>scalability</b> which addresses the intent of these differing usages. Many common definitions <b>of</b> <b>scalability</b> can be viewed as simple restrictions <b>of</b> this multivariate <b>scalability</b> function to some subset of the variables in its domain. The quantitative nature of this definition allows systems to be compared based on their <b>scalability</b> instead <b>of</b> their relative performance at some level of capability. The utility of the proposed general and restricted definitions <b>of</b> <b>scalability</b> is discussed...|$|R
30|$|Quantitative {{evaluation}} <b>of</b> <b>scalability</b> for {{big data}} (including {{hundreds of thousands}} of nodes) with increased resolutions.|$|R
30|$|The {{provisioning}} of {{data has}} to be performed with high reactivity and high level <b>of</b> <b>scalability.</b>|$|R
5000|$|IASO Backup : Advanced data {{reduction}} technology. Data de-duplication mechanism. High level <b>of</b> <b>scalability</b> and cost effectiveness.|$|R
3000|$|... where K is {{the point}} number <b>of</b> <b>scalability,</b> and U and V are {{addition}} and multiplication operations, respectively.|$|R
50|$|MOSES and Intel Corporation partner under Cooperative Research and Development Agreement #ARL-15-00xx. MOSES/Intel {{performs}} series <b>of</b> <b>scalability</b> experiments.|$|R
5000|$|Best {{practices}} {{to avoid the}} problems of traditional JavaScript programming (such as browser inconsistencies and lack <b>of</b> <b>scalability)</b> ...|$|R
40|$|Abstract. Learning {{to solve}} small {{instances}} {{of a problem}} should help in solving large instances. Unfortunately, most neural network architectures do not exhibit this form <b>of</b> <b>scalability.</b> Our Multi-Dimensional Recurrent LSTM Networks, however, show a high degree <b>of</b> <b>scalability,</b> as we empirically show {{in the domain of}} flexible-size board games. This allows them to be trained from scratch up to the level of human beginners, without using domain knowledge. ...|$|R
50|$|Percona freely {{includes}} a number <b>of</b> <b>scalability,</b> availability, security and backup features only available in MySQL's commercial Enterprise edition.|$|R
30|$|The {{resulting}} dataset gave us {{the possibility}} to evaluate the performance of both our approach and other selected algorithms in terms <b>of</b> <b>scalability</b> on the number <b>of</b> pixels and <b>scalability</b> {{on the number of}} labels (density).|$|R
40|$|Title: Implementation of {{selected}} database operations in parallel environment Author: Bc. Ján Majdan Department: Department of Software Engineering Supervisor: RNDr. David Bednárek, Ph. D. Abstract: This thesis describes several design possibilities of database operations {{in a parallel}} environment called Bobox. First, the thesis covers theory of da- tabases as well as algorithms needed to implement database operations in main memory. Next part discusses the usage of hash tables to implement parallel data- base operations. The key principles discussed contain hashing approaches based on a shared hash table, as well as <b>principles</b> <b>of</b> data preprocessing (local hash Table). The thesis then describes practical usage {{of selected}} database operations, which were implemented based on the principles described in the first theory sec- tions. The implementation also involves performance measurement {{at different levels of}} parallelism. Finally, acquired results are analyzed and discussed in terms <b>of</b> <b>scalability</b> <b>of</b> parallelism and performance in the Bobox environment. Keywords: database operations, Bobox, parallelism, hashing, data preprocessing...|$|R
40|$|This subject {{will provide}} {{students}} with: skills for generating 3 D VR worlds; animation techniques; visualization and rendering techniques; VR interfaces and devices; the <b>principles</b> <b>of</b> development for large VR worlds; dynamics and persistence of VR environments;. evolution and <b>scalability</b> <b>of</b> VR; VR applications: 3 D games, movies and special effects, GIS, aerospace...|$|R
5000|$|Among {{the methods}} for {{addressing}} the problems <b>of</b> the <b>scalability</b> <b>of</b> many tabs: ...|$|R
50|$|Database {{services}} {{take care}} <b>of</b> <b>scalability</b> and high {{availability of the}} database. Database services make the underlying software-stack transparent to the user.|$|R
40|$|Information {{retrieval}} applications increasingly have to {{deal with}} multimedia content. Since image and video databases became ever larger, scalability is a critical requirement for visual information retrieval. This chapter first describes the types of processes that support either content-based retrieval or mining and have to scale. The nature of the problems to be solved and the <b>principle</b> <b>of</b> the solutions are presented next. An emphasis is put on key ideas supporting recent progress, like the use of approximation or of shared-neighbor similarity. To keep the pace with the evolution <b>of</b> <b>scalability</b> requirements, due to more complex visual descriptions and higher volumes of data, further advances are needed. Embeddings, filtering based on simplified descriptions, optimization of content representations and distributed processing are a few directions that deserve being followed...|$|R
5000|$|Within {{evolutionary}} computation, {{the need}} for artificial development techniques was motivated by the perceived lack <b>of</b> <b>scalability</b> and evolvability <b>of</b> direct solution encodings (Tufte, 2008). Artificial development entails indirect solution encoding. Rather than describing a solution directly, an indirect encoding describes (either explicitly or implicitly) {{the process by which}} a solution is constructed. Often, but not always, these indirect encodings are based upon biological <b>principles</b> <b>of</b> development such as morphogen gradients, cell division and cellular differentiation (e.g. Doursat 2008), gene regulatory networks (e.g. Guo et al., 2009), [...] degeneracy (Whitacre et al., 2010), grammatical evolution (de Salabert et al., 2006), or analogous computational processes such as re-writing, iteration, and time. The influences of interaction with the environment, spatiality and physical constraints on differentiated multi-cellular development have been investigated more recently (e.g. Knabe et al. 2008).|$|R
