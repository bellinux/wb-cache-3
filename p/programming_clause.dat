0|232|Public
30|$|The atom α in (1) {{is called}} {{the head of the}} <b>program</b> <b>clause.</b> If p is the {{predicate}} of α then the clause is called a <b>program</b> <b>clause</b> defining p. The formula at the left-hand side of → in (1) {{is called the}} body of the <b>program</b> <b>clause.</b>|$|R
30|$|An eDatalog^program is a finite set of eDatalog ^ <b>program</b> <b>clauses.</b> An eDatalog^knowledge base is a pair 〈P,A〉 {{consisting}} of an eDatalog^ program P and an ABox A. A query is defined to be a formula {{that can be}} the body of an eDatalog ^ <b>program</b> <b>clause.</b>|$|R
3000|$|Let T = (5), [...]..., (17). It is a stratifiable TBox. Only (11) is a DL TBox axiom, {{while the}} other axioms are eDatalog^ <b>program</b> <b>clauses.</b> The <b>program</b> <b>clauses</b> (5), (13), (15) and (17) can also be {{expressed}} as DL TBox axioms, treating withGarden, acceptable, excluded_ 1, preferable_ 1, excluded_ 2, preferable_ 2, excluded_ 3 and mayRent as concept names.|$|R
40|$|Synaptic weights for neurons in logic {{programming}} {{can be calculated}} either by using Hebbian learning or by Wan Abdullah’s method. In other words, Hebbian learning for governing events corresponding to some respective <b>program</b> <b>clauses</b> is equivalent with learning using Wan Abdullah’s method for the same respective <b>program</b> <b>clauses.</b> In this paper we will evaluate experimentally the equivalence between {{these two types of}} learning through computer simulations. </p...|$|R
40|$|In {{this paper}} {{definite}} Horn <b>clause</b> <b>programs</b> are investigated within a proof-theoretic framework; <b>program</b> <b>clauses</b> being considered rules {{of a formal}} system. Based on this approach, the soundness and completeness of SLD-resolution is established by purely proof-theoretic methods. Extended Horn clauses are defined as rules of higher levels and related to an approach based on implication formulae in the bodies of clauses. In a further extension, which is treated in Part II of this series, <b>program</b> <b>clauses</b> are viewed as clauses in inductive definitions of atoms, justifying an additional inference schema: a reflection principle that roughly corresponds to interpreting the <b>program</b> <b>clauses</b> as introduction rules {{in the sense of}} natural deduction. The evaluation procedures for queries with respect to the defined extensions of definite Horn <b>clause</b> <b>programs</b> are shown to be sound and complete. The sequent calculus with the general elimination schema even permits the introduction of a genuine notion of falsity which is not defined via a meta-rule...|$|R
30|$|The program P {{given in}} Example  1 is a {{stratified}} eDatalog^ program with two strata. Each <b>program</b> <b>clause</b> of P forms a stratum.|$|R
30|$|A (WORL) TBox axiom {{is either}} a DL TBox axiom (as defined by (3)) or an RBox axiom (as defined by (4)) or an eDatalog^ <b>program</b> <b>clause.</b>|$|R
40|$|Machine (WAM). The {{provision}} of implications in goals {{results in the}} possibility of <b>program</b> <b>clauses</b> being added to the program for the purpose of solving specific subgoals. A naive scheme based on asserting and retracting <b>program</b> <b>clauses</b> does not suffice for implementing such additions for two reasons. First, it is necessary to also support the resurrection of an earlier existing program in the face of backtracking. Second, the possibility for implication goals to be surrounded by quantifiers requires a consideration of the parameterization of <b>program</b> <b>clauses</b> by bindings for their free variables. Devices for supporting these additional requirements are described as also is the integration of these devices into the WAM. Further extensions to the machine are outlined for handling higher-order additions to the language. The ideas Work on this paper has been partially supported by NSF Grants CCR- 89 - 05825 and CCR [...] 92 - 08465. Address correspondence to Gopalan Nadathur, Department of Compute [...] ...|$|R
25|$|Subgoals {{encountered}} in a query evaluation are maintained in a table, along with {{answers to these}} subgoals. If a subgoal is re-encountered, the evaluation reuses information from the table rather than re-performing resolution against <b>program</b> <b>clauses.</b>|$|R
3000|$|... when π _ 2 is {{applicable}} to a formula ψ of predicate logic, π _ 2 (ψ [...]) {{is a set}} of eDatalog^ <b>program</b> <b>clauses</b> such that, for any interpretation I, Iπ _ 2 (ψ [...]) iff Iψ.|$|R
3000|$|Let 〈P,A〉 be an eDatalog^ {{knowledge}} base. By P^ gr _A we {{denote the}} set of all ground instances of the <b>program</b> <b>clauses</b> of P∪ EqAxioms [...] that use only individuals and data constants occurring in P or A.|$|R
50|$|Another way of {{defining}} STL {{would be as}} a 'script generation language'; its <b>programming</b> <b>clauses</b> are identical to REXX, {{but they need to}} be translated (i.e. 'script-generated') into the TPNS language in order to be executable during the simulation run.|$|R
3000|$|... {{for every}} eDatalog^ <b>program</b> <b>clause</b> φ in T∪ EqAxioms, if q is the {{predicate}} {{of the head}} of φ and p is a predicate from DPreds that occurs in the body of φ then f(p) < f(q), and additionally, if p occurs under negation in φ then f(p) < f(q); [...]...|$|R
5000|$|The {{bottom-up}} {{evaluation strategy}} maintains {{the set of}} facts proved so far during evaluation. This set is initially empty. With each step, new facts are derived by applying a <b>program</b> <b>clause</b> to the existing facts, and {{are added to the}} set. For example, the bottom up evaluation of the following program requires two steps: ...|$|R
40|$|This paper {{examines}} the implementation issues arising from introducing universally quantified goals and {{some forms of}} implication goals in a logic programming language. While these constructs are useful in providing a logical approach to lexical scoping, they raise new implementation problems. Since universal and existential quantifiers may appear in mixed order in goals, the unification operation used for solving existential goals must respect this ordering. We present {{a solution to this}} problem based on a tagging of constants and variables and the use of these tags in constraining unification. This solution is amenable to an efficient implementation and can be easily assimilated into the machinery of the Warren Abstract Machine (WAM). Implication goals require new <b>program</b> <b>clauses</b> to be introduced dynamically for the purpose of solving specific subgoals. A naive scheme based on asserting and retracting <b>program</b> <b>clauses</b> does not suffice to solve this problem, for two reasons: First, bac [...] ...|$|R
2500|$|The logical {{status of}} {{negation}} as failure was unresolved until Keith Clark [...] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation {{with respect to the}} completion of the program. Completion amounts roughly to regarding the set of all the <b>program</b> <b>clauses</b> with the same predicate on the left hand side, say ...|$|R
40|$|We {{present a}} {{systematic}} reconstruction of a compilation method for an extension to logic programming that permits procedure definitions {{to be given}} a scope. At a logical level, this possibility is realized by permitting implications to be embedded in goals. <b>Program</b> <b>clauses</b> that appear in the antecedents of such implications may contain variables that are bound by external quantifiers, leading to non-local variables in procedure declarations. In compiling programs in this extended language, there is, therefore, a need to consider the addition to given <b>programs</b> of <b>program</b> <b>clauses</b> that are parameterized by bindings for some of their variables. A proposed approach to dealing with this aspect uses a closure representation for clauses. This representation separates an instance of a clause with parameterized bindings into a skeleton part that is fixed at compile-time and an environment that maintains the part that is dynamically determined. A development of this implementation scheme is pro [...] ...|$|R
40|$|AbstractThe {{inclusion}} of universal quantification and {{a form of}} implication in goals in logic programming is considered. These additions provide a logical basis for scoping, but they also raise new implementation problems. When universal and existential quantifiers are permitted to appear in mixed order in goals, the devices of logic variables and unification that are employed in solving existential goals must be modified to ensure that constraints arising out {{of the order of}} quantification are respected. Suitable modifications that are based on attaching numerical tags to constants and variables and on using these tags in unification are described. The resulting devices are amenable to an efficient implementation and can, in fact, be assimilated easily into the usual machinery of the Warren Abstract Machine (WAM). The provision of implications in goals results in the possibility of <b>program</b> <b>clauses</b> being added to the program for the purpose of solving specific subgoals. A naive scheme based on asserting and retracting <b>program</b> <b>clauses</b> does not suffice for implementing such additions for two reasons. First, it is necessary to also support the resurrection of an earlier existing program in the face of backtracking. Second, the possibility for implication goals to be surrounded by quantifiers requires a consideration of the parameterization of <b>program</b> <b>clauses</b> by bindings for their free variables. Devices for supporting these additional requirements are described as also is the integration of these devices into the WAM. Further extensions to the machine are outlined for handling higher-order additions to the language. The ideas presented here are relevant to the implementation of the higher-order logic programming language λProlog...|$|R
30|$|To reflect {{modularity}} of ontologies (e.g., {{the import}} feature of ontologies), we define a knowledge base to be a hierarchy of layers (a tree or a rooted {{directed acyclic graph}} of layers). Each layer in turn may be stratifiable and divided further into strata. The granulation is not substantial for the well-founded semantics, as the whole knowledge base will be flattened to a set of <b>program</b> <b>clauses</b> and facts.|$|R
40|$|AbstractSome of {{the basic}} results {{in the theory of}} logic programming, e. g. the mgu lemma, lifting lemma and {{completeness}} theorem, have been incorrectly stated in standard texts. We explain how these errors arise and how they may be avoided by suitable “standardising apart” of the <b>program</b> <b>clauses</b> used. We also discuss the significance of standardising apart for other purposes, e. g. soundness, obtaining most general answer...|$|R
40|$|AbstractUnification {{complexity}} of Horn <b>clause</b> <b>programs</b> is introduced, and its complexity is investigated for various classes of universal Horn formulas. A faithful simulation theorem is proved which associates with every k-tape Turing machine a Horn <b>clause</b> <b>program</b> requiring exactly as many unification steps as the Turing machine. From this {{it follows that}} Horn <b>clause</b> <b>programs</b> are computationally complete even {{in the case of}} bi-Horn (=Krom) formulas, and that the unification {{complexity of}} Horn <b>clause</b> <b>programs</b> is not recursively bounded. The faithful simulation theorem is also used to give a new interpretation to hierarchy theorems in the context of logic programming...|$|R
50|$|By the {{aftermath}} of the party, the left-wing faction of the party had lost nearly all leading positions to the right-wing faction, however the party's program became radicalized and the majority of the delegates wanted a capitalist and Labour critical stance. However Halvorsen disobeyed some party <b>programs</b> <b>clauses,</b> the most notable being to not collaborate with capitalist or capitalist friendly parties such as, for instance, the Labour or the Centre Party.|$|R
40|$|The {{success of}} Prolog motivates {{people to use}} full firstorder logic instead of only Horn clauses {{as the basis of}} logic programming. One of the main work in this {{extending}} is to seek proof procedure for new logic programming. Positive disjunctive logic <b>programming</b> extends Horn <b>clause</b> <b>programming</b> by allowing more than one atoms to occur in the head of a <b>program</b> <b>clause.</b> In this paper we propose a new proof procedure for disjunctive logic programming which is based on novel program transformation. With this transformation, the new proof procedure shares many important properties enjoyed by SLD-resolution. The soundness and completeness of the proof procedure with respect to computing answers are given. Key Words: Disjunctive Logic Programming, SLDresolution, Proof Procedure. 1. Introduction The success of Prolog motivates people to use full firstorder logic instead of only Horn clauses as the basis of logic programming (Loveland 1987). Positive disjunctive logic programming is one of th [...] ...|$|R
40|$|AbstractWe {{show the}} {{completeness}} of {{an extension of}} SLD-resolution to the equational setting. This proves a conjecture of Laurent Fribourg and shows the completeness of an implementation of his. It is the first completeness result for superposition of equational Horn clauses which reduces to SLD resolution in the non-equational case. The inference system proved complete is actually more general than the one of Fribourg, because it allows for a selection rule on <b>program</b> <b>clauses.</b> Our completeness result also has implications for Conditional Narrowing and Basic Conditional Narrowing...|$|R
40|$|Abstract. We {{present a}} method for {{knowledge-based}} agents to learn strategies. Using techniques of inductive logic programming, strategies are learned in two steps: A given example set is first generalized into an overly general theory, which then gets refined. We show how a learning agent can exploit background knowledge of its actions and environment in order to restrict the hypothesis space, which enables the learning of complex logic <b>program</b> <b>clauses.</b> This {{is a first step}} toward the long term goal of adaptive, reasoning agents capable of changing their behavior when appropriate. ...|$|R
40|$|PROLOG {{has been}} shown to be an {{effective}} tool for expressing the logic of many problems dealing with parsing, natural language processing, and spelling verification [1, 7, 8, 9, 12]. As a class, these problems deal with the manipulation of lexical databases as Horn clauses. Since PROLOG does not generally differentiate between <b>program</b> <b>clauses</b> and data clauses, the internal representation and manipulation of data may not be optimal for a particular application. This paper discusses an alternative method of representing and manipulating lexical databases through the use of N-gram analysis, prefiltering, and integration with another high level language...|$|R
40|$|We {{present a}} logical <b>programming</b> language, {{generative}} <b>clause</b> <b>programs,</b> which generalize definite <b>clause</b> <b>programs</b> with a meta-programming level. The fundamental notion {{in the language}} {{is that of a}} meta-goal which consists of a plain goal together with a program in which it is expected to be true. The declarative semantics can be defined in terms of Herbrand models consisting of such meta-goals. The well-known fixpoint characterization of the least Herbrand model for definite <b>clause</b> <b>programs</b> generalizes immediately. The procedural semantics is considered in terms of an interpreter program and a compiler for the language. 1 Introduction A meta-level in logic programming is traditionally introduced by means of a selfinterpreter programmed in the logical language itself, e. g., (Bowen, Kowalski, 1982, Bowen, Weinberg, 1985, Bowen, 1985, Hill, Lloyd, 1988, Subrahmanian, 1988). In this way, the access to the meta-level is syntactically just a call of a predicate and consequently, its semant [...] ...|$|R
40|$|We present many-valued {{disjunctive}} logic {{programs in}} which classical disjunctive logic <b>program</b> <b>clauses</b> are extended by a truth value that respects the material implication. Interestingly, these many-valued disjunctive logic programs have both a probabilistic semantics in probabilities over possible worlds and a truth-functional semantics. We then define minimal, perfect, and stable models {{and show that}} they have the same properties like their classical counterparts. In particular, perfect and stable models are always minimal models. Under local stratification, the perfect model semantics coincides with the stable model semantics. Finally, we show that some special cases of propositional many-valued disjunctive logic programming under minimal, perfect, and stable model semantics have the same complexity like their classical counterparts. </p...|$|R
40|$|Abstract. Developing a good {{formalism}} and {{an efficient}} decision procedure for the instance checking problem is desirable for practical application of description logics. The data {{complexity of the}} instance checking problem is coNP-complete even for Horn knowledge bases in the basic description logic ALC. In this paper, we present and study weakenings with PTime data complexity of the instance checking problem for Horn knowledge bases in regular description logics. We also study cases when the weakenings are an exact approximation. In contrast to previous related work of other authors, our approach deals with the case when the constructor ∀ is allowed in premises of <b>program</b> <b>clauses</b> that are used as terminological axioms. ...|$|R
40|$|Abstract. Inductive logic {{programming}} {{is a form}} of machine learning from examples which employs the representation formalism of clausal logic. One of the earliest inductive {{logic programming}} systems was Ehud Shapiro’s Model Inference System [90], which could synthesise simple recursive programs like append/ 3. Many of the techniques devised by Shapiro, such as top-down search of <b>program</b> <b>clauses</b> by refinement operators, the use of intensional background knowledge, and the capability of inducing recursive clauses, are still in use today. On the other hand, significant advances have been made regarding dealing with noisy data, efficient heuristic and stochastic search methods, the use of logical representations going beyond definite clauses, and restricting the search space by means of declarative bias. The latter is a general term denoting any form of restrictions on the syntactic form of possible hypotheses. These include the use of types, input/output mode declarations, and clause schemata. Recently, some researchers have started using alternatives to Prolog featuring strong typing and real functions, which alleviate the need for some of the above ad-hoc mechanisms. Others have gone beyond Prolog by investigating learning tasks in which the hypotheses are not definite <b>clause</b> <b>programs,</b> but for instance sets of indefinite clauses or denials, constraint logic <b>programs,</b> or <b>clauses</b> representing association rules. The chapter gives an accessible introduction to the above topics. In addition, it outlines the main current research directions which have been strongly influenced by recent developments in data mining and challenging real-life applications. ...|$|R
40|$|International audienceIn {{an earlier}} work, a {{termination}} analyzer for Java bytecode was developed that translates a Java bytecode program into a constraint logic program and then proves {{the termination of}} the latter. An efficiency bottleneck of the termination analyzer is {{the construction of a}} proof of termination for the generated constraint logic program, which is often very large in size. In this paper, a set of program simplifications are presented that reduce the size of the constraint logic program without changing its termination behavior. These simplifications remove <b>program</b> <b>clauses</b> and/or predicate arguments that do not affect the termination behavior of the constraint logic program. Their effect is to reduce significantly the time needed to build the termination proof for the constraint logic program, as our experiments show...|$|R
40|$|Interpretation.................................. 107 12 XSB Modules 108 13 Handling Large Fact Files CONTENTS iv 13. 1 Compiling Fact Files.................................... 109 13. 2 Dynamically Loaded Fact Files.............................. 110 13. 3 Indexing Static <b>Program</b> <b>Clauses.............................</b> 111 14 Table Builtins 112 15 XSB System Facilities Chapter 1 Background and Motivation There is {{a flaw in}} {{the very}} {{foundations}} of Logic Programming: Prolog is nondeclarative. Of course, everyone knows that real Prolog as it is used is nondeclarative. Prolog programs abound with cuts, var tests, asserts, {{and all kinds of}} ugly warts. Everyone agrees that Prolog should be more declarative and much research time has been spent, and spent productively, trying to remedy these problems by providing more declarative alternatives. But I believe that there [...] ...|$|R
40|$|AbstractIn {{an earlier}} work, a {{termination}} analyzer for Java bytecode was developed that translates a Java bytecode program into a constraint logic program and then proves {{the termination of}} the latter. An efficiency bottleneck of the termination analyzer is {{the construction of a}} proof of termination for the generated constraint logic program, which is often very large in size. In this paper, a set of program simplifications are presented that reduce the size of the constraint logic program without changing its termination behavior. These simplifications remove <b>program</b> <b>clauses</b> and/or predicate arguments that do not affect the termination behavior of the constraint logic program. Their effect is to reduce significantly the time needed to build the termination proof for the constraint logic program, as our experiments show...|$|R
40|$|AND-parallel {{execution}} of logic programs {{turns out to}} be an intricate matter whenever clause literals are linked by shared variables. Shared variable dependencies call for special precautions to prevent processes from computing inconsistent bindings while working on clause literals simultaneously. Therefore, most systems restrict AND-parallelism to its producer/consumer style by allowing only independent literals to run in parallel. These literals then act as producers of variable bindings which will be consumed by dependent literals in subsequent steps. However, to implement producer/consumer parallelism efficiently, appropriate methods must be available for detecting dependencies caused by shared variables. Concerning such dependencies this paper presents a methodology which may serve as a basis for efficient dependency checking by performing compile and run-time analysis of <b>program</b> <b>clauses...</b>|$|R
40|$|AbstractWe devise an {{interpretation}} of a binarised definite logic program in a geometric framework which is closer to Dynamical Systems than to Logic. The building blocks of our framework are a family of affine sub-modules in a free finitely generated module over {{a special kind of}} ring. We describe SLD-resolution of definite binary programs as the iterated action of a finite union of affine graphs, associated to the <b>program</b> <b>clauses,</b> on a certain set of affine varieties associated to the original syntactic terms. This action is shown to faithfully represent the running of the corresponding program on a given goal, since all answers and only those answers the program would output are obtained. Hence, a programming language such as pure Prolog completely falls within our description...|$|R
40|$|We {{present a}} new {{approach}} to probabilistic logic programs with a possible worlds semantics. Classical <b>program</b> <b>clauses</b> are extended by a subinterval of [0, 1] that describes the range for the conditional probability of the head of a clause given its body. We show that deduction in the defined probabilistic logic programs is computationally more complex than deduction in classical logic programs. More precisely, restricted deduction problems that are P-complete for classical logic programs are already NP-hard for probabilistic logic programs. We then elaborate a linear programming approach to probabilistic deduction that is efficient in interesting special cases. In the best case, the generated linear programs have a number of variables that is linear in the number of ground instances of purely probabilistic clauses in a probabilistic logic program. </p...|$|R
