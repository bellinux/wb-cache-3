3|31|Public
50|$|Domain <b>propagation</b> <b>memory</b> is {{also called}} bubble memory. The basic {{idea is to}} control domain wall motion in a {{magnetic}} medium that is free of microstructure. Bubble refers to a stable cylindrical domain. Data is then recorded by the presence/absence of a bubble domain. Domain <b>propagation</b> <b>memory</b> has high insensitivity to shock and vibration, so its application is usually in space and aeronautics.|$|E
40|$|Neuron-glia {{cultures}} {{serve as}} a valuable model system for exploring the bio-molecular activity of single cells. Since neurons in culture can be conveniently recorded with great fidelity from many sites simultaneously, {{it has long been}} suggested that uniform cultured neurons may also be used to investigate network-level mechanisms pertinent to information processing, activity <b>propagation,</b> <b>memory</b> and learning. But how much of the functionality of neural circuits can be retained in-vitro remains an open question. Recent studies utilizing patterned networks suggest that they provide a most useful platform to address fundamental questions in neuroscience. Here we review recent efforts in the realm of patterned networks' activity investigations. We give a brief overview of the patterning methods and experimental approaches commonly employed in the field, and summarize the main results reported in the literature. The general picture that emerges from these reports indicates that patterned networks with uniform connectivity do not exhibit unique activity patterns. Rather, their activity is very similar to that of unpatterned uniform networks. However, by breaking the connectivity homogeneity, using a modular architecture, it is possible to introduce pronounced topology-related gating and delay effects. These findings suggest that patterned cultured networks may {{serve as a}} new platform for studying the role of modularity in neuronal circuits...|$|E
40|$|The {{solar cycle}} and its {{associated}} magnetic activity are the main drivers behind changes in the interplanetary environment and the Earth's upper atmosphere (commonly referred to as space weather). These changes have {{a direct impact on}} the lifetime of space-based assets and can create hazards to astronauts in space. In recent years there has been an effort to develop accurate solar cycle predictions (with aims at predicting the long-term evolution of space weather), leading to nearly a hundred widely spread predictions for the amplitude of solar cycle 24. A major contributor to the disagreement is the lack of direct long-term databases covering different components of the solar magnetic field (toroidal vs. poloidal). Here we use sunspot area and polar faculae measurements spanning a full century (as our toroidal and poloidal field proxies), to study solar cycle <b>propagation,</b> <b>memory,</b> and prediction. Our results substantiate predictions based on the polar magnetic fields, whereas we find sunspot area to be uncorrelated to cycle amplitude unless multiplied by area-weighted average tilt. This suggests that the joint assimilation of tilt and sunspot area is a better choice (with aims to cycle prediction) than sunspot area alone, and adds to the evidence in favor of active region emergence and decay as the main mechanism of poloidal field generation (i. e. the Babcock-Leighton mechanism). Finally, by looking at the correlation between our poloidal and toroidal proxies across multiple cycles, we find solar cycle memory to be limited to only one cycle. Comment: 7 pages, 5 figure...|$|E
5000|$|Ribeiro S, Nicolelis MAL. Reverberation, {{storage and}} post-synaptic <b>propagation</b> of <b>memories</b> during sleep. Learn. Mem. 11(6): 686-696, 2004.|$|R
40|$|Enhancements: better input {{validation}} and error reporting (alignment/starting tree) Bugfixes: compilation with gcc 7 (# 17) failed assertion in MSA::remove_sites failed assertion in pllmod_algo_spr_round (fix libpll error <b>propagation)</b> fix excessive <b>memory</b> allocation during pattern compression ([URL]...|$|R
40|$|This {{paper is}} {{intended}} to provide a general overview of the emergent field of teleimmersion. We detail {{some of the major}} issues facing teleimmersive application developers and suggest seven significant factors that determine how data should be distributed. In particular the impact of data size on delivery of quality-of-service requirements is examined. A flexible model for data sharing in distributed STL-style containers is proposed. In this model containers and iterators are used as abstractions for update <b>propagation</b> and <b>memory</b> consistency models. ...|$|R
40|$|In {{mammals and}} birds, long episodes of nondreaming sleep (“slow-wave ” sleep, SW) are {{followed}} by short episodes of dreaming sleep (“rapid-eye-movement ” sleep, REM). Both SW and REM sleep {{have been shown to}} be important for the consolidation of newly acquired memories, but the underlying mechanisms remain elusive. Here we review electrophysiological and molecular data suggesting that SW and REM sleep play distinct and complementary roles on memory consolidation: While postacquisition neuronal reverberation depends mainly on SW sleep episodes, transcriptional events able to promote long-lasting memory storage are only triggered during ensuing REM sleep. We also discuss evidence that the wake–sleep cycle promotes a postsynaptic <b>propagation</b> of <b>memory</b> traces away from the neural sites responsible for initial encoding. Taken together, our results suggest that basic molecular and cellular mechanisms underlie the reverberation, storage, and <b>propagation</b> of <b>memory</b> traces during sleep. We propose that these three processes alone may account for several important properties of memory consolidation over time, such as deeper memory encoding within the cerebral cortex, incremental learning several nights after memory acquisition, and progressive hippocampal disengagement. In mammals and birds, long episodes of nondreaming sleep (“slow-wave ” sleep, SW) {{are followed by}} short episodes of dream-ing sleep (“rapid-eye-movement ” sleep, REM) (Aserinsky an...|$|R
40|$|This study {{builds on}} the {{assumption}} that large-scale social phenomena emerge out of the interaction between individual cognitive mechanisms and social dynamics. Within this framework, we empirically investigated the <b>propagation</b> of <b>memory</b> eﬀects (retrieval induced forget- ting and practice eﬀects) through sequences of social interactions. We found that the inﬂuence a public ﬁgure has on an individual’s memories propagates in conversations between attitudinally similar, but not attitudinally dissimilar interactants, further aﬀecting their subsequent memories [3]. The implementation of this transitivity principle in agent based simulations revealed the impact of community size, number of conversations and network structure on the dynamics of collective memory...|$|R
40|$|ISBN : 978 - 3 - 540 - 88457 - 6 International audienceIn this paper, {{we present}} a novel {{hardware}} architecture to, achieve erosion and dilation with a large structuring element. We are, proposing a modification of HGW algorithm with a block mirroring, scheme to ease the <b>propagation</b> and <b>memory</b> access and to minimize, memory consumption. It allows to suppress the needs for backward scan-, ning and gives the possibility for hardware architecture to process very, large lines with a low latency. It compares well with the Lemonnier's, architecture in terms of ASIC gates area and shows the interest of our, solution by dividing the circuit area {{by an average of}} 10...|$|R
40|$|We study {{nonlinear}} {{heat conduction}} equations with memory effects {{within the framework}} of the fractional calculus approach to the generalized Maxwell-Cattaneo law. Our main aim is to derive the governing equations of heat propagation, considering both the empirical temperature-dependence of the thermal conductivity coefficient (which introduces nonlinearity) and memory effects, according to the general theory of Gurtin and Pipkin of finite velocity thermal <b>propagation</b> with <b>memory.</b> In this framework, we consider in detail two different approaches to the generalized Maxwell-Cattaneo law, based on the application of long-tail Mittag-Leffler memory function and power law relaxation functions, leading to nonlinear time-fractional telegraph and wave-type equations. We also discuss some explicit analytical results to the model equations based on the generalized separating variable method and discuss their meaning in relation to some well-known results of the ordinary case...|$|R
40|$|A simple {{supervised}} learning algorithm for recurrent neural networks is proposed. It needs only O(n 2) memories and O(n 2) calculations where n {{is the number}} of neurons, by limiting the problems to delayed recognition (short-term memory) problem. Since O(n 2) {{is the same as the}} order of the number of connections in the neuralnetwork, it is reasonablefor implementation. This learningalgorithm is similar to the conventional static back-propagation learning. Connection weights are modified by the products of the propagated errorsignal and somevariables that holdthe information about the past pre-synaptic neuron's output. Key Words : Recurrent Neural Network, Back <b>Propagation,</b> Short-Term <b>Memory,</b> Delayed Recognition Problem 1. Introduction We, living creatures, obtain a lot of pieces of information from various sensors. However, since much information is necessary to represent our real world, we cannot recognize the whole of them in a moment of time. Therefore we utilize not only [...] ...|$|R
40|$|This paper {{describes}} an algorithm for parallel assembling of the stiffness matrix in simulation of crack <b>propagation</b> in distributed <b>memory</b> environment using masterworkers method. In this algorithm, element stiffness matrix is formed by groups in each processor related by the finite element mesh. Each processor assembles a specific {{group of elements}} and no synchronization is required to avoid two or more worker processors sending the calculation result to master processor concurrently. This paper gives the speed-up rate in the simulation of crack propagation. The results indicate excellent performance and reduction of total computational tim...|$|R
40|$|Equals is {{a system}} for {{parallel}} evaluation of lazy functional programs implemented on a Sequent Symmetry. A preliminary implementation of equals [4] was used to establish the validity of Normal Form (NF) demand <b>propagation</b> and <b>memory</b> reclamation via reference counting. However, that implementation did not exploit vertical parallelism, where the arguments to a function can be evaluated in parallel with the function itself. The overheads of vertical parallelism can {{be as high as}} the overheads in systems that do not propagate NF demand. Hence careful integration of vertical parallelism with horizontal parallelism (parallelism among the arguments to a function) is needed to fully benefit from NF-demand propagation. In this paper we describe schemes to harness vertical parallelism {{within the context of the}} current equals system. We identify the aspects of the runtime system (task management) that affect the overall efficiency of the combined system. We also provide preliminary performance figures indicating the effectiveness of the selected schemes...|$|R
40|$|This paper {{proposes a}} novel View-based Consistency model for Distributed Shared Memory, {{in which a}} new concept, view, is coined. A view {{is a set of}} data objects that a {{processor}} has the right to access in the shared memory. The View-based Consistency model only requires that the data objects of a processor's view are updated before a processor accesses them. In this way, it can achieve the maximum relaxation of constraints on modification propagation and execution in data-race-free programs. This paper first briefly reviews a number of related consistency models in terms of their use of three techniques [...] time, processor and data selection [...] which each eliminate some unnecessary <b>propagation</b> of <b>memory</b> modifications while guaranteeing sequential consistency for data-race-free programs. Then, we present the View-based Consistency model and its implementation. In contrast with other models, the View-based Consistency model can achieve transparent data selection without programmer annotation and can offer the maximum performance advantage. Differences among related work are discussed through illustrative examples...|$|R
30|$|In {{relation}} to molecular diffusion channel models, more recent {{works such as}} [55] presented a 3 D channel model of a molecular communications and analyzed its attenuation and propagation delay characteristics for an absorbing receiver. An end-to-end analysis of <b>propagation</b> noise and <b>memory</b> for molecular communication over microfluidic channels is presented in [56]. In [57], a realistic channel model for a table-top molecular communication platform that is capable for transmitting short text messages across a room is proposed based on an experimental platform presented in [58]. We note that [55 – 58] do not consider any biochemical possibilities in the respective models.|$|R
40|$|Since quantum {{mechanics}} {{was applied for}} a quantum protocol which could transport a quantum state from one location to another, great attention has been drawn in quantum information and communications [1]. During the last decade {{there has been much}} progress in quantum information science such as entanglement generations, single photon generations, quantum key distributions, quantum universal gate operations, and quantum teleportation. Unlike classical information processing, any measurement on the quantum information could destroy it. Therefore, on-demand quantum measurement has been intensively studied for quantum error corrections for practical quantum information science. So far the most advanced technique of the quantum measurement is based on nonlinear optics in which strong interactions are inevitable and may negatively affects on the quantum information status. Electromagnetically induced transparency (EIT) is a quantum optical phenomenon of showing nonabsorption resonance in an optically dense medium. The induced transparency is a direct result from refractive index change due to destructive atom-field interactions. Most interesting aspects of EIT is in its nonlinear quantum applications such as slow light <b>propagations,</b> quantum <b>memory,</b> quantum switch, entanglement generations, and Schrodinger cat stat...|$|R
40|$|Aromatic {{amino acids}} in proteins, (tryptophan, tyrosine, {{histidine}} and phenylalanine) have electron resonant ring structures in which electrons are mobile and delocalizable. Electron transport is examined in these complexes with electrons hopping or tunneling between aromatics that {{have less than}} 2 nm separations. Microtubule cylindrical lattices form distinct helical pattern topologies that can perform information processing, information <b>propagation,</b> computing and <b>memory</b> storage within cells. The microtubule patterns correspond with binding patterns of microtubule-associated proteins. These proteins correspond to cytoskeletal and cellular structure and function. This paper reports a concept of information propagation and information processing in microtubules from an electronics viewpoint. Analysis and modeling are performed in this paper...|$|R
40|$|Dynamic taint {{analysis}} (DTA) is {{a technique}} used for tracking information flow by propagating taint <b>propagation</b> across <b>memory</b> locations during program execution. Most implementations of DTA are based on dynamic binary instrumentation (DBI) frameworks or whole-system emulators/virtual machine monitors. The boundary of information tracking with DBI frameworks is a single process, while system emulators can cover a host, including the OS. Using system emulators, {{it may be possible}} to consider taint propagation across multiple processes executing locally, within the emulator. However, there is an increasing need for tracking information flow across single-system boundaries and across the whole enterprise. We describe a proof-of-concept architecture for tracking multiple mixed-information flows among several processes across a distributed enterprise. Our DTA tool is based on PIN, a DBI framework by Intel, and the concatenated DTA processing is realized with per-host flow managers. We have tested our prototype with typical enterprise applications. As a motivating example, we track information leakage due to a SQL injection attack from a web-based database server query. Our work is of an exploratory nature, aiming to expose our early findings and identify areas where additional research is needed in improving usability and performance...|$|R
40|$|The {{field of}} {{autonomous}} mobile robotics has recently gained many researchers’ interests. Due {{to the specific}} needs required by various applications of mobile robot systems, especially in navigation, designing a real time obstacle avoidance and path following robot system has become the backbone of controlling robots in unknown environments. The main objective of our project is applications based mobile robot systems, especially in navigation, designing real time obstacle avoidance and path following robot system has become the backbone of controlling robots in unknown environments. The main objective behind using the obstacle avoidance approach is to obtain a collision-free trajectory from the starting point to the target in monitoring environments. The ability of the robot to follow a path, detects obstacles, and navigates around them to avoid collision. It also shows that the robot has been successfully following very congested curves and has avoided any obstacle that emerged on its path. Motion planning that allows the robot to reach its target without colliding with any obstacles that may exist in its path. To avoid collision in the mobile robot environment, providing a path planning& line following approach. Line following, path planning, collision avoidance, back <b>propagation,</b> improved <b>memory,</b> detecting long distance obstacles. Cheap and economical than the former one. Also work with back propagation technique...|$|R
40|$|Supply Chain Formation (SCF) is {{the process}} of {{determining}} the participants in a supply chain, who will exchange what with whom, and the terms of the exchanges. Decentralized SCF appears as a highly intricate task because agents only possess local information, have limited knowledge about the capabilities of other agents, and prefer to preserve privacy. Very recently, the decentralized SCF problem has been cast as an optimization problem that can be e ciently approximated using max-sum loopy belief <b>propagation.</b> Unfortunately, the <b>memory</b> and communication requirements of this approach largely hinder its scalability. This paper presents a novel encoding of the problem into a binary factor graph (containing only binary variables) along with an alternative algorithm. These allow to scale up to form supply chains in markets with higher degrees of competition...|$|R
40|$|International audienceIn this article, we {{introduce}} a new logical clock, the barrier-lock clock, whose conception {{is based on the}} lazy release consistency memory model (LRC) supported by several distributed shared memory (DSM) systems. Since in the LRC, the <b>propagation</b> of shared <b>memory</b> updates performed by the processes of a parallel application is induced by lock and barrier operations, our logical clock has been modeled on those operations. Each barrier-lock times-tamp encodes the synchronization operation with which it is associated. Its size is not dependent on the number of processes of the system, as the traditional logical vector clocks, but it is proportional to the number of locks. The barrier-lock time characterizes the causality of shared memory updates performed by processes of a parallel application running on a LRC-based DSM system. A formal proof and experimental tests have confirmed such property...|$|R
40|$|We {{establish}} sufficient {{conditions on}} durations that are stationary with finite variance and memory parameter null {{to ensure that}} the corresponding counting process N (t) satisfies Var N (t) ~ Ct 2 (C > 0) as t → ∞, with the same memory parameter null that was assumed for the durations. Thus, these conditions ensure that the memory parameter in durations propagates to the same memory parameter in the counts. We then show that any autoregressive conditional duration ACD(1, 1) model with a sufficient number of finite moments yields short memory in counts, whereas any long memory stochastic duration model with d > 0 and all finite moments yields long memory in counts, with the same d. Finally, we provide some results about the <b>propagation</b> of long <b>memory</b> to the empirically relevant case of realized variance estimates affected by market microstructure noise contamination. ...|$|R
40|$|SummaryThe primate {{temporal}} cortex implements neural {{mechanisms for}} memory retrieval from visual long-term storage, and memory neurons {{have been identified}} at the single-neuron level whose activities following cue presentation encode the presented object (“cue-holding” neurons) or to-be-recalled target (“pair-recall” neurons). Although {{these two types of}} neurons can potentially interact during the target recall, little is known about information flow among these neurons. We conducted simultaneous recordings of multiple single units in macaque perirhinal cortex while they performed a pair-association memory task. Granger causality analysis revealed the emergence of directed couplings during the delay period predominantly from cue-holding neurons to pair-recall neurons. Moreover, these interactions coincided with unidirectional signal flow from the recipient recall neuron to another recall neuron, implying cascade-like signal <b>propagation</b> among the <b>memory</b> cell assembly. These results suggest that directed interactions among perirhinal memory neurons are dynamically modulated to implement functional microcircuitry for retrieval of object association memory...|$|R
40|$|Me propongo ofrecer un sucinto análisis de los textos y las fuentes que afirman o se limitan a referir ciertas "veneradas tradiciones" que se incardinan en los momentos iniciales de la presencia del cristianismo en nuestra península. Analizaré tres que han quedado grabadas de forma especial -por su progresiva consolidación multisecular y su arribo a nuestros días-, en la propagación oral y en memoria colectiva de nuestro pueblo y en la transmisión escrita de la {{historia de}} nuestra nación. I have {{the purpose of}} {{offering}} a brief analysis of the texts and sources that affirm or simply recite some “worship traditions” that {{have their origin in}} the beginnings of Christianity in our peninsula. I’ll analyse three traditions that have been recorded in a special way –thanks to a progressive centuries-old consolidation and their continuity to the present day-, in the oral <b>propagation</b> and collective <b>memory</b> of our people and in the written transmission of the history of our nation...|$|R
40|$|Abstract—Most current {{content-based}} image retrieval {{systems are}} still incapable of providing users with their desired results. The major difficulty {{lies in the}} gap between low-level image features and high-level image semantics. To address the problem, this study reports a framework for effective image retrieval by employing a novel idea of memory learning. It forms a knowledge memory model to store the semantic information by simply accumulating user-provided interactions. A learning strategy is then applied to predict the semantic relationships among images according to the memorized knowledge. Image queries are finally performed based on a seamless combination of low-level features and learned semantics. One important advantage of our framework {{is its ability to}} efficiently annotate images and also propagate the keyword annotation from the labeled images to unlabeled images. The presented algorithm has been integrated into a practical image retrieval system. Experiments on a collection of 10 000 general-purpose images demonstrate the effectiveness of the proposed framework. Index Terms—Annotation <b>propagation,</b> image retrieval, <b>memory</b> learning, relevance feedback, semantics. I...|$|R
40|$|We {{describe}} {{construction of}} a novel modification, “ 6 C,” of chromatin looping assays that allows specific proteins that may mediate long-range chromatin interactions to be defined. This approach combines the standard looping approaches previously defined with an immunoprecipitation step to investigate involvement of the specific protein. The efficacy {{of this approach is}} demonstrated by using a Polycomb group (PcG) protein, Enhancer of Zeste (EZH 2), {{as an example of how}} our assay might be used. EZH 2, as a protein of the PcG complex, PRC 2, has an important role in the <b>propagation</b> of epigenetic <b>memory</b> through deposition of the repressive mark, histone H 3, lysine 27, tri-methylation (H 3 K 27 me 3). Using our new 6 C assay, we show how EZH 2 is a direct mediator of long-range intra- and interchromosomal interactions that can regulate transcriptional down-regulation of multiple genes by facilitating physical proximities between distant chromatin regions, thus targeting sites within to PcG machinery...|$|R
40|$|Epigenetic {{modifications}} influence {{gene expression}} pattern {{and provide a}} unique signature of a cell differentiation status. Without external stimuli or signalling events, this cell identity remains stable and unlikely to change over many cell divisions. The epigenetic signature of a particular cell fate therefore needs to be replicated faithfully in daughter cells; otherwise a cell lineage cannot be maintained. However, the mechanism of transmission of cellular memory from mother to daughter cells remains unclear. It {{has been suggested that}} the inheritance of an active or silent gene state involves different kinds of epigenetic mechanisms, e. g., DNA methylation, histone modifications, replacement of histone variants, Polycomb group (PcG) and Trithorax group (TrxG) proteins. Emerging evidence supports the role of histone variant H 3. 3 in maintaining an active gene status and in remodelling nucleosomal composition. Here we discuss some recent findings on the <b>propagation</b> of epigenetic <b>memory</b> and propose a model for the inheritance of an active gene state through the interaction of H 3. 3 with other epigenetic components. © 2008 Landes Bioscience. link_to_subscribed_fulltex...|$|R
40|$|Abstract—Molecular {{communication}} (MC) {{between a}} transmit-ter and a receiver {{placed in the}} chambers attached to a microflu-idic channel is investigated. A linear end-to-end channel model is developed capturing {{the effects of the}} diffusion and the junction transition at the chambers, as well as the microfluidic channel shapes and the fluid flow. The spectral density of the propagation noise is studied, and the flat frequency bands are identified for the chambers and the microfluidic channel. This suggests that in cer-tain microfluidic design choices, the spectral density of noise may end up naturally being flat. Motivated by this result, the additive white Gaussian noise (AWGN) model is developed based on the chamber, the microfluidic channel, and the fluid flow parameters for the end-to-end propagation noise. Furthermore, the molecular memory is modeled due to inter-diffusion among transmitted molecular signals. The effect of the molecular memory on the end-to-end propagation noise is also analyzed. To substantiate our analytical results, the ranges of physical parameters that yield a linear end-to-end MC channel are investigated. These results show the validity of the AWGN model for MC over microfluidic channels and characterize the impact of the microfluidic channel and chamber geometry on the <b>propagation</b> noise and <b>memory.</b> Index Terms—Molecular communication, microfluidics, noise, channel models, Gaussian channels, memory...|$|R
40|$|By {{studying}} {{the behavior of}} several programs that crash due to memory errors, we observed that locating the errors can be challenging because significant <b>propagation</b> of corrupt <b>memory</b> values can occur prior {{to the point of}} the crash. In this paper, we present an automated approach for locating memory errors in the presence of <b>memory</b> corruption <b>propagation.</b> Our approach leverages the information revealed by a program crash: when a crash occurs, this reveals a subset of the memory corruption that exists in the execution. By suppressing (nullifying) the effect of this known corruption during execution, the crash is avoided and any remaining (hidden) corruption may then be exposed by subsequent crashes. The newly-exposed corruption can then be suppressed in turn. By iterating this process until no further crashes occur, the first point of memory corruption – and the likely root cause of the program failure – can be identified. However, this iterative approach may terminate prematurely, since programs may not crash even when memory corruption is present during execution. To address this, we show how crashes can be exposed in an execution by manipulating the relative ordering of particular variables within memory. By revealing crashes through this variable re-ordering, the effectiveness and applicability of the execution suppression approach can be improved. We describe a set of experiments illustrating the effectiveness o...|$|R
40|$|Though Belief Propagation (BP) {{algorithms}} generate {{high quality}} results {{for a wide}} range of Markov Random Field (MRF) formulated energy minimization problems, they require large memory bandwidths and could not achieve real-time performance when applied to many real-life inference tasks. There is an increasing demand for efficient parallel inference algorithms as the size of problems increase and computer architectures move towards multi-core. In this work, we proposed a new high speed parallel computational structure for hierarchical Belief <b>Propagation</b> on shared <b>memory</b> architecture, which is based on a modification and generalization of the hierarchical BP algorithm presented by Felzenszwalb and Huttenlocher. Our parallel hierarchical belief propagation (PHBP) computational structure supports arbitrary grouping of nodes in multiscale computation and works for graphs in general topologies (including non grid structure graphs). Secondly, a fully parallel framework of hierarchical BP using sequential asynchronous message updating scheme (accelerated message updating) is developed. We achieved parallelization of both pre-computation portion and computational intense message passing portion. Lastly, we empirically evaluated the performance of algorithm on several computer vision tasks where we achieved nearly linear parallel scaling and outperform other alternative algorithms. Specifically, for the task of restoring a 608 * 456 noisy image with 16 gray levels, our PHBP takes around 100 ms while a comparable result needs around 30 s using Parallel Splash on a same 8 core shared memory system. 2018 - 05 - 2...|$|R
40|$|Mechanisms that {{maintain}} transcriptional memory through cell division {{are important to}} maintain cell identity, and sequence-specific transcription factors that remain associated with mitotic chromatin are emerging as key players in transcriptional <b>memory</b> <b>propagation.</b> Here, we show that the major transcriptional effector of Notch signaling, RBPJ, is retained on mitotic chromatin, and that this mitotic chromatin association is mediated through the direct association of RBPJ with DNA. We further demonstrate that RBPJ binds directly to nucleosomal DNA in vitro, with a preference for sites close to the entry/exit position of the nucleosomal DNA. Genome-wide analysis in the murine embryonal-carcinoma cell line F 9 revealed that roughly 60 % of the sites occupied by RBPJ in asynchronous cells were also occupied in mitotic cells. Among them, we found that a fraction of RBPJ occupancy sites shifted between interphase and mitosis, suggesting that RBPJ can be retained on mitotic chromatin by sliding on DNA rather than disengaging from chromatin during mitotic chromatin condensation. We propose that RBPJ can function as a mitotic bookmark, marking genes for efficient transcriptional activation or repression upon mitotic exit. Strikingly, we found that sites of RBPJ occupancy were enriched for CTCF-binding motifs in addition to RBPJ-binding motifs, and that RBPJ and CTCF interact. Given that CTCF regulates transcription and bridges long-range chromatin interactions, our results raise the intriguing hypothesis that by collaborating with CTCF, RBPJ may participate in establishing chromatin domains and/or long-range chromatin interactions that could be propagate...|$|R
40|$|Literary {{responses}} to Republican exile are diverse and autobiographical works {{have emerged as}} a significant modality of this exilic literature. Utilising poetics {{as a mode of}} inquiry, this thesis aims to examine some of the complex and nuanced ways in which exile has shaped autobiographical writing by both first and second-generation female exiles. To this end, I trace a poetics of exile in a selected corpus of nineteen autobiographical works by twelve authors: Constancia de la Mora, Isabel Oyarzábal de Palencia, Silvia Mistral, Clara Campoamor, Victoria Kent, Luisa Carnés, Remedios Oliva Berenguer, Francisca Muñoz Alday, Angelina Muñiz-Huberman, María Rosa Lojo, María Luisa Elío and Arantzazu Amezaga Iribarren. These texts were published across a seventy year period (1939 – 2009) in a number of geographical locations and written in a variety of circumstances. Exilic autobiographical texts are not homogeneous and relatively few have adhered to traditional models of autobiography. As such, the works examined are drawn from a variety of autobiographical sub-genres including propagandistic autobiographies, diaries, political essays, hybrid texts, autofiction, memoirs, childhood autobiographies, more experimental semi-autobiographical texts and a film. The main body of this thesis presents six aspects of a poetics of exile — the notion of the addressee, generic hybridization, polyphony, the <b>propagation</b> of collective <b>memory,</b> postmemory, and retroprogressive representations of childhood — and adopts a multi-disciplinary approach that draws upon a number of fields. This thesis aims to offer an illumination of the breadth and difference of women’s exilic autobiographical writing as highlighted in the identification of six very different aspects of a poetics of exile...|$|R
40|$|Massé, dans ses deux volumes (1946), discute le problème de la gestion optimale des lâchures dans le cas d'un seul réservoir quand le bénéfice est dérivé de la {{production}} d'énergie hydroélectrique. Massé obtint ses résultats à la fois par un raisonnement économique et par une généralisation du Calcul des Variations. Sa méthode lui permit de fournir la preuve rigoureuse de la méthode graphique de Varlet (1923), dite du "fil tendu". Dans cet {{article on}} généralise la procédure de Massé au cas où (1) le bénéfice est réalisé bien en aval du point de lâchure, et (2) il y a plusieurs "point-cibles" (points où un certain objectif doit être assuré). Massé avait trouvé que la gestion optimale est celle qui maintient la valeur marginale du bénéfice constante dans le temps, pourvu que la gestion soit en régime libre, c' est à dire tant que le réservoir ne fonctionne ni à plein ni à vide. Par contre si le réservoir fonctionne par exemple à plein, Massé montra que la stratégie qui consiste à garder le réservoir plein ne peut être optimale que si la valeur marginale du bénéfice croît constamment avec le temps durant la période où le réservoir reste plein. On montre de manière rigoureuse dans le cas général que pour une gestion optimale ce qui doit rester constant c' est la valeur marginale future du bénéfice. Dans un article ultérieur on fournira la généralisation pour plusieurs réservoirs. The problem for operations of reservoirs {{is to choose}} on a day to day basis {{the value of the}} release at the dam location. The choice of the value of that discharge is conditioned by a criterion of satisfaction of one or several objectives. These objectives are defined in one or several points in the system on the river, or the rivers, downstream from the point, or the points, of release. Typical objectives may be to maximize electric production, or to minimize damage due to flooding downstream from the dams or due to shortages of water in the rivers at diversion points for municipal water supply or other uses, etc. adapted to the concerns of the managers, and relatively intuitive. The approach described in this article pursues the reasoning of Massé (1946) but generalizes it and therefore makes it more applicable. At first we look at the case of a single reservoir, located directly on the stream for the production of electric energy. In this case the target-point (the point where an objective function is to be evaluated) coincides with the point of release. This was precisely the problem studied by Massé (1946) in his classical two volumes on "Reserves and the Regulation of the Future". We pursue his reasoning but we use a more appropriate mathematical procedure which will allow us to obtain more general results. The same results are derived using two different approaches. The first one is more intuitive and uses the concept of marginal value to secure the necessary condition of optimality to be satisfied by the releases. The second procedure is more mathematical and uses, basically, the method of Calculus of Variations, generalized to the case where there are inequality constraints that must be satisfied. In the case of a single reservoir one shows that the optimality condition provides the rigorous proof of the graphical method of Varlet (1923). The results of Massé are generalized to the case where the objective function is evaluated downstream from the point of release and the management strategy must account for the phenomenon of propagation of discharges in the streams. Again in this case the results are obtained in two ways, (1) by the economic reasoning on the marginal values and (2) with the Constrained Calculus of Variations. Massé had found that the optimal policy for the releases was the one that maintained the marginal benefit constant in time. That applied for the case of a single reservoir and where the target-point coincides with the point of release. If B{x(t),t} is the instantaneous benefit obtained from making the release at the dam at a rate x(t) at time t, then the optimality condition is mathematically: b{x(t),t}=L=constant with timewhere b{x(t),t} is the marginal benefit, i. e. the partial derivative of B{x(t),t} with respect to the argument x. L is a constant, which in the mathematical formulation of the problem is the Lagrange multiplier associated with the mass balance constraint to be satisfied over the selected horizon of operations. In other words the cumulative volume of releases over the time horizon must be equal to the cumulative volume of inflows plus the drop in reservoir storage between the initial and final times. Economically the marginal benefit is the incremental benefit realized by making an extra release of one unit of water, given that the rate of release was x(t). Typically the marginal benefit decreases as the rate of release increases and that {{is often referred to as}} the "law of decreasing returns". For the case of electric production the marginal benefit will depend on the amount of releases made through the turbines but also on the season of year or day of week or hour of day. The price of electricity is higher in winter than it is in summer. It is higher during peak hours during the week than it is on weekends, etc. If on the other hand the marginal benefit is only a function of the release, and not a function of time, then the constancy of the marginal benefit with time is equivalent to the constancy of the release with time. Optimality becomes synonymous with regulation, i. e. releasing at a constant rate. It is only under these conditions that the graphical method of Varlet is applicable. In the graphical domain of cumulative volume of releases versus time, the optimal "trajectory" is a straight line where such a strategy is feasible i. e. does not make the reservoir more than full nor less than empty. When the objective is evaluated at a point downstream from the point of release and the marginal benefit (or cost) has a seasonal character, neither the graphical procedure of Varlet nor the mathematical result of Massé apply. For this more general case the derived optimality condition states that it is no longer an instantaneous marginal benefit that must remain constant in time. What must remain constant in time is a time integrated and weighted value of the marginal benefit (or damage) between the time the release is made and a later time. That later time is the release time plus the <b>memory</b> of the <b>propagation</b> system. The <b>memory</b> time is the time that must lapse before an upstream release is no longer felt at the target point downstream. The longer the distance between the release point and the target-point the longer is the <b>memory</b> of the <b>propagation</b> system. At the downstream point the damage depends on the discharge at that point, which is of course related to the release rate but also to the lateral inflows in between from tributaries and on the amount of attenuation that happens between the point of release and the target-point downstream. The integrand at dummy integration time t' is the marginal damage at that time multiplied by the instantaneous unit hydrograph at that time. Mathematically the integrand is: f{q(t'),t'}*k(t'-t) where f is marginal damage, q(t') is discharge at target point and k(.) is instantaneous unit hydrograph of propagation between release and target points. This integrand is to be integrated between time t of the release and time t + M, where M is the memory of the system. It is that integral that we have called the "Integrated Marginal Future" (or IMF for short) value that must remain constant in time. That optimality condition applies as long as the trajectory remains in the feasible domain bounded by the constraints of the problem, the "interior domain". When on a bound, the IMF value does not remain constant but must vary monotonically in a given direction, i. e. increases or decreases with time, depending on the constraint on which the solution rests...|$|R

