54|37|Public
5000|$|The {{physical}} implementation {{format of}} the Logical Data Model entities, e.g., message formats, file structures, <b>physical</b> <b>schema.</b> In DoDAF V1.5, this was the SV-11.|$|E
50|$|<b>Physical</b> <b>schema</b> is a {{term used}} in data {{management}} to describe how data is to be represented and stored (files, indices, et al.) in secondary storage using a particular database management system (DBMS) (e.g., Oracle RDBMS, Sybase SQL Server, etc.).|$|E
5000|$|Entity Types {{form the}} class of objects {{entities}} conform to, with the Entities being instances of the entity types. Entities represent individual objects that form {{a part of the}} problem being solved by the application and are indexed by a key. For example, converting the <b>physical</b> <b>schema</b> described above, we will have two entity types: ...|$|E
40|$|<b>Physical</b> <b>schemas</b> are {{representations}} of simple physically grounded relationships and interactions such as "move," "push," and "contain. " We {{believe they are}} the conceptual primitives an agent employs to understand its environment. <b>Physical</b> <b>schemas</b> can be used at varying levels of abstraction {{across a variety of}} domains. We have designed a domain-general agent simulation and control testbed based on <b>physical</b> <b>schemas.</b> If a domain can be described in physical terms as agents moving and applying force, it can be simulated in this testbed. Furthermore, we show that <b>physical</b> <b>schemas</b> can be viewed as abstract plans and can be used as the basis of a domain-general planner. Our simulation and planning system is currently being evaluated in a continuous, dynamic, and adversarial domain based on the game of Capture the Flag...|$|R
50|$|To add further flexibility, {{more than}} one main table is allowed, with main and submain tables having a one-to-many relation. Each main table can have its own {{dimension}} tables. To provide further query optimization, a data set can be partitioned into separate <b>physical</b> <b>schemas</b> on either the same database server or different database servers.|$|R
40|$|We {{present an}} {{optimization}} method and algorithm designed for three objectives: physical data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and "backchase" with constraints (dependencies). By using dictionaries (finite functions) in <b>physical</b> <b>schemas</b> we can capture with constraints useful access {{structures such as}} indexes, materialized views, source capabilities, access support relations, gmaps, etc...|$|R
5000|$|Physical data {{independence}} {{is the ability}} to modify the <b>physical</b> <b>schema</b> without causing application programs to be rewritten. Modifications at the physical level are occasionally necessary to improve performance. It means we change the physical storage/level without affecting the conceptual or external view of the data. The new changes are absorbed by mapping techniques.|$|E
5000|$|Physical data independence: The {{ability to}} change the <b>physical</b> <b>schema</b> without {{changing}} the logical schema is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.|$|E
5000|$|At the center, the {{conceptual}} schema defines the ontology {{of the concepts}} as the users think of them and talk about them. The <b>physical</b> <b>schema</b> according to Sowa (2004) [...] "describes the internal formats of the data stored in the database, and the external schema defines {{the view of the}} data presented to the application programs".|$|E
40|$|We {{present an}} {{optimization}} method and al# gorithm designed for three objectives# physi# cal data independence# semantic optimization# and generalized tableau minimization. The method relies on generalized forms of chase and #backchase# with constraints #dependen# cies#. By using dictionaries ##nite functions# in <b>physical</b> <b>schemas</b> we can capture with con# straints useful access {{structures such as}} indexes# materialized views# source capabilities# access support relations# gmaps# etc...|$|R
5000|$|... #Subtitle level 2: Postulated <b>physical</b> {{mechanisms}} underlying <b>schemas</b> and stages ...|$|R
40|$|Recent {{developments}} in philosophy, linguistics, devel- opmental psychology and articial intelligence {{make it possible}} to envision a developmental path for an ar- ticial agent, grounded in activity-based sensorimotor representations. This paper describes how Neo, an ar- ticial agent, learns concepts by interacting with its simulated environment. Relatively little prior struc- ture is required to learn fairly accurate representations of objects, activities, locations and other aspects of Neo 2 ̆ 7 s experience. We show how classes (categories) can be abstracted from these representations, and discuss how our representation might be extended to express <b>physical</b> <b>schemas,</b> general, domain-independent activi- ties that could be the building blocks of concept forma- tion...|$|R
50|$|At the center, the {{conceptual}} schema defines the ontology {{of the concepts}} as the users think of them and talk about them. The <b>physical</b> <b>schema</b> describes the internal formats of the data stored in the database, and the external schema defines {{the view of the}} data presented to the application programs. The framework attempted to permit multiple data models to be used for external schemata.|$|E
5000|$|At the center, the {{conceptual}} schema defines the ontology {{of the concepts}} as the users think of them and talk about them. The <b>physical</b> <b>schema</b> according to Sowa (2004) [...] "describes the internal formats of the data stored in the database, and the external schema defines {{the view of the}} data presented to the application programs." [...] The framework attempted to permit multiple data models to be used for external schemata.|$|E
50|$|The {{logical schema}} {{was the way}} data were {{represented}} {{to conform to the}} constraints of a particular approach to database management. At that time the choices were hierarchical and network. Describing the logical schema, however, still did not describe how physically data would be stored on disk drives. That is the domain of the <b>physical</b> <b>schema.</b> Now logical schemas describe data in terms of relational tables and columns, object-oriented classes, and XML tags.|$|E
40|$|SAS Institute Inc. in the USA {{and other}} countries. ® {{indicates}} USA registration. Other brand and product names are registered trademarks or trademarks {{of their respective}} companies. This paper will discuss {{the implementation of a}} dimensional data warehouse. A dimensional warehouse is a design or modeling teChnique that was developed by Ralph Kimball (Kimball. 1996) to help us develop our data models in a structured. visual way. Here we will discuss a strategy for the design, development and imlllementation of this structure using tools available with the SASe System. In addition, we will talk about several modeling techniques, implementation of logical and <b>physical</b> <b>schemas,</b> transformation and aggregation strategies, and the loading and unloading of data to the warehouse...|$|R
40|$|Our {{research}} {{seeks to}} provide scientific programmers with simpler, more abstract interfaces for accessing persistent multidimensional arrays, and to produce advanced i/o libraries supporting more eficient layod alternatives for these arrays on disk and in main memory. We report on our experience to date applying these techniques to applications in {{computational fluid dynamics}} {{in the areas of}} checkpoint/resiart, output data, and visualization. In the applications we have studied, we find that a simple, abstract interface can be used to insulate programmers from physical storage implementation details, while providing improved i/o performance at the same time. For example, we found that the use of "chunked” <b>physical</b> <b>schemas</b> for arrays gave approximately a factor of 10 improvement in time step output performance on the Intel iPSC/ 860...|$|R
40|$|The goal of {{the work}} is to design and {{implement}} a SPARQL compiler for the Bobox system. In addition to lexical and syntactic analysis corresponding to W 3 C standard for SPARQL language, it performs semantic analysis and optimization of queries. Compiler will constuct an appropriate model for execution in Bobox, that depends on the <b>physical</b> database <b>schema...</b>|$|R
50|$|The logical scheme stays {{unchanged}} {{even though}} the storage space or type of some data is changed for reasons of optimization or reorganization. In this external schema does not change. In this internal schema changes may be required due to some <b>physical</b> <b>schema</b> were reorganized here. Physical data independence is present in most databases and file environment in which hardware storage of encoding, exact location of data on disk,merging of records, so on this are hidden from user.|$|E
50|$|The entity {{types are}} an {{aggregation}} of multiple typed fields - each field maps {{to a certain}} column in the database - and can contain information from multiple physical tables. The entity types {{can be related to}} each other, independent of the relationships in the <b>physical</b> <b>schema.</b> Related entities are also exposed similarly - via a field whose name denotes the relation they are participating in and accessing which, instead of retrieving the value from some column in the database, traverses the relationship and returns the entity (or a collection of entities) it is related with.|$|E
50|$|It {{was decided}} that a {{compiled}} language such as C++ was not sufficiently flexible, reliable, and productive, and so a new programming language was created, Accent. Accent has many features similar to Java, but pre-dates it by five years. It has a compiler that compiles to machine-independent byte-codes, and a virtual machine execution environment with automatic memory management. Except for the compiler and execution environment, the entire Amplify Control product {{was written in the}} Accent language, including a scalable, networked client-server architecture and use of a SQL database with a schema flexible enough to allow customer extension of the built-in data types in Accent without changes to the <b>physical</b> <b>schema.</b>|$|E
40|$|We {{describe}} <b>physical</b> <b>schemas</b> {{for storing}} multidimensional arrays on disk. We {{have developed an}} i/o library supporting these schemas that provides an abstract interface shielding scientific application developers from physical storage details. Our library has resulted in simplified programming and improved i/o performance in the applications we have studied. 1 Motivation Scientific applications often center around computation on large arrays [Bell 87]. Some of the goals in traditional database systems research {{are relevant to the}} needs for persistent array storage in scientific computing applications. Traditional database systems focus on providing persistent storage that is efficient and easy-to-use. A strong emphasis is placed on providing application programmers with a logical view of their data to insulate them from physical storage details. A benefit of this approach is to provide a more natural interface in user understandable terms and to allow physical storage details to change [...] ...|$|R
30|$|Future work on GWML 2 {{includes}} {{the development of}} additional syntactical encodings with associated <b>physical</b> <b>schemas.</b> The most notable of such encodings are JSON and RDF/OWL, which would facilitate data exchange with web applications and Semantic Web initiatives, respectively. A foray into RDF/OWL raises many questions. For instance, which GWML 2 schema is to be targeted as a prospective RDF/OWL ontology? A superficial analysis suggests if the intended purpose is data exchange, then the logical schema is the natural target for an RDF/OWL ontology, given that the logical schema is intrinsically constructed with data transfer in mind. However, if the intended purpose is semantic interoperability, e.g. to interoperate with other domain ontologies such as those emerging for hydrology, then the optimal target for RDF/OWL ontology representation is likely the GWML 2 conceptual schema (as begun in [13]). These and other GWML 2 efforts will continue within the OGC Groundwater Standards Working Group.|$|R
40|$|We {{present an}} {{optimization}} method and algorithm designed for three objectives: physical data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and " with constraints (dependencies). By using dictionaries (nite functions) in <b>physical</b> <b>schemas</b> we can capture with constraints useful access {{structures such as}} indexes, materialized views, source capabilities, access support relations, gmaps, etc. The search space for query plans is de ned and enumerated in a novel manner: the chase phase rewrites the original query into a " plan that integrates all the access structures and alternative pathways that are allowed by applicable constraints. Then, the backchase phase produces optimal plans by eliminating various combinations of redundancies, again according to constraints. This method is applicable (sound) to a large class of queries, physical access structures, and semantic constraints. We prove {{that it is in}} fact complete for -conjunctive " queries and views with complex objects, classes and dictionaries, going beyond previous theoretical work on processing queries using materialized views...|$|R
50|$|The {{logical schema}} and its mapping with the <b>physical</b> <b>schema</b> is {{represented}} as an Entity Data Model (EDM), specified as an XML file. ADO.NET Entity Framework uses the EDM to actually perform the mapping letting the application {{work with the}} entities, while internally abstracting the use of ADO.NET constructs like DataSet and RecordSet. ADO.NET Entity Framework performs the joins necessary to have entity reference information from multiple tables, or when a relationship is traversed. When an entity is updated, it traces back which table the information came from and issues SQL update statements to update the tables in which some data has been updated. ADO.NET Entity Framework uses eSQL, a derivative of SQL, to perform queries, set-theoretic operations, and updates on entities and their relationships. Queries in eSQL, if required, are then translated to the native SQL flavor of the underlying database.|$|E
30|$|The GWML 2 {{conceptual}} and logical schemas {{are expressed in}} UML, and the <b>physical</b> <b>schema</b> is represented as a GML-XML schema accompanied by associated rules and examples. All three schemas {{are available in the}} GWML 2 SVN repository [11], and the <b>physical</b> <b>schema</b> is also available online from OGC [30]. The three schemas are summarized below, and their full descriptions can be found in the OGC standards specification [4].|$|E
30|$|The <b>physical</b> <b>schema</b> {{implements}} {{some or all}} of {{the logical}} schema, and typically refers to a schema intended for data transfer and encoded using a specific data language e.g. XML [31, 36]. It could also refer to a schema for data storage, such as a particular relational database design. Out of scope here are schemas for data access, such as for web services, though such standards might mandate the use of certain schemas for data discovery or delivery. The purpose of the <b>physical</b> <b>schema</b> is to specify the pattern for a particular implementation, as well as associated rules and best practices, often following constraints imposed by a standards body, e.g. GML-XML schemas must follow OGC/ISO encoding rules [32]. As with the logical schema, the <b>physical</b> <b>schema</b> is optional if it is not the standard’s target.|$|E
40|$|Analysis of a {{planning}} domain {{has been shown}} to produce information that can greatly reduce the space of plans a controller must search through. Domain analysis is generally based on the logical inferences one can make given a set of planning operators and sometimes information about goals. An alternative heuristic approach is possible as well. We have developed a small set of <b>physical</b> <b>schemas</b> for classifying operators in planning domains. We show how they perform on some commonly available domains and describe how this information can help to explain the structure of a plan in physical, task-oriented terms. Introduction Planning algorithms have taken dramatic steps over the past few years. Modern planning algorithms such as GraphPlan and SAT-based algorithms can solve problems that are orders of magnitude more difficult than earlier algorithms (Weld 1999). Still, as might be expected, many large or difficult problems remain beyond the abilities of the current generation of planners, [...] ...|$|R
40|$|The W 3 C XML Schema {{recommendation}} {{defines the}} structure and data types for XML documents, but lacks explicit support for time-varying XML documents or for a time-varying schema. In previous work we introduced τXSchema which is an infrastructure and suite of tools to support the creation and validation of time-varying documents, without requiring any changes to XML Schema. In this paper we extend τXSchema to support versioning of the schema itself. We introduce {{the concept of a}} bundle, which is an XML document that references a base (nontemporal) schema, temporal annotations describing how the document can change, and physical annotations describing where timestamps are placed. When the schema is versioned, the base schema and temporal and <b>physical</b> <b>schemas</b> can themselves be time-varying documents, each with their own (possibly versioned) schemas. We describe how the validator can be extended to validate documents in this seeming precarious situation of data that changes over time, while its schema and even its representation are also changing. 1...|$|R
30|$|While the {{conceptual}} schema is conflated with the logical schema in many OGC domain data standards, we consider distinct {{development of a}} conceptual schema to be vital {{to the design of}} a data standard for several reasons: it is more easily understood by domain specialists than technology-laden logical or <b>physical</b> <b>schemas,</b> making it easier to engage them during design and usage; it provides a stable foundation for representation within the domain, one that is independent of technological changes and minimizes conceptual biases and conflicts, thus reducing both development and maintenance efforts; it eases schema development, by separating domain concerns from technological concerns, allowing focus on a single set of domain problems instead of a mixture of domain issues, technological issues, and varied understandings of existing schemas and their conceptual commitments—as a side-effect, this also allows more focused allotment of domain versus technological expertise; and most importantly, it provides the flexibility to have multiple logical implementations. A negative consequence of this approach is the increased maintenance cost of keeping conceptual and logical schemas aligned, however we consider the benefits to far outweigh the detriments.|$|R
40|$|The {{selection}} of an efficient <b>physical</b> <b>schema</b> is an NP-complete problem. In this paper, {{we show that}} crucial parts of physical database design can be smoothly modelled as a Dempster-Shafer application. We exploit {{the properties of the}} Dempster-Shafer theory to model explicitly a rich set of heuristics [...] used for the {{selection of}} an efficient <b>physical</b> <b>schema</b> [...] into knowledge rules. These rules may be loaded into a knowledge base, which, in turn, can be embedded in database design tools...|$|E
40|$|Physical {{database}} design can {{be marked}} {{as a crucial}} step in the overall design process of databases. The outcome of physical database design is a <b>physical</b> <b>schema</b> which describes the storage and access structures of the stored database. The selection of an ecient <b>physical</b> <b>schema</b> is an NP-complete problem. A signi cant number of eorts {{has been reported to}} develop tools that assist in the selection of physical schemas. Most of the eorts implicitly apply a number of heuristics to avoid the evaluation of all schemas. In this paper, we present an approach, based on the Dempster-Shafer theory, that explicitly models a rich set of heuristics |used for the selection of an ecient <b>physical</b> <b>schema</b> | into knowledge rules. These rules may be loaded into a knowledge base, which, in turn, can be embedded in physical database design tools. ...|$|E
3000|$|The system ingests a data {{instance}} {{from its}} source’s API that is {{typically associated with}} a release version. Analysis of the instance’s <b>physical</b> <b>schema</b> is needed to obtain its source schema S [...]...|$|E
40|$|Two {{groups of}} children, one {{composed}} of healthy children, {{the other of}} children with mucoviscidosis, underwent an effort test calibrated in successive levels of progressively increasing intensity. Physiological parameters (cardiorespiratory) and thermal regulation were measured at each level. No statistically significant difference was shown between these two groups. The recorded values correspond to those pointed out by numerous authors. Some bases are established as to the organisation of a <b>physical</b> activity <b>schema</b> for children with mucoviscidosis. SCOPUS: NotDefined. jinfo:eu-repo/semantics/publishe...|$|R
40|$|We {{present an}} {{optimization}} method and al gorithm designed for three objectives: physi cal data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and 2 ̆ 2 backchase 2 ̆ 2 with constraints (dependen cies). By using dictionaries (finite functions) in <b>physical</b> <b>schemas</b> we can capture with con straints useful access {{structures such as}} indexes, materialized views, source capabilities, access support relations, gmaps, etc. The search space for query plans is defined and enumerated in a novel manner: the chase phase rewrites the original query into a 2 ̆ 2 universal 2 ̆ 2 plan that integrates all the access structures and alternative pathways that are allowed by appli cable constraints. Then, the backchase phase produces optimal plans by eliminating various combinations of redundancies, again according to constraints. This method is applicable (sound) to a large class of queries, physical access structures, and semantic constraints. We prove {{that it is in}} fact complete for 2 ̆ 2 path-conjunctive 2 ̆ 2 queries and views with complex objects, classes and dictio naries, going beyond previous theoretical work on processing queries using materialized views...|$|R
40|$|Query workloads and {{database}} schemas in OLAP {{applications are}} becoming increasingly complex. Moreover, the queries and the schemas have to continually evolve to address business requirements. During such repetitive transitions, the order of index deployment has to be considered while designing the <b>physical</b> <b>schemas</b> such as indexes and MVs. An effective index deployment ordering can produce (1) a prompt query runtime improvement and (2) a reduced total deployment time. Both of these are essential qualities of design tools for quickly evolving databases, but optimizing the problem is challenging because of complex index interactions and a factorial number of possible solutions. We formulate the problem in a mathematical model and study several techniques for solving the index ordering problem. We demonstrate that Constraint Programming (CP) is a more flexible and efficient platform {{to solve the problem}} than other methods such as mixed integer programming and A* search. In addition to exact search techniques, we also studied local search algorithms to find near optimal solution very quickly. Our empirical analysis on the TPC-H dataset shows that our pruning techniques can reduce the size of the search space by tens of orders of magnitude. Using the TPC-DS dataset, we verify that our local search algorithm is a highly scalable and stable method for quickly finding a near-optimal solution...|$|R
