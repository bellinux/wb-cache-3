7|20|Public
5000|$|The show is {{produced}} by using <b>prescoring,</b> and projection mapping {{is used in the}} title sequence that describes [...] "The Adventure of the Dancing Men".|$|E
40|$|We {{present a}} novel method for the {{efficient}} denting and bending of rigid bodies {{without the need}} for expensive finite element simulations. Denting is achieved by deforming the triangulated surface of the target body based on a dent map computed on-the-fly from the projectile body using a Z-buffer algorithm with varying degrees of smoothing. Our method accounts for the angle of impact, is applicable to arbitrary shapes, readily scales to thousands of rigid bodies, is amenable to artist control, and also works well in combination with <b>prescoring</b> algorithms for fracture. Bending is addressed by augmenting a rigid body with an articulated skeleton which is used to drive skinning weights for the bending deformation. The articulated skeleton is simulated to include the effects of both elasticity and plasticity. Furthermore, we allow joints to be added dynamically so that bending can occur in a non-predetermined way and/or as dictated by the artist. Conversely, we present an articulation condensation method that greatly simplifies large unneeded branches and chains on-the-fly for increased efficiency. 1...|$|E
40|$|This thesis {{considers}} the numerical simulation {{of a variety}} of phenomena, particularly rigid bodies, deformable bodies, and incompressible fluids. We consider each of these simulations types in isolation, addressing challenges specific to each. We also address the problem of monolithic two-way coupling of each of these phenomena. First we address the stability of rigid body simulation with large time steps. We develop an energy correction for orientation evolution and another correction for collisions. In practice, we have found these two corrections to be sufficient to produce stable simulations. We also explore a simple scheme for rigid body fracture that is as inexpensive as <b>prescoring</b> rigid bodies but more flexible. Next we develop a method for simulating deformable but incompressible solids. Many constitutive models for deforming solids, such as the neo-Hookean model, break down in the incompressible limit. Simply enforcing incompressibility per tetrahedron leads to locking, where the mesh non-physically resists deformation. We present a method that uses a pressure projection similar to what is commonly used to simulate incompressible solids and apply it to deforming solids. We also address the complication...|$|E
30|$|To get the results, 30 -s windows (so called epochs) {{were chosen}} as the time window for {{comparison}} as the <b>prescored</b> annotations are also arranged in 30 -s epochs {{in compliance with the}} rules of Rechtschaffen and Kales [2]. Each epoch is assigned to the sleep stage that occurs most frequently during the time window of that epoch. While comparing the calculated hypnogram with the <b>prescored</b> hypnogram from Physionet database, an agreement of 41.3 % could be reached.|$|R
30|$|To {{achieve the}} result of 41.3 %, the misdetection of each epoch has to be {{compared}} to the <b>prescored</b> annotations of Physionet. Assuming that the scores in the Physionet database are precise, we see that 41.3 % of the detection agreed with the scores of Physionet and a misdetection of 58.7 %.|$|R
40|$|This paper {{investigates the}} use of promptbased content {{features}} for the automated assessment of spontaneous speech in a spoken language proficiency assessment. The results show that single highest performing promptbased content feature measures the number of unique lexical types that overlap with the listening materials and are not contained in either the reading materials or a sample response, with a correlation of r = 0. 450 with holistic proficiency scores provided by humans. Furthermore, linear regression scoring models that combine the proposed promptbased content features with additional spoken language proficiency features are shown to achieve competitive performance with scoring models using content features based on <b>prescored</b> responses. ...|$|R
40|$|Our goal is {{to design}} robust {{algorithms}} {{that can be used}} for building real-time systems, but rather than starting with overly simplistic particle-based methods, we aim to modify higher-end visual effects algorithms. A major stumbling block in utilizing these visual effects algorithms for real-time simulation is their computational intensity. Physics engines struggle to fully exploit available resources to handle high scene complexity due to their need to divide those resources among many smaller time steps, and thus to obtain the maximum spatial complexity we design our algorithms to take only one time step per frame. This requires addressing both accuracy and stability issues for collisions, contact, and evolution in a manner significantly different from a typical simulation in which one can rely on shrinking the time step to ameliorate accuracy and stability issues. In this paper we present a novel algorithm for conserving both energy and momentum when advancing rigid body orientations, as well as a novel technique for clamping energy gain during contact and collisions. We also introduce a technique for fast and realistic fracture of rigid bodies using a novel collision-centered <b>prescoring</b> algorithm...|$|E
40|$|We {{propose a}} virtual node {{algorithm}} that allows material to separate along arbitrary (possibly branched) piecewise linear paths through a mesh. The material within an element is fragmented by creating several {{replicas of the}} element and assigning a portion of real material to each replica. This results in elements that contain both real material and empty regions. The missing material is contained in another copy (or copies) of this element. Our new virtual node algorithm automatically determines the number of replicas and the assignment of material to each. Moreover, it provides the degrees of freedom required to simulate the partially or fully fragmented material in a fashion consistent with the embedded geometry. This approach enables efficient simulation of complex geometry with a simple mesh, i. e. the geometry need not align itself with element boundaries. It also alleviates many shortcomings of traditional Lagrangian simulation techniques for meshes with changing topology. For example, slivers do not require small CFL time step restrictions since they are embedded in well shaped larger elements. To enable robust simulation of embedded geometry, we propose new algorithms for handling rigid body and self collisions. In addition, we present several mechanisms for influencing and controlling fracture with grain boundaries, <b>prescoring,</b> etc. We illustrate our method for both volumetric and thin-shell simulations...|$|E
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Molecular {{recognition}} {{plays an}} important role in biological systems. The purpose of this study was to get a better understanding of the process by incorporating computational tools. Molecular Mechanics-Generalized Born Surface Area (MM-GBSA) method and Molecular Mechanics-Poisson Boltzmann Surface Area (MM-PBSA) method, the end-point free energy calculations provide the binding free energy the can be used to rank-order protein–ligand structures in virtual screening for compound or target identification. Free energy calculations were performed on a diverse set of 11 proteins bound to 14 small molecules was carried out for. A direct comparison was taken between the calculated free energy and the experimental isothermal titration calorimetry (ITC) data. Four and three systems in MM-GBSA and MM-PBSA calculations, respectively, reproduced the ITC free energy within 1 kcal•mol– 1. MM-GBSA exhibited better rank-ordering with a Spearman ρ of 0. 68 compared to 0. 40 for MM-PBSA with dielectric constant (ε = 1). The rank-ordering performance of MM-PBSA improved with increasing ε (ρ = 0. 91 for ε = 10), but the contributions of electrostatics became significantly lower at larger ε level, suggesting that the only nonpolar and entropy components contribute to the improved results. Our previously developed scoring function, Support Vector Regression Knowledge-Based (SVRKB), resulted in excellent rank-ordering (ρ = 0. 81) when applied into MD simulations. Filtering MD snapshots by <b>prescoring</b> protein–ligand complexes with a machine learning-based approach (SVMSP) resulted in a significant improvement in the MM-PBSA results (ε = 1) from ρ = 0. 40 to ρ = 0. 81. Finally, the nonpolar components in the free energy calculations showed strong correlation to the ITC free energy while the electrostatic components did not; the computed entropies did not correlate with the ITC entropy. Explicit-solvent molecular dynamics (MD) simulations offer an opportunity to sample multiple conformational states of a protein-ligand system in molecular recognition. SVMSP is a target-specific rescoring method that combines machine learning with statistical potentials. We evaluate the performance of SVMSP in its ability to enrich chemical libraries docked to MD structures. Seven proteins from the Directory of Useful Decoys (DUD) were involved in the study. We followed an innovative approach by training SVMSP scoring models using MD structures (SVMSPMD). The resulting models remarkably improved enrichment in two cases. We also explored approaches for a prior identification of MD snapshots with high enrichment power from an MD simulation in the absence of active compounds. SVMSP rescoring of protein–compound MD structures was applied for the search of small-molecule inhibitors of the mitochondrial enzyme aldehyde dehydrogenase 2 (ALDH 2). Rank-ordering of a commercial library of 50, 000 compounds docked to MD optimized structures of ALDH 2 led to five small-molecule inhibitors. Four compounds had IC 50 s below 5 μM. These compounds serve as leads for the design and synthesis of more potent and selective ALDH 2 inhibitors...|$|E
40|$|The {{fact that}} {{difference}} scores {{tend to be}} less reli-able than the original measurements from which they are calculated should not be a matter of concern in testing the significance of treatment-induced change. The reliabilities of the original measurements are im-portant because unreliability attenuates correlation, and substantial correlation between <b>prescores</b> and post-scores is required for difference scores to be of value in controlling for individual differences. Reliability notwithstanding, difference scores provide superior control over true baseline differences in quasi-experi-mental research, whereas the analysis of covariance (ANCOVA) is generally preferable for baseline control in randomized experimental designs. Index terms: analysis of covariance, baseline correction, differenc...|$|R
40|$|Automatic scoring {{of short}} text {{responses}} to educational assessment items is a challenging task, particularly because {{large amounts of}} labeled data (i. e., human-scored responses) {{may or may not}} be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e. g., to give higher scores to responses that are similar), <b>prescored</b> student responses (e. g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measure...|$|R
30|$|The lack of Dimensions of Teamwork {{significance}} may {{be attributed}} to unusually high <b>prescores</b> for this group of students. It is unclear whether these students actually did enter the project with well-developed collaboration skills (although instructor/author Steege notes that many of these students knew each other from their study of industrial engineering and may have worked collaboratively before) or if rather their pretest scores were inflated. In subsequent projects, we plan to not only implement both a pretest and posttest of the Dimensions of Teamwork as we did for this study but also collect “retrospective pretest” data {{at the end of the}} collaborative experience. A retrospective pretest asks respondents to rate both their current abilities post the collaborative experiences as well as to reflect upon and report their assessment of their collaborative abilities prior to the collaborative experience, thus allowing respondents to reflect upon and report changes in skills.|$|R
40|$|Molecular {{recognition}} {{plays an}} important role in biological systems. The purpose of this study was get better understanding of the process by incorporating computational tools. Molecular Mechanics-Generalized Born Surface Area (MM-GBSA) method and Molecular Mechanics-Poisson Boltzmann Surface Area (MM-PBSA) method, the end-point free energy calculations provide the binding free energy the can be used to rank-order protein-ligand structures in virtual screening for compound or target identification. Free energy calculations were performed on a diverse set of 11 proteins bound to 14 small molecules was carried out for. A direct comparison was taken between the calculated free energy and the experimental isothermal titration calorimetry (ITC) data. Four and three systems in MM-GBSA and MM-PBSA calculations, respectively, reproduced the ITC free energy within 1 kcal·mol- 1. MM-GBSA exhibited better rank-ordering with a Spearman ρ of 0. 68 compared to 0. 40 for MM-PBSA with dielectric constant (2 ̆ 6 epsiv; = 1). The rank-ordering performance of MM-PBSA improved with increasing 2 ̆ 6 epsiv; (ρ = 0. 91 for 2 ̆ 6 epsiv; = 10), but the contributions of electrostatics became significantly lower at larger 2 ̆ 6 epsiv; level, suggesting that the only nonpolar and entropy components contribute to the improved results. Our previously developed scoring function, Support Vector Regression Knowledge-Based (SVRKB), resulted in excellent rank-ordering (ρ = 0. 81) when applied into MD simulations. Filtering MD snapshots by <b>prescoring</b> protein-ligand complexes with a machine learning-based approach (SVMSP) resulted in a significant improvement in the MM-PBSA results (2 ̆ 6 epsiv; = 1) from ρ = 0. 40 to ρ = 0. 81. Finally, the nonpolar components in the free energy calculations showed strong correlation to the ITC free energy while the electrostatic components did not; the computed entropies did not correlate with the ITC entropy. ^ Explicit-solvent molecular dynamics (MD) simulations offer an opportunity to sample multiple conformational states of a protein-ligand system in molecular recognition. SVMSP is a target-specific rescoring method that combines machine learning with statistical potentials. We evaluate the performance of SVMSP in its ability to enrich chemical libraries docked to MD structures. Seven proteins from the Directory of Useful Decoys (DUD) were involved in the study. We followed an innovative approach by training SVMSP scoring models using MD structures (SVMSPMD). The resulting models remarkably improved enrichment in two cases. We also explored approaches for a priori identification of MD snapshots with high enrichment power from an MD simulation in the absence of active compounds. SVMSP rescoring of protein-compound MD structures was applied for the search of small-molecule inhibitors of the mitochondrial enzyme aldehyde dehydrogenase 2 (ALDH 2). Rank-ordering of a commercial library of 50, 000 compounds docked to MD optimized structures of ALDH 2 led to five small-molecule inhibitors. Four compounds had IC 50 s below 5 2 ̆ 6 mgr;M. These compounds serve as leads for the design and synthesis of more potent and selective ALDH 2 inhibitors. ...|$|E
40|$|This study {{compared}} 70 English learners (ELs) and English-only (EO) second-grade students’ writing samples {{before and}} after a yearlong writing program. The school utilized Write From the Beginning (J. Buckner, 2006) and focused on personal narratives. A subgroup of students also participated in an intervention supporting expository writing on curricular topics. Sociocognitive theory framed the Modeled Writing (MW) used in this study. An analysis of covariance used <b>prescores</b> on 2 writing assessments to compare students’ writing achievement {{at the end of}} the year, and t tests compared students’ writing by gender, language, and group on various pre- and posttest scores. Results indicate that MW benefited both EOs and ELs and that the MW students outscored the controls on all items of the standardized writing assessment at year’s end. The comparison affords greater understanding of writing development and achievement differences among young ELs and EOs and suggests instructional and research opportunities...|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 240 p. PlanIt!, a computerized problem-solving tool, was designed to assist students in problem solving by presenting 20 prompts which were classified under four phases: (a) Understanding the problem, (b) Planning a solution, (c) Solving the problem, and (d) Reviewing the solution. These prompts were designed to help students organize and reflect upon their problem-solving processes. This study analyzed the effectiveness of PlanIt! as a problem-solving tool. Subjects were administered a pretest to assess their initial ability to solve multi-step, algebraic word problems. Subjects were randomly assigned to experimental and control groups. The control group used an abbreviated word processing version of PlanIt! During the treatment period, subjects were administered an alternate form of the pretest problems. Additional information collected included {{the amount of time spent}} on the treatment task and, for the experimental subjects, the number of PlanIt! prompts answered. The following week, an algebraic posttest was administered to assess whether any portion of PlanIt!'s solving heuristic had been remembered. In the final session, a second posttest, an open-ended problem different from the previous tasks, was administered to assess whether the PlanIt! heuristic would transfer to a novel problem type. Analyses of variance were performed to analyze group differences for the dependent variables. There were significant differences between the experimental and control groups on treatment test scores and time on treatment task. The experimental group spent significantly more time on the treatment test using PlanIt!, but the control group scored significantly higher on the treatment test. There were no significant differences between experimental and control groups on pretest or either posttest. The regression analysis revealed that for the experimental group, time on task and <b>prescore</b> were predictive of posttest performance. For the control group, math level and <b>prescore</b> were predictive of posttest performance. Thus, when used with minimal practice, brief training, and multi-step mathematical problems, PlanIt! did not significantly improve problem-solving performance. Implications for the PlanIt! program and training general problem solving heuristics are discussed...|$|R
30|$|The {{purpose of}} this {{experiment}} is to first review existing sleep staging methods that use the heart rate signal as a classifier. In order to test whether ECG alone {{can be used for}} accurate sleep stage classification compared to PSG, one algorithm was selected and implemented within a desktop application. Different parameters are computed from the heart rate signal to distinguish between the different sleep stages. The reason for choosing the algorithm described in the paper [3] by Canisius was that it was only using the heart rate as a discriminator, looked detailed and quite accurate (57.5 % agreement). The question that came up was if {{it would be possible to}} reproduce the result by using a pre-scored recording of the ECG. For the algorithm development, the ECG data from the “Sleep Heart Health Study PSG Database” available on the Physionet database was used [4]. It comes with annotations from a <b>prescored</b> hypnogram by specialists using the entirety of all 11 channels of the record. Therefore, this record is applicable for an evaluation of the developed algorithm.|$|R
40|$|The {{present study}} {{investigated}} whether MI is {{a mechanism for}} change {{in the treatment of}} obsessive-compulsive disorder (OCD). The Magical Ideation scale (MI), the Obsessive-Compulsive Inventory - Short Version (OCI-SV) and the Padua Inventory were completed by 34 obsessive-compulsive patients pre- and post cognitive-behavioural treatment. Treatment did not target magical styles of thinking. Significant improvements on all three measures of obsessive-compulsive symptoms were demonstrated by t tests over the course of treatment. Improvement in magical thinking was also shown to be significant in t test results. In support of the hypothesis, correlations between MI improvement and improvement on the obsessive-compulsive symptom scales were significant (at a level of. 05) suggesting that there is an association between improvement in magical thinking and improvement in obsessive-compulsive symptoms. Notably, a significant negative correlation was obtained between <b>prescores</b> on MI and change scores on the OCD measures. This suggests that high levels of MI are associated with high levels of treatment intractability. High MI appears to be a poor prognostic factor in OCD. 7 page(s...|$|R
40|$|Protoplasts {{of genetically}} marked {{derivatives}} of Staphylococcus aureus NCTC 8325 were fused with polyethylene glycol and regenerated without selection. Recombinants possessing one specific resistance marker from each parent {{were selected from}} the regenerated population and scored for seven or eight unselected markers. The results of these 9 - and 10 -factor crosses were entered directly into a programmed microcomputer from <b>prescored</b> replica plates. The data then were condensed into an array of phenotypes, together with {{the frequency with which}} each occurred. Further analyses by computer included the calculation of coinheritance frequencies for all possible pairs of markers; after entering a proposed order for the markers being analyzed, the minimum number of crossover events required to generate each phenotypic class was calculated. The linkage relationships of markers, based on the protoplast fusion data, were entirely consistent with the linkage relationships of markers already known to exist within each of the three linkage groups previously defined by transformation. The fusion data defined an arrangement of the three linkage groups into a circular chromosome map and predicted the approximate location of four previously unmapped markers (tet- 3490, fus- 149, purC 193 ::Tn 551, and omega [Chr::Tn 551] 42) on this map...|$|R
40|$|Owing to the {{increased}} volume of laboratory services and the shortage of skilled medical microbiologists who presently spend up to 30 % {{of their time in}} clerical matters, pragmatic applications of electronic sorting techniques and computers should be considered to alleviate this problem. Moreover, surveillance of the hospital community, with particular reference to changing patterns of microbial resistance and the distribution of potentially infectious pathogens, requires detailed information which can be readily supplied by electronic sorting analysis. Mark-sense and <b>prescored</b> Port-A-Punch IBM cards were used to: (i) analyze antibiotic susceptibility data; (ii) tabulate total test loads according to conditions set down by the American Society for Clinical Pathologists; and (iii) to prepare a bacteriological report on the surveillance of hospital infections. After proper sorting and analysis, the cards also serve as a convenient reference file in the laboratory for pertinent information recorded by either blackening the appropriate areas (mark-sense style) or pushing out the preperforated rectangular holes with a simple inexpensive board and stylus (Port-A-Punch). No one scheme can fulfill the requirements of all laboratories or purposes, but ideas contained herein might serve as starting points for the design of similar systems in other laboratories...|$|R
40|$|Self-report {{questionnaires}} are economical {{instruments for}} routine outcome assessment. In this study, {{the performance of}} the German version of the Outcome Questionnaire- 45 (OQ- 45) and the Brief Symptom Inventory (BSI) was evaluated when applied in analysis of the outcome quality of psychiatric and psychotherapeutic interventions. Pre-post data from two inpatient samples (N= 5711) and one outpatient sample (N= 239) were analyzed. Critical differences (reliable change index) and cut-off points between functional and dysfunctional populations were calculated using the Jacobson and Truax method of calculating clinical significance. Overall, the results indicated that the BSI was more accurate than the OQ- 45 in correctly classifying patients as clinical subjects. Nonetheless, even with the BSI, about 25 % of inpatients with schizophrenia attained a score at admission below the clinical cut-off. Both questionnaires exhibited the highest sensitivity to psychopathology with patients with personality disorders. When considering the differences in the <b>prescores,</b> both questionnaires showed the same sensitivity to change. The advantage of using these self-report measures is observed primarily in assessing outpatient psychotherapy outcome. In an inpatient setting two main problems—namely, the low response rate and the scarce sensitivity to psychopathology with severely ill patients—limit the usability of self-report questionnaires...|$|R
5000|$|Similar in {{format to}} the M26, the M33 is a spherical {{fragmentation}} grenade also filled with Composition B explosive. Unlike the M26, the inner wall is <b>prescored</b> {{and does not}} contain a fragmentation coil. The grenade has a smooth surface, unlike the Mk II series [...] "pineapple" [...] casing. The M67 {{was part of a}} similar PIP, and is identical to the M33 {{with the exception of the}} additional safety clip for the spoon of the grenade on the M67. Early M33 grenades were also more of an oval shape before transitioning over to the more spherical shape of the M67. The M67 is the current issue HE-Frag grenade for the US military. The M33A1 is the impact version of the M33, using the M217 electrical impact fuze in place of the normal delay fuze. At an unknown time the M33A1 was redesignated as the M59. There should be no difference between these grenades except that the M33A1 may be more ovular in shape than the more spherical M59. The M68 is the same as the M59, with the exception of the additional safety clip.|$|R
40|$|Background: Trimetazidine is a fatty {{oxidation}} inhibitor, {{leading to}} shifting of energy substrate from fatty acid oxidation toward glucose oxidation {{that leads to}} the reduction of oxygen requirement. The aims of the present study were to elucidate the effects of trimetazidine on psychomotor performance and vigilance on normal healthy volunteers. Materials and Methods: A total of 234 subjects (age 22 – 25 years) were recruited in this study. The volunteers were randomizing into two groups with 117 volunteers in each group. Group I received an inert starch capsule served as a control, and Group II received trimetazidine tablet 15 mg/day. The duration of therapy was 5 days. Test procedure was done at 9. 00 a. m. on the psychomotor tester. Before the drug administration, <b>prescore</b> values were recorded and then after 5 days of therapy, the postscore values were recorded. Results: The placebo did not demonstrate a significant effect on all psychomotor performances and flicker-fusion elements (P > 0. 05). Trimetazidine therapy produced a highly significant effect on all components of psychomotor performances and flicker-fusion parameters (P < 0. 001) compared with pretreatment era. Conclusion: We conclude that trimetazidine improves psychomotor performance and vigilance in normal healthy volunteers through advancing total reaction time and critical flicker-fusion frequency...|$|R
40|$|Previous physics {{education}} research {{has raised the}} question of “hidden variables” behind students’ success in learning certain concepts. In the context of the force concept, {{it has been suggested that}} students’ reasoning ability is one such variable. Strong positive correlations between students’ preinstruction scores for reasoning ability (measured by Lawson’s Classroom Test of Scientific Reasoning) and their learning of forces [measured by the Force Concept Inventory (FCI) ] have been reported in high school and university introductory courses. However, there is no published research concerning the relation between students’ ability to interpret multiple representations consistently (i. e., representational consistency) and their learning of forces. To investigate this, we collected 131 high school students’ pre- and post-test data of the Representational Variant of the Force Concept Inventory (for representational consistency) and the FCI. The students’ Lawson pretest data were also collected. We found that the preinstruction level of students’ representational consistency correlated strongly with student learning gain of forces. The correlation (0. 51) was almost equal to the correlation between Lawson <b>prescore</b> and learning gain of forces (0. 52). Our results support earlier findings which suggest that scientific reasoning ability is a hidden variable behind the learning of forces. In addition, we suggest that students’ representational consistency may also be such a factor, and that this should be recognized in physics teaching...|$|R
40|$|Materials and Methods: We {{developed}} {{questionnaire to}} address QoL {{issues to be}} studied in Muslim patients. This instrument was tested and retested and then 100 patients were studied to assess the influence of stoma on QoL of Muslim Patients with stoma. Results: A total of 100 patients were studied with mean age 38. 56 (range 16 - 78 years) and male to female ratio of 3. 54 : 1 Majority of patients (n= 82) had ileostomy while 18 patients had colostomy. Majority (80 %) of stomas were due to infective causes, while 12 were for trauma and 8 due to malignant disease. Permanent stoma was fashioned in only 3 for malignant disease. Mean <b>Prescore</b> of 79. 63 fell to a postscore of 55. 79 (P value 0. 000). Mean score of work and social function was 33. 75, sexuality and body image was 39. 45, stoma function was 7. 56, financial concern was 34. 50, skin irritation was 25. 10, religious well being was 49. 60. Conclusion: Stoma adversely affects QoL in our population. QoL in religious matters is however relatively preserved, indicating a better counseling and control of religious leaders in Muslim community. This is probably because of poor management and counseling services provided to stoma patients in our community. standards of QoL in our populatio...|$|R
40|$|We {{have found}} that non-STEM (science, technology, engineering, and mathematics) majors taking either a {{conceptual}} physics or astronomy course at two regional comprehensive institutions score significantly lower preinstruction on the Lawson’s Classroom Test of Scientific Reasoning (LCTSR) in comparison to national average STEM majors. Based on LCTSR score, the majority of non-STEM students can be classified as either concrete operational or transitional reasoners in Piaget’s theory of cognitive development, whereas in the STEM population formal operational reasoners are far more prevalent. In particular, non-STEM students demonstrate significant difficulty with proportional and hypothetico-deductive reasoning. <b>Prescores</b> on the LCTSR are correlated with normalized learning gains on various concept inventories. The correlation is strongest for content that can be categorized as mostly theoretical, meaning a lack of directly observable exemplars, and weakest for content categorized as mostly descriptive, where directly observable exemplars are abundant. Although the implementation of research-verified, interactive engagement pedagogy can lead to gains in content knowledge, significant gains in theoretical content (such as force and energy) are more difficult with non-STEM students. We also observe no significant gains on the LCTSR without explicit instruction in scientific reasoning patterns. These results further demonstrate that differences in student populations are important when comparing normalized gains on concept inventories, and the achievement of significant gains in scientific reasoning requires a reevaluation of the traditional approach to physics for non-STEM students...|$|R
40|$|Five issues {{relative}} {{to the use of}} different Ordinary Least Squares (OLS) and Hierarchical Linear Modeling (HLM) models to identify effective schools and teachers were examined using data from all students in the Dallas (Texas) public schools in grade 3 in 1994 and grade 4 in 1995. OLS models using first- and second-order interactions produced results that were very close to those produced by two-level HLM models at the school level and two- and three-level HLM models at the teacher level. Most OLS regression and HLM models used in this study accounted for more than 70 % of the variance in student achievement in reading and mathematics. Results produced by all the models were extremely consistent, and correlations produced by the various models were all generally above 0. 90. Correlations of results with important school, teacher, and student-level contextual variables were negligible for all models, meaning that the various models produced results that were free from bias {{relative to}} important contextual variables. Correlations of results with <b>prescore</b> characteristics were negligible for all models, meaning that the various models produced results that were free from bias {{relative to the}} level of pretest scores. Taking all results into consideration, it is recommended that a two-level HLM model (student-school) be implemented to determine school effect, and that the empirical Bayes residuals from the model be adjusted with an adjustment for shrinkage to form the basis for estimates of teacher effect. Appropriat...|$|R
40|$|Although {{hospitals}} {{are the most}} likely place of death, the quality of care received by dying inpatients remains variable. This is concerning for both the dying person and their relatives, with poorer bereavement outcomes likely for those who perceived their family member suffered unduly. There is a real need to consider how this situation can be improved. This work was conducted with the aim of exploring the feasibility of including bereaved relatives’ experiences {{as part of a larger}} project exploring the use of a care bundle to improve care of the dying inpatients. Fifty relatives of inpatients who had died previously in hospital were contacted by letter with a request for interview before the implementation of a care bundle for the dying, with a care bundle being a collection of care processes that are implemented together. After this project had been in place for 6 months, a further 50 families were contacted who had died on the bundle. Ten families responded initially to the first request and 10 the second, with the interviews based on the Quality of Dying and Death (QODD) tool and a final open-ended question. Although all families who agree to be interviewed completed the session, with regard to the QODD, some families indicated that they would rather talk than provide numeric scores. No major differences in the <b>prescores</b> and postscores were noted. When invited to share their experiences, without prompting, families spoke of consistent concerns that included communication, place of death, and symptom control. This work confirms that it is highly feasible to incorporate assessments of bereaved family members’ opinions as part of the wider assessment of research into end-of-life care...|$|R

