3|12|Public
5000|$|Files are {{regularly}} backed up to tape {{unless they have}} been marked as NOSAVE. The file save process includes full and partial backups. Full saves are typically done once a week with no users {{signed on to the}} system. Partial saves save just the files that have changed since the last full or <b>partial</b> <b>save</b> and are typically done once each day in the late evening or early morning during normal operation with users signed on to the system. At the University of Michigan two copies of the full save tapes were made and one copy was stored [...] "off-site". Save tapes were kept for six weeks and then reused. The tapes from every sixth full save were kept [...] "forever". Files are saved to allow recovery from [...] "disk disasters" [...] in which the file system becomes damaged or corrupt, usually due to a hardware failure. But users could restore individual files as well using the program *RESTORE.|$|E
40|$|Abstract⎯⎯Forcing all IP packets {{to carry}} correct source {{addresses}} can greatly help network security, attack tracing, and network problem debugging. However, due to asymmetries in today's Internet routing, routers {{do not have}} readily available information to verify the correctness of the source address for each incoming packet. In this paper we describe a new protocol, named SAVE, that can provide routers with the information needed for source address validation. SAVE messages propagate valid source address information from the source location to all destinations, allowing each router {{along the way to}} build an incoming table that associates each incoming interface of the router with a set of valid source address blocks. This paper presents the protocol design and evaluates its correctness and performance by simulation experiments. The paper also discusses the issues of protocol security, the effectiveness of <b>partial</b> <b>SAVE</b> deployment, and the handling of unconventional forms of network routing, such as mobile IP and tunneling. I...|$|E
50|$|Addition of <b>Partial</b> State <b>Saving</b> and DOM updates {{are part}} of the {{built-in}} standardized Ajax support.|$|R
5000|$|Without {{diluting}} or denying any... criticisms, {{it should}} be said that from World War I to Korea, Milestone could put the viewer {{into the middle of}} a battlefield, and make the hellish confusion of it seem all too real to the viewer. Steven Spielberg noted as much when he credited Milestone's work as <b>partial</b> inspiration for <b>Saving</b> Private Ryan ...Lewis Milestone made significant contributions to genre of the war film.|$|R
50|$|Division can be {{performed}} in a similar fashion. Let's divide 46785399 by 96431, the two numbers we used in the earlier example. Put the bars for the divisor (96431) on the board, {{as shown in the}} graphic below. Using the abacus, find all the products of the divisor from 1 to 9 by reading the displayed numbers. Note that the dividend has eight digits, whereas the <b>partial</b> products (<b>save</b> for the first one) all have six. So you must temporarily ignore the final two digits of 46785399, namely the '99', leaving the number 467853. Next, look for the greatest partial product that is less than the truncated dividend. In this case, it's 385724. You must mark down two things, as seen in the diagram: since 385724 is in the '4' row of the abacus, mark down a '4' as the left-most digit of the quotient; also write the partial product, left-aligned, under the original dividend, and subtract the two terms. You get the difference as 8212999. Repeat the same steps as above: truncate the number to six digits, chose the partial product immediately less than the truncated number, write the row number as the next digit of the quotient, and subtract the partial product from the difference found in the first repetition. Following the diagram should clarify this. Repeat this cycle until the result of subtraction is less than the divisor. The number left is the remainder.|$|R
40|$|A {{method of}} {{deductive}} learning is developed to control deductive inference. Our {{goal is to}} improve problem solving time by experience, when that experience monotonically adds knowledge to the knowledge base. In particular, for deductive reasoning systems where <b>partial</b> results are <b>saved</b> during a derivation and at least some partial results are themselves deduction rules, we suggest ways of taking maximal advantage of old partial results and avoiding the regeneration of partial results when solving new problems. We approach the problem of accumulating past deductive experience and using it in subsequent similar deductions bythescheme of knowledge migration and knowledge shadowing. Knowledge migration generates a specific rule from a general rule during a deduction, and accumulates deduction experience that {{is represented by the}} specificity relationship between the general (migrating) rule and the specific (migrated) rule. Knowledge shadowing uses deduction experience obtained in previous inferences for faster reasoning in future inference. If both general and specific knowledge are applicable in subsequent deduction, the knowledge shadowing scheme is activated to select only the specific knowledge. Three principles for knowledge shadowing are presented. A preliminary result shows that knowledge migration and shadowing greatly contribute to the system performance...|$|R
40|$|Meta-heuristic methods show better {{performance}} in solving large-size sequential process scheduling problems than mixed-integer linear programming. Heuristic rules {{are often used}} in meta-heuristic methods and play {{a very important role}} in reducing search space of the scheduling problems. In our previous work (He and Hui, Ind Eng Chem Res. 2006; 45 : 4679 - 4692), approaches of how to automatically select rules from a set of heuristic rules have been proposed. This work proposes a novel approach of how to construct a comprehensive, but not very large set of rules according to the analysis of impact factors. Working with the new rule set, the original approaches are improved, in which a full rule sequence is used for schedule synthesis. A new automatic rule combination approach is proposed, in which a partial rule sequence is intentionally formed and used for schedule synthesis. The adoption of the <b>partial</b> rule sequences <b>saves</b> computational sources and increases the search ability of the algorithms. The automatic rule combination approach almost has the same search ability as the long time tabu search to find the near-optimal solutions to the large-size problems, but with much higher convergence speed. (c) 2007 American Institute of Chemical Engineers...|$|R
50|$|On 23 November U-69 {{was ordered}} by Naval Command to sail to sector AK in the Atlantic {{southeast}} of Greenland and southwest of Iceland. Through adverse weather U-69 {{set to the}} new course. At 8 p.m. on 26 November 1941 Zahn under inclement weather decided to track a lone freighter moving slowly in heavy seas under snow and hail. After two hours of sailing on the surface Zahn decided on a surface attack against the freighter and released four torpedoes all of which failed to hit the target. Zahn ordered the submarine to submerge to load the four tubes with new torpedoes and upon resurfacing the target could not be located again. Although Dönitz sent messages concerning more targets after that U-69 {{was not able to}} locate them and on 3 December it was ordered back to St. Nazaire. Upon arrival, after 39 days of patrol in the Atlantic, captain Eberhard Godt, the U-boat chief of operations, reprimanded Zahn for his failure to sink any targets and although he acknowledged the impact of the severe weather he told Zahn in future not to submerge {{for such a long time}} to reload all torpedo tubes but to only perform a <b>partial</b> reloading to <b>save</b> time.|$|R
40|$|In {{the paper}} Pedersen, Nielsen, and Andersen [5] we {{developed}} an algorithm for ranking n×n assign-ments using reoptimization and compare our algorithm with other algorithms {{with the same}} time complex-ity. However, as pointed out by Dr. A. Volgenant, we unfortunately missed one available implementation written by Miller, Stone, and Cox [3] in IEEE Transactions on Aerospace and Electronic Systems. The algorithm of Miller et al. [3] (Miller) and Pedersen et al. [5] (DU 1) are both based on the branching technique presented in Murty [4] where the set of possible assignments is partitioned into at most n− 1 disjoint subsets for each additional ranking made. In each subset the best assignment is found by applying the successive shortest path procedure implementation of Jonker and Volgenant [2]. The algorithms differ in the following ways: 1. In DU 1 dual variables are updated before reoptimization. This {{is not the case}} in Miller. 2. In Miller (<b>partial)</b> assignments are <b>saved</b> while maintaining the candidate set. In DU 1 the optimal assignment is recalculated each time a subset is selected from the candidate set to reduce memory size. 3. An interval heap is used in DU 1 to maintain a priority deque of the candidate set and keeping the size of the queue low. Miller uses a priority queue of the candidate set...|$|R
40|$|CONCLUSIONS: In {{the near}} future salvage supracricoid {{laryngectomy}} (SCL) will be used more extensively for failures of radiotherapy for glottic carcinoma. OBJECTIVES: Primary radiotherapy {{has been used for}} patients with early glottic carcinomas in northern Europe and North America {{for more than half a}} century. Local recurrences after radiotherapy for glottic malignancies occur in 5 - 25 % for T 1 carcinomas and in 15 - 50 % for T 2 carcinomas. The classic choice as salvage surgery in cases of glottic squamous cell carcinoma recurrence after irradiation failure is total laryngectomy. The development of extended conservation procedures such as SCL has permitted an increasing number of successful <b>partial</b> laryngectomies that <b>save</b> laryngeal functions after radiotherapy failure. SCL allows the creation of a neo-larynx, permitting both swallowing and speech; in most cases the tracheostoma can be closed. METHODS: The electronic database Pubmed was searched without publication date limits. RESULTS: Considering available data (103 cases), 84. 5 % of the cases treated with salvage SCL for irradiation failure did not present a new local recurrence; laryngeal recurrences after salvage SCL (15. 5 %) were successfully treated with total laryngectomy in 66. 7 % of the cases. Tracheostoma closure was possible in all except two cases after a mean period ranging between 12 and 28 days. Swallowing results seemed good, with longer recovery time in irradiated than in non-irradiated patients who underwent SCL. Voice quality determined with psychoacoustic methods had acceptable intelligibility...|$|R
30|$|Due to the {{increased}} data traffic and the co-existence of different radio access technologies, efficient resource management is a key challenge for future 5 G networks. In this paper, we presented SDN-based Wi-Fi data offloading and load balancing algorithms. The new algorithms utilized the controller’s global view of the network to take more informed decisions for efficient resource management. We also analyzed {{the performance of the}} proposed algorithms under realistic load conditions. To this end, we first introduced a queuing model with Pareto arrivals to analyze the processing and forwarding delays incurred due to the SDN architecture. Then, we analyzed the performance of the proposed SDN-based partial data offloading scheme in terms of the threshold miss probability and the amount of data offloaded successfully onto Wi-Fi. Through simulations, it was shown that <b>partial</b> data offloading <b>saves</b> primary resources and decreases threshold miss probability by 20 %∼ 50 %, which ultimately improves the application performance at the user end. Furthermore, the simulation results also confirmed that SDN-based LB outperforms the baseline methods by minimizing the number of required handovers by 50 % and by balancing the loads more evenly across multiple cells. Our results and discussions showed that the delay incurred by SDN is well within the acceptable limits for most applications. Particularly, it was shown that SDN-based solutions perform better for large data traffic with high delay tolerance. All in all, SDN is shown to be a suitable enabling technology for introducing intelligence within the wireless networks and for providing fine-grained control to the network operators.|$|R
40|$|Calculating the {{likelihood}} of observed DNA sequence data at the leaves of a tree is the computational bottleneck for phylogenetic analysis by Bayesian methods or by the method of maximum likelihood. Because analysis of even moderately sized data sets can require hours of computational time on fast desktop computers, algorithmic changes that substantially increase {{the speed of the}} basic likelihood calculation are significant. It has long been recognized that the contribution to {{the likelihood}} at sites with identical patterns is the same and need only be computed once for each unique pattern. We note that sites whose patterns are not identical on the entire tree may be identical on subtrees, and hence partial likelihood calculations made for one site may be stored and used for calculations at another. The bookkeeping and memory requirements are large, but not too excessive for current desktop computers. Timed calculations on many genuine data sets indicate that the computational algorithm we present in this paper for likelihood calculations on trees result in decreases in running time by a factor ranging from 1. 1 to 5. 2 for data sets considered. There is reason to believe similar increases in speed would be more generally realized on alternative trees and data sets. 1 <b>Saving</b> <b>partial</b> calculations for the likelihood at one site for use in the calculation of the likelihood at another site can greatly decrease the computational time necessary to evaluate {{the likelihood of}} a phylogenetic tree. We have taken advantage of this in ou...|$|R
40|$|IN 1940 an {{excellent}} opportunity arose to obtain data and specimens {{for the study of}} several phases of the life history of the river otter (L u t r a canadensis). The Michigan De-partment of Conservation after maintaining complete protec-tion of otter for the preceding fifteen years again established an open season. Regulations permitted each lioensed trapper to take two otters and required him to turn in the carcasses to conservation officers at the time the pelts were presented for the official seal. The conservation officers obtained data on localities of capture from the trappers and forwarded this data with the carcasses to the Game Division Laboratory of the Conservation Department at Lansing. There in 1940 and again {{at the end of the}} trapping period in 1941 and 1943, Ostenson recorded weights, measurements, and other data and preserved certain parts of the carcasses for study. Stomachs and intestines were removed for an analysis of the food habits and have been reported by Lagler and Ostenson (1942). From a total of several hundred individuals handled, 131 skulls and <b>partial</b> skeletons were <b>saved</b> for the osteological collections of the University of Michigan Museum of Zoology. These form the basis for the present study, supplemented by 2 Hooper and Ostenson ~ c c. Papels thirteen specimens taken at various seasons of the year which were already available in the Museum's collections. All specimens reported here with the exception of not more than ten individuals, were collected in March or April in the legal trapping seasons as follows: I n the Lower Peninsula, Marc...|$|R
40|$|Air {{conditioning}} and refrigeration systems {{are designed to}} meet the load at some particular design conditions. At off-design conditions, the compressor is either cycled or a variable speed drive enables the compressor to run at a speed that matches the load. However, cycling can incur efficiency losses as large as 10 - 20 % of total compressor power. Similarly, the variable speed compressors suffer from an efficiency penalty {{as a result of the}} losses in the inverter drive (4 - 6 %). Part 1 of this report focuses on the use of tandem compressors, that is, a combination of two or more compressors running in parallel with each other, to more closely match the load at different conditions. Simulation models were run for various operating conditions and the results from the tandem simulations were compared with the single speed and the variable speed compressor results, showing a significant COP over the single speed case and a smaller improvement over the variable speed case. Temperature data from Pittsburgh and Dallas was then incorporated, and the optimum compressor sizing and operating strategies were compared. Results again showed a COP gain of 7. 5 % and 6. 2 % for the two cities, respectively, over the case of a single-speed compressor, and 2. 2 % and 2. 0 % compared to a variable-speed compressor. Part 2 focuses on vapor injection in case of scroll compressors used in air-{{conditioning and}} refrigeration applications. Vapor injection alters the normal refrigeration cycle by adding an injection port to the scroll compressor. After partial expansion upstream of the evaporator, refrigerant vapor is injected into the compressor. It saves compressor energy because that fraction of the flow needs only <b>partial</b> compression, and <b>saves</b> compressor volume because less vapor enters the suction port and the refrigerating effect increases due to the lower inlet quality. Simulation models were run for different operating conditions and it was discovered that for systems sized to meet the same design load, the VI systems had a higher COP of around 8 - 10 %, for air-conditioning applications. Similarly for the refrigeration case, the VI system registered a COP increase of as much 16 %. VI systems also helped in reducing the compressor displacement by around 25 % in the air-conditioning and 30 % in the refrigeration case respectively. Air Conditioning and Refrigeration Project 14...|$|R

