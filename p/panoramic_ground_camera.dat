0|162|Public
50|$|There {{is a lot}} of {{behind the}} scenes work prior to {{organization}} of any big way event. This includes not only the invitation of individual skydivers, but also the coordination of aircraft, pilots, freefall and <b>ground</b> <b>cameras</b> as well as all support and ground staff.|$|R
30|$|The {{proposed}} unwrapping scheme enables three commonly performed unwrapping forms, namely, cylindrical panoramic, cuboid <b>panoramic,</b> and <b>ground</b> plane view to be done. The {{different forms}} of unwrapping {{can be achieved by}} selecting an appropriate projection plane to be populated as the unwrapped image.|$|R
25|$|Jumps are filmed using a <b>ground</b> based <b>camera</b> (with an {{exceptional}} lens {{to record the}} performance).|$|R
40|$|We present control {{methods for}} an {{autonomous}} four-rotor helicopter, called a quadrotor, using visual feedback {{as the primary}} sensor. The vision system uses a <b>ground</b> <b>camera</b> to estimate the pose (position and orientation) of the heli-copter. Two methods of control are studied one using a series of mode-based, feedback linearizing controllers, and the other using a backstepping-like control law. Various simulations of the model demonstrate the implementation of feedback linearization and the backstepping controllers. Finally, we present initial flight experiments where the helicopter is restricted to vertical and yaw motions. ...|$|R
50|$|Conditions {{outside of}} the camera can cause objects in an image to appear {{displaced}} from their true ground position. Such conditions may include <b>ground</b> relief, <b>camera</b> tilt and atmospheric refraction.|$|R
40|$|Abstract — In this paper, a vision-based {{stabilization}} and output tracking control method for a four-rotor helicopter has been proposed. A novel 2 camera method {{has been described}} for estimating the full 6 DOF pose of the helicopter. This two camera system is consisting of a pan-tilt <b>ground</b> <b>camera</b> and an onboard camera. The pose estimation algorithm is compared in simulation to other methods (such as four point method, and a stereo method) and is shown to be less sensitive to feature detection errors on the image plane. The proposed pose estimation algorithm and nonlinear control techniques have been implemented on a remote controlled quadrotor helicopter. I...|$|R
40|$|The {{results of}} {{measurements}} of the contact potential difference of not shielded and shielded samples of cables with twisted pair without shielded chamber, inside a shielded non-grounded and <b>grounded</b> <b>camera</b> are presented. Inspections carrying out in a grounded chamber are more effective {{in the case of}} shielded cables. It is shown that utilization of an electrostatic voltmeter with higher sensitivity in comparison with digital voltmeter leads to registration as individual own internal noises of the cable as external ones. The coefficients of correlation between the {{results of measurements}} of the contact potential difference of the power cable by electrostatic voltmeter and by digital multimeter are determined...|$|R
40|$|We {{describe}} an interactive system that models regions {{of an urban}} environment, such {{as a group of}} tall buildings. Traditional image-based modeling methods often cannot model such large areas due to error accumulation and limited camera field of view. Our approach widens the camera field of view by constructing a 360 degree panorama from ground-level images and uses a high resolution orthorectified aerial image to provide the building footprints. Users draw the building outlines in the aerial image and select a point as the approximate <b>ground</b> <b>camera</b> location. The method automatically extracts roof corners in the ground images and registers the panorama to the aerial image according to geometric constraints. The height of each building is calculated from an estimated camera pose. The resulting textured model of the buildings is constructed of planar surfaces. 1...|$|R
30|$|We now {{proceed to}} present optical {{data from the}} west coast of Hudson Bay and relate it to {{detailed}} images from nearby <b>ground</b> <b>cameras.</b> We then apply the automated meridian modeling (AMM) inversion technique (Connors and Rostoker 2015) to magnetic data from a “Churchill chain” made up of stations of several organizations and examine to what degree a simple uniform electrojet model represents this data. We explain individual magnetograms to insert some details that are missed by a uniform model, supplemented by a rough inversion of data from a single station in a critical location. We conclude that the event is best explained as a strong pseudobreakup or weak initial expansive phase, an expansive phase, and a poleward boundary intensification (PBI), using previous terminology. Finally, we connect the PBI to magnetic field changes detected by the THEMIS spacecraft near the midnight sector.|$|R
30|$|KITTI dataset is {{designed}} for evaluating vision systems in a driving scenario and includes many types of data [87]. In the dataset, visual odometry dataset is provided. <b>Ground</b> truth <b>camera</b> poses are obtained using RTK-GPS. In KITTI dataset webpage 6, evaluation results are listed. The results of LSD-SLAM and ORB-SLAM algorithms {{can be found in}} the Web page.|$|R
25|$|In the UK music video, Gary Lightbody lies on open <b>ground</b> as <b>cameras</b> film {{him from}} {{different}} angles. It starts raining, splashing {{his face and}} hands. Gary enters {{a pool of water}} next to him and, {{at the end of the}} video, he gets out of the water, rises to his feet and looks up at the camera as it zooms out overhead.|$|R
50|$|Hot Spot uses two infra-red cameras {{positioned}} {{at either}} end of the <b>ground.</b> These <b>cameras</b> sense and measure heat from friction generated by a collision, such as ball on pad, ball on bat, ball on ground or ball on glove. Using a subtraction technique a series of black-and-white negative frames is generated into a computer, precisely localising the ball's point of contact.|$|R
50|$|In the UK music video, Gary Lightbody lies on open <b>ground</b> as <b>cameras</b> film {{him from}} {{different}} angles. It starts raining, splashing {{his face and}} hands. Gary enters {{a pool of water}} next to him and, {{at the end of the}} video, he gets out of the water, rises to his feet and looks up at the camera as it zooms out overhead.|$|R
5000|$|Wāng Tāo ( [...] ; born 1980), {{also known}} as Frank Wang, is a Chinese {{businessman}} who is the founder (2006) {{and chief executive officer}} of DJI ("Dà-Jiāng Innovations Science and Technology Co., Ltd", [...] ), a Chinese technology company headquartered in Shenzhen, Guangdong, which manufacture unmanned and autonomous aerial vehicles, flight controllers, Zenmuse aerial gimbals, Ronin <b>ground</b> gimbals, <b>cameras</b> and FPV Goggles.|$|R
40|$|Autonomous {{vehicles}} need a {{means of}} detecting obstructions on its path, to avoid collision. In this paper, a novel approach to obstacle detection is presented. A camera moves on a visible ground plane with the optical axis parallel to the <b>ground.</b> <b>Camera</b> motion parameters are linearly related to first order spatio-temporal derivatives of the taken image sequence; image flow is not needed. Motion is robustly estimated using RANSAC. An error measure for each image point corresponds to the likelihood of an obstacle in that point. 1 Introduction Visual systems must detect {{when there is a}} risk of colliding with obstructions. Many such situations can be conceptually reduced to obstacles protruding above a flat ground; the task of finding these protrusions is referred to as obstacle detection, for which various schemes using a moving monocular camera have been presented in the literature. The obstacle detection is performed by studying image motion, which differ between regions view [...] ...|$|R
40|$|We {{present a}} model of early word {{learning}} which learns from natural audio and visual input. The model has been successfully implemented to learn words and their audio-visual <b>grounding</b> from <b>camera</b> and microphone input. Although simple in its current form, this model {{is a first step}} towards a more complete, fully-grounded model of language acquisition. Practical applications include adaptive human-machine interfaces for information browsing, assistive technologies, education, and entertainment...|$|R
40|$|An {{overview}} of the procedures required for reduction of Hubble Space Telescope Wide Field/Planetary Camera images is presented, based on WFPC performance as observed during <b>ground</b> tests. <b>Camera</b> artifacts that must be removed prior to analysis of WFPC images are characterized. WFPC flat-field and dark-current images are also presented. Special emphasis {{is given to the}} design of WFPC imaging programs that minimize the effects of camera artifacts on image analysis...|$|R
40|$|We {{propose a}} new method to {{estimate}} the 6 -dof trajectory of a flying object such as a quadrotor UAV within a 3 D airspace monitored using multiple fixed <b>ground</b> <b>cameras.</b> It {{is based on a}} new structure from motion formulation for the 3 D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics). Furthermore, we can infer the underlying control input sent to the UAV's autopilot that determined its flight trajectory. Our method requires neither perfect single-view tracking nor appearance matching across views. For robustness, we allow the tracker to generate multiple detections per frame in each video. The true detections and the data association across videos is estimated using robust multi-view triangulation and subsequently refined during our bundle adjustment procedure. Quantitative evaluation on simulated data and experiments on real videos from indoor and outdoor scenes demonstrates the effectiveness of our method...|$|R
40|$|This paper {{presents}} the architecture {{developed in the}} framework of the AWARE project for the autonomous distributed cooperation between unmanned aerial vehicles (UAVs), wireless sensor/actuator networks and <b>ground</b> <b>camera</b> networks. One of the main goals was the demonstration of useful actuation capabilities involving multiple ground and aerial robots in the context of civil applications. A novel characteristic is the demonstration in field experiments of the transportation and deployment of the same load with single/multiple autonomous aerial vehicles. The architecture is endowed with different modules that solve the usual problems that arise during the execution of multi-purpose missions, such as task allocation, conflict resolution, task decomposition and sensor data fusion. The approach had to satisfy two main requirements: robustness for the operation in disaster management scenarios and easy integration of different autonomous vehicles. The former specification led to a distributed design and the latter was tackled by imposing several requirements on the execution capabilities of the vehicles to be integrated in the platform. The full approach was validated in field experiments with different autonomous helicopter...|$|R
50|$|The rover has {{two pairs}} {{of black and white}} {{navigation}} cameras mounted on the mast to support <b>ground</b> navigation. The <b>cameras</b> have a 45° angle of view and use visible light to capture stereoscopic 3-D imagery.|$|R
30|$|D. dos Santos Jr., I.N. da Silva, R. Modugno, H. Pazelli and A. Castellar. Software Development Using an Agile Approach for Satellite <b>Camera</b> <b>Ground</b> Support Equipment. Advances and Innovations in Systems, Computing Sciences and Software Engineering, pages 71 – 76, 2007.|$|R
40|$|Abstract—Finding an {{available}} {{parking spot}} in big and crowded city centers {{has been an}} important problem for people who use their own cars for transportation. Hence, parking spot detection has been an interesting problem that has drawn considerable attention. Although parking spot detection using <b>ground</b> based <b>camera</b> imagery has been studied extensively, doing the same using aerial images is a relatively unexplored problem. In this work we present a machine learning based approach for the latter. I...|$|R
40|$|Investigation {{of urban}} built {{environment}} includes {{a wide range}} of applications that require 3 -D information. New approaches are needed for near-real-time analysis of the urban environment with natural 3 -D visualisation of extensive coverage. Hyperspectral remote sensing technology is a promising and powerful tool to assess quantitative classification of urban materials by exploring possible chemical/physical changes using spectral information across the VIS-NIR-SWIR spectral region. Light Detection And Ranging (LiDAR) technology offers precise information about the geometrical properties of the surfaces and can reflect the different shapes and formations in the complex urban environment. Generating a monitoring system that is based on integrative fusion of hyperspectral and LiDAR data may enlarge the application envelope of each individual technology and contribute valuable information on urban built environments and planning. A fusion process defined by a data-registration algorithm and including spectral/spatial and 3 -D information is developed and presented. The proposed practical 3 -D urban environment application for photogrammetric and urban planning purposes integrates the hyperspectral (spectrometer, <b>ground</b> <b>camera</b> and airborne sensor) and LiDAR data. This application may provide urban planners, civil engineers and decision-makers with tools to consider quantitative spectral information in the 3 -D urban space. ...|$|R
50|$|Pornographers {{have taken}} {{advantage}} of each technological advance in the production and distribution of pornography. They have used lithographs, the printing press, and photography. Pornography is considered a driving force in the development of technologies from the printing press, through photography (still and motion), to satellite TV, other forms of video, and the Internet. With the invention of tiny cameras and wireless equipments voyeur pornography is gaining <b>ground.</b> Mobile <b>cameras</b> are used to capture pornographic photos or videos, and forwarded as MMS, a practice known as sexting.|$|R
5000|$|A {{building}} across from deep right-center field was {{topped by a}} neon sign for Baby Ruth candy beginning in the mid-1930s and running for some 40 years. That placement by the Chicago-based Curtiss Candy Company, coincidentally positioned {{in the line of}} sight of [...] "Babe Ruth's called shot", proved fortuitous when games began to be televised in the 1940s—the sign was also in the line of sight of the <b>ground</b> level <b>camera</b> behind and to the left of home plate. The aging sign was eventually removed in the early 1970s.|$|R
30|$|In this article, a closed-form {{solution}} for spherical omnidirectional image unwrapping incorporating advantages {{from the different}} techniques is presented. Different from previous works, the proposed unwrapping technique (1) does not require any prior knowledge on sensor parameters or ground-truth information, (2) produces output that scales accordingly with {{the resolution of the}} image, (3) utilises closed form forward and backward mapping equations, and (4) is designed for multiple output forms such as cylindrical panoramic view, cuboid <b>panoramic</b> view, and <b>ground</b> plane view unwrapping.|$|R
40|$|JPL is {{currently}} developing the multi-angle spectro-polarimetric imager (MSPI), targeted for the Aerosol-Cloud-Ecosystems (ACE) mission, {{as defined in}} the National Academies 2007 Decadal Survey. In preparation for the space instrument, the MSPI team has built two incremental camera systems (Ground- and Air-MSPI) to improve understanding of the proposed architecture. Ground-MSPI is a gimballed instrument used primarily for stationary observation and characterization of the imager and optics. The ER- 2 based Air-MSPI operates in a step-and-stare mode, providing multi-angle imaging of a static target. This mode-of-operation simulates the observation scenario of the space instrument. Physically, MSPI is a pushbroom camera with a specialized frontend. Before imaging, light entering the camera passes through a pair of photoelastic modulators {{and a set of}} pattern polarizers. These optical elements act on the light to make polarimetric extraction computationally feasible. Calculating polarimetric parameters from the imager's data stream requires a real-time least-squares computation that produces coefficients of a truncated time-series expansion of the image. As reported in, the data processing algorithm can operate in real-time on a Xilinx Virtex- 5 FPGA. Moving beyond verification with an onboard data source, the algorithm has been validated on a commercial development board interfaced with the <b>ground</b> <b>camera.</b> In addition, the algorithm has been instantiated within the Air-MSPI electronics board's FPGA, and in situ first-light has been achieved...|$|R
40|$|Fade-in. From a dark {{corridor}} a shaky, grainy camera captures an {{open door}} to the outside. A pig is led {{out of the barn}} and the camcorder follows. Pan to a man standing nearby, pan back to the pig. A slaughtering gun held by two hands appears in the frame and is pressed to the pig’s forehead. Zoom-in. The gun shoots; the pig falls to the <b>ground,</b> the <b>camera</b> closely following the pig’s convulsing body. Pause. Rewind. Slow motion. Again the gun shoots, the pig falls to the <b>ground,</b> the <b>camera</b> closely following the pig’s convulsing body. Stop. The screen flickers with snow and the title of Michael Haneke’s 1992 feature appears in red: Benny’s Video. The second installment in Haneke’s “Vergletscherungs-Trilogie ” begins with a scene that makes clear, in Haneke’s words, “worum es geht”: 1 “what it’s about ” is placing the spectator in the voyeuristic perspective to view ritually and fetishistically the slaughter of a pig on home video in a teenager’s room with drawn shades and outfitted with every imaginable piece of video, TV, stereo, and surveillance equipment. The spectator soon realizes that the truly disturbing aspect of what a Swiss newspaper called “the most disgusting film of the year” 2 is not what’s on the screen, but how what’s on the screen is perceived and processed...|$|R
50|$|In the 2012 film Abraham Lincoln Vampire Hunter, a Federal unit, {{presumably the}} 154th New York Volunteer Infantry, is {{attacked}} by Confederate vampires {{and only one}} member survives. A photograph falls out of a soldier's hand and falls to the <b>ground</b> near the <b>camera.</b> It {{is the same one}} that Sergeant Humiston carried.|$|R
50|$|Sol 874: Opportunity {{used its}} {{panoramic}} camera {{to survey the}} ground, then took a picture with its navigation camera to determine where to point the miniature thermal emission spectrometer. The miniature thermal emission spectrometer was also used to observe the sky and <b>ground.</b> The <b>panoramic</b> <b>camera</b> took thumbnail images of the sky.|$|R
40|$|Methods for {{estimation}} of grow index of tealeaves based on Bi-directional Reflectance Distribution Function: BRDF measurements with <b>ground</b> based network <b>cameras</b> is proposed. Due to {{a fact that}} Near Infrared: NIR camera data is proportional to total nitrogen while that shows negative correlation to fiber contents, {{it is possible to}} estimate nitrogen and fiber contents in tealeaves with <b>ground</b> based NIR <b>camera</b> data and remote sensing satellite data. Through regressive analysis between measured total nitrogen and fiber contents and NIR reflectance of tealeaves in tea estates, it is found that there is a good correlation between both then regressive equations are created. Also it is found that monitoring of a grow index of tealeaves with BRDF measured with networks cameras is valid. Thus it is concluded that a monitoring of tea estates with network cameras of visible and NIR is appropriate...|$|R
30|$|This article {{proposes a}} novel method of image unwrapping for spherical {{omnidirectional}} images acquired through a non-single viewpoint (NSVP) omnidirectional sensor. It has three key steps i.e. (1) calibrate {{the camera to}} obtain parameters describing the spherical omnidirectional sensor, (2) map world points onto mirror points and, subsequently, onto image points, and (3) set up the projection plane for the final image unwrapping. Based on the projection plane selected, the algorithm is able to produce three common forms of unwrapping, namely cylindrical panoramic, cuboid <b>panoramic,</b> and <b>ground</b> plane view using closed form mapping equations. The motivation for developing this technique is to address the complexity in using a NSVP omnidirectional sensor and ultimately encouraging its application in robotics field. One of the main advantages of a NSVP omnidirectional sensor is that the mirror can often be obtained {{at a lower price}} as compared to the single viewpoint counterpart.|$|R
5000|$|In 1985, VF-111 was one {{of several}} NAS Miramar based squadrons to {{participate}} in the filming of the film Top Gun. Some VF-111 and VF-51 aircraft were repainted in fictitious squadron markings for the film. To be able to film the sequences, the F-14s were fitted with cameras mounted in pods attached to the underbelly Phoenix pallets and the under wing pylons, as well as using <b>ground</b> mounted <b>cameras.</b> Also, one of the fictional RIOs in the film, played by Clarence Gilyard, uses the callsign [...] "Sundown" [...] and wears a VF-111 styled helmet and squadron patch on his flight suit.|$|R
40|$|We {{describe}} {{methods for}} using colour and texture to discriminate cloud and sky in images captured using a <b>ground</b> based colour <b>camera.</b> Neither method alone has proved sufficient {{to distinguish between}} different types of cloud, and between cloud and sky in general. Classification can be improved by combining the features using a Bayesian scheme. ...|$|R
50|$|On 31 August 2016, ESA {{announced}} they {{had discovered that}} a solar panel on the Copernicus Sentinel-1A satellite {{had been hit by}} a millimetre-size particle in orbit on 23 August. Thanks to onboard <b>cameras,</b> <b>ground</b> controllers were able to identify the affected area. The satellite’s routine operations didn't seem to be altered by the impact.|$|R
50|$|The Mars Curiosity rover has {{two pairs}} {{of black and white}} {{navigation}} cameras mounted on the mast to support <b>ground</b> navigation. The <b>cameras</b> have a 45 degree angle of view and use visible light to capture stereoscopic 3-D imagery. These cameras, like those on the Mars Pathfinder missions support use of the ICER image compression format.|$|R
