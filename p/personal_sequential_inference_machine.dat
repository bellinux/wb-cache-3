0|411|Public
40|$|R. Freivalds and C. H. Smith [FS 92] {{proved that}} {{probabilistic}} limited memory inductive <b>inference</b> <b>machines</b> can learn some classes of total recursive functions with a probability I which cannot be learned by deterministic limited memory inductive <b>inference</b> <b>machines.</b> We introduce quantum limited memory inductive <b>inference</b> <b>machines</b> as quantum finite automata used as inductive <b>inference</b> <b>machines.</b> Our main result below shows that quantum limited memory inductive <b>inference</b> <b>machines</b> can learn classes of total recursive functions not learnable by any deterministic and even by probabilistic limited memory inductive <b>inference</b> <b>machines...</b>|$|R
40|$|AbstractFreivalds and Smith [R. Freivalds, C. H. Smith Memory limited {{inductive}} <b>inference</b> <b>machines,</b> Springer Lecture Notes in Computer Science 621 (1992) 19 – 29] {{proved that}} probabilistic limited memory inductive <b>inference</b> <b>machines</b> can learn with probability  1 certain classes of total recursive functions, which cannot be learned by deterministic limited memory inductive <b>inference</b> <b>machines.</b> We introduce quantum limited memory inductive <b>inference</b> <b>machines</b> as quantum finite automata acting as inductive <b>inference</b> <b>machines.</b> These machines, we show, can learn classes of total recursive functions not learnable by any deterministic, nor even by probabilistic, limited memory inductive <b>inference</b> <b>machines...</b>|$|R
40|$|AbstractThe {{notion of}} an {{inductive}} <b>inference</b> <b>machine</b> aggregating a team of <b>inference</b> <b>machines</b> models the problem of making use of several explanations for a single phenomenon. This article investigates {{the amount of information}} necessary for a successful aggregation of the theories given by a team of <b>inference</b> <b>machines.</b> Variations of using different kinds of identification and aggregation are investigated...|$|R
40|$|Normative {{models of}} human {{cognition}} often appeal to Bayesian filtering, which provides optimal online estimates of unknown or hidden {{states of the}} world, based on previous observations. However, in many cases {{it is necessary to}} optimise beliefs about sequences of states rather than just the current state. Importantly, Bayesian filtering and <b>sequential</b> <b>inference</b> strategies make different predictions about beliefs and subsequent choices, rendering them behaviourally dissociable. Taking data from a probabilistic reversal task we show that subjects' choices provide strong evidence that they are representing short sequences of states. Between-subject measures of this implicit <b>sequential</b> <b>inference</b> strategy had a neurobiological underpinning and correlated with grey matter density in prefrontal and parietal cortex, as well as the hippocampus. Our findings provide, to our knowledge, the first evidence for <b>sequential</b> <b>inference</b> in human cognition, and by exploiting between-subject variation in this measure we provide pointers to its neuronal substrates...|$|R
40|$|An {{important}} feature of Bayesian statistics is the possibility to do sequential inference: the posterior distribution obtained after seeing a first dataset {{can be used as}} prior for a second inference. However, when Monte Carlo sampling methods are used for the inference, we only have one set of samples from the posterior distribution, which is typically insufficient for accurate <b>sequential</b> <b>inference.</b> In order to do <b>sequential</b> <b>inference</b> in this case, it is necessary to estimate a functional description of the posterior probability from the Monte Carlo samples. Here, we explore whether it is feasible to perform <b>sequential</b> <b>inference</b> based on Monte Carlo samples, in a multivariate context. To approximate the posterior distribution, we can use either the apparent density based on the sample positions (density estimation) or the relative posterior probability of the samples (regression). Specifically, we evaluate the accuracy of kernel density estimation, Gaussian mixtures, vine copulas and Gaussian process regression; and we test whether they can be used for <b>sequential</b> Bayesian <b>inference.</b> Additionally, both the density estimation and the regression methods can be used to obtain a post-hoc estimate of the marginal likelihood. In low dimensionality, Gaussian processes are most accurate, whereas in higher dimensionality Gaussian mixtures or vine copulas perform better. We show that <b>sequential</b> <b>inference</b> can be computationally more efficient than joint inference, and we also illustrate the limits of this approach with a failure case. Since the performance is likely to be case-specific, we provide an R package mvdens that provides a unified interface for the density approximation methods...|$|R
30|$|We {{consider}} time-varying phenomena {{with diverse}} characteristics, provide a flexible and compact mathematical {{representation of the}} data, and propose generic SMC methods for <b>sequential</b> <b>inference</b> of latent time-series with different memory properties.|$|R
40|$|Bayesian {{inference}} as iterated random functions with {{applications to}} <b>sequential</b> <b>inference</b> in graphical models Anonymous Author(s) Affiliation Address email We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates {{can be viewed}} as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The <b>sequential</b> <b>inference</b> algorithm and its supporting theory are validated by simulated examples. ...|$|R
50|$|In the mid-1980s, Integrated <b>Inference</b> <b>Machines</b> (IIM) built prototypes of Lisp {{machines}} named Inferstar.|$|R
40|$|Abstract—Based on the {{research}} of the intelligent cataract ultrasonic emulsification instrument, mainly discusses the principle of design and inference algorithm of the <b>inference</b> <b>machine</b> of intelligent ultrasonic emulsification instrument. Designing the cataract expert knowledge base {{by the use of}} relational database, the <b>inference</b> <b>machine</b> can simulate the real doctor thinking features effectively; at the same time, according to the patient's cataract symptoms, it can make intelligent judgment to select the ultrasonic energy level for the emulsifying needle, and effectively control the release of energy. The inference strategy is the combined of deductive reasoning, certainty and forward inference, so as to improve the efficiency of <b>inference</b> <b>machine.</b> Through the verification tests, the system obtains better inference results...|$|R
40|$|Ph. D. ThesisInductive {{inference}} is {{a process}} of hypothesizing a general rule from examples. As a successful inference criterion for inductive inference of formal languages and models of logic programming, we have mainly used Gold 2 ̆ 7 s identification in the limit. An <b>inference</b> <b>machine</b> M is said to infer a concept L in the limit, if the sequence of guesses from M which is successively fed a sequence of examples of L converges to a correct expression of L, that is, all guesses from M become a unique expression in a finite time and that the expression is a correct one. A class, or a hypothesis space, is said to be inferable in the limit, if there is an <b>inference</b> <b>machine</b> M which infers every concept in the class. In the present thesis, we mainly investigate three criteria related to the identification in the limit. As we will see, they are necessary for practical applications of machine learning or machine discovery. The first criterion requires an <b>inference</b> <b>machine</b> to produce a unique guess. That is, we apply so-called finite identification to concept learning. As stated above, ordinary inductive inference is an infinite process. Thus we can not decide in general whether a sequence of guesses from an <b>inference</b> <b>machine</b> has converged or not at a certain time. To the contrary, in the criterion of finite identification, if an <b>inference</b> <b>machine</b> produces a guess, then it is a conclusive answer. The second criterion requires an <b>inference</b> <b>machine</b> to refute a hypothesis space in question, if a target concept is not in the hypothesis space. In the ordinary inductive inference, the behavior of an <b>inference</b> <b>machine</b> is not specified, when we feed examples of a target concept not belonging to the hypothesis space. That is, we implicitly assume that every target concept belongs to the hypothesis space. As far as data or facts are presented according to a concept that is unknown but guaranteed to be in the hypothesis space, the machine will eventually identify the hypothesis. However this assumption is not appropriate, if we want an <b>inference</b> <b>machine</b> to infer or to discover an unknown rule which explains examples or data obtained from scientific experiments. Thus we propose a successful inference criterion where, if there is no concept in the hypothesis space which coincides with a target concept, then an <b>inference</b> <b>machine</b> explicitly tells us this and stops in a finite time. The third criterion requires an <b>inference</b> <b>machine</b> to infer a minimal concept within the hypothesis space concerned. In actual applications of inductive inference, there are many cases where we want an <b>inference</b> <b>machine</b> to infer an approximate concept within the hypothesis space, even when there is no concept which exactly coincides with the target concept. Here we take a minimal concept as an approximate concept within the hypothesis space, and discuss inferability of a minimal concept of the target concept which may not belong to the hypothesis space. That is, we force an <b>inference</b> <b>machine</b> to converge to an expression of a minimal concept of the target concept, if there is a minimal concept of the target concept within the hypothesis space. In the present thesis, we discuss inferability of recursive concepts under the above three criteria, and show some necessary and sufficient conditions for inferability and some comparisons between inferable classes. Furthermore as practical and concrete hypothesis spaces, we take the classes definable by so-called lengt h-bounded elementary formal systems (EFS 2 ̆ 7 s, for short) and discuss their inferability in the above three criteria. In 1990, Shinohara showed that the classes definable by length-bounded EFS 2 ̆ 7 s with at most n axioms are inferable in the limit from positive data for any n 2 : 1. In the present thesis, we show that the above classes are also refutably inferable from complete data, i. e. positive and negative data, as well as minimally inferable from positive data. This means that there are rich hypothesis spaces that are refutably inferable from complete data or minimally inferable from positive data...|$|R
40|$|Abstract. In {{ordinary}} learning paradigm, {{a target}} concept, whose examples are fed to an <b>inference</b> <b>machine,</b> {{is assumed to}} belong to a hypothesis space which is given in advance. However this assumption is not appropriate, if we want an <b>inference</b> <b>machine</b> to infer or to discover an unknown rule which explains examples or data obtained from scientific experiments. In their previous paper, Mukouchi and Arikawa discussed both refut ability and inferability of a hypothesis space from examples. In this paper, we take a minimal concept as an approximate concept within a hypothesis space, and discuss inferability of a minimal concept of the target concept which may not belong to the hypothesis space. That is, we force an <b>inference</b> <b>machine</b> to converge to a minimal concept of the target concept, if there are minimal concepts of the target concept within the hypothesis space. We also show that there are some rich hypothesis spaces that are minimally inferable from positive data. 1...|$|R
40|$|A simulation-based filter {{algorithm}} for <b>sequential</b> <b>inference</b> in the Markov switching stochastic volatility (MSSV) {{model is}} proposed. Our algorithm {{is based on}} both Pitt and Shephard's (1999) auxiliary particle filter (APF) and Liu and West's (2001) sequential density estimation ideas. The algorithm is tested against a synthetic time series that mimics real-world situations...|$|R
40|$|This paper {{develops}} particle filtering {{and learning}} for <b>sequential</b> <b>inference</b> in em-pirical asset pricing models. We provide a computationally feasible toolset for se-quential parameter learning, hypothesis testing, and model monitoring, incorporat-ing multiple observable variables, unobserved stochastic volatility, and unobserved “drifting ” regression coefficients. <b>Sequential</b> <b>inference</b> {{allows us to}} observe how the views of economic decision makers evolve in real time. Empirically, we analyze time series predictability of equity returns, using both the traditional dividend yield and net payout yield, which incorporates issuances and repurchases. We find that the data rejects the traditional model for both predictors, in favor of models with drift-ing coefficients or stochastic volatility. We study the optimal portfolio allocation problem under parameter, state variable, and model uncertainty, and show that the Bayesian portfolios are more stable and have better out-of-sample performance than rolling regressions...|$|R
40|$|Abstract. The paper {{introduces}} {{a method of}} transition from TIL into Prolog system and vice versa, in order to utilize Prolog <b>inference</b> <b>machine</b> in the deductive system of TIL. We specify {{a subset of the}} set of TIL constructions the elements of which can be encoded in Prolog language, and introduce the method of translation from TIL into Prolog. Since Prolog is less expressive than TIL, we have to build up a TIL functional overlay that makes it possible to realize the reverse transition from Prolog into TIL in a near to equivalent way. Key words: TIL, T IL-Script language, <b>inference</b> <b>machine,</b> Prolog...|$|R
40|$|In this paper, we {{describe}} a video tracking application using the dual-tree polar matching algorithm. The models are specified in a probabilistic setting, and a particle filter {{is used to}} perform the <b>sequential</b> <b>inference.</b> Computer simulations demonstrate {{the ability of the}} algorithm to track a simulated video moving target in an urban environment with complete and partial occlusions. 1...|$|R
40|$|Inductive <b>inference</b> <b>machines</b> are {{algorithmic}} devices which {{attempt to}} synthesize (in the limit) programs for a function while they examine {{more and more}} of the graph of the function. There are many possible criteria of success. We study the inference of nearly minimal size programs. Our principal results imply that nearly minimal size programs can be inferred (in the limit) without loss of inferring power provided we are willing to tolerate a finite, but not uniformly, bounded, number of anomalies in the synthesized programs. On the other hand, there is a severe reduction of inferring power in inferring nearly minimal size programs if the maximum number of anomalies allowed is any uniform constant. We obtain a general characterization for the classes of recursive functions which can be synthesized by inferring nearly minimal size programs with anomalies. We also obtain similar results for Popperian inductive <b>inference</b> <b>machines.</b> The exact tradeoffs between mind change bounds on inductive <b>inference</b> <b>machines</b> and anomalies in synthesized programs are obtained. The techniques of recursive function theory including the recursion theorem are employed...|$|R
40|$|Ground {{investigations}} {{often use}} trial pits and borehole cores on construction sites {{to determine the}} strata likely to be encountered at various depths. The data obtained from trial pits can be coded into a form {{that can be used}} as sample observations for input to a grammatical <b>inference</b> <b>machine.</b> A grammatical <b>inference</b> <b>machine</b> is a black box, which when presented with a sample of observations of some unknown source language, produces a grammar which is compatible with the sample. This article presents a heuristic model for a grammatical <b>inference</b> <b>machine,</b> which takes as data sentences and non-sentences identified as such, and is capable of inferring grammars in the class of context-free grammars expressed in Chomsky Normal Form. An algorithm and its corresponding software implementation have been developed based on this model. The software takes, as input, coded representations of ground investigation data, and produces as output a grammar which describes and classifies the geotechnical data observed in the area, and also promises the possibility of being able to predict the likely configuration of strata across the site...|$|R
40|$|All {{processes}} in a software implemented <b>inference</b> <b>machine</b> are called fuzzy processes. These processes are fuzzification, fuzzy inference and defuzzification, executed in that order. The implementation itself {{is called a}} fuzzy <b>inference</b> <b>machine</b> or a fuzzy logic system. This thesis describes such a system in a parallel implementation with a specific approach, where complex operations are broken down into multiple simpler ones. We used the CUDA architecture, which allows us the usage of general purpose parallel computing on modern GPU’s. At the end we tested this implementation, {{in comparison with the}} sequential implementation on the CPU, by comparing the precision of the computational results and the needed times of operation algorithms...|$|R
30|$|We {{consider}} the general problem of <b>sequential</b> <b>inference</b> of latent time-series, observed via nonlinear functions. We describe the considered mathematical framework in Section 2, where we adopt a state-space {{representation of the}} processes. The latent time-series is modeled as an ARMA(p,q) process driven by innovations correlated in time. We make minimal assumptions about the observation equation so that any computable nonlinear function can be accommodated.|$|R
30|$|Step 4 - 2 Otherwise, {{the score}} shall be calculated. In this paper, {{the design of}} the QoE score is {{accomplished}} by employing intelligent <b>inference</b> <b>machine</b> method to avoid the constraint by linear method, and directly reasoning {{on the basis of the}} parameter indexes.|$|R
40|$|AbstractDegrees of inferability {{have been}} {{introduced}} to measure the learning power of inductive <b>inference</b> <b>machines</b> which have access to an oracle. The classical concept of degrees of unsolvability measures the computing power of oracles. In this paper we determine the relationship between both notions...|$|R
50|$|Particle {{filters and}} Feynman-Kac {{particle}} methodologies find application in signal and image processing, Bayesian <b>inference,</b> <b>machine</b> learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.|$|R
40|$|In {{this paper}} we {{describe}} fuzzy rules {{used in the}} developed prototype of a “fuzzy music interpretation system ” [4]. The core of this system consists of two essential units, the rule base and the <b>inference</b> <b>machine.</b> The rule base contains general IF–THEN interpretation rules, formulated by an experienced pianist. The <b>inference</b> <b>machine</b> contains both conventional and advanced fuzzy information processing strategies. Once the system is fed with the information—the notes and special signs such as “ppp ” and “legato”, coded {{in accordance with the}} MIDI format—contained in the score of Beethoven’s “Für Elise”, it generates an interpretation of this piece of music and renders it {{in the form of a}} MIDI file. Certain refinement parameters allow us to modify the character of the interpretation. 1...|$|R
40|$|International audienceIn this paper, {{we address}} the problem of {{detection}} and tracking of multiple contaminant clouds. We develop a stochastic extension of the Gaussian puff model to characterize evolution of the average atmospheric pollutant concentration. To perform the <b>sequential</b> <b>inference</b> on this difficult problem, we propose a Markov Chain Monte Carlo (MCMC) -based particle algorithm. Numerical simulations illustrate the ability of the algorithm to detect and track multiple contaminant clouds...|$|R
40|$|We examine uniform {{procedures}} {{for improving the}} scientific competence of inductive <b>inference</b> <b>machines.</b> Formally, such procedures are construed as recursive operators. Several senses of improvement are considered, including (a) enlarging the class of functions on which success is certain, and (b) transforming probable success into certain success...|$|R
40|$|We {{demonstrate}} <b>sequential</b> mass <b>inference</b> of {{a suspended}} bag of milk powder from simulated {{measurements of the}} vertical force component at the pivot while the bag is being filled. We compare the predictions of various <b>sequential</b> <b>inference</b> methods both with and without a physics model to capture the system dynamics. We find that non-augmented and augmented-state unscented Kalman filters (UKFs) {{in conjunction with a}} physics model of a pendulum of varying mass and length provide rapid and accurate predictions of the milk powder mass as a function of time. The UKFs outperform the other method tested - a particle filter. Moreover, inference methods which incorporate a physics model outperform equivalent algorithms which do not. Comment: 5 pages, 7 figures. Copyright IEEE (2015...|$|R
40|$|We outline an {{abstract}} <b>inference</b> <b>machine</b> for producing discourse models in natural language understanding. This machine has tableaux as its central data structure and can operate in model generation and theorem proving modes. Search spaces {{are controlled by}} keeping track of NP saliences and equipping proof rules with costs...|$|R
50|$|Five running Parallel <b>Inference</b> <b>Machines</b> (PIM) were {{eventually}} produced: PIM/m, PIM/p, PIM/i, PIM/k, PIM/c. The project also produced applications {{to run on}} these systems, such as the parallel database management system Kappa, the legal reasoning system HELIC-II, and the automated theorem prover MGTP, as well as applications to bioinformatics.|$|R
40|$|The {{aggregation}} {{problem is}} to design an inferential agent that makes intelligent use of the theories offered {{by a team of}} inductive <b>inference</b> <b>machines</b> working in a common environment. The present paper formulates several versions of the aggregation problem and investigates them from a recursion theoretic point of view...|$|R
40|$|A {{method for}} <b>sequential</b> <b>inference</b> of the fixed {{parameters}} of a dynamic latent Gaussian models is proposed and evaluated {{that is based}} on the iterated Laplace approximation. The method provides a useful trade-off between computational performance and the accuracy of the approximation to the true posterior distribution. Approximation corrections are shown to improve the accuracy of the approximation in simulation studies. A population-based approach is also shown to provide a more robust inference method...|$|R
40|$|In this paper, {{we address}} the problem of {{detection}} and tracking of group and individual targets. In particular, we focus on a group model with a virtual leader which models the bulk or group parameter. To perform the <b>sequential</b> <b>inference,</b> we propose a Markov Chain Monte Carlo (MCMC) -based Particle algorithm with a marginalisation scheme using pairwise Kalman filters. Numerical simulations illustrate the ability of the algorithm to detect and track target...|$|R
40|$|This paper {{presents}} a simulation-based framework for <b>sequential</b> <b>inference</b> from partially and dis-cretely observed point process (PP’s) models. We build upon sequential Monte Carlo (SMC) methods for such cases, investigating {{the problems of}} performing sequential filtering and smoothing in complex examples, where current methods often fail. We consider various approaches for approximating pos-terior distributions using SMC. Our approaches, with some theoretical discussion are illustrated on a running example of intensity estimation for financial models...|$|R
40|$|Enhanced Vision Systems (EVS) are {{currently}} developed {{with the goal}} to alleviate restrictions in airspace and airport capacity in low-visibility conditions. EVS relies on weather penetrating forward-looking sensors that augment the naturally existing visual cues in the environment and provide a real-time image of prominent topographical objects that may be identified by the pilot. In this paper an automatic analysis of millimetre wave radar images for Enhanced Vision Systems is presented. The core {{part of the system}} is a fuzzy rule based <b>inference</b> <b>machine</b> which controls the data analysis based on the uncertainty in the actual knowledge in combination with a-priori knowledge. Compared with standard TV or IR images the quality of MMW images is rather poor and data is highly corrupted with noise and clutter. Therefore, one main task of the <b>inference</b> <b>machine</b> is to handle uncertainties as well as ambiguities and inconsistencies to draw the right conclusions. The output of different sensor data analysis processes are fused and evaluated within a fuzzy/possibilistic clustering algorithm whose results serve as input to the <b>inference</b> <b>machine.</b> The only a-priori knowledge used in the presented approach is the same pilots already know from airport charts which are available of almost every airport. The performance of the approach is demonstrated with real data acquired during extensive flight tests to several airports in Northern Germany...|$|R
50|$|Knowledge {{retrieval}} (KR) {{seeks to}} return {{information in a}} structured form, consistent with human cognitive processes as opposed to simple lists of data items. It draws {{on a range of}} fields including epistemology (theory of knowledge), cognitive psychology, cognitive neuroscience, logic and <b>inference,</b> <b>machine</b> learning and knowledge discovery, linguistics, and information technology.|$|R
40|$|Abstract. Presents a {{fuzzy logic}} {{inference}} algorithm {{based on the}} feature of concentration difference {{to solve the problem}} of odor source spatial positioning to mobile robot. The algorithm simulates the dynamic stimulation’s behavior of merit, let the robot to make decisions on the direction of travel by sensing the rate of change of concentration in different positions, and finally achieve the function of autonomous positioning to odor source. Let the robot's actual environment as the premise, design a fuzzy <b>inference</b> <b>machine,</b> and determine the fuzzy <b>inference</b> <b>machine’s</b> input variables, output variables, language value of input and output and corresponding fuzzy inference rules. Simulation results show that the proposed fuzzy logic algorithm has a strong applicability on bionic odor source spatial positioning to mobile robot; it can make the robot search to the odor source faster and more accurate...|$|R
40|$|Statistical Inference for Fractional Diffusion Processes {{looks at}} {{statistical}} inference for stochastic processes modeled by stochastic differential equations driven by fractional Brownian motion. Other related processes, such as <b>sequential</b> <b>inference,</b> nonparametric and non parametric inference and parametric estimation are also discussed. The book {{will deal with}} Fractional Diffusion Processes (FDP) in relation to statistical influence for stochastic processes. The books main focus is on parametric and non parametric inference problems for fractional diffusion processes when a complete path of...|$|R
