2620|10000|Public
5|$|To assess {{prospective}} {{wind power}} sites a <b>probability</b> <b>distribution</b> <b>function</b> is often {{fit to the}} observed wind speed data. Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution {{can be used as}} a less accurate, but simpler model.|$|E
25|$|Not every {{function}} is the characteristic {{function of a}} legitimate probability distribution (that is, one whose cumulative distribution {{function is}} real and goes from 0 to 1 without decreasing), but the characteristic functions given above will be legitimate {{so long as the}} parameters are in their ranges. The value of the characteristic function at some value t is the complex conjugate of its value at −t as it should be so that the <b>probability</b> <b>distribution</b> <b>function</b> will be real.|$|E
25|$|A random {{variable}} has a probability distribution, which specifies {{the probability that}} its value falls in any given interval. Random variables can be discrete, that is, taking any of a specified finite or countable list of values, endowed with a probability mass function characteristic of the {{random variable}}'s probability distribution; or continuous, taking any numerical value in an interval or collection of intervals, via a probability density function that is characteristic of the random variable's probability distribution; or a mixture of both types. Two random variables with the same probability distribution can still differ {{in terms of their}} associations with, or independence from, other random variables. The realizations of a random variable, that is, the results of randomly choosing values according to the variable's <b>probability</b> <b>distribution</b> <b>function,</b> are called random variates.|$|E
5000|$|... #Subtitle level 2: Family of <b>probability</b> <b>distribution</b> <b>functions</b> ...|$|R
40|$|The {{behavior}} of the first layer of a weightless artificial neural network is analyzed in this paper. The {{way in which the}} neural network receives external information changes accordingly to different <b>probability</b> <b>distribution</b> <b>functions</b> that control data sampling from many different patterns. This paper describes the architecture of this system and shows the effect of the different <b>probability</b> <b>distribution</b> <b>functions</b> over 3 _dimensional pattern recognition. 1...|$|R
3000|$|... and P(c(k)) are the <b>probability</b> <b>distribution</b> <b>functions</b> (PDF) of the {{features}} of interest and the labels, respectively; and [...]...|$|R
25|$|The {{microscopic}} {{description in}} statistical mechanics {{is based on}} a model that analyzes a system into its fundamental particles of matter or into a set of classical or quantum-mechanical oscillators and considers the system as a statistical ensemble of microstates. As a collection of classical material particles, temperature {{is a measure of the}} mean energy of motion, called kinetic energy, of the particles, whether in solids, liquids, gases, or plasmas. The kinetic energy, a concept of classical mechanics, is half the mass of a particle times its speed squared. In this mechanical interpretation of thermal motion, the kinetic energies of material particles may reside in the velocity of the particles of their translational or vibrational motion or in the inertia of their rotational modes. In monatomic perfect gases and, approximately, in most gases, temperature is a measure of the mean particle kinetic energy. It also determines the <b>probability</b> <b>distribution</b> <b>function</b> of the energy. In condensed matter, and particularly in solids, this purely mechanical description is often less useful and the oscillator model provides a better description to account for quantum mechanical phenomena. Temperature determines the statistical occupation of the microstates of the ensemble. The microscopic definition of temperature is only meaningful in the thermodynamic limit, meaning for large ensembles of states or particles, to fulfill the requirements of the statistical model.|$|E
2500|$|The {{kinetic theory}} assumes that {{pressure}} {{is caused by}} the force associated with individual atoms striking the walls, and that all energy is translational kinetic energy. [...] Using a sophisticated symmetry argument, Boltzmann deduced what is now called the Maxwell–Boltzmann <b>probability</b> <b>distribution</b> <b>function</b> for the velocity of particles in an ideal gas. [...] From that <b>probability</b> <b>distribution</b> <b>function,</b> the average kinetic energy, Ek (per particle), of a monatomic ideal gas is: ...|$|E
2500|$|Let [...] and [...] be {{respectively}} the cumulative <b>probability</b> <b>distribution</b> <b>function</b> and the probability density {{function of the}} N(0,1) distribution.|$|E
3000|$|... be {{the set of}} all nonnegative real numbers, Δ {{denote the}} set of all <b>probability</b> <b>distribution</b> <b>functions,</b> i.e., Δ = { [...]...|$|R
30|$|In {{the present}} study, a {{nonlinear}} unconstrained optimization model {{is presented to}} transmute UHs into <b>probability</b> <b>distribution</b> <b>functions.</b> Six <b>probability</b> <b>distribution</b> <b>functions</b> are considered: two-parameter gamma, two-parameter Gumbel, two-parameter log-normal, two-parameter normal, three-parameter Pearson, and two-parameter Weibull distribution. The nonlinear least squares optimization formulation is solved by (1) programming in Mathematica and (2) by applying genetic algorithm. The potential of these six <b>probability</b> <b>distribution</b> <b>functions</b> is tested on data from the Lighvan catchment in the northwest of Iran. The nonlinear optimization method is compared with the traditional linear least squares method. One particular novelty {{of this study is}} the use of Mathematica for solving the nonlinear optimization formulation problem involved in deriving UH. Since Mathematica has extensive symbolic and numerical capabilities, it enables the calculations in a simpler, faster, and more accurate manner. It also has several statistical distributions already built-in.|$|R
30|$|In {{the case}} of a {{continuous}} distribution, it gives the area under the <b>probability</b> <b>distribution</b> <b>functions,</b> also used to specify the distribution of multivariable random variables.|$|R
2500|$|The {{central limit theorem}} applies in {{particular}} to sums of independent and identically distributed discrete random variables. [...] A sum of discrete random variables is still a discrete random variable, so that we are confronted with a sequence of discrete random variables whose cumulative <b>probability</b> <b>distribution</b> <b>function</b> converges towards a cumulative <b>probability</b> <b>distribution</b> <b>function</b> corresponding to a continuous variable (namely that of the normal distribution). [...] This means that if we build a histogram of the realisations of the sum of [...] independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as [...] approaches infinity, this relation is known as de Moivre–Laplace theorem. The binomial distribution article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.|$|E
2500|$|Here X {{represents}} {{the space of}} messages transmitted, and Y the space of messages received during a unit time over our channel. Let [...] be the conditional <b>probability</b> <b>distribution</b> <b>function</b> of Y given X. We will consider [...] to be an inherent fixed property of our communications channel (representing {{the nature of the}} noise of our channel). Then the joint distribution of X and Y is completely determined by our channel and by our choice of , the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the signal, we can communicate over the channel. The appropriate measure for this is the mutual information, and this maximum mutual information is called the channel capacity and is given by: ...|$|E
2500|$|Building off {{previous}} work done on an Aloha model, the coverage probability {{for the typical}} user was derived for a Poisson network. The Poisson model of a cellular network proves to be more tractable than a hexagonal model. Meanwhile, this observation could be argued {{by the fact that}} a detailed and precise derivation for the channel attenuation <b>probability</b> <b>distribution</b> <b>function</b> between a random node and a reference base-station for a hexagonal model was explicitly derived in; and this result could be used to tractably derive the outage probability. Furthermore, in the presence of sufficiently large log-normal shadow fading (or shadowing) and a singular power-law attenuation function, it was [...] observed [...] by simulation [...] for hexagonal networks and then later mathematically proved [...] for general stationary (including hexagonal) networks that quantities like the SINR and SIR of the typical user behave stochastically as though the underlying network were Poisson. In other words, given a [...] power-law attention function, using a Poisson cellular network model with [...] constant shadowing is equivalent (in terms of SIR, SINR, etc.) to assuming large log-normal shadowing in the mathematical model with the base stations positioned according to either a deterministic or random configuration with a constant density.|$|E
30|$|For other hazard-affected elements, a {{vulnerability}} curve {{based on}} previous research results {{can be used to}} construct new prior and observational <b>probability</b> <b>distribution</b> <b>functions</b> for calibration.|$|R
5000|$|For example if both <b>probability</b> <b>distribution</b> <b>functions</b> {{of random}} {{variables}} X and Y are normal distributions (N) {{having the same}} standard deviation , integrating [...] yields to: ...|$|R
5000|$|... where p(x,y) is {{the joint}} {{probability}} function of X and Y, and [...] and [...] are the marginal <b>probability</b> <b>distribution</b> <b>functions</b> of X and Y respectively.|$|R
50|$|The main {{outcomes}} are new stochastic non-Markov process - fractional Poisson process and new <b>probability</b> <b>distribution</b> <b>function</b> - fractional Poisson <b>probability</b> <b>distribution</b> <b>function.</b>|$|E
50|$|The {{fractional}} Poisson process, Fractional compound Poisson {{process and}} fractional Poisson <b>probability</b> <b>distribution</b> <b>function</b> have been invented, developed and encouraged for applications by Nick Laskin (2003) {{who coined the}} terms fractional Poisson process, Fractional compound Poisson process and fractional Poisson <b>probability</b> <b>distribution</b> <b>function.</b>|$|E
5000|$|... #Subtitle level 2: Fractional Poisson <b>probability</b> <b>distribution</b> <b>function</b> ...|$|E
40|$|For many driven-nonequilibrium systems, the <b>probability</b> <b>distribution</b> <b>functions</b> of {{magnitude}} and recurrence-time of large events follow a powerlaw indicating a strong temporal correlation. In this paper we argue why these <b>probability</b> <b>distribution</b> <b>functions</b> are ubiquitous in driven nonequilibrium systems, and we derive universal scaling laws connecting the magnitudes, recurrence-time, and spatial intervals of large events. The {{relationships between the}} scaling exponents have also been studied. We show that the ion-channel current in Voltage-dependent Anion Channels obeys the universal scaling law. Comment: 9 pages, 5 figure...|$|R
40|$|Weightless Artificial Neural Networks {{are based}} on a {{technique}} that considers Hamming distance for pattern learning and recognition, instead of managing weights associated to each link between the nodes of the network. The way in which the neural network receives external information changes accordingly to different <b>probability</b> <b>distribution</b> <b>functions</b> that control data sampling from many different patterns. This paper describes the architecture of this system, its computational complexity, memory space requirements, and it also shows the effect of the different <b>probability</b> <b>distribution</b> <b>functions</b> over 3 -dimensional pattern recognition. I...|$|R
30|$|In this study, six popular <b>probability</b> <b>distribution</b> <b>functions</b> are considered: gamma, Gumbel, log-normal, normal, Pearson, and Weibull. A brief {{description}} of these functions {{can be found in}} Table  9.|$|R
5000|$|... #Subtitle level 2: Presumed <b>probability</b> <b>distribution</b> <b>function</b> model ...|$|E
5000|$|... and [...] is the {{fractional}} Poisson <b>probability</b> <b>distribution</b> <b>function.</b>|$|E
5000|$|... #Subtitle level 4: Identities of the {{particle}} <b>probability</b> <b>distribution</b> <b>function</b> ...|$|E
5000|$|These models {{quantify}} the {{uncertainty in the}} [...] "true" [...] value of the parameter of interest by <b>probability</b> <b>distribution</b> <b>functions.</b> They have been traditionally classified as stochastic programming and stochastic optimization models.|$|R
40|$|We {{propose a}} new {{approach}} based on the path integral formalism to the calculation of the <b>probability</b> <b>distribution</b> <b>functions</b> of quadratic quantities of the Gaussian polymer chain in d-dimensional space, such as the radius of gyration and potential energy in the parabolic well. In both cases we obtain the exact relations for the characteristic function and cumulants. Using the standard steepest-descent method, we evaluate the <b>probability</b> <b>distribution</b> <b>functions</b> in two limiting cases of the large and small values of corresponding variables. Comment: This manuscript has been accepted for publication in Journal of Statistical Mechanics: Theory and experimen...|$|R
40|$|I have {{proposed}} anon-Abelianand stochastic self-organized criticality {{model in which}} each avalanche contains one stochastic site and all remaining sites in the avalanche are deterministic with a constant threshold E_c^I. Studies of avalanche structures, waves and autocorrelations, size moments and <b>probability</b> <b>distribution</b> <b>functions</b> of avalanche size, for the thresholds 4 ≤ E_c^I≤ 256, were performed. The shell-like avalanche structures, correlated waves within avalanches, complex size moments and <b>probability</b> <b>distribution</b> <b>functions</b> show multifractal scaling like the Abelian and deterministic BTW model {{despite the fact that}} the model is non-Abelianand stochastic with unbalanced relaxation rules at each stochastic site...|$|R
50|$|N(d) is {{cumulative}} <b>probability</b> <b>distribution</b> <b>function</b> for {{a standard}} normal distribution.|$|E
50|$|Any prior {{assumptions}} about distribution {{can influence the}} <b>probability</b> <b>distribution</b> <b>function</b> produced.|$|E
5000|$|Waiting time <b>probability</b> <b>distribution</b> <b>function</b> [...] has the {{following}} asymptotic behavior (see, Ref.1) ...|$|E
2500|$|The <b>probability</b> <b>distribution</b> <b>functions</b> {{associated}} with the position wave function [...] and the momentum wave function [...] have dimensions of inverse length and momentum respectively, but the entropies may be rendered dimensionless by ...|$|R
50|$|Let D+ be {{the set of}} all <b>probability</b> <b>distribution</b> <b>functions</b> F {{such that}} F(0) = 0 (F is a nondecreasing, leftcontinuous mapping from R into 1 such that sup(F(x)) = 1 for x∈R.|$|R
50|$|The low-LET {{mortality}} rate per sievert, mi is writtenwhere m0 is the baseline {{mortality rate}} per sievert and xα are quantiles (random variables) whose values are sampled from associated <b>probability</b> <b>distribution</b> <b>functions</b> (PDFs), P(Xa).|$|R
