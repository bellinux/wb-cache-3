4|18|Public
40|$|Abstract- In today’s technologically {{advanced}} world, network security {{is most important}} to protect confidential documents against misuse of the system. Network security is simply a process or action adopted to detect as well as prevent unauthorized usage of computer. Therefore, it is vital for any network administrator, regardless of the size and type of network, to implement stringent security policies to prevent potential losses. Contemporary network security applications and traditional hardware solutions are not adoptable for antivirus with ever-larger pattern set. So, {{our goal is to}} provide two <b>phase</b> <b>dictionary</b> based heuristic algorithm for network security for embedded system. In the first phase, filtering engine filters the secure data efficiently by using merged shift signature table. In the second phase, exact matching engine verifies the alarm caused by the filtering engine and filters the remaining unsafe data...|$|E
40|$|Abstract — — This paper {{presents}} {{the design and}} implementation of English language Tutorial system, which supports students at home learning the English language. We concentrate on implementation of different modules of this system such as the learn phase, test <b>phase,</b> <b>dictionary,</b> pronunciation section which is devoted to generate, in a cognitively transparent way, the right tense for the verb appearing in the exercises presented to the student. A model edition of English language Tutorial system has been fully implemented {{with the help of}} visual basic 6. 0 as front end, ms access and flash files as back end. English language tutorial system essentially classify into three level like primary level, middle level and high level. This paper depicts the components and technology used to design and implement of this system with intelligent feedback system which used to provide the response to user so as to helps {{to be aware of the}} knowledle intensity of user after interaction with this system...|$|E
40|$|Abstract [...] - The {{ordinary}} {{network security}} applications require {{the ability to}} perform powerful pattern matching to protect against attacks such as viruses. In normal case hardware solutions are intended for firewall routers. The solutions in the literature for firewalls are not scalable {{and they do not}} address the difficulty of an antivirus with ever large pattern set. This work provides a systematic virus detection hardware solution for embedded network security systems. It is a two <b>phase</b> <b>dictionary</b> based antivirus processor that works by condensing as much of the important filtering information as possible onto a chip and infrequently accessing off chip data to make the matching mechanism scalable to large pattern sets. In the first stage, the filtering engine can filter out more than 93 % of data as safe, using a merged shift table. Only 7 % or less of potentially unsafe data must be precisely checked in the second stage by the exact-matching engine from off-chip memory...|$|E
40|$|In {{this paper}} {{we present a}} {{biomedical}} event extraction system for the BioNLP 2013 event extraction task. Our system consists of two phases. In the learning <b>phase,</b> a <b>dictionary</b> and patterns are generated automatically from annotated events. In the extraction <b>phase,</b> the <b>dictionary</b> and obtained patterns are applied to extract events from input text. When evaluated on the GENIA event extraction task of the BioNLP 2013 shared task, the system obtained the best results on strict matching and the third best on approximate span and recursive matching, with F-scores of 48. 92 and 50. 68, respectively. Moreover, it has excellent performance in terms of speed. ...|$|R
50|$|Several {{resources}} {{have been added}} under subsequent <b>phases.</b> A <b>dictionary</b> of 10,000 sports terms has been added as well as tools for translation memories and other resources for translators including a link facility to The New Corpus for Ireland.|$|R
5000|$|Many in the {{leadership}} of ASALA and JCAG were reported to be highly educated, multilingual individuals. The most notorious was the shadowy figure of Hagop Hagopian, presumed an alias, who led ASALA during most of its active <b>phase.</b> Historical <b>Dictionary</b> of Armenia, by Rouben Paul Adalian, 2010 - p. 170 ...|$|R
40|$|Generation of compound-complex {{sentences}} for English tutorial {{system is a}} challenging task, because of the Users that interact with this system they make English influent by using this tutorial system and ability of English tutorial system helps users to make growth {{in any kind of}} business and organization by taking advantage of English language. The task requires designing of Structural representation, which can encode the information contained in the English sentence, and designing a represent a module, which can be use to learn the English tutorial from the structural representation, with the use of rule sets and the lexicon. We concentrate on implementation of different modules of this system such as the learn phase, test <b>phase,</b> <b>dictionary,</b> pronunciation section which is devoted to generate, in a cognitively transparent way, the right tense for the verb appearing in the exercises presented to the student. This paper is concerned with the specifications and the implementation of a particular concept of word-based lexicon to be used for large natural language processing systems such as machine translation systems. This paper tries to explore the development of English language tutorial system for world. In this model we introduce six phases of this tutorial model and each perform some specific task...|$|E
40|$|Abstract—The set of minutia {{points is}} {{considered}} to be the most distinctive feature for fingerprint representation and is widely used in fingerprint matching. It was believed that the minutiae set does not contain sufficient information to reconstruct the original fingerprint image from which minutiae were extracted. However, recent studies have shown that it is indeed possible to reconstruct fingerprint images from their minutiae representations. Reconstruction techniques demonstrate the need for securing fingerprint templates, improve the template interoperability and improve fingerprint synthesis. But, there is still a large gap between the matching performance obtained from original fingerprint images and their corresponding reconstructed fingerprint images. In this paper, the prior knowledge about fingerprint ridge structures is encoded in terms of orientation patch and continuous <b>phase</b> patch <b>dictionaries</b> to improve the fingerprint reconstruction. The orientation patch dictionary is used to reconstruct the orientation field from minutiae, while the continuous <b>phase</b> patch <b>dictionary</b> is used to reconstruct the ridge pattern. Experimental results on three public domain databases (FVC 2002 DB 1 A, FVC 2002 DB 2 A and NIST SD 4) demonstrate that the proposed reconstruction algorithm outperforms the state-of-the-art reconstruction algorithms in terms of both i) spurious and missing minutiae and ii) matching performance with respect to type-I attack (matching the reconstructed fingerprint against the same impression from which minutiae set was extracted) and type-II attack (matching the reconstructed fingerprint against a different impression of the same finger). Index Terms—fingerprint reconstruction, orientation patch <b>dictionary,</b> continuous <b>phase</b> patch <b>dictionary,</b> minutiae set, AM-FM model. I...|$|R
40|$|Languages {{other than}} English have {{received}} little attention {{as far as}} the application of natural language processing techniques to text composition is concerned. The present paper describes briefly work under development aiming at the design of an integrated environment for the construction and verification of documents written in Spanish. in a first <b>phase,</b> a <b>dictionary</b> of Spanish has been implemented, together with a synonym dictionary. The main features of both dictionaries will be summarised, and how they are applied in an environment for document verification and composition...|$|R
30|$|The results {{reported}} in the previous subsections assume that a single day’s worth of data is utilized during the training <b>phase</b> where the <b>dictionary</b> D is obtained. Here, we investigate the recovery capability of the proposed SS-MC method {{as a function of}} the amount of training data, i.e., the number of days used for training.|$|R
40|$|In {{this paper}} we propose a new {{approach}} of fault location technique for testing analog circuits using a fault dictionary based on the simulation before test. Based-on Nichlos chart, in terms of magnitude and <b>phase,</b> a fault <b>dictionary</b> is a priori generated by collecting signatures of different fault conditions. From the analog circuits fault diagnosis examples, the performances of the advocated methodology can be demonstrated. I...|$|R
40|$|Bilingual {{dictionaries}} hold {{great potential}} {{as a source of}} lexical resources for training and testing automated systems for optical character recognition, machine translation, and cross-language information retrieval. In this paper, we describe a system for extracting term lexicons from printed bilingual dictionaries. Our work was divided into three <b>phases</b> - <b>dictionary</b> segmentation, entry tagging, and generation. In segmentation, pages are divided into logical entries based on structural features learned from selected examples. The extracted entries are associated with functional labels and passed to a tagging module which associates linguistic labels with each word or phrase in the entry. The output of the system is a structure that represents the entries from the dictionary. We have used this approach to parse a variety of dictionaries with both Latin and non-Latin alphabets, and demonstrate the results of term lexicon generation for retrieval from a collection of French news stories using English queries. (LAMP-TR- 106) (CAR-TR- 991) (UMIACS-TR- 2003 - 97...|$|R
40|$|Bilingual {{dictionaries}} hold {{great potential}} {{as a source of}} lexical resources for training automated systems for optical character recognition, machine translation, and cross-language information retrieval. More importantly, they represent a class of document that {{have a great deal of}} structure, and this structure is not fundamentally spatial. Structure is provided by the authors (or publishers) who use of different fonts, font style, spacing and special symbols to implicitly convey information on the structure of the content. Our system is divided into three <b>phases</b> – <b>Dictionary</b> Segmentation, Entry Tagging and Generation. In segmentation, pages are divided into logical entries based on structural features learned from selected examples. The extracted entries are associated with functional labels and passed to a tagging module that associates linguistic labels with each word or phrase in the entry. The output of the system is a structure that represents the entries of the dictionary. In this document, we discuss the fundamental image processing approach that lets us extract this structure. Details of how we perform content tagging is mentioned briefly and references to related publications are provided...|$|R
40|$|In {{big data}} image/video analytics, we {{encounter}} {{the problem of}} learning an overcomplete dictionary for sparse representation from a large training dataset, {{which can not be}} processed at once because of storage and computational constraints. To tackle the problem of dictionary learning in such scenarios, we propose an algorithm for parallel dictionary learning. The fundamental idea behind the algorithm is to learn a sparse representation in two phases. In the first phase, the whole training dataset is partitioned into small non-overlapping subsets, and a dictionary is trained independently on each small database. In the second <b>phase,</b> the <b>dictionaries</b> are merged to form a global dictionary. We show that the proposed algorithm is efficient in its usage of memory and computational complexity, and performs on par with the standard learning strategy operating on the entire data at a time. As an application, we consider the problem of image denoising. We present a comparative analysis of our algorithm with the standard learning techniques, that use the entire database at a time, in terms of training and denoising performance. We observe that the split-and-merge algorithm results in a remarkable reduction of training time, without significantly affecting the denoising performance...|$|R
40|$|This paper {{proposes a}} {{two-stage}} method for hand depth image denoising and superresolution, using bilateral filters and learned dictionaries via noise-aware orthogonal matching pursuit (NAOMP) based K-SVD. The bilateral filtering phase recovers singular points and removes artifacts on silhouettes by averaging depth data using neighborhood pixels on which both depth difference and RGB similarity restrictions are imposed. The <b>dictionary</b> learning <b>phase</b> uses NAOMP for training dictionaries which separates faithful depth from noisy data. Compared with traditional OMP, NAOMP adds a residual reduction step which effectively weakens the noise term within the residual during the residual decomposition {{in terms of}} atoms. Experimental results demonstrate that the bilateral phase and the NAOMP-based learning <b>dictionaries</b> <b>phase</b> corporately denoise both virtual and real depth images effectively...|$|R
40|$|Abstract Image {{deblurring}} is {{a challenging}} problem in vision computing. Traditionally, this task is addressed as an inverse {{problem that is}} enclosed into the image itself. This paper presents a learning-based framework where the knowl-edge hidden in huge amounts of available data is explored and exploited for image deblurring. To this end, our algo-rithm is developed under the conceptual framework of cou-pled dictionary learning. Specifically, given pairs of blurred image patches and their corresponding clear ones, a learning model is constructed to learn a pair of dictionaries. Among them, one dictionary {{is responsible for the}} representation of clear images, while the other is responsible for that of the blurred images. Theoretically, the learning model is analyzed with coupled sparse representations for training samples. As the atoms of these dictionaries are coupled together one-by-one, the reconstruction information can be transmitted between the clear and blurry images. In application <b>phase,</b> the blurry <b>dictionary</b> is employed to reconstruct linearly the blurry image to be restored. Then, the reconstruction coeffi...|$|R
40|$|While vector {{quantization}} (VQ) {{has been applied}} widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding {{has emerged as a}} strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding <b>phase,</b> where the <b>dictionary</b> is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find {{it is more important to}} choose a good encoder—which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB...|$|R
40|$|AbstractLexicographical {{corpora for}} {{dictionary}} making are normally {{used as an}} empirical base of the dictionary. They provide highly valuable lexical material, patterns, attestations of use, etc. However, once exploited by the lexicographer, they remain hidden to the user. The question thus is: Can a corpus become even more useful {{not only to the}} lexicographer, but also to the user? How can a corpus become a functional component of the dictionary? I will address these questions by outlining the methodological issues related to the design and exploitation of an oral corpus for the construction of Oenolex, a dictionary of wine tasting. In Oenolex, the corpus is not simply the empirical base of the dictionary: it is lexicographically structured {{so that it can be}} accessed by the user. Oenolex also shows how oral corpora can be an asset for specialised dictionary projects, particularly in domains in which terminology is subject to a high degree of variation. This in turn calls for an even closer cooperation with experts; I will therefore argue, on the basis of the Klosa model of lexicographical phases (2013), that experts from the field of specialised communication and knowledge covered by the dictionary are truly needed in almost all <b>phases</b> of the <b>dictionary</b> process...|$|R
40|$|In this thesis, a {{theoretical}} background of algorithms called NLS-BB-NMF and K-SVD for computing the image dictionary have been introduced. The NLS-BB-NMF algorithm computes the matrix factorization V ≈ WH {{of the training}} data matrix V (in our case the set of image patches from training image) using gradient descent methods by applying non-negative constraint on matrices W and H. The K-SVD in turn computes the matrix factorization WH applying sparsity constraint on the coefficient matrix H using Orthogonal Matching Pursuit (OMP) and Singular Value Decomposition (SVD). In the factorization, matrix W is the so called dictionary and it contains features, also called atoms, of the data V. The atoms serve as a building blocks of the original data, and they are also assumed to represent data {{that is similar to}} the training data V. The testing of the methods were carried in two phases. Initially, in the so called training <b>phase,</b> the <b>dictionary</b> was learned by the algorithms from a training image. The visual structure of the atoms learned by the algorithms were notably different although the approximations WH made by both dictionaries were visually very close to the original image. The visual difference between the learned dictionaries was seen {{as a consequence of the}} sparsity constraint that was forced for the coefficient matrix in K-SVD but not in NLS-BB-NMF. Secondly, in the test phase, a test image with various noise levels was approximated using the learned dictionary. The algorithms were able to produce approximations that were closer to the clean test image than the noisy test image. This was seen as the effect of dictionaries whose atoms were representing only the features of clean images. This observation led to a second test where the algorithms were tested to compute the denoised reconstructions of the test image with varying noise levels by using an extended dictionary containing additionally atoms learned from a noise sample. The qualities of the reconstructions were evaluated by using the Frobenius matrix norm and Structural Similarity (ssim) index that has been observed to adapt better the visual perception of human eyes...|$|R
40|$|Real-world data {{processing}} problems often involve various image modalities {{associated with a}} certain scene, including RGB images, infrared images or multispectral images. The fact that different image modalities often share diverse attributes, such as certain edges, textures and other structure primitives, represents an opportunity to enhance various image processing tasks. This paper proposes {{a new approach to}} construct a high-resolution (HR) version of a low-resolution (LR) image given another HR image modality as reference, based on joint sparse representations induced by coupled dictionaries. Our approach, which captures the similarities and disparities between different image modalities in a learned sparse feature domain in lieu of the original image domain, consists of two <b>phases.</b> The coupled <b>dictionary</b> learning <b>phase</b> is used to learn a set of dictionaries that couple different image modalities in the sparse feature domain given a set of training data. In turn, the coupled super-resolution phase leverages such coupled dictionaries to construct a HR version of the LR target image given another related image modality. One of the merits of our sparsity-driven approach relates {{to the fact that it}} overcomes drawbacks such as the texture copying artifacts commonly resulting from inconsistency between the guidance and target images. Experiments on both synthetic data and real multimodal images demonstrate that incorporating appropriate guidance information via joint sparse representation induced by coupled dictionary learning brings notable benefits in the super-resolution task with respect to the state-of-the-art. Comment: 13 pages, submitted to IEEE Transactions on Image Processin...|$|R
40|$|Background and Problem Definition: The CEO {{letter is}} one {{significant}} narrative document through which senior management have {{opportunity to express}} beliefs and values to their shareholders. The CEO letter is unregulated in its nature and thereby subject to management opportunism through tone management. Tone management could further be used to manipulate {{the perception of the}} firm, which causes information asymmetry. Similar to the purpose of tone management, accruals could be opportunistically managed in order to manipulate users’ perceptions of firm fundamentals. Thus, managers could through CEO letters employ tone management to potentially hide earnings management, and thereby mislead users about firm fundamentals. Purpose: The purpose of this thesis is to test the possible association between tone management in CEO letters and earnings management, using data from 2013 including firms listed on the London Stock Exchange. Subsequently, the intention is to investigate if earnings management and tone management are substitutes or complements. The purpose is additionally to test the relation between tone management in CEO letters and financial performance Research Design and Methodology: The theoretical framework of this thesis is used to develop hypotheses, which subsequently guide the research forward. In the execution <b>phase,</b> a customized <b>dictionary</b> is developed to fit with the purpose of the thesis. The execution phase continues in correlation and multiple regression analysis. The outcome of the statistical tests contributes to fulfill the purpose. Empirical Results and Analysis: In accordance with previous literature, the empirical results reveal a positive association between tone in CEO letters and financial performance. Strengthening the purpose and the contribution of this thesis, empirical evidences reveal that abnormal tone in CEO letters and abnormal accruals are positively associated. Concluding Remarks: The tone in CEO letters is generally positive regardless of management discretion. Furthermore the tone in CEO letters can be derived from firm performance, firm size and annual stock return. Finally, the findings indicate that tone management and earnings management through the use of abnormal tone and abnormal accruals functions as complements and are thus not substitutes...|$|R
40|$|We can use {{creative}} interactive {{methods to}} engage speakers in language documentation {{and the development}} of language materials. Friendly competition keeps speakers involved in what could otherwise be a dull task of listing words. New participants seek out the opportunity {{to be involved in the}} future as participants when positive engagement attitudes are developed. To this end, we applied principles of gamification (Hamari et al. 2014) to the Rapid Word collection <b>phase</b> of the <b>Dictionary</b> Development Program (DDP) (Shore & van den Berg 2006; Moe 2004). We present the outcomes of applying gamification to the DDP in an effort to collect and verify words in a narrow set of semantic domains for the Mòòré language [mos]. Mòòré is widely spoken, underdocumented Gur language primarily spoken in Burkina Faso. Data was collected by native speakers of Mòòré on a university campus. The basic procedure for the game is as follows: In round 1, two teams of 3 - 4 speakers compete with each other in attempt to accumulate more words than the other team in each semantic domain as it is presented. Each team appoints a scribe equipped with paper and pen who records words in the existing Mòòré orthography; other team members participate verbally. Points accrue for words that are collected that are distinct from the other team, i. e. unique words will score points. In round 2, the teams exchange sets of words and must use the words compiled by the other team in round 1 to make sentences in the language that exemplify the meaning of the word. Round 2 verifies the words and offers some idea that shared meaning exists between the speakers on the two competing teams. We present a methodology involving multiple speakers that appears to be effective and efficient, compared to elicitation of terms with one speaker in the same time frame. At the same time, we show the possibility of collecting discourse data as participants negotiate amongst themselves key definitions and example sentences. In post-game interviews with participants, we also asked speakers about their reactions to the task, and the likelihood of them participating again, though it remains to be seen if they will participate again. References: Hamari, Juho, Jonna Koivisto, & Harri Sarsa. 2014. Does Gamification Work? – A Literature Review of Empirical Studies on gamification. In Proceedings of the 47 th Hawaii International Conference on System Sciences, Hawaii, USA, January 6 - 9, 2014. Moe, Ronald. 2003. Compiling dictionaries using semantic domains. Lexicos 13 : 215 - 223. [URL] Shore, Susan, and Rene van den Berg. 2006. A new mass elicitation technique: the Dictionary Development Program. Paper presented at Tenth International Conference on Austronesian Linguistics. 17 - 20 January 2006. Puerta Princesa City, Palawan, Philippines. [URL]...|$|R

