652|1062|Public
5000|$|... #Subtitle level 2: <b>Probability</b> <b>analysis</b> of I Ching divination ...|$|E
5000|$|Statistical Independence in <b>Probability,</b> <b>Analysis</b> and Number Theory, by Mark Kac ...|$|E
5000|$|GALENA - {{includes}} stability analysis, back analysis, and <b>probability</b> <b>analysis,</b> {{using the}} Bishop, Spencer-Wright and Sarma methods.|$|E
5000|$|... #Article: Applications of p-boxes and <b>probability</b> bounds <b>analysis</b> ...|$|R
40|$|This {{document}} {{provides a}} detailed overview of <b>probability</b> bounds <b>analysis.</b> In the sections that follow, the conceptual {{background of the}} approach is briefly presented, followed by the mathematical derivation of probability bounds around parametric, non-parametric, empirical, and assumed or stipulated models. Computation with p-boxes is then described, and numerical examples of computations are provided. In the next section, <b>probability</b> bounds <b>analysis</b> is compared and contrasted with Monte Carlo simulation techniques. Methods used by Monte Carlo analysts for treating input variables, dependencies between input variables, and model uncertainty are compared to methods used in <b>probability</b> bounds <b>analysis.</b> Techniques for implementing microexposure event analysis models are also compared, along with methods for conducting sensitivity analysis. Finally, the use of <b>probability</b> bounds <b>analysis</b> within the tiered framework for conducting probabilistic risk assessments recommended by EPA is discussed...|$|R
50|$|Mathematics {{is divided}} into fields of Algebra, Geometry, Linear Algebra, Analytical Geometry, Mathematical <b>Analysis,</b> <b>Probability,</b> Statistics, Numerical <b>Analysis,</b> and Selected Chapters in Mathematics.|$|R
5000|$|Mark Kac, Statistical Independence in <b>Probability,</b> <b>Analysis</b> and Number Theory, Carus Mathematical Monographs, Mathematical Association of America, 1959.|$|E
5000|$|Examples of data {{compaction}} {{methods are}} {{the use of}} fixed-tolerance bands, variable-tolerance bands, slope-keypoints, sample changes, curve patterns, curve fitting, variable-precision coding, frequency analysis, and <b>probability</b> <b>analysis.</b>|$|E
5000|$|Fink, Bob, 2002-3, [...] "The Neanderthal flute and {{origin of}} the scale: fang or flint? A response," [...] in: Ellen Hickmann, Anne Draffkorn Kilmer and Ricardo Eichmann (Eds.), Studies in Music Archaeology III, Verlag Marie Leidorf GmbH., Rahden/Westf. Germany, pp 83-87. <b>Probability</b> <b>analysis.</b>|$|E
50|$|Michael RÃ¶ckner is a {{mathematician}} {{working in the}} fields of <b>Probability</b> Theory, <b>Analysis</b> and Mathematical Physics.|$|R
40|$|Composite {{aircraft}} structures {{suffer from}} more complex damage modes than traditional metals. Many investigations {{tend to look}} into specific aspects of composite damage mechanisms but seldom take a more systematic view. This paper introduces a new fault tree analysis to capture information on damage modes of composite structures by identifying damage sources and process defects. Minimal cut sets are calculated based {{on the construction of}} the fault tree. Qualitative analysis is then performed incorporating Structure Importance <b>Analysis,</b> <b>Probability</b> Importance <b>Analysis</b> and Relative <b>Probability</b> Importance <b>Analysis</b> to determine the main damage modes and sources. Some potential solutions are proposed accordingly to improve the reliability of composite structures...|$|R
5000|$|P-boxes and <b>probability</b> bounds <b>analysis</b> {{have been}} used in many {{applications}} spanning many disciplines in engineering and environmental science, including: ...|$|R
5000|$|Steven N. Goodman is an American Professor of Medicine and of Health Research and Policy (Epidemiology) at Stanford School of Medicine. He has {{extensively}} {{contributed to}} statistics and <b>probability</b> <b>analysis</b> within the biosciences, and in 1999 he {{coined the term}} [...] "p-value fallacy".|$|E
50|$|A <b>probability</b> <b>analysis</b> {{was carried}} out for flows in the River Foss {{combined}} with high levels in the River Ouse. This indicated a maximum flow of 30 t of water per second. In March 2016, a budget was approved to increase capacity to 40 t per second.|$|E
5000|$|The {{proof of}} {{correctness}} follows from choice of parameters and some <b>probability</b> <b>analysis.</b> The proof of security is by reduction to the decision version of LWE: an algorithm for distinguishing between encryptions (with above parameters) of [...] and [...] {{can be used to}} distinguish between [...] and the uniform distribution over ...|$|E
5000|$|... #Subtitle level 2: <b>Analysis,</b> <b>probability,</b> and astronomical {{stability}} ...|$|R
5000|$|Programming, algorithms, {{information}} technology, mathematics, <b>analysis,</b> <b>probabilities,</b> statistics, ...|$|R
50|$|This {{bounding}} approach permits {{analysts to}} make calculations without requiring overly precise assumptions about parameter values, dependence among variables, or even distribution shape. <b>Probability</b> bounds <b>analysis</b> {{is essentially a}} combination of the methods of standard interval <b>analysis</b> and classical <b>probability</b> theory. <b>Probability</b> bounds <b>analysis</b> gives the same answer as interval analysis does when only range information is available. It also gives the same answers as Monte Carlo simulation does when information is abundant enough to precisely specify input distributions and their dependencies. Thus, it is a generalization of both interval <b>analysis</b> and <b>probability</b> theory.|$|R
50|$|American {{football}} win probability estimates {{often include}} whether a team is home or away, the down and distance, score difference, time remaining, and field position. American football has many more possible states than baseball with far fewer games, so football estimates {{have a greater}} margin of error. The first win <b>probability</b> <b>analysis</b> was done in 1971 by Robert E. Machol and former NFL quarterback Virgil Carter.|$|E
50|$|On April 5, 2012, {{astronomer}} Mikko Tuomi of the University of Hertfordshire {{submitted a}} paper to Astronomy and Astrophysics approved for publishing on the April 6, 2012 that proposed a nine planet {{model for the}} system. Re-analysing the data using Bayesian <b>probability</b> <b>analysis,</b> previously known planets' parameters were revised and further evidence was found for the innermost planet (b) as well as evidence of two additional planets (i and j).|$|E
5000|$|The {{profession}} got {{its start}} in 1914 within the American Institute of Mining, Metallurgical and Petroleum Engineers (AIME). The first Petroleum Engineering degree was conferred in 1915 by the University of Pittsburgh. [...] Since then, the profession has evolved to solve increasingly difficult situations, {{as much of the}} [...] "low hanging fruit" [...] of the world's oil fields have been found and depleted. Improvements in computer modeling, materials and the application of statistics, <b>probability</b> <b>analysis,</b> and new technologies like horizontal drilling and enhanced oil recovery, have drastically improved the toolbox of the petroleum engineer in recent decades.|$|E
50|$|Alexandra Bellow (formerly Alexandra Ionescu Tulcea; born 30 August 1935) is a {{mathematician}} from Bucharest, Romania, {{who has made}} contributions to the fields of ergodic theory, <b>probability</b> and <b>analysis.</b>|$|R
40|$|Composite {{airframes}} {{suffer from}} complex damage modes during operation. Many investigations {{tend to look}} into specific aspects of damage mechanisms but seldom take a systematic view. This paper introduces a new fault tree methodology to synthesize various damage modes of composite structures by identifying possible damage causes. Qualitative analysis is performed incorporating structure importance <b>analysis,</b> <b>probability</b> importance <b>analysis</b> and relative <b>probability</b> importance <b>analysis.</b> Quantitative analysis by Monte Carlo simulation is then conducted as a validation to demonstrate the feasibility of the fault tree for composite damages. A number of options addressing main damage causes are proposed to improve the reliability of composite structures. Engineers from airlines and manufacturers can use this method to prioritize the main damage causes in different situations as a failure preventative tool or damage evaluation. Also, this approach can be extended to provide valuable inputs to other advanced methodologies to perform better diagnosis and prognosis for composites...|$|R
50|$|Electrical Engineering careers usually include {{courses in}} Calculus (single and multivariable), Complex Analysis, Differential Equations (both {{ordinary}} and partial), Linear Algebra and <b>Probability.</b> Fourier <b>Analysis</b> and Z-Transforms are also subjects {{which are usually}} included in electrical engineering programs.|$|R
50|$|The final {{decision}} to accept or reject a model hypothesis {{is based on a}} detailed probabilistic model. This method first computes the expected number of false matches to the model pose, given the projected size of the model, the number of features within the region, and the accuracy of the fit. A Bayesian <b>probability</b> <b>analysis</b> then gives the probability that the object is present based on the actual number of matching features found. A model is accepted if the final probability for a correct interpretation is greater than 0.98. Lowe's SIFT based object recognition gives excellent results except under wide illumination variations and under non-rigid transformations.|$|E
50|$|To avoid wasting scarce bombe time on menus {{that were}} likely to yield an {{excessive}} number of false stops, Turing performed a lengthy <b>probability</b> <b>analysis</b> (without any electronic aids) of the estimated number of stops per rotor order. It was adopted as standard practice only to use menus that were estimated to produce {{no more than four}} stops per wheel order. This allowed an 8-letter crib for a 3-closure menu, an 11-letter crib for a 2-closure menu and a 14-letter crib for a menu with only one closure. If there was no closure, at least 16 letters were required in the crib. The longer the crib, however, the more likely it was that turn-over of the middle rotor would have occurred.|$|E
50|$|Some of the {{advantages}} of this analysis are that it has minimum systemic errors, easy to perform by both beginners and expert dentists, it can be used for both the arches, can be performed on both casts or in mouth, and does not require any radiographic projections. Limitations of this analysis are that this is a <b>probability</b> <b>analysis,</b> does not account for tipping of incisors lingual or buccal and that maxillary tooth sizes are predicted by the mandibular tooth sizes. Other limitations of this analysis is that Moyer's never clarified which population this analysis was done on initially. Therefore, this analysis cannot be truly applied to different populations. This was confirmed by a systematic review which was performed in 2004.|$|E
3000|$|Crack <b>{{probability}}.</b> The <b>analysis</b> of {{the crack}} probability for transactions and patterns in both databases CoopProd and CoopCat highlighted {{that after the}} data transformation around 90 % of the transactions can be broken with probability strictly less than [...]...|$|R
5000|$|S. Bobkov [...] "is {{well known}} internationally for his {{research}} in mathematics {{on the border}} of <b>probability</b> theory, <b>analysis,</b> convex geometry and information theory. He has achieved important results about isoperimetric problems, concentration of measure and other high-dimensional phenomena." ...|$|R
30|$|We {{investigate}} {{in detail}} the effects of shadowing, power control factor, and BSsâ density on the coverage <b>probability</b> using <b>analysis</b> and simulations. We show that under all three power control schemes, density of BSs has no {{significant effect on the}} coverage.|$|R
5000|$|In a 2003 {{newspaper}} article, Edwin Cameronâa senior South African {{judge who}} has AIDSâdescribed the tactics used {{by those who}} deny the Holocaust and by those who deny that the AIDS pandemic is due to infection with HIV. He states that [...] "For denialists, the facts are unacceptable. They engage in radical controversion, for ideological purposes, of facts that, by and large, are accepted by almost all experts and lay persons as having been established {{on the basis of}} overwhelming evidence". To do this they employ [...] "distortions, half-truths, misrepresentation of their opponents' positions and expedient shifts of premises and logic." [...] Edwin Cameron notes that a common tactic used by denialists is to [...] "make great play of the inescapable indeterminacy of figures and statistics", as scientific studies of many areas rely on <b>probability</b> <b>analysis</b> of sets of data, and in historical studies the precise numbers of victims and other facts may not be available in the primary sources.|$|E
50|$|Between {{the time}} of the {{previous}} analysis and June 2012, the rest of the radial-velocity measurements used in 2011 were made public, allowing them to be reduced using HARPS-TERRA. These were then analysed via a Bayesian <b>probability</b> <b>analysis,</b> which was previously used to discover HD 10180 i and j, which confirmed planet b and made a first characterisation of planet c, which was previously only described as a trend. After the first two signals were introduced, the next most powerful signal was at around 35.5 days, with an analytic false alarm probability of 0.156. Through 104 trials, the false alarm probability was found to be 0.44%, low enough for it to be included as a periodic, planetary signal. With a minimum mass of around 11 Earths, the planet lies at the accepted border between Super-Earths and gaseous, Neptune-like bodies of 10 Earths.After accepting the third signal, a strong peak at 3.6 days became apparent. With a false alarm probability much lower than that of the previously accepted body, it was immediately accepted. With a minimum mass of around 4.5 Earths, it is a small Super-Earth.|$|E
5000|$|The {{purpose of}} this {{algorithm}} is to match MSERs to establish correspondence points between images. First MSER regions are computed on the intensity image (MSER+) and on the inverted image (MSER-). Measurement regions are selected at multiple scales: {{the size of the}} actual region, 1.5x, 2x, and 3x scaled convex hull of the region. Matching is accomplished in a robust manner, so it is better to increase the distinctiveness of large regions without being severely affected by clutter or non-planarity of the region's pre-image. A measurement taken from an almost planar patch of the scene with stable invariant description are called a 'good measurement'. Unstable ones or those on non-planar surfaces or discontinuities are called 'corrupted measurements'. The robust similarity is computed:For each [...] on region [...] regions [...] from the other image with the corresponding i-th measurement [...] nearest to [...] are found and a vote is cast suggesting correspondence of A and each of [...] Votes are summed over all measurements, and using <b>probability</b> <b>analysis,</b> we pick out the 'good measurements' as the 'corrupt measurements' will likely spread their votes randomly. By applying RANSAC to the centers of gravity of the regions, we can compute a rough epipolar geometry. An affine transformation between pairs of potentially corresponding regions is computed, and correspondences define it up to a rotation, which is then determined by epipolar lines. The regions are then filtered, and the ones with correlation of their transformed images above a threshold are chosen. RANSAC is applied again with a more narrow threshold, and the final eipolar geometry is estimated by the eight-point algorithm.|$|E
40|$|This {{research}} {{employed the}} theory of the dialogical self to study the effect of migration on the self. Data from structured interviews of a sample of 38 migrants to Australia yielded life narratives that incorporated distinct Australian and country of origin perspectives or I-positions Cluster <b>analysis,</b> <b>probability</b> distribution <b>analysis</b> and case studies identified two modes of biculturalism, compatible and oppositional, and showed that many migrants had I-positions with unambiguously different content. The data support the utility of a dialogical approach to understanding the sense of self rather than models based on a single integrated self...|$|R
5000|$|He {{is renowned}} as the {{developer}} of the [...] "Gittins index", {{which is used}} for sequential decision-making, especially {{in research and development}} in the pharmaceutical industry. He has research interests in applied <b>probability,</b> decision <b>analysis</b> and optimal decisions, including optimal stopping and stochastic optimization.|$|R
25|$|The Intergovernmental Panel on Climate Change {{relies on}} Monte Carlo methods in <b>probability</b> density {{function}} <b>analysis</b> of radiative forcing.|$|R
