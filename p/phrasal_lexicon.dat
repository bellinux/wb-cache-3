16|5|Public
40|$|Abstract. In this work, {{the use of}} a <b>phrasal</b> <b>lexicon</b> for {{statistical}} machine translation is proposed, and the relation between data acquisition costs and translation quality for different types and sizes of language resources has been analyzed. The language pairs are Spanish-English and Catalan-English, and the translation is performed in all directions. The <b>phrasal</b> <b>lexicon</b> is used to increase as well as to replace the original training corpus. The augmentation of the <b>phrasal</b> <b>lexicon</b> with the help of additional monolingual language resources containing morpho-syntactic information has been investigated for the translation with scarce training material. Using the augmented <b>phrasal</b> <b>lexicon</b> as additional training data, a reasonable translation quality can be achieved with only 1000 sentence pairs from the desired domain. 1 Introduction and Related Work The goal of statistical machine translation (SMT) is to translate an input word sequence f J 1 = f 1 [...] . fj [...] . fJ into a target word sequence eI 1 = e 1 [...] . ei [...] . eI by maximising the probabilit...|$|E
40|$|This article {{proposes a}} novel {{technique}} to generate natural language descriptions {{for a wide}} class of relational database queries. The approach to describing queries is phrasal and is restricted to a class of queries that return only whole schema tuples as answers. Query containment and equivalence are decidable for this class and this property is exploited in the maintenance and use of a <b>phrasal</b> <b>lexicon.</b> The query description mechanism is implemented within the STEP (Schema Tuple Query Processor) syste...|$|E
40|$|One key to {{the success}} of EBMT is the removal of the {{boundaries}} limiting the potential of translation memories (TMs). We discuss a linguistically enhanced TM system, a <b>Phrasal</b> <b>Lexicon</b> (PL), which takes advantage of the huge, underused resources available in existing translation aids. We claim that PL and EBMT systems can only provide valuable translation solutions for restricted domains, especially where controlled language restrictions are imposed. When integrated into a hybrid and/or multi-engine MT environment, the PL will yield significant improvements in translation quality. We establish a future model of translation usage and anticipate that EBMT and the PL will have a central place in future hybrid integrated translation platforms...|$|E
40|$|We present new direct data {{analysis}} showing that dynamically-built context-dependent <b>phrasal</b> translation <b>lexicons</b> are more useful resources for phrase-based {{statistical machine translation}} (SMT) than conventional static <b>phrasal</b> translation <b>lexicons,</b> which ignore all contextual information. After several years of surprising negative results, recent work suggests that context-dependent <b>phrasal</b> translation <b>lexicons</b> are an appropriate framework to successfully incorporate Word Sense Disambiguation (WSD) modeling into SMT. However, this approach has so far only been evaluated using automatic translation quality metrics, which are important, but aggregate many different factors. A direct analysis is still needed to understand how context-dependent <b>phrasal</b> translation <b>lexicons</b> impact translation quality, and whether the additional complexity they introduce is really necessary. In this paper, {{we focus on the}} impact of context-dependent translation lexicons on lexical choice in phrase-based SMT and show that context-dependent lexicons are more useful to a phrase-based SMT system than a conventional lexicon. A typical phrase-based SMT system makes use of more and longer phrases with context modeling, including phrases that were not seen very frequently in training. Even when the segmentation is identical, the context-dependent lexicons yields translations that match references more often than conventional lexicons. 1...|$|R
40|$|This paper {{presents}} a novel {{view of the}} boundary between the generalizable and the idiosyncratic in MT lexicons. We argue that the domain of the idiosyncratic should, in fact, be broader than in most current approaches. While at present most MT systems involve <b>phrasal</b> <b>lexicons,</b> these typically contain terminology from a particular field. In order to facilitate naturalness of translation, specifically, to carry the level of “conventionality ” of meaning expression across languages, it becomes necessary to use the concept of a grammatical construction, a (possibly, discontiguous) syntactic structure or productive syntactic pattern whose meaning it is often impossible to derive solely based on the meanings of its components. Identification of constructions allows an MT system to select the most appropriate conventional way of expressing a meaning from among the available ways. After discussing the notion of construction, we suggest the format for a construction lexicon for a knowledge-based MT system. ...|$|R
40|$|This paper {{presents}} a Cockney rhyming slang recognizing and converting modules of a cyberbullying detection system. Firstly, we introduce {{the concept of}} rhyming slang, analyze its phrasal constructions and discuss the usefulness of features of the rhyming slang, such as resemblance to code-mixing. Secondly, we describe the corpus and <b>phrasal</b> rhyming <b>lexicon</b> created {{for the purpose of}} the research and present the results of the experiments on recognizing and transforming rhyming slang constructions into casual English sentences. Finally, we process the obtained output and verify usability of the modules in cyberbullying and crime detection system...|$|R
40|$|This {{demonstration}} showcases the STEP {{system for}} natural language access to relational databases. In STEP an administrator authors a highly structured semantic grammar through coupling phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. The resulting <b>phrasal</b> <b>lexicon</b> {{serves as a}} bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is nearing completion and will soon be field tested in several domains. 1...|$|E
40|$|This paper {{introduces}} the STEP system for natural language access to relational databases. In STEP the administrator couples phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. This <b>phrasal</b> <b>lexicon</b> {{serves as a}} bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus expressions. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is implemented and is currently being evaluated over a geography database. Additional evaluations are planned and will also be available on STEP's website ([URL]...|$|E
40|$|One key to {{the success}} of EBMT is the removal of the {{boundaries}} limiting the potential of translation memories. To bring EBMT to fruition, researchers and developers have to go beyond the self-imposed limitations of what is now traditional, in computing terms almost old fashioned, TM technology. Experiments have shown that the probability of finding exact matches at phrase level is higher than the probability of finding exact matches at the current TM segment level. We outline our implementation of a linguistically enhanced translation memory system (or <b>Phrasal</b> <b>Lexicon)</b> implementing phrasal matching. This system takes advantage of the huge and underused resources available in existing translation memories and develops a traditional TM into a sophisticated example-based machine translation engine which when integrated into a hybrid MT solution can yield significant improvements in translation quality...|$|E
40|$|Phrasal Verbs are an {{important}} feature of the English language. Properly identifying them provides the basis for an English parser to decode the related structures. Phrasal verbs have been a challenge to Natural Language Processing (NLP) because they sit at the borderline between lexicon and syntax. Traditional NLP frameworks that separate the lexicon module from the parser {{make it difficult to}} handle this problem properly. This paper presents a finite state approach that integrates a <b>phrasal</b> verb expert <b>lexicon</b> between shallow parsing and deep parsing to handle morpho-syntactic interaction. With precision/recall combined performance benchmarked consistently at 95. 8 %- 97. 5 %, the Phrasal Verb identification problem has basically been solved with the presented method...|$|R
40|$|The article investigates {{situational}} {{variability of}} means of expression of positive emotions in Modern English. This problem is topical for linguistics of emotions, pragma- and sociolinguistics. The author analyzes verbal, paraverbal and nonverbal means of expressing positive emotions {{of joy and}} surprise in three communicative registers: official, neutral and unofficial. These registers are singled out {{on the basis of}} situational context and participants of the situation. The investigation gave the author the opportunity to {{come to the conclusion that}} in the official register emotions are controlled and very often are subject to strategic purposes. They implement the upgrading strategy the aim of which is to give a high evaluation of the addressee and of everything what is happening. It stipulates the use of corresponding expressive words and word combinations, exclamatory sentences, repetitions and rhetorical questions. Paraverbal means include only light laughter and nonverbal – a smile, the expression of the eyes, nods and tangled breath. The range of the means in question is wider in the neutral register where the speaker can use such verbal means as interjections with long sounds, expletives, pauses, descriptions of own emotions. Such paraverbal means as the emotional color of the voice and its volume, intonation, different kinds of laughter and such nonverbal means as different kinds of smile, the expression of the face, gestures and movements are also used. Unofficial register is especially rich in means of expressing positive emotions. Besides means typical of other registers such verbal means as obscene <b>lexicon,</b> <b>phrasal</b> words and graphical means are used here. Paraverbal means include numerous verbs reflecting the volume of voice and nonverbal – rich mimic, friendly hugs and kisses...|$|R
40|$|We {{introduce}} the ML-Peba system, {{a system which}} generates textual descriptions of animals in both English and Spanish from an abstract knowledge base. The system relies upon a <b>phrasal</b> <b>lexicon</b> for linguistic realization. The paper discusses {{the pros and cons}} of this approach for multilingual generation, suggesting that its use enables a more language-independent knowledge representation than most generation approaches. 0 The work reported in this paper was completed while both authors were employed at the Microsoft Research Institute, Macquarie University, Sydney, Australia. It was supported with a Macquarie University Research Grant. 1 Introduction Natural language generation (NLG) systems aim to produce texts from an abstract representation of information. One fundamental premise of this work is that knowledge representation in this context is not structured according to realization in a specific language. The possibility of multilingual generation systems follows from this premise [...] ...|$|E
40|$|This paper {{presents}} a phrase-based speech translation system that combines <b>phrasal</b> <b>lexicon,</b> language, and acoustic model features in a loglinear model. Automatic speech recognition and machine translation are coupled by using large word lattices as the input for translation. For the first time, all features are directly {{integrated into the}} decoding process. The feature weights are iteratively optimized for an objective error measure. We prove that acoustic recognition scores of the recognized words in the lattices together with a source language model score positively and significantly affect the translation quality. We show the advantage of using loglinear model combination for a robust optimization of scaling factors. We report consistent improvements compared with translations of single best recognition output on an Italian-to-English translation task. First encouraging results were also obtained on a large vocabulary task of translating European parliamentary speeches. 1...|$|E
40|$|Users of the World Wide Web have {{needs and}} {{interests}} which can help to determine what of the vast quantities of information available might be relevant to them. Intelligent agents {{might be used to}} select content for a particular user. However, {{it is also important to}} consider how that content is provided to a user. We suggest that this information presentation must also take into consideration the needs of a user, and discuss a set of agents which utilizes natural language generation techniques to present information in an appropriate way. In this paper we describe two systems we have built which dynamically generate descriptions of knowledge base entities, and consider the extension of the techniques used there for multilingual information presentation. We describe the notion of a <b>phrasal</b> <b>lexicon</b> as a basis for dynamic object description, and propose a model for dynamic multilingual description which builds on that notion...|$|E
40|$|Computer {{programs}} so {{far have}} not fared well in modeling language acquisition. For one thing, learning methodology applicable in general domains does not readily lend itself in the linguistic domain. For another, linguistic representation used by language processing systems is not geared to learning. We introduced a new linguistic representation, the Dynamic Hierarchical <b>Phrasal</b> <b>Lexicon</b> (DHPL) [Zernik 88], to facilitate language ac- quisition. From this, a language learning model was implemented in the program RINA, which enhances its own lexical hierarchy by processing examples in context. We identified two tasks: First, how linguistic concepts are acquired from training exam- ples and organized in a hierarchy; this task was discussed in previous papers [Zernik 87]. Second, we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts. Thus, a program does not stall even {{in the presence of}} a lexical unknown, and a hypothesis can be produced for covering that lexical gap...|$|E
40|$|Uri Zcrnik Michael G. Dyer Artificial Intelligence l,aboratory Computer Science l) cpartmcnt 3531 Boeltcr I[all University of California Los Angeles, CalilBrnia 90024 USA 1. l Phrasal Parsing The phrasal {{approach}} to langnagc processing emphasizes {{the role of}} the lexicon as a knowledge source. Rather than maintaining a single generic lexical entry for caeh word e. g., take, tile lexicon contains many phrases, e. g., kake on, take Lo the streets, take to swimrain 9, take ove, etc. Although this,'tpproach proves effective in parsing and in generation, there are two acute problems which still require solutions. First, due to the huge size of the <b>phrasal</b> <b>lexicon,</b> especially when considering snblle meanings and idiosyncratic behavior of phrases, encoding of lexical entries cannot be done numaally. Thus, phrase acquisition mnst be employed to construct tim lexlcon. Second, when a set of phrases is morpho-synlactically eqnivalcnt, disambiguation must be perbrnmd by sero;mile means. These problems are addrcsscd in the program V, INA...|$|E
40|$|This paper {{describes}} PEBA-II, {{a working}} {{natural language generation}} system which interactively describes animals in a taxonomic knowledge base via the production of World Wide Web pages. Our aim is to construct a natural language document generation system with real practical applicability: to this end, the system reconstructs and combines a number of existing ideas in the literature in a novel way, and proposes {{a solution to the}} problem of breadth of coverage that is based on a pragmatic approach to knowledge representation and linguistic realisation. The system embodies the following features: ffl a reconstruction of some of the core ideas in schema [...] based text generation [McKeown 1985], applied to the generation of hypertext documents; ffl the principled use of a <b>phrasal</b> <b>lexicon</b> to ease surface generation, in concert with a knowledge base whose elements may correspond to pre [...] compiled collections of atomic units; ffl a user model and discourse model that permit interesting varia [...] ...|$|E
40|$|One of the {{limitations}} of translation memory systems is that the smallest translation units currently accessible are aligned sentential pairs. We propose an example-based machine translation system which uses a 2 ̆ 7 <b>phrasal</b> <b>lexicon</b> 2 ̆ 7 in addition to the aligned sentences in its database. These phrases are extracted from the Penn Treebank using the Marker Hypothesis as a constraint on segmentation. They are then translated by three on-line machine translation (MT) systems, and a number of linguistic resources are automatically constructed which are used in the translation of new input. We perform two experiments on testsets of sentences and noun phrases to demonstrate the effectiveness of our system. In so doing, we obtain insights into {{the strengths and weaknesses of}} the selected on-line MT systems. Finally, like many example-based machine translation systems, our approach also suffers from the problem of ‘boundary friction’. Where the quality of resulting translations is compromised as a result, we use a novel, post hoc validation procedure via the World Wide Web to correct imperfect translations prior to their being output to the user...|$|E
40|$|Most current {{statistical}} {{machine translation}} (SMT) systems make very little use of contextual information to select a translation candidate for a given input language phrase. However, despite evidence that rich context features are useful in stand-alone translation disambiguation tasks, recent studies reported that incorporating context-rich approaches from Word Sense Disambiguation (WSD) methods directly into classic word-based SMT systems, surprisingly, did not yield the expected improvements in translation quality. We argue here that, instead, {{it is necessary to}} design a contextdependent lexicon that is specifically matched to a given phrase-based SMT model, rather than simply incorporating an independently built and tested WSD module. In this approach, the baseline SMT <b>phrasal</b> <b>lexicon,</b> which uses translation probabilities that are independent of context, is augmented with a context-dependent score, defined using insights from standalone translation disambiguation evaluations. This approach reliably improves performance on both IWSLT and NIST Chinese-English test sets, producing consistent gains on all eight of the most commonly used automated evaluation metrics. We analyze the behavior of the model along a number of dimensons, including an analysis confirming that the most important context features are not available in conventional phrase-based SMT models. ...|$|E
40|$|Phrasal constructs, the lexicon, and {{multilingual}} generation The lexicon {{is normally}} {{conceived of as}} the repository of word-specific information. Traditional lexical resources, such as machine readable dictionaries, therefore contain lists of words. These lists might delineate senses of a word, represent {{the meaning of a}} word, or specify the syntactic frames in which a word can appear, but the level of granularity with which they are concerned is the individual word. There are many linguistic phenomena which pose a challenge to this ”word focus ” in the lexicon. The incorporation of elements at a higher level of abstraction — at the phrasal level, where particular words are grouped together into fixed phrases — provides a basis for improved computational processing of language. In this talk, I will examine the <b>phrasal</b> <b>lexicon</b> and discuss its utility in the context of multilingual text generation. It comes as no surprise to any student of language that text analysis at the level of individual words ignores many important components of language (Becker 1975, inter alia). To list but a few examples, languages contain idioms (e. g. to kick the bucket), fixed expressions (e. g. by and large, to boldly go), phrasal proper nouns (e. g. The Big Apple), collocations (e. g. eye dropper, re...|$|E
40|$|Under {{consideration}} for other conferences (specify) ? No Statistically-based phrase extractors are fundamental tools {{for the improvement}} of Natural Language Processing applications designed for the new languages of the emerging countries. In this context, we will present a new architecture called SENTA (Software for the Extraction of N-ary Textual Associations) that identifies verbal phrases from lemmatized corpora. In particular, SENTA proposes a solution to the definition of ad hoc association measure thresholds and introduces a new association measure called the Mutual Expectation. In order to evaluate SENTA, an exhaustive linguistic analysis of the results has been carried out that takes advantage of a pre-existing manually created <b>phrasal</b> <b>lexicon.</b> Automatic Extraction of Verb Phrases from Annotated Corpora: A Linguistic Evaluation for Estonian In {{order to be able to}} analyze and synthesize real sentences of a language, one has to be aware of the common expressions, which may be complicated idioms as well as simple frequent phrases. A special case of such common expressions is verb phrases i. e. phrasal verbs like to pay off and idiomatic expressions like to laugh one to pieces. In this paper, we will present the SENTA system that proposes an innovative architecture that avoids the definition of global association measure thresholds and defines a new association measure that does not over-evaluate the degree of cohesion of sequences of words containing frequent fragments. Finally, we will present a case study to demonstrate a successful way of combining linguistic and statistical processing to extract Estonian phrasal verbs from a text corpus. ...|$|E

