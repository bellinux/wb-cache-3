65|345|Public
5000|$|... #Caption: A {{conditional}} <b>probability</b> <b>tree</b> {{can be used}} {{to discern}} the probability with which C rolls higher than D.|$|E
40|$|The {{relations}} between Bell's inequality and quantum probability trees are explained {{against the background}} offered by {{the concept of a}} quantum <b>probability</b> <b>tree</b> built in others works. It is shown that f we use a concept of <b>probability</b> <b>tree</b> it will not be necessary we set aside the principle of separability and principle of locality. Comment: 8 page...|$|E
40|$|In {{this chapter}} we show how {{algebraic}} geometry {{can be used}} to define and then analyse the properties of certain important classes of discrete probability models described through probability trees. Our aim is to show how much wider classes of discrete statistical models than have been considered previously have a useful algebraic formulation and unlike its competitors this class is closed under discovery of arbitrary marginal distributions. We proceed to illustrate how the identifiability of certain causal functions can be articulated and analysed within this framework which generalises the causal Bayesian network formulation. We note that as with causal Bayesian networks the most convenient polynomial parametrisation is one that is based on conditional rather than marginal probabilities. In Section 1 we introduce the <b>probability</b> <b>tree</b> representation of a discrete model and show that discrete Bayesian networks and some of their recent generalisations are special subclasses of these models. We then introduce an algebraic representation of important classes of <b>probability</b> <b>tree</b> models called algebraic constraint models (ACTs). In Section 3 we proceed to examine how ACTs are closed under the discovery of the marginal distribution of a random variable measurable with respect to the path sigma algebra of its underlying <b>probability</b> <b>tree.</b> <b>Probability</b> <b>tree</b> representations are especially useful to specify and study the implications of certain causal hypotheses. In Sections 4 and 5 we relate these causal models to ACTs and give a formal discussion of the conditional probability graphs of discrete models that can also be expressed as Bayesian networks. In Section 6 we illustrate these ideas {{from the perspective of a}} simple modelling context. 1 The algebra of probability trees Begin by considering a finite discrete probability space whose atomic events are given by the N root to leaf paths on a <b>probability</b> <b>tree</b> T = (V (T), E(T)), CRiSM Paper No. 06 - 09, www. warwick. ac. uk/go/cris...|$|E
40|$|<b>Probability</b> <b>trees</b> (or <b>Probability</b> Estimation <b>Trees,</b> PET’s) are {{decision}} <b>trees</b> with <b>probability</b> distributions in the leaves. Usually decision {{trees are}} learned in a top-down manner with pre- or postpruning. Which pruning criterion is used strongly influences {{the size of}} the resulting tree {{and the quality of the}} probability estimates. While the effect of pruning criteria on classification accuracy is well-studied, only recently there is more interest in the effect on probability estimates or probability-based rankings. Hence, there is currently no clear view on the relative performance of all different pruning criteria for <b>probability</b> <b>trees</b> and it is unclear which criteria are preferable under which circumstances. In this paper we survey six of the most important pruning criteria for <b>probability</b> <b>trees.</b> We discuss their theoretical advantages and disadvantages and we perform an extensive experimental study of their relative performance. The main conclusion is that a pruning criterion based on randomization tests usually performs best and learns trees that are relatively small. We identify several scenarios in which other pruning criteria achieve results comparable to those of randomization tests...|$|R
40|$|We justify {{and discuss}} {{expressions}} for joint lower and upper expectations in imprecise <b>probability</b> <b>trees,</b> {{in terms of}} the sub- and supermartingales that can be associated with such <b>trees.</b> These imprecise <b>probability</b> <b>trees</b> can be seen as discrete-time stochastic processes with finite state sets and transition probabilities that are imprecise, {{in the sense that they}} are only known to belong to some convex closed set of probability measures. We derive various properties for their joint lower and upper expectations, and in particular a law of iterated expectations. We then focus on the special case of imprecise Markov chains, investigate their Markov and stationarity properties, and use these, by way of an example, to derive a system of non-linear equations for lower and upper expected transition and return times. Most importantly, we prove a game-theoretic version of the strong law of large numbers for submartingale differences in imprecise <b>probability</b> <b>trees,</b> and use this to derive point-wise ergodic theorems for imprecise Markov chains. Comment: 35 pages, 3 figure...|$|R
40|$|AbstractEffective {{handling}} of uncertainty {{is one of the}} central problems in medical decision making. The sources and effects of uncertainty in medical decision making are examined and some new quantitative approaches for solving the associated problems are outlined. To handle uncertainty in the branching probabilities and node utilities for <b>probability</b> <b>trees</b> representing alternative treatment strategies, a public domain software package {{that can be used for}} the construction, analysis and comparison of <b>probability</b> <b>trees</b> with random parameters was developed. To facilitate specification of the random variables that arise in medical decision making problems, public domain software packages for both data-driven and subjective estimation of probability densities from the Johnson translation system of distributions have also been developed. For the analysis of complex problems that cannot be adequately represented by <b>probability</b> <b>trees</b> or by simple stochastic processes such as Markov chains, network simulation approaches that are oriented toward the sequence of activities seen by individual patients in the course of treatment are described...|$|R
40|$|FIGURE 4. Bayesian {{posterior}} <b>probability</b> <b>tree</b> was reconstructed from 16 S ribosomal RNA {{mitochondrial gene}} sequences with Philautus aurifasciatus, Kurixalus eiffingeri and K. odontotarsus as outgroups. Maximum-likelihood tree produced nearidentical topology. Two reliability indices are given on nodes: the Bayesian posterior probabilities / the maximum likelihood bootstrap percentages...|$|E
40|$|High-performance {{computing}} {{systems will}} increasingly rely on prefetching data from disk to overcome long disk access times and maintain high utilization of parallel I/O systems. This paper evaluates a prefetching technique that chooses which blocks to prefetch {{based on their}} probability of access and decides whether to prefetch a particular block {{at a given time}} using a cost-benefit analysis. The algorithm uses a <b>probability</b> <b>tree</b> to record past accesses and to predict future access patterns. We simulate this prefetching algorithm with a variety of I/O traces. We show that our predictive prefetching scheme combined with simple oneblock -lookahead prefetching produces good performance for a variety of workloads. The scheme reduces file cache miss rates by up to 36 % for workloads that receive no benefit from sequential prefetching. We show that the memory requirements for building the <b>probability</b> <b>tree</b> are reasonable, requiring about a megabyte for good performance. The probabilit [...] ...|$|E
40|$|The {{unreliability}} {{of public}} power lines {{have led to}} the need of uninterruptible power supply (UPS). Utility power failures will cause unacceptably high risk to the profitability, existence and growth of the vital aspect of business that depends heavily on uninterrupted power supply. For this reason it is important to develop a method to estimate the reliability of such system, to ensure that it will perform satisfactorily when needed. This paper describes and discusses an approach to predict the reliability parameters of the UPS system using the <b>probability</b> <b>tree</b> method. Important UPS reliability parameters such as failure rates (lambda), mean time between failures (MTBF), and reliability (R), can be obtained from this method. These quantitative reliability parameters can play an essential role in selection and application of the UPS. The method was applied to different topologies of UPS systems and comparisons were made between the results obtained form <b>probability</b> <b>tree</b> method and the reliability block diagram (RBD) method...|$|E
40|$|<b>Probability</b> <b>trees</b> (or <b>Probability</b> Estimation <b>Trees,</b> PET’s) are {{decision}} <b>trees</b> with <b>probability</b> distributions in the leaves. Several {{approaches for}} learning <b>probability</b> <b>trees</b> {{have been proposed}} in the literature. Currently no thorough comparison of these alternative approaches exists. In this paper we experimentally compare the main approaches using the relational decision tree learner Tilde (both on non-relational and on relational datasets). Next to the main existing approaches, we also consider a novel variant of an existing approach based on the Bayesian Information Criterion (BIC). Our main conclusion is that trees built using the C 4. 5 -approach or the C 4. 4 -approach (C 4. 5 without post-pruning) typically have the best predictive performance. If the number of classes is low, however, BIC is equally good. An additional advantage of BIC is that its trees are considerably smaller than trees for the C 4. 5 - or C 4. 4 -approach...|$|R
40|$|AbstractThis paper {{presents}} an approximate algorithm {{to obtain a}} posteriori intervals of probability, when available information is also given with intervals. The algorithm uses <b>probability</b> <b>trees</b> {{as a means of}} representing and computing with the convex sets of probabilities associated to the intervals...|$|R
40|$|AbstractWe give an {{overview}} of two approaches to probability theory where lower and upper probabilities, rather than probabilities, are used: Walley's behavioural theory of imprecise probabilities, and Shafer and Vovk's game-theoretic account of probability. We show that the two theories are more closely related than would be suspected at first sight, and we establish a correspondence between them that (i) has an interesting interpretation, and (ii) allows us to freely import results from one theory into the other. Our approach leads to an account of <b>probability</b> <b>trees</b> and random processes {{in the framework of}} Walley's theory. We indicate how our results can be used to reduce the computational complexity of dealing with imprecision in <b>probability</b> <b>trees,</b> and we prove an interesting and quite general version of the weak law of large numbers...|$|R
40|$|International audienceThe {{reliability}} of a fixed wireless backhaul network is {{the probability that}} the network can meet all the communication requirements considering the uncertainty (e. g., due to weather) in the maximum capacity of each link. We provide an algorithm to compute the exact {{reliability of}} a backhaul network, given a discrete probability distribution on the possible capacities available at each link. The algorithm computes a conditional <b>probability</b> <b>tree,</b> where at each leaf in the tree a valid routing for the network is evaluated. Any such tree provides bounds on the reliability, and the algorithm improves these bounds by branching in the tree. We also consider the problem of determining the topology and configuration of a backhaul network that maximizes reliability subject to a limited budget. We provide an algorithm that exploits properties of the conditional <b>probability</b> <b>tree</b> used to calculate reliability of a given network design, and we evaluate its computational efficiency...|$|E
40|$|A {{method and}} a {{computer}} program are presented to calculate probability of system success from an arbitrary reliability block diagram. The class of reliability block diagrams that can be handled include any active/standby combination of redundancy, and the computations include the effects of dormancy and switching in any standby redundancy. The mechanics of the program are based on {{an extension of the}} <b>probability</b> <b>tree</b> method of computing system probabilities...|$|E
40|$|The {{reliability}} of a fixed wireless backhaul network is {{the probability that}} the network can meet all the communication requirements considering the uncertainty (e. g., due to weather) in the maximum capacity of each link. We provide an algorithm to compute the exact {{reliability of}} a backhaul network, given a discrete probability distribution on the possible capacities available at each link. The algorithm computes a conditional <b>probability</b> <b>tree,</b> where each leaf in the tree requires a valid routing for the network. Any such tree provides an upper and lower bound on the reliability, and the algorithm improves these bounds by branching in the tree. We also consider the problem of determining the topology and configuration of a backhaul network that maximizes reliability subject to a limited budget. We provide an algorithm that exploits properties of the conditional <b>probability</b> <b>tree</b> used to calculate reliability of a given network design. We perform a computational study demonstrating that the proposed methods can calculate reliability of large backhaul networks, and can optimize topology for modest size networks...|$|E
40|$|FIGURE 1 – 2. Distribution map and phylogeny: 1, Map {{with the}} type {{locality}} of Compositermes bani sp. n. and the localities of the samples of C. vindai used in the phylogeny; 2, Bayesian Phylogeny of Compositermes with the mitochondrial gene COI showing posterior <b>probabilities.</b> <b>Tree</b> rooted on terminal Heterotermes crinitus...|$|R
40|$|Abstract. An {{association}} classification algorithm {{has been}} developed to explore adverse drug reactions in a large medical transaction dataset with unbalanced classes. Rules discovered {{can be used to}} alert medical practitioners when prescribing drugs, to certain categories of patients, to potential adverse effects. We assess the rules using survival charts and propose two kinds of <b>probability</b> <b>trees</b> to present them. Both of them represent the risk of given adverse drug reaction for certain categories of patients in terms of risk ratios, which are familiar to medical practitioners. The first approach shows risk ratios when all rule conditions apply. The second presents the risk associated with a single risk factor with other parts of the rule identifying the cohort of the patient subpopulation. Thus, the <b>probability</b> <b>trees</b> can present clearly the risk of specific adverse drug reactions to prescribers. ...|$|R
40|$|Several {{approaches}} for learning (propositional or relational) <b>probability</b> <b>trees</b> {{have been proposed}} in the literature. Little {{is known about the}} relative (dis) advantages of the different approaches. In this work we investigate these different approaches in a relational context by performing a set of experiments with the first-order logical decision tree learner Tilde. status: publishe...|$|R
30|$|While the <b>probability</b> <b>tree</b> {{represents}} the agent’s subjective model explaining {{the order in}} which the random values are resolved, it does not necessarily correspond to the temporal order in which the events are revealed to us. So for instance, under hypothesis Θ=θ, the value of the variable Y might be revealed before X, even though X causally precedes Y; and the causal hypothesis Θ, which precedes both X and Y, is never observed.|$|E
40|$|Probability {{trees are}} a {{powerful}} data structure for representing probabilistic potentials. However, their complexity can become intractable if {{they represent a}} probability distribution over a large set of variables. In this paper, we study the problem of decomposing a <b>probability</b> <b>tree</b> {{as a product of}} smaller trees, with the aim of being able to handle bigger probabilistic potentials. We propose exact and approximate approaches and evaluate their behaviour through an extensive set of experiments...|$|E
40|$|A {{recursive}} <b>probability</b> <b>tree</b> (RPT) is an incipient {{data structure}} for representing the distributions in a probabilistic graphical model. RPTs capture {{most of the}} types of independencies found in a probability distribution. The explicit representation of these features using RPTs simplifies computations during inference. This paper describes a learning algorithm that builds a RPT from a probability distribution. Experiments prove that this algorithm generates a good approximation of the original distribution, thus making available all the advantages provided by RPT...|$|E
40|$|Classification {{trees are}} widely used in the machine {{learning}} and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to <b>probability</b> estimation <b>trees.</b> Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational <b>probability</b> <b>trees</b> (RPTs) extend standard <b>probability</b> estimation <b>trees</b> to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e. g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has {{identified a number of}} statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance. 1...|$|R
40|$|Temporal {{difference}} networks (or TD-Nets) offer {{a framework}} for predictive state representations. TD-Nets break up into two parts: the question network and the answer network. The question network defines which questions about future observations are of importance, while the answer network provides a way to update the {{answers to those questions}} as the environment changes. Currently, TD-Nets use logistic regression functions to represent the answer networks. We propose the use of <b>probability</b> <b>trees</b> in their stead. Trees offer a different but powerful way of generalisation and using them may be beneficial in a number of applications. Moreover, we believe this aids in {{a better understanding of the}} strengths and weaknesses of TD-Nets and represents an important first step towards the application of temporal difference networks in environments with more extensive, i. e. complex and numerous, observations than those currently employed. We compare the learning behavior of TD-Nets using logistic regression and <b>probability</b> <b>trees</b> using an array of experiments in two simple grid worlds and a ring world...|$|R
50|$|Grégory Miermont (born on 16 July 1979) is a French {{mathematician}} {{working on}} <b>probability,</b> random <b>trees</b> and random maps.|$|R
40|$|FIGURE 4. Bayesian {{posterior}} <b>probability</b> <b>tree</b> was reconstructed from 16 S ribosomal RNA {{mitochondrial gene}} sequences with Metaphrynella pollicaris, Metaphrynella sundana and Phrynella pulchra as outgroups. Maximum-likelihood tree produced near-identical topology. Two reliability indices are given on nodes: the Bayesian posterior probabilities / the maximum likelihood bootstrap percentages. Symbol (*) indicates nodes with good bootstrap supports for ML (> 80 %) inferences and Bayesian posterior probabilities (BPP> 95 %), and symbol (-) represents that node values {{are less than}} 60 %...|$|E
40|$|AbstractA Recursive <b>Probability</b> <b>Tree</b> (RPT) is a data {{structure}} for representing the potentials involved in Probabilistic Graphical Models (PGMs). This structure is developed {{with the aim}} of capturing some types of independencies that cannot be represented with previous structures. This capability leads to improvements in memory space and computation time during inference. This paper describes a learning algorithm for building RPTs from probability distributions. The experimental analysis shows the proper behavior of the algorithm: it produces RPTs encoding good approximations of the original probability distributions...|$|E
40|$|We {{present a}} general {{framework}} for defining priors on model structure and sampling from the posterior using the Metropolis-Hastings algorithm. The key {{idea is that}} structure priors are defined via a <b>probability</b> <b>tree</b> and that the proposal mechanism for the Metropolis-Hastings algorithm operates by traversing this tree, thereby defining a cheaply computable acceptance probability. We have applied this approach to Bayesian net structure learning using a number of priors and tree traversal strategies. Our results show that these must be chosen appropriately for this approach to be successful. Comment: Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI 2001...|$|E
50|$|Russell Lyons with Yuval Peres (2012), <b>Probability</b> on <b>Trees</b> and Networks, Cambridge University Press, in preparation. Current version at http://mypage.iu.edu/~rdlyons/prbtree/prbtree.html.|$|R
40|$|Causality in the {{abstract}} is a grand theme. We take it up when we want to penetrate {{to the bottom of}} things—to understand general laws that govern the working at the world of the deepest and most detailed level. In this essay, I argue for a more situated understanding of causality. To counter our desire for ever greater generality, I suggest that causal relations, even those that hold only on average, require context. To counter our desire for ever greater detail, I suggest that causal relations may exist only at a certain level of granularity. The case for the situatedness of causality is based on the epistemic character of causality. A causal structure is a structure for predictions that might be made by an ideally knowledgeable and observant scientist. It tells us about the unfolding of that scientist’s knowledge. It has objective aspects, because knowledge must have objects. But it is also subjective aspects, because knowledge must also have a subject, and possibilities and probabilities that are meaningless without reference to that subject. The epistemic character and situatedness of causality tell us something about what kinds of objects should be called causes. The objects available to statisticians—variables and Moivrean events—are not causes because they are not situated. But they can be used to detect and aggregate causes. The arguments advanced here build on my forthcoming book, The Art of Causal Conjecture (Shafer 1996), which develops a detailed account of causal relations in <b>probability</b> <b>trees.</b> 2 1 Using <b>Probability</b> <b>Trees</b> to Talk About Causality The <b>probability</b> <b>trees</b> in Figures 1 and 2 can be understood as situated causal structures. They will serve to anchor our discussion of the situatedness of causality...|$|R
40|$|Abstract—We {{introduce}} and validate Spatiotemporal Relational Random Forests, {{which are}} random forests created with spatiotemporal relational <b>probability</b> <b>trees.</b> We {{build on the}} documented success of random forests by bringing spatiotemporal capabilities to the trees, enabling them to identify critical spatial, temporal, and spatiotemporal features in the data. We validate our results on simulated data and realworld convectively-induced turbulence data from a commercial airline flying in the continental United States. Keywords-Spatiotemporal data mining, Relational learning, Random forests, Turbulence I...|$|R
40|$|International audienceTeaching {{abstract}} concepts {{is notoriously}} difficult, {{especially when we}} lack concrete metaphors that map to those abstractions. Combinatorix offers a novel approach that combines tangible objects with an interactive tabletop to help students explore, solve and understand probability problems. Students rearrange physical tokens to see the effects of various constraints on the problem space; a second screen displays the associated changes in an abstract representation, e. g., a <b>probability</b> <b>tree.</b> Using participatory design, college students in a combinatorics class helped iteratively refine the Combinatorix prototype, which was then tested successfully with five students. Combinatorix serves as an initial proof-of-concept that demonstrates how tangible tabletop interfaces that map tangible objects to abstract concepts can improve problem-solving skills...|$|E
40|$|In {{this thesis}} {{we present a}} Lüroth series A that we {{conjecture}} to be normal. The series A is constructed by concatenating elements from a <b>probability</b> <b>tree</b> that enumerates all finite Lüroth series. We give an outline for a proof of the normality of A in four steps, of which we will prove three. Regarding the fourth step, which we cannot prove yet, we will state some remarks on its plausibility and a possible strategy for a proof. To conclude we describe the possibilities of extending the construction presented to other Generalized Lüroth Series and compare the mentioned construction to constructions for normal numbers in other expansions. Applied MathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|A cis-regulatory module (CRM) is a DNA {{region of}} a few hundred base pairs that {{consists}} of clustering of several transcription factor binding sites and regulates the expression of a nearby gene. This thesis presents a new computational approach to CRM detection. It is believed that tissue-specific CRMs tend to regulate nearby genes in a certain tissue and that they consist of binding sites for transcription factors (TFs) that are also expressed in that tissue. These facts allow us to make use of tissue-specific gene expression data to detect tissue-specific CRMs and improve the specificity of module prediction. We build a Bayesian network to integrate the sequence information about TF binding sites and the expression information about TFs and regulated genes. The network is then used to infer whether a given genomic region indeed has regulatory activity in a given tissue. A novel EM algorithm incorporating <b>probability</b> <b>tree</b> learning is proposed to train the Bayesian network in an unsupervised way. A new <b>probability</b> <b>tree</b> learning algorithm is developed to learn the conditional probability distribution for a variable in the network that has a large number of hidden variables as its parents. Our approach is evaluated using biological data, and the results show that it is able to correctly discriminate among human liver-specific modules, erythroid-specific modules, and negative-control regions, even though no prior knowledge about the TFs and the target genes is employed in our algorithm. In a genome-wide scale, our network is trained to identify tissue-specific CRMs in ten tissues. Some known tissue-specific modules are rediscovered, and a set of novel modules are predicted to be related with tissue-specific expression...|$|E
40|$|This paper {{presents}} non-random algorithms for approximate computation in Bayesian networks. They {{are based}} on the use of <b>probability</b> <b>trees</b> to represent <b>probability</b> potentials, using the Kullback-Leibler cross entropy as a measure of the error of the approximation. Different alternatives are presented and tested in several experiments with difficult propagation problems. The results show how it is possible to find good approximations in short time compared with Hugin algorithm. � 2000 John Wiley & Sons, Inc. 1...|$|R
40|$|We {{present an}} {{efficient}} procedure for factorising probabilistic potentials represented as <b>probability</b> <b>trees.</b> This new procedure {{is able to}} detect some regularities that cannot be captured by existing methods. In cases where an exact decomposition is not achievable, we propose a heuristic way to carry out approximate factorisations guided by a parameter called factorisation degree, which is fast to compute. We show how this parameter {{can be used to}} control the tradeoff between complexity and accuracy in approximate inference algorithms for Bayesian networks...|$|R
40|$|Abstract. We analyze a Relational Neighbor (RN) classifier, {{a simple}} {{relational}} predictive model that predicts only based on class labels of related neighbors, using no learning and no inherent attributes. We {{show that it}} performs surprisingly well by comparing it to more complex models such as Probabilistic Relational Models and Relational <b>Probability</b> <b>Trees</b> on three data sets from published work. We argue that a simple model such as this should {{be used as a}} baseline to assess the performance of relational learners. ...|$|R
