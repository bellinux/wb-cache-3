2|18|Public
40|$|The {{perceived}} spatial {{spread of}} amplitude panned virtual sources {{is dependent on}} the number of loudspeakers that are used to produce them. When pair-wise or triplet-wise panning is applied, the number of active loudspeakers varies {{as a function of the}} <b>panning</b> <b>direction.</b> This may cause unwanted changes in spatial spread and coloration of a virtual source if it is moved in the sound stage. In this paper a method is presented to make the directional spread of amplitude panned virtual sources independent of their <b>panning</b> <b>direction.</b> This is accomplished by panning the sound signal to multiple directions near each other simultaneously. This forms a single virtual source with constant directional spread as a function of direction. 1...|$|E
40|$|Spatial audio aims to {{recreate}} or synthesize spatial attributes when reproducing audio over loudspeakers or headphones. Such spatial attributes include, for example, locations of perceived sound sources and an auditory sense of space. This thesis focuses on {{new methods of}} spatial audio for loudspeaker listening and on measuring the quality of spatial audio by subjective and objective tests. In this thesis the vector base amplitude panning (VBAP) method, which is an amplitude panning method to position virtual sources in arbitrary 2 -D or 3 -D loudspeaker setups, is introduced. In amplitude panning the same sound signal is applied {{to a number of}} loudspeakers with appropriate non-zero amplitudes. With 2 -D setups VBAP is a reformulation of the existing pair-wise panning method. However, differing from earlier solutions it can be generalized for 3 -D loudspeaker setups as a triplet-wise panning method. A sound signal is then applied to one, two, or three loudspeakers simultaneously. VBAP has certain advantages compared to earlier virtual source positioning methods in arbitrary layouts. Previous methods either used all loudspeakers to produce virtual sources, which results in some artefacts, or they used loudspeaker triplets with a non-generalizable 2 -D user interface. The virtual sources generated with VBAP are investigated. The human directional hearing is simulated with a binaural auditory model adapted from the literature. The interaural time difference (ITD) cue and the interaural level difference (ILD) cue which are the main localization cues are simulated for amplitude-panned virtual sources and for real sources. Psychoacoustic listening tests are conducted to study the subjective quality of virtual sources. Statistically significant phenomena found in listening test data are explained by auditory model simulation results. To obtain a generic view of directional quality in arbitrary loudspeaker setups, directional cues are simulated for virtual sources with loudspeaker pairs and triplets in various setups. The directional qualities of virtual sources generated with VBAP can be stated as follows. Directional coordinates used for this purpose are the angle between a position vector and the median plane (θcc), and the angle between a projection of a position vector to the median plane and frontal direction (Φcc). The perceived θcc direction of a virtual source coincides well with the VBAP <b>panning</b> <b>direction</b> when a loudspeaker set is near the median plane. When the loudspeaker set is moved towards a side of a listener, the perceived θcc direction is biased towards the median plane. The perceived Φcc direction of an amplitude-panned virtual source is individual and cannot be predicted with any panning law. reviewe...|$|E
50|$|Since the SC-55 has no {{programmable}} memory, CM-32P and MT-32 emulation is done {{by providing}} the same sound arrangement as the preset sounds of actual devices. These variation banks are enabled by playing back special SysEx containing MIDI files, for example GS32.MID (included in the SCC-1 Utility Software), prior to loading a software title. These specially arranged tone tables contain the relevant GS sound mapped at either CM-32P or MT-32 program number. Pitch bend range is changed to 12 semitone from GS default 2 semitones. Master tuning and modulation depth are not altered by the emulation. <b>Pan</b> <b>directions</b> are reversed from actual CM-32P or MT-32 devices. CM-32P or MT-32 specific MIDI SysEx messages are also ignored by the SC-55.|$|R
5000|$|... #Caption: Markham <b>Pan</b> Am Centre <b>direction</b> sign at Unionville GO Station for the 2015 Pan Am Games.|$|R
40|$|The {{long range}} {{position}} and orientation tracking system (LRPOTS) {{will consist of}} two measurement pods, a VME-based computer system, and a detector array. The system is {{used to measure the}} position and orientation of a target that may be attached to a robotic arm, teleoperated manipulator, or autonomous vehicle. The pods have been designed to be mounted in the man-ways of the domes of the Fernald K- 65 waste silos. Each pod has two laser scanner subsystems as well as lights and camera systems. One of the laser scanners will be oriented to scan in the <b>pan</b> <b>direction,</b> the other in the tilt direction. As the lasers scan across the detector array, the angles of incidence with each detector are recorded. Combining measurements from each of the four lasers yields sufficient data for a closed-form solution of the transform describing the location and orientation of the Content Mobilization System (CMS). Redundant detectors will be placed on the CMS to accommodate occlusions, to provide improved measurement accuracy, and to determine the CMS orientation...|$|R
5000|$|In a mixed review, Inkoo Kang of TheWrap praised 13 Hours for its action scenes, but <b>panned</b> Bay's <b>direction</b> as [...] "myopic". She writes, [...] "13 Hours is {{the rare}} Michael Bay movie that wasn't made with teenage boys in mind. But that doesn't make his latest any less callously juvenile." [...] Lindsey Bahr of the Associated Press was {{critical}} of the film's direction and cinematography, and found the screenplay to be confusing. Similarly, The Economist described the film as [...] "a sleek, poorly scripted and largely meaningless film".|$|R
5000|$|Released on 8 November 1996, {{the film}} {{garnered}} $120,932 in box-office receipts during its opening weekend and grossed {{a total of}} $220,198 during its two-week run. Film critic Emanuel Levy gave the film a score of 2 out of 5. Joe Leydon in Variety described Santa with Muscles as a [...] "weakling of a comedy" [...] and thought that Hogan's performance was lacking the charisma of his previous work such as Suburban Commando. Leydon <b>panned</b> the <b>direction</b> in particular, stating: [...] "Working from an irredeemably bland screenplay, John Murlowski directs with all the enthusiasm of someone {{going through the motions}} to pay off a debt." [...] Chris Hicks, writing for the Deseret News, stated that films such as Santa with Muscles make films like Jingle All The Way look better, and said that Hulk Hogan [...] "makes Arnold Schwarzenegger seem like Laurence Olivier".|$|R
5000|$|After carving {{a highly}} {{successful}} career throughout the 1970s as a rock singer, Stewart elected to follow the disco trend that was at its peak in 1978 for some tracks of this album. The first single was [...] "Da Ya Think I'm Sexy" [...] which became a number one hit in the UK, US, Australia {{and a number of}} other countries. Many critics <b>panned</b> the <b>direction</b> of song towards disco, but it nevertheless became one of his biggest hits. Stewart has since defended the song commenting that Paul McCartney and The Rolling Stones had also dabbled with disco music by this time. The second single was [...] "Ain't Love a Bitch", which became a No.11 hit in the UK and No.22 in the US. The third and final single [...] "Blondes (Have More Fun)" [...] peaked at 63 in the UK, his lowest-charting single there at this time, but performed better in Ireland at No.23.|$|R
50|$|Grayson teamed {{again with}} Keel in the 1952 Technicolor musical Lovely to Look At, {{a remake of}} the 1935 Astaire and Rogers film Roberta. She was {{released}} to the Warner Brothers studio in January 1953, with the stipulation that she return to MGM for one more film. She returned to co-star {{for a third time}} with Howard Keel in her most acclaimed role, as Lilli Vanessi/Katharina in Kiss Me Kate, released in November 1953. The film was lavishly produced (the only musical other than Those Redheads from Seattle (1953) to be filmed in 3-D), with songs by Cole Porter, choreography by Hermes <b>Pan,</b> and musical <b>direction</b> by André Previn. Grayson's double role allowed her to display many moods, from feisty to gentle to humorous to forgiving.|$|R
40|$|A {{prototype}} twin-camera {{stereo vision}} system for autonomous robots {{has been developed}} at Goddard Space Flight Center. Standard charge coupled device (CCD) imagers are interfaced with commercial frame buffers and direct memory access to a computer. The overlapping portions of the images are analyzed using photogrammetric techniques to obtain information about the position and orientation of objects in the scene. The camera head consists of two 510 x 492 x 8 -bit CCD cameras mounted on individually adjustable mounts. The 16 mm efl lenses are designed for minimum geometric distortion. The cameras can be rotated in the pitch, roll, and yaw (<b>pan</b> angle) <b>directions</b> {{with respect to their}} optical axes. Calibration routines have been developed which automatically determine the lens focal lengths and pan angle between the two cameras. The calibration utilizes observations of a calibration structure with known geometry. Test results show the precision attainable is plus or minus 0. 8 mm in range at 2 m distance using a camera separation of 171 mm. To demonstrate a task needed on Space Station Freedom, a target structure with a movable I beam was built. The camera head can autonomously direct actuators to dock the I-beam to another one so that they could be bolted together...|$|R
40|$|A {{mathematical}} model {{is presented to}} objectively derive sound localisation performance using HRIR (Head Related Impulse Response) based binaural sound reproduction systems. Rendering a sound source via panning methods causes artefacts {{that will lead to}} errors in localisation by human subjects. A localisation function and a localisation blur will be derived by comparing reference HRIRs with the distorted HRIRs, assuming that the cues specified by the reference HRIRs result in optimal localisations. Psychophysical effects will be incorporated as well. Studying the relationship between <b>panning</b> and perceived <b>directions</b> using listening tests entails an enormous effort of time. In addition, the presented {{mathematical model}} can be used to minimise the number of parameters which need to be evaluated by listening tests. Furthermore the localisation performance of several HRIR-based panning methods will also be evaluated. 1...|$|R
40|$|This paper {{deals with}} the problem of {{correcting}} the perspective distortion in panoramic images created by parallel motion stitching. The distortion is revealed by lines that appear to converge at the infinity, but are actually parallel. A camera cart shoots from multi-viewpoints aiming a parallel motion to the scene that is photographed. The perspective effect arises on panoramas while stitching several images taken from the camera, slightly <b>panning</b> in both <b>directions</b> between shots along the motion path. In this paper, we propose a solution to handle different camera translation motions and be able to stitch together images with a high-level of similarity, also having repetition patterns along a vast continuity of elements belonging to the scene. The experimental tests were performed with real data obtained from supermarket shelves, with the goal of maintaining the correct amount of product items on the resulting panorama. After applying the perspective correction in th e input images, to reduce cumulative registration errors during stitching, it is possible to extract more information about the similarity between consecutive images so that matching mistakes are minimized...|$|R
30|$|In this paper, {{a concept}} of frame-by-frame {{intermittent}} tracking to achieve motion-blur-free and high-brightness images when video shooting fast moving scenes is proposed. In our tracking concept, two control methods are applied alternately at hundreds of hertz, according to the open or closed shutter state of the camera. When the shutter is open, the target’s speed in images is controlled at zero and visual feedback is transmitted to achieve motion blur reduction, and when the shutter is closed, the camera returns to its home position. We developed a prototype of our motion-blur-free video shooting system, which consists of our tracking method implemented on a high-speed two degrees-of-freedom tracking vision platform that controls the <b>pan</b> and tilt <b>directions</b> of the camera view by using high-speed video processing {{in order to reduce}} motion blur. Our motion-blur-free video shooting system can capture gray-level 512 × 512 images at 125  fps with frame-by-frame intermittent tracking. Its performance is verified by the experimental results for several video sequences of fast moving objects. In the experiments, without a decrease in the exposure times our method reduced image degradation caused by motion blur.|$|R
40|$|In this paper, we {{consider}} the problem of estimating the state of camera from the real-time traffic images obtained with the fixed camera with pan, tilt and zoom functions. This problem comes from the research of image processing of the road image for traffic state estimation. Since this camera can be controlled from the authorities of political surveillance, the camera is subject to move suddenly. The state of the camera means the camera direction {{and the amount of}} zooming. This processing needs real-time processing from the nature of the problem. If we apply the simple 2 -D matching method, it requires a large processing time. Hence, in this paper, we will use two 1 -D vectors for matching using two low-dimensional vectors of the vertical and holizontal average values of the pixels of certain area in the image. The feature spaces are defined from the primal eigenvectors of sample images, and the distance between the current image and the feature space is used for matching. Moreover, when we estimate the zoom magnification, eigenimage is derived from the non-zoomed images after positional matching. Similar methods are applied both for the calibration of camera <b>direction</b> (<b>pan</b> and tilt) and zooming...|$|R
6000|$|After dinner I {{watch him}} as he washes dishes: he hangs up a whole row of tin; the ship gives a lurch, and knocks them all down. He {{looks as if it}} was just what he expected. [...] "Such is life!" [...] he says, as he pursues a frisky tin <b>pan</b> in one <b>direction,</b> and arrests the gambols of the ladle in another; while the wicked sea, meanwhile, with another lurch, is {{upsetting}} all his dishwater. I can see how these daily trials, this performing of most delicate and complicated gastronomic operations in the midst of such unsteady, unsettled circumstances, have gradually given this poor soul a despair of living, and brought him into this state of philosophic melancholy. Just as Xantippe made a sage of Socrates, this whisky, frisky, stormy ship life has made a sage of our cook. Meanwhile, not to do him injustice, let it be recorded, that in all dishes which require grave conviction and steady perseverance, rather than hope and inspiration, he is eminently successful. Our table excels in viands of a reflective and solemn character; mighty rounds of beef, vast saddles of mutton, and the whole tribe of meats in general, come on in a superior style. English plum pudding, a weighty and serious performance, is exhibited in first-rate order. The jellies want lightness,--but that is to be expected.|$|R
30|$|Two piezo tilt stages {{were used}} for a mirror-drive 2 -DOF active vision system to realize frame-by-frame {{intermittent}} tracking in <b>pan</b> and tilt <b>directions.</b> The piezo tilt stage can shift its surface in the rotation direction with a 2.78 × 10 ^- 6 ^∘ resolution, and its size, weight, resonant frequency, and the range within which it can move are 36 × 42 × 29  mm, 100  g, 3900  Hz, and 0.173 ^∘, respectively, when no objects are mounted on it. On {{the surface of the}} piezo stage, a 30 × 30 × 5 mm-size aluminum mirror (TFA- 30 S 05 - 1, Sigma Koki Co., Japan) weighing 20  g was mounted. The piezo stage for the pan angle was installed 25  mm in front of the CCTV-zoom lens, and that for the tilt angle was installed 75  mm in front of that for the pan angle; the light from the target object passes to the tilt-mirror stage and the pan-mirror stage, and then is captured on the image sensor on the camera head. The drive voltage for the piezo stages, supplied by a high-capacity piezo driver (PH 601, Nano Control Co., Japan), was 0 – 150  V, and the motor commands from the PC were amplified in the piezo driver in order to operate the piezo stages periodically.|$|R
40|$|A {{mechatronic}} {{system is}} designed, constructed, and tested to aid filmmakers {{in the movement}} and control of a video camera. The system design allows for 6 -DOF camera movement (movement in all three spatial <b>directions,</b> <b>pan,</b> tilt, and roll). The system is controlled by a human operator, using an implementation of a gamepad controller, and the system is battery-powered; the theoretical range {{of the system is}} therefore limited only by the onboard battery power, and the operator’s ability to keep within cord-length of the system as it moves. A misallocation of time resources resulted in an incomplete physical design, but preliminary testing indicates that the design is sound, and that mechanical specifications are sufficiently robust for a working final system. Further time and resources would be used to complete physical construction and electronic implementation, and to implement a feedback system to allow for closed-loop actuator control and the function of repeatable motion. Executive Summary The system should be able to move a camera through the same paths of motion that a human camera-man can move it through. Project constraints include a $ 2700 budget, a vertical camera elevation range of 2 - 6 ft and compatability with the filming environment. Design criteria include ease of set up, performance in a flat, closed room, silent system operation, ease of mode...|$|R
40|$|The Interactive 3 D Mars Visualization system {{provides}} high-performance, immersive visualization of satellite and surface vehicle imagery of Mars. The software {{can be used}} in mission operations to provide the most accurate position information for the Mars rovers to date. When integrated into the mission data pipeline, this system allows mission planners to view the location of the rover on Mars to 0. 01 -meter accuracy with respect to satellite imagery, with dynamic updates to incorporate the latest position information. Given this information so early in the planning process, rover drivers are able to plan more accurate drive activities for the rover than ever before, increasing the execution of science activities significantly. Scientifically, this 3 D mapping information puts all of the science analyses to date into geologic context on a daily basis instead of weeks or months, as was the norm prior to this contribution. This allows the science planners to judge the efficacy of their previously executed science observations much more efficiently, and achieve greater science return as a result. The Interactive 3 D Mars surface view is a Mars terrain browsing software interface that encompasses the entire region of exploration for a Mars surface exploration mission. The view is interactive, allowing the user to <b>pan</b> in any <b>direction</b> by clicking and dragging, or to zoom in or out by scrolling the mouse or touchpad. This set currently includes tools for selecting a point of interest, and a ruler tool for displaying the distance between and positions of two points of interest. The mapping information can be harvested and shared through ubiquitous online mapping tools like Google Mars, NASA WorldWind, and Worldwide Telescope...|$|R
30|$|In this study, we {{developed}} a motion-blur-free video shooting system based on a concept of frame-by-frame intermittent tracking, in which {{the control of the}} camera shutter state is alternated at a rate of hundreds of fps. The target’s speed in images is controlled at zero during exposure; otherwise, the camera’s position returns to its home position. Our system can capture 512 × 512 images of fast moving objects at 125  fps with an exposure time of 4  ms without motion blur being incurred by controlling the <b>pan</b> and tilt <b>directions</b> of a mirror-drive 2 -DOF active vision system using high-speed video processing. The system’s performance was verified by conducting several experiments using fast moving objects. We focused on motion blur reduction for moving objects in uniform backgrounds in this study, whereas both moving objects and static backgrounds can be clearly observed without blurring when the mirror speed is alternatively switched from the target object’s speed to zero in frame-by-frame intermittent tracking so that image capturing “with tracking” (IT) and “without tracking” (NT) can be simultaneously conducted. Currently, the limited responses of the piezo actuators become the major bottleneck in frame-by-frame intermittent tracking at a higher frame rate; the duration time for back-to-home-control is 2  ms or more on our system, whereas the duration time for vision-based tracking control was set to approximately 4  ms, as shown in the angular displacements in Fig.  7. On the basis of these results, we plan to improve our motion-blur-free video shooting system by adapting it for video shooting of fast moving objects in complex scenes with improved accuracy using fast general-purpose motion detection algorithms and faster frame-by-frame intermittent tracking using a free-vibration-type actuator such as a resonant mirror vibrating at hundreds or thousands of hertz, to apply our motion-blur-free video shooting system to highly magnified observations of fast moving scenes in various applications, such as the precise inspection of products moving fast on a conveyor line and tunnel and road inspection from a car moving at a high speed.|$|R
40|$|Mobile {{phones are}} {{increasingly}} being equipped {{with a wide range}} of sensors which enable a variety of interaction techniques. Sensor-based interaction techniques are particularly promising for domains such as map-based applications, where the user is required to interact with a large information space on the small screen of a mobile phone. Traditional interaction techniques have several shortcomings for interacting with mobile map-based applications. Keypad interaction offers limited control over <b>panning</b> speed and <b>direction.</b> Touch-screen interaction is often a two-handed form of interaction and results in the display being occluded during interaction. Sensor-based interaction provides the potential to address many of these shortcomings, but currently suffers from several limitations. The aim of this research was to propose enhancements to address the shortcomings of sensor-based interaction, with a particular focus on tilt interaction. A comparative study between tilt and keypad interaction was conducted using a prototype mobile map-based application. This user study was conducted in order to identify shortcomings and opportunities for improving tilt interaction techniques in this domain. Several shortcomings, including controllability, mental demand and practicality concerns were highlighted. Several enhanced tilt interaction techniques were proposed to address these shortcomings. These techniques were the use of visual and vibrotactile feedback, attractors, gesture zooming, sensitivity adaptation and dwell-time selection. The results of a comparative user study showed that the proposed techniques achieved several improvements in terms of the problem areas identified earlier. The use of sensor fusion for tilt interaction was compared to an accelerometer-only approach which has been widely applied in existing research. This evaluation was motivated by advances in mobile sensor technology which have led to the widespread adoption of digital compass and gyroscope sensors. The results of a comparative user study between sensor fusion and accelerometer-only implementations of tilt interaction showed several advantages for the use of sensor fusion, particularly in a walking context of use. Modifications to sensitivity adaptation and the use of tilt to perform zooming were also investigated. These modifications were designed to address controllability shortcomings identified in earlier experimental work. The results of a comparison between tilt zooming and Summary gesture zooming indicated that tilt zooming offered better results, both in terms of performance and subjective user ratings. Modifications to the original sensitivity adaptation algorithm were only partly successful. Greater accuracy improvements were achieved for walking tasks, but the use of dynamic dampening factors was found to be confusing. The results of this research were used to propose a framework for mobile tilt interaction. This framework provides an overview of the tilt interaction process and highlights how the enhanced techniques proposed in this research can be integrated into the design of tilt interaction techniques. The framework also proposes an application architecture which was implemented as an Application Programming Interface (API). This API was successfully used in the development of two prototype mobile applications incorporating tilt interactio...|$|R

