148|81|Public
2500|$|Though {{incompatible}} {{in every}} way with any other consumer electronics product, the 64DD's magnetic storage technology resembles the generic floppy disk, and the large and sturdy shell of the proprietary Zip disk for personal computers. [...] Though various prominent sources have mistakenly referred to the medium as being magneto-optical technology, Nintendo's own developer documentation refers to it in detail as being magnetic. [...] Complementing their proprietary and copy-protected cartridge strategy, the proprietary 64MB disk format was Nintendo's faster, more flexible, and copy-protected answer to the commodity Compact Disc format, which is cheaper to produce but is much slower, read-only, and easier to copy on personal computers. The most advanced CD technology delivered by the contemporaneous Sega Saturn and Sony PlayStation game consoles can hold at least 650megabytes (MB) of information with a peak 300kB/s throughput and more than 200ms seek speed. This compares to the Nintendo 64's cartridge's 4 to 64MB size and 5 to 50MB/s of low latency and instantaneous load times, and the 64DD's 64MB disk size and 1MB/s <b>peak</b> <b>throughput</b> with 75 ms average seek latency. The high seek latency and low maximum throughput of a 2x CD-ROM drive contribute to stuttering and to very long loading times throughout a gameplay session in many titles, {{in addition to a}} much higher production cost, testing cycle, and potential development time for all the potential extra content.|$|E
5000|$|Added RESTful API and mass-submission {{and results}} {{retrieval}} scripts - resulting in <b>peak</b> <b>throughput</b> above 20,000 predictions per day.|$|E
50|$|Ultra-high speed LANs {{may fall}} into this category, where {{protocol}} tuning is critical for achieving <b>peak</b> <b>throughput,</b> on account of their extremely high bandwidth, even though their delay is not great.|$|E
30|$|We {{compare the}} {{performance}} of the conventional FFR schemes (i.e., reuse- 1 and reuse- 3) with the proposed QoS-DFFR scheme under the HetNet scenario in terms of the user's average throughput, packet loss rate, interference, and cell-edge and <b>peak</b> <b>throughputs.</b>|$|R
5000|$|The {{ability to}} use all cores {{simultaneously}} to provide improved <b>peak</b> performance <b>throughput</b> of the SoC compared to IKS.|$|R
40|$|Web {{sites have}} {{gradually}} shifted from delivering just static html pages and images to customized, user-specific content and plethora of online services. The new features and facilities are {{made possible by}} dynamic content which is produced at request time. Multi-tiered database-driven web sites form the predominant infrastructure for most structured and scalable approaches to dynamic content delivery. However, even with these scalable approaches, the request-time computation and high resource demands for dynamic content generation result in significantly higher latencies and lower throughputs than for sites with just static content. This thesis proposes the caching of dynamic content as the solution for improving the performance of web sites with significant amount of dynamic content. This work shows that there is significant locality in the data accesses and computations for content generation which can be exploited by caching to improve performance. This work introduces a novel multi-tier caching architecture that incorporates multiple, independent caching components to enable easy deployment and effective performance over the prevalent multi-tiered database-driven architecture for dynamic content delivery. The dynamic content infrastructure and the proposed caching strategy is evaluated with e-commerce workloads from the TPC-W benchmark. The evaluation of the system without caching shows that content generation overheads are dominated by the database component for e-commerce workloads. With multi-tier caching, each caching component overcomes specific overheads during content generation while the combination provides overall improvements in performance significantly greater than the individual contributions. The increased <b>peak</b> <b>throughputs</b> with caching range from 1. 58 to 8. 72 times the <b>peak</b> <b>throughputs</b> without caching at similar or significantly reduced average response times. At the same load as for the <b>peak</b> <b>throughputs</b> without caching, the response times were reduced by 90 % to 97 % with caching. The evaluations also establish {{the effectiveness of the}} strategy in relation to variation in platform and site configurations. Overall, the proposed multi-tier caching strategy brings about dramatic improvements in performance for dynamic content delivery...|$|R
50|$|The {{processor}} {{consists of}} different clock domains, {{meaning that the}} entire chip does not operate the same clock speed. This causes some difficulty when measuring <b>peak</b> <b>throughput</b> of its various functions. Further adding to the confusion, it is listed as 667 MHz in Intel G965 white paper, but listed as 400 MHz in Intel G965 datasheet. There are various rules that define the IGP's processing capabilities.|$|E
50|$|Compound TCP is a {{modified}} TCP congestion avoidance algorithm, meant to improve networking performance in all applications. It is not enabled by default in the pre-Service Pack 1 version of Windows Vista, but enabled in SP1 and Windows Server 2008. It uses a different algorithm {{to modify the}} congestion window - borrowing from TCP Vegas and TCP New Reno. For every acknowledgement received, it increases the congestion window more aggressively, thus reaching the <b>peak</b> <b>throughput</b> much faster, increasing overall throughput.|$|E
5000|$|Since Katmai {{was built}} in the same 0.25 µm process as Pentium II [...] "Deschutes", it had to {{implement}} SSE using as little silicon as possible. To achieve this goal, Intel implemented the 128-bit architecture by double-cycling the existing 64-bit data paths and by merging the SIMD-FP multiplier unit with the x87 scalar FPU multiplier into a single unit. To utilize the existing 64-bit data paths, Katmai issues each SIMD-FP instruction as two μops. To compensate partially for implementing only half of SSE’s architectural width, Katmai implements the SIMD-FP adder as a separate unit on the second dispatch port. This organization allows one half of a SIMD multiply and one half of an independent SIMD add to be issued together bringing the <b>peak</b> <b>throughput</b> back to four floating point operations per cycle — at least for code with an even distribution of multiplies and adds.|$|E
3000|$|For both channel types, {{but more}} {{prominently}} for the ITU-A channel, {{the choice of}} H= 100 yields the best results in terms of <b>peak</b> normalized <b>throughput</b> {{and the degree of}} robustness for increasing t [...]...|$|R
40|$|Abstract—We {{present a}} new routing {{algorithm}} called Adaptive Distance Vector (ADV) for mobile, ad hoc networks (MANETs). ADV is a distance vector routing algorithm that exhibits some on-demand characteristics by varying {{the frequency and}} the size of the routing updates in response to the network load and mobility conditions. Using simulations we show that ADV outperforms AODV and DSR especially in high mobility cases by giving significantly higher (50 % or more) <b>peak</b> <b>throughputs</b> and lower packet delays. Furthermore, ADV uses fewer routing and control overhead packets than that of AODV and DSR, especially at moderate to high loads. Our results indicate the benefits of combining both proactive and on-demand routing techniques in designing suitable routing protocols for MANETs. I...|$|R
40|$|The {{design and}} {{implementation}} of distributed systems is helped by the availability of design patterns which offer robust and reliable solutions for the processing of parallel messaging. We develop continuous-time Markov chain models of two commonly used design patterns, Half-Sync/Half-Async and Leader/Followers, for their performance evaluation in multi-core machines. We propose a unified modeling approach which contemplates {{a detailed description of}} the application level logic and abstracts away from operating system calls and complex locking and networking application programming interfaces. By means of a validation study against implementations on a 16 -core machine, we show that the models accurately predict <b>peak</b> <b>throughputs</b> and variation trends with increasing concurrency levels for a wide range of message processing workloads. We also discuss the limits of our models when memory-level internal contention is not captured...|$|R
50|$|GeForce 8's unified shader {{architecture}} {{consists of}} a number of stream processors (SPs). Unlike the vector processing approach taken with older shader units, each SP is scalar and thus can operate only on one component at a time. This makes them less complex to build while still being quite flexible and universal. Scalar shader units also have the advantage of being more efficient in a number of cases as compared to previous generation vector shader units that rely on ideal instruction mixture and ordering to reach <b>peak</b> <b>throughput.</b> The lower maximum throughput of these scalar processors is compensated for by efficiency and by running them at a high clock speed (made possible by their simplicity). GeForce 8 runs the various parts of its core at differing clock speeds (clock domains), similar to the operation of the previous GeForce 7 Series GPUs. For example, the stream processors of GeForce 8800 GTX operate at a 1.35 GHz clock rate {{while the rest of the}} chip is operating at 575 MHz.|$|E
5000|$|Though {{incompatible}} {{in every}} way with any other consumer electronics product, the 64DD's magnetic storage technology resembles the generic floppy disk, and the large and sturdy shell of the proprietary Zip disk for personal computers. [...] Though various prominent sources have mistakenly referred to the medium as being magneto-optical technology, Nintendo's own developer documentation refers to it in detail as being magnetic. [...] Complementing their proprietary and copy-protected cartridge strategy, the proprietary 64 MB disk format was Nintendo's faster, more flexible, and copy-protected answer to the commodity Compact Disc format, which is cheaper to produce but is much slower, read-only, and easier to copy on personal computers. The most advanced CD technology delivered by the contemporaneous Sega Saturn and Sony PlayStation game consoles can hold at least 650 megabytes (MB) of information with a peak 300 kB/s throughput and more than 200 ms seek speed. This compares to the Nintendo 64's cartridge's 4 to 64 MB size and 5 to 50 MB/s of low latency and instantaneous load times, and the 64DD's 64 MB disk size and 1 MB/s <b>peak</b> <b>throughput</b> with 75 ms average seek latency. The high seek latency and low maximum throughput of a 2x CD-ROM drive contribute to stuttering and to very long loading times throughout a gameplay session in many titles, {{in addition to a}} much higher production cost, testing cycle, and potential development time for all the potential extra content.|$|E
30|$|Degradation of <b>peak</b> <b>throughput</b> {{performance}} {{occurs in}} the reuse- 3 case because the frequency bands are partitioned into FR and PR zones. Hence, there is a trade-off between the <b>peak</b> <b>throughput</b> performance degradation and interference reduction when using the reuse- 1 and reuse- 3 schemes.|$|E
40|$|One {{promising}} {{application of}} photonics to astronomical instrumentation is the miniaturization of near-infrared (NIR) spectrometers for large ground- and space-based astronomical telescopes. Here we present new results from our effort to fabricate {{arrayed waveguide grating}} (AWG) spectrometers for astronomical applications entirely in-house. Our latest devices have a <b>peak</b> overall <b>throughput</b> of 23...|$|R
40|$|In {{software}} engineering, {{design patterns}} {{are commonly used}} and represent robust solution templates to frequently occurring problems in software design and implementation. In this paper, we consider performance simulation for two design patterns for processing of parallel messaging. We develop continuous-time Markov chain models of two commonly used design patterns, Half-Sync/Half-Async and Leader/Followers, for their performance evaluation in multicore machines. We propose a unified modeling approach which contemplates {{a detailed description of}} the application-level logic and abstracts away from operating system calls and complex locking and networking application programming interfaces. By means of a validation study against implementations on a 16 -core machine, we show that the models accurately predict <b>peak</b> <b>throughputs</b> and variation trends with increasing concurrency levels for a wide range of message processing workloads. We also discuss the limits of our models when memory-level internal contention is not captured...|$|R
3000|$|... 0 enforces high {{transmit}} power for all users. Thus, users {{closer to the}} eNB should experience higher SINR values and, consequently, better UL performance. This is translated into higher <b>peak</b> user <b>throughput</b> in the cell {{at the expense of}} more user battery consumption. In contrast, cell-edge users that are power-limited cannot increase their {{transmit power}} when increasing P [...]...|$|R
30|$|In view {{of these}} results, the average and <b>peak</b> <b>throughput</b> cost {{efficiencies}} are respectively defined as (which give {{a sense of}} cost per throughput) of the different backhaul solutions indicate that the proposed hybrid solutions, which use a combination of LOS, NLOS, PtP, PtMP and different frequency bands, have the best trade-offs in terms of both average and <b>peak</b> <b>throughput</b> cost efficiencies. They achieve the lowest ‘cost per throughput’. The sub- 6 GHz PtMP solution also provides good average and <b>peak</b> <b>throughput</b> cost efficiencies. However, its low achievable average throughput makes this solution unsuitable for small cell deployments targeted at high capacities.|$|E
30|$|Interference is a {{more serious}} problem in the current LTE-A system, which in turn makes this {{trade-off}} as a prominent criterion to efficiently operate LTE systems. Therefore, the proposed QoS-DFFR scheme is well suited to improve the cell-edge user's throughput performance, {{at the cost of}} a very small <b>peak</b> <b>throughput</b> degradation of the system. On the other hand, this <b>peak</b> <b>throughput</b> degradation is balanced by interference reduction achieved by partitioning the frequency resources between the FR and PR zones in the HetNet scenario.|$|E
40|$|The 3 rd {{generation}} {{mobile communication}} systems, known as IMT- 2000 systems, have data transmission capability {{of up to}} 2 Mbps [1]. Recently, high-speed downlink packet access (HSDPA) aiming at a maximum <b>peak</b> <b>throughput</b> of around 10 Mbps is under development for the enhancement of the IMT- 2000 systems [2]. However, since information transferred over the Internet is becoming increasingly rich, even wireles...|$|E
3000|$|However, {{contrary}} to the opinion expressed in [19] that such a presaturation <b>throughput</b> <b>peak</b> might not be sustainable and therefore the traffic load should be maintained far below the saturation load for DCF with a backoff mechanism, our simulation results show that for some CW settings, such a presaturation <b>throughput</b> <b>peak</b> is sustainable. Furthermore, it can be far above the saturation load, while the total packet delay can be very low. For example, as illustrated in Fig. 4, when CW = 20 (which {{is a case of}} k [...]...|$|R
40|$|We {{present a}} new routing {{algorithm}} called Adaptive Distance Vector (ADV) for mobile, ad hoc networks (MANETs). ADV is a distance vector routing algorithm that exhibits some on-demand characteristics by varying {{the frequency and}} the size of the routing updates in response to the network load and mobility conditions. Using simulations we show that ADV outperforms AODV and DSR especially in high mobility cases by giving significantly higher (50 % or more) <b>peak</b> <b>throughputs</b> and lower packet delays. Furthermore, ADV uses fewer routing and control overhead packets than that of AODV and DSR, especially at moderate to high loads. Our results indicate the benefits of combining both proactive and on-demand routing techniques in designing suitable routing protocols for MANETs. I. INTRODUCTION A mobile, ad hoc network (MANET) facilitates mobile hosts such as laptops with wireless radio networks communicate among themselves even when there is no wired infrastructure. Since a MANET can be formed with [...] ...|$|R
5000|$|NVidia's {{documentation}} {{states a}} <b>peak</b> encoder <b>throughput</b> of 8x realtime at {{a resolution of}} 1920x1080 (where the baseline [...] "1x" [...] equals 30fps). Actual throughput varies on the selected preset, user controlled parameters and settings, and the GPU/memory clock frequencies. The published 8x rating is achievable with the NVENC high-performance preset, which sacrifices compression efficiency and quality for encoder throughput. It is considerably slower, but produces fewer compression artifacts.|$|R
40|$|Abstract—In this paper, {{we present}} a high {{throughput}} and low latency LDPC (low-density parity-check) decoder implementation on GPUs (graphics processing units). The existing GPU-based LDPC decoder implementations suffer from low throughput and long latency, which prevent them from being used in practical SDR (software-defined radio) systems. To overcome this problem, we present optimization techniques for a parallel LDPC decoder including algorithm optimization, fully coalesced memory access, asynchronous data transfer and multi-stream concurrent kernel execution for modern GPU architectures. Experimental results demonstrate that the proposed LDPC decoder achieves 316 Mbps (at 10 iterations) <b>peak</b> <b>throughput</b> on a single GPU. The decoding latency, which is much {{lower than that of}} the state of the art, varies from 0. 207 ms to 1. 266 ms for different throughput requirements from 62. 5 Mbps to 304. 16 Mbps. When using four GPUs concurrently, we achieve an aggregate <b>peak</b> <b>throughput</b> of 1. 25 Gbps (at 10 iterations). Index Terms—LDPC codes, software-defined radio, GPU, high throughput, low latency...|$|E
40|$|Datacenters, or {{warehouse}} scale computers, {{are rapidly}} increasing {{in size and}} power consumption. However, this growth comes {{at the cost of}} an increasing thermal load that must be removed to prevent overheating and server failure. In this paper, we propose to use phase changing materials (PCM) to shape the thermal load of a datacenter, absorbing and releasing heat when it is advantageous to do so. We present and validate a methodology to study the impact of PCM on a datacenter, and evaluate two important opportunities for cost savings. We find that in a datacenter with full cooling system subscription, PCM can reduce the necessary cooling system size by up to 12 % without impacting <b>peak</b> <b>throughput,</b> or increase the number of servers by up to 14. 6 % without increasing the cooling load. In a thermally constrained setting, PCM can increase <b>peak</b> <b>throughput</b> up to 69 % while delaying the onset of thermal limits by over 3 hours. 1...|$|E
40|$|In this work, {{we propose}} a new {{batching}} scheme called temporal merge, which dispatches discontiguous block requests using a single I/O operation. It overcomes the disadvantages of narrow block interface and enables an OS to exploit <b>peak</b> <b>throughput</b> of a storage device for small random requests {{as well as}} a single large request. Temporal merge significantly enhances device and chan-nel utilization regardless of access sequentiality of a workload, which has not been achievable by traditional schemes. We extended the block I/O interface of a DRAM-based SSD in cooperation with its vendor, and implemented temporal merge into I/O subsystem in Linux 2. 6. 32. The experimental results show that under multi-threaded ran-dom access workload, the proposed solution can achieve 87 %∼ 100 % of <b>peak</b> <b>throughput</b> of the SSD. We expect that the new temporal merge interface will lead to bet-ter design of future host controller interfaces such as NVMHCI for next-generation storage devices. ...|$|E
5000|$|... 250 HSDPA {{networks}} have commercially launched mobile broadband services in 109 countries. 169 HSDPA networks support 3.6 Mbit/s <b>peak</b> downlink data <b>throughput.</b> A growing number are delivering 21 Mbit/s peak data downlink and 28 Mbit/s.|$|R
30|$|The self-backhauling of Figure 1 b, {{with use}} of macro {{resources}} in the backhaul, is key factor while utilizing the benefits of cost-effectiveness and flexibility in LPN deployment. However, the reuse of the macro resources usually results in a backhaul capacity bottleneck, whereby the aggregate served capacity of LPN access exceeds the achievable wireless backhaul capacity. This self-backhauling capacity bottleneck will consequently limit <b>peak</b> achievable <b>throughputs</b> for the UE served by the LPN.|$|R
30|$|From the {{cumulative}} distribution function of the user throughput, {{it is observed that}} the 95 % ile (<b>peak)</b> user <b>throughput</b> of the MU-MIMO system is lower than that of the SU-MIMO system. At the 5 % ile (cell edge) user throughput there is no difference in the performance of MU-MIMO system and SU-MIMO system. This behavior {{comes from the fact that}} in the MU-MIMO PS we try not to schedule cell-edge UEs in MU-MIMO mode Section 4.2.|$|R
40|$|In multivector processors, the {{effective}} throughput {{of the memory}} system is a decisive factor {{in the performance of}} the computer. The lost cycles due to conflicts between concurrent vector streams make {{the effective}} throughput be lower than the <b>peak</b> <b>throughput.</b> When the request rate of all the concurrent vector streams to every memory module is {{less than or equal to}} the service rate, conflicts appear because concurrent vector streams reference memory modules in different orders. This paper proposes an access sequence to the vector stream elements that eliminates this kind of conflicts, allowing the effective throughput reach the <b>peak</b> <b>throughput.</b> In other cases, when request rate is greater than the service rate, the proposed order reduces the number of lost cycles, and the effective throughput increases. The proposed order generates the memory references in such a way that the memory modules shared by the concurrent self-conflict-free vector streams, are referenced using the same order [...] ...|$|E
40|$|This paper {{proposes a}} mutlipath {{interference}} canceller (MPIC) associated with orthogonal code-multiplexing that achieves much higher <b>peak</b> <b>throughput</b> than 2 mb/s with adaptive data modulation for high-speed packet transmission in the wideband direct sequence-code {{division multiple access}} (W-CDMA) forward link, and evaluates its throughput performance by computer simulation. The simulation results elucidate that sufficient multipath interference (MPI) suppression is achieved by a four-stage MPIC with 6 – 12 orthogonal code-multiplexing using one iterative channel estimation with pilot and decision feedback data symbols and further that the interference rejection weight control according {{to the number of}} observed multipaths is effective in improving the throughput. It is also demonstrated that MPIC exhibits a superior MPI suppression effect to a chip equalizer in the lower received signal energy per bit-to-background noise spectrum density (0) channel around 0 – 3 dB owing to the successive channel estimation at each stage. We show that the maximum <b>peak</b> <b>throughput</b> using MPIC is approximately 2. 1 fold that without MPIC in a two-path and three-path Rayleigh fading channel and that the <b>peak</b> <b>throughput</b> of 8. 0 mb/s is achieved using 64 QAM data modulation in a two-path fading channel within a 5 -MHz bandwidth. Furthermore, the required average 0 for satisfying the same throughput with MPIC is decreased by more than 2. 0 dB. MPIC utilizes an efficient high-level data modulation scheme and is very effective in extending the coverage in which much higher throughput can be provided under multipath fading channels for high-speed packet transmission in the W-CDMA forward link...|$|E
40|$|The Yottabyte NetStorage(TM) Company, today {{announced}} a new world record for TCP disk-to-disk data transfer using the company's NetStorager(R) System. The record-breaking demonstration transferred 5 terabytes of data between Chicago, Il. to Vancouver, BC and Ottawa, ON, at a sustained average throughput of 11. 1 gigabits per second. <b>Peak</b> <b>throughput</b> exceeded 11. 6 gigabits per second, more than 15 -times faster than previous records for TCP transfer from disk-to-disk (1 page) ...|$|E
40|$|Due {{to massive}} {{available}} spectrum in the millimeter wave (mmWave) bands, cellular systems in these frequencies may provides {{orders of magnitude}} greater capacity than networks in conventional lower frequency bands. However, due to high susceptibility to blocking, mmWave links can be extremely intermittent in quality. This combination of high <b>peak</b> <b>throughputs</b> and intermittency can cause significant challenges in end-to-end transport-layer mechanisms such as TCP. This paper studies the particularly challenging problem of bufferbloat. Specifically, with current buffering and congestion control mechanisms, high throughput-high variable links can lead to excessive buffers incurring long latency. In this paper, we capture the performance trends obtained while adopting two potential solutions that have been proposed in the literature: Active queue management (AQM) and dynamic receive window. We show that, over mmWave links, AQM mitigates the latency but cannot deliver high throughput. The main reason relies {{on the fact that}} the current congestion control was not designed to cope with high data rates with sudden change. Conversely, the dynamic receive window approach is more responsive and therefore supports higher channel utilization while mitigating the delay, thus representing a viable solution...|$|R
40|$|Abstract. We discuss {{high-performance}} programmable asynchronous pipeline arrays (PAPAs). These pipeline arrays are coarse-grain field programmable gate arrays (FPGAs) that realize {{high data}} throughput with fine-grain pipelined asynchronous circuits. We {{show how the}} PAPA architecture maintains most of the speed and energy benefits of a custom asynchronous design, while also providing post-fabrication logic reconfigurability. We report results for a prototype PAPA design in a 0. 25 µm CMOS process that has a <b>peak</b> pipeline <b>throughput</b> of 395 MHz for asynchronous logic. ...|$|R
30|$|The {{backhaul}} capacity {{must not}} constrain the small cell capacity [3]. Thus, the backhaul capacity {{should be able}} to support the busy hour traffic and have enough margin to cover its future growth and statistical variation [10]. In wireless backhaul, the available bandwidth, share of radio resources and modulation scheme (and hence SINR) impact the backhaul capacity. However, dimensioning the backhaul capacity for the worst case scenario will result in over-provisioning and more expensive solutions. This is because providing high capacity may require the deployment of more PoPs and the use of more sophisticated technologies. As a common indicator for dimensioning, the busy hour traffic can be assessed with regard to two different loading conditions, known as busy times and quiet times, resulting in two traffic indicators, the quiet time <b>peak</b> cell <b>throughput</b> and the busy time mean cell throughput, respectively [4]. During quiet times, it is most likely that a single UE has access to the whole spectrum. If the signal quality of this UE is high, the cell <b>throughput</b> reaches its <b>peak.</b> This condition {{is referred to as the}} quiet time <b>peak</b> cell <b>throughput.</b> In contrast, during busy times, many UEs access the spectral resources of the cell and experience different signal qualities, and the busy time mean cell throughput can be computed averaging the throughputs of all UEs during the busy hour. Dimensioning the backhaul network for the busy time mean cell throughput will result in a reduced cost, since it is always lower than the quiet time <b>peak</b> cell <b>throughput,</b> but may prevent operators to exploit the full benefit of small cells. The minimum target today in order to backhaul LTE small cells is around 50 Mbps, and 150 Mbps or higher capacities are required to support peak data rates [10]. These numbers are expected to grow as multiple radio access technologies and additional spectrum become available for small cells.|$|R
