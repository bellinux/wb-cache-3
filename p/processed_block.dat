15|1308|Public
50|$|Again, this {{transform}} {{does not}} alter {{the size of the}} <b>processed</b> <b>block.</b> Each of the symbols in use in the document is placed in an array. When a symbol is processed, it is replaced by its location (index) in the array and that symbol is shuffled {{to the front of the}} array. The effect is that immediately recurring symbols are replaced by zero symbols (long runs of any arbitrary symbol thus become runs of zero symbols), while other symbols are remapped according to their local frequency.|$|E
40|$|Abstract. This paper {{presents}} a parallel architecture that can simultaneously perform block-matching motion estimation (ME) and discrete cosine transform (DCT). Because DCT and ME are both <b>processed</b> <b>block</b> by block, it {{is preferable to}} put them in one module for resource sharing. Simulation results performed using Simulink demonstrate that the parallel fashioned architecture improves the performance in terms of running time by 18. 6 % compared to the conventional sequential fashioned architecture. ...|$|E
40|$|Abstract—We compare some block {{equalizers}} for singlecarrier {{satellite systems}} subject to hard propagation {{conditions such as}} high Doppler spread, non-line-of-sight (NLOS), and nonnegligible multipath fading. These impairments are relevant when the very high speed of a mobile receiver forces to use a nondirectional antenna. First, we compare low-complexity timedomain and frequency-domain block equalizers. Then, we propose a novel frequency-domain equalizer based on a fast Fourier transform (FFT) with size higher than {{the length of the}} <b>processed</b> <b>block.</b> Thanks to the increased frequency resolution, the proposed equalizer yields improved performance with respect to the conventional frequency-domain equalizer. Simulation results validate the effectiveness of the proposed technique. Keywords—Frequency-domain equalization; Doppler spread; NLOS; multipath; satellite communications. I...|$|E
50|$|Process blocks: the <b>process</b> <b>block</b> {{represents}} {{the simplest of}} steps and requires no analyses. When a <b>process</b> <b>block</b> is encountered the action inside the block is performed and we move onto the next block.|$|R
50|$|The test last {{block is}} simply reversed, the <b>process</b> <b>blocks</b> are {{completed}} before {{the test is}} performed. The test last loop allows for the <b>process</b> <b>blocks</b> to be performed at least once before the first test.|$|R
40|$|Abstract This paper {{presents}} a methodology for accurately measuring system call service times and <b>process</b> <b>blocking</b> {{times in a}} UNIX-like computing environment. Problems that may cause inaccurate measurements stem from the method used to charge processes for execution time, {{the difference in the}} time consumed during a system call and a time quantum or dispatch interval, and the instrumentation overhead included in the measurement program. The collected measurements for each system call are used to form a distribution from which <b>process</b> <b>blocking</b> times can be drawn or for which the mean can be used as the <b>process</b> <b>blocking</b> time. 1...|$|R
40|$|Denoising {{as one of}} {{the most}} {{significant}} task in video processing was studied widely in the literature. We propose an efficient video denoising method based on decomposition approach for matrix com-pletion. A noisy video is processed in blockwise manner and for each <b>processed</b> <b>block</b> we find similar blocks in other frames. The similar blocks then will stack together and unreliable pixels will re-move using fast matrix completion method [1]. We demonstrate the effectiveness of our algorithm in removing the mixed noise through the results. Our results also proved the effectiveness of our algorithm in removing noise from regular structures. We also compare with other denoising technique using matrix completion. Our method re-sults in comparable performance with significantly lower computa-tion complexity. Index Terms — Video denoising, Matrix Completion, Block matchin...|$|E
30|$|Other {{state-of-the-art}} algorithms {{found in}} literature {{work in the}} same manner; for example in Liu and Freeman [16], a framework that integrates robust optical flow into a non-local means framework with noise level estimation is used, and the temporal coherence is taken into account in removing structured noise. In the paper by Dabov et al. [17], it is interesting to see how they propose a method based on highly sparse signal representation in local 3 D transform domain; a noisy video is processed in blockwise manner, and for each <b>processed</b> <b>block,</b> they form data array by stacking together blocks found to be similar to the currently processed one. In [18], Mairal et al. presented a framework for learning multiscale sparse representations of color images and video with overcomplete dictionaries. They propose a multiscaled learned representation obtained by using an efficient quadtree decomposition of the learned dictionary and overlapping image patches. This provides an alternative to predefined dictionaries such as wavelets.|$|E
40|$|Denoising {{as one of}} {{the most}} {{significant}} tools in medical imaging was studied widely in the literature. However, most existing medical image denoising algo-rithms have assumed the additive white Gaussian noise. In this work, we propose an efficient medical image denoising method that can handle a noise mixture of various types. Our method is based on block matching filtering and low-rank matrix completion as follows. A noisy slice is processed in blockwise manner and for each <b>processed</b> <b>block</b> we find similar blocks in other slices. The similar blocks then will stack together and unreliable pixels will remove using fast matrix comple-tion method [1]. We demonstrate the effectiveness of our algorithm in removing the mixed noise through the results. Our results also proved the effectiveness of our algorithm in removing noise from regular structures. We also compare with other denoising technique using matrix completion. Our method results in comparable performance with significantly lower computation com-plexity. Keywords-Medical Image denoising, Matrix Comple-tion, Block matchin...|$|E
50|$|There are {{two main}} types of testing loops, test first and test last blocks. The only {{difference}} between the two is the order in which the steps involved are completed.In the test first situation, when the program encounters the block it tests to see if the condition is fulfilled, then, if it is not completes the <b>process</b> <b>blocks</b> and then loops back. The test is performed again and, if the condition is still unfulfilled, it processes again. If at any stage the condition is fulfilled the program skips the <b>process</b> <b>blocks</b> and continues onto the next block.|$|R
50|$|A TCB is a {{general-purpose}} {{instance of}} a <b>Process</b> control <b>block</b> in OS/360 and successor systems. An SRB {{is a highly}} optimized instance of a <b>Process</b> control <b>block</b> in MVS/370 and successor systems.|$|R
50|$|An SRB is a highly-optimized {{instance}} of a <b>Process</b> control <b>block</b> in these MVS/370 and successor systems. A TCB is a general-purpose {{instance of}} a <b>Process</b> control <b>block</b> in these OS/360 and successor systems.|$|R
30|$|A real {{convolution}} code in systematic form [16] is used {{to compute}} parity values associated with the processing outputs as shown in Figure 2. Certain classes of errors occurring anywhere in the overall system including the parity generation and regeneration subsystems are easily detected. A convolution code with its encoding memory can sense the onset of errors before they increase beyond detection limits. For a rate k/n real convolution code with constraint parameter, it is always possible by simple linear operations to extract the parity generating part. The (n - k) parity samples for each <b>processed</b> <b>block</b> of samples are produced in block processing fashion. Since processing resources are in close proximity, it is easily demonstrated [9] that an efficient block processing structure can produce the (n - k) parity values directly from the inputs. When these two comparable parity values are subtracted, one from the outputs and the others directly from the inputs, only the stochastic effects remain, and the syndromes are produced as shown in Figure 2.|$|E
40|$|In this paper, the Line-Scan Clustering (LSC) {{algorithm}}, a novel one-pass algorithm for labeling arbitrarily connected components is presented. In {{currently available}} connected components labeling approaches, only 4 or 8 connected components can be labeled. We overcome this limitation by introducing the new notion n-ED-neighbors. In designing the algorithm, we fully considered the particular properties of a connected component in an image and employed two data structures, the LSC algorithm turns {{to be highly}} efficient. On top of this, it has three more favorable features. First, as its capability to be <b>processed</b> <b>block</b> by block means that it is suitable for parallel processing, improving the speed when multiple processors are used. Second, its applicability is extended from working on binary images only to directly work on gray images, implying an efficiency gain in time spent on image binarization. Moreover, the LSC algorithm provides a more convenient way to employ the labeling result for conducting processing in later stages. Finally we compare LSC with an efficient connected labeling algorithm that is recently published, demonstrating how the LSC algorithm is faster. Department of Computin...|$|E
40|$|Mining {{operations}} have traditionally used specialised software packages to process and visualise valuable mining data. The downside to {{this approach is}} that the information can only be viewed when the expert who knows how to operate the software is available. In this study we have built upon an existing, easy to use, interactive intranet-based visualisation system that integrates geological, geophysical, geotechnical and mining information. This system facilitates the communication between groups by making the information accessible to all within an organisation. We have developed a process through which complex block model data can be organised into a form that makes it suitable for viewing within the system. This process, along with the high level architecture of the system that allows the <b>processed</b> <b>block</b> model data to be added to it is discussed. An example of where the process has been applied to a visualisation system in industry is presented. Users of the visualisation system have found that its strengths lie in the increased availability of important information for effective communication of complex data by mine site personnel, and increased productivity and safety at the mine site...|$|E
50|$|Testing loops: {{this block}} allows {{the program to}} loop one or a set of {{processes}} until a particular condition is fulfilled. The <b>process</b> <b>blocks</b> covered by each loop are subset with a side-bar extending out from the condition.|$|R
5000|$|... vmstat vmstat reports {{information}} about runable or <b>blocked</b> <b>processes,</b> memory, paging, <b>block</b> I/O, traps, and CPU.|$|R
50|$|Optionally, if a <b>process</b> <b>blocks</b> for I/O, it is 'promoted' one level, {{and placed}} {{at the end of}} the next-higher queue. This allows I/O bound {{processes}} to be favored by the scheduler and allows processes to 'escape' the base level queue.|$|R
40|$|Abstract:- In this paper, {{based on}} the {{analysis}} to relationship between the currently being <b>processed</b> <b>block</b> and its neighbors, a motion vector context-based adaptive 3 -D recursive search (MVCA- 3 DRS) block matching motion estimation algorithm is developed. In the proposed algorithm, the candidate vectors are partitioned into two categories, one stationary vector implying the current block belongs to one of neighboring large objects, one nonstationary vector implying the current block either belongs to one of neighboring small objects, or is a new object, then predicted with extended vector median estimator and anti-median estimator as well as random-updated estimator {{based on the}} spatial-temporal motion vector contexts. The simulation {{results show that the}} proposed algorithm can significantly improve the consistence of the resulting motion vector field. Test experiments on motion-compensation (MC) deinterlaced system with typical video sequences confirm that, compared with the 3 DRS algorithm, the proposed MVCA- 3 DRS can significantly improve the interpolated images quality. Although introducing the additional vector filtering operations, the proposed algorithm still can maintain a comparative computation complexity with that of the 3 DRS algorithm due to its shrink to the number of candidate vectors. Key-Words:- 3 DRS, block matching, motion estimation, de-interlace, frame rate up-conversion...|$|E
40|$|We propose an e¤ective video {{denoising}} method {{based on}} highly sparse signal representation in local 3 D transform domain. A noisy video is processed in blockwise manner {{and for each}} <b>processed</b> <b>block</b> we form a 3 D data array that we call “group ” by stacking together blocks found similar to the currently processed one. This grouping is realized as a spatio-temporal predictive-search block-matching, similar to techniques used for motion estimation. Each formed 3 D group is …ltered by a 3 D transform-domain shrinkage (hard-thresholding and Wiener …ltering), the result of which are estimates of all grouped blocks. This …ltering— that we term “collaborative …ltering” — exploits the correlation between grouped blocks and the corresponding highly sparse representation of the true signal in the transform domain. Since, in general, the obtained block estimates are mutually overlapping, we aggregate them by a weighted average in order to form a non-redundant estimate of the video. Signi…cant improvement {{of this approach is}} achieved by using a two-step algorithm where an intermediate estimate is produced by grouping and collaborative hard-thresholding and then used both for improving the grouping and for applying collaborative empirical Wiener …ltering. We develop an e ¢-cient realization of this video denoising algorithm. The experimental results show that at reasonable computational cost it achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality. 1...|$|E
40|$|Quantitative (stereological) {{studies were}} {{performed}} {{to determine the number}} of germ cells In the developing rat testis. Sprague-Dawley rats aged 1 - 70 days were fixed by immersion or perfusion and embedded in Epon Araldite. Blocks of tissue were sectioned at 1. 5 pm and stained with toluidine blue dye. Sections were systematically scanned and the area! density of nuclear profiles counted using an unbiased counting frame. Numerical density and absolute number of germ cells in the <b>processed</b> <b>block</b> were then estimated. Corrections for processing shrinkage were determined by comparing the volume of processed and unprocessed samples. The results demonstrate the necessity of determining absolute number rather than volume density (or area! density) in comparing germ cell numbers. In these experiments, spermatogonial numbers stabilized in the range 18. 4 - 2 3. 6 million per testis on Day 30. The number of primary spermatocytes that were first apparent on Day 15 increased rapidly to 54. 6 million per testis on Day 30 and then slowly to 73. 6 million on Day 70. Round spermatids were first apparent on Day 25 and Increased rapidly to 85. 7 million per testis on Day 40, then continued to increase to 151. 9 million on Day 70. The study provides both methods and baseline data for future experiments involving manipulation of the spermatogenic potential of the testis...|$|E
40|$|In {{the field}} of optical systems (e. g. lasers) {{microassembly}} steps are mainly of manual nature. Product development {{takes place in a}} lab environment, making highly specialized machine solutions for a latter transfer into series production mandatory. In the course of the SCALAB project a microassembly cell, based on modular <b>process</b> <b>blocks,</b> has been developed in order to ease and speed-up this transfer process. The cell is suitable both for development processes as well as series production. The <b>process</b> <b>blocks</b> cover versatile handling solutions, sensor- and measurement technology, automation friendly bonding processes and the flexible control architecture. The automated assembly of an optical resonator served as proof of concept for this approach...|$|R
40|$|Abstract — Entity Resolution (ER) is {{the problem}} of {{identifying}} which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most <b>blocking</b> techniques <b>process</b> <b>blocks</b> separately and do not exploit the results of other blocks. In this paper, we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently <b>processed</b> <b>blocks.</b> Blocks are now iteratively <b>processed</b> until no <b>block</b> contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking is more accurate and efficient than blocking, especially for large datasets. I...|$|R
50|$|Interestingly, {{immature}} human dendritic cells {{appear to}} require {{interactions with the}} EGF-like domains of selectins during their maturation <b>process.</b> <b>Blocking</b> of this interaction with monoclonal anti-EGF-like domain antibodies prevents dendritic cell maturation. The immature cells fail to activate T-cells and produce less interleukin 12 than wild-type dendritic cells.|$|R
40|$|Employing Rate-Distortion framework, a fast video {{compression}} {{algorithm is proposed}} for real-time PC implementations. Digital storage and transmission of video data are rarely possible when compression algorithms are not employed. We shall consider a scheme that may be PC-implemented in different variants depending on the computational capacity required. Let frame be a matrix of luminance samples (pixels) having M 1 rows and M 2 columns: () lkb,=B, k= 0, 1,…,M 1 - 1, l= 0, 1,…,M 2 - 1. We refer video sequence to series of frames ÿÿ,,,, 10 iBBB representing time sampled video scene. Let (y,x) -block of frame B (y, x are integer) be a submatrix () lkxy b,, =B, where 1,, 1, 1 −++ = Nyyyk ÿ, 1,, 1, 2 −++ = Nxxxl ÿ. Minding that we consider lossy compression algorithms, we use cap • ̂ to indicate restored data that may differ from original. For instance, ÿÿ,ˆ,,ˆ, ˆ 10 iBBB denotes decoded video sequence. ∗ The author completed this paper and submited it for publication during the visit to Växjö University (June, 2001) supported by “Mathematical modeling ” program of international collaboration. 2 HIGH-LEVEL ALGORITHM DESCRIPTION Like many schemes, our algorithm uses block-wise frame encoding. Each frame is covered by non-overlapping blocks,nmB of fixed size 8 × 8, m,n= 0, 8, 16 … Blocks are scanned in zigzag order according to the figure. For this order, any currently <b>processed</b> <b>block</b> has just-processed neighboring block always; that property will be used further. If any block 1 −iy,xB is similar in certain sense to i nm,B...|$|E
40|$|Cluster dot {{dithering}} is {{very useful}} halftoning for multifunction printers with electrophotographic printing process. However, {{there is an}} inevitable trade-off between spatial resolution and grey tone levels: sharpness and smoothness. In addition, it tends to produce undesired artifacts when printed images such as newspapers and magazines are copied. In this paper, we propose efficient, locally-content adaptive, and clustered dot halftoning, which improves image sharpness, increases text readability, suppresses artifacts, and maintain smoothness in scanned images from printed documents or natural images. We first split an image into 5 x 5 pixel blocks, and then each block is processed in raster scan order. Each pixel in the current <b>processed</b> <b>block</b> is classify into one of three categories {{by the process of}} non-smooth detection and halftone extraction. The disconnected characteristic of halftones is then used to separate halftone pixels from edge pixels. An adaptive approach to generate halftone images is used in each different type of block. Edge-enhanced cluster dot dithering is applied to edge blocks to reproduce sharp edges and also minimize block artifacts. The scaled and weighted factors are then used to determine the number and the position of black dots in the current processing block. To get the proper weighting and scaling values, we use average lightness and minimum square error from dithered images of several step inputs. Unlike edge block, preprocessing is performed before halftoning in halftone and complex blocks to recover the original continuous image from a halftone image. For halftone block, we estimate halftone resolution from the gradients at halftone pixels by three pairs one-dimensional derivative masks with different size, and then apply the average filter, which is chosen by the estimate of halftone resolution, to each pixel. For complex block, we group all pixels by virtual edge lines and then compute the average value of pixels in each group. The virtual edge lines are determined by edge direction and edge pixels along the edge direction, and these edge lines are used to separate a complex block into two or more groups which have different grey values in the original continuous-tone image. We use only one screen in an entire image to minimize block artifacts that appear when switching between different approaches at boundaries of different types of blocks. The proposed method is suitable for hardware implementation because it requires a small amount of memory and simple operations. Our experiments show that text readability and edge sharpness are enhanced while image smoothness are reproduced. ...|$|E
40|$|Cryptology is {{the science}} of secret communication, which {{consists}} of two complementary disciplines: cryptography and cryptanalysis. Cryptography is dealing with design and development of new primitives, algorithms and schemas for data enciphering and deciphering. For many centuries cryptographic technics have been applied in protection of secrecy and authentication in diplomatic, political and military correspondences and communications. Cryptanalysis is dealing with different attacks on cryptographic schemas and algorithms, with purpose to retrieve the hidden information and the same later to use, modify, forge etc. There is a big interconnection between these two disciplines. Cryptographer who design a new algorithm, must evaluate its security for all known cryptanalytic attacks and technics, if he wants its algorithm to be practical and useful. For future users to have confidence in a new algorithm and to use it, a long-time analysis and evaluation of its security from bigger group of cryptanalysts is needed, without any resulting weakness. Quasigroups are very suitable for application in cryptography, because of their structure, features and big number. One {{of the problems is}} which quasigroup is suitable to choose for using, concerning what preconditions quasigroup must fulfill. Several classi¯cation and separations of quasigroups are made for that purpose, with possibility for more. Quasigroups are used for definition of a quasigroup transformations. Sequences produced by quasigroup transformations are also examined and their analysis shows that they can be used as building elements of different cryptographic primitives. Cryptology as a science is developing with huge speed, because a new cryptographic schemas and algorithms, a new design strategies, a new fields of application, a new requirements and a new attacks are appearing, continuously. Appearance of new successful attacks and discovering weaknesses in declared standards, as well as requirements for augmented key and blocks lengths, induce the necessity of a new approaches in design and security evaluation, deployment of new building elements, modi¯cation of existing algorithms and schemas etc. The thesis investigates several issues: (1) What properties should have some quasigroup, so it can be used as non-linear building block in cryptographic primitives and it can contributed to the defense of linear and differential attacks? (2) How to generate and how to compute fast operation of huge quasigroups? (3) What kind of features have huge quasigroups obtained by new construction method? (4) In which way to use huge quasi- groups as building blocks of cryptographic primitives? The contents of the thesis is as follows. First, we introduce the theory of quasigroups and quasigroup transformations. We introduce a new way of computing the number of n-ary quasigroups, with which we obtained the number of ternary quasigroups of order 4 divided in 12 isotopy classes. We introduce some new kind of quasigroup transformations and we represent a prop ratio tables and correlation matrices of quasigroups of small order and some quasigroup transformations. This induce new classification of quasigroups according to their prop ratio tables and correlation matrices. We use the notation of the shapeless quasigroup and we introduce a notation of a perfect quasigroup. Then, we investigate different ways of producing huge quasigroups and suggest a new way of computing a huge quasigroup operation with applying Extended Feistel networks. This approach deploy Feistel network with special preconditions as an orthomorphism of a group. We analyze quasigroups obtained by Extended Feistel networks and show in which cases they are suitable for cryptographic needs. Next, we give a survey of quasigroup based hash functions, stream and block ciphers, public-key algorithms etc. We design two new cryptographic primitives which are using huge quasigroups as building blocks. We introduce NaSHA family of hash functions, with our implementation that is a candidate for NIST competition for SHA- 3 standard and we show how by using Extended Feistel network we can apply different huge quasigroups for processing single message block and even how used quasigroups can depend of <b>processed</b> <b>block.</b> This features make harder the cryptanalyst job. We introduce Alexsmile family of block ciphers and give one implementation for 128 -bit block size and key size of 128, 192 and 256 bits...|$|E
5000|$|Blocking writes {{can be used}} so that a <b>process</b> <b>blocks</b> if a FIFO is full. This {{approach}} may unfortunately lead to an artificial deadlock unless the designer properly derives safe bounds for FIFOs (Parks, 1995). Local artificial detection at run-time {{may be necessary to}} guarantee the production of the correct output.|$|R
5000|$|Actions can be {{included}} {{to be taken}} during the parsing process as well and the [...] function {{can be used to}} <b>process</b> <b>blocks</b> or strings. At the string parsing level [...] must handle the [...] "low level" [...] parsing, taking into account characters and delimiters. Block parsing is higher level, handling the scanning at the level of Rebol values.|$|R
50|$|The {{second type}} of {{branching}} block is a multiple branching block. This block is used when a select case is needed in a program. The block usually contains a question or select case. The block provides the program {{with an array of}} choices and is often used in conjunction with sub <b>process</b> <b>blocks</b> to save space.|$|R
50|$|The CRYPTON {{algorithm}} <b>processes</b> <b>blocks</b> of 128 bits in {{the form}} of 4×4 byte arrays. The round transformation consists of four steps: byte-wise substitution, column-wise bit permutation, column-to-row transposition and finally key addition. CRYPTON uses 12 rounds of this encryption process. Due to the algorithm's nature, the decryption process can be made identical to the encryption process using a different key.|$|R
5000|$|Pointer to the <b>Process</b> control <b>block</b> (PCB) of {{the process}} that the thread lives on ...|$|R
50|$|LTT {{allows the}} user to see {{in-depth}} information about the processes that were running during the trace period, including when context switches occurred, how long the <b>processes</b> were <b>blocked</b> for, and how much time the processes spent executing vs. how much time the <b>processes</b> were <b>blocked.</b> The data is logged to a text file and various console-based and graphical (GTK+) tools are provided for interpreting that data.|$|R
50|$|Denelcor, Inc. {{introduced}} multi-threading {{with the}} HEP (Heterogeneous Element Processor) in 1982. The HEP pipeline {{could not hold}} multiple instructions that {{belong to the same}} process. Only one instruction from a given process was allowed to be present in the pipeline at any point in time. Should an instruction from a given <b>process</b> <b>block</b> in the pipe, instructions from the other processes would continue after the pipeline drained.|$|R
40|$|Abstract: We {{consider}} some technical problems dealing with numerical implementation of explicit methods for computers with multiprocessing architecture. We give few examples written in Fortran {{and using the}} MPI library. We also consider problems of <b>processes</b> <b>blocking,</b> increasing of productivity (including processes balancing) and debugging. We believe that these tricks and fragments of the code will be useful for numerical implementation of parallel programs for various problems. Note: Publication language:russia...|$|R
40|$|The paper {{presents}} {{method used}} to creating patterns for hydroacoustics signals for necessity of sound identification or classification. First the mathematical fundamentals, with breaking to separate <b>processed</b> <b>blocks,</b> of proposed method were introduced. Next {{the description of}} realized research and discussion about some obtained results were presented. At {{the end of the}} paper the direction of development in creating patterns for hydroacoustics signals and its selectors were pointed. 1...|$|R
