102|10000|Public
25|$|The first aids to {{computation}} were purely mechanical devices {{which required}} the operator {{to set up}} the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form, for instance distance along a scale, rotation of a shaft, or a voltage. Numbers could also be represented in the form of digits, automatically manipulated by a mechanical mechanism. Although this approach generally required more complex mechanisms, it greatly increased the <b>precision</b> <b>of</b> <b>results.</b> A series of breakthroughs, such as miniaturized transistor computers, and the integrated circuit, caused digital computers to largely replace analog computers. The cost of computers gradually became so low that by the 1990s, personal computers, and then, in the 2000s, mobile computers, (smartphones and tablets) became ubiquitous in industrialized countries.|$|E
50|$|Its most novel feature was unnormalized {{significance}} arithmetic floating point. This allowed {{users to}} determine the change in <b>precision</b> <b>of</b> <b>results</b> {{due to the nature}} of the computation.|$|E
50|$|Contextual {{search is}} a form of {{optimizing}} web-based search results based on context provided by the user and the computer being used to enter the query. Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their relevance to the query. Rather, contextual search attempts to increase the <b>precision</b> <b>of</b> <b>results</b> based on how valuable they are to individual users.|$|E
30|$|Select {{the testing}} samples of each class, and {{calculate}} the <b>precision</b> <b>of</b> classification <b>result.</b>|$|R
40|$|Students {{enrolled}} in two beginning bowling classes {{received the same}} basic instruction on bowling techniques, but practiced under two different levels <b>of</b> <b>precision</b> <b>of</b> knowledge <b>of</b> <b>results</b> (KOR). The experimental group practiced by rolling at {{a full set of}} ten bowling pins with each ball (precise KOR). The control group practiced by shadow bowling (general KOR). There {{were no significant differences between}} groups in the effects of the two practice conditions upon the acquisition skills. There were no significant groups by tests interaction either. There was, however, a significant trials effect across groups. It was concluded that although both practice conditions resulted in skill acquisition, neither level <b>of</b> <b>precision</b> <b>of</b> knowledge <b>of</b> <b>results</b> was superior to the other...|$|R
40|$|Twelve multivariate {{calibration}} method alternatives are compared {{to establish the}} effect of spectral nonlinearity and collinearity on accuracy and <b>precision</b> <b>of</b> determined <b>results.</b> Simulated and real spectral data are used in this research. This study can help us to select an optimum method for determination. Twelve multivariate {{calibration method}} alternatives are compared to establish the effect of spectral nonlinearity and collinearity on accuracy and <b>precision</b> <b>of</b> determined <b>results.</b> Simulated and real spectral data are used in this research. This study can help us to select an optimum method for determination. (C) 1999 Elsevier Science B. V. All rights reserved...|$|R
50|$|The {{quality of}} {{individual}} AQC efforts can be variable {{depending on the}} training, professional pride, and importance of a particular project to a particular analyst. The burden of an individual analyst originating AQC efforts can be lessened through the implementation of quality assurance programs. Through the implementation of established and routine quality assurance programs, two primary functions are fulfilled: the determination of quality, and the control of quality. By monitoring the accuracy and <b>precision</b> <b>of</b> <b>results,</b> the quality assurance program should increase confidence in {{the reliability of the}} reported analytical results, thereby achieving adequate AQC.|$|E
50|$|The first aids to {{computation}} were purely mechanical devices {{which required}} the operator {{to set up}} the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form, for instance distance along a scale, rotation of a shaft, or a voltage. Numbers could also be represented in the form of digits, automatically manipulated by a mechanical mechanism. Although this approach generally required more complex mechanisms, it greatly increased the <b>precision</b> <b>of</b> <b>results.</b> A series of breakthroughs, such as miniaturized transistor computers, and the integrated circuit, caused digital computers to largely replace analog computers. The cost of computers gradually became so low that by the 1990s, personal computers, and then, in the 2000s, mobile computers, (smartphones and tablets) became ubiquitous in industrialized countries.|$|E
50|$|Explicitly {{supplied}} context effectively {{increases the}} <b>precision</b> <b>of</b> <b>results,</b> however, these search services tend {{to suffer from}} poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps. Google has an extensive list of search operators that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words. Bing also uses a similar set of search operators to assist users in explicitly narrowing down {{the context of their}} queries. Bing allows users to search within a time range, by file type, by location, language, and more.|$|E
30|$|In this study, the <b>precision</b> <b>of</b> the <b>results</b> were {{evaluated}} by the pooled standard deviation, and relative standard deviation <b>of</b> the <b>results</b> <b>of</b> nine measurements for a given bulk sample (i.e. three samples (n[*]=[*] 3) and triplicate readings for each sample).|$|R
30|$|Precision can be {{determined}} by standard deviation, variance, coefficient of variance, relative standard deviation, and range of series measurements (Miller and Miller 2000). In this study the <b>precision</b> <b>of</b> the <b>results</b> were evaluated by the pooled standard deviation and relative standard deviation <b>of</b> the <b>results</b> <b>of</b> triplicate samples and three reading (n =  9) obtained for each sample. The <b>result</b> <b>of</b> analysis was reported with corresponding standard deviation at 95  % confidence limit and relative standard deviation. It {{can be seen that}} the values of percentage relative standard deviations (% RSD) are less than 10  % for all the mean concentrations. This shows the <b>precision</b> <b>of</b> the <b>results</b> obtained by this method is good and acceptable.|$|R
40|$|A new {{approach}} is proposed {{in this paper}} to detect the stuck faults in linear analog circuits. Ideal switches are inserted to indicate stuck-at, bridging and stuck-open locations. Then the resulting circuit is analyzed and stuck faults are directly identified. A recently developed method for multiple analog fault diagnosis is used eliminating a need for fault dictionary approach. The effect of locating stuck-at, bridging and stuckopen faults is modeled with full <b>precision</b> <b>of</b> <b>resulting</b> test conditions. An analog IC- μA 741 is given as an example. 1...|$|R
40|$|The {{accuracy}} and <b>precision</b> <b>of</b> <b>results</b> of the radionuclide analyses in environmental samples are widely claimed internationally {{due to its}} consequences in the decision process coupled to evaluation of environmental pollution, impact, internal and external population exposure. These characteristics of measurement of the laboratories can be shown clearly using intercompariso...|$|E
40|$|This {{paper is}} {{concerned}} with the manner in which an investigation by simulated experimentation is conducted, after the model has been formulated and the computer program completed. It considers the problem of estimating the precision of the results obtained, and methods of improving the <b>precision</b> <b>of</b> <b>results.</b> Alternative procedures are discussed and recommendations are given. ...|$|E
40|$|Two {{computer}} {{programs for the}} analysis of computation of gravity anomalies are presented. The theory applied is outlined. The computations include: transformations from instrumental data to gravity units, tide corrections, drift corrections, theoretical gravity and gradients and free air and Bouger anomalies. The reference system is the GRS 80. The <b>precision</b> <b>of</b> <b>results</b> is at the level of some microgals. Peer reviewe...|$|E
50|$|This {{integral}} {{can readily}} be evaluated numerically using the trapezium rule, which converges exponentially in this case. That {{means that the}} <b>precision</b> <b>of</b> the <b>result</b> doubles {{when the number of}} nodes is doubled. In routine cases, this is bypassed by Sylvester's formula.|$|R
40|$|A natural {{language}} interface system for an Internet search engine shows substantial increases in the <b>precision</b> <b>of</b> query <b>results</b> {{and the percentage of}} queries answered correctly. The system expands queries based on a word-sense-disambiguation method and postprocesses retrieved documents to extract only the parts relevant to a query...|$|R
40|$|In this article, we analyze <b>results</b> <b>of</b> a {{study with}} 15 {{university}} {{students about the}} way binomial and normal distributions are constructed and how probabilities are calculated through computer simulation. The connection with theoretical probability distributions, the analysis of low probability values {{and the effect of}} the number of simulations on the <b>precision</b> <b>of</b> the <b>results</b> are highlighted. We chose the software Fathom for its cognitive potential and flexibility to simulate random events. The results indicate that the simulation of distributions was relatively simple for the students, who showed an understanding of the effect of the number of simulations on the <b>precision</b> <b>of</b> the <b>results,</b> an ability to interpret the calculated probabilities with more facility than when used probability tables and formulas, and seemed to overcome their misconception that values that do not appear in probability tables are impossible to happen...|$|R
40|$|The era of {{automation}} in haematology, although {{improving the}} accuracy and <b>precision</b> <b>of</b> <b>results,</b> has also introduced the laboratory haematologist to {{a vast array}} of spurious parameters. The identification of these results is important so that inappropriate management decisions are avoided. The case presented here illustrates a spuriously raised automated platelet count resulting from bacterial overgrowth in the blood sample...|$|E
40|$|The {{advanced}} {{development of}} various technologies on social network, e-commerce and online education {{has contributed to}} an increasing amount of large-scale network data. Among all sorts of network analysis tasks, one basic task is to search important nodes in a network. Closeness centrality {{is one of the}} popular metrics which measure the importance of a node in a network. Based on the closeness centrality, the basic task is called top-k closeness centrality search. However, the existing exact approaches cannot process large-scale networks because of their polynomial time complexity. Recently, some approximation algorithms are proposed, which achieve high performance by sacrificing the <b>precision</b> <b>of</b> <b>results.</b> But according to our study, we find that the loss of the <b>precision</b> <b>of</b> <b>results</b> is too much. To improve the <b>precision</b> <b>of</b> <b>results</b> while maintaining the high performance, in this paper, we propose a Sketch-based approximation algorithm for fast searching top-k closeness centrality in a large-scale network. The new algorithm is developed based on a new computation method which calculates the centrality by estimating the number of nodes within a certain distance by a data structure called FM-Sketch. The new algorithm has time complexity O(m tD), where t is a constant, D is the diameter of a network and m is the number of edges in a network. With the small-world phenomenon assumption, the Sketch-based algorithm is a linear algorithm. Finally, we compare our Sketch-based algorithm with the state-of-the-art exact and approximation algorithms through extensive experiments. The results demonstrate the advantages of the new solution...|$|E
40|$|This {{article is}} aimed on finding {{the best method}} of {{measuring}} a force in professional defence. We chose direct punch for our experiment, {{because it is a}} basic defence technique in majority of martial arts. There are many methods for measuring but we decided for strain gauges. In this article we compare two strain gauges and we find the best choice according to function, simplicity of measuring and <b>precision</b> <b>of</b> <b>results...</b>|$|E
5000|$|Calculating the {{derivatives}} of a polynomial is straightforward. One {{must also}} be able to calculate the first and second derivatives of f(x). Remez's algorithm requires an ability to calculate , , and [...] to extremely high precision. The entire algorithm must be carried out to higher precision than the desired <b>precision</b> <b>of</b> the <b>result.</b>|$|R
40|$|When {{analyzing}} {{a program}} via an abstract interpretation framework {{we would like}} to analyze the program in a context-sensitive interprocedural manner. Analyzing the program in a manner that considers interprocedural flow can lead to much more accurate results than local or context-insensitive analyses. However, the computational cost (both time and memory consumption) associated with context-sensitive interprocedural analysis techniques makes them infeasible for all but very small programs or simple domains. This paper presents several novel, domain independent, heuristics for reducing the cost of analyzing a program in a contextsensitive manner while having a small impact on the <b>precision</b> <b>of</b> the <b>results.</b> The heuristics are motivated by observations about fundamental properties of software design and the semantics of iterative dataflow analysis. We validate the effectiveness of the heuristics via experimental evaluation which shows both good scalability and high <b>precision</b> <b>of</b> the <b>results...</b>|$|R
40|$|We have {{developed}} a method of evaluating for hfs recordings obtained by a photoelectric Fabry-Perot. This method utilizes the informations contained in the recordings to a higher degree than the usual ones. Thus the <b>precision</b> <b>of</b> the <b>resulting</b> data increases {{and the range of}} application of the optical investigations is enlarged. A short outline of the computer-program is given and some examples show the <b>precision</b> <b>of</b> derived data...|$|R
40|$|The {{effects of}} discreteness {{arising from the}} use of the N-body method on the {{accuracy}} of simulations of cosmological structure formation are not currently well understood. In {{the first part of this}} paper we discuss the essential question of how the relevant parameters introduced by this discretisation should be extrapolated in convergence studies if the goal is to recover the Vlasov-Poisson limit. In the second part of the paper we study numerically, and with analytical methods we have developed recently, the central issue of how finite particle density affects the <b>precision</b> <b>of</b> <b>results</b> above the force smoothing scale. In particular we focus on the <b>precision</b> <b>of</b> <b>results</b> for the power spectrum at wavenumbers around and above the Nyquist wavenumber, in simulations in which the force resolution is taken smaller than the initial interparticle spacing. Using simulations of identical theoretical initial conditions sampled on four different “pre-initial ” configurations (three different Bravais lattices, and a glass) we obtain a lower bound on the real discreteness error. With the guidance of our analytical results, which match extremely well this measured dispersion into the weakl...|$|E
30|$|Since {{the bush}} canopy area {{coverage}} {{was the main}} aspect of the study, plot area was taken as a parameter. Comparatively much larger trial plots were selected {{in order to minimize}} noise and improve the <b>precision</b> <b>of</b> <b>results</b> and thus the credibility of the study. Measurements were taken plot-wise. This experiment was done in a VP tea where all tea bushes in a plot are descendants of a single mother plant and hence genetically alike. Any variations observed would have been due to varying environmental conditions.|$|E
40|$|In {{examining}} {{the study of}} government performance, this paper asks whether field experiments can improve the explanatory <b>precision</b> <b>of</b> <b>results</b> generated by public opinion surveys. Survey research on basic health and education services sub-Saharan Africa shows that the perceived 'user friendliness' (or ease of use) of services drives popular evaluations of government performance. For the reliable attribution of causality, however, surveys and field experiments, combined {{in a variety of}} mixed research designs, are more rigorous and reliable than either method alone. The paper proposes a menu of such designs...|$|E
40|$|This paper {{describes}} FLAVERS, a finite-state verification {{approach that}} analyzes whether concurrent systems satisfy user-defined, behavioral properties. FLAVERS automatically creates a compact, event-based {{model of the}} system that supports efficient data-flow analysis. FLAVERS achieves this efficiency at the cost <b>of</b> <b>precision.</b> Analysts, however, can improve the <b>precision</b> <b>of</b> analysis <b>results</b> by selectively and judiciously incorporating additional semantic information into an analysis...|$|R
40|$|Abstract—We {{consider}} a troublesome form of non-isoplanatism in synthesis radio telescopes: non-coplanar baselines. We present a novel {{interpretation of the}} non-coplanar baselines effect as being due to differential Fresnel diffraction {{in the neighborhood of}} the array antennas. We have developed a new algorithm to deal with this ef-fect. Our new algorithm, which we call “W-projection”, has markedly superior performance compared to existing algorithms. At roughly equivalent levels of accuracy, W-projection can be up to an order of magnitude faster than the corresponding facet-based algorithms. Furthermore, the <b>precision</b> <b>of</b> <b>result</b> is not tightly coupled to computing time. W-projection has important consequences for the design and operation of the new generation of radio telescopes operating at centimeter and longer wavelengths. I...|$|R
40|$|A {{systematic}} review is {{an overview of}} primary studies that used explicit and reproducible methods. A meta-analysis is a mathematical synthesis <b>of</b> the <b>results</b> <b>of</b> two or more primary studies that addressed the same hypothesis in the same way. Although meta-analysis can increase the <b>precision</b> <b>of</b> a <b>result,</b> {{it is important to}} ensure that the methods used for the review were valid and reliable...|$|R
40|$|Abstract—The paper {{presents}} {{some specific}} methods used at testing of MEMS materials, which impose other rules and conditions as classical materials. MEMS materials are specific properties, {{such as their}} fragility, easy damages or fractures and small size of specimen at handling and testing, which needs special technique and installations adapted these conditions. The main characteristics of MEMS materials are internal stress, Young’s modulus, Poisson’s ratio, fracture strength and toughness, and thermal conductivity and specific heat. The accuracy of measurements required determination of high <b>precision</b> <b>of</b> <b>results</b> that {{are affected by the}} specimen boundary or specimen shape, and specific approaches methods...|$|E
40|$|For {{electric}} vehicular propulsion {{the generation}} of electricity can be made using systems based on Proton Exchange Membrane Fuel Cells, that allow vehicle operation with zero emissions in urban environments. It is often advantageous to couple these Fuel cells with supercapacitors for improved performance. For correnct design of different parts of this hybrid generation system {{it is important to}} have suitable models for all components, to be insterted in simulation programs. This paper is intended to propose these models, posing the maximum care in choosing the right complexity level to obtain a good balance between model manageability and <b>precision</b> <b>of</b> <b>results...</b>|$|E
40|$|The {{research}} report introduces design of localization methods for mobile robots. All presented methods employ laser range finder sensor. Two of them {{work on the}} principle position tracking and the other localization method employs external laser range finder {{in order to determine}} absolute position of the mobile robot. Utilization of an environment model can meaningfully improve <b>precision</b> <b>of</b> <b>results</b> of position tracking methods. Therefore, one method for a building of the simple line model is also introduced. All suggested methods have been experimentally verified and the achieved results are presented and discussed. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
5000|$|Because [...] and [...] can be {{very similar}} numbers, {{cancellation}} {{can lead to the}} <b>precision</b> <b>of</b> the <b>result</b> to be much less than the inherent <b>precision</b> <b>of</b> the floating-point arithmetic used to perform the computation. Thus this algorithm should not be used in practice. This is particularly bad if the standard deviation is small relative to the mean. However, the algorithm can be improved by adopting the method of the assumed mean.|$|R
30|$|The {{appropriate}} partitioning of {{a program}} into program scopes depends on the concrete testing goal. For example, in case of our research on measurement-based timing analysis [7] we use the partitioning of the program into scopes to achieve a compromise between <b>precision</b> <b>of</b> measurement <b>results</b> (the larger the segments the more precise) and number of necessary measurements.|$|R
40|$|We {{demonstrate}} {{a system for}} querying probabilistic XML documents with simple XPath queries. A user chooses between a variety of query answering techniques, both exact and approximate, and observes the running behavior, pros, and cons, of each method, in terms <b>of</b> efficiency, <b>precision</b> <b>of</b> the <b>result,</b> and data model and query language supported. Categories and Subject Descriptor...|$|R
