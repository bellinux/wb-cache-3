20|493|Public
40|$|This article takes under {{consideration}} a notorious problem area for Greek Italian and Spanish Greek translators, {{caused by the}} gap of standardized international academic terminology, a situation that creates incongruity: 1) between the Italian and the Greek terms and 2) between the Spanish and the Greek terms, as Greek institutional academic terms have potential Italian and Spanish equivalents with a similar core but different edges. Cet article <b>prend</b> <b>en</b> <b>compte</b> la problématique des traducteurs du grec vers l’italien et de l’espagnol vers le grec par rapport à l’insuffisance de la terminologie universitaire. Cette situation pose des difficultés dans les équivalences entre les langues...|$|E
40|$|International audienceWe {{present a}} novel algorithm, SufPref, {{computing}} an exact pvalue for Hidden Markov models (HMM). The algorithm inductively traverses specific data structure, the overlap graph. Nodes of the graph {{are associated with}} the overlaps of words from a given set H. Edges are associated to the prefix and suffix relations between ovelaps. An originality of our data structure is that pattern H need not be explicitly represented in nodes or leaves. The algorithm relies on the Cartesian product of the overlap graph and the graph of HMM states; the approach is analogous to a weighted automaton approach. The gain in size of SufPref data structure leads to significant space and time complexity improvements. We suppose that all words in the pattern H are of the same length m. The algorithm SufPref was implemented as a C++ program; it can be used both as Web-server and a stand alone program for Linux and Windows. Cet article présente un nouvel algorithme, SufPref, qui calcule la pvaleur pour un ensemble de mots qui <b>prend</b> <b>en</b> <b>compte</b> les modèles de Markov cachés (HMM). Il est implémenté en C++ et peut être utilisé en ligne ou téléchargé...|$|E
40|$|This article {{presents}} a technique for modeling sound propagation from an airgun array, using the parabolic equation (PE) method, {{that takes into}} full account the far-field, angle-dependent radiation pattern of the array. This is achieved by generating a PE starting field for the array by summing together shaded, phase-shifted replicas of the PE self-starter. The array starter has been implemented using the RAM parabolic equation model. A validation comparison is presented of field predictions generated using the array starter against exact normal mode solutions for an array source computed using the ORCA model. Examples of synthetic waveform airgun array calculations performed using the array starter are also provided. The method {{presented in this article}} can be used to accurately predict pressure waveforms from an airgun array in the ocean environment provided that the modeler knows (or can compute) far-field source signatures for individual airguns in the array. s o m m a i r e Cet article présente une technique permettant de modéliser la propagation du son provenant dun réseau de canons à air, en utilisant la méthode de léquation parabolique (EP), qui <b>prend</b> <b>en</b> <b>compte</b> le patron de directivité du réseau en champ lointain. Cette approche est réalisée en créant un champ initial pour l...|$|E
5000|$|<b>En</b> <b>comptes</b> de la lletera. Barcelona: La Seca. Espai Brossa, 2012 ...|$|R
40|$|Complex 3 D digital {{objects are}} used in many domains such as {{animation}} films, scientific visualization, medical imaging and computer vision. These objects are usually represented by triangular meshes with many triangles. The simplification of those objects {{in order to keep}} them {{as close as possible to}} the original has received a lot of attention in the recent years. In this context, we propose a simplification algorithm which is focused on the accuracy of the simplifications. The mesh simplification uses edges collapses with vertex relocation by minimizing an error metric. Accuracy is obtained with the two error metrics we use: the Accurate Measure of Quadratic Error (AMQE) and the Symmetric Measure of Quadratic Error (SMQE). AMQE is computed as the weighted sum of squared distances between the simplified mesh and the original one. Accuracy of the measure of the geometric deviation introduced in the mesh by an edge collapse is given by the distances between surfaces. The distances are computed in between sample points of the simplified mesh and the faces of the original one. SMQE is similar to the AMQE method but computed in the both, direct and reverse directions, i. e. simplified to original and original to simplified meshes. The SMQE approach is computationnaly more expensive than the AMQE but the advantage of computing the AMQE in a reverse fashion results in the preservation of boundaries, sharp features and isolated regions of the mesh. For both measures we obtain better results than methods proposed in the literature. Les objets numériques 3 D sont utilisés dans de nombreux domaines, les films d'animations, la visualisation scientifique, l'imagerie médicale, la vision par ordinateur [...] Ces objets sont généralement représentés par des maillages à faces triangulaires avec un nombre énorme de triangles. La simplification de ces objets, avec préservation de la géométrie originale, a fait l'objet de nombreux travaux durant ces dernières années. Dans cette thèse, nous proposons un algorithme de simplification qui permet l'obtention d'objets simplifiés de grande précision. Nous utilisons des fusions de couples de sommets avec une relocalisation du sommet résultant qui minimise une métrique d'erreur. Nous utilisons deux types de mesures quadratiques de l'erreur : l'une uniquement entre l'objet simplifié et l'objet original (Accurate Measure of Quadratic Error (AMQE)) et l'autre <b>prend</b> aussi <b>en</b> <b>compte</b> l'erreur entre l'objet original et l'objet simplifié ((Symmetric Measure of Quadratic Error (SMQE)). Le coût calculatoire est plus important pour la seconde mesure mais elle permet une préservation des arêtes vives et des régions isolées de l'objet original par l'algorithme de simplification. Les deux mesures conduisent à des objets simplifiés plus fidèles aux originaux que les méthodes actuelles de la littérature...|$|R
5000|$|French singer Serge Gainsbourg says « Je dirai en {{substance}} ceci : « touchez pas au dragon chasing, chasse au dragon qui se <b>prend</b> <b>en</b> shoot ou en shit » in {{the song}} [...] "Aux enfants de la chance" ...|$|R
40|$|The {{objective}} of this thesis is to develop models and algorithms to simulate efficiently the mass exchanges occurring at the interface between the nuclear waste deep geological repositories and the ventilation excavated galleries. To model such physical processes, one needs to account in the porous medium for {{the flow of the}} liquid and gas phases including the vaporization of the water component in the gas phase and the dissolution of the gaseous components in the liquid phase. In the free flow region, a single phase gas free flow is considered assuming that the liquid phase is instantaneously vaporized at the interface. This gas free flow has to be compositional to account for the change of the relative humidity in the free flow region which has a strong feedback on the liquid flow rate at the interface. L'objectif de cette thèse est de fournir des modèles et des outils de simulation pour décrire les échanges de masse entre les circuits de ventilation (galeries) et les milieux poreux des ouvrages souterrains d'enfouissement des déchets nucléaires. La modélisation <b>prend</b> <b>en</b> <b>compte</b> le couplage à l'interface poreux-galerie entre les écoulements liquide gaz compositionnels dans le milieu poreux constituant le stockage et les écoulements gazeux compositionnels dans le milieu galerie libre...|$|E
40|$|Valérie PERRIER (Présidente du jury), Yvon MADAY (Rapporteur), Grégory PANASENKO (Rapporteur), Denis CAILLERIE (Directeur de thèse), Annie RAOULT (Directeur de thèse), Yves USSON (Examinateur). MY THESIS FOCUSES ON THE GEOMETRICAL, MECHANICAL AND NUMERICAL MODELLING OF THE HUMAN HEART. IN THE GEOMETRICAL PART, WE CHECKED A CONJECTURE ACCORDING TO WHICH MYOCARDIAL FIBRES RUN AS GEODESICS ON A NESTED SET OF SURFACES. TO THIS AIM, THE FIBRE TRAJECTORIES AND LAYERS OF FIBRES HAVE BEEN IDENTIFIED. IN THE MECHANICAL MODELLING, WE ESTABLISHED A NEW MACROSCOPIC CONSTITUTIVE LAW FOR THE MYOCARDIUM BY MEANS OF DISCRETE HOMOGENIZATION TECHNIQUE FROM GEOMETRICAL AND MECHANICAL MICROSCOPIC INFORMATION ON MYOCARDIAL CELLS. THIS CONSTITUTIVE LAW TAKES INTO ACCOUNT THE FIBROUS STRUCTURE OF THE MUSCLE. MOREOVER, WE USED OUR METHOD OF HOMOGENIZATION IN MODELLING THE MECHANICAL BEHAVIOUR OF CARBON NANOTUBES. DANS MON TRAVAIL DE THESE JE M'INTERESSE A LA MODELISATION GEOMETRIQUE, MECANIQUE ET NUMERIQUE DU MYOCARDE. LA PARTIE GEOMETRIQUE CONSISTE A VERIFIER UNE CONJECTURE SELON LAQUELLE LES FIBRES MYOCARDIQUES COURENT COMME DES GEODESIQUES SUR DES SURFACES EMBOITEES. POUR CELA, LES TRAJECTOIRES ET LES SURFACES DE FIBRES ONT ETE IDENTIFIEES. DANS LA PARTIE MECANIQUE, NOUS AVONS ETABLI UNE NOUVELLE LOI DE COMPORTEMENT MACROSCOPIQUE DU MYOCARDE PAR UNE TECHNIQUE D'HOMOGENEISATION DISCRETE A PARTIR DE LA DESCRIPTION MICROSCOPIQUE DE L'ARRANGEMENT DES CELLULES CARDIAQUES ET DE LEUR COMPORTEMENT MECANIQUE INDIVIDUEL. CETTE LOI DE COMPORTEMENT <b>PREND</b> <b>EN</b> <b>COMPTE</b> LA STRUCTURE FIBREUSE. DE PLUS, NOUS AVONS APPLIQUE NOTRE METHODE D'HOMOGENEISATION AUX NANOTUBES DE CARBONE...|$|E
40|$|Abstract- Modelling pelagic {{food web}} and primary {{production}} in the English Channel. This paper deals with {{a model of the}} pelagic ecosystem in the English Channel. The model takes into account nitrogen biogeochemical cycle explicitly and indirectly phosphorus, silicon and carbon ones. The pelagic ecosystem is described precisely, on the one hand with the help of three phytoplanktonic compartments, and the other by considering the microbial food web. Primary production appears to be mainly controlled by stratification and turbidity, except in the Bay of Seine where high terrestrial nutrient inputs generate a sharp enrichment. Moreover, the model allows some general ecological conclusions to be drawn. From the simulation, it appears that inorganic nitrogen in the water column is mainly produced by an intense recycling. In accordance with in situ measurements, these results tend to demonstrate the predominance of the regenerated production over the new one in the English Channel. Simulation of nitrogen fluxes on an annual basis underlines the importance of bacterial remineralization, underestimating microzooplanktonic excretion in the recycling of elements. Furthermore, exchanges between pelagic and benthic compartments appear to be weak. 0 Elsevier, Paris English Channel / ecological model / primary production / nitrogen fluxes R&urn 6 - Cet article presente un modble de la production pelagique en Manche. I 1 <b>prend</b> <b>en</b> <b>compte</b> explicitement le cycle biogeochimique de l’azote et, de man&e indirecte ceux du phosphore, du silicium et du carbone. L’ecosystbme pelagique est d&it de man&e precise, d’une part a l’aide de trois compartiments phytoplanctoniques, d’autre part par la prise e...|$|E
5000|$|... « En raison de la gravité de la situation, je <b>prends</b> <b>en</b> mon nom {{personnel}} les pouvoirs civils et militaires. Un comité restreint, placé sous l'autorité du chef de l'État, aura pour tâche le rétablissement de l'ordre, la reprise du travail et {{la mise en place}} des réformes qui s'imposent. » ...|$|R
40|$|Towards an {{industrial}} economy of banking alliances ? In order {{to broaden the}} scope of investigation of industrial banking economy, this article aims at checking if the analytical body of industrial alliances {{can serve as a}} reference for the understanding of banking alliances. While banking alliances seem to be an appropriate answer to the new competitive environment, their interpretation by the theoretical body of industrial cooperation shows {{that it is possible to}} borrow concepts from industrial economy and to apply them to financial intermediaries. For example, it has been possible to outline a typology and to search a theoretical explanation for alliances between banks based on ideas originally developed in actual industries. This conceptual transfer needs, however, to adapt actual tools to the specificity of banking activity. Therefore, the approach in terms of transaction cost seems more appropriate to the explanation of vertical and horizontal alliances. In the same way, the analysis in terms of complementariness, which finds in network and know-how a satisfactory field of application, does not take into account some incentives, like the will to share risks. As regards interpretation by the learning process, the increasing importance attached to innovation and technology in industrial cooperation makes the adaptation effort even stronger. seems more appropriate to the explanation of vertical and horizontal alliances. In the same way, the analysis in terms of complementariness, which finds in network and know-how a satisfactory field of application, does not take into account some incentives, like the will to share risks. As regards interpretation by the learning process, the increasing importance attached to innovation and technology in industrial cooperation makes the adaptation effort even stronger. Dans le but d'élargir le champ d'investigation de l'économie industrielle bancaire, cet article cherche à vérifier si le corps analytique des alliances industrielles peut servir de référence pour comprendre les alliances bancaires. Si les alliances bancaires semblent être une réponse appropriée au nouveau contexte concurrentiel, leur interprétation par le corps théorique de la coopération industrielle montre qu'il est possible d'emprunter des concepts à l'économie industrielle et de les appliquer aux intermédiaires financiers. À titre d'exemple, il a été possible de dresser une typologie et de rechercher des explications théoriques des alliances entre banques à partir de notions originellement développées dans les industries réelles. Ce transfert conceptuel nécessite cependant d'adapter les outils de la sphère réelle à la spécificité de l'activité bancaire. Ainsi, l'approche en termes de coût de transaction semble mieux appropriée à l'explication des alliances verticales qu'horizontales. De même, l'analyse en termes de complémentarité qui trouve un terrain d'application satisfaisant en matière de réseau et de savoir-faire ne <b>prend</b> pas <b>en</b> <b>compte</b> certaines motivations comme la volonté de partager les risques. En ce qui concerne l'interprétation par le processus d'apprentissage, l'importance plus grande accordée à l'innovation et à la technologie dans la coopération industrielle rend plus intense l'effort d'adaptation. Minda Alexandre, Paguet Jean-Michel, Mérieux Antoine, Marchand Christophe. Vers une économie industrielle des alliances bancaires ?. In: Revue d'économie financière, n° 35, 1995. La tarification des services bancaires. pp. 181 - 206...|$|R
40|$|CE MEMOIRE EST CONSACRE A LA CONCEPTION D'UN SUPPORT DE COMMUNICATION SUR DE FONCTIONNEMENT, A TEMPS D'ACCES BORNE ET FAIBLE, POUR SYSTEMES DISTRIBUES DE SURVEILLANCE ET SECURITE. L'ANALYSE EFFECTUEE CONDUIT A UNE APPROCHE PAR DOUBLE DECOMPOSITION INTEGRANT LA VALIDATION PROGRESSIVE DES CHOIX: LA CONCEPTION EST MENEE PAR AFFINEMENTS SUCCESSIFS, CONJOINTEMENT SUR PLUSIEURS NIVEAUX D'ABSTRACTION AFIN DE PRENDRE <b>EN</b> <b>COMPTE</b> L'ENSEMBLE DES CONTRAINTES ET DE LEURS INTERACTIONSIndisponibl...|$|R
40|$|International audienceThe {{increasing}} {{price of}} energies transforms {{the geography of}} mobilities. This issue is very sensitive in Oceania because societies are strongly dependant to air transport technologies. This paper offers a methodology to map the air transport framework according to “energy-space” concept. This method is {{taking into account the}} mains factors of energy consumption of air transport. In this scope, it emphasizes the distinctiveness of the situation in Oceania that increases the vulnerability to energy cost increasing. The energy-space concept that is developed by the way of this new mapping tool implies and allows thinkinggeographical space through a new approach that improves the current concepts (metric space, time-space, social-space [...] .). Le renchérissement des énergies transforme la géographie des mobilités. Cette problématique est particulièrement sensible en Océanie en raison de l'extrême dépendance des sociétés insulaires au système de transport aérien. Cet article propose une méthode permettant de cartographier le réseau de transport aérien selon la notion de distance-énergie. Cette méthode <b>prend</b> <b>en</b> <b>compte</b> les principaux facteurs de consommation du transport aérien et met en évidence les spécificités océaniennes qui, dans ce domaine, se révèlent être des facteurs aggravant de vulnérabilité. Le concept d'espace-énergie qui est proposé au travers de cet outil cartographique nouveau implique et permet de penser l'espace avec une nouvelle approche qui complète et enrichit les conceptions actuelles (distance métrique, distance-temps, espace social [...] .). Abstract. The increasing price of energies transforms the geography of mobilities. This issue is very sensitive in Oceania because societies are strongly dependant to air transport technologies. This paper offers a methodology to map the air transport framework according to " energy-space " concept. This method is taking into account the mains factors of energy consumption of air transport. In this scope, it emphasizes the distinctiveness of the situation in Oceania that increases the vulnerability to energy cost increasing. The energy-space concept that is developed by the way of this new mapping tool implies and allows thinking geographical space through a new approach that improves the current concepts (metric space, time-space, social-space [...] .) ...|$|E
40|$|AFIN DE SIMULER L'INFLUENCE DES APPORTS FLUVIAUX EN ZONE COTIERE, UN MODELE SPATIALISE DE LA PRODUCTION PRIMAIRE SUR LE PLATEAU CONTINENTAL ATLANTIQUE FRANCAIS A ETE DEVELOPPE. LE MODELE HYDRODYNAMIQUE TRIDIMENSIONNEL A ETE DEVELOPPE A L'IFREMER. LE CALCUL DE LA TEMPERATURE ET DE LA SALINITE <b>PREND</b> <b>EN</b> <b>COMPTE</b> L'ADVECTION ET LA DIFFUSION INDUITES PAR LA MAREE, LE VENT ET LES COURANTS DE DENSITE. LE COUPLAGE AVEC LE SOUS-MODELE BIOLOGIQUE SE FAIT PAR L'INTERMEDIAIRE DU TRANSPORT, DE LA DIFFUSION ET DES FLUX DE CHALEUR. LE MODELE BIOLOGIQUE DECRIT LES CYCLES DE L'AZOTE, DU PHOSPHORE ET DU SILICIUM AU TRAVERS DU RESEAU TROPHIQUE. UN MODELE SPECIFIQUE DE L'ESPECE GYMNODINIUM MIKIMOTOI A ENSUITE ETE INTRODUIT. LA PARAMETRISATION SPECIFIQUE A CETTE ESPECE EST FONDEE SUR UNE ETUDE BIBLIOGRAPHIQUE ET SUR DES EXPERIENCES EN LABORATOIRE. LA NOUVEAUTE DANS CETTE APPROCHE CONSISTAIT A INTRODUIRE UN MODELE DE DYNAMIQUE SIMPLE, POUR UNE ESPECE PHYTOPLANCTONIQUE PARTICULIERE, DANS UN MODELE TRIDIMENSIONNEL PHYSICO-BIOLOGIQUE DE BIOMASSE GLOBALE. LA CALIBRATION ET LA VALIDATION DU MODELE ONT ETE REALISEES GRACE AUX RESULTATS DES DIFFERENTES CAMPAGNES OCEANOGRAPHIQUES ET AUX MESURES DES RESEAUX DE SURVEILLANCE DE L'IFREMER : REPHY ET RNO. CETTE ETAPE S'EST EGALEMENT APPUYEE SUR L'UTILISATION DES IMAGES SATELLITAIRES SEAWIFS DE LA COULEUR DE L'EAU. LA SIMULATION DE LA STRUCTURE PHYSIQUE ET DES PRINCIPAUX PROCESSUS BIOLOGIQUES FOURNIT UNE VUE SYNOPTIQUE DES RELATIONS ENTRE LES PHENOMENES PHYSIQUES ET BIOLOGIQUES A MESO-ECHELLE SUR LE PLATEAU CONTINENTAL. LE MODELE SIMULE AINSI, A UNE ECHELLE SAISONNIERE ET INTER ANNUELLE, LES CYCLES DES SELS NUTRITIFS ET DU PHYTOPLANCTON, EN REPONSE AUX FORCAGES PHYSIQUES ET AUX APPORTS FLUVIAUX. LE MODELE MET AINSI EN EVIDENCE L'INFLUENCE DES UPWELLINGS, DES FRONTS ET DES HAUTS FONDS SUR LA PRODUCTION PRIMAIRE. CE MODELE A AUSSI PERMIS DE MIEUX CONNAITRE LE DEGRE D'INFLUENCE DE LA LOIRE ET DE LA GIRONDE, EN ESTIMANT LA PART DE LA PRODUCTION PRIMAIRE LIEE AUX APPORTS ANTHROPIQUES DE CES DEUX FLEUVES, PART QU'IL EST IMPOSSIBLE D'ESTIMER A PARTIR DES MESURES IN-SITU. ENFIN, L'ANALYSE DES SIMULATIONS SUR 10 ANS SOULIGNE LE ROLE DETERMINANT DU VENT SUR LA STRUCTURE VERTICALE DE LA COLONNE D'EAU, SUR L'EXTENSION OU LE CONFINEMENT DES PANACHES ET PAR CONSEQUENT SUR LA PRODUCTION PRIMAIRE. PARIS-BIUSJ-Thèses (751052125) / SudocBANYULS/MER-Observ. Océanol. (660162201) / SudocCentre Technique Livre Ens. Sup. (774682301) / SudocPARIS-BIUSJ-Physique {{recherche}} (751052113) / SudocSudocFranceF...|$|E
40|$|The {{potential}} of the social capital in the Local Action Groups. The study analyses the "social capital" {{of the members of}} the LAGs. The expression "social capital" is defined by the "connections between individuals – the social networks, the norms of reciprocity, and the trust that results from this" (R. Putnam), and consists of three elements. First of all there is the trust that characterises the members of the LAG and which is made up of relationships of trust between people who know each other, an overall attitude of trust, and trust in institutions. The second element is formed of values that can be broken down into factors such as local patriotism, representation of the interests of the partnership, and social activity. The third element is the network. This can be evaluated using factors such as participation in the decision-making processes of the LAG, the cooperation between the members of the LAG belonging to the three sectors, and the frequency of participation in the meetings of the LAG. The motivation {{of the members of the}} LAGs to participate in this structure is taken into account. The results of sociological research into these three elements of social capital are presented here. Empirical research carried out in the voivodeship of Łódź serves as a base for the analyses. L'étude analyse le « capital social » des membres des GAL. L'expression de « capital social » est définie par les « liens entre les individus – les réseaux sociaux, les normes de réciprocité et la confiance qui en résulte » (R. Putnam) et comprend trois composantes. Il s'agit tout d'abord de la confiance qui caractérise les membres du GAL et qui se décompose en relations de confiance envers des personnes connues, de confiance généralisée et de confiance à l'égard des institutions. La seconde composante sont les valeurs qui peuvent être décomposées en variables telles que le patriotisme local, la représentation des intérêts du partenariat, l'activité sociale. La troisième composante est le réseau. Elle peut être évaluée par des variables telles que la participation aux processus de décision du GAL, la coopération des membres du GAL appartenant aux trois secteurs, la fréquence de la participation aux rencontres du GAL. On <b>prend</b> <b>en</b> <b>compte</b> la motivation des membres des GAL à participer à cette structure. Les résultats des recherches sociologiques concernant ces trois composantes du capital social sont ici exposés. Les recherches empiriques menées dans la voïvodie de Łódź servent de base aux analyses...|$|E
40|$|The {{exponential}} increasing of {{the number}} of images requires efficient ways to classify them based on their visual content. The most successful and popular approach is the Bag of visual Word (BoW) representation due to its simplicity and robustness. Unfortunately, this approach fails to capture the spatial image layout, which plays an important roles in modeling image categories. Recently, Lazebnik et al (2006) introduced the Spatial Pyramid Representation (SPR) which successfully incorporated spatial information into the BoW model. The idea of their approach is to split the image into a pyramidal grid and to represent each grid cell as a BoW. Assuming that images belonging to the same class have similar spatial distributions, it is possible to use a pairwise matching as similarity measurement. However, this rigid matching scheme prevents SPR to cope with image variations and transformations. The main objective of this dissertation is to study a more flexible string matching model. Keeping the idea of local BoW histograms, we introduce a new class of edit distance to compare strings of local histograms. Our first contribution is a string based image representation model and a new edit distance (called SMD for String Matching Distance) well suited for strings composed of symbols which are local BoWs. The new distance benefits from an efficient Dynamic Programming algorithm. A corresponding edit kernel including both a weighting and a pyramidal scheme is also derived. The performance is evaluated on classification tasks and compared to the standard method and several related methods. The new method outperforms other methods thanks to its ability to detect and ignore identical successive regions inside images. Our second contribution is to propose an extended version of SMD replacing insertion and deletion operations by merging operations between successive symbols. In this approach, the number of sub regions ie. the grid divisions may vary according to the visual content. We describe two algorithms to compute this merge-based distance. The first one is a greedy version which is efficient but can produce a non optimal edit script. The other one is an optimal version but it requires a 4 th degree polynomial complexity. All the proposed distances are evaluated on several datasets and are shown to outperform comparable existing methods. L'augmentation exponentielle du nombre d'images nécessite des moyens efficaces pour les classer en fonction de leur contenu visuel. Le sac de mot visuel (Bag-Of-visual-Words, BOW), en raison de sa simplicité et de sa robustesse, devient l'approche la plus populaire. Malheureusement, cette approche ne <b>prend</b> pas <b>en</b> <b>compte</b> de l'information spatiale, ce qui joue un rôle important dans les catégories de modélisation d'image. Récemment, Lazebnik ont introduit la représentation pyramidale spatiale (Spatial Pyramid Representation, SPR) qui a incorporé avec succès l'information spatiale dans le modèle BOW. Néanmoins, ce système de correspondance rigide empêche la SPR de gérer les variations et les transformations d'image. L'objectif principal de cette thèse est d'étudier un modèle de chaîne de correspondance plus souple qui prend l'avantage d'histogrammes de BOW locaux et se rapproche de la correspondance de la chaîne. Notre première contribution est basée sur une représentation en chaîne et une nouvelle distance d'édition (String Matching Distance, SMD) bien adapté pour les chaînes de l'histogramme qui peut calculer efficacement par programmation dynamique. Un noyau d'édition correspondant comprenant à la fois d'une pondération et d'un système pyramidal est également dérivée. La seconde contribution est une version étendue de SMD qui remplace les opérations d'insertion et de suppression par les opérations de fusion entre les symboles successifs, ce qui apporte de la souplesse labours et correspond aux images. Toutes les distances proposées sont évaluées sur plusieurs jeux de données tâche de classification et sont comparés avec plusieurs approches concurrente...|$|R
40|$|The {{introduction}} of robots {{in our daily}} lives raises a key issue that is "added" to the "standard challenge" of autonomous robots: the presence of humans in {{the environment and the}} necessity to interact with them. In the factory, the robot is physically separated and a security distance is always kept from human workers. With this separation, the primary concern in such environments, the "safety", is ensured. However this separation cannot be applied to future applications where the robot will be in a situation where it will have to assist humans. In a scenario where the robot has to move among people, the notion of safety becomes more important and should be studied in every detail. Yet the biggest di erence in these two environments does not come from the definition of their primary concern, the safety, but comes from a secondary concern. In factory, when the safety is ensured, the feasibility of the task gains in importance. The robot's environment is perfectly structured and all the robots are perfectly coordinated in order to accomplish their tasks. On the contrary, the feasibility of the task leaves its place to the "comfort" for an interactive robot. For a robot that physically interacts with humans, accomplishing a task with the expense of human comfort is not acceptable even the robot does not harm any person. The robot has to perform motion and manipulation actions and should be able to determine where a given task should be achieved, how to place itself relatively to a human, how to approach him/her, how to hand the object and how to move in a relatively constrained environment by taking into account the safety and the comfort of all the humans in the environment. In this work, we propose a novel motion planning framework answering these questions along with its implementation into a navigation and a manipulation planner. We present the Human-Aware Navigation Planner that takes into account the safety, the elds of view, the preferences and the states of all the humans as well as the environment and generates paths that are not only collision free but also comfortable. We also present the Human-Aware Manipulation Planner that breaks the commonly used human-centric approaches and allows the robot to decide and take initiative about the way of an object transfer takes place. Human's safety, field of view, state, preferences as well as its kinematic structure is taken into account to generate safe and most importantly comfortable and legible motions that make robot's intention clear to its human partner. L'introduction des robots dans la vie quotidienne apporte un problème important qui "s'ajoute" au "défi standard" des robots autonomes : la présence d'hommes dans son environnement et le besoin d'interagir avec eux. Ce travail s'intéresse aux problèmes de l'interaction proche entre humains et robots, en se plaçant du point de vue des décisions de mouvement qui doivent être prises par le robot pour assurer un mouvement sûr, effectif, compréhensible et confortable pour l'homme. On présente un cadre général de planification de mouvement qui <b>prend</b> explicitement <b>en</b> <b>compte</b> la présence de l'homme. Ce cadre est matérialisé par deux planificateurs. Le premier, "Human-Aware Navigation Planner", est un planificateur de navigation qui raisonne sur la sécurité, la visibilité, la posture et les préférences de l'homme pour générer des mouvements sûrs et confortables pour l'homme. Le deuxième, "Human-Aware Manipulation Planner", est un planificateur qui traite les problèmes de transfert d'objet entre l'homme et le robot. Ce planificateur transforme le problème initial de planification de mouvement en un problème beaucoup plus riche de recherche d'un chemin "pour réaliser une tache" fournissant ainsi la possibilité de raisonner à un niveau d'abstraction supérieur. Les deux planificateurs sont intégrés dans deux plates-formes robotiques, Jido et Rackham, et validés à travers des études utilisateurs dans le cadre du projet européen COGNIRON...|$|R
5000|$|... "Et mon coeur <b>en</b> <b>prend</b> plein la gueule" [...] (Daniel DeShaime) — 4:31 ...|$|R
40|$|LOTOS (Language Of Temporal Ordering Specification) is a {{language}} for {{the description of}} concurrent and communicating systems, standardized by ISO and CCITT to allow formal definition of telecommunication protocols and services. LOTOS is based on algebraic abstract types to specify data structures and on a process calculus, close to CSP and CCS, to express control structures. This thesis proposes a compiling technique which allows to translate a significant subset of LOTOS into both interpreted Petri nets (which {{may serve as a}} basis for executable code generation) and finite state automata (which allow formal validation of LOTOS programs, either by reduction and comparison according to equivalence relations, or by evaluation of temporal logics formulas). The method is different from existing approaches, based on term rewriting, that directly build the state graph corresponding to a given LOTOS program. Conversely, translation is achieved by three successive steps (expansion, generation and simulation) dealing with intermediate semantic models (namely SUBLOTOS language and networks). It involves a static and global analysis of program behaviours. It can handle data structures, that have to be compiled by already known algorithms. These compiling principles are fully implemented in the software tool CAESAR. Its demonstrated performances confirm the interest of the approachLOTOS (Language Of Temporal Ordering Specification) est un langage de description de systemes paralleles communicants, normalise par l'ISO et le CCITT afin de permettre la definition formelle des protocoles et des services de telecommunications. Le langage utilise des types abstraits algebriques pour specifier les donnees et un calcul de processus proche de CSP et CCS pour exprimer le controle. Cette these propose une technique de compilation permettant de traduire un sous-ensemble significatif de LOTOS vers un modele reseau de Petri interprete (pouvant servir a produire du code executable) puis vers un modele automate d'etats finis (permettant la verification formelle de programmes LOTOS soit par reduction ou comparaison modulo des relations d'equivalence, soit par evaluation de formules de logiques temporelles). La methode employee differe des approches usuelles basees sur la reecriture de termes, qui construisent directement le graphe d'etats correspondant a un programme LOTOS. Ici au contraire la traduction est effectuee en trois etapes successives (expansion, generation et simulation) s'appuyant sur des modeles semantiques intermediaires (le langage SUBLOTOS et le modele reseau). Elle met en oeuvre une analyse statique globale du comportement des programmes. Elle <b>prend</b> <b>en</b> <b>compte</b> les donnees, celles-ci devant etre compilees au moyen dalgorithmes deja existants. Ces principes de compilation ont ete entierement implementes dans le logiciel CAESAR. Les performances obtenues confirment l'interet de la methode...|$|E
40|$|THIS THESIS IS DEDICATED TO THE COMPUTED EXPLORATION OF SYRIAC MANUSCRIPTS, IT IS THE FIRST STUDY OF THE SORT. SYRIAC IS A LANGUAGE THAT DEVELOPPED IN THE EASTERN REGION OF THE MEDITERRANEAN COAST, ABOUT TWENTY CENTURIES AGO, AND IS STILL IN PRACTICE TODAY. THE HISTORY AS WELL AS THE DEVELOPPMENT OF THE LANGUAGE ARE PRESENTED IN THE FIRST CHAPTER. SYRIAC IS WRITTEN FROM RIGHT TO LEFT WITH A DISTINCT FEATURE WHICH IS A TILT OF ABOUT 45 ° WHICH RENDERS CLASSICAL SIGNAL AND DOCUMENT ANALYSIS ALGORITHMS WHICH WERE DEVELOPPED FOR OTHER LANGAUGES RATHER USELESS. IN THE SECOND CHAPTER, AFTYER DESCRIBING AND EXTRACTING THE DOCUMENTS STRUCTURE, WE DEVELOPPED A WORD SEGMENTATION METHOD THAT TAKES THIS TILT INTO CONSIDERATION, THIS LEAD US TO ABOUT THIRTY STABLE SHAPES WHICH ARE VERTICAL LETTRES AND "N-GRAMMES" MADE OUT OF TILTED LETTERS. IN THE SECOND PART OF THIS THESIS, WE WERE INTERESTED IN THE CONTENT OF THE DOCUMENTS FOR INDEXATION PURPOSES. WE DEVELOPPED A WORD SPOTTING METHOD THAT ALLOWED US TO FIND ALL THE OCCURRENCES OF A WORD IN A DOCUMENT USING SEVERAL WORD QUERY APPROCHES (WORD SPOTTING, WORD RETRIEVAL). IT IS BASED OM SHAPE SIMILARITY EVALUATED AFTER A THOUROUGH ANALYSIS OF THE ORIENATIONS OF THE HANDWRITING. THE LAST CHAPTER CONSISTS OF A FIRST CONTRIBUTION TO ASSISTED TRANSCRIPTION OF SYRIAC MANUSCRIPTS WHICH RELIES ON THE ABOVE DESCRIBED SEGMENTATION. WE SHOWED THAT TRANSCRIPTION BASED ON INTERACTION, IS IN CONFLICT WITH THE TRADITIONNAL APPROACHES OF O. C. R RECOGNITION. CETTE THESE EST DEDIEE A L'EXPLORATION INFORMATIQUE DE MANUSCRITS SYRIAQUES, C'EST LA PREMIERE ETUDE DE CE TYPE MISE EN ŒUVRE. LE SYRIAQUE EST UNE LANGUE QUI S'EST DEVELOPPE A L'EST DU BASSIN MEDITERRANEEN, IL Y A PLUS DE VINGT SIECLES ET QUI AUJOURD'HUI EST ENCORE PRATIQUEE. LA PRESENTATION DE L'HISTOIRE DU DEVELOPPEMENT DE CETTE LANGUE FAIT L'OBJECT DU PREMIER CHAPITRE. LE SYRIAQUE S'ECRIT DE DROITE A GAUCHE, AVEC UN ASPECT TRES SINGULIER, UN PENCHE D'UN ANGLE D'ENVIRON 45 ° QUI REND LES ALGORITHMES DE TRAITEMENT ET D'ANALYSE DE DOCUMENTS DEVELOPPES POUR LES AUTRES ECRITURES INOPERANTS. DANS LE SECOND CHAPITRE, APRES NOUS ETRE INTERESSES A LA DESCRIPTION ET L'EXTRACTION DES STRUCTURES DES DOCUMENTS, NOUS AVONS ELABORE UNE METHODE DE SEGMENTATION DES MOTS QUI <b>PREND</b> <b>EN</b> <b>COMPTE</b> CE PENCHE; ELLE NOUS CONDUIT A UNE TRENTAINE DE FORMES STABLES QUI SONT DES LETTRES INDIVIDUELLES VERTICALES ET DES "N-GRAMMES" CONSTITUES PAR DES LETTRES PENCHEES. DANS LA DEUXIEME PARTIE DE LA THESE, NOUS NOUS SOMMES INTERESSES AU CONTENU DES DOCUMENTS POUR DES FINS D'INDEXATION. NOUS AVONS DEVELOPPE UNE METHODE DE REPERAGE DE MOTS QUI PERMET DE RETROUVER, DANS IN DOCUMENT, TOUTES LES OCCURRENCES D'UN MOT SELON PLUSIEUS MODES DE REQUETES (WORD SPOTTING, WORD RETRIEVAL). ELLE REPOSE SUR UNE SIMILARITE DE FORME EVALUEE A PARTIR D'UNE ANALYSE TRES FINE DE L'ORIENTATION DU TRACE DE L'ECRITURE. LE DERNIER CHAPITRE EST UNE PREMIERE CONTRIBUTION A LA TRANSCRIPTION ASSISTEE DES MANUSCRITS SYRIAQUES QUI REPOSE SUR LA SEGMENTATION DES MOTS DECRITE CI-DESSUS. NOUS MONTRONS QUE LA TRANSCRIPTION, QUI S'APPUIE SUR L'INTERACTION, EST EN RUPTURE AVES LES TRADITIONNELLES DEMARCHES DE RECONNAISSANCE PAR O. C. R...|$|E
40|$|The present work {{investigates the}} {{technology}} development of state-of-the-art SiGe and SiGeC Heterojunction Bipolar Transistors (HBT) {{by means of}} technology computer aided design (TCAD). The objective of this work is to obtain an advanced HBT {{very close to the}} real device not only in its process fabrication steps, but also in its physical behavior, geometric architecture, and electrical results. This investigation may lead to achieve the best electrical performances for the devices studied, in particular a maximum operating frequency of 500 GHz. The results of this work should help to obtain more physical and realistic simulations, a better understanding of charge transport, and to facilitate the development and optimization of SiGe and SiGeC HBT devices. The TCAD simulation kits for SiGe/SiGeC HBTs developed during our work have been carried out in the framework of the STMicroelectronics bipolar technology evolution. In order to achieve accurate simulations we have used, developed, calibrated and implemented adequate process models, physical models and extraction methodologies. To our knowledge, this work is the first approach developed for SiGe/SiGeC HBTs which takes into account the impact of the strain, and of the germanium and carbon content in the base, for both: process and electrical simulations. In this work we will work with the successive evolutions of B 3 T, B 4 T and B 5 T technologies. For each new device fMAX improves of 100 GHz, thus the technology B 3 T matches to 300 GHz, B 4 T and B 5 T to 400 and 500 GHz, respectively. Chapter one introduces the SiGe SiGeC heterojunction bipolar technologies and their operating principles. This chapter deals also with the high frequency AC transistor operation, the extraction methods for fMAX and the carrier transport in extremely scaled HBTs. Chapter two analyzes the physical models adapted to SiGeC strained alloys used in this work and the electrical simulation of HBT devices. This is also an important work of synthesis leading to the selection, implementation and development of dedicated models for SiGeC HBT simulation. Chapter three describes the B 3 T TCAD simulation platform developed to obtain an advanced HBT very close to the real device. In this chapter the process fabrication of the B 3 T technology is described together with the methodology developed to simulate advanced HBT SiGeC devices by means of realistic TCAD simulations. Chapter four describes the HBT architectures developed during this work. We will propose low-cost structures with less demanding performance requirements and highly performing structures but with a higher cost of production. The B 4 T architecture which has been manufactured in clean-room is deeply studied in this chapter. The impact of the main fabrication steps is analyzed in order to find the keys process parameters to increase fMAX without degrading other important electrical characteristics. At the end of this chapter the results obtained is used to elaborate a TCAD simulation platform taking into account the best trade-off of the different key process parameters to obtain a SiGeC HBT working at 500 GHz of fMAX. Le travail porte sur le développement et l’optimisation de transistors bipolaires à hétérojonction (TBH) SiGe et SiGeC par conception technologique assistée par ordinateur (TCAD). L'objectif est d'aboutir à un dispositif performant réalisable technologiquement, en tenant compte de tous les paramètres : étapes de fabrication technologiques, topologie du transistor, modèles physiques. Les études menées permettent d’atteindre les meilleures performances, en particulier une amélioration importante de la fréquence maximale d’oscillation (fMAX). Ce travail est la première approche développée pour la simulation des TBH SiGeC qui <b>prend</b> <b>en</b> <b>compte</b> l'impact de la contrainte et de la teneur en germanium et en carbone dans la base; conjointement pour les simulations des procédés de fabrication et les simulations électriques. Pour ce travail, nous avons développé et implémenté dans le simulateur TCAD des méthodes d'extraction de fMAX prenant en compte les éléments parasites intrinsèques et extrinsèques. Nous avons développé et implémenté un modèle pour la densité effective d’états fonction de la teneur en germanium et en carbone dans la base. Les modèles pour la bande interdite, la mobilité et le temps de relaxation de l'énergie sont calibrés sur la base de simulations Monte-Carlo. Les différentes analyses présentées dans cette thèse portent sur six variantes technologiques de TBH. Trois nouvelles architectures de TBH SiGeC avancés ont été élaborées et proposées pour des besoins basse et haute performance. Grace aux résultats obtenus, le meilleur compromis entre les différents paramètres technologiques et dimensionnels permettent de fabriquer un TBH SiGeC avec une valeur de fMAX de 500 GHz, réalisant ainsi l’objectif principal de la thèse...|$|E
5000|$|Miller, Catherine, 1983, [...] "Aperçu du système verbal <b>en</b> Juba-Arabic", <b>Comptes</b> rendu du GLECS, XXIV-XXVIII, 1979-1984, T. 2, Paris, Geuthner, pp 295-315.|$|R
40|$|The {{end of the}} {{monopoly}} of sociologists on social scientific network analysis has raised new questions about time and space in networks. This paper, mostly intended to be read by historians, archaeologists and political scientists, reviews existing works and circumscribes open questions as regards time, and especially historical time. It presents two currents of research that have produced cumulative results: on the one hand, the production of interpretable animated visualizations of changing networks, {{and on the other}} hand, the so-called "actor-oriented" statistical modeling of network dynamics. It also presents comparatively under-discussed questions: those {{that have to do with}} the definition, gathering and coding of data, and the drawing up of hypotheses. Cet article discute des manières de prendre <b>en</b> <b>compte</b> le temps <b>en</b> analyse de réseaux. Il présente la littérature sur le sujet en essayant de distinguer les questions déjà assez bien connues (la présentation visuelle sous forme d'animations, la modélisation de type Siena) et celles, beaucoup moins discutées, qui ont trait à la construction des données, et en particulier à la datation des liens, ainsi qu'à l'élaboration d'hypothèses prenant les temporalités <b>en</b> <b>compte...</b>|$|R
40|$|International audienceAPRES AVOIR REFLECHI AUX ENJEUX DE LA FORMATION INITIALE DES ENSEIGNANTS DE LANGUES DANS LE PREMIER DEGRE, L'IUFM DE LORRAINE ADOSSE A LA REFLEXION SCIENTIFIQUE D'UN LABORATOIRE DE RECHERCHE, LE CRAPEL, A DEVELOPPE ET EXPERIMENTE UN DISPOSITIF PLURIDIMENSIONNEL INNOVANT ALLIANT POLYVALENCE ET SPECIALISATION. ONT ETE MIS EN AVANT LA REFLEXION CROISEE AUTOUR DE NOUVEAUX PARADIGMES COMME LE PLURILINGUISME, MAIS AUSSI LES APPRENTISSAGES MUTUELS, L'AUTO-DIRECTION, L'OUVERTURE INTERNATIONALE, L'ACCOMPAGNEMENT REFLEXIF DES FORMATEURS ET DES ETUDIANTS, LA PRISE <b>EN</b> <b>COMPTE</b> DES CONTEXTES SOCIO-ECONOMIQUE...|$|R
40|$|The French National Radioactive Waste Management Agency (Andra) {{began in}} 2000 the {{construction}} of an Underground Research Laboratory (URL) with the main goal of demonstrating the feasibility of a geological repository in Callovo-Oxfordian claystone. Several research programs have taken place to improve the knowledge of the rock properties and its response to the excavation progress. A network of experimental drifts has been constructed with variations on: excavation method, structure geometry, supports system and orientations with respect to principal stresses’ directions. In each drift different sections have been instrumented to monitor the hydro-mechanical behavior of the rock mass formation. Continuous monitoring of the excavated zone around the drifts in the main level (- 490 m) revealed the development of a fractured zone (extensional and shear fractures) induced by the excavation. The extent of this fractured zone depends on the drift orientation regarding the in-situ stress field. Accordingly, the convergence measurements showed an anisotropic closure which depends also on the drifts’ orientations. Moreover, marked overpressures and an anisotropic pore pressure field around the drifts have been also observed. The approach proposed in this work is mainly based on a direct analysis of the convergence measurements, for studying the anisotropic response of the rock formation during and after excavation. The convergence evolution is analyzed {{on the basis of the}} semi-empirical law proposed by Sulem et al. (1987) [Int J Rock Mech Min Sci Geomech Abstr 24 : 145 – 154]. The monitoring and analysis of convergence data can provide a reliable approach of the interaction between rock mass and support. Therefore, the anisotropy and the variability of the closure are analyzed taking into account different field cases: drifts excavated in two different orientations (i. e. influence of the initial stress state), different methods, sizes and rates of excavation and different supports systems with different conditions of installation. This broad range of cases permits to refine the analysis for reliable predictions of the convergence evolution in the long term. This approach can thus be used for the design of various types of support and the evaluation of its performance in the long term. On the other hand, the pore pressure evolution induced by excavation of drifts as recorded in situ has been analyzed. The anisotropic response observed in-situ suggests that the intrinsic anisotropy of the material plays a key role in the response of the rock formation. To understand these phenomena, an anisotropic poroelastic analysis of the pore pressure evolution induced by the drift excavation is performed. The main goal is to simulate the main trends of the pore pressure evolution with a simple model taking into account the inherent anisotropy of the material. Finally, an analysis of the onset of failure shows the key role of the hydro-mechanical coupling on the extension of the failed zone around the driftsL'Agence nationale pour la gestion des déchets radioactifs (Andra) a commencé en 2000 la construction du Laboratoire Souterrain de Meuse / Haute-Marne (LS-M/HM) avec l'objectif principal de démontrer la faisabilité d’un stockage géologique dans l’argilite du Callovo-Oxfordien. Un réseau de galeries expérimentales a été excavé, principalement en suivant les directions des contraintes horizontales (majeure et mineure), avec des variations sur : la méthode d'excavation, la géométrie de la structure et le soutènement. Chaque galerie a été instrumentée en différentes sections pour suivre le comportement hydromécanique de la roche face à l’excavation. Le suivi de la zone autour des galeries excavées au niveau principal (- 490 m) a révélé le développement d'une zone fracturée (fractures en extension et en cisaillement) induite par l'excavation. La distribution de la zone fracturée dépend à la fois de l'orientation de la galerie et du champ de contraintes in-situ et a une influence importante sur la déformation des galeries. En effet, les mesures de convergence ont montré une fermeture anisotrope de la section de la galerie. De plus, il a été observé un champ de distribution anisotrope de la pression de pores ainsi que des surpressions autour des galeries. Afin d’analyser la réponse anisotrope du massif pendant l’excavation et après celle-ci, les travaux effectués dans le cadre de la thèse sont axés principalement sur une étude directe des mesures de convergence in-situ. Cette analyse s’effectue à l’aide de la loi semi-empirique proposée par Sulem et al. (1987) [Int J Rock Mech Min Sci Geomech Abstr 24 : 145 – 154]. A cet égard, différentes galeries excavées dans le LS-M/HM ont été étudiées. Ces galeries présentent certaines différences dans leurs orientations et l’état initial des contraintes, dans la méthode et la vitesse d’excavation ainsi que dans les diamètres de la section et les types de soutènements installées. Cette analyse permet d’obtenir des prédictions fiables de la convergence à long-terme, ce qui peut servir pour le dimensionnement et la prévision de la performance du soutènement à long-terme. En outre, nous avons étudié la réponse anisotrope du champ de pression interstitielle observée in-situ. Cette analyse est basée sur une approche poroélastique anisotrope. L’objectif principal est de reproduire qualitativement l’évolution de la pression des pores autour des galeries avec une approche simple qui <b>prend</b> <b>en</b> <b>compte</b> l’anisotropie intrinsèque du matériau. Enfin, une analyse de l’apparition de la rupture montre le rôle clé que joue le couplage hydromécanique dans l’extension de la zone fracturé...|$|E
40|$|La modélisation du {{comportement}} hydrologique des bassins versants est incontournable dès lors que l'on s'intéresse à des problématiques relatives à l’évaluation et la gestion optimale des ressources en eau. Ceci s’illustre par un aspect quantitatif, dans {{les pays}} comme l’Algérie où l’alimentation en eau est un facteur limitant. L’objectif principal de cet article est d’expliciter les relations entre les conditions hydrologiques et la disponibilité en eau de surface. A cet effet, un modèle pluie-débit à l’échelle annuelle qui <b>prend</b> <b>en</b> <b>compte</b> les paramètres physiques et climatiques a été mis au point. L’application porte sur l’Algérie du Nord dont la superficie est de 325 000 km 2. La mise au point de ce modèle nécessite au préalable une base de données qui a été acquise lors des travaux antérieurs incluant la carte des pluies médianes, la carte des perméabilités et le modèle numérique de terrain. Afin de compléter cette base de données, la cartographie des pluies des différentes années pour lesquelles on dispose des débits a été réalisée en utilisant une méthodologie intitulée "cartographie de la pluie centrée réduite". Ainsi, les données de 467 postes pluviométriques ont été traitées et ont permis de tracer les cartes des isohyètes annuelles. L’étude de la relation pluie-débit à l’échelle annuelle est basée sur les données de débits de 50 stations hydrométriques réparties à travers la zone d’étude. La démarche utilisée s’est inspirée de la fonction de production du S. C. S (Soil Conservation Service). Les résultats du modèle après calage ont permis d’obtenir un coefficient d’explication de 0, 75, ce qui signifie que 75 % de la variance est expliquée par la pluie moyenne, la surface et un coefficient (a) qui correspond à la pente moyenne des bassins versants. Modelling the hydrological behaviour of drainage basins {{is very important}} for solving problems related to the evaluation and optimal management of water resources. This is illustrated quantitatively in countries such as Algeria where water supply is a limiting factor. The principal aim of this paper was to explain the relationship between hydrological conditions and the availability of surface water. A model of rainfall-discharge was developed on a yearly scale, taking into account physical and climatic parameters. The application was carried out in northern Algeria where the total land surface is about 325 000 km 2. The development of this model required a database, which was acquired during previous studies where maps of median rainfall and permeability as well as the digital elevation model were developed. In order to complete this database, the cartography of rainfall for the years for which we have discharge data was carried out using a methodology entitled "mapping standardized rainfall". To estimate and map annual rainfall, the kriging method was used. Two problems were encountered:- The presence of a drift highly altered the variogram and made it very difficult to infer a sub- structure function;- The variogram is significant only if the hypothesis of ergodicity is valid, which was not easy to assume for any given year. In order to resolve these difficulties, a homogeneous random and secondary stationary order function (same mean at all points and same covariance function) must be calculated. A previous study by ANRH (1993) allowed us to know the statistical parameters of the distribution at each point. These parameters were mapped, taking into consideration the topographical relief and distance to the sea. For every year and at each rainfall measure point, the standardized rainfall could be deduced. The correlogram gave information about the spatial variability of the phenomenon and its range, and subsequently the standardized rainfall was then interpolated. Annual rainfall was calculated by combining the grids of the means, the variances and the centered reduced rainfall (TOUAZI and LABORDE, 2000). Thus, the data of 467 rainfall gauges were used in order to create maps of the yearly isohyets. The rainfall-discharge relationship on an annual scale was based on 50 hydrometric stations distributed throughout the study area. The methodology used was derived from the production function of the S. C. S (Soil Conservation Service). This production function was part of modelling, which transformed total rainfall to net rainfall. This method was very representative of the natural hydrological processes. Indeed, it takes into account rainfall and the maximum infiltration capacity (S), which depends {{on the nature of the}} soil (lithology), vegetation and soil moisture content. In the current study, the basin surface and a regional parameter (a) were introduced in order to calibrate the model. This production function was implemented by supplying different values for the parameter (S). The values (n+ 1) were obtained by increasing the previous value (n) by 10 %. We evaluated the different values of (S) in the same way to obtain the last value (i). We calculated for these different values of the parameter (S) the square of the difference between the measured and estimated discharges for each year by measuring the discharge at different stations. For each station, we calculated the sum of these values for all the years and we retained the value of (S) that gave the minimal value. The results demonstrated that the values of (S) obtained were not significant because they tend to the infinite. For this reason, (S) was considered as a constant. In order to improve the model, we repeated the same operation, but instead of (S), we used the parameter (a) and performed the same calculation. After calibration of the model the results gave a coefficient of determination of 0. 75, which means that 75 % of the variance was explained by the mean rainfall, the surface and the parameter (a). To explain the parameter (a), we calculated the correlation between its value at each station with the corresponding geology. This latter variable was characterized by the average storage capacity, which corresponds to the weighted average of the surfaces of the basin assigned to each permeability category (TOUAZI, 2001). The results demonstrated a coefficient of determination of 0. 1. The correlation with the topographical relief was not necessary because it was taken into account in the cartography of the rainfall. We then proceeded to the cartography of the parameter (a). The results demonstrated an east-west gradient that was constant and a north-south gradient that decreased from north to south. With the digital elevation model, we used a geographical information system to deduce the slopes. For each basin, the average slope was calculated by taking the average of the values of the slopes of all the pixels that constituted the individual basin. The correlation between slopes and corresponding values of the parameter (a) gave a coefficient of correlation of 0. 6. The results obtained by this model after calibration gave a coefficient of determination of 0. 75, which means that 75 % of the variance was explained by the mean rainfall, the surface and a coefficient (a), which corresponds to the average slope of the drainage basins...|$|E
40|$|Quelques mois après le début de la mise en eau du barrage de Petit Saut, la mise en service normale de l'usine conduisait à une désoxygénation de l'eau du tronçon de rivière aval, le rendant {{incompatible}} avec la vie aquatique. La solution retenue a été la construction d'un seuil, afin d'apporter de l'oxygène et d'éliminer les gaz réducteurs produits au fond de la retenue, notamment le méthane, consommateur potentiel d'oxygène dissous. Un seuil métallique à deux lames déversantes successives a été construit; sa configuration <b>prend</b> <b>en</b> <b>compte</b> les principaux critères physiques jouant un rôle significatif sur l'oxygénation de l'eau (hauteur de chute, épaisseur de la lame déversante, le dimensionnement du bassin de réception des chutes, la présence de dispositifs favorisant l'éclatement de la lame d'eau). Placé dans le canal de fuite de l'usine, à une centaine de mètres à l'aval du barrage principal, il est à l'abri des crues et ne crée pas d'obstacle supplémentaire en rivière. L'article chiffre l'effet d'aération de ce seuil pour les deux gaz O 2 et CH 4 dans deux configurations : celles consécutives à l'abaissement partiel de la chute amont réalisé en deux étapes. Après décembre 2001, pour le débit moyen turbiné (près de 200 m 3 /s), l'efficacité d'aération du seuil a baissé de près de 10 % (gain de 80 % en oxygène dissous et élimination de 70 % et 75 % du méthane dissous). Après février 2003, pour un débit de 100 m 3 /s, 75 % du déficit amont en oxygène dissous est comblé et près de 70 % du méthane dissous éliminé. From {{the moment}} tropical reservoirs are impounded, climatic conditions cause rapid (within several weeks) and marked thermal stratification, {{especially during the}} dry season. This phenomenon is further exacerbated by the chemical and biochemical processes {{taking place in the}} reservoir due to the decomposition of submerged organic matter. In dense tropical forests, the overhead biomass is estimated at roughly 170 t(C) /ha, and the carbon contained in the soil is also not negligible since it is on the order of 100 t(C) /ha. The degree of biodegradability of the different compounds in the flooded biomass is variable, ranging from a few weeks for bacteria to several centuries for tree trunks. The studies carried out at Petit Saut (French Guiana) show that, immediately after impoundment, only the epilimnion (a few dozen centimetres thick) was oxygenated whereas the hypolimnion was characterized by complete anoxia and a very high methane content (about 15 mg/L). Water quality in the river downstream from the reservoir was of course strongly linked to variations in the water quality in the reservoir as well as to its operating mode. The waters passing through the turbines, coming from the bottom layers, were anoxic and loaded with fixed or volatile reducing compounds (e. g., CH 4, H 2 S), and were responsible for a high immediate or progressive oxygen demand. At Petit Saut, despite an inflow of good quality water, there has been a progressive deoxygenation in the river downstream due to the high methane content (roughly 8 mg/L) of the turbined water. Thus, 40 km downstream from the dam, the oxygen content was less than 2 mg/L and therefore incompatible with most aquatic life. To solve this problem, it was necessary to build an aerating weir capable of reoxygenating the turbined waters and, more importantly, eliminating reducing gases such as methane at the same time. The function of the overflow weir was to entrain air bubbles into the water and to give these bubbles a sufficiently long immersion time to ensure that they dissolve. At the time of its installation, only three examples of oxygenating weirs existed in the entire world, all located in the United States. The weir configuration was tested using a physical model to qualitatively examine the form of the flow both across the weir and downstream from it. The degree to which air bubbles were entrained in the water was also tested, but not the question of evaluating the flux of gaseous exchanges between the air and the water. The system that was finally designed by EDF, in October 1994, was a metallic weir with two consecutive falls, the configuration of which respected the main physical criteria that {{play a significant role in}} the oxygenation of water, i. e. :- the height of the falls (roughly 5. 40 m, depending on the flow rate);- the thickness of the water stream, the function of which is to entrain air bubbles and keep them in the water for a sufficiently long period of time for the oxygen to dissolve (between 12 and 25 seconds, depending on the flow rate); - the dimensions of the receiving basin of the first waterfall where the air bubbles are held (5 hexagonal alveoli); and- systems to promote the fragmentation of the flow. This structure was placed in the tailrace channel of the plant, approximately 100 m downstream from the main dam. This location protected it from floods and did not create an extra obstacle in the river. In addition, it allowed the water to be re-oxygenated as soon as it left the reservoir. The efficiency of the two waterfalls of the Petit Saut re-aerating weir was tested at two different turbine flow rates: 80 m 3 /s and 230 m 3 /s. In 1996, the results of the measurements showed that for a flow rate of 230 m 3 /s, upstream of the weir the concentrations of CH 4 were around 5 mg/L and dissolved oxygen was 0. 8 mg/L. Downstream from the weir CH 4 concentrations were 1. 3 mg/L and dissolved oxygen concentrations were 6. 8 mg/L. The dissolved methane elimination rate was approximately 75 per cent. At a flow rate of 80 m 3 /s, upstream of the weir the concentration of CH 4 was 5. 5 mg/L and the dissolved oxygen concentration was 0. 7 mg/L. Downstream from the weir concentrations of CH 4 and dissolved oxygen were 1. 0 mg/L and 7. 1 mg/L, respectively. The dissolved methane elimination rate was around 80 %. The efficiency of the re-oxygenation was always greater than 90 %. These data prove that the efficiency of the Petit Saut weir installation was higher when the turbine flow rate was lower. This could be due to a greater waterfall height, the better entrainment of air bubbles per unit volume and/or a longer air bubble residence time in the downstream flow. Between December 2001 and February 2003, for a flow rate of 200 m 3 /s, the efficiency of the weir decreased by 10 %, with the dissolved methane elimination rate at around 70 - 75 %. The level of re-oxygenation was around 80 %. Since February 2003, for a flow rate of 100 m 3 /s, the efficiency of the weir has decreased by 10 %, the dissolved methane elimination rate was around 70 % and the level of re-oxygenation was around 75 %. On a local scale, the effect on the quality of the river water has been very positive, as aquatic life has been maintained. Without the weir, the methane contained in the turbined water would have been progressively transformed, along the course of the river, into carbon dioxide. In the absence of significant additions of good quality water and without the weir, a large part of the course of the river would have a dissolved oxygen content of less than 2 mg/L, the critical threshold for the maintenance of aquatic life. At present time, the results of the current ecological survey are used to support studies on biogeochemical processes...|$|E
5000|$|Laugier, Paul-Auguste-Ernest, “Notice sur l’apparition de la comète de Halley <b>en</b> 1378”, <b>Comptes</b> rendus hebdomadaires des séances de l’Académie des sciences, 16 (1843), 1003-1006 Gallica.|$|R
5000|$|... 1998 : Socan Award - Most played song {{on radio}} - Et mon cœur <b>en</b> <b>prend</b> plein la gueule ...|$|R
40|$|L’ «  image-espace  » : propositions théoriques pour la prise <b>en</b> <b>compte</b> d’un «  espace circulant  » spécifique aux images de cinéma Warning The {{contents}} of this site {{is subject to}} the French law on intellectual property and is the exclusive property of the publisher. The works on this site can be accessed and reproduced on paper or digital media, provided that they are strictly used for personal, scientific or educational purposes excluding any commercial exploitation. Reproduction must necessarily mention the editor, the journal name, the author and the document reference. Any other reproduction is strictly forbidden without permission of the publisher, except in cases provided by legislatio...|$|R
40|$|En France, les sécheresses consécutives des années 1985, 1986, 1989 et 1990 ont mis en lumière les problèmes relatifs à l'alimentation en eau potable, l'irrigation des terres agricoles et la préservation des écosystèmes aquatiques. Dans le cas des zones humides, continentales et littorales, caractérisées par une compartimentation {{hydraulique}} souvent complexe, le manque de connaissance se fait particulièrement sentir. Bien que de nombreux travaux aient permis d'évaluer l'évaporation des masses d'eau et l'évapotranspiration de certaines espèces d'hydrophytes et d'hélophytes, les études débouchant sur des bilans quantitatifs restent peu fréquentes. Le bilan hydrologique du marais de Moëze (2250 ha) a été calculé par décade entre le 11 / 06 / 89 et le 31 / 08 / 89. Il <b>prend</b> <b>en</b> <b>compte</b> le débit au droit de l'ouvrage d'alimentation, les volumes prélevés pour l'irrigation hors marais, les infiltrations et l'évapotransplration sur les 318 km de canaux. L'estimation de la consommation d'eau des parcelles est globalisée {{au niveau}} des mesures d'infiltration. Les pertes par infiltration sont secondaires (9, 4 %) au regard des volumes prélevés pour l'irrigation (38, 0 %) et évapotransplrés par les canaux (43, 7 %) dont 51, 1 % uniquement par les 28, 6 % des plans d'eau colonisés par Typha latifolia. L'optimisation de la gestion estivale de l'eau d'un marais littoral agricole nécessite dans un premier temps de minimiser les pertes. C'est essentiellement sur la consommation d'eau des canaux colonisés par les hélophytes que l'on peut intervenir. Nous proposons un abaque qui permet d'évaluer l'importance des économies d'eau réalisées en fonction de plusieurs scénarios d'aménagement du réseau hydraulique. In France, the drought {{that occurred during}} the years 1985, 1986, 1989 and 1990 have emphazised the problems of freshwater supplies for human consumption, for irrigation and for the conservation of aquatic systems. Today the water has to be economized through a rationalized management. Water balance must be evaluated in order to compare supply an demand. Hydrological functioning is particularly badly known as far as continental and coastal wetlands are concerned, probably because of a generally very complex hydrological partition. Many papers deal with the evaporation rate from a clear water surface or the evapotranspiration rate from several species of hydrophytes and halophytes. However studies of quantitative water budgets of wetlands remain few in number. This paper reports an analysis of the budget summer water in that was a salt marsh now containing freshwater. The 25 km 2 marsh of Moëze is located on the French Atlantic coast; it has been progressively constituted by filling up a tidal bay since the flandrien period. The soils correspond to fluvio-marine silts, locally called « bri » accumulated over several tens of meters thick. The marsh is bounded on the North by the ancient limestone coast, on the South by the Arnoult River, and on the West by the coastline. Its drainage network includes permanently flowing main canals and also small silted-up ditches which sometimes dry up in summer and are largely colonized by aquatic plants, particularly Typha latifolia. The channels network is very dense (144 meters of ditches per hectare) and complex because of a close connection between all the canals and ditches. The regional oceanic climate is characterized by a surplus water balance from October to April (+ 315 mm) and quite a short one tram May to September (- 338 mm). The important terms of the water budget equation in this study were : the quantity of water pumped from the Arnoult River through the inflow sluicegate (Qa), the precipitations (P), the irrigation out of the marsh (lr), the evapotranspiration of water bodies (Epo), the seepage through canals and ditches (ls), and the change in water soil strorage (Vs). These terms are not equally susceptible to be measured. Groundwater seepage and evapotranspiration are difficult to measure and they are often determined by difference, but, they contain the residual error of ail the terms. The methods to evaluate each term of water budget were carried as follows : 1. As the inflow gate functions as a siphon, the flow rate (Qa) was calculated with the drowned orifice formula (LENCASTRE, 1984). The upstream and downstream water levels were permanently recorded by 2 limnigraphs. The upstream and downstream water velocities were measured every 2 days with a micro-currentmeter. 2. The precipitation values (P) used in the water budget equation correspond to the average of 4 rain gauges placed around the marsh. 3. The evapotranspiration of the channel network (Epo) was directly estimated through 4 experimental floating tanks (0. 50 x 0. 55 x 1. 05 m size) previously used by GIRAUD (1985). One of the tanks was placed in clear water, the others was planted the typical aquatic vegetation of the marsh (Lemna sp., Ceratophyllum sp., and Typhia latifolia). The drop the water levels in the tanks corresponded to the loss of water due tou evapotranspiration. All the tanks were filled up to a fixed level, and the amount of water added, measured every 2 days. 4. The outputs for irrigation (Ir) concern 298 ha of maïze out of the marsh, and 23 farmers. The water amounts taken off were estimated form an inquiry of irrigation practices associated to a field control. 5. The water losses by seepage (ls) through canals and ditches were directly measured on the field by using the closed basin method. A length of canal was closed by 2 watertight bulkheads. The fall of water level was recorded and the amount of water added to maintain a fixed level was measured. This method is considered by KRAATZ (1977) as being the most accurate specially for low seepage. The fall of water level never exceeds 10 % of the water depth in the basin. 34 canals and ditches in the marsh were sampled. According to CHEVALLIER et al. (1984), 3 parameters influencing the soil permeability were measured : granulometry, CaC 03 content and sodicity. After the sampling plan we have retained 4 experimental canals (average length = 47 m, average water surface = 135 m 2, average depth = 0. 44 m). 6. The water strorage in the soil (Vs) was evaluated by analyzing the groundwater table fluctuations and moisture changes. The water budget calculated for 10 -day periods depending on the climatology calculations, from 10 th June 1989 to 31 st August 1989. The water losses due to seepage were secondary (9. 4 %) compared to the amounts of water taken off by irrigation (38. 0 %) and channel network evapotranspiration (43. 7 %). The water consumption of helophytes such as Typhia latifolia was 2 to 3 times higher than the evaporation of a clear surface water body as shown in figure 3. In the marsh of Moëze, 51. 1 % of channel evapotranspiration was due to the colonization by Typhia latifolia of the canals and ditches although they represent only 28. 6 % of the channel network surface. This study shows that is possible to quantify a water budget for a large scale wetland from field measurements associated to experimental approaches, with a satisfactory accuracy : less than 10 %. To reduce the water consumption of the marsh of Moëze, three essential recommendations may be given : the reduction of the global channel network surface, the cleaning of a part of ditches colonized by Typhia latifolia, or the combination of both techniques. According to the different management schemes, it is possible to predict the amounts of freshwater saved (fig. 5) ...|$|E
40|$|Les {{stations}} d'épuration des eaux usées municipales du Québec, comme ailleurs au Canada et aux États Unis, sont en général peu efficaces sur le plan énergétique. Il est donc possible de concevoir des hypothèses de chaînes épuratoires améliorées au plan de leur efficacité énergétique et de leur performance globale en y introduisant, d'une part, diverses MEEE et, d'autre part, certains procédés unitaires améliorant l'épuration. Cependant, le problème de choisir, parmi ces hypothèses de chaînes d'épuration, celle correspondant au procédé le plus adéquat pour prendre {{en charge}} une situation donnée, demeure entier. Cet article analyse ce problème de choix pour les stations d'épuration de capacité comprise entre 5, 000 m 3 /d (≈ 5000 personnes) et 100, 000 m 3 /d (≈ 100, 000 personnes). En faisant l'hypothèse, d'un côté, que l'ajout des mesures d'efficacité énergétique peut améliorer la consommation énergétique d'une chaîne d'épuration, et d'un autre côté, que l'insertion de segments de procédés peut contribuer à améliorer leur performance globale, nous avons élaboré, à partir des stations d'épuration de types Biofiltration, Physico-chimique, Réacteurs Biologiques séquentiels (deux variantes), Boues activées et Étangs aérés, six (6) hypothèses de chaînes épuratoires (chaînes 1 à 6) respectant les exigences opérationnelles et épuratoires et nous les avons comparées entre elles, sur la base d'une analyse multicritère d'aide à la décision, en vue d'en déterminer les plus performantes. Cette analyse multicritère intègre les aspects techniques, énergétiques, économiques, etc. et <b>prend</b> <b>en</b> <b>compte</b> les préférences du(des) décideur(s) dans le processus de choix. Les résultats obtenus montrent que, parmi les six hypothèses de chaînes étudiées, les trois premières positions sont occupées par les chaînes 3, 1 et 4 respectivement. Ce type d'analyse pourrait jouer un rôle complémentaire à une étude technico-économique visant le choix de technologies d'épuration. Quebec municipal wastewater treatment facilities, like those elsewhere in Canada and the United States, generally are low efficiency energy consumers (ELECTRIC POWER RESEARCH INSTITUTE (EPRI), 1993; OWEN, 1982; ONTARIO-HYDRO, 1993; METCALF and EDDY, INC., 1992, SASSEVILLE et al., 1995). The work of METCALF and EDDY, INC. (1992) and of EPRI (1993) concluded {{that it would be}} possible to substantially reduce electricity demand and to improve the utilization of electrical energy in the municipal wastewater treatment processes by introducing Electricity Saving Measures (ESMs) in the processes and their management. In the province of Quebec, given the potential savings linked to the reduction of electricity consumption in municipal wastewater treatment facilities, and the progressive expansion of the province's wastewater treatment facilities, the adoption of energetically efficient wastewater treatment technologies is particularly timely. SASSEVILLE et al. (1995) estimated that {{it would be possible to}} save 5 M $ at the present level of wastewater treatment, based on a cost of about 24 M $ for the 400 GWh of electricity annually consumed in the municipal wastewater treatment facilities. This saving would come solely from the implementation of appropriate energy-saving measures. In hypothesizing on one hand that adding energy-saving measures can improve the energy consumption of a wastewater treatment chain, and on the other hand that the introduction of segments of processes can contribute in improving overall performance, we have elaborated from existing wastewater treatment facilities six hypotheses of liquid treatment chains that respect operational and regulation requirements, on the basis of the experience developed in the operation of municipal wastewater treatment facilities. The six hypothetical treatment chains were elaborated from facilities of the following types: biofiltration (chain 1), physico-chemical (chain 2), sequencing biological (batch) reactors system A (chain 3), sequencing biological (batch) reactors system B (chain 4), activated sludge (chain 5) and aerated lagoons (chain 6). The energy-saving measures utilized in the elaboration of these hypothetical treatment chains were chosen on the basis of a conjunctive analysis (KIBI et al., 1997). However, the problem of choosing among these treatment chains the one corresponding to the most adequate process for a particular situation is still present. The present article analyzes this choice for wastewater treatment facilities of a capacity between 5 000 m 3 /d (≈ 5 000 persons) and 100 000 m 3 /d (≈ 100 000 persons). How then to choose the most efficient of these hypothetical municipal wastewater treatment chains?Generally, the choice of the treatment technologies is done on the basis of single-criterion mathematical models: for example, the reduction of construction costs, or of exploitation and maintenance costs (ECKENFELDER, 1982; PINEAU et al., 1985; WANG and WANG, 1979; TYTECA et al., 1977). There are other approaches based on dynamic simulation models or on technological and econometric analyses (HYDRO-QUÉBEC, 1993; MACRAE 1989; LESSARD, 1989; BROCKTON, 1987; HOLDREN, 1987; FOSBERG and MUKHOPADHYAY, 1981; REID, CROWTHER and PARTNERS, 1978; KLEMETSON and GRENNEY, 1976). These different approaches are often insufficient to distinguish the real value of the different technological options. Furthermore, they do not take into account many important factors (technological, economic, financial and environmental, ergonomic and socio-political) that affect their implementation and should be considered in identifying an acceptable and viable solution. The multicriteria approach of decision-making advocated in this article can mitigate this difficulty. It will take into account key factors in the conception and operation of treatment technologies, especially energy and environmental factors, likely to give rise to efficient treatment facilities from both an energy and a treatment point of view. However the analysis of decisional factors to consider in this multicriteria analysis gives rise to a particular problem. They affect the decision that can be appreciated by deterministic relationships, offering a high level of certainty as to its evaluation, while others have a non-deterministic nature (uncertainty and imprecision). Research using the multicriteria analysis approach has been performed in similar situations over the last twenty years, in some cases applied to the environmental and energy sectors (e. g. KEENEY and NAIR 1977; ROY and VINCKE 1981; TEGHEM and KUNSCH 1985; SIMOS 1990; HANSON 1991; ROUSSEAU and MARTEL 1994). Here again, these approaches have limits since the cases were treated either in a situation of certainty or a situation of uncertainty and imprecision. The proposed model deals with the case of certainty and uncertainty at the same time, therefore improving the applicability of the multicriteria approach in the situation under study. The solution retained consists of applying this type of modelling in order to classify from the best to the worst, the six hypothetical treatment chains. This approach utilizes in the modelling process, fourteen evaluation criteria, various criteria weights, quantitative and qualitative evaluations, as well as the indifference, preference and veto thresholds. The main steps of the model are the construction of evaluated outclass relationships and the exploitation of these outclass relationships. The multicriteria aggregation procedure utilizes an elaborated mathematical model based upon the methods of ELECTRE III (Roy, 1978) and PROMETHEE II (BRANS et al., 1984), as well as the works of DÉROT et al. (1994). The ranking of these treatment chain hypotheses, elaborated on an empirical level in consultation with the operators and others involved in wastewater treatment and obtained on the basis of this procedure, can discriminate among their overall performance characteristics rather well. It also emphasizes their energy efficiency, since the energy criteria have on average in the analysis a weight that is 28 % higher than the other evaluation criteria. The results obtained show that the hypothetical chain 3 is ranked first, chain 1 occupies the second rank, whereas chain 4 is in the third rank. The last three ranks are occupied respectively by chains 6, 5 and 2. In a decisional and strategic approach, the first three treatment chain hypotheses can be considered overall as being the highest achievers. This result signifies that in the scope of investments related to the expansion of treatment facilities and the construction of facilities with a flow rate contained within the considered range, these three treatment chains (when considering different modification hypotheses), should be preferred over the other chains when the emphasis is on their overall performance including energy efficiency. However, other analyses would be necessary in the case of the construction of a new wastewater treatment facility with a flow rate above the level considered in this study. Generally, the results of this analysis can assist in discriminating among the behaviors of the technologies considered, and in judging their relative performance in the investments of the construction of new wastewater treatment facilities, in addition to a technico-economic analysis. Overall, the multicriteria model described in this study identified a compromise solution between evaluations of a different and conflicting nature. This result demonstrates that this type of analysis is appropriate for tackling multidimensional problems...|$|E
40|$|Between 1999 and 2001, three {{measurements}} changed Cosmology forever: {{the discovery}} of Cosmic Acceleration (Riess et al. 1998, Perlmutter et al. 1999) indicated that {{the density of the}} Universe is dominated by some kind of repulsive energy of unknown nature (namely Dark Energy). The measurement of the first acoustic peak in the CMB temperature anisotropy spectrum (de Bernardis et al. 2000) combined with the precise determination of H 0 (Freedman et al. 2001) gave strong constraints on the flatness of space-time. These measurements contributed to solve the persisting disagreements between the observations that were favouring a low density of matter, and theoretical motivations for a higher-density (critical) Universe. It favoured the emergence of the Standard Model of Cosmology (CDM) that describes nearly all of today's observations with only a handful of free parameters (Planck Collaboration et al. 2013 b). Cosmology has now entered an era of precision measurements, and the goal of observations is now to hunt for "tensions" within the cosmological model. The case of Supernova cosmology is very characteristic of this situation. The measurement of luminosity distances to SNe-Ia as a function of their redshift allowed one to discover (with less than 100 supernovae) the acceleration of cosmic expansion. Today, SNe-Ia are still the most sensitive probe to w, the Dark Energy equation of state parameter, and growing number of SNe-Ia are being detected and studied by several Collaborations all over the world, in order to pin down the value of w, and to start ruling out Dark Energy models. The precision on w is now as low as 7 % (Conley et al. 2011, Sullivan et al. 2011) with nearly 1000 SNe-Ia in the Hubble diagram. Unfortunately, the measurement is now dominated by systematic uncertainties, the dominant source of systematics being the photometric calibration of the imagers used to measure the SNe-Ia fluxes. This work is about photometric calibration. This is a rather esoteric subject, which is seldom chosen by PhD students. But the thing is that, to improve on the current results, astronomers {{have no choice but to}} revisit the ancient calibration schemes. Since 2005, most Dark Energy Collaborations (with the invaluable help of the HST calibration program) have launched ambitious calibration efforts, redefined primary standards and metrology between those standards and their science images and push down their error budget well below 1 % (e. g. Betoule et al. 2012). One suspect however, that these techniques, which rely on observations of stellar calibrators, will not allow one to reach the calibration requirements of future surveys. For this reason, several groups in the world are working on experimental laboratory sources, that would allow one to inject very well characterised light into the telescope optics and derive, from these measurements, the telescope throughput as a function of wavelength. Since 2007, LPNHE cosmology group has been involved in the construction of a spectro-photometric calibration system for the last generation of wide field imagers (Barrelet and Juramy 2008). In particular, the team has designed and built two devices: SnDICE (Supernovae Direct Illumination Calibration Experiment) and SkyDICE (SkyMapper Direct Illumination Calibration Experiment), the first installed in the enclosure of the Canada France Hawaii Telescope (CFHT) on top of Mauna Kea, and the other in the dome of SkyMapper (Siding Springs Observatory, NSW, Australia). I started my PhD a few months after the project was funded. I was involved in nearly all stages of the project, in particular the integration and the calibration of the device on our test bench, as well as the installation and commissioning at Siding Springs. I then spent my third year analysing the commissioning data. We have shown that it is possible to build a LED based light source that samples evenly the full visible wavelength range. The stability of the source is remarkable, ranging from a few 10 − 4 for a few of the LEDs, to 10 − 3 for the less stable channels. I have detailed the spectrophotometric characterisation of the device on our test bench at LPNHE. More importantly, I have shown that it is possible to build a smooth spectrophotometric model of each LED, that can predict the LED spectrum at any temperature (in a temperature range representative of what is measured in the telescope enclosure). Each of these models comes with an uncertainty budget that accounts for (1) -the finite number of spectroscopic and photometric measurements and (2) -the test bench uncertainties. Finally, I have described a method to calibrate the effective passbands of the imager, and monitor their fronts from series of calibration frames taken with SkyDICE. This method takes into account all the test bench uncertainties are propagate them as exactly as possible to the final result. It is currently being applied to the real SkyDICE dataset, and what has been presented here is a set of tests performed on (realistic) simulated datasets. A important result of this work is that, despite the fact that the LEDs are not monochromatic sources, we are able to control the position of the filter fronts with an accuracy well below 1 -nm. Regarding the passband inter-calibration, we have computed the expected uncertainties affecting our estimates of the passband normalisation, relative to the r-band. These uncertainties actually depend on how we interpret the uncertainties that affect the calibration of the NIST photodiode. In the best-case scenario, where the NIST uncertainties are all positively correlated, we have shown that after a few calibration runs, we get down to a precision of 0. 4 % in the u and v-bands (near-UV) and of 0. 3 % in the other bands. Depending on how we estimate the CALSPEC uncertainties (which are themselves uncertain), this result is either a major improvement on CALSPEC, or on par with what can be obtained with CALSPEC. In any case, this means that by using routinely a DICE source to calibrate a survey telescope, we should be able to test the CALSPEC flux scale. The analysis of the SkyDICE commissioning dataset is still ongoing. The main missing ingredient is the control of the relative positions of the telescope and the sources, as well as an estimate of the pollution of the calibration frames. These two aspects of the analysis are actively worked on, and the first constrains should be published soon. La cosmologie est maintenant entré dans une ère de mesures de précision, et l'objectif des observations est maintenant la chasse aux contradictions au sein du Modèle Cosmologique. La mesure des distances de luminosité de SNe Ia en fonction de leur décalage vers le rouge a permis de découvrir l'accélération de l'expansion cosmique. Aujourd'hui, les SNe-Ia sont encore la sonde la plus sensible à w, l'équation d'état de l'énergie noire, et le nombre croissant de SNe-Ia sont détectés et étudiés par plusieurs collaborations partout dans le monde, afin d'affiner la mesure du valeur de w. La précision sur w est maintenant aussi bas que 7 %, avec près de 1000 SNe-Ia dans le diagramme de Hubble. Malheureusement, la mesure est désormais dominé par les incertitudes systématiques, la principale source de la systématique en étant l'étalonnage photométrique des imageurs utilisés pour mesurer le flux des SNe Ia. Ce travail de thèse a pour sujet l'étalonnage photométrique. Pour améliorer les résultats actuels, les astronomes n'ont pas d'autre choix que de revoir les systèmes d'étalonnage anciens. Depuis 2005, les collaborations sur l'énergie noire ont lancé des efforts d'étalonnage ambitieux, redéfini les standards primaires et la métrologie entre ces standards et leurs images scientifiques pour pousser le budget d'erreur bien inférieure à 1 %. Depuis 2008, le groupe de Cosmologie de l'LPNHE a été impliqué dans la construction d'un système d'étalonnage spectrophotométrique pour la dernière génération des imageurs grand-champ. En particulier, l'équipe a conçu et construit SkyDICE (SkyMapper Direct Illumination Calibration Experiment), installé dans le dôme du télescope SkyMapper (Observatoire Siding Springs, Australie). Dans ce projet nous avons montré qu'il est possible de construire une source lumineuse à base des LEDs qui échantillonnent uniformément toute la gamme des longueur d'ondes visible du télescope SkyMapper. La stabilité de la source est remarquable, allant de quelques 10 - 4 pour la majorité des LEDs, à 10 − 3 pour les canaux les moins stables. J'ai détaillé l'étalonnage spectrophotométrique de l'appareil sur notre banc de test au LPNHE. Plus important encore, j'ai montré qu'il est possible de construire un modèle spectrophotométrique de chaque LED, qui peut prédire le spectre des LEDs à n'importe quelle température T. Chacun de ces modèles est livré avec un budget d'incertitude que représente (1) -le nombre limité de mesures spectroscopiques et photométriques et (2) -les incertitudes du banc de test. Enfin, j'ai décrit une méthode pour calibrer les bandes passantes effectives de l'imageur, et de surveiller les filtre avec des sériés d'images d'étalonnage prises avec SkyDICE. Cette méthode <b>prend</b> <b>en</b> <b>compte</b> toutes les incertitudes du banc d'essai et le propage aussi exactement que possible. Le méthode est actuellement appliqué à l'ensemble de données réelles de SkyDICE, et ce qui a été présenté ici est un ensemble de tests effectués sur des ensembles de données simulées. Un résultat important de ce travail est que, malgré le fait que les LEDs ne sont pas des sources monochromatiques, nous sommes en mesure de contrôler la position des fronts de filtre avec une précision bien inférieure à 1 -nm. En ce qui concerne la bande passante étalonné, nous avons calculé les incertitudes affectant nos estimations sur la normalisation de la bande passante, par rapport à la bande r. Dans le meilleur scénario, où les incertitudes sont tous corrélés positivement, nous avons montré que, après quelques analyses d'étalonnage, nous nous attelons à une précision d'environ 0, 4 % dans les bandes u et v et d'environ 0, 3 % dans les autres bandes. L'analyse de l'ensemble de données des SkyDICE est toujours en cours et le premier contraintes seront publiés bientôt...|$|E
40|$|CE TRAVAIL CONCERNE LA CONDUITE THERMIQUE D'UN REACTEUR DISCONTINU A DOUBLE-ENVELOPPE EQUIPE D'UN SYSTEME DE CHAUFFAGE/REFROIDISSEMENT DE TYPE MONOFLUIDE. CE MONOFLUIDE CIRCULE, AVEC UN DEBIT CONSTANT, A TRAVERS LA DOUBLE-ENVELOPPE. LE CHAUFFAGE ET LE REFROIDISSEMENT SONT ASSURES RESPECTIVEMENT PAR UN CRAYON CHAUFFANT ET DEUX ECHANGEURS DE CHALEUR A PLAQUES. LE CONTROLE DE LA TEMPERATURE MASSE EST BASE SUR UNE STRATEGIE DE GESTION DES APPAREILS THERMIQUES PAR ANALYSE DES FLUX THERMIQUES LIMITES ET LA MODULATION DE LA TEMPERATURE DU MONOFLUIDE PAR ACTION SOIT SUR LA PUISSANCE DE CHAUFFE SOIT SUR LE TAUX D'OUVERTURE DE LA VANNE A TROIS VOIES QUI DERIVE LE MONOFLUIDE VERS L'ECHANGEUR. UN REACTEUR DE 1 LITRE A ETE UTILISE POUR VALIDER EXPERIMENTALEMENT CETTE ETUDE ET PARALLELEMENT, UN MODELE DE SIMULATION DE L'ENSEMBLE REACTEUR - SYSTEME THERMIQUE A ETE DEVELOPPE POUR REALISER RAPIDEMENT UN GRAND NOMBRE D'ESSAIS. NOUS AVONS ETUDIE DEUX TYPES DE COMMANDE AVANCEE, LA COMMANDE ADAPTATIVE ET PREDICTIVE GENERALISEE A DOUBLE MODELE DE REFERENCE (GPCMR) AVEC UNE POLITIQUE DE REGULATION EN PSEUDO-CASCADE ET LA COMMANDE PREDICTIVE FONCTIONNELLE (PFC) AVEC UNE POLITIQUE DE REGULATION EN CASCADE. L'ETUDE MONTRE QUE L'APPLICATION DE CES DEUX ALGORITHMES DONNE DE BONS RESULTAS ET QUE LA PRISE <b>EN</b> <b>COMPTE</b> DES DYNAMIQUES DES DIFFERENTS APPAREILS AMELIORE FORTEMENT LA QUALITE DE CONDUITE. DE PLUS, L'APPLICATION A LA CONDUITE THERMIQUE D'UNE REACTION CHIMIQUE EXOTHERMIQUE MONTRE QUE LA PREDICTION ET LA PRISE <b>EN</b> <b>COMPTE</b> DU DEGAGEMENT DE CHALEUR DANS LE MODELE DE REPRESENTATION PERMETTENT D'AMELIORER LES RESULTATS OBTENUS EN EVITANT TOUT DEPASSEMENT DE CONSIGNE AU DEBUT ET A LA FIN DE LA PHASE REACTIONNELLE. TOULOUSE-ENSIACET (315552325) / SudocSudocFranceF...|$|R
40|$|CE TRAVAIL DE THESE A VISE A AMELIORER LE CALCUL DES FILETS PARAVALANCHES ET LA PREDICTION DES EFFORTS AUXQUELS CES OUVRAGES SONT SOUMIS. DANS CE CADRE, SEULS LES MOUVEMENTS LENTS DE LA NEIGE SOUS L EFFET DE LA GRAVITE ONT ETE PRIS <b>EN</b> <b>COMPTE.</b> CE TRAVAIL EST COMPOSE DE DEUX PARTIES. DANS UN PREMIER TEMPS, L INTERACTION ENTRE LE MANTEAU ET LA STRUCTURE A ETE ABORDEE DANS UN CADRE BIDIMENSIONNEL PAR UNE DESCRIPTION CONTINUE DU MANTEAU (METHODE DES ELEMENTS FINIS). LA STRUCTURE, REPRESENTEE PAR UN CABLE, EST PRISE <b>EN</b> <b>COMPTE</b> AU TRAVERS D UNE CONDITION LIMITE MIXTE PERMETTANT DE RELIER LES EFFORTS DE REACTION APPLIQUES AUX NŒUDS AVAL DU MANTEAU A LEURS DEPLACEMENTS. CETTE APPROCHE A PERMIS DE CONSTATER QUE L INFLUENCE DE LA COMPOSANTE NORMALE A LA PENTE DE LA GRAVITE SUR LE CHARGEMENT APPLIQUE AU CABLE RESTAIT NEGLIGEABLE PAR RAPPORT A CELLE DE SA COMPOSANTE PARALLELE AU TERRAIN. CELA A CONDUIT A METTRE EN ŒUVRE, DANS UNE SECONDE PARTIE, UNE APPROCHE DISCRETE DANS LAQUELLE LE MANTEAU EST DECRIT PAR UN ENSEMBLE D ELEMENTS PARALLELEPIPEDIQUES RIGIDES POUVANT GLISSER ENTRE EUX SELON LA LIGNE DE PLUS GRANDE PENTE. LE PHENOMENE DE PENETRATION DU MANTEAU AU SEIN DU FILET PEUT ETRE AINSI DECRIT. DEVELOPPEE TOUT D ABORD DANS UN CADRE BIDIMENSIONNEL, CETTE APPROCHE A ETE ENSUITE ETENDUE AU CAS TRIDIMENSIONNEL. EN PARALLELE DE CE TRAVAIL DE MODELISATION, UNE IMPORTANTE CAMPAGNE D EXPERIMENTATIONS A ETE MENEE SUR UN OUVRAGE OPERATIONNEL IMPLANTE A FLAINE (HTE SAVOIE). L ENREGISTREMENT DES EFFORTS APPLIQUES AUX ANCRAGES DE LA STRUCTURE, COMPLETE PAR UNE CARACTERISATION REGULIERE DU MANTEAU, A PERMIS D OBTENIR DES PREMIERS ELEMENTS DE COMPARAISON TENDANT A CONFORTER LA VALIDITE DU MODELE. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|R
40|$|PRESENTATION DES MODELES RELATIFS AUX DIFFERENTS ELEMENTS CONSTITUTIFS DE LA BOUCLE THERMIQUE ET DU MODELE GLOBAL DE CELLE-CI. APPLICATION DE LA METHODE CLASSIQUE DE COMMANDE CLASSIQUE AVEC CRITERE QUADRATIQUE SANS PRENDRE <b>EN</b> <b>COMPTE</b> DE FACON EXPLICITE LES CONTRAINTES. ELABORATION D'UNE COMMANDE UTILISANT "L'ETAT ADJOINT" METTANT A PROFIT CERTAINES PARTICULARITES DU SYSTEME. SIMULATION DES COMMANDES POUR DIFFERENTS ENSOLEILLEMENTS ET DIFFERENTS CHANGEMENTS DE CONSIGNE PRESENTATION OF THE MODELS RELATIVE TO THE VARIOUS ELEMENTS CONSTITUTING THE THERMAL LOOP AND OF THE GLOBAL MODEL OF THE LOOP. APPLICATION OF THE CLASSICAL METHOD OF CONVENTIONAL DRIVE WITH QUADRATIC CRITERION WITHOUT EXPLICITELY TAKING INTO ACCOUNT THE CONSTRAINTS. ELABORATION OF A DRIVE USING THE "ADJOINT STATE" AND TAKING ADVANTAGE OF SOME PECULIARITIES OF THE SYSTEM. SIMULATION OF THE DRIVES FOR VARIOUS DEGREES OF SUNSHINE AND FOR VARIOUS CHANGES IN THE INSTRUCTIONSIndisponibl...|$|R
