250|10000|Public
5000|$|Performance characterization, {{and figures}} of merit Like most arenas in the {{physical}} sciences, chemometrics is quantitatively oriented, so considerable {{emphasis is placed on}} performance characterization, model selection, verification & validation, and {{figures of merit}}. The performance of quantitative models is usually specified by root mean squared error in predicting the attribute of interest, and the <b>performance</b> <b>of</b> <b>classifiers</b> as a true-positive rate/false-positive rate pairs (or a full ROC curve). A recent report by Olivieri et al. provides a comprehensive overview of figures of merit and uncertainty estimation in multivariate calibration, including multivariate definitions of selectivity, sensitivity, SNR and prediction interval estimation. [...] Chemometric model selection usually involves the use of tools such as resampling (including bootstrap, permutation, cross-validation).|$|E
50|$|In a {{paper to}} be {{published}} in proceedings the 2010 ACM Special Interest Group on Information Retrieval Conference, White and Horvitz present research on predicting escalations in medical concerns based on the structure and content of Web pages encountered during medical search sessions. They construct and then characterize the <b>performance</b> <b>of</b> <b>classifiers</b> that predict whether an escalation will occur in issued queries following the visit to a page. Their findings show that features such as serious illness preceding benign explanations in page (e.g., cancer is mentioned before caffeine in pages pertaining to headaches), serious illness vs. benign explanation appears in page title or near beginning of page, page from Web forum, and page has external verification are all important predictors of subsequent escalation (or non-escalation).|$|E
30|$|These merged feature sub list will be {{employed}} to learn to the supervised classifiers to compare the <b>performance</b> <b>of</b> <b>classifiers</b> with feature subsets obtained from individual feature selection method.|$|E
40|$|In this study, {{determination}} {{of the type of}} digital modulation of received signal in communication systems was aimed. For this purpose, high order cumulants, which are the most addressed ones, and frequency-based features were together examined for improving <b>performance</b> <b>of</b> <b>classifier.</b> Simulation studies were performed to evaluate the effect of these features on <b>performance</b> <b>of</b> <b>classifier,</b> both separately and together. Performed simulation studies showed that the proposed approach had higher performanc...|$|R
40|$|Naive Bayes {{classification}} {{algorithm is}} an effective simple classification algorithm. Most researches in traditional Naive Bayes classification focus on {{the improvement of the}} classification algorithm, ignoring the selection of training data which has a great effect on the <b>performance</b> <b>of</b> <b>classifier.</b> And so a method is proposed to optimize the selection of training data in this paper. Adopting this method, the noisy instances in training data are eliminated by user-defined effectiveness threshold, improving the <b>performance</b> <b>of</b> <b>classifier.</b> Experimental results on large-scale data show that our approach significantly outperforms the baseline classifier...|$|R
30|$|These {{extracted}} statistical {{features were}} fed into {{probabilistic neural network}} (PNN) classifier as an input for training and testing the <b>performance</b> <b>of</b> <b>classifier</b> in the classification of brain tumor images into normal and abnormal.|$|R
40|$|Abstract: Dimension {{reduction}} is important during {{the analysis of}} gene expression microarray data, because the high dimensionality in the data set hurts the generalisation <b>performance</b> <b>of</b> <b>classifiers.</b> Partial Least Squares Based Dimension Reduction (PLSDR) is a frequently used method, since it is specialised in handling high dimensional data set and leads to satisfying classification performance. However, the previous works exist an ambiguous usage of projection weights in PLSDR. To assure the orthogonality of projected components, the usually used project weights Copyright © 2009 Inderscience Enterprises Ltd. Orthogonal projection weights in dimension 101 are nonorthogonal. Here, we propose to use orthogonal project weights for PLSDR. Experimental results on four microarray data sets show our proposed orthogonal project weights are better than the previous used to help improve the generalisation <b>performance</b> <b>of</b> <b>classifiers...</b>|$|E
40|$|Our aim in {{this chapter}} is to study the {{conditions}} for the reasonably good <b>performance</b> <b>of</b> <b>classifiers</b> on brain functional magnetic resonance imaging. We propose a synthetic model for the systematic study of as-pects such as dimensionality, sample size, subject variability and noise. Our simulations highlight the key factors that affect generalization ac-curacy. ...|$|E
40|$|Gene {{function}} prediction from microarray data is a {{first step}} toward better understanding the machinery of the cell from relatively cheap and easy-to-produce data. In this paper we investigate whether the knowledge of many metabolic pathways and their catalyzing enzymes accumulated over the years can help improve the <b>performance</b> <b>of</b> <b>classifiers</b> for this problem...|$|E
40|$|Prediction {{and correct}} voting is {{critical}} task in imbalance data multi-class classification. Accuracy and <b>performance</b> <b>of</b> multi-class depends on voting and prediction of new class data. Assigning of {{new class of}} imbalance data generate confusion and decrease the accuracy and <b>performance</b> <b>of</b> <b>classifier.</b> Various authors and research modified the multiclass classification approach such as one against one and one against all. In both method OAO and OAA create a unclassified region for data and decrease the <b>performance</b> <b>of</b> <b>classifier</b> such as support vector machine. Some other method such as decision tree classifier, nearest neighbor and probability based classifier also suffered from imbalance data classification. In this paper we discuss various method and approach for multiclass classification for imbalance data...|$|R
30|$|The {{researchers}} [17] {{obtained the}} highest accuracy 86.9 after combining the feature selection method CHI, DFD and OCFS. They implemented a maximum entropy modelling (MEM) classifier to accomplish sentiment classification and the <b>performance</b> <b>of</b> <b>classifier</b> evaluated on movie review dataset with fivefold cross validation.|$|R
40|$|Conference Name: 2 nd International Conference on Mechatronics and Intelligent Materials 2012, MIM 2012. Conference Address: GuiLin, China. Time:May 18, 2012 - May 19, 2012. Naive Bayes {{classification}} {{algorithm is}} an effective simple classification algorithm. Most researches in traditional Naive Bayes classification focus on {{the improvement of the}} classification algorithm, ignoring the selection of training data which has a great effect on the <b>performance</b> <b>of</b> <b>classifier.</b> And so a method is proposed to optimize the selection of training data in this paper. Adopting this method, the noisy instances in training data are eliminated by user-defined effectiveness threshold, improving the <b>performance</b> <b>of</b> <b>classifier.</b> Experimental results on large-scale data show that our approach significantly outperforms the baseline classifier. 漏 (2012) Trans Tech Publications, Switzerland...|$|R
40|$|In this paper, {{we compare}} the <b>performance</b> <b>of</b> <b>classifiers</b> trained using word n-grams, {{character}} n-grams, and phoneme n-grams for recognizing subjective utterances in multiparty conversation. We {{show that there}} is value in using very shallow linguistic representations, such as character n-grams, for recognizing subjective utterances, in particular, gains in the recall of subjective utterances. Copyright © 2008 ISCA...|$|E
30|$|The {{remainder}} of the paper is organized as follows: ‘Measuring infection state’ section proposes infection betweenness centrality. In ‘Infection state estimation’ section, we introduce infection state classifiers using infection betweenness centrality and different centrality measures. ‘Experimental results’ section represents the experimental results about the <b>performance</b> <b>of</b> <b>classifiers</b> with infection betweenness centrality. ‘Related work’ section reviews the related literature. Finally, ‘Conclusion’ section presents our conclusions and future work.|$|E
40|$|Evaluating the <b>performance</b> <b>of</b> <b>classifiers</b> {{is not as}} trivial as {{it would}} seem at a first glance. Even {{the most widely used}} methods such as {{measuring}} accuracy or error rate on a test set has severe limitations. Two of the most prominent limitations of these measures are that they do not consider misclassification costs and can be misleading when the classes have very di#erent prior probabilities...|$|E
30|$|The {{conformance}} of {{the curves}} summarizes the good <b>performance</b> <b>of</b> the <b>classifiers.</b>|$|R
30|$|To {{measure the}} <b>performance</b> <b>of</b> the <b>classifier</b> on a “small” class, a good {{alternative}} is to use precision, recall, or F-score (the harmonic mean between precision and recall). But F-score measures the <b>performance</b> <b>of</b> a <b>classifier</b> for only one class. To report the aggregate performance over multiple classes, a good solution {{is to use the}} macro-average measure (average F-score over all k classes).|$|R
40|$|In many {{problems}} <b>of</b> classification, the <b>performances</b> <b>of</b> a <b>classifier</b> are often evaluated {{by a factor}} (rate of error). the factor is not well adapted for the complex real problems, in particular the problems multiclass. Our contribution consists in adapting an evolutionary method for optimization of this factor. Among the methods of optimization used we chose the method PSO (Particle Swarm Optimization) which {{makes it possible to}} optimize the <b>performance</b> <b>of</b> <b>classifier</b> SVM (Separating with Vast Margin). The experiments are carried out on corpus TIMIT. The results obtained show that approach PSO-SVM gives a better classification in terms of accuracy even though the execution time is increased...|$|R
40|$|This paper {{examines}} {{different approaches}} to remote sensing images classification. Included in the study are statistical approach, in particular Gaussian maximum likelihood classifier, and two different neural networks paradigms: multilayer perceptron trained with EDBD algorithm, and ARTMAP neural network. These classification methods are compared on data acquired from Landsat- 7 satellite. Experimental results showed that to achieve better <b>performance</b> <b>of</b> <b>classifiers</b> modular neural networks and committee machines should be applied...|$|E
40|$|The use of unlabeled {{samples in}} {{improving}} the <b>performance</b> <b>of</b> <b>classifiers</b> is studied. When the number of training samples is fixed and small, additional feature measurements may reduce {{the performance of a}} statistical classifier. It is shown that by using unlabeled samples estimates of the parameters can be improved and therefore this phenomenon may be mitigated. Various methods for using unlabeled samples are reviewed and experimental results are provided...|$|E
40|$|Abstract Background Since {{the high}} {{dimensionality}} of gene expression microarray data sets degrades the generalization <b>performance</b> <b>of</b> <b>classifiers,</b> feature selection, which selects relevant features and discards irrelevant and redundant features, {{has been widely}} used in the bioinformatics field. Multi-task learning is a novel technique to improve prediction accuracy of tumor classification by using information contained in such discarded redundant features, but which features should be discarded or used as input or output remains an open issue. Results We demonstrate a framework for automatically selecting features to be input, output, and discarded by using a genetic algorithm, and propose two algorithms: GA-MTL (Genetic algorithm based multi-task learning) and e-GA-MTL (an enhanced version of GA-MTL). Experimental results demonstrate that this framework is effective at selecting features for multi-task learning, and that GA-MTL and e-GA-MTL perform better than other heuristic methods. Conclusions Genetic algorithms are a powerful technique to select features for multi-task learning automatically; GA-MTL and e-GA-MTL are shown to to improve generalization <b>performance</b> <b>of</b> <b>classifiers</b> on microarray data sets. </p...|$|E
3000|$|... is larger, and {{parameters}} including ‘cost factor’ of {{each class}} and kernel parameter are {{need to be}} optimized. <b>Performance</b> <b>of</b> <b>classifier</b> is evaluated by average classification accuracy after k-fold cross-validation. Experiments demonstrate that comparing with earlier ABC algorithms, our method have great improvement on convergence rate, and better parameters are obtained which lead to higher classification accuracy.|$|R
40|$|This paper {{compare the}} <b>performance</b> <b>of</b> <b>classifier</b> {{algorithms}} on a stan- dard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report {{measurements of the}} fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold. 1...|$|R
3000|$|The <b>performance</b> <b>of</b> a <b>classifier</b> was {{determined}} using the computation {{of the following}} statistical parameters: [...]...|$|R
30|$|Note that {{increasing}} {{the size of}} feature vectors can in fact degrade the <b>performance</b> <b>of</b> <b>classifiers</b> due to the curse of dimensionality that occurs when {{there is not enough}} training data and the classifier cannot generalize and perform well on test data. That effect may be partly responsible for not observing an improvement with the feature fusion approach. For the same reason, feature fusion with larger number of features was not investigated.|$|E
40|$|International audienceEvaluating the <b>performance</b> <b>of</b> <b>classifiers</b> is a {{difficult}} task in machine learning. Many criteria have been proposed and used in such a process. Each criterion measures some facets of classifiers. However, none {{is good enough for}} all cases. In this communication, we justify the use of discrimination measures for evaluating classifiers. The justification is mainly based on a hierarchical model for discrimination measures, which was introduced and used in the induction of decision trees...|$|E
30|$|The final {{issue was}} {{selection}} of the appropriate machine learning method, which is able to learn a generalized attention model, applicable to any student or person in the classroom. We tackle this problem by preparing five combinations of input features and machine learning classifiers and data splitting strategies and analyze their accuracy on the test set of 18 persons (Section 5). We present detailed evaluation of results including comparison of <b>performance</b> <b>of</b> <b>classifiers</b> and discussion of method limitations.|$|E
30|$|For movie review dataset, 25, 000 {{samples are}} {{categorized}} as {{for training and}} another 25, 000 for testing purpose. However, we noticed the distribution is sub-optimal since the training samples are not sufficient according to 25, 000 testing reviews. Finally, to improve the <b>performance</b> <b>of</b> <b>classifier</b> we decided to use cross validation for movie as well as product review datasets.|$|R
30|$|It {{is often}} said that the <b>performance</b> <b>of</b> a <b>classifier</b> is data {{dependent}} [23]. Notwithstanding, work that proposes new classifiers usually neglects data-dependency when analyzing their performance. The strategy usually employed {{is to provide a}} few cases in which the proposed classifier outperforms baseline classifiers according to some performance measure. Similarly, theoretical studies that analyze the behavior <b>of</b> <b>classifiers</b> also tend to neglect data-dependency. They end up evaluating the <b>performance</b> <b>of</b> a <b>classifier</b> {{in a wide range of}} problems, resulting in weak performance bounds.|$|R
40|$|Abstract—This paper {{proposes a}} survey <b>of</b> the <b>performances</b> <b>of</b> binary <b>classifiers</b> based on {{low-level}} audio features, for music similarity in large-scale databases. Various low-level descriptors are used individually and then combined using several fusion schemes in a content-based audio retrieval system. We show the <b>performances</b> <b>of</b> the <b>classifiers</b> in terms <b>of</b> pruning and loss and we demonstrate that some combination schemes achieve a better performance {{at a minimum}} computational cost. I...|$|R
40|$|Dimensionality {{reduction}} {{is a well}} known technique in signal processing oriented to improve both the computational cost and the <b>performance</b> <b>of</b> <b>classifiers.</b> We use an electroencephalogram (EEG) feature matrix based on three extraction methods: tracks extraction, wavelets coefficients and Fractional Fourier Transform. The dimension {{reduction is}} performed by Mutual Information (MI) and a forward-backward procedure. Our results show that feature extraction and dimension reduction could {{be considered as a}} new alternative for solving EEG classification problems...|$|E
40|$|This paper proposes the use {{of mutual}} {{information}} for feature selection in multi-label classification, a surprisingly almost not studied problem. A pruned problem transformation method is first applied, transforming the multi-label problem into a single-label one. A greedy feature selection procedure based on multidimensional mutual information is then conducted. Results on three databases clearly demonstrate {{the interest of the}} approach which allows one to sharply reduce the dimension of the problem and to enhance the <b>performance</b> <b>of</b> <b>classifiers...</b>|$|E
40|$|Techniques and methodologies for validating the {{authenticity}} of digital images and testing {{for the presence of}} doctoring and manipulation operations on them has recently attracted attention. We review three categories of forensic features and discuss the design of classifiers between doctored and original images. The <b>performance</b> <b>of</b> <b>classifiers</b> with respect to selected controlled manipulations as well as to uncontrolled manipulations is analyzed. The tools for image manipulation detection are treated under feature fusion and decision fusion scenarios...|$|E
3000|$|The {{principle}} of AdaBoost is to linearly combine multiple component classifiers into ensemble <b>classifier.</b> The value <b>of</b> parameters (C and σ) plays {{a big role}} in the <b>performance</b> <b>of</b> the component <b>classifier</b> during the AdaBoost iterations. Different values of parameters will get different component classifiers, resulting in different <b>performance</b> <b>of</b> the ensemble <b>classifier.</b> Therefore, the <b>performance</b> <b>of</b> the ensemble <b>classifier</b> depends on the parameter value <b>of</b> each component <b>classifier.</b> Although the AdaBoostSVM algorithm can achieve good classification performance, it needs to set the value of C [...]...|$|R
40|$|Abstract—Floating Centroids Method (FCM) is a {{new method}} to improve the <b>performance</b> <b>of</b> neural network <b>classifier.</b> But the K-Means {{clustering}} algorithm used in FCM is sensitive to outliers. So this weakness will influence the <b>performance</b> <b>of</b> <b>classifier</b> to a certain extent. In this paper, K-Medoids clustering algorithm which can diminish the sensitivity to the outliers is used to partition the mapping points into some disjoint subsets to improve FCM’s robustness and performance. Some data sets from UCI Machine Learning Repository are employed in our experiments. The results show a better performance for the FCM using our improved method. Keywords-neural network; classification; clustering; K-means; K-medoids; Floating Centroids Method I...|$|R
3000|$|The {{idea behind}} AdaBoost, {{developed}} by Freund and Schapire [56], {{is to produce}} a series <b>of</b> <b>classifiers.</b> The training data used for {{each member of the}} series is chosen based on the <b>performance</b> <b>of</b> earlier <b>classifiers</b> in the series. Incorrectly predicted examples are selected more frequently than correctly predicted examples. Thus, boosting produces classifiers that are better in prediction that the current ensemble. Unlike bagging, AdaBoost considers <b>performance</b> <b>of</b> the earlier <b>classifiers.</b> The algorithm is detailed as follows: [...]...|$|R
