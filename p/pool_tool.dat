3|53|Public
50|$|There {{is also a}} <b>pool</b> <b>tool</b> feature, with {{a custom}} version and triangular, square and {{octagonal}} pool tools too.|$|E
50|$|In Nightlife, {{players can}} {{own their own}} cars instead of relying on the carpool {{to get to work}} or the bus to get to school. There are several models and colors available. In addition, {{driveways}} and garages can be built into the Sim's lot. Also new, more customized swimming pools can be designed. Included is a new diagonal shaped swimming <b>pool</b> <b>tool,</b> which can be used to create extensive shapes. However, the pool ladder and diving board may only be attached to pool tiles square on the lot.|$|E
5000|$|Another {{change in}} {{progress}} in Canadian English, {{part of a}} continental trend affecting many North American varieties, is the fronting of , whereby the nucleus of [...] moves forward to high-central or even high-front position, directly behind [...] There is {{a wide range of}} allophonic dispersion in the set of words containing [...] (i.e., the GOOSE set), extending over most of the high region of the vowel space. Most advanced are tokens of [...] in free position after coronals (do, too); behind these are tokens in syllables closed with coronals (boots, food, soon), then tokens before non-coronals (goof, soup); remaining in back position are tokens of [...] before [...] (cool, <b>pool,</b> <b>tool).</b> Unlike in some British speech, Canadian English does not show any fronting or unrounding of the glide of , and most Canadians show no parallel centralization of , which generally remains in back position, except in Cape Breton Island and Newfoundland.|$|E
2500|$|... a {{strategic}} challenge by the <b>pooling</b> of <b>tools</b> and {{the exchange of}} the good practices ...|$|R
50|$|For large {{projects}} with adequate funding, hydrodynamic resistance {{can be tested}} experimentally in a hull testing <b>pool</b> or using <b>tools</b> of computational fluid dynamics.|$|R
50|$|Startup studios own an {{infrastructure}} made of <b>pooled</b> resources: technical <b>tools,</b> management processes, and a multi-disciplinary team. By building several projects {{a year with}} the same team, startups studios can re-use this infrastructure, software and best practices across products.|$|R
40|$|The thesis {{deals with}} the {{individual}} aspects of liquidity and solvency management {{in the context of}} financial risk management and working capital components. The main accent is put on the foreign exchange risk management and the cash management tools on the group level: netting and cash <b>pooling.</b> These <b>tools</b> are supported by the cash forecasting system and the actual cash flows evaluation. The methods of liquidity and solvency management are demonstrated on the example of a Shared Service Center organisation...|$|R
40|$|Indonesia is an {{agricultural}} country with abundant natural resources, including fishery. Fishermen are found almost throughout parts of Indonesia with different size pool or pond. Manually feeding is ineffective {{because of the}} number of fish 2 ̆ 7 s food used was not always fixed even if it takes a long time feeding evenly throughout the <b>pool.</b> Spreader <b>tool</b> forage fish species AT 89 S 51 microcontroller-based pasta has been made to provide the feed of pasta with fixed volumes levelled DC motor based machine...|$|R
40|$|Summary: A host of formal, textual {{languages}} for modeling cellular processes {{have recently}} emerged, but their simulation tools often re-quire an installation process which can pose a barrier for use. Bio Simulators is {{a framework for}} easy online deployment of simulators, providing a uniform web-based user interface to a diverse <b>pool</b> of <b>tools.</b> The framework is demonstrated through two plugins based on the KaSim Kappa simulator, one running directly in the browser and another running in the cloud. Availability: Web tool: bsims. azurewebsites. net. KaSim client side simulator: github. com/NicolasOury/KaSimJS. KaSim cloud simulator: github. com/mdpedersen/KaSimCloud. Contact...|$|R
5000|$|A data {{monitoring}} switch is a networking hardware appliance {{that provides a}} <b>pool</b> of monitoring <b>tools</b> with access to traffic from {{a large number of}} network links. It provides a combination of functionality that may include aggregating monitoring traffic from multiple links, regenerating traffic to multiple tools, pre-filtering traffic to offload tools, and directing traffic according to one-to-one and many-to-many port mappings.|$|R
30|$|All this {{contributes}} to an ever growing visibility of the photogrammetric tools across various fields {{of science and}} engineering, the growing market interest, and subsequently a multitude of photogrammetric/computer vision libraries and software solutions, be it commercial or free/open-source. MicMac – together with Bundler, PMVS, VisualSfM, openMVG, OpenCV and others – belongs to the open-source solutions 1. This publication aims at familiarizing the reader with the philosophy behind MicMac, some of its crucial algorithmic aspects, the software architecture and the <b>pool</b> of available <b>tools.</b>|$|R
40|$|A video {{compression}} standard incorporates many tools and technologies {{which must be}} licensed by systems that deploy the standard. The licensing determines the royalty costs that must {{be paid to the}} holders of intellectual property on the respective tools. With current abundance of well understood and effective {{video compression}} tools, one can imagine the formation of cross cutting tool libraries with tools drawn from different video compression standards. This allows dynamic selection from a large <b>pool</b> of <b>tools,</b> having potentially overlapping functionality, when encoding individual video sequences. In this paper we examine the royalty cost aspect of the scenario where video is encoded using a library of royalty bearing tools by considering encoding that jointly optimizes rate, distortion, and royalty cost. We provide a system that optimizes video delivery under various licensing conditions imposed on tool intellectual property. We present an example of royalty based encoding (using assumed royalty costs) to show the merit of the proposed framework. 1...|$|R
40|$|Providing an {{integrated}} access to multiple heterogeneous sourcesis a challenging issue in global information systems for cooperation and interoperability. In the past, companies haveequipped themselves with data storing systems building upinformative systems containing {{data that are}} related one another,but which are often redundant, not homogeneous and not alwayssemantically consistent. Moreover, {{to meet the requirements}} ofglobal, Internet-based information systems, it is important thatthe tools developed for supporting these activities aresemi-automatic and scalable as much as possible. To face the issues related to scalability in the large-scale, in this paper we propose the exploitation of mobile agents in the information integration area, and, in particular, their integration in the Momis infrastructure. MOMIS (Mediator EnvirOnment for Multiple Information Sources) is a system that has been conceived as a <b>pool</b> of <b>tools</b> to provide {{an integrated}} access to heterogeneous information stored in traditional databases (for example relational, object oriented databases) or in file systems, as well as in semi-structured data sources (XML-file). This proposal has been implemented within the MIKS (Mediator agent for Integration of Knowledge Sources) system and it is completely described in this paper...|$|R
40|$|Design {{search and}} {{optimisation}} algorithms {{can be used}} by engineers to yield improved designs. Design search involving the analysis of the aerodynamic properties of a design, using Computational Fluid Dynamics, is both computationally and data intensive, making this problem well matched to Grid computing. Evaluation of the quality of a design (the objective function) may require commercial and user supplied software packages to be called in sequence, data transferred to and from suitable compute resources, in addition to preand post-processing. To allow engineers to express these necessarily complex workflows we expose a suite of Grid-enabled tools to a high-level scripting language. These tools include client functionality to Globus compute resources, and to a job submission Web service which exposes a cycle-scavenging Condor <b>pool.</b> These <b>tools</b> are exposed as functions to the Matlab environment that can be used directly by the engineer, or intergrated into higher level functions for design search and optimisation. Here the benefits of the scripting approach are discussed, together with details of the Grid middleware used and the implementation of the client functionality...|$|R
40|$|Artículos en revistasAlgorithms {{that involve}} {{some kind of}} {{optimisation}} have been adopted by several electricity <b>pools</b> as a <b>tool</b> to clear the market. Traditionally, this kind of models were used on a cost minimising basis, but recent papers {{have pointed out that}} alternative dispatches may be obtained that, even with higher production costs, provide cheaper electricity prices for consumers. This paper studies this new payment-minimisation approach, focusing on the long-term economic signals that it provides and analysing their impact on future investments. Our results show that minimising consumer payment results in discriminatory scheduling for certain generation resources and may cause, in the long-run, higher prices for consumers. info:eu-repo/semantics/publishedVersio...|$|R
50|$|The wood this {{particular}} tree produces {{is believed to}} be the blackest of all timber-producing Diospyros species, and the heartwood from this tree has been in extremely high demand since ancient Egyptian times. It is hard and durable with very fine pores, and it polishes to a high luster. It is used to make sculptures, carvings, <b>pool</b> cues, doorknobs, <b>tool</b> and knife handles, gun grips, the black keys on pianos, organ-stops, guitar fingerboards and bridges, and chess pieces. It is the wood of choice for the fingerboards, tailpieces, and tuning pegs used on all orchestral stringed instruments, including violins, violas, cellos, and double basses.|$|R
30|$|Alternatively, Electron {{transfer}} dissociation (ETD) and {{electron capture}} dissociation (ECD) have also shown great promise since the phosphate group remains attached {{during and after}} activation. Many detected phosphopeptides contain multiple Ser/Thr/Tyr residues representing the likely possibility {{that there is more}} than one possible location for the site of phosphorylation within the peptide. The abundant NL observed in low energy CID can hamper the correct assignment of the phosphor-sites in such peptides. Thus, a concerted effort has been made to understand, in detail, the rules of phosphopeptide fragmentation[72 – 76]. Finally, it must be said that the resulting proteomic-MS based data is <b>pooled</b> via bioinformatic <b>tools,</b> therefore, it is easier to interpret the biological and clinical meaning[77, 78].|$|R
40|$|This paper reports {{progress}} {{on a project}} to develop a design tool for large arrays of nucleation sites at specified locations to achieve high rates of cooling by <b>pool</b> boiling. The <b>tool</b> {{will be based on}} an improved version of a hybrid simulation, in which the 3 -D temperature field in the wall is solved numerically, along with simple sub-models for bubble-driven heat transfer that require experimental calibration. Improvements to the computer code and progress with the experiments are reported briefly. The paper focuses on the development of a sub-model for the lateral coalescence of bubbles, which is shown to cause irregularity in the bubble production by a regular array of nucleation sites...|$|R
40|$|Abstract. Providing an {{integrated}} access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In the past, companies have equipped themselves with data storing systems building up informative systems containing {{data that are}} related one another, but which are often redundant, not homogeneous and not always semantically consistent. Moreover, {{to meet the requirements}} of global, Internet-based information systems, {{it is important that the}} tools developed for supporting these activities are semi-automatic and scalable as much as possible. To face the issues related to scalability in the large-scale, in this paper we propose the exploitation of mobile agents in the information integration area, and, in particular, their integration in the MOMIS infrastructure. MOMIS (Mediator EnvirOnment for Multiple Information Sources) is a system that has been conceived as a <b>pool</b> of <b>tools</b> to provide {{an integrated}} access to heterogeneous information stored in traditional databases (for example relational, object oriented databases) or in file systems, as well as in semi-structured data sources (XML-file). This proposal has been implemented within the MIKS (Mediator agent for Integration of Knowledge Sources) system and it is completely described in this paper. ...|$|R
40|$|Despite of many Mining Software Repositories (MSR) {{tools in}} use, it is a {{relatively}} new research domain, which forms the basis of classifying various tools and comparing them. In this paper we present a comparative analysis of different tools for MSR, based on some existing and new criteria proposed in this paper. This will assist in determining an appropriate tool that performs the best for a given type of application and to use it directly, instead of relying on the usual trial-and-error approach. This work has several purposes; it acts as a formative evaluation mechanism for tool designers (by quickly understanding and comparing different tools), as an assessment tool for potential tool users (by simply going through the comparative analysis chart to know at a glance, the essential components needed to be incorporated into the intended tool) and as a comparative milestone so that MSR tool researchers can easily differentiate amongst a <b>pool</b> of <b>tools,</b> thereby identifying other new research avenues. The tabular presentation furthers the work by providing a quick index to the reader and a means for quick analysis of the desired tool...|$|R
40|$|OBJECTIVE: We {{reviewed}} case-control studies {{concerning the}} diagnostic accuracy of Heat Shock Protein 70 (Hsp- 70) auto antibodies in {{the detection of}} immunomediated inner ear disease. MATERIALS AND METHODS: We searched for relevant articles published in English language on PubMed and Scopus up to December 2011. A quality assessment of the retrieved articles was performed using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS) 2 <b>tool.</b> <b>Pooled</b> data on {{the accuracy of the}} test were calculated, where possible. RESULTS: Three articles were deemed eligible. Among them, 2 evaluated the relationship between Hsp- 70 and immunomediated inner ear disease by using the Western blot, whereas one report used the enzyme-linked immunosorbent assay method. Pooled sensitivity of Western blot test for Hsp- 70 was 0. 70 (95...|$|R
40|$|Computational {{forensic}} engineering (CFE) aims {{to identify}} the entity that created a particular intellectual property (IP). Rather than relying on watermarking content or designs, the generic CFE methodology analyzes the statistics of certain features of a given IP and quantizes {{the likelihood that a}} well known source has created it. In this paper, we describe the generic methodology of CFE and present a set of techniques that, given a <b>pool</b> of compilation <b>tools,</b> identify the one used to generate a particular hardware/software design. The generic CFE approach has four phases: feature and statistics data collection, feature extraction, entity clustering, and validation. In addition to IP protection, the developed CFE paradigm can have other potential applications: optimization algorithm selection and tuning, benchmark selection, and source-verification for mobile code...|$|R
40|$|Computationalforensic {{engineering}} (CFE) aims {{to identify}} the entity thatc reated a particTW- intellecGWproperty (IP). Rather than relying on watermarkingc ontent or designs, the generic CFE methodology analyzes the statistic ofc ertain features of a given IP and quantizes {{the likelihood that a}} well knownsourc hascW:fi:G it. In this paper, wedescG/ e the generic methodology of CFE and present a set of tec hniques that, given a <b>pool</b> ofc ompilation <b>tools,</b> identify the one used to generate a partic[Whardware /software design. Thegeneric CFE approac h has four phases: feature andstatistic data caW:;fi/bWfeature extracxfiTT entityc lustering, and validation. In addition to IPprotec tion, the developed CFE paradigmcr have other potential applicW-:G/[optimization algorithm selecthm and tuning, benc hmarkselec:/fiW and sourcTT erificbxWfor mobilec de. ...|$|R
40|$|Abstract. Computational {{forensic}} engineering (CFE) aims {{to identify}} the entity that created a particular intellectual property (IP). Rather than relying on watermarking content or designs, the generic CFE methodology analyzes the statistics of certain features of a given IP and quantizes {{the likelihood that a}} well known source has created it. In this paper, we describe the generic methodology of CFE and present a set of techniques that, given a <b>pool</b> of compilation <b>tools,</b> identify the one used to generate a particular hardware/software design. The generic CFE approach has four phases: feature and statistics data collection, feature extraction, entity clustering, and validation. In addition to IP protection, the developed CFE paradigm can have other potential applications: optimization algorithm selection and tuning, benchmark selection, and source-verification for mobile code. ...|$|R
40|$|The {{teaching}} of programming languages will often involve students being assessed {{by means of}} programming assignments. The administration of such assessments is a complex and demanding task which is compounded by increasing numbers of students. Many institutions have attempted {{to address this problem}} by developing computer-assisted learning and assessment systems. However, in the rush to use technology, they have circumscribed the scope and severely restricted the educational experience to be gained from these courses. At Warwick, we have adopted an evolutionary approach to computer science teaching, in which students are exposed to an evolving <b>pool</b> of different <b>tools</b> and techniques from which they can choose according to their own needs and preferences. It is in this context that we have developed software for the automatic submission of assignments. This paper discusses this approach and describes how our automatic submission system fits in with it. ...|$|R
40|$|In the general, the {{productivity}} of pulses is low and due to large number of abiotic and biotic constraints. Thus, major in improvement of pulses have on breeding. Excellent progress through traditional breeding methods {{has been made in}} development of varieties resistant to some diseases, e. g., fusarium wilt in chickpea and plg,eOllpea; sterility mosaic in pigeonpea; mungbean yellow mosaic virus (MYMV) in mungbean and urdbean; powdery mildew in urdbean, mungbean and pea; and rust in pea and lentil Similar level of success could not be achieved i 11 breeding for resistance to other stresses due to unavailability of high level of in the cultivated and cross compatible wild species. Mutation breeding has been rewarding in development of disease resistant varieties in chickpea and mungbean. Sources of high level of resistance have been identified in the wild species for several biotic stresses. Resistance to some stresses is available only in the wild species, for example, resistance to cyst nematodes and bruchids in wild Cicer. However, limited {{progress has been made in}} introgression of resistance from spe 1 cies into the cultivated species, mainly of barriers to interspecific hybridization cases. Concerted efforts are required to understand and to find the ways to overcome the barriers to hybridization between the cultivated species and the wild species of tertiary gene <b>pool.</b> New <b>tools</b> of biotechnology such as transgenic and molecular marker assisted technologies are now available that can be used to accelerate the progress of crop improvement...|$|R
40|$|International audienceInsulin {{secretion}} from pancreatic β-cells is {{a complex}} process, involving the integration and interaction of multiple external and internal stimuli, in which glucose plays a major role. Understanding the physiology leading to insulin release is a crucial step toward the identification of new targets. In this study, we evaluated the presence of insulinotropic metabolites in Naja kaouthia snake venom. Only one fraction, identified as cardiotoxin-I (CTX-I) was able to induce insulin secretion from INS- 1 E cells without affecting cell viability and integrity, as assessed by MTT and LDH assays. Interestingly, CTX-I was also able to stimulate insulin secretion from INS- 1 E cells {{even in the absence}} of glucose. Although cardiotoxins have been characterized as potent hemolytic agents and vasoconstrictors, CTX-I was unable to induce direct hemolysis of human erythrocytes or to induce potent vasoconstriction. As such, this newly identified insulin-releasing toxin will surely enrich the <b>pool</b> of existing <b>tools</b> to study β-cell physiology or even open a new therapeutic avenue...|$|R
30|$|Five {{criteria}} {{are used to}} “evaluate” each tool identified during RESTS’ Phase 2 for their applicability to the ROAM-guided project’s decision-making context. The tools are evaluated using five criteria that can assist analysts in selecting the most appropriate tools for each project. To minimize subjectivity in evaluating the tools, we ranked each on a 3 -point scale from typically most (1) to least (3) suitable for each criterion. The criteria build on a past review (Bagstad et al. 2013) and have been further refined in conjunction with IUCN. They include (1) scalability, (2) cost requirements, (3) time requirements, (4) reporting uncertainty, and (5) applicability to benefit cost analysis (BCA). A matrix showing each tool’s evaluation against the five criteria is provided in Additional file 2. The intent of the criteria is to show an analyst how each tool performs relative to other tools. This relative comparison will allow the analyst to choose from among a <b>pool</b> of suitable <b>tools</b> to select the tool(s) that best fit(s) their needs.|$|R
50|$|Octane is {{an active}} rock radio station on Sirius XM Satellite Radio channel 37 (previously 20) and Dish Network channel 6037. As {{a part of the}} Sirius XM Merger, Octane {{replaced}} the XM station SquiZZ on XM channel 48 (later moving to 37) and DirecTV channel 835 (until February 9, 2010). The channel is uncensored, and its musical focus is active rock in current development on. Core artists include Shinedown, Linkin Park, Breaking Benjamin, Disturbed, Alice in Chains, Drowning <b>Pool,</b> Pantera, Saliva, <b>Tool,</b> System of a Down, Three Days Grace, Evanescence, Anthrax, Rammstein, Rage Against the Machine, 10 Years, Five Finger Death Punch, Down, Staind, Godsmack, Korn, Rob Zombie, Nine Inch Nails, Static-X, Fear Factory, Audioslave, Seether, Slipknot, Lamb of God, Avenged Sevenfold, Otep, Bullet for My Valentine, Killswitch Engage, Limp Bizkit, Skillet, and Metallica. Octane is a newer active rock music similar to that played on most hard rock-leaning terrestrial radio stations in the United States, similar to the old Squizz on XM. Octane had the most fans of any Sirius XM Satellite Radio station on Facebook with over 200,000.|$|R
40|$|Geographic Information Systems (GIS) {{require the}} power of {{sophisticated}} database management systems for efficiently managing the persistency and consistency of huge amounts of data. A <b>pool</b> of software <b>tools</b> {{takes care of the}} production process, which consists of editing, visu-alising and selecting geographic data. However, the quality of the data – its well-formedness and integrity – is not so easily ensured. Quality is represented by declarative ex-pressions that constrain the relations between concepts of the geographic domain knowledge. However, there exists a mismatch between this implicit model which takes into account the spatial and multi-dimensional character of the data, and the format in which the geographic data is actu-ally stored in the database. Current practices have shown that hard coding these quality ensuring constraints using software engineering results in a loss of expressiveness and even accuracy. Therefore, in this research, we envisioned a co-existence of the usual components of a GIS with an ex-plicit representation of the geographic domain knowledge and constraints, for which we employed knowledge engi-neering techniques. In this paper we illustrate and validate our ideas at the hand of the GeoObjects System, a GIS we developed together with our industrial partner, TeleAtlas. 1...|$|R
40|$|Aim: To {{determine}} {{the prevalence of}} musculoskeletal disorders and anatomical regions which are most frequently injured in ballet dancers. Methods: Published (AMED, CiNAHL, EMBASE, SPORTDiscus, psycINFO, MEDLINE, the Cochrane Library) and grey literature databases (OpenGrey, the WHO International Clinical Trials Registry Platform, Current Controlled Trials and the UK National Research Register Archive) were searched from their inception to 25 th May 2015 for papers presenting data on injury prevalence in ballet dancers. Two reviewers independently identified all eligible papers, data extracted and critically appraised studies. Study appraisal was conducted using the CASP appraisal <b>tool.</b> <b>Pooled</b> prevalence data with 95 % confidence intervals were estimated to determine period prevalence of musculoskeletal disorders and anatomical regions affected. Results: Nineteen studies were eligible, reporting 7332 injuries in 2617 ballet dancers. The evidence was moderate in quality. Period prevalence of musculoskeletal injury was 280 % (95 % CI: 217 %- 343 %). The most prevalent musculoskeletal disorders included: hamstring strain (51 %), ankle tendinopathy (19 %) and generalized low back pain (14 %). No papers explored musculoskeletal disorders in retired ballet dancers. Conclusions: Whilst we have identified which regions and what musculoskeletal disorders are commonly seen ballet dancers. The long-term injury impact of musculoskeletal disorders in re-tired ballet dancers remains unknown...|$|R
40|$|Abstract Direct {{machining}} steel parts at a hardened state, known as hard turning, offers {{a number of}} potential benefits over traditional grinding in some applications. In addition, hard turning has several unique process characteristics, e. g., segmented chip formation and microstructural alterations at the machined surfaces, fundamentally different from con-ventional turning. Hard turning is, therefore, of a great interest to both the manufacturing industry and research community. Development of superhard materials such as polycrystalline cubic boron nitride (known as CBN) has been a key to enabling hard turning technology. A significant <b>pool</b> of CBN <b>tool</b> wear studies has been surveyed, {{in an attempt to}} achieve better processing and tooling applications, and discussed from the tool wear pattern and mechanism perspectives. Although various tool wear mechanisms, or a combination of several, coexist and dominate in CBN turning of hardened steels, {{it has been suggested that}} abrasion, adhesion (possibly complicated by tribochemical interactions), and diffusion may primarily govern the CBN tool wear in hard turning. Further, wear rate modeling including one approach developed in a recent study, on both crater and flank wear, is discussed as well. In conclusion, a summary of the CBN tool wear survey and the future work are outlined...|$|R
40|$|At the Atucha-I {{pressurized}} {{heavy water}} reactor in Argentina, fuel assemblies in {{the spent fuel}} pools are stored by suspending them in two vertically stacked layers. This introduces the unique problem of verifying the presence of fuel in either layer without physically moving the fuel assemblies. Movement of fuel, especially from the lower layer, would involve a major {{effort on the part}} of the operator. Given that the facility uses both natural uranium and slightly enriched uranium at 0. 85 w% {sup 235 }U, and has been in operation since 1974, a wide range of burnups and cooling times can exist in any given pool. Additionally, while fuel assemblies are grouped together in a uniform fashion, the packing density from group to group can vary within a single <b>pool.</b> A <b>tool</b> called the Spent Fuel Neutron Counter (SFNC) was developed and successfully tested at the site to verify, in an in-situ condition, the presence of fuel up to burnups of 8, 000 MWd/t. Since the neutron source term becomes a nonlinear function of burnup beyond this burnup, a new algorithm was developed to predict expected response from the SFNC at measurement locations covering the entire range of burnups, cooling times, and initial enrichments. With the aid of a static database of parameters including intrinsic sources and energy group-wise detector response functions, as well as explicit spent fuel information including burnups, cooling times, enrichment types, and spacing between fuel assemblies, an expected response for any given location can be calculated by summing the contributions from the relevant neighboring fuel assemblies. Thus, the new algorithm maps the expected responses across the various pools providing inspectors with a visual aid in verifying the presence of the spent fuel assemblies. This algorithm has been fully integrated into a standalone application built in LabVIEW. The GUI uses a step-by-step approach to allow the end-user to first calibrate the predicted database against a set of measurements with SFNC at selected locations where spent fuel is present. Once the database is calibrated it can be used to detect gross defects by comparing the measured signal to the one predicted by the database with differences beyond a set tolerance indicating missing fuel...|$|R
30|$|From {{the vast}} <b>pool</b> of {{potential}} <b>tools,</b> the Varanus collection service uses collectd, a well established data collection daemon with an extensive plugin library, to obtain {{data from the}} OS and applications. collectd is an ideal basis for building a more complex data collection tool as it has a small footprint (in terms of resource usage and its size on disk), has {{a wide range of}} plugins, an active development community and a simple, well designed architecture. The design of collectd is orientated entirely around plugins. The core of collectd does very little, its primary function it to invoke plugins which obtain data and then to invoke plugins which stores or transmits that data. Exactly which plugins are invoked, what data is collected and where it is transmitted is determined by a configuration file. This limits the effectiveness of collectd as an autonomic tool and necessitates a human or an external service provide this configuration. In large deployments it is common place to use Chef, Puppet, Salt or an alternative automation tool to install and configure collectd instances. For deploying and managing a monitoring tool, it is undesirable to have external dependencies as it is most desirable to ensure that the monitoring tool continues to function despite the failure of external services. It {{is for this reason that}} the collection service manages the configuration of collectd with no dependency other than the Varanus coordination service.|$|R
40|$|In {{the present}} work we discuss opportunities, problems, tools and {{techniques}} encountered when interconnecting discipline-specific subject classifications, primarily organized as search devices in bibliographic databases, with general classifications originally devised for book shelving in public libraries. We first state the fundamental distinction between topical (or subject) classifications and object classifications. Then we trace the structural limitations that have constrained subject classifications since their library origins, and the devices {{that were used}} to overcome the gap with genuine knowledge representation. After recalling some general notions on structure, dynamics and interferences of subject classifications and of the objects they refer to, we sketch a synthetic overview on discipline-specific classifications in Mathematics, Computing and Physics, on one hand, and on general classifications on the other. In this setting we present The Scientific Classifications Page, which collects groups of Web pages produced by a <b>pool</b> of software <b>tools</b> for developing hypertextual presentations of single or paired subject classifications from sequential source files, as well as facilities for gathering information from KWIC lists of classification descriptions. Further we propose a concept-oriented methodology for interconnecting subject classifications, with the concrete support of a relational analysis of the whole Mathematics Subject Classification through its evolution since 1959. Finally, we recall a very basic method for interconnection provided by coreference in bibliographic records among index elements from different systems, and point out the advantages of establishing the conditions of a more widespread application of such a method...|$|R
30|$|To build a {{successful}} cloud programming environment {{the advantages and}} functionality of traditional, desktop-based IDEs need to be maintained and augmented with additional features and strengths. Powerful code editors with a rich set of functionalities (e.g. highlighting, autofill, etc.) are incorporated in web browsers. Compilation (Ansari et al. 2011) and testing execution are performed on cloud infrastructures and in several cases deployment can be supported by cloud providers. Clear advantages of cloud-based IDEs include: (a) the access to a very wide <b>pool</b> of programming <b>tools</b> that are maintained by the provider, thus relieving the developer from the burden to setup, configure and upgrade their programming environments, (b) the ability to develop software {{without the use of}} powerful local computers, since the frequently compute-intensive tasks of the compilation and testing are performed remotely in the cloud, and (c) the straightforward way to reuse code developed by other software engineers that share the same cloud environment. However, a cloud application development environment needs to go far beyond a powerful toolchain to implement, debug and test code. For large-scale software projects to be viable and profitable, significant attention needs to be paid to the pre-implementation, design phase with the incorporation of appropriate modeling tools, and to post-implementation tasks including documentation and maintenance. Such features are rarely incorporated in cloud-based application development environments in their entirety a fact that makes them lag behind their desktop-based counterparts. On the other hand, network latency can also be a limiting factor (Piao and Yan 2010), especially for small jobs, while security and privacy concerns are certainly valid for valuable intellectual property as is the case of software code (Zhu et al. 2010; Zissis and Lekkas 2012).|$|R
