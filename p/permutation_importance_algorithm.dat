0|547|Public
40|$|This {{paper is}} about {{variable}} selection with the random forests algorithm in presence of correlated predictors. In high-dimensional regression or classification frameworks, variable selection {{is a difficult}} task, that becomes even more challenging {{in the presence of}} highly correlated predictors. Firstly we provide a theoretical study of the <b>permutation</b> <b>importance</b> measure for an additive regression model. This allows us to describe how the correlation between predictors impacts the <b>permutation</b> <b>importance.</b> Our results motivate the use of the Recursive Feature Elimination (RFE) algorithm for variable selection in this context. This algorithm recursively eliminates the variables using <b>permutation</b> <b>importance</b> measure as a ranking criterion. Next various simulation experiments illustrate the efficiency of the RFE algorithm for selecting a small number of variables together with a good prediction error. Finally, this selection algorithm is tested on a real life dataset from aviation safety where the flight data recorders are analysed for the prediction of a dangerous event...|$|R
30|$|Many {{forecasting}} methodologies provide specific metrics {{to support}} a decision on feature exclusion. Statistical methodologies apply hypothesis testing routines (i.e. Student’s test) for identifying significant features; neural networks allow estimation of elasticities of input components [40]; and random forests include <b>permutation</b> <b>importance</b> heuristics [41]. Using these metrics, researchers can refine the feature set of the forecasting model.|$|R
40|$|With the {{development}} of high-throughput single-nucleotide polymorphism (SNP) technologies, the vast number of SNPs in smaller samples poses {{a challenge to the}} application of classical statistical procedures. A possible solution is to use a two-stage approach for case-control data in which, in the first stage, a screening test selects a small number of SNPs for further analysis. The second stage then estimates the effects of the selected variables using logistic regression (logReg). Here, we introduce a novel approach in which the selection of SNPs is based on the <b>permutation</b> <b>importance</b> estimated by random forests (RFs). For this, we used the simulated data provided for the Genetic Analysis Workshop 15 without knowledge of the true model...|$|R
40|$|Motivation: Genome-wide {{association}} (GWA) {{studies have}} proven to be a successful approach for helping unravel the genetic basis of complex genetic diseases. However, the identified associations are not well suited for disease prediction, and only a modest portion of the heritability can be explained for most diseases, such as Type 2 diabetes or Crohn's disease. This may partly be due to the low power of standard statistical approaches to detect gene–gene and gene–environment interactions when small marginal effects are present. A promising alternative is Random Forests, which have already been successfully applied in candidate gene analyses. Important single nucleotide polymorphisms are detected by <b>permutation</b> <b>importance</b> measures. To this day, the application to GWA data was highly cumbersome with existing implementations because of the high computational burden...|$|R
30|$|Figure  8 (b) {{shows the}} top 16 {{predictors}} ranked by median out of sample <b>permutation</b> <b>importance</b> which gauges the relative increase in error {{that would result}} from introducing additional noise to that predictor while holding others at their observed values. The results provide several interesting insights. First, both telecommunications activity and land-cover measures consistently rank high in importance. Second, telecommunications activity averaged over a larger 3 × 3 window performs better than the 1 × 1 interpolation. Third, the top performing telecommunications measures include time windows and activities that are not just those that are mostly highly correlated with population but rather those that contribute unique information not found elsewhere (e.g. calls out made at 11  pm, text messages received at 8 am, and calls in at 2 am).|$|R
40|$|This article {{presents}} a stochastic method {{for studying the}} structure of large small worlds graphs. The principle of this method is to apply a PageRank-like <b>importance</b> <b>algorithm,</b> with a damping factor and an external importance source. By varying the source vector, one obtains a powerful graph visualization tool, which reveals the structural organization of small worlds graphs...|$|R
30|$|The RF {{classification}} {{algorithm is}} an extension of the classification and regression trees (CART) (Breiman 1984). Classification and regression trees have been widely used to group two or more known classes of observations based on a suite of predictor variables. Classification using CART is achieved through recursive partitioning of the dataset into successively more homogeneous groups. The results are homogeneous subsets of the data based on a series of splits using all of the predictor variables, where the best tree structure is determined by the Gini Index. We used the cforest function in the party package to build our random forest model which using conditional <b>permutation</b> <b>importance.</b> Random forest in this implementation produces multiple CART-like tree classifiers, each based on sub-sampling without replacement (Hothorn et al. 2006). The advantage of using the ctree function in the party package over the original random forest implementation by Breiman (2001) is that it produces unbiased individual trees.|$|R
40|$|We {{provide a}} {{different}} view of rejection sampling by putting it in the framework of impor-tance sampling. When rejection sampling with an envelope function g is viewed as a special <b>importance</b> sampling <b>algorithm,</b> we show that it is inferior to the <b>importance</b> sampling <b>algorithm</b> with g as the proposal distribution in terms of the Chi-square distance between the proposal distribution and the target distribution. Similar conclusions are drawn for comparing rejection control with importance sampling...|$|R
30|$|We used {{area under}} the {{receiver}} operating characteristic curve (AUC) values generated by the routine that tests the model with the 30 % withheld data to assess model performance. The AUC metric ranges from 0 to 1 and is {{the probability that a}} randomly chosen occupied site has a higher suitability value than a randomly chosen background site. An AUC value {{less than or equal to}} 0.5 represents a random result; a perfect result would achieve an AUC of 1.0 (Phillips et al. 2006). We assessed the importance of individual variables in each model by examining <b>permutation</b> <b>importance</b> (percent decrease in AUC when both presence and background training values of a variable are randomly shuffled) and the jack-knifed test that AUC values achieved without each variable and with each variable individually. The test AUC values for individual variables indicated which were most effective for predicting distribution of the 30 % set-aside data points; the higher the contribution to the AUC, the more influence that particular variable had on predicting environmental suitability for that community.|$|R
40|$|The {{prediction}} {{accuracy of}} short-term load forecast (STLF) depends on prediction model choice and feature selection result. In this paper, a novel random forest (RF) -based feature selection method for STLF is proposed. First, 243 related features were extracted from historical load {{data and the}} time information of prediction points to form the original feature set. Subsequently, the original feature set was used to train an RF as the original model. After the training process, the prediction error of the original model on the test set was recorded and the <b>permutation</b> <b>importance</b> (PI) value of each feature was obtained. Then, an improved sequential backward search method was used to select the optimal forecasting feature subset based on the PI value of each feature. Finally, the optimal forecasting feature subset was used to train a new RF model as the final prediction model. Experiments showed that the prediction accuracy of RF trained by the optimal forecasting feature subset was {{higher than that of}} the original model and comparative models based on support vector regression and artificial neural network...|$|R
40|$|We {{introduce}} the Piecewise-Constant Conditional Intensity Model, {{a model for}} learning temporal dependencies in event streams. We describe a closed-form Bayesian approach to learning these models, and describe an <b>importance</b> sampling <b>algorithm</b> for forecasting future events using these models, using a proposal distri-bution based on Poisson superposition. We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our <b>importance</b> sam-pling <b>algorithm</b> can effectively forecast future events. ...|$|R
40|$|This paper {{investigates the}} use of a class of <b>importance</b> {{sampling}} <b>algorithms</b> for probabilistic graphs in graphical structures. A general model for constructing <b>importance</b> sampling <b>algorithms</b> is given and then some particular cases are considered. Logical sampling and likelihood weighting are particular cases of the model. Our proposal will be an algorithm which uses the functions with less entropy (more informative) to simulate the variables and the functions with more entropy to weight the simulations, in this way we expec [...] ...|$|R
40|$|This paper {{considers}} importance sampling for {{estimation of}} rare-event probabilities in a Markovian intensity model {{commonly used in}} the context of credit risk. The main contribution is the design of efficient <b>importance</b> sampling <b>algorithms</b> using subsolutions of a certain Hamilton-Jacobi equation. We provide theoretical results that quantify the performance of <b>importance</b> sampling <b>algorithms</b> and for certain instances of the model under consideration the proposed algorithm is proved to be asymptotically optimal. The computational gain compared to standard Monte Carlo is illustrated by numerical examples...|$|R
5000|$|Migraine - A {{program which}} {{implements}} coalescent algorithms for a maximum likelihood analysis (using <b>Importance</b> Sampling <b>algorithms)</b> of genetic data {{with a focus}} on spatially structured populations.|$|R
40|$|Importance {{sampling}} is {{a popular}} method for efficient computation of various properties of a distribution such as probabilities, expectations, quantiles etc. The output of an <b>importance</b> sampling <b>algorithm</b> can be represented as a weighted empirical measure, where the weights are given by the likelihood ratio between the original distribution and the sampling distribution. In this paper the efficiency of an <b>importance</b> sampling <b>algorithm</b> is studied by means of large deviations for the weighted empirical measure. The main result, which is stated as a Laplace principle for the weighted empirical measure arising in importance sampling, {{can be viewed as}} a weighted version of Sanov's theorem. The main theorem is applied to quantify the performance of an <b>importance</b> sampling <b>algorithm</b> over a collection of subsets of a given target set as well as quantile estimates. The analysis yields an estimate of the sample size needed to reach a desired precision as well as of the reduction in cost for importance sampling compared to standard Monte Carlo...|$|R
30|$|The {{following}} experiments aim to {{show that}} the slice sampling-based strategy generates better estimates of a target posterior distribution compared to the <b>importance</b> re-sampling <b>algorithm</b> [14] and the Metropolis-Hastings algorithm.|$|R
40|$|This thesis {{consists}} of four papers, presented in Chapters 2 - 5, on the topics large deviations and stochastic simulation, particularly importance sampling. The four papers make theoretical contributions {{to the development of}} a new approach for analyzing efficiency of <b>importance</b> sampling <b>algorithms</b> by means of large deviation theory, and to the design of efficient algorithms using the subsolution approach developed by Dupuis and Wang (2007). In the first two papers of the thesis, the random output of an <b>importance</b> sampling <b>algorithm</b> is viewed as a sequence of weighted empirical measures and weighted empirical processes, respectively. The main theoretical results are a Laplace principle for the weighted empirical measures (Paper 1) and a moderate deviation result for the weighted empirical processes (Paper 2). The Laplace principle for weighted empirical measures is used to propose an alternative measure of efficiency based on the associated rate function. The moderate deviation result for weighted empirical processes is an extension of what can be seen as the empirical process version of Sanov's theorem. Together with a delta method for large deviations, established by Gao and Zhao (2011), we show moderate deviation results for importance sampling estimators of the risk measures Value-at-Risk and Expected Shortfall. The final two papers of the thesis are concerned with the design of efficient <b>importance</b> sampling <b>algorithms</b> using subsolutions of partial differential equations of Hamilton-Jacobi type (the subsolution approach). In Paper 3 we show a min-max representation of viscosity solutions of Hamilton-Jacobi equations. In particular, the representation suggests a general approach for constructing subsolutions to equations associated with terminal value problems and exit problems. Since the design of efficient <b>importance</b> sampling <b>algorithms</b> is connected to such subsolutions, the min-max representation facilitates the construction of efficient algorithms. In Paper 4 we consider the problem of constructing efficient <b>importance</b> sampling <b>algorithms</b> for a certain type of Markovian intensity model for credit risk. The min-max representation of Paper 3 is used to construct subsolutions to the associated Hamilton-Jacobi equation and the corresponding <b>importance</b> sampling <b>algorithms</b> are investigated both theoretically and numerically. The thesis begins with an informal discussion of stochastic simulation, followed by brief mathematical introductions to large deviations and importance sampling.   QC 20140424 </p...|$|R
40|$|Precision {{achieved}} by stochastic sampling algorithms for Bayesian networks typically deteriorates in face of extremely unlikely evidence. To address this problem, we propose the Evidence Pre-propagation <b>Importance</b> Sampling <b>algorithm</b> (EPIS-BN), an <b>importance</b> sampling <b>algorithm</b> that computes an approximate importance function using two techniques: loopy belief propagation [19, 25] and ɛ-cutoff heuristic [2]. We tested {{the performance of}} EPIS-BN on three large real Bayesian networks: ANDES [3], CPCS [21], and PathFinder [11]. We observed that {{on each of these}} networks the EPIS-BN algorithm outperforms AIS-BN [2], {{the current state of the}} art algorithm, while avoiding its costly learning stage. ...|$|R
40|$|AbstractThis paper {{investigates the}} use of a class of <b>importance</b> {{sampling}} <b>algorithms</b> for probabilistic graphs in graphical structures. A general model for constructing <b>importance</b> sampling <b>algorithms</b> is given and then some particular cases are considered. Logical sampling and likelihood weighting are particular cases of the model. Our proposal will be an algorithm which uses the functions with less entropy (more informative) to simulate the variables and the functions with more entropy to weight the simulations. In this way we expect to obtain more uniform weights. Some experimental tests are carried out comparing the performance of the proposed algorithms in randomly generated graphs...|$|R
40|$|Enormous {{advances}} in computing power and programming environments have obscured the <b>importance</b> of <b>algorithms,</b> {{one of the}} foundational pillars of software engineering. Today, even university curricula too often pay only {{lip service to the}} teaching of algorithmic fundamentals, reinforcing the popular belief that their place at the core of a software engineer's education is past. Yet even today, the <b>importance</b> of <b>algorithms</b> in software engineering has not diminished, and the effects of neglect are evident everywhere in needlessly inefficient industrial applications. The study of algorithms must regain its rightful place of central importance in the everyday work of today's practitioner...|$|R
40|$|Abstract. In his {{original}} paper on random forests, Breiman proposed two different decision tree ensembles: one generated from “orthogonal” trees with thresholds on individual features in every split, {{and one from}} “oblique ” trees separating the feature space by randomly oriented hyperplanes. In spite of a rising interest in the random forest framework, however, ensembles built from orthogonal trees (RF) have gained most, if not all, attention so far. In the present work we propose to employ “oblique ” random forests (oRF) built from multivariate trees which explicitly learn optimal split directions at internal nodes using linear discriminative models, rather than using random coefficients as the original oRF. This oRF outperforms RF, {{as well as other}} classifiers, on nearly all data sets but those with discrete factorial features. Learned node models perform distinctively better than random splits. An oRF feature importance score shows to be preferable over standard RF feature importance scores such as Gini or <b>permutation</b> <b>importance.</b> The topology of the oRF decision space appears to be smoother and better adapted to the data, resulting in improved generalization performance. Overall, the oRF propose here may be preferred over standard RF on most learning tasks involving numerical and spectral data. ...|$|R
40|$|Particle {{filtering}} {{can be used}} {{to handle}} the non－liner and non－Gaussian problems,while the sequential <b>importance</b> resampling (SIR) <b>algorithm</b> can be better to solve the degeneracy phenomenon in particle filtering and be applied in the semiblind estimation of MIMO time－varying channels. Simulation results show that compared to traditional particle filtering method,the MIMO time－varying channel semiblind estimation method based on the sequential <b>importance</b> resampling <b>algorithm</b> reduces the Mean square error and Symbol error rate,consequently improves the symbol detection performance at the receiver side...|$|R
40|$|Monte Carlo methods, such as {{importance}} sampling, {{have become}} a major tool in Bayesian inference. However, {{in order to produce}} an accurate estimate, the sampling distribution is required to be close to the target distribution. Several adaptive <b>importance</b> sampling <b>algorithms,</b> proposed over the last years, attempt to learn a good sampling distribution automatically, but their performance is often unsatisfactory. In addition, a theoretical analysis, which takes into account the computational cost of the sampling algorithms, is still lacking. In this paper, we present a first attempt of such analysis, and we propose some modifications of existing adaptive <b>importance</b> sampling <b>algorithms,</b> which produce significantly more accurate estimates...|$|R
40|$|Precision {{achieved}} by stochastic sampling algorithms for Bayesian networks typically deteriorates in face of extremely unlikely evidence. To address this problem, we propose the Evidence Pre-propagation <b>Importance</b> Sampling <b>algorithm</b> (EPIS-BN), an <b>importance</b> sampling <b>algorithm</b> that computes an approximate importance function by the heuristic methods: loopy belief Propagation and e-cutoff. We tested {{the performance of}} e-cutoff on three large real Bayesian networks: ANDES, CPCS, and PATHFINDER. We observed that {{on each of these}} networks the EPIS-BN algorithm gives us a considerable improvement over {{the current state of the}} art algorithm, the AIS-BN algorithm. In addition, it avoids the costly learning stage of the AIS-BN algorithm. Comment: Appears in Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI 2003...|$|R
40|$|Background: Random forests {{have often}} been claimed to uncover {{interaction}} effects. However, if and how interaction effects can be differentiated from marginal effects remains unclear. In extensive simulation studies, we investigate whether random forest variable importance measures capture or detect gene-gene interactions. With capturing interactions, we define the ability to identify a variable that acts through an interaction with another one, while detection {{is the ability to}} identify an interaction effect as such. Results: Of the single importance measures, the Gini importance captured interaction effects in most of the simulated scenarios, however, they were masked by marginal effects in other variables. With the <b>permutation</b> <b>importance,</b> the proportion of captured interactions was lower in all cases. Pairwise importance measures performed about equal, with a slight advantage for the joint variable importance method. However, the overall fraction of detected interactions was low. In almost all scenarios the detection fraction in a model with only marginal effects was larger than in a model with an interaction effect only. Conclusions: Random forests are generally capable of capturing gene-gene interactions, but current variable importance measures are unable to detect them as interactions. In most of the cases, interactions are masked by marginal effects and interactions cannot be differentiated from marginal effects. Consequently, caution is warranted when claiming that random forests uncover interactions...|$|R
40|$|Abstract With the {{development}} of high-throughput single-nucleotide polymorphism (SNP) technologies, the vast number of SNPs in smaller samples poses {{a challenge to the}} application of classical statistical procedures. A possible solution is to use a two-stage approach for case-control data in which, in the first stage, a screening test selects a small number of SNPs for further analysis. The second stage then estimates the effects of the selected variables using logistic regression (logReg). Here, we introduce a novel approach in which the selection of SNPs is based on the <b>permutation</b> <b>importance</b> estimated by random forests (RFs). For this, we used the simulated data provided for the Genetic Analysis Workshop 15 without knowledge of the true model. The data set was randomly split into a first and a second data set. In the first stage, RFs were grown to pre-select the 37 most important variables, and these were reduced to 32 variables by haplotype tagging. In the second stage, we estimated parameters using logReg. The highest effect estimates were obtained for five simulated loci. We detected smoking, gender, and the parental DR alleles as covariates. After correction for multiple testing, we identified two out of four genes simulated with a direct effect on rheumatoid arthritis risk and all covariates without any false positive. We showed that a two-staged approach with a screening of SNPs by RFs is suitable to detect candidate SNPs in genome-wide association studies for complex diseases. </p...|$|R
40|$|Backgroud: Type III {{secretion}} systems (T 3 SSs) {{are central}} to the pathogenesis and specifically deliver their secreted substrates (type III secreted proteins, T 3 SPs) into host cells. Since T 3 SPs {{play a crucial role in}} pathogen-host interactions, identifying them is crucial to our understanding of the pathogenic mechanisms of T 3 SSs. This study reports a novel and effective method for identifying the distinctive residues which are conserved different from other SPs for T 3 SPs prediction. Moreover, the importance of several sequence features was evaluated and further, a promising prediction model was constructed. Results: Based on the conservation profiles constructed by a position-specific scoring matrix (PSSM), 52 distinctive residues were identified. To our knowledge, this is the first attempt to identify the distinct residues of T 3 SPs. Of the 52 distinct residues, the first 30 amino acid residues are all included, which is consistent with previous studies reporting that the secretion signal generally occurs within the first 30 residue positions. However, the remaining 22 positions span residues 30 – 100 were also proven by our method to contain important signal information for T 3 SP secretion because the translocation of many effectors also depends on the chaperone-binding residues that follow the secretion signal. For further feature optimisation and compression, <b>permutation</b> <b>importance</b> analysis was conducted to select 62 optimal sequence features. A prediction model across 16 species was developed using random forest to classify T 3 SPs and non-T 3 SPs, wit...|$|R
40|$|In {{this paper}} we study rare event {{simulation}} for the tail probability of an affine point process (Jt) t≥ 0 that generalizes the Hawkes process. By constructing a suitable exponential martingale, {{we are able}} to construct an <b>importance</b> sampling <b>algorithm</b> that is logarithmically efficient in the Gartner-Ellis asymptotic regime. ...|$|R
40|$|Stochastic {{simulation}} can {{be applied}} to estimate the number of feasible solutions in a combinatorial problem. This idea will be illustrated to count the number of possible Sudoku grids. It will be argued why this becomes a rare-event simulation, and how an <b>importance</b> sampling <b>algorithm</b> resolves this difficulty. ...|$|R
40|$|We {{show that}} {{rejection}} sampling is {{inferior to the}} <b>importance</b> sampling <b>algorithm</b> {{in terms of the}} [chi] 2 distance between the proposal distribution and the target distribution. Similar conclusions are drawn for comparing rejection control with importance sampling. [chi] 2 distance Effective sample size Importance sampling Rejection control Rejection sampling...|$|R
40|$|To {{express the}} {{ambiguity}} and uncertainty of customer needs <b>importance,</b> an <b>algorithm</b> was proposed. It integrated KJ method, rough analytic Hierarchy Process and KANO model. It calculated the customer needs importance in rough set. A {{case study of}} cigarettes customer needs importance illustrated the feasibility {{and validity of the}} algorithm...|$|R
5000|$|... {{then it can}} {{be shown}} that the {{variance}} [...] vanishes, and the error in the estimate will be zero. [...] In practice {{it is not possible}} to sample from the exact distribution g for an arbitrary function, so <b>importance</b> sampling <b>algorithms</b> aim to produce efficient approximations to the desired distribution.|$|R
40|$|Probabilistic {{inference}} for Bayesian networks is {{in general}} NP-hard using either exact algorithms or approximate methods. However, for very complex networks, only the approximate {{methods such as}} stochastic sampling {{could be used to}} provide a solution given any time constraint. There are several simulation methods currently available. They include logic sampling- the first proposed stochastic method for Bayesian networks, the likelihood weighting algorithm- the most commonly used simulation method because of its simplicity and efficiency, the Markov blanket scoring method, and the <b>importance</b> sampling <b>algorithm.</b> In this paper, we first briefly review and compare these available simulation methods, then we propose an improved <b>importance</b> sampling <b>algorithm</b> called linear Gaussian <b>importance</b> sampling <b>algorithm</b> for general hybrid model (LGIS). LGIS is aimed for hybrid Bayesian networks consisting of both discrete and continuous random variables with arbitrary distributions. It uses linear function and Gaussian additive noise to approximate the true conditional probability distribution for continuous variable given both its parents and evidence in a Bayesian network. One of the most important features of the newly developed method is that it can adaptively learn the optimal important function from the previous samples. We test the inference performance of LGIS using a 16 -node linear Gaussian model and a 6 -node general hybrid model. The performance comparison with other well-known methods such as Junction tree (JT) and likelihood weighting (LW) shows that LGIS-GHM is very promising...|$|R
40|$|In {{automatic}} electrocardiogram (ECG) {{processing the}} detection of the QRS complexes is of fundamental <b>importance.</b> Many <b>algorithms</b> {{have been developed for}} this purpose. These algorithms are divided into three categories: (1) non-syntactic (2) syntactic and (3) hybrid. A syntactic algorithm, described by an attribute grammar, is presented in this paper. © 1986...|$|R
40|$|Thesis (M. S.) [...] University of Hawaii at Manoa, 2008. Includes bibliographical {{references}} (leaves 58 - 60). viii, 60 leaves, bound 29 cmFault {{detection and}} failure prediction for nonlinear non-Gaussian systems {{is an important}} issue both from the economic and safety point of view. Most of the fault detection techniques assume the system model to be linear and the noise to be Gaussian. These linearization assumptions tend to suffer form poor detection and imprecise prediction. Also, they may lead to false alarms which would incur unnecessary economic expenditure. This thesis aims at using particle filter approach for fault detection and failure prediction in nonlinear non-Gaussian systems. A major advantage of this approach is that the complete probability distribution information of the state estimates from particle filter is utilized for fault detection and failure prediction. Particle filtering methods represent and recursively generate an approximation of the posterior state probability density function. They are Sequential Monte Carlo Methods based on point mass representation of probability densities, which have been applied to the Vertical Take Off and Landing (VTOL) aircraft model and DC motor model in this thesis. Two variants of particle filters: Sequential <b>Importance</b> Sampling <b>Algorithm</b> and Sequential <b>Importance</b> Resampling <b>Algorithm</b> have been studied. Sequential <b>Importance</b> Sampling <b>algorithm</b> suffers from degeneracy problem because of which Sequential Importance Resampling technique is preferred. The system is represented in state space format and the estimates are made according to the Sequential <b>Importance</b> Resampling <b>algorithm.</b> The decision rule for fault detection is evaluated using the likelihood of the estimation parameter over a sliding window. The threshold values for fault detection are set using a heuristic approach. A fault is said to be detected if the likelihood exceeds the expected threshold value. A p-step ahead prediction is done for the DC motor model after the fault has been detected, which is utilized to determine the remaining useful life of the model...|$|R
40|$|In {{this short}} {{communication}} we analyze the tail asymptotics {{corresponding to the}} maximum value attained by a Lévy process with negative drift. The note has two contributions: a short and elementary proof of these asymptotics, and an <b>importance</b> sampling <b>algorithm</b> to estimate the rare-event probabilities under consideration. Keywords: Lévy processes; Second factorization identity; Tail asymptotics; Importance samplin...|$|R
