1401|7727|Public
5|$|In {{some cases}} {{parallelism}} is transparent to the programmer, {{such as in}} bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, {{are more difficult to}} write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good <b>parallel</b> <b>program</b> performance.|$|E
25|$|Parallel {{computer}} programs {{are more difficult}} to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good <b>parallel</b> <b>program</b> performance.|$|E
25|$|VMI was {{the last}} U.S. {{military}} college to admit women, having excluded women from the Corps of Cadets until 1997. In 1990 the U.S. Department of Justice filed a discrimination lawsuit against VMI for its all-male admissions policy. While the court challenge was pending, a state-sponsored Virginia Women's Institute for Leadership (VWIL) was opened at Mary Baldwin College in Staunton, Virginia, as a <b>parallel</b> <b>program</b> for women. The VWIL continued, even after VMI's admission of women.|$|E
40|$|<b>Parallel</b> <b>programming</b> offers {{potentially}} large performance {{benefits for}} computationally intensive problems. Unfortunately, {{it is difficult}} to obtain these benefits because <b>parallel</b> <b>programs</b> are more complex than their sequential counterparts. One way to reduce this complexity is to use a <b>parallel</b> <b>programming</b> system to write <b>parallel</b> <b>programs.</b> This dissertation shows a new approach to writing object-oriented <b>parallel</b> <b>programs</b> based on design patterns, frameworks, and multiple layers of abstraction. This approach is intended as the basis {{for a new generation of}} <b>parallel</b> <b>programming</b> systems. A critica...|$|R
40|$|Skeletal <b>parallel</b> <b>programming</b> ease <b>parallel</b> <b>programming</b> by {{providing}} efficient ready-made skeletons. However, nested use of skeletons {{are difficult to}} be implemented in an efficiently and load unbalanced way. In this paper, we propose a novel transformation theorem called the segmented diffusion theorem, {{an extension of the}} diffusion theorem, which can flatten many nested uses of skeletons into more efficient skeletal <b>parallel</b> <b>programs.</b> This theorem is not only useful for guiding construction of efficient skeletal <b>parallel</b> <b>programs,</b> but also suitable for optimizing skeletal <b>parallel</b> <b>programs</b> in compilers...|$|R
40|$|Introduction to <b>Parallel</b> <b>Programming</b> {{focuses on}} the techniques, processes, methodologies, and {{approaches}} involved in <b>parallel</b> <b>programming.</b> The book first offers information on Fortran, hardware and operating system models, and processes, shared memory, and simple <b>parallel</b> <b>programs.</b> Discussions focus on processes and processors, joining processes, shared memory, time-sharing with multiple processors, hardware, loops, passing arguments in function/subroutine calls, program structure, and arithmetic expressions. The text then elaborates on basic <b>parallel</b> <b>programming</b> techniques, barriers and rac...|$|R
25|$|According to Euzko Deya, on August 1, 1942, Lieutenants Nemesio Aguirre, Fernández Bakaicoa and Juanana {{received}} a Basque-coded message from San Diego for Admiral Chester Nimitz, warning {{him of the}} upcoming Operation Apple to remove the Japanese from the Solomon Islands. They also translated the start date, August 7, for the attack on Guadalcanal. As the war extended over the Pacific, there was a shortage of Basque speakers and the US military came to prefer the <b>parallel</b> <b>program</b> based {{on the use of}} Navajo speakers.|$|E
25|$|That same year, a CPI was {{established}} to investigate the military’s autonomous nuclear program. Among the main findings were details of illicit trade of nuclear material, {{as well as information}} about illegal financial operations that had served to keep the secret program going. In its final report, the inquiry commission recommended that the <b>parallel</b> <b>program</b> be dismantled with some of its activities to be integrated into the safeguarded civilian program. It also recommended the establishment of accountability mechanisms to increase the safety and security of the program.|$|E
25|$|The Air Scouts program {{established}} in 1941 and renamed Air Explorers in 1949, was dis{{established in}} 1965 and fully merged into the then existing Explorer {{program of the}} BSA as a specialty called 'Aviation Explorers', eventually discontinuing its uniforms by the early 1970s. It still exists today {{as part of the}} BSA's Learning for Life Explorer program. A <b>parallel</b> <b>program</b> with a nautical emphasis known as Sea Scouts continues to exist today as Sea Scouting, part of the Venturing program that the Boy Scouts of America offers for young men and women.|$|E
40|$|This {{research}} investigated {{an automated}} approach to re-writing traditional sequential computer <b>programs</b> into <b>parallel</b> <b>programs</b> for networked computers. A tool {{was designed and}} developed for generating <b>parallel</b> <b>programs</b> automatically and also executing these <b>parallel</b> <b>programs</b> on a network of computers. Performance is maximized by utilising all idle resources...|$|R
40|$|This book makes a clear {{presentation}} of the traditional topics included in a course of undergraduate <b>parallel</b> <b>programming.</b> As explained by the authors, it was developed from their own experience in classrooms, introducing their students to <b>parallel</b> <b>programming.</b> It can be used almost directly to teach basic <b>parallel</b> <b>programming...</b>|$|R
40|$|The {{architecture}} of system for optimization of <b>parallel</b> <b>programs</b> {{that allows the}} development, optimization and preparing of <b>parallel</b> <b>programs</b> is offered. The {{architecture of}} system is constructed as open system {{that it could be}} adjusted easily for use of various support libraries for <b>parallel</b> <b>programs</b> and optimization techniques...|$|R
25|$|The GAU-8 {{was created}} as a <b>parallel</b> <b>program</b> with the A-X (or Attack Experimental) {{competition}} that produced the A-10. The specification for the cannon was laid out in 1970, with General Electric and Philco-Ford offering competing designs. Both of the A-X prototypes, the YA-10 and the Northrop YA-9, were designed to incorporate the weapon, {{although it was not}} available during the initial competition; the M61 Vulcan was used as a temporary replacement. Once completed, the entire GAU-8 assembly (correctly referred to as the A/A 49E-6 Gun System) represents about 16% of the A-10 aircraft's unladen weight. Because the gun plays a significant role in maintaining the A-10's balance and center of gravity, a jack must be installed beneath the airplane's tail whenever the gun is removed for inspection in order to prevent the aircraft from tipping rearwards.|$|E
500|$|Subtasks in a <b>{{parallel}}</b> <b>program</b> {{are often}} called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, [...] "threads" [...] is generally {{accepted as a}} generic term for subtasks. Threads will often need to update some variable that is shared between them. The instructions between the two programs may be interleaved in any order. For example, consider the following program: ...|$|E
500|$|One of {{the first}} {{consistency}} models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a <b>parallel</b> <b>program</b> that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if [...] "… the results of any execution {{is the same as}} if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program".|$|E
40|$|Developing {{applications}} {{is becoming}} increasingly difficult due to recent growth in machine complexity along many dimensions, especially that of parallelism. We are studying data types {{that can be used}} to represent data parallel operations. Developing <b>parallel</b> <b>programs</b> with these data types have numerous advantages and such a strategy should facilitate <b>parallel</b> <b>programming</b> and enable portability across machine classes and machine generations without significant performance degradation. In this paper, we discuss our vision of data <b>parallel</b> <b>programming</b> with powerful abstractions. We first discuss earlier work on data <b>parallel</b> <b>programming</b> and list some of its limitations. Then, we introduce several dimensions along which is possible to develop more powerful data <b>parallel</b> <b>programming</b> abstractions. Finally, we present two simple examples of data <b>parallel</b> <b>programs</b> that make use of operators developed as part of our studies. ...|$|R
40|$|FastFlow is a C++ <b>parallel</b> <b>programming</b> {{framework}} advocating high-level, pattern-based <b>parallel</b> <b>programming.</b> It chiefly supports streaming {{and data}} parallelism, targeting heterogenous platforms composed of clusters of shared-memory platforms, possibly equipped with computing accelerators such as GPGPUs, Xeon Phi, Tilera TILE 64. The main design philosophy of FastFlow {{is to provide}} application designers with key features for <b>parallel</b> <b>programming</b> (e. g. time-to-market, efficiency, functional and performance portability) via suitable <b>parallel</b> <b>programming</b> abstractions and a carefully designed run-time support...|$|R
40|$|Structured <b>parallel</b> <b>programming,</b> and in {{particular}} programming models using the algorithmic skeleton or parallel design pattern concepts, are increasingly {{considered to be the}} only viable means of supporting effective development of scalable and efficient <b>parallel</b> <b>programs.</b> Structured <b>parallel</b> <b>programming</b> models have been assessed in a number of works in the context of performance. In this paper we consider how the use of structured <b>parallel</b> <b>programming</b> models allows knowledge of the parallel patterns present to be harnessed to address both performance and energy consumption. We consider different features of structured <b>parallel</b> <b>programming</b> that may be leveraged to impact the performance/energy trade-off and we discuss a preliminary set of experiments validating our claims...|$|R
500|$|Task parallelisms is the {{characteristic}} of a <b>parallel</b> <b>program</b> that [...] "entirely different calculations can be performed on either the same or different sets of data". This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks simultaneously and often cooperatively. Task parallelism does not usually scale {{with the size of}} a problem.|$|E
500|$|In 1963, the USAF {{asked for}} {{proposals}} for an Airborne Warning and Control System (AWACS) to replace its EC-121 Warning Stars, which {{had served in}} the airborne early warning role for over a decade. The new aircraft would take advantage of improvements in radar technology and in computer aided radar data analysis and data reduction. These developments allowed airborne radars to [...] "look down", detect the movement of low-flying aircraft (see Look-down/shoot-down), and discriminate, even over land, target aircraft's movements—previously this had been impossible, due to the inability to discriminate an aircraft's track from ground clutter. Contracts were issued to Boeing, Douglas, and Lockheed, the latter being eliminated in July 1966. In 1967, a <b>parallel</b> <b>program</b> was put into place to develop the radar, with Westinghouse Electric and the Hughes Aircraft being asked to compete in producing the radar system. In 1968, it was referred to as Overland Radar Technology (ORT) during development tests on the modified EC-121Q. The Westinghouse radar antenna was going to be used by whichever company won the radar competition, since Westinghouse had pioneered in the design of high-power RF phase-shifters.|$|E
2500|$|In 1977, Intiman opened year-round {{administrative}} {{offices in}} Pioneer Square and hired Simon Siegl {{as its first}} general manager. With a season of five classic plays, Intiman also began a <b>parallel</b> <b>program</b> [...] "New Plays Onstage," [...] staged readings of contemporary works directed and performed {{by members of the}} ensemble. Over the next several years, Intiman was awarded institutional status by the King County and Washington State Arts Commissions and received an NEA challenge grant.|$|E
40|$|Pact is a <b>parallel</b> <b>programming</b> {{environment}} relieving the programmer {{from the}} burdens of <b>parallel</b> <b>programming</b> which are not really necessary to write efficient <b>parallel</b> <b>programs.</b> This is done by providing a simple synchronization model and virtual shared data with user-defined granularity and automatic consistency control. Pact guarantees user-transparent fault-tolerance with low overhead by using atomic actions as basic units of parallel execution. Additionally, the runtime system maps parallel actions to server processes using dynamic load-balancing. An included on-line visualization tool helps tuning and debugging <b>parallel</b> <b>programs.</b> ...|$|R
40|$|<b>Parallel</b> <b>programming</b> {{is seen as}} an eective {{technique}} to improve the performance of computationally-intensive programs. This is done at the cost of increasing the complexity of the program, since new issues must be addressed for a concurrent application. <b>Parallel</b> <b>programming</b> environments provide a way for users to reap the bene ts of concurrent programming while minimizing the eort required to create them. The CO 2 P 3 S <b>parallel</b> <b>programming</b> system is one such tool which uses a pattern-based approach to create <b>parallel</b> <b>programs...</b>|$|R
40|$|The {{availability}} of modern commodity multicore processors and multiprocessor computer systems {{has resulted in}} the widespread adoption of parallel computers in a variety of environments, ranging from the home to workstation and server environments in particular. Unfortunately, <b>parallel</b> <b>programming</b> is harder and requires more expertise than the traditional sequential programming model. The variety of tools and <b>parallel</b> <b>programming</b> models available to the programmer further complicates the issue. The primary goal of this research was to identify and describe a selection of <b>parallel</b> <b>programming</b> tools and techniques to aid novice parallel programmers {{in the process of developing}} efficient <b>parallel</b> C/C++ <b>programs</b> for the Linux platform. This was achieved by highlighting and describing the key concepts and hardware factors that affect <b>parallel</b> <b>programming,</b> providing a brief survey of commonly available software development tools and <b>parallel</b> <b>programming</b> models and libraries, and presenting structured approaches to software performance tuning and <b>parallel</b> <b>programming.</b> Finally, the performance of several <b>parallel</b> <b>programming</b> models and libraries was investigated, along with the programming effort required to implement solutions using the respective models. A quantitative research methodology was applied to the investigation of the performance and programming effort associated with the selected <b>parallel</b> <b>programming</b> models and libraries, which included automatic parallelisation by the compiler, Boost Threads, Cilk Plus, OpenMP, POSIX threads (Pthreads), and Threading Building Blocks (TBB). Additionally, the performance of the GNU C/C++ and Intel C/C++ compilers was examined. The results revealed that the choice of <b>parallel</b> <b>programming</b> model or library is dependent on the type of problem being solved and that there is no overall best choice for all classes of problem. However, the results also indicate that <b>parallel</b> <b>programming</b> models with higher levels of abstraction require less programming effort and provide similar performance compared to explicit threading models. The principle conclusion was that the problem analysis and parallel design are an important factor in the selection of the <b>parallel</b> <b>programming</b> model and tools, but that models with higher levels of abstractions, such as OpenMP and Threading Building Blocks, are favoured. ...|$|R
2500|$|The Ground Combat Infantry Fighting Vehicle was an {{infantry}} fighting vehicle {{being developed}} for the U.S. Army. The program originated as the lead vehicle of the U.S. Army's Ground Combat Vehicle program coordinated by TACOM and spawned a <b>parallel</b> <b>program</b> coordinated by DARPA. The purpose {{of the program was}} to replace existing armored personnel carriers and infantry fighting vehicles in U.S. Army service. [...] The DARPA project aimed to have the vehicle designed by 2015. Derivatives of the vehicle based on a common chassis—such as tanks and ambulances—were expected to be manufactured. It replaced the previous attempt at a next-generation infantry transport, the XM1206 Infantry Carrier Vehicle. The Ground Combat Vehicle program was cancelled in February 2014.|$|E
50|$|Relaxed {{sequential}} {{in computer}} science is an execution model describing the ability for a <b>parallel</b> <b>program</b> to run sequentially. If a <b>parallel</b> <b>program</b> has a valid sequential execution {{it is said to}} follow a relaxed sequential execution model. It {{does not need to be}} efficient.|$|E
5000|$|MPView - A visual {{profiling}} and debugger for <b>parallel</b> <b>program</b> ...|$|E
40|$|Parallel Java is a <b>parallel</b> <b>programming</b> API whose {{goals are}} (1) to support both shared memory (thread-based) <b>parallel</b> <b>programming</b> and cluster (message-based) <b>parallel</b> <b>programming</b> {{in a single}} unified API, {{allowing}} one to write <b>parallel</b> <b>programs</b> combining both paradigms; (2) to provide the same capabilities as OpenMP and MPI in an object oriented, 100 % Java API; and (3) to be easily deployed and run in a heterogeneous computing environment of single-core CPUs, multi-core CPUs, and clusters thereof. This paper describes Parallel Java’s features and architecture; compares and contrasts Parallel Java to other Javabased parallel middleware libraries; and reports performance measurements of <b>Parallel</b> Java <b>programs.</b> 1...|$|R
40|$|Abstract. This paper {{proposes a}} novel View-Oriented <b>Parallel</b> <b>Programming</b> style for <b>parallel</b> <b>programming</b> on cluster computers. View-Oriented <b>Parallel</b> <b>Programming</b> {{is based on}} Distributed Shared Memory. It {{requires}} the programmer to divide the shared memory into views according {{to the nature of}} the parallel algorithm and its memory access pattern. The advantage of this programming style is that it can help the Distributed Shared Memory system optimise consistency maintenance. Also it allows the programmer to participate in performance optimization of a program through wise partitioning of the shared memory into views. The View-based Consistency model and its implementation, which supports View-Oriented <b>Parallel</b> <b>Programming,</b> is discussed as well in this paper. Finally some preliminary experimental results are shown to demonstrate the performance gain of View-Oriented <b>Parallel</b> <b>Programming.</b> ...|$|R
40|$|This paper {{describes}} {{how to reduce}} the burden of <b>parallel</b> <b>programming</b> by utilizing relevant <b>parallel</b> <b>programs.</b> <b>Parallel</b> algorithms are divided into four classes and a case base for <b>parallel</b> <b>programming</b> is developed by retrieving <b>parallel</b> <b>programs</b> in each class. Cases consist of indices, a skeleton, a program, parallelization effects and a history. Skeletons include {{the most important issues}} such as task division, synchronization, mutual exclusion, parallelization methods and threads. <b>Parallel</b> <b>programs</b> for image data storage, three dimensional spline, edge detection, thinning, knapsack problem and package wrapping algorithm are developed by retrieving the most relevant case and adapting it to the given problem. The experiment demonstrates that threads and synchronization can be reused from skeletons, and task division should be adapted by programmers. 1...|$|R
5000|$|Here&Now. Fabrika Project. Moscow, Russia (<b>parallel</b> <b>program,</b> 3rd Moscow Biennale of Contemporary Art) ...|$|E
5000|$|LUX. pop/off/art Gallery. Moscow, Russia (<b>parallel</b> <b>program,</b> 2nd Moscow Biennale of Contemporary Art) ...|$|E
5000|$|... 2007: Light Time, <b>Parallel</b> <b>program</b> to the 2. Biennale Moscow, Krokin Gallery, Russia ...|$|E
40|$|Trees {{are useful}} data types, but {{developing}} efficient <b>parallel</b> <b>programs</b> manipulating trees {{is known to}} be difficult, because of their irregular and imbalance structure. Parallel tree skeletons are designed to ease <b>parallel</b> <b>programming</b> by encouraging programmers to build <b>parallel</b> <b>programs</b> by combining them. However, for distributed systems, efficient implementations of these parallel tree skeletons are known to be hard. In thi...|$|R
40|$|With {{the arrival}} of {{multicore}} systems, <b>parallel</b> <b>programming</b> is becoming increasingly mainstream. Writing correct <b>parallel</b> <b>programs,</b> however, {{has turned out to}} be difficult and prone to errors without proper support from the employed programming languages, compilers, and runtime systems. Over the last years, researchers and engineers have developed numerous abstractions and programming models that make developing <b>parallel</b> <b>programs</b> easier, safer, and more efficient. Despite the advances made in <b>parallel</b> <b>programming</b> models, libraries, and runtime systems, the corresponding compilers still remain largely ignorant of the parallelism exhibited by the program execution. In particular, current compilers do not have any knowledge about what tasks are scheduled when in the program and how they are ordered—even though many higherlevel <b>parallel</b> <b>programming</b> models and libraries contain a wealth of task-order information that can be exploited by the compiler. Without task-scheduling knowledge, however, compilers are missing important optimization and verification opportunities. This dissertation explores how compilers for <b>parallel</b> <b>programs</b> with shared memory ca...|$|R
40|$|With the {{increasing}} popularity of <b>parallel</b> <b>programming</b> environments such as PC clusters, {{more and more}} sequential programmers, with little knowledge about parallel architectures and <b>parallel</b> <b>programming,</b> are hoping to write <b>parallel</b> <b>programs.</b> Numerous {{attempts have been made}} to develop high-level <b>parallel</b> <b>programming</b> libraries that use abstraction to hide low-level concerns and reduce difficulties in <b>parallel</b> <b>programming.</b> Among them, libraries of parallel skeletons have emerged as a promising way towards this direction. Unfortunately, these libraries are not well accepted by sequential programmers, because of incomplete elimination of lower-level details, ad-hoc selection of library functions, unsatisfactory performance, or lack of convincing application examples. This paper addresses principle of designing skeleton libraries of <b>parallel</b> <b>programming</b> and reports implementation details and practical applications of a skeleton library SkeTo. The SkeTo library is unique in its feature that it has a solid theoretical foundation based on the theory of Constructive Algorithmics, and is practical to be used to describe various parallel computations in a sequential manner. 1...|$|R
