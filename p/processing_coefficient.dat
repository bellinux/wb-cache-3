1|79|Public
30|$|We tested ECT in two {{different}} computing environments that were typical in the public clouds, the slightly and highly heterogeneous environments. To resemble the heterogeneous environment in the clouds, we assumed the base processing speeds of all 1000 task slots were evenly distributed within a fixed range, while the two slots on the same VM had identical base speed. In the simulation, we actually used slot processing times to represent slot processing speeds. There were two types of slot processing times, Local Processing Time (LPT) and Remote Processing Time (RPT), which indicated {{the amount of time}} it took a slot to process a local and remote block, respectively. LPT was randomly generated within the range [(1 -VPPT)t, (1 [*]+[*]VPPT)t], where t was the base processing time of the task slot, and VPPT was the Variation Percentage of Processing Time, which was used to reflect the fact that the actual slot processing times fluctuated during the whole processing period. Most MapReduce jobs belong to the relatively short and interactive category, so their job completion times are usually measured in minutes instead of hours. As a result, {{it is unlikely that the}} actual slot processing times would fluctuate significantly during the whole processing period. Therefore, we ran the simulation at three different VPPT values: 2.5 %, 5 % and 10 %. The base processing time t’s of slots on different VM’s were assumed to be evenly distributed within the range [(1 -P)T, (1 [*]+[*]P)T], where T[*]=[*] 100, 000 / total number of data blocks. In the simulation, the value of P was set to 0.2 and 0.5 to resemble the slightly and highly heterogeneous environments respectively, which was based on the experimental results obtained on Amazon EC 2 by Zaharia et al. [10]. The RPT consisted of two parts, the LPT and the amount of time it took the processing slot to fetch the data bock across the network. In the simulation, the RPT was calculated as (RPC[*]×[*]LPT), where RPC was the Remote <b>Processing</b> <b>Coefficient</b> used to reflect the overhead of fetching remote data blocks and hence was greater than one. Since LPT was randomly generated within the range [(1 -VPPT)t, (1 [*]+[*]VPPT)t], RPT was randomly generated within the range [RPC(1 -VPPT)t, RPC(1 [*]+[*]VPPT)t].|$|E
40|$|M-DAS is {{a ground}} data {{processing}} system designed for analysis of multispectral data. M-DAS operates on multispectral data from LANDSAT, S- 192, M 2 S and other sources in CCT form. Interactive training by operator-investigators using a variable cursor on a color display was used to derive optimum <b>processing</b> <b>coefficients</b> and data on cluster separability. An advanced multivariate normal-maximum likelihood processing algorithm was used to produce output in various formats: color-coded film images, geometrically corrected map overlays, moving displays of scene sections, coverage tabulations and categorized CCTs. The analysis procedure for M-DAS involves three phases: (1) screening and training, (2) analysis of training data to compute performance predictions and <b>processing</b> <b>coefficients,</b> and (3) <b>processing</b> of multichannel input data into categorized results. Typical M-DAS applications involve iteration between each of these phases. A series of photographs of the M-DAS display are used to illustrate M-DAS operation...|$|R
3000|$|The <b>processing</b> of <b>coefficient</b> Ci(k) {{balances}} the transparency and robustness of watermark. The specific methods are as follows: [...]...|$|R
40|$|The authors {{present a}} number of low power FIR filter {{implementations}}. The implementations are based on <b>processing</b> <b>coefficients</b> in a non-conventional order using both direct form and transpose direct form realisation of FIR filters. The paper describes the overall filter architecture of each implementation and their key components. An overall power reduction of up to 34 % is reported as compared to a conventional filter implementation. A power profile of the main contributing components in each implementation is also provided. 1...|$|R
40|$|A large river system {{typically}} derives {{the majority}} of its biomass from production within the floodplain. The Neches River in the Big Thicket National Preserve is a large blackwater river that has an extensive forested floodplain. Organic carbon was analyzed within the floodplain waters and the river (upstream and downstream of the floodplain) to determine the amount of organic carbon from the floodplain that is contributing to the nutrient dynamics in the river. Dissolved organic carbon was significantly higher at downstream river locations during high discharge. Higher organic carbon levels in the floodplain contributed to increases in organic carbon within the Neches River downstream of the floodplain when Neches River discharges exceeded 10, 000 cfs. Hurricane Rita passed through the Big Thicket National Preserve in September 2005. Dissolved organic carbon concentrations recorded after Hurricane Rita in the Neches River downstream of the floodplain were significantly higher than upstream of the floodplain. Dissolved organic carbon was twice as high after the hurricane than levels prior to the hurricane, with floodplain concentrations exceeding 50 ppm C. The increase in organic carbon was likely due to nutrients leached from leaves, which were swept from the floodplain trees prior to normal abscission in the fall. A continuum of leaf breakdown rates was observed in three common floodplain species of trees: Sapium sebiferum, Acer rubrum, and Quercus laurifolia. Leaves collected from blowdown as a result of Hurricane Rita did not break down significantly faster than leaves collected prior to abscission in the fall. <b>Processing</b> <b>coefficients</b> for leaf breakdown in a continuously wet area of the floodplain were significantly higher than <b>processing</b> <b>coefficients</b> for leaf breakdown on the floodplain floor. The forested floodplain of the Neches River is the main contributor of organic carbon. When flow is greater than 10, 000 csf, the floodplain transports organic carbon directly to the river, providing a source of nutrition for riverine organisms and contributing to the overall health of the ecosystem...|$|R
3000|$|... [...]), {{and used}} to form a block of symbols that is passed onto the Turbo encoder. The {{quantization}} step size for <b>processing</b> the transform <b>coefficients</b> is therefore [...]...|$|R
3000|$|... are {{the same}} as the ones in a SC-FDE system {{employing}} a MMSE-based decision-feedback equalizer together with widely linear <b>processing.</b> Thus, the <b>coefficients</b> of the Tomlinson-Harashima precoder [...]...|$|R
40|$|The {{table and}} {{algorithmic}} {{method of calculation}} of polynomials based on preliminary <b>coefficient</b> <b>processing</b> is offered. Possibility of acceleration of calculation of polynomials in comparison with realization of the well-known table and algorithmic methods is shown. ????????? ????????-??????????????? ????? ?????????? ???????????, ?????????? ?? ??????????????? ????????? ?????????????. ???????? ??????????? ????????? ?????????? ??????????? ?? ????????? ? ??????????? ????????? ????????-??????????????? ???????...|$|R
40|$|Abstract — Audio {{signals are}} often {{contaminated}} by background environment noise and buzzing or humming noise from audio equipments. Audio denoising aims at attenuating the noise while retaining the underlying signals. Removing noise from audio signals requires a nondiagonal <b>processing</b> of time-frequency <b>coefficients</b> to avoid producing “musical noise. ” A block thresholding estimation procedure is introduced, which adjusts all parameters adaptively to signal property by minimizing a Stein {{estimation of the}} risk. Non Diagonal time-frequency audio denoising algorithm attenuates the noise by <b>processing</b> each spectrogram <b>coefficient</b> independently. This Estimator is to minimize the error between clean signal and the enhanced signal. Numerical experiments demonstrate the performance and robustness of this procedure through objective and subjective evaluations...|$|R
30|$|From Property 1, {{the energy}} in {{high-frequency}} parts is completely mapped to radial component. Therefore, {{we just need}} to process the radial component in spherical coordinate system. This idea can avoid <b>processing</b> the wavelet <b>coefficients</b> directly. It reduces the image distortion in a certain extent.|$|R
40|$|Performed is {{strengthening}} of the surface layer of steel method of low-temperature carbonitriding in macrodispersed carbo - and nitrogen-bearing powder mixture. Chosen and justified the optimal technological parameters of process developed chemical-thermal <b>processing.</b> The diffusion <b>coefficient</b> of nitrogen alloyed steel 40 X for the proposed treatment...|$|R
40|$|Abstract—In this paper, {{we propose}} a new {{approach}} to query-by-humming, focusing on MP 3 songs database. Since MP 3 songs are much more difficult in melody representation than symbolic performance data, we adopt to extract feature descriptors from the vocal sounds part of the songs. Our approach is based on signal filtering, sub-band spectral <b>processing,</b> MDCT <b>coefficients</b> analysis and peak energy detection by ignorance of the background music as much as possible. Finally, we apply dual dynamic programming algorithm for feature similarity matching. Experiments will show us its online performance in precision and efficiency...|$|R
40|$|In this paper, we {{describe}} a simple gradient algorithm for adapting the phase response of an FIR filter whose magnitude response is already specified. The algorithm {{is an extension}} of a previously-developed gradient adaptive algorithm for allpass filtering. The algorithm directly adjusts the impulse response of the FIR filter using input and desired response signals in a simple manner without the need for frequency-domain <b>processing</b> or <b>coefficient</b> monitoring. A stationary point analysis of the algorithm verifies its desirable estimation capabilities. Simulations confirm the capability of the algorithm in a filter modeling task. 1...|$|R
5000|$|One of the derivations of the von Kries {{coefficient}} {{law is the}} von Kries transform, a {{chromatic adaptation}} method that is sometimes used in camera image <b>processing.</b> Using the <b>coefficient</b> law, cone responses [...] from two radiant spectra can be matched by appropriate choice of diagonal adaptation matrices D1 and D2: ...|$|R
40|$|The prime {{objective}} of this piece of work is to devise novel techniques for computer based classification of Electrocardiogram (ECG) arrhythmias {{with a focus on}} less computational time and better accuracy. As an initial stride in this direction, ECG beat classification is achieved by using feature extracting techniques to make a neural network (NN) system more effective. The feature extraction technique used is Wavelet Signal <b>Processing.</b> <b>Coefficients</b> from the discrete wavelet transform were used to represent the ECG diagnostic information and features were extracted using the coefficients and were normalised. These feature sets were then used in the classifier i. e. a simple feed forward back propagation neural network (FFBNN). This paper presents a detail study of the classification accuracy of ECG signal by using these four structures for computationally efficient early diagnosis. Neural network used in this study is a well-known neural network architecture named as multi-Layered perceptron (MLP) with back propagation training algorithm. The ECG signals have been taken from MIT-BIH ECG database, and are used in training to classify 3 different Arrhythmias out of ten arrhythmias. These are normal sinus rhythm, paced beat, left bundle branch block. Before testing, the proposed structures are trained by back propagation algorithm. The results show that the wavelet decomposition method is very effective and efficient for fast computation of ECG signal analysis in conjunction with the classifier...|$|R
40|$|Study {{of remote}} probing of inhomogeneous media {{making use of}} the known {{reflection}} coefficient of the medium {{as a function of}} the varied incident angle of the probing plane wave. An analytical method is developed for <b>processing</b> these reflection <b>coefficient</b> data. A critical evaluation of the numerical aspects of the method is presented...|$|R
40|$|The VIIRS (Visible-Infrared Imaging Radiometer Suite) {{instrument}} onboard the Suomi NPP (National Polar-orbiting Partnership) spacecraft started acquiring Earth {{observations in}} November 2011. Since then, radiometric calibration {{applied to the}} VIIRS RSB (Reflective Solar Band) measurements for the SDR (Sensor Data Record) production has been improved several times. Initially, radiometric calibration coefficients were updated once per week to correct for the responsivity degradation that occurs {{for some of the}} sensor’s spectral bands due to a pre-launch contamination of the VIIRS telescope’s mirrors. In August 2012, magnitude of the radiometric coefficient changes between the updates was greatly reduced by implementing a procedure that predicts (about a week ahead) values of the calibration coefficients for each Earth scan until a subsequent update. The updates have been continued with the weekly frequency, and coefficient prediction errors were monitored. The weekly predicted coefficients have been compared with the coefficients derived once per orbit from the onboard solar diffuser measurements by an automated procedure, called RSBAutoCal, scheduled for implementation in the VIIRS SDR operational processing in 2014. The presentation evaluates the changes in the VIIRS RSB coefficient updates and their potential impacts on application of the remote sensing measurements in the visible (Vis), near-infrared (NIR), and short-wave infrared (SWIR) bands throughout the two-year period of the Suomi NPP mission. Preliminary comparisons between the calibration coefficients generated by the RSBAutoCal and the current operational versions are also presented, as is the dependence of the radiometric calibration on <b>processing</b> <b>coefficients</b> such as attenuation screen transmittance and solar diffuser bidirectional reflectance...|$|R
40|$|In {{the paper}} {{telecommunication}} based informative educational environment (TIEE) macrovariables dynamics analysis method is presented. The method {{is based on}} generalized information model (GIM) that is first order autonomous dynamic system. The principles of the statistic data <b>processing</b> for GIM <b>coefficients</b> calculation are described. The method approbation was carrying out for academy and corporate TIEE. When you are citing the document, use the following link [URL]...|$|R
40|$|AbstractThe {{presented}} paper {{deals with}} the preparation inorganic materials and their use as filler in the polymeric rubber blend. The influence of used quantity of filler was evaluated from result of curing characteristics (minimum torque, maximum torque, optimum cure time, <b>processing</b> safety, rate <b>coefficient</b> of cure) and physical-mechanical properties (tensile strength, elongation, hardness, young's modulus). All these properties of polymeric rubber blends were compared with properties reference blend...|$|R
40|$|A {{combined}} coefficient segmentation {{and block}} processing algorithm for low power implementation of FIR digital filters {{is described in}} this paper. The algorithm processes data and coefficients in blocks of fixed sizes. During the manipulation of each block, coefficients are segmented into two primitive components. The accumulative effect of processing a sequence of blocks and segmentation results in up to 80 % reduction in power consumption in the multiplier circuit compared to conventional filtering. The paper describes {{the implementation of the}} algorithm, its constituent components, and the power evaluation environment developed. Simulations are performed using eight practical digital filter examples with various filter orders and data/coefficient wordlengths. In addition, the algorithm is compared with conventional filtering implementations and those using block <b>processing</b> and <b>coefficient</b> segmentation algorithms alone...|$|R
40|$|Abstract — Autoregressive (AR) {{modeling}} {{is widely}} used in signal <b>processing.</b> The <b>coefficients</b> of an AR model can be easily obtained with a least mean square (LMS) prediction error filter. However, {{it is known that}} this filter gives a biased solution when the input signal is corrupted by white Gaussian noise. Treichler suggested the -LMS algorithm to remedy this problem and proved that the mean weight vector can converge to the Wiener solution. In this paper, we develop a new algorithm that extends works of Vijayan et al. for adaptive AR modeling in the presence of white Gaussian noise. By theoretical analysis, we show that the performance of the new algorithm is superior to the -LMS filter. Simulations are also provided to support our theoretical results. I...|$|R
30|$|Multilevel Dual-Tree Complex Wavelet Transform (DT-WT) {{is also a}} {{comparable}} method but it requires the design of special filters with desirable properties: approximate half-sample delay property, perfect reconstruction (orthogonal or bi-orthogonal), finite support, vanishing moments (good stop band) and linear phase characteristics. Also since DT-WT involves complex <b>coefficients,</b> <b>processing</b> these (both real and imaginary) coefficients increases the computational complexity and the memory requirement, thereby increasing the cost of fusion method (Singh and Khare 2014).|$|R
40|$|Removing {{noise from}} audio signals {{requires}} a nondiagonal <b>processing</b> of time-frequency <b>coefficients</b> to avoid producing “musical noise. ” State {{of the art}} algorithms perform a parameterized filtering of spectrogram coefficients with empirically fixed parameters. A block thresholding estimation procedure is introduced, which adjusts all parameters adaptively to signal property by minimizing a Stein estimation of the risk. Numerical experiments demonstrate the performance and robustness of this procedure through objective and subjective evaluations...|$|R
40|$|This paper {{describes}} an approach for accomplishing sub- octave wavelet analysis and its discrete implementation for noise reduction and feature enhancement. Sub-octave wavelet transforms {{allow us to}} more closely characterize features within distinct frequency bands. By dividing each octave into sub-octave components, we demonstrate a superior ability to capture transient activities in a signal or image more reliably. De-noising and enhancement are accomplished through techniques of minimizing noise energy and nonlinear <b>processing</b> of transform <b>coefficient</b> energy by gain...|$|R
40|$|Contrast is an {{important}} factor affecting the image quality. In order to overcome the problems of local band-limited contrast, a novel image contrast assessment method based on the property of HVS is proposed. Firstly, the image by low-pass filter is performed fast wavelet decomposition. Secondly, all levels of band-pass filtered image and its corresponding low-pass filtered image are obtained by <b>processing</b> wavelet <b>coefficients.</b> Thirdly, local band-limited contrast is calculated, and the local band-limited contrast entropy is calculated according to the definition of entropy, Finally, the contrast entropy of image is obtained by averaging the local band-limited contrast entropy weighed using CSF coefficient. The experiment results show that the best contrast image can be accurately identified in the sequence images obtained by adjusting the exposure time and stretching gray respectively, the assessment results accord with human visual characteristics and make up the lack of local band-limited contrast. </p...|$|R
40|$|Abstract—Image coding {{systems have}} been {{traditionally}} tai-lored for Multiple Instruction, Multiple Data (MIMD) computing. In general, they partition the (transformed) image in codeblocks that can be coded in the cores of MIMD-based processors. Each core executes a sequential flow of instructions to process the coefficients in the codeblock, independently and asynchronously from the others cores. Bitplane coding is a common strategy to code such data. Most of its mechanisms require sequential <b>processing</b> of the <b>coefficients.</b> The last years have seen the up-raising of processing accelerators with enhanced computational performance and power efficiency whose architecture is mainly based on the Single Instruction, Multiple Data (SIMD) principle. SIMD computing refers to {{the execution of the}} same instruction to multiple data in a lockstep synchronous way. Unfortunately, current bitplane coding strategies can not fully profit from such processors due to inherently sequential coding task. This paper presents bitplane image coding with parallel <b>coefficient</b> <b>processing</b> (BPC-PaCo), a coding method that can process many coefficients within a codeblock in parallel and synchronously. To this end, the scanning order, the context formation, the probability model, and the arithmetic coder of the coding engine have been re-formulated. Experimental results suggest that the penalization in coding performance of BPC-PaCo with respect to traditional strategies is almost negligible. Index Terms—Bitplane image coding, Single Instruction Mul-tiple Data (SIMD), JPEG 2000...|$|R
40|$|AbstractThe paper {{presents}} {{new method}} for production of closed-cell Al foams of improved sound absorbing ability with no machining operations. Opening the closed-cell structure of solid foamed Al {{is provided by}} micrometer sized cracks lengthways the eutectic domains created by finish heat treatment that includes heating below the solidus temperature followed by water quenching. The presence of through cracks provides for interconnection of the cells, making them like Helmholtz micro-perforated resonators and ensuring the increase by 15 % of sound absorption <b>coefficient.</b> <b>Processing</b> parameters of foaming and followed heat treatment required for transformation of closed cells into micro-perforated resonators are discussed and specified...|$|R
50|$|At the receiver, the {{infrared}} signal strengths are measured by {{some form of}} infrared detector. Generally photodiode detectors are preferred, and are essential for the higher modulation frequencies, whereas slower photoconductive detectors may be required for longer wavelength regions. The signals are fed to low-noise amplifiers, then invariably subject {{to some form of}} digital signal <b>processing.</b> The absorption <b>coefficient</b> of the gas will vary across the passband, so the simple Beer-Lambert law cannot be applied directly. For this reason the processing usually employs a calibration table, applicable for a particular gas, type of gas, or gas mixture, and sometimes configurable by the user.|$|R
40|$|Network science {{describes}} how entities in complex systems interact, {{and argues that}} {{the structure of the}} network influences <b>processing.</b> Clustering <b>coefficient,</b> C – one measure of network structure – refers {{to the extent to which}} neighbors of a node are also neighbors of each other. Previous simulations suggest that networks with low C dissipate information (or disease) to a large portion of the network, whereas in networks with high C information (or disease) tends to be constrained to a smaller portion of the network (Newman, 2003). In the present simulation we examined how C influenced the spread of activation to a specific node, simulating retrieval of a specific lexical item in a phonological network. The results of the network simulation showed that words with lower C had higher activation values (indicating faster or more accurate retrieval from the lexicon) than words with higher C. These results suggest that a simple mechanism for lexical retrieval can account for the observations made in Chan and Vitevitch (2009), and have implications for diffusion dynamics in other fields...|$|R
40|$|Abstract: The {{successful}} enterprises deliver {{products and}} services in shorter throughput time and turnover inventory as quickly as possible. Three important approaches to achieve these goals are Materials Requirements Planning (MRPI, MRPII), Just-in-Time (JIT), and Theory of Constraints (TOC). TOC {{seems to be a}} viable proposition because it does not require costly affair of system change; rather it is simply based on scheduling of the capacity constraint resources. The scheduling system of theory of constraints (TOC) is often referred as drum-buffer-rope (DBR) system. DBR systems operate by developing a schedule for the system’s primary resource constraint. Based on simulation of a flow shop operation considering non-free goods, this study suggests that the performance of DBR results in lower in process inventory compared to conventional production system. In addition, the behavior of the system when bottleneck shifts its position has been studied. A methodology has been proposed using neural networks based on back propagation algorithm for accurate prediction of throughput considering mean <b>processing</b> time, <b>coefficient</b> of variation, buffer size, number of stations and signal buffer...|$|R
40|$|Conference PaperWavelet-domain hidden Markov models (HMMs) {{provide a}} {{powerful}} new approach for statistical modeling and <b>processing</b> of wavelet <b>coefficients.</b> In addition to characterizing the statistics of individual wavelet coefficients, HMMs capture {{some of the key}} interactions between wavelet coefficients. However, as HMMs model an increasing number of wavelet coefficient interactions, HMM-based signal processing becomes increasingly complicated. In this paper, we propose a new approach to HMMs based on the notion of context. By modeling wavelet coefficient inter-dependencies via contexts, we retain the approximation capabilities of HMMs, yet substantially reduce their complexity. To illustrate the power of this approach, we develop new algorithms for signal estimation and for efficient synthesis of nonGaussian, long-range-dependent network traffic...|$|R
40|$|The {{reason of}} zero point errors of IR gas sensors {{are often the}} {{asymmetric}} properties of the used multi-channel pyroelectric detectors. A symmetrisation of the channels by means of technological measures is very expensive. It is possible to symmetrise these properties in a limited temperature range by a linear transformation, which can {{be included in the}} sensor-specific signal <b>processing.</b> The mean <b>coefficient</b> for this transformation, which determines the temperature dependence of the zero point, can be estimated by the manufacturer of the pyroelectric detectors, independently of the optical setup of the sensor and should be specified in the data sheet. This way it is possible to reduce the expense for calibration of the sensors considerably...|$|R
40|$|Insulation {{deterioration}} is {{an increasing}} problem with the age of high voltage apparatus and current deregulation circumstance. Partial discharge (PD) measurement {{has been used for}} insulation assessment for many years. It has been arguably {{recognized as one of the}} most effective diagnosis tool for insulation condition monitoring. However, on site partial discharge measurement is often affected by excessive electromagnetic interference (EMI). This makes it very difficult to interpret data for insulation assessment purpose. The differential circuit was adopted by power supply industry to reject common mode interference a long time ago. However for on site partial discharge measurement, the differential circuit is still very hard to implement for reliable operation. In practice, it would be difficult to find two sensors with matching electrical characteristics so that they can be used to cancel the common mode interference. To solve this problem, a software-based approach for implementing the differential detection has been developed by authors. Interference and true partial discharge signal could be distinguished by analyzing their polarities and the correlation coefficients. The viability of this new approach will be verified by laboratory experiments. Key words: partial discharge, condition monitoring, differential circuit, digital signal <b>processing,</b> correlation <b>coefficients,</b> polarity. 1...|$|R
40|$|Image <b>processing</b> based wavelet <b>coefficients</b> prior {{statistical}} models plays one of great improtant roles in modern image processing techniques. Owing to the defaults of fitting of Gaussian or Laplace functions, a Bayesian model of neural network(BMNN) {{to study the}} statistical dependency of wavelet coefficients is firstly presented. Secondly, its parameters are estimated by modern particle samplers (Monte Carlo) methods [...] Gibbs algorithm according to {{the characteristics of the}} suggested BMNN model. Then the relationship of wavelet coefficients is discussed in detail. Finally, a practical application of denoising image by using the BMNN model is demonstrated and the result shows that, on one hand the suggested method can express wavelet coefficients dependency efficiently, on the other, high quality virual effects and peak signal-to-noise ratio (PSNR) are achieved...|$|R
40|$|This paper {{presents}} {{a novel approach}} for subband feedback Active Noise Control (ANC). Wideband ANC systems often involve adaptive filters with hundreds of taps. Using subband processing can considerably reduce {{the length of the}} adaptive filter. Conventional subband algorithms are generally based in the frequency domain and use at least 2 sensors. This paper {{presents a}} time domain algorithm for single sensor subband feedback ANC using relatively short fixed FIR filters to do the subband <b>processing.</b> The adaptive <b>coefficients</b> in the system are updated using a weight constrained NLMS algorithm for feedback ANC. The proposed subband algorithm had a significant performance advantage over the traditional single band ANC algorithm in terms of the rate of convergence and the noise attenuation that could be obtaine...|$|R
40|$|The AA 6061 -T 6 {{aluminum}} alloy samples including annealed Fe 78 Si 9 B 13 particles were prepared by friction stir processing (FSP) and investigated by various techniques. The Fe 78 Si 9 B 13 -reinforced particles are uniformly dispersed in the {{aluminum alloy}} matrix. The XRD {{results indicated that}} the lattice parameter of α-Al increases and the preferred orientation factors F of (200) plane of α-Al reduces after friction stir <b>processing.</b> The <b>coefficient</b> of thermal expansion (CTE) for FSP samples increases at first with the temperature but then decreases as the temperature further increased, which {{can be explained by the}} dissolving of Mg and Si from β phase and Fe 78 Si 9 B 13 particles. The corrosion and wear resistance of FSP samples have been improved compared with that of base metal, which can be attributed to the reduction of grain size and the CTE mismatch between the base metal and reinforced particles by FSP, and the lubrication effect of Fe 78 Si 9 B 13 particles also plays a role in improving wear resistance. In particular, the FSP sample with reinforced particles in amorphous state exhibited superior corrosion and wear resistance due to the unique metastable structure...|$|R
