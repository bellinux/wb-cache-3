0|10000|Public
3000|$|Inducing {{the initial}} {{detection}} model {{from the initial}} available <b>training</b> <b>set,</b> i.e., <b>training</b> <b>set</b> available up to day d (the initial <b>training</b> <b>set</b> includes 574 PDF files) [...]...|$|R
3000|$|Several {{candidate}} <b>training</b> <b>sets</b> were processed {{to produce}} a mixture pdf among which were the multichannel <b>training</b> <b>set</b> of [2] (1 minute of an orchestra recording), a white noise <b>training</b> <b>set,</b> a Brownian noise <b>training</b> <b>set,</b> and a pink noise <b>training</b> <b>set.</b> Pink noise {{proved to be the}} most suitable <b>training</b> <b>set</b> and led to smaller cepstral reconstruction errors (up to 5 % less in all subbands compared to the other sets) during enhancement of the 4 generic music pieces. The power spectrum of pink noise is proportional to [...]...|$|R
40|$|Various {{computational}} {{models have}} been developed to transfer annotations of gene essentiality between organisms. However, despite the increasing number of microorganisms with well-characterized sets of essential genes, selection of appropriate <b>training</b> <b>sets</b> for predicting the essential genes of poorly-studied or newly sequenced organisms remains challenging. In this study, a machine learning approach was applied reciprocally to predict the essential genes in 21 microorganisms. Results showed that <b>training</b> <b>set</b> selection greatly influenced predictive accuracy. We determined four criteria for <b>training</b> <b>set</b> selection: (1) essential genes in the selected <b>training</b> <b>set</b> should be reliable; (2) the growth conditions in which essential genes are defined should be consistent in training and prediction sets; (3) species used as <b>training</b> <b>set</b> should be closely related to the target organism; and (4) organisms used as <b>training</b> and prediction <b>sets</b> should exhibit similar phenotypes or lifestyles. We then analyzed the performance of an incomplete <b>training</b> <b>set</b> and an integrated <b>training</b> <b>set</b> with multiple organisms. We found that the size of the <b>training</b> <b>set</b> should be at least 10 % of the total genes to yield accurate predictions. Additionally, the integrated <b>training</b> <b>sets</b> exhibited remarkable increase in stability and accuracy compared with single sets. Finally, we compared the performance of the integrated <b>training</b> <b>sets</b> with the four criteria and with random selection. The results revealed that a rational selection of <b>training</b> <b>sets</b> based on our criteria yields better performance than random selection. Thus, our results provide empirical guidance on <b>training</b> <b>set</b> selection for th...|$|R
30|$|In {{the third}} {{experiment}} {{we wanted to}} determine the importance of updating the <b>training</b> <b>set</b> over time. Thus, we divided the test collection into years and evaluated <b>training</b> <b>sets</b> of selected years on the next years. Evaluation results show that an update in the <b>training</b> <b>set</b> is needed. Using 10 % malicious files in the <b>training</b> <b>set</b> showed a clear trend in which the performance improves when the <b>training</b> <b>set</b> is updated on a yearly basis.|$|R
30|$|The GVCA method {{based on}} {{horizontally}} grouped <b>training</b> <b>set</b> is {{as shown in}} Algorithm  2. Meanwhile, regarding the problem of too high eigenvector dimension of <b>training</b> <b>set,</b> a feasible approach is to group the features, i.e., to randomly group a feature set into several non-intersecting subsets, and project the originally integral <b>training</b> <b>set</b> into those feature sets, thereby composing several new grouped <b>training</b> <b>sets.</b> After that, the original VCA method is used to obtain vanishing component polynomial respectively, and finally, the vanishing component polynomial in multiple grouped <b>training</b> <b>sets</b> is combined into the vanishing component polynomial in an integral <b>training</b> <b>set.</b> This approach is called the vertical grouping method. The GVCA method based on vertically grouping <b>training</b> <b>sets</b> is as shown in Algorithm  3. Furthermore, to handle both problems of over-scaled <b>training</b> <b>set</b> and too high eigenvector dimension, the horizontal and vertical grouping approaches can be applied simultaneously.|$|R
40|$|Compression of <b>training</b> <b>sets</b> is a {{technique}} for reducing <b>training</b> <b>set</b> size without degrading classification accuracy. By reducing {{the size of a}} <b>training</b> <b>set,</b> <b>training</b> will be more efficient in addition to saving storage space. In this paper, an incremental clustering algorithm, the Leader algorithm, is used {{to reduce the size of}} a <b>training</b> <b>set</b> by effectively subsampling the <b>training</b> <b>set.</b> Experiments on several standard data sets using SVM and KNN as classifiers indicate that the proposed method is more efficient than CONDENSE in reducing the size of <b>training</b> <b>set</b> without degrading the classification accuracy. While the compression ratio for the CONDENSE method is fixed, the proposed method offers variable compression ratio through the cluster threshold value...|$|R
40|$|In {{this paper}} {{we present a}} {{technique}} for reverse engineering vector quantizers by synthesizing a <b>training</b> <b>set</b> that has sim-ilar statistics to the original <b>training</b> <b>set</b> used in designing the vector quantizer. Most VQ codebooks are designed using the LBG or generalized Lloyd algorithm {{which is similar to}} the construction of nonuniform bin histograms. Thus the VQ codebook and the number of <b>training</b> <b>set</b> vectors allocated to each of its codebook vectors is an approximation of the underlying pdf of the <b>training</b> <b>set.</b> This observation is used to synthesize a <b>training</b> <b>set</b> that has a histogram similar to the original <b>training</b> <b>set.</b> This synthesized <b>training</b> <b>set</b> can be used to construct VQs to describe subspaces of the original vector space or spaces transformed by a linear transforma-tion. 1...|$|R
50|$|Over the years, three {{versions}} of rolling stock were used on this line, {{as well as}} on the through services between Nanshijiao and Beitou. Originally, the line used a large fleet of C301 <b>train</b> <b>sets.</b> In 1999, only a few C341 <b>train</b> <b>sets</b> were used. In 2007, some C371 <b>train</b> <b>sets</b> were introduced. Today, the entire fleet used on this line is the C371 <b>train</b> <b>sets</b> after the original C301 <b>train</b> <b>sets</b> were confined to the Tamsui and Xindian Lines upon the opening of Dongmen Station on September 30, 2012.|$|R
40|$|Introduction In recent {{research}} in combining predictors, {{it has been}} recognized that the critical thing to success in combining low-bias predictors such as trees and neural nets has been through methods that reduce the variability in the predictor due to <b>training</b> <b>set</b> variability. Assume that the <b>training</b> <b>set</b> consists of N independent draws from the same underlying distribution. Conceptually, <b>training</b> <b>sets</b> of size N can be drawn repeatedly and the same algorithm used to construct a predictor on each <b>training</b> <b>set.</b> These predictors will vary, {{and the extent of}} the variability is a dominant factor in the generalization prediction error. 2 Given a <b>training</b> <b>set</b> {(y n,x n),n= 1, [...] . N} where the y's are either class labels or numerical values, the most common way of reducing variability is by perturbing the <b>training</b> <b>set</b> to produce alternative <b>training</b> <b>sets,</b> growing a predictor o...|$|R
25|$|This shows Transrapid <b>train</b> <b>sets</b> {{are likely}} to cost over {{twice as much as}} ICE 3 {{conventional}} fast rail <b>train</b> <b>sets</b> at this time. However each Transrapid <b>train</b> <b>set</b> is more than twice as efficient due to their faster operating speed and acceleration according to UK Ultraspeed. In their case study only 44% as many Transrapid <b>train</b> <b>sets</b> are needed to deliver the same amount of passengers as conventional high-speed trains.|$|R
40|$|Abstract—Bug triage is an {{important}} step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the <b>training</b> <b>sets</b> of bug reports. In practice, these approaches suffer from the large-scale and low-quality <b>training</b> <b>sets.</b> In this paper, we propose the <b>training</b> <b>set</b> reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm 2 -testχ, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the <b>training</b> <b>set</b> reduction on the bug data of Eclipse. For the <b>training</b> <b>set,</b> 70 % words and 50 % bug reports are removed after the <b>training</b> <b>set</b> reduction. The experimental results show that the new and small <b>training</b> <b>sets</b> can provide better accuracy than the original one. Keywords-bug triage; <b>training</b> <b>set</b> reduction; feature selection; instance selection; software quality I...|$|R
40|$|The {{passenger}} <b>train</b> <b>sets</b> is {{the carrier}} of railway passenger transport production and reasonable {{use of the}} passenger <b>train</b> <b>sets</b> {{is one of the}} key goals of railway transportation plan. In order to improve the operation efficiency of passenger <b>train</b> <b>sets,</b> optimization model of railway passenger <b>train</b> <b>sets</b> assignment have been built to minimize the non-production staying time of passenger <b>train</b> <b>sets</b> at passenger station based on established train diagram and established configuration of <b>train</b> <b>sets</b> system. On that basis, considering that the parameters of simulated Annealing Algorithm (SA) directly affect the efficiency and precision of solving, SA parameters is optimized based on nested partitions, then improved simulated annealing algorithm (ISA) is designed to solve this model. Finally, a case study has been carried out taking Zhengzhou railway station of china as an example in order to testify validity of this model and its algorithm by using calculating and comparing analysis and further solved practical problems is analyzed. The results show that the number of passenger <b>train</b> <b>sets</b> required and carriage staying time at passenger station are reduced and this model and its algorithm can be used to optimize railway passenger <b>train</b> <b>sets</b> assignment to improve efficiency of railway passenger <b>train</b> <b>sets</b> assignment...|$|R
5000|$|... 13 October 2006: Prasarana signs an {{agreement}} with a Bombardier-Hartasuma joint venture {{for the purchase of}} 22 four-car <b>train</b> <b>sets</b> for the Kelana Jaya Line with an option to purchase an additional 13 <b>train</b> <b>sets</b> for RM1.2 billion. First 22 <b>train</b> <b>sets</b> to be delivered in 2008.|$|R
5000|$|Given a <b>training</b> <b>set</b> S of size m, we will build, {{for all i}} = 1....,m, {{modified}} <b>training</b> <b>sets</b> as follows: ...|$|R
40|$|We {{consider}} empirical risk minimization {{for large-scale}} datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton {{is to increase}} the size of the <b>training</b> <b>set</b> by a factor larger than one {{in a way that the}} minimization variable for the current <b>training</b> <b>set</b> is in the local neighborhood of the optimal argument of the next <b>training</b> <b>set.</b> This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each <b>training</b> <b>set</b> with only one iteration of Newton's method. We show theoretically and empirically that Ada Newton can double the size of the <b>training</b> <b>set</b> in each iteration to achieve the statistical accuracy of the full <b>training</b> <b>set</b> with about two passes over the dataset...|$|R
40|$|With {{the rapid}} {{development}} of high-speed railway in China, {{the problem of}} motor <b>train</b> <b>set</b> assignment and maintenance scheduling {{is becoming more and}} more important for transportation organization. This paper focuses on considering the special maintenance items of motor <b>train</b> <b>set</b> and mainly meets the two maintenance cycle limits on aspects of mileage and time for each item. And then, a 0 - 1 integer programming model for motor <b>train</b> <b>set</b> assignment and maintenance scheduling is proposed, which aims at maximizing the accumulated mileage before each maintenance and minimizing the number of motor <b>train</b> <b>sets.</b> Restrictions of the model include the matching relation between motor <b>train</b> <b>sets</b> and routes as well as that between motor <b>train</b> <b>sets</b> and maintenance items and maintenance capacity of motor <b>train</b> <b>set</b> depot. A heuristic solution strategy based on particle swarm optimization is also proposed to solve the model. In the end, a case study is designed based on the background of Beijing south depot in China, and the result indicates that the model and algorithm proposed in this paper could solve the problem of motor <b>train</b> <b>set</b> assignment and maintenance scheduling effectively...|$|R
40|$|This paper {{exploits}} {{the criteria}} {{to optimize the}} <b>training</b> <b>set</b> construction for video annotation. Most existing learningbased semantic annotation approaches require a large <b>training</b> <b>set</b> to achieve good generalization capacity, in which {{a considerable amount of}} labor-intensively manual labeling is desirable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution rather than the size of the training data. We argue that a <b>training</b> <b>set</b> which includes most temporal and spatial distribution of the whole data will achieve a satisfying performance even in the case of limited size of <b>training</b> <b>set.</b> In order to capture the geometrical distribution characteristics of a given video collection, we propose the following four metrics for constructing an optimal <b>training</b> <b>set,</b> including Salience, Time Dispersiveness, Spatial Dispersiveness and Diversity. Moreover, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data for a <b>training</b> <b>set</b> with a given size. Experimental results demonstrate that these rules are effective for <b>training</b> <b>set</b> construction for video annotation, and significantly outperform random <b>training</b> <b>set</b> selection as well...|$|R
30|$|We {{focus only}} {{on the part of}} the <b>training</b> <b>set,</b> which {{describes}} inputs. The given <b>training</b> <b>set,</b> based on the robot configuration and the distribution of the scanned area into five parts, comprises 1, 048, 576 patterns. The <b>training</b> <b>set</b> also contains patterns that admit a possibility of the existence of obstacles in several given intervals. Therefore, it is necessary to specify a group of inputs I_i belonging to a given sensor. In the case that the group comprises more than one value of,, 1 “, such a pattern is not included in the <b>training</b> <b>set.</b> The reduced <b>training</b> <b>set</b> (RTS) comprises 1296 patterns, see Table  1.|$|R
30|$|Grouping the {{training}} set: this paper proposed the GVCA method to address two situations of over-scaled <b>training</b> <b>set</b> and too high dimension of eigenvector. Regarding {{the problem of}} over-scaled <b>training</b> <b>set,</b> it is a practicable way to group the examples, i.e., to horizontally segment the entire <b>training</b> <b>set</b> into several <b>training</b> subsets (for instance, take 10 / 20 / 30 / 40 / 50 examples as a group), then acquire the features (i.e., vanishing component polynomial) by the original VCA method respectively and combine the vanishing component polynomial in multiple grouped <b>training</b> <b>sets</b> into the vanishing component polynomial in an integral <b>training</b> <b>set.</b> This approach is called the horizontal grouping method.|$|R
50|$|Montenegro Railways have {{received}} six class 412/416 EMU <b>train</b> <b>sets</b> {{which have been}} purchased by Yugoslav Railways for ŽTP Titograd in 1985. In 2015, there are five operational <b>train</b> <b>sets,</b> with two recently overhauled. Main depot is at Podgorica. One <b>train</b> <b>set</b> was destroyed in the Bioče disaster in 2006.|$|R
50|$|<b>Train</b> <b>set</b> 013/014 {{had been}} {{previously}} split and modified to serve as 3-car formations on the Xinbeitou Branch Line. When new 3-car formation C371 cars were made, <b>train</b> <b>set</b> 013/014 was subsequently re-joined and put back into normal service on Line 2. <b>Train</b> <b>set</b> 013/014 is also equipped with IGBT-VVVF inverters.|$|R
30|$|We {{exploit the}} {{criteria}} to optimize <b>training</b> <b>set</b> construction for the large-scale video semantic classification. Due {{to the large}} gap between low-level features and higher-level semantics, {{as well as the}} high diversity of video data, it is difficult to represent the prototypes of semantic concepts by a <b>training</b> <b>set</b> of limited size. In video semantic classification, most of the learning-based approaches require a large <b>training</b> <b>set</b> to achieve good generalization capacity, in which large amounts of labor-intensive manual labeling are ineluctable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution of the training data rather than the size. We argue that a <b>training</b> <b>set</b> which includes most temporal and spatial distribution information of the whole data will achieve a good performance even if the size of <b>training</b> <b>set</b> is limited. In order to capture the geometrical distribution characteristics of a given video collection, we propose four metrics for constructing/selecting an optimal <b>training</b> <b>set,</b> including salience, temporal dispersiveness, spatial dispersiveness, and diversity. Furthermore, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data using a <b>training</b> <b>set</b> with a given size. Experimental results demonstrate these rules are effective for <b>training</b> <b>set</b> construction in video semantic classification, and significantly outperform random <b>training</b> <b>set</b> selection.|$|R
30|$|In <b>training</b> <b>sets</b> for Holiday and Oxford 5 K, we use Flickr 60 k [19] and Paris {{datasets}} [16] as vocabulary <b>training</b> <b>sets,</b> respectively.|$|R
5000|$|... #Caption: The {{original}} TOGO-built {{train of}} The Roller Coaster; this <b>train</b> <b>set</b> {{has been replaced}} with a new <b>train</b> <b>set</b> built by Premier Rides ...|$|R
40|$|Many {{real-world}} {{machine learning}} tasks {{are faced with}} the problem of small <b>training</b> <b>sets.</b> Additionally, the class distribution of the <b>training</b> <b>set</b> often does not match the target distribution. In this paper we compare the performance of many learning models on a substantial benchmark of binary text classification tasks having small <b>training</b> <b>sets...</b>|$|R
5000|$|Different {{aspects of}} {{validation}} of QSAR models that need attention includes methods of selection of <b>training</b> <b>set</b> compounds, <b>setting</b> <b>training</b> <b>set</b> size {{and impact of}} variable selection [...] for <b>training</b> <b>set</b> models for determining the quality of prediction. Development of novel validation parameters for judging quality of QSAR models is also important.|$|R
30|$|The <b>training</b> <b>set</b> (<b>Train)</b> has two {{different}} versions: clean and multi-condition. The clean <b>training</b> <b>set</b> {{consists of the}} clean <b>training</b> data <b>set</b> of WSJCAM 0. The multi-condition <b>training</b> <b>set</b> consists of simulated reverberant speech with additional background noise at an SNR of 20 dB. A script with which to generate the multi-condition training data from the clean training data was available to the challenge participants {{as well as several}} room impulse responses and noise signals measured in real rooms [25]. We refer to that multi-condition <b>training</b> data <b>set</b> as the baseline multi-condition <b>training</b> data <b>set.</b> Note that the challenge regulations also allowed the use of additional training data.|$|R
40|$|Bug triage is an {{important}} step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the <b>training</b> <b>sets</b> of bug reports. In practice, these approaches suffer from the large-scale and low-quality <b>training</b> <b>sets.</b> In this paper, we propose the <b>training</b> <b>set</b> reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the <b>training</b> <b>set</b> reduction on the bug data of Eclipse. For the <b>training</b> <b>set,</b> 70 % words and 50 % bug reports are removed after the <b>training</b> <b>set</b> reduction. The experimental results show that the new and small <b>training</b> <b>sets</b> can provide better accuracy than the original one. Comment: 6 pages, 3 figures, Proceedings of 35 th Annual IEEE International Computer Software and Applications Conference(COMPSAC), 201...|$|R
3000|$|... cannot cover {{samples that}} lie outside the {{distribution}} of the <b>training</b> <b>set.</b> Since the <b>training</b> <b>set</b> is actually a subset of the patterns included in the dataset [...]...|$|R
3000|$|... is {{a support}} vector {{obtained}} from a <b>training</b> <b>set</b> by an optimization method. The data points from the <b>training</b> <b>set</b> lying on the boundaries are the support vectors.|$|R
30|$|In every {{experiment}} the <b>training</b> <b>set</b> {{starts with}} two pre-labeled instances. At each iteration a new instance is queried for its label {{and added to}} the <b>training</b> <b>set.</b>|$|R
5|$|The three {{individual}} {{cars that}} make up a <b>train</b> <b>set</b> are distinguished by the second digit. For example, <b>train</b> <b>set</b> 3001 consists of the cars 3101, 3201, and 3301.|$|R
40|$|Optimization of railway {{passenger}} <b>train</b> <b>set</b> {{assignment is}} a complicated system engineering that is influenced by multitudinous factors. To improve the use efficiency of <b>train</b> <b>set,</b> the impact of train delay on <b>train</b> <b>set</b> assignment was analyzed, and the optimization model of railway passenger <b>train</b> <b>set</b> assignment have been built according to train delay propagation, and on that basis, optimization algorithm have also been put forward based on simulated annealing algorithm. Finally, a case study {{has been carried out}} taking four passenger stations in railway network as an example in order to testify validity, objectivity and applicability of this model by using calculating and comparing analysis. The results show that this model could improve the efficiency of railway passenger <b>train</b> <b>set</b> assignment...|$|R
40|$|Supervised pattern {{discovery}} {{techniques have}} been successfully used for motif detection. However, this {{requires the use of}} an efficient <b>training</b> <b>set.</b> Even in cases where a lot of examples are known, using all the available examples can introduce bias during the training process. In practice, this is done with the help of domain experts. Whenever such expertise is not available, <b>training</b> <b>sets</b> are usually picked at random. We present a new strategy for designing good <b>training</b> <b>sets</b> that uses phylogenetic trees to automatically reduce the bias in <b>training</b> <b>sets.</b> When applied to helix-turn-helix motif detection, we show that the technique improved the error rates over a random choice of <b>training</b> <b>set.</b> More specifically, false positive rates show a marked improvement. 1...|$|R
50|$|After {{learning}} a function {{based on the}} <b>training</b> <b>set</b> data, that function is validated on a test set of data, data that did {{not appear in the}} <b>training</b> <b>set.</b>|$|R
50|$|The three {{individual}} {{cars that}} make up a <b>train</b> <b>set</b> are distinguished by the second digit. For example, <b>train</b> <b>set</b> 3001 consists of the cars 3101, 3201, and 3301.|$|R
