3759|2605|Public
5|$|Fine-mapping {{requires}} all variants in {{the associated}} region {{to have been}} genotyped or imputed (dense coverage), very stringent quality control resulting in high-quality genotypes, and large sample sizes sufficient in separating out highly correlated signals. There are several different methods to perform fine-mapping, and all methods produce a <b>posterior</b> <b>probability</b> that a variant in that locus is causal. Because the requirements are often difficult to satisfy, there are still limited examples of these methods being more generally applied.|$|E
25|$|P(A3)= 1/2 by {{the smaller}} <b>posterior</b> <b>probability</b> P(A3|B)= 5/24.|$|E
25|$|As of 2009, the {{phylogeny}} of Malpighiales is, at its deepest level, an unresolved polytomy of 16 clades. It {{has been}} estimated that complete resolution of the phylogeny will require at least 25000 base pairs of DNA sequence data per taxon. A similar situation exists with Lamiales and it has been analyzed in some detail. The phylogenetic tree shown below is from Wurdack and Davis (2009). The statistical support for each branch is 100% bootstrap percentage and 100% <b>posterior</b> <b>probability,</b> except where labeled, with bootstrap percentage followed by <b>posterior</b> <b>probability.</b>|$|E
5000|$|Boostrap values vs <b>Posterior</b> <b>Probabilities.</b> It {{has been}} {{observed}} that bootstrap support values, calculated under parsimony or maximum likelihood, tend to be lower than the <b>posterior</b> <b>probabilities</b> obtained by Bayesian inference. This fact leads {{to a number of}} questions such as: Do <b>posterior</b> <b>probabilities</b> lead to overconfidence in the results? Are bootstrap values more robust than <b>posterior</b> <b>probabilities?</b> ...|$|R
40|$|Word <b>posterior</b> <b>probabilities</b> are {{a common}} {{approach}} for confidence estimation in automatic speech recognition and machine translation. We will generalize this idea and introduce n-gram <b>posterior</b> <b>probabilities</b> and show how these {{can be used to}} improve translation quality. Additionally, we will introduce a sentence length model based on <b>posterior</b> <b>probabilities.</b> We will sho...|$|R
40|$|Voice {{conversion}} (VC) using sequence-to-sequence {{learning of}} context <b>posterior</b> <b>probabilities</b> is proposed. Conventional VC using shared context <b>posterior</b> <b>probabilities</b> predicts target speech parameters from the context <b>posterior</b> <b>probabilities</b> estimated {{from the source}} speech parameters. Although conventional VC can be built from non-parallel data, {{it is difficult to}} convert speaker individuality such as phonetic property and speaking rate contained in the <b>posterior</b> <b>probabilities</b> because the source <b>posterior</b> <b>probabilities</b> are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target <b>posterior</b> <b>probabilities.</b> The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates <b>posterior</b> <b>probabilities</b> and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC. Comment: Accepted to INTERSPEECH 201...|$|R
25|$|As, in {{the general}} case, the theory linking data with model {{parameters}} is nonlinear, the <b>posterior</b> <b>probability</b> in the model space may {{not be easy to}} describe (it may be multimodal, some moments may not be defined, etc.).|$|E
25|$|To sum up this formula: the <b>posterior</b> <b>probability</b> of the {{hypothesis}} {{is equal to}} the prior probability of {{the hypothesis}} multiplied by the conditional probability of the evidence given the hypothesis, divided by the probability of the new evidence.|$|E
25|$|While it is {{possible}} to define an ad hoc cost function, frequently a particular cost function is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the <b>posterior</b> <b>probability</b> of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.|$|E
40|$|In this paper, we {{introduce}} REMAP, {{an approach}} {{for the training}} and estimation of <b>posterior</b> <b>probabilities</b> using a recursive algorithm that {{is reminiscent of the}} EM-based Forward-Backward (Liporace 1982) algorithm for the estimation of sequence likelihoods. Although very general, the method is developed {{in the context of a}} statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional <b>posterior</b> <b>probabilities</b> of transitions to estimate global <b>posterior</b> <b>probabilities</b> of word sequences. Although we still use ANNs to estimate <b>posterior</b> <b>probabilities,</b> the network is trained with targets that are themselves estimates of local <b>posterior</b> <b>probabilities.</b> An initial experimental result shows a significant decrease in error-rate in comparison to a baseline system. 1 INTRODUCTION The ultimate goal in speech recognition is to determine the sequence of words that [...] ...|$|R
40|$|In this paper, we briefly {{describe}} REMAP, {{an approach}} {{for the training}} and estimation of <b>posterior</b> <b>probabilities,</b> and report its application to speech recognition. REMAP is a recursive algorithm that {{is reminiscent of the}} Expectation Maximization (EM) [5] algorithm for the estimation of data likelihoods. Although very general, the method is developed {{in the context of a}} statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional <b>posterior</b> <b>probabilities</b> of transitions to estimate global <b>posterior</b> <b>probabilities</b> of word sequences. As with earlier hybrid HMM/ANN systems we have developed, ANNs are used to estimate <b>posterior</b> <b>probabilities.</b> In the new approach, however, the network is trained with targets that are themselves estimates of local <b>posterior</b> <b>probabilities.</b> Initial experimental results support the theory by showing an increase in the estima [...] ...|$|R
40|$|Abstract: This paper {{presents}} scene classification methods using spatial {{relationship between}} local <b>posterior</b> <b>probabilities</b> of each category. Recently, the authors proposed the probability higher-order local autocorrelations (PHLAC) feature. This method uses autocorrelations of local <b>posterior</b> <b>probabilities</b> to capture spatial distributions of local <b>posterior</b> <b>probabilities</b> of a category. Although PHLAC achieves good recognition accuracies for scene classification, {{we can improve}} the performance further by using crosscorrelation between categories. We extend PHLAC features to crosscorrelations of <b>posterior</b> <b>probabilities</b> of other categories. Also, we introduce the subtraction operator for describing another spatial relationship of local <b>posterior</b> <b>probabilities,</b> and present vertical/horizontal mask patterns for the spatial layout of auto/crosscorrelations. Since the combination of category index is large, we compress the proposed features by two-dimensional principal component analysis. We confirmed {{the effectiveness of the}} proposed methods using Scene- 15 dataset, and our method exhibited competitive performances to recent methods without using spatial grid informations and even using linear classifiers. ...|$|R
25|$|In Bayesian statistics, one can compute (Bayesian) {{prediction}} intervals {{from the}} <b>posterior</b> <b>probability</b> of the random variable, as a credible interval. In theoretical work, credible intervals are not often {{calculated for the}} prediction of future events, but for inference of parameters â€“ i.e., credible intervals of a parameter, not for the outcomes of the variable itself. However, particularly where applications are concerned with possible extreme values of yet to be observed cases, credible intervals for such values can be of practical importance.|$|E
25|$|Often these {{conditional}} distributions include parameters {{which are}} unknown {{and must be}} estimated from data, sometimes using the maximum likelihood approach. Direct maximization of the likelihood (or of the <b>posterior</b> <b>probability)</b> is often complex when there are unobserved variables. A classical approach to this problem is the expectation-maximization algorithm which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions this process converges on maximum likelihood (or maximum posterior) values for parameters.|$|E
25|$|The major {{advance in}} speed was made {{possible}} by the development of an approach for calculating the significance of results integrated over a range of possible alignments. In discovering remote homologs, alignments between query and hit proteins are often very uncertain. While most sequence alignment tools calculate match scores using only the best scoring alignment, HMMER3 calculates match scores by integrating across all possible alignments, to account for uncertainty in which alignment is best. HMMER sequence alignments are accompanied by <b>posterior</b> <b>probability</b> annotations, indicating which portions of the alignment have been assigned high confidence and which are more uncertain.|$|E
40|$|Figure 1 - 50 % {{consensus}} tree {{produced by}} the Bayesian analysis of a concatenated matrix with three loci (nuLSU, mtSSU and RPB 1) with 2531 characters and highlighting the Arctomiaceae and the newly described Arctomia borbonica. Branches supported by MPBS and MLBS > 70 % and Bayesian <b>posterior</b> <b>probabilities</b> > 0. 95 are in black; those supported by MLBS > 70 % and Bayesian <b>posterior</b> <b>probabilities</b> > 0. 95 in dark grey and those only by Bayesian <b>posterior</b> <b>probabilities</b> > 0. 95 in light grey...|$|R
40|$|FIGURE 20. Bayesian {{inference}} consensus tree of the D 3 - D 5 data set. Branch support> 50 of corresponding clades indicated as follow: The bold numbers {{refer to}} bayesian <b>posterior</b> <b>probabilities.</b> Numbers in italics are minimum-evolution bootstrap support values of corresponding clades. The inset displays the Maximum-likelihood tree {{resulting from the}} separate analysis of the Halichondriidae s. s. / Suberitidae clade (see text). The bold numbers refer to bayesian <b>posterior</b> <b>probabilities,</b> numbers in italics are minimum-evolution bootstrap support values, followed by bayesian <b>posterior</b> <b>probabilities</b> under secondary structure specific models (regular font) ...|$|R
40|$|Figure 1 - Bayesian phylogram {{showing the}} phylogenetic {{placement}} of the taxa described in this study within the Doryctinae. Black circles near branches represent <b>posterior</b> <b>probabilities</b> â‰¥ 0. 95; blank circles represent <b>posterior</b> <b>probabilities</b> between 0. 90 and 0. 94. Names of the major clades are according to ZaldÃ­var-RiverÃ³n et al. (2008) ...|$|R
25|$|When {{analyzing}} {{an inverse}} problem, obtaining a maximum likelihood model {{is usually not}} sufficient, as we normally also wish to have information on the resolution power of the data. In the general case {{we may have a}} large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the <b>posterior</b> <b>probability</b> distribution and to analyze and display the models {{in such a way that}} information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available.|$|E
25|$|An {{alternative}} method of structural learning uses optimization based search. It requires a scoring function and a search strategy. A common scoring function is <b>posterior</b> <b>probability</b> {{of the structure}} given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential {{in the number of}} variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.|$|E
25|$|One naÃ¯ve Bayesian {{approach}} to hypothesis testing is to base {{decisions on the}} <b>posterior</b> <b>probability,</b> but this fails when comparing point and continuous hypotheses. Other approaches to decision making, such as Bayesian decision theory, attempt to balance the consequences of incorrect decisions across all possibilities, rather than concentrating on a single null hypothesis. A number of other approaches to reaching a decision based on data are available via decision theory and optimal decisions, {{some of which have}} desirable properties. Hypothesis testing, though, is a dominant {{approach to}} data analysis in many fields of science. Extensions to the theory of hypothesis testing include the study of the power of tests, i.e. the probability of correctly rejecting the null hypothesis given that it is false. Such considerations can be used for the purpose of sample size determination prior to the collection of data.|$|E
40|$|FIGURE 5. Genus Protosuberites. Phylogenetic tree {{obtained}} using a concatenated {{analysis of}} COI, 18 S, and 28 S and analyzed by both Bayesian inference (BI) and maximum likelihood (ML) methods. Numbers over branches indicate <b>posterior</b> <b>probabilities</b> and below bootstrap values. Only <b>posterior</b> <b>probabilities</b> over 0. 95 and bootstrap values over 70 are indicated...|$|R
40|$|A novel fully {{automatic}} Bayesian procedure for variable selection in normal regression model is proposed. The procedure uses the <b>posterior</b> <b>probabilities</b> {{of the models}} to drive a stochastic search. The <b>posterior</b> <b>probabilities</b> are computed using intrinsic priors, which can be considered default priors for model selection problems. That is, they are derived from the model structure and are free from tuning parameters. Thus, they {{can be seen as}} objective priors for variable selection. The stochastic search is based on a Metropolis-Hastings algorithm with a stationary distribution proportional to the model <b>posterior</b> <b>probabilities.</b> The procedure is illustrated on both simulated and real examples...|$|R
3000|$|... are re-estimated via the EM {{procedure}}. The EM procedure {{includes two}} alternate steps: (i) an expectation (E) step where <b>posterior</b> <b>probabilities</b> are computed for the latent variables {{based on the}} current estimates of the parameters, (ii) a maximization (M) step where PLSA parameters are updated based on the <b>posterior</b> <b>probabilities</b> computed in the E-step [15].|$|R
500|$|However, as {{the number}} of {{sequences}} increases and especially in genome-wide studies that involve many MSAs it is impossible to manually curate all alignments. Furthermore, manual curation is subjective. And finally, even the best expert cannot confidently align the more ambiguous cases of highly diverged sequences. In such cases it is common practice to use automatic procedures to exclude unreliably aligned regions from the MSA. For the purpose of phylogeny reconstruction (see below) the Gblocks program is widely used to remove alignment blocks suspect of low quality, according to various cutoffs on the number of gapped sequences in alignment columns. However, these criteria may excessively filter out regions with insertion/deletion events that may still be aligned reliably, and these regions might be desirable for other purposes such as detection of positive selection. A few alignment algorithms output site-specific scores that allow the selection of high-confidence regions. Such a service was first offered by the SOAP program, which tests the robustness of each column to perturbation in the parameters of the popular alignment program CLUSTALW. The T-Coffee program uses a library of alignments {{in the construction of the}} final MSA, and its output MSA is colored according to confidence scores that reflect the agreement between different alignments in the library regarding each aligned residue. [...] Its extension, [...] : (Transitive Consistency Score), uses T-Coffee libraries of pairwise alignments to evaluate any third party MSA. Pairwise projections can be produced using fast or slow methods, thus allowing a trade-off between speed and accuracy. Another alignment program that can output an MSA with confidence scores is FSA, which uses a statistical model that allows calculation of the uncertainty in the alignment. The HoT (Heads-Or-Tails) score can be used as a measure of site-specific alignment uncertainty due to the existence of multiple co-optimal solutions. The GUIDANCE program calculates a similar site-specific confidence measure based on the robustness of the alignment to uncertainty in the guide tree that is used in progressive alignment programs. An alternative, more statistically justified approach to assess alignment uncertainty is the use of probabilistic evolutionary models for joint estimation of phylogeny and alignment. A Bayesian approach allows calculation of posterior probabilities of estimated phylogeny and alignment, which is a measure of the confidence in these estimates. In this case, a <b>posterior</b> <b>probability</b> can be calculated for each site in the alignment. Such an approach was implemented in the program BAli-Phy.|$|E
2500|$|Two {{of eight}} {{symptoms}} favored polio, with the <b>posterior</b> <b>probability</b> of polio shown for each: ...|$|E
2500|$|Six {{of eight}} {{symptoms}} favored GBS, with the <b>posterior</b> <b>probability</b> of GBS shown for each: ...|$|E
40|$|FIGURE 1. Bayesian {{consensus}} tree showing inferred evolutionary {{relationships within}} Phyllomedusinae and the phylogenetic position of Agalychnis terranova sp. nov. Bayesian support on nodes is shown as <b>posterior</b> <b>probabilities</b> and circles represent <b>posterior</b> <b>probabilities</b> of 1. 0. We included as outgroups representatives of Dendropsophus, Litoria and Hypsiboas. We used Dendropsophus ebraccatus to root trees. Dendropsophus and Hypsiboas, not shown...|$|R
40|$|FIGURE 2. Maximum {{likelihood}} tree of Bolitoglossa (Eladinea) {{based on}} the concatenated sequences of 16 S rRNA and cyt b genes. The new sequences of the Bolitoglossa species included in this work are highlighted in different colors. Numbers on nodes represent bootstrap support and <b>posterior</b> <b>probabilities,</b> respectively. ML bootstrap support and Bayesian <b>posterior</b> <b>probabilities</b> are shown on the internodes...|$|R
40|$|FIGURE 2. Species tree {{based on}} * BEAST inference. Node support values at {{the heads of}} nodes refer to * BEAST <b>posterior</b> <b>probabilities,</b> where as those above and below nodes are the <b>posterior</b> <b>probabilities</b> of {{speciation}} from the BPP analyses when using algorithm 0 (above) and algorithm 1 (below) and four combinations of different Î¸ and Ï„ prior distributions...|$|R
2500|$|... is the <b>posterior</b> <b>{{probability}},</b> is {{the probability}} for A after {{taking into account}} B for and against A.|$|E
2500|$|... form {{a sample}} from the <b>posterior</b> <b>probability</b> distribution. To {{see this in}} the scalar case with : Let , and [...] Then ...|$|E
2500|$|Given data [...] and {{parameter}} , {{a simple}} Bayesian analysis {{starts with a}} prior probability (prior) [...] and likelihood [...] to compute a <b>posterior</b> <b>probability</b> [...]|$|E
40|$|FIGURE 6. Bayesian {{phylogeny}} of the M. coosae species {{group based}} on the partial mitochondrial ND 2 gene. Numbers above nodes are <b>posterior</b> <b>probabilities.</b> Numbers following <b>posterior</b> <b>probabilities</b> or below nodes are bootstrap values resulting from the Maximum Parsimony analysis. Symbols correspond to those in Figure 5. Letters after species names correspond to individual identifiers used in Table 1...|$|R
40|$|International audienceOwing to the {{exponential}} growth of genome databases, phylogenetic trees are now widely {{used to test}} a variety of evolutionary hypotheses. Nevertheless, computation time burden limits the application of methods such as maximum likelihood nonparametric bootstrap to assess reliability of evolutionary trees. As an alternative, the much faster Bayesian inference of phylogeny, which expresses branch support as <b>posterior</b> <b>probabilities,</b> has been introduced. However, marked discrepancies exist between nonparametric bootstrap proportions and Bayesian <b>posterior</b> <b>probabilities,</b> leading to difficulties {{in the interpretation of}} sometimes strongly conflicting results. As an attempt to reconcile these two indices of node reliability, we apply the nonparametric bootstrap resampling procedure to the Bayesian approach. The correlation between <b>posterior</b> <b>probabilities,</b> bootstrap maximum likelihood percentages, and bootstrapped <b>posterior</b> <b>probabilities</b> was studied for eight highly diverse empirical data sets and were also investigated using experimental simulation. Our results show that the relation between <b>posterior</b> <b>probabilities</b> and bootstrapped maximum likelihood percentages is highly variable but that very strong correlations always exist when Bayesian node support is estimated on bootstrapped character matrices. Moreover, simulations corroborate empirical observations in suggesting that, being more conservative, the bootstrap approach might be less prone to strongly supporting a false phylogenetic hypothesis. Thus, apparent conflicts in topology recovered by the Bayesian approach were reduced after bootstrapping. Both <b>posterior</b> <b>probabilities</b> and bootstrap supports are of great interest to phylogeny as potential upper and lower bounds of node reliability, but they are surely not interchangeable and cannot be directly compared...|$|R
40|$|Abstract: In this paper, we {{show that}} the hinge loss can be {{interpreted}} as the neg-log-likelihood of a semi-parametric model of <b>posterior</b> <b>probabilities.</b> From this point of view, SVMs represent the parametric component of a semiparametric model fitted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated <b>posterior</b> <b>probabilities.</b> Unlike previous proposals, the suggested mapping is intervalvalued, providing a set of <b>posterior</b> <b>probabilities</b> compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classification, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures...|$|R
