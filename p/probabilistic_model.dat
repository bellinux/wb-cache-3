6842|9947|Public
5|$|Miller's Language and Communication {{was one of}} {{the first}} {{significant}} texts in the study of language behavior. The book was a scientific study of language, emphasizing quantitative data, and was based on the mathematical model of Claude Shannon's information theory. It used a <b>probabilistic</b> <b>model</b> imposed on a learning-by-association scheme borrowed from behaviorism, with Miller not yet attached to a pure cognitive perspective. The first part of the book reviewed information theory, the physiology and acoustics of phonetics, speech recognition and comprehension, and statistical techniques to analyze language. The focus was more on speech generation than recognition. The second part had the psychology: idiosyncratic differences across people in language use; developmental linguistics; the structure of word associations in people; use of symbolism in language; and social aspects of language use.|$|E
25|$|This logic has applications, {{for example}} a {{cryptographic}} attack called the birthday attack, which uses this <b>probabilistic</b> <b>model</b> {{to reduce the}} complexity of finding a collision for a hash function.|$|E
25|$|Mecca Chiesa {{notes that}} the {{probabilistic}} or selectionistic determinism of B.F. Skinner comprised a wholly separate conception of determinism that was not mechanistic at all. Mechanistic determinism assumes that every event has an unbroken chain of prior occurrences, but a selectionistic or <b>probabilistic</b> <b>model</b> does not.|$|E
40|$|<b>Probabilistic</b> <b>models</b> have {{recently}} been utilized for the optimization of large combinatorial search problems. However, complex <b>probabilistic</b> <b>models</b> that attempt to capture interparameter dependencies can have prohibitive computational costs. The algorithm presented in this paper, termed COMIT, provides a method for using <b>probabilistic</b> <b>models</b> in conjunction with fast search techniques. We show how COMIT {{can be used with}} two very different fast search algorithms: hillclimbing and Population-based incremental learning (PBIL). The resulting algorithms maintain many of the benefits of <b>probabilistic</b> <b>modeling,</b> with far less computational expense. Extensive empirical results are provided; COMIT has been successfully applied to jobshop scheduling, traveling salesman, and knapsack problems. This paper also presents a review of <b>probabilistic</b> <b>modeling</b> for combinatorial optimization. ...|$|R
40|$|<b>Probabilistic</b> <b>modeling</b> is a {{powerful}} approach for analyzing empirical information. We describe Edward, a library for <b>probabilistic</b> <b>modeling.</b> Edward's design reflects an iterative process pioneered by George Box: build {{a model of a}} phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of <b>probabilistic</b> <b>models,</b> efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex <b>probabilistic</b> <b>models</b> and their algorithms at a massive scale...|$|R
40|$|Abstract <b>Probabilistic</b> <b>models</b> {{that are}} {{applicable}} to probabilistic inference are often given as graphical representations Moreover {{it is known}} that the <b>probabilistic</b> <b>models</b> for <b>probabilistic</b> inference are sometimes equivalent to familiar statisticalmechanical models A cluster varia tion method is one of familiar statisticalmechanical techniques for calculating some statistical quantities in massive <b>probabilistic</b> <b>models</b> The algorithms constructed by applying the cluster variation method for <b>probabilistic</b> <b>models</b> with loopy graphical representations is equivalent to belief propagation algorithms In this paper the review of the application of a cluster variation method to probabilistic inference is given and a new method for calculating correlation functions between any pairs of nodes is proposed by combining the cluster variation method with a linear response theor...|$|R
25|$|Entropy {{is defined}} {{in the context of}} a <b>probabilistic</b> <b>model.</b> Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next {{character}} will always be a 'B'.|$|E
25|$|Around 1620 Galileo wrote a paper called On a {{discovery}} concerning dice that used an early <b>probabilistic</b> <b>model</b> to address specific questions. In 1654, prompted by Chevalier de Méré's interest in gambling, Blaise Pascal corresponded with Pierre de Fermat, {{and much of}} the groundwork for probability theory was laid. Pascal's Wager was noted for its early use of the concept of infinity, and the first formal use of decision theory. The work of Pascal and Fermat influenced Leibniz's work on the infinitesimal calculus, which in turn provided further momentum for the formal analysis of probability and randomness.|$|E
25|$|Next-generation {{instruments}} are now also enabling the sequencing of whole uncultured metagenomic communities. The sequencing scenario {{is more complicated}} here and there are various ways of framing design theories for a given project. For example, Stanhope developed a <b>probabilistic</b> <b>model</b> {{for the amount of}} sequence needed to obtain at least one contig of a given size from each novel organism of the community, while Wendl et al. reported analysis for the average contig size or the probability of completely recovering a novel organism for a given rareness within the community. Conversely, Hooper et al. propose a semi-empirical model based on the Gamma distribution.|$|E
40|$|Numerous methodologies for {{estimating}} {{the reliability of}} fibre-reinforced plastics {{have been published in}} the past few decades. Several of these methodologies use different mechanical and <b>probabilistic</b> <b>models,</b> each one based on a number of assumptions and approximations. The objective {{of this study is to}} assess common assumptions and approximations made on mechanical and <b>probabilistic</b> <b>models</b> used in reliability analyses of fibre-reinforced plastic laminates. The assessment consists of two parts: a theoretical overview of the models and their justification, and an investigation of the quantitative influence of the models on the reliability estimates of a group of fibre-reinforced cross-ply laminates. The reliability estimates are calculated through Monte Carlo simulations using different mechanical and <b>probabilistic</b> <b>models.</b> This study concludes that both mechanical and <b>probabilistic</b> <b>models</b> can significantly influence the reliability estimations. For the mechanical models, the factor with the greatest influence is the definition and modelling of matrix cracking. While, for the <b>probabilistic</b> <b>models,</b> the choice of probability distribution for modelling ply property variability has the greatest influence...|$|R
40|$|Probabilistic {{approaches}} are widely adopted for the modeling of rockfall trajectories, {{but are not}} widely discussed in the literature. This paper aims to help fill this gap by reviewing <b>probabilistic</b> <b>models</b> of rockfall trajectories, while providing some perspectives for future study. We first make it clear that, from a theoretical point of view, the probabilistic approach is necessitated by both the ontic (inherent) uncertainty associated with rockfalls and the epistemic (information) uncertainty associated with numerical modeling. The review suggests {{that there may be}} the potential to improve the <b>probabilistic</b> <b>modeling</b> of rockfall trajectories in various aspects, including the systematic <b>probabilistic</b> <b>modeling</b> criterion, the random sampling approaches employed for probabilistic variables, the <b>probabilistic</b> <b>modeling</b> of rock shape, and the probabilistic prediction of rockfall intensity. However, there are still some open questions regarding the promotion of <b>probabilistic</b> <b>modeling</b> in practice. It is not clear whether probabilistically treating all of the variables of rockfall trajectory model with reasoned distributions will lead to significantly improved results, or whether the improvements will be great enough (given the difficulties and costs involved) that it is worth quantifying all of the uncertainties involved in rockfall trajectory modeling. The answers to these questions {{can be found in the}} practice of <b>probabilistic</b> <b>modeling</b> itself...|$|R
40|$|AbstractHere, a new Real-coded Estimation of Distribution Algorithm (EDA) is {{proposed}}. The proposed EDA {{is called}} Real-coded EDA using Multiple <b>Probabilistic</b> <b>Models</b> (RMM). RMM includes multiple types of <b>probabilistic</b> <b>models</b> with different learning rates and diversities. The search capability of RMM was examined through {{several types of}} continuous test function. The {{results indicated that the}} search capability of RMM is better than or equivalent to that of existing Real-coded EDAs. Since better searching points are distributed for other <b>probabilistic</b> <b>models</b> positively, RMM can discover the global optimum {{in the early stages of}} the search...|$|R
25|$|The log-logistic {{has been}} used as a model for the period of time {{beginning}} when some data leaves a software user application in a computer and the response is received by the same application after travelling through and being processed by other computers, applications, and network segments, most or all of them without hard real-time guarantees (for example, when an application is displaying data coming from a remote sensor connected to the Internet). It has been shown to be a more accurate <b>probabilistic</b> <b>model</b> for that than the log-normal distribution or others, as long as abrupt changes of regime in the sequences of those times are properly detected.|$|E
2500|$|Although entropy {{is often}} used as a {{characterization}} of the information content of a data source, this information content is not absolute: it depends crucially on the <b>probabilistic</b> <b>model.</b> A source that always generates the same symbol has an entropy rate of 0, but the definition of what a symbol is depends on the alphabet. Consider a source that produces the string ABABABABAB… in which A is always followed by B and vice versa. If the <b>probabilistic</b> <b>model</b> considers individual letters as independent, the entropy rate of the sequence is 1 bit per character. But if the sequence is considered as [...] "AB AB AB AB AB …" [...] with symbols as two-character blocks, then the entropy rate is 0 bits per character.|$|E
2500|$|PCFG design impacts the {{secondary}} structure prediction accuracy. Any useful structure prediction <b>probabilistic</b> <b>model</b> based on PCFG has to maintain simplicity without much compromise to prediction accuracy. Too complex {{a model of}} excellent performance on a single sequence may not scale. A grammar based model should be able to: ...|$|E
5000|$|Edward: A {{library for}} <b>probabilistic</b> <b>modeling,</b> inference, and criticism.|$|R
40|$|Abstract. <b>Probabilistic</b> <b>models</b> are {{developed}} to determine {{changes in the}} survival probability particularly when prior loading contributes to changes in residual stress and fracture toughness. Experimental evidence for variations in toughness and residual stress is examined and then incorporated into <b>probabilistic</b> <b>models.</b> The structure’s reliability is consistently better after prior loading...|$|R
40|$|AbstractVarious <b>probabilistic</b> <b>models</b> for {{the random}} {{locations}} {{of individuals in}} a region are considered. Models for both infinite and finite regions are developed. In the literature, certain marginal distributions are used to characterize the numbers of individuals in subregions. Relationships between these marginal distributions and our <b>probabilistic</b> <b>models</b> are discussed...|$|R
2500|$|However, {{if we use}} {{very large}} blocks, then the {{estimate}} of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are [...] published books, and each book is only published once, the estimate of the probability of each book is , and the entropy (in bits) is [...] As a practical code, this corresponds to assigning each book a unique identifier and using it {{in place of the}} text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: {{it is not possible to}} reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the <b>probabilistic</b> <b>model</b> must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the <b>probabilistic</b> <b>model),</b> is one such program, but it may not be the shortest.|$|E
2500|$|The {{notion that}} self-organising {{biological}} systems – like a cell or brain – {{can be understood}} as minimising variational free energy is based upon Helmholtz’s observations on unconscious inference [...] and subsequent treatments in psychology [...] and machine learning. Variational free energy is a function of some outcomes and a probability density over their (hidden) causes. This variational density is defined in relation to a <b>probabilistic</b> <b>model</b> that generates outcomes from causes. In this setting, free energy provides an (upper bound) approximation to Bayesian model evidence. Its minimisation can therefore be used to explain Bayesian inference and learning. When a system actively samples outcomes to minimise free energy, it implicitly performs active inference and maximises the evidence for its (generative) model.|$|E
5000|$|<b>Probabilistic</b> <b>model</b> (non-parametric, e.g. Kernel Smoothing; parametric, e.g. Normal, Beta) ...|$|E
50|$|<b>Probabilistic</b> <b>models</b> require transitivity {{only within}} the bounds of errors of {{estimates}} of scale locations of entities. Thus, decisions need not be deterministically transitive in order to apply <b>probabilistic</b> <b>models.</b> However, transitivity will generally hold for a large number of comparisons if models such as the BTL can be effectively applied.|$|R
5000|$|The {{process by}} which a subject is {{proposed}} to go about forming such rules or hypothesis has been the topic of formal <b>probabilistic</b> <b>modeling,</b> a discussion {{of which can be}} found in the references. A conceptual framework for formal <b>probabilistic</b> <b>modeling</b> of hypotheses in cognitive research has been given by Rudolf Groner ...|$|R
40|$|The {{hierarchical}} Bayesian {{optimization algorithm}} (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling <b>probabilistic</b> <b>models</b> of promising solutions. This paper analyzes <b>probabilistic</b> <b>models</b> in hBOA on two common test problems: concatenated traps and 2 D Ising spin glasses with periodic boundary conditions. We argue that although Bayesian networks with local structures can encode complex probability distributions, analyzing these models in hBOA is relatively straightforward {{and the results}} of such analyses may provide practitioners with useful information about their problems. The results show that the <b>probabilistic</b> <b>models</b> in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in subsequent iterations of BOA, and creating adequate <b>probabilistic</b> <b>models</b> by hand is not straightforward even with complete knowledge of the optimization problem...|$|R
5000|$|A <b>Probabilistic</b> <b>Model</b> of Learning in Games, 64 Econometrica 1375 (1996) ...|$|E
50|$|In {{the field}} of {{information}} retrieval, divergence from randomness is one type of <b>probabilistic</b> <b>model.</b>|$|E
5000|$|A <b>Probabilistic</b> <b>Model</b> of Redundancy in Information Extraction, D. Downey, O. Etzioni, and S. Soderland.|$|E
40|$|We {{introduce}} {{a class of}} neural networks derived from <b>probabilistic</b> <b>models</b> {{in the form of}} Bayesian belief networks. By imposing additional assumptions {{about the nature of the}} <b>probabilistic</b> <b>models</b> represented in the belief networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian belief networks, providing distributed versions of <b>probabilistic</b> <b>models.</b> Comment: 7 pages, 3 figures, 1 table, submitted to Phys Rev...|$|R
5000|$|Edward is a Python {{library for}} <b>probabilistic</b> <b>modeling,</b> inference, and criticism.|$|R
40|$|<b>Probabilistic</b> <b>modeling</b> of lexicalized grammars is {{difficult}} because these grammars exploit com-plicated data structures, such as typed feature structures. This prevents us from applying common methods of <b>probabilistic</b> <b>modeling</b> in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied {{on the structure of}} the target problem, namely lattices or trees, and cannot be applied to graph structures including typed fea-ture structures. This article proposes the feature forest model as {{a solution to the problem}} of <b>probabilistic</b> <b>modeling</b> of complex data structures including typed feature structures. The feature forest model provides a method for <b>probabilistic</b> <b>modeling</b> without the independence assumption when prob-abilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus <b>probabilistic</b> <b>modeling</b> o...|$|R
5000|$|Beneish M-Score is a <b>probabilistic</b> <b>model,</b> so it cannot detect {{companies}} that manipulate their earnings with 100% accuracy.|$|E
5000|$|In 2005, Etzioni {{received}} an IJCAI Distinguished Paper Award for [...] "A <b>Probabilistic</b> <b>Model</b> of Redundancy in Information Extraction".|$|E
5000|$|... with θ being a set (vector) of parameters. The {{function}} taking θ to log P(X|θ) is the log-likelihood of the <b>probabilistic</b> <b>model.</b>|$|E
5000|$|... 2001. Conditional Random Fields: <b>Probabilistic</b> <b>Models</b> for Segmenting and Labeling Sequence Data.|$|R
40|$|The {{method of}} logic and <b>probabilistic</b> <b>models</b> {{constructing}} for multivariate heterogeneous time series is offered. There are some important properties of these models, e. g. universality. In this paper also discussed the logic and <b>probabilistic</b> <b>models</b> distinctive features {{in comparison with}} hidden Markov processes. The early proposed time series forecasting algorithm is tested on applied task...|$|R
40|$|How do <b>probabilistic</b> <b>models</b> {{represent}} their targets {{and how do}} they allow us to learn about them? The answer to this question depends on a number of details, in particular on the meaning of the probabilities involved. To classify the options, a minimalist conception of representation (Suárez 2004) is adopted: Modelers devise substitutes ("sources") of their targets and investigate them to infer something about the target. <b>Probabilistic</b> <b>models</b> allow us to infer probabilities about the target from probabilities about the source. This leads to a framework in which we can systematically distinguish between different <b>models</b> of <b>probabilistic</b> <b>modeling.</b> I develop a fully Bayesian view of <b>probabilistic</b> <b>modeling,</b> but I argue that, as an alternative, Bayesian degrees of belief about the target may be derived from ontic probabilities about the source. Remarkably, some accounts of ontic probabilities can avoid problems if they are supposed to apply to sources only...|$|R
