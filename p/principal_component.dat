10000|10000|Public
5|$|On Mars, {{observations}} of the Phoenix Mars lander reveal that water-based snow crystals occur at high latitudes. Additionally, carbon dioxide precipitates from clouds during the Martian winters at the poles and contributes to a seasonal deposit of that compound, which is the <b>principal</b> <b>component</b> of that planet's ice caps.|$|E
5|$|The <b>principal</b> <b>component</b> of the Solar System is the Sun, a G2 {{main-sequence star}} that {{contains}} 99.86% of the system's known mass and dominates it gravitationally. The Sun's four largest orbiting bodies, the giant planets, account for 99% {{of the remaining}} mass, with Jupiter and Saturn together comprising more than 90%. The remaining objects of the Solar System (including the four terrestrial planets, the dwarf planets, moons, asteroids, and comets) together comprise less than 0.002% of the Solar System's total mass.|$|E
25|$|From this, he {{constructed}} phylogenetic {{trees that}} showed genetic distances diagrammatically. His team also performed <b>principal</b> <b>component</b> analyses, {{which is good}} at analysing multivariate data with minimal loss of information. The information that is lost can be partly restored by generating a second <b>principal</b> <b>component,</b> and so on. In turn, the information from each individual <b>principal</b> <b>component</b> (PC) can be presented graphically in synthetic maps. These maps show peaks and troughs, which represent populations whose gene frequencies take extreme values compared {{to others in the}} studied area.|$|E
40|$|This paper {{presents}} a method called quadtree <b>principal</b> <b>components</b> analysis for facial expression classification. The quadtree <b>principal</b> <b>components</b> analysis {{is an image}} transformation that {{takes its name from}} the quadtree partition scheme on which it is based. The quadtree <b>principal</b> <b>components</b> analysis method implements a global-local decomposition of the input face image. This solves the problems associated with the existing <b>principal</b> <b>components</b> analysis and local <b>principal</b> <b>components</b> analysis methods when applied to facial expression classification...|$|R
30|$|Rotate the <b>principal</b> <b>components</b> using varimax {{rotation}} [57]. Varimax rotation is an orthogonal transform that rotates the <b>principal</b> <b>components</b> {{such that the}} variance of the factors is maximized. This rotation improves the interpretability of the <b>principal</b> <b>components.</b>|$|R
40|$|The {{essence of}} <b>principal</b> <b>components</b> {{analysis}} {{and the problem}} of dimension reduction are described. A method of <b>principal</b> <b>components</b> calculation is presented, which is based on the covariance matrix eigenvalues determination. Practical implementations of <b>principal</b> <b>components</b> analysis are described, which are based on QR-algorithm. Application of <b>principal</b> <b>components</b> analysis in space images classification for the reduction of training samples dimension is discussed...|$|R
25|$|<b>Principal</b> <b>component</b> analysis. The {{method of}} fitting a linear {{subspace}} to multivariate data by minimising the chi distances.|$|E
25|$|<b>Principal</b> <b>component</b> {{analysis}} {{is used to}} study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, {{in the field of}} psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, <b>principal</b> <b>component</b> analysis {{can be used as a}} method of factor analysis in structural equation modeling.|$|E
25|$|<b>Principal</b> <b>component</b> {{regression}} (PCR) is {{used when}} the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using <b>principal</b> <b>component</b> analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|E
3000|$|... ϕxk and Ψxl are the <b>principal</b> <b>components</b> after decomposing pxt and rxt, respectively, {{using the}} {{weighted}} <b>principal</b> <b>components</b> algorithm.|$|R
40|$|<b>Principal</b> <b>components</b> {{analysis}} {{relates to}} the eigenvalue distribution of Wishart matrices. Given few observations and very many variables this distribution maps to eigenvalue statistics in the Gaussian orthogonal ensemble. <b>Principal</b> <b>components</b> selection can then be based on existing analytical results. <b>Principal</b> <b>components</b> analysis Random matrix theory...|$|R
40|$|AbstractBased on {{the concept}} of {{complexity}} or minimum description length developed by Kolmogorov, Rissanen, Wallace, and others, an index of predictive power is proposed as a criterion to select the <b>principal</b> <b>components</b> of a random vector distributed in a parametric family. This criterion, when applied to the <b>principal</b> <b>components</b> selection, considers the lost information due to the reduction of the parameters as well as the observed variables. The <b>principal</b> <b>components,</b> obtained by minimizing the index of predictive power, turn out to be identical to the classical <b>principal</b> <b>components</b> when the assumed distribution is normal. A test procedure for the <b>principal</b> <b>components</b> selection is constructed and discussed. Finally, <b>principal</b> <b>components</b> for a type of ϵ-contaminated normal family are given, and are shown to converge to those of the normal distribution. Results from a simulation study are also presented...|$|R
25|$|Several {{important}} {{problems can}} be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm {{is based on the}} singular value decomposition. The corresponding tool in statistics is called <b>principal</b> <b>component</b> analysis.|$|E
25|$|Multilayer kernel {{machines}} (MKM) {{are a way}} {{of learning}} highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel <b>principal</b> <b>component</b> analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of the deep learning architecture.|$|E
25|$|The {{approach}} above {{belongs to}} the model-based approach. Another appearance-based approach recognizes individuals through binary gait silhouette sequences. For example, silhouette sequences of full gait cycles can be treated as 3D tensor samples, and multilinear subspace learning, such as the multilinear <b>principal</b> <b>component</b> analysis, can be employed to learning features for classification.|$|E
40|$|PRIN obtains the <b>principal</b> <b>{{components}}</b> of a {{group of}} series. The number of such components obtained may be a fixed number, or it may be determined by the amount of variance in the original series explained by the <b>principal</b> <b>components.</b> <b>Principal</b> <b>components</b> are a set of orthogonal vectors with the same number of observations as the original set of series which explain as much variance as possible of the original series. Users of this procedure should be familiar with the method and uses of <b>principal</b> <b>components,</b> which are described in many standard texts such as Harman or Theil (see the references). Usage: To obtain <b>principal</b> <b>components</b> in TSP give the word PRIN followed by a list of series whose <b>principal</b> <b>components</b> you want. The options determine how many <b>principal</b> <b>components</b> will be found. The resulting <b>principal</b> <b>components</b> are also series and are stored in data storage under the names created from the NAME = option. Options: NAME = the prefix to be given to the names of the <b>principal</b> components: the <b>components</b> will be called prefix 1, prefix 2, and so forth. You may use any legal TSP name as the name for the <b>principal</b> <b>components,</b> but the names generated by adding the numbers must also be legal TSP names (i. e., of the appropriate length). NCOM = the maximum number of components to be determined. The actual number will be the minimum of the numbe...|$|R
40|$|PCA in high dimensions. • Sparsity of <b>principal</b> <b>components.</b> • Consistent {{estimation}} and minimax theory. • Feasible algorithms using convex relaxation. <b>Principal</b> <b>Components</b> Analysis • I have iid {{data points}} X 1, [...] .,Xn on p variables. • p may be large, so I {{want to use}} <b>principal</b> <b>components</b> analysis (PCA) for dimension reduction...|$|R
30|$|Mathematically, the <b>principal</b> <b>components</b> {{of a given}} set of {{biometric}} images {{characterize the}} variations of the data [18, 19]. In other words, any image in the set is expressed as a linear combination of <b>principal</b> <b>components.</b> The number of possible <b>principal</b> <b>components</b> {{is equal to the}} number of the images in the set. However, the contribution of some <b>principal</b> <b>components</b> is small enough and negligible. The most dominant components are related to the greatest eigenvalues of the covariance matrix of the image set. Then, the problem reduces to find the eigenvectors of the covariance matrix related to greatest M eigenvalues.|$|R
25|$|Geneticists {{have found}} that Europe is {{relatively}} genetically homogeneous, but distinct sub-population patterns of various types of genetic markers have been found, particularly along a southeast-northwest cline. For example, Cavalli-Sforza’s <b>principal</b> <b>component</b> analyses revealed five major clinal patterns throughout Europe, and similar patterns have continued {{to be found in}} more recent studies.|$|E
25|$|A 2007 {{study by}} Bauchet, which {{utilised}} about 10,000 autosomal DNA SNPs arrived at similar results. <b>Principal</b> <b>component</b> analysis clearly identified four widely dispersed groupings, corresponding to Africa, Europe, Central Asia and South Asia. PC1 separated Africans {{from the other}} populations, PC2 divided Asians from Europeans and Africans, whilst PC3 split Central Asians apart from South Asians.|$|E
25|$|Bronze asses {{and their}} {{fractions}} (all now struck rather than cast) {{continued to be}} produced to a standard of about 55grams; this was very quickly reduced to a sextantal standard and finally an uncial standard of roughly 32 gms. By this time, asses outnumbered their fractions, perhaps because legionary pay was increased {{to the point where}} the as could become the <b>principal</b> <b>component.</b>|$|E
50|$|In PCR, {{instead of}} regressing the {{dependent}} variable on the explanatory variables directly, the <b>principal</b> <b>components</b> of the explanatory variables are used as regressors. One typically uses only a subset of all the <b>principal</b> <b>components</b> for regression, thus making PCR {{some kind of a}} regularized procedure. Often the <b>principal</b> <b>components</b> with higher variances (the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance-covariance matrix of the explanatory variables) are selected as regressors. However, for the purpose of predicting the outcome, the <b>principal</b> <b>components</b> with low variances may also be important, in some cases even more important.|$|R
30|$|A {{combination}} of methods for image subtraction and <b>principal</b> <b>components</b> analysis and {{the division of}} images and <b>principal</b> <b>components</b> analysis were used to achieve exact results since the application of only one method limits the precision of the results. <b>Principal</b> <b>components</b> analysis cannot be considered {{for the purpose of}} research because information on LULC change will not appear for only one component (Trincsi et al., 2014; Gong et al. 2015).|$|R
40|$|Velicer (1976) {{proposed}} that, when conducting <b>principal</b> <b>components</b> {{analysis as}} {{a version of}} factor analysis, the number of components one should extract is that at which the average partial correlation of the variables, after partialling out m <b>principal</b> <b>components,</b> would be a minimum. minap calculates this minimum average partial correlation. It can take as input either a variable list or a correlation matrix. <b>principal</b> <b>components,</b> partial correlation, factor analysis...|$|R
25|$|Methane {{is used in}} {{industrial}} chemical processes and may be transported as a refrigerated liquid (liquefied natural gas, or LNG). While leaks from a refrigerated liquid container are initially heavier than air due to the increased density of the cold gas, the gas at ambient temperature is lighter than air. Gas pipelines distribute large amounts of natural gas, of which methane is the <b>principal</b> <b>component.</b>|$|E
25|$|Another {{lysosomal}} {{storage disease}} often {{confused with the}} mucopolysaccharidoses is mucolipidosis. In this disorder, excessive amounts of fatty materials known as lipids (another <b>principal</b> <b>component</b> of living cells) are stored, in addition to sugars. Persons with mucolipidosis may {{share some of the}} clinical features associated with the mucopolysaccharidoses (certain facial features, bony structure abnormalities, and damage to the brain), and increased amounts of the enzymes needed to break down the lipids are found in the blood.|$|E
25|$|The SVD is also applied {{extensively}} to {{the study}} of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to <b>principal</b> <b>component</b> analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.|$|E
40|$|Often, {{all of the}} {{variables}} in a model are latent, random, or subject to measurement error, or {{there is not an}} obvious dependent variable. When any of these conditions exist, an appropriate method for estimating the linear relationships among {{the variables}} is Least <b>Principal</b> <b>Components</b> Analysis. Least <b>Principal</b> <b>Components</b> are robust, consistent, and sufficient maximum likelihood estimates of the best total linear fit to. observed data. They are more appropriate than regression estimates when the smallest eigenvalue exists and is distinct from the next smallest, and the variability to minimize is in more than one variable or when multicollinearity is a problem. They are as easy to compute as are common <b>principal</b> <b>components</b> because they are <b>principal</b> <b>components.</b> T. W. Anderson (1963) provides a theory of inferential statistics for <b>Principal</b> <b>Components</b> {{that can be used in}} computing significance levels and confidence intervals for least <b>principal</b> <b>components</b> as well. Bootstrap approaches have also been developed. (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
3000|$|... {{are called}} <b>principal</b> <b>components.</b> Assuming that our unlabeled data are mean normalized, i.e., m= 0, and {{comparing}} this equation with Equation (7) {{we see that}} the eigenvectors and the <b>principal</b> <b>components</b> correspond to the basis functions [...]...|$|R
40|$|<b>Principal</b> <b>components</b> are the {{benchmark}} for linear dimension reduction, {{but they are}} not always easy to interpret. For this reason, some alternatives have been proposed in recent years. These methods produce <b>components</b> that, unlike <b>principal</b> <b>components,</b> are correlated and/or have nonorthogonal loadings. This article shows that the criteria commonly used to evaluate <b>principal</b> <b>components</b> are not adequate for evaluating such alternatives, and proposes two new criteria that are more suitable for this purpose...|$|R
25|$|A 2014 {{study by}} Carmi et al. {{published}} by Nature Communications found that Ashkenazi Jewish population originates from mixing between Middle Eastern and European peoples. According to the authors, that mixing likely occurred some 600–800 years ago, followed by rapid growth and genetic isolation (rate per generation 16–53%;). The {{study found that}} all Ashkenazi Jews descent from around 350 individuals, and that the <b>principal</b> <b>component</b> analysis of common variants in the sequenced AJ samples, confirmed previous observations, namely, the proximity of Ashkenazi Jewish cluster to other Jewish, European and Middle Eastern populations".|$|E
25|$|As {{the world}} ocean is the <b>principal</b> <b>component</b> of Earth's hydrosphere, it is {{integral}} to life, forms part of the carbon cycle, and influences climate and weather patterns. The world ocean is the habitat of 230,000 known species, but because {{much of it is}} unexplored, the number of species that exist in the ocean is much larger, possibly over two million. The origin of Earth's oceans is unknown; oceans are thought to have formed in the Hadean eon and may have been the impetus for the emergence of life.|$|E
25|$|Small genetic {{differences}} were reportedly found among Southeastern European (Greece, Albania) populations and {{especially those of}} the Dniester–Carpathian (Romania, Moldova, Ukraine) region. Despite this low level of differentiation between them, tree reconstruction and <b>principal</b> <b>component</b> analyses allowed a distinction between Balkan–Carpathian (Romanians, Moldovans, Ukrainians, Macedonians and Gagauzes) and Balkan Mediterranean (Greeks, Albanians, Turks) population groups. The genetic affinities among Dniester–Carpathian and southeastern European populations do not reflect their linguistic relationships. According to the report, {{the results indicate that}} the ethnic and genetic differentiations occurred in these regions to a considerable extent independently of each other.|$|E
40|$|FIGURE 19. Relative warp grids for {{the first}} and second <b>principal</b> <b>components</b> of the {{variance}} within a size series of G. c. carapo and between subspecies of G. carapo. A – B. First and second <b>principal</b> <b>components</b> of the variance within the G. c. carapo size series (n = 42, 38 – 316 mm). C – D. First and second <b>principal</b> <b>components</b> of the variance between the seven subspecies of G. carapo (n = 88) ...|$|R
50|$|Suppose {{ordinary}} PCA {{is applied}} to a dataset where each input variable represents a different asset, it may generate <b>principal</b> <b>components</b> that are weighted combination of all the assets. In contrast, sparse PCA would produce <b>principal</b> <b>components</b> that are weighted combination {{of only a few}} input assets, so one can easily interpret its meaning. Furthermore, if one uses a trading strategy based on these <b>principal</b> <b>components,</b> fewer assets imply less transaction costs.|$|R
5000|$|... 1. [...] Perform PCA on the {{observed}} data matrix for the explanatory variables {{to obtain the}} <b>principal</b> <b>components,</b> and then (usually) select a subset, based on some appropriate criteria, of the <b>principal</b> <b>components</b> so obtained for further use.|$|R
