0|13|Public
3000|$|... aOnly upper {{triangle}} part {{is stored}} in practice due to the symmetry of the <b>point</b> pairs <b>lookup</b> table. bThe demo application of 4 PCS algorithm [8] is available at [URL] [...]...|$|R
3000|$|One of the {{attributes}} {{assigned to each}} node is a lookup table containing the costs of passing through the space corresponding to this node. Each space can have several entrance/leaving points, especially the ones like halls or corridors. Let us assume that the edges incident to each node are numbered and each number represents one entry/leaving <b>point.</b> The <b>lookup</b> table assigned to a given node v is an array LUT [...]...|$|R
40|$|This paper {{describes}} {{the use of}} an evolutionary algorithm to develop lookup tables which consist of an ordered list of regions, each of which encloses training examples of only one category. Compared to a simpler type of lookup table which consists of an unordered list of the training points and their categories, region based tables are smaller and, in general, faster to use. The development of a region based lookup table for the Frey and Slate character recognition problem is described and the size and accuracy achieved are compared with the original Frey and Slate <b>point</b> based <b>lookup</b> table. The reasons why it outperforms the original lookup table are discussed...|$|R
50|$|Polynomials {{can be used}} to {{approximate}} complicated curves, for example, the shapes of letters in typography, given a few points. A relevant application is the evaluation of the natural logarithm and trigonometric functions: pick a few known data <b>points,</b> create a <b>lookup</b> table, and interpolate between those data points. This results in significantly faster computations. Polynomial interpolation also forms the basis for algorithms in numerical quadrature and numerical ordinary differential equations and Secure Multi Party Computation, Secret Sharing schemes.|$|R
40|$|Abstract—This work {{designed}} an embedded MPEG- 4 {{simple profile}} video decoder with OpenMax API standard. OpenMAX API has two advantages; exploiting prominent {{features of the}} target architecture and avoiding compatibility problems. The target platform is SMDK 2450 with ARM 926 EJ-S CPU. The decoder is first developed in C with XViD as reference, followed by plugging of OpenMAX defined modules and optimizations using ARM assembly language. Exploiting DSP features, fixed <b>point</b> arithmetic, <b>lookup</b> tables, multiple 32 -bit loads, and avoiding pipeline stalls by proper ordering of instructions are among the prominent optimization techniques. At each optimization step, we measured Mega Cycles Per Second (MCPS), decoding time, frame rate and SNR. The experimental results with a VGA sized input bitstream shows a final frame rate of about 27. 23 fps. Comparing to initial implementation, the frame rate is improved by about 80 %...|$|R
40|$|Abstract. Many of today’s {{applications}} {{attempt to}} connect mobile users with resources available in their immediate surroundings. Exist-ing approaches for discovering available resources are either centralized, providing a single <b>point</b> of <b>lookup</b> {{somewhere in the}} cloud or ad hoc, re-quiring mobile devices to directly connect to other nearby devices. In this paper, we explore an approach based on cloudlets, marrying these two approaches to reflect both the proximity requirements of the applications and the dynamic nature of the resources. We present the design and im-plementation of a cloudlet-based proximal discovery service, solving key technical challenges along the way. We then use real world data traces to demonstrate, evaluate, and benchmark our service and compare it to a completely centralized approach. We find that, in supporting highly lo-calized queries, our service outperforms the centralized approach without significantly a↵ecting {{the quality of the}} discovery results. Key words: location-based discovery, pervasive computing...|$|R
30|$|In {{order to}} take {{advantage}} of a unified platform with multiple form unwrapping output capability, the mapping functions have been conveniently made to accept and produce points in the Cartesian form. Therefore, the location of each element in a virtual plane (ends up as an image pixel) described in Cartesian points would be easily translated into their respective image <b>points.</b> Generally, a <b>lookup</b> table of corresponding points will be generated so that subsequent unwrapping can be speeded up. Such practice is commonly applied in omnidirectional image unwrapping field and is documented in details in Jeng and Tsai [29] work.|$|R
40|$|National audienceOne of {{the main}} {{elements}} of an Aspect-Oriented Programming (AOP) language or framework is its pointcut language. A pointcut is a predicate which selects program execution points and determines at which points the execution should be affected by an aspect. Experimenting with AspectJ shows that two basic primitive pointcuts, call and execution, dealing with method invoca- tion from the caller and callee standpoints, respectively, lead to confusion. This is due to a subtle interplay between the use of static and dynamic types to select execution <b>points,</b> dynamic <b>lookup,</b> and the expectation to easily select the caller and callee execution points related to the same invocation. As a result, alternative semantics have been proposed but have remained paper design. In this article, we reconsider these various semantics in a practical way by implementing them using CALI, our Common Aspect Language Interpreter. This framework reuses both Java as a base language and AspectJ as a way to select the program execution points of interest. An additional interpretation layer can then be used to prototype interesting AOP variants in a full-blown environment. The paper illustrates the benefits of applying such a setting to the case of the call and execution pointcuts. We show that alternative semantics can be implemented very easily and exercised in the context of AspectJ without resorting to complex compiler technology...|$|R
40|$|A {{heuristic}} {{algorithm is}} developed {{to find the}} optimum locations of interpolation <b>points</b> in constructing <b>lookup</b> tables in electronic circuit design. Cubic spline interpolators are used, and three versions of the algorithm are introduced. Computational results are reported to illustrate {{the effectiveness of the}} methodology. 1991 A. M. S. (MOS) subject classification Codes. 65 D 07, 65 D 17. Key Words and Phrases. Cubic splines, optimum location of nodes, heuristic algorithm. 1. INTRODUCTION. Advances in integrated circuit technology have resulted in increased clock frequencies and higher speeds of operations. With an increased demand on smaller chip size and higher density in high performance systems, the analytical modeling became very difficult and complex. Even analytical modeling of passive elements such as resistors, capacitors, inductors is quite complicated and requires so much computer time in simulation that it is often replaced by modeling based on stored tables of relevant valu [...] ...|$|R
40|$|The {{lookup table}} option, as an {{alternative}} to analytical calculation for evaluating the nonlinear heterogeneous soil characteristics, is introduced and compared for both the Picard and Newton iterative schemes in the numerical solution of Richards’ equation. The lookup table method can be a cost-effective alternative to analytical evaluation in the case of heterogeneous soils, but it has not been examined in detail in the hydrological modeling literature. Three layered soil test problems are considered, and the robustness and accuracy of the lookup table approach are assessed for uniform and non-uniform distributions of <b>lookup</b> <b>points</b> in the soil moisture retention curves. Results from the three one-dimensional test simulations show that the uniform distributed option gives improved convergence and robustness for the drainage problem compared to the non-uniform strategy. On the other hand, the non-uniform technique can be chosen for test problems involving flow into initially dry layered soils...|$|R
40|$|A common {{situation}} in industry is to store measurements for different operating <b>points</b> in the <b>lookup</b> tables, often called maps. They {{are used in}} many tasks, e. g., in control and estimation, and therefore considerable investments in engineering time are spent in measuring them which usually make them accurate descriptions of the fault-free system. They are thus well suited for fault detection, but, however, such a model cannot give fault isolation since only the fault free behavior is modeled. One way to handle this situation would be also to map all fault cases but that would require measurements for all faulty cases, which would be costly if at all possible. Instead, the main contribution here is a method to combine the lookup model with analytical fault models. This makes good use of all modeling efforts of the lookup model for the fault-free case, and combines it with fault models with reasonable modeling and calibration efforts, thus decreasing the engineering effort in the diagnosis design. The approach is exemplified by designing a diagnosis system monitoring the power electronics and the electric machine in a hybrid electric vehicle. An extensive simulation study clearly shows that the approach achieves both good fault detectability and isolability performance. A main point {{is that this is}} achieved without the need for neither measurements of a faulty system nor detailed physical modeling, thus saving considerable amounts of development time...|$|R
40|$|In many {{distributed}} applications, end hosts need to {{know the}} network locations of other nearby participating hosts in order to enhance overall performance. Potential applications that can benefit from the location information include automatic selection of nearby Web servers, proximity routing in a peer-to-peer system, and loss recovery in reliable multicasting. We focus in this paper on the network neighborhood discovery problem in large-scale distributed systems. In these systems, the number of participating nodes can be very large, and the membership can dynamically change. Our goal is for each node to discover other "nearby" participating nodes in a completely decentralized manner, where each node probes only a small subset of other nodes in the system. This approach will lead to improved overall performance by matching client requests for services with participants in the peer-to-peer service system that are, on average, nearby in the network sense. Recent works in distributed peer-to-peer systems, such as Chord, CAN, Tapestry and Pastry, provide efficient distributed lookup structures. In this paper, we investigate a rendezvous-based scheme for a node to discover other nearby participating nodes using a peer-to-peer lookup system such as Chord. Given a key, the Chord protocol maps the key onto a node. Our idea for network neighborhood discovery is for each host to compute a key that characterizes its network location on the Internet. We call such a key the location key, and the nodes that these location keys are mapped to the Rendezvous <b>Points.</b> To <b>lookup</b> other nearby participating nodes, a node seeking some service queries its corresponding rendezvous point using its location key. We focus on the issue of how to generate the location key in a distributed fashion such that nodes that are close {{to each other in the}} actual network will have similar location key values, and therefore be mapped to nearby locations on the Chord ring. In this paper, we examine the performance tradeoffs of such a rendezvous scheme using the Global Network Positioning (GNP) approach to generate the location keys. In GNP, each node measures its network distances to a few landmark nodes to derive its coordinates in a D-dimensional geometric space. We generate a host's Chord location key from its 1 -dimensional GNP coordinate, and use coordinates from a higher dimensional space to refine the searching process for the closest node. We evaluate our scheme in the context of the nearest neighbor discovery problem. Using data from the Active Measurement Project of the National Laboratory for Applied Network Research (NLANR), we compare its performance with a random mapping scheme, where location keys are randomly generated. Using our coordinate-based rendezvous scheme, 66 % of the nodes found their actual closest network neighbor by pinging only a small number of nodes. Singapore-MIT Alliance (SMA...|$|R
40|$|Efficient, robust {{simulation}} of groundwater {{flow in the}} unsaturated zone remains computationally expensive, especially for problems characterized by sharp fronts in both space and time. Standard approaches that employ uniform spatial and temporal discretizations for the numerical solution of these problems lead to inefficient and expensive simulations. Accurate solution of the pressure-head form of Richards' equation is very difficult using standard time integration methods, because the mass balance errors grow unless very small time steps {{are used in the}} time integration process. Richards' equation may be solved for many problems more economically and robustly with variable time step size instead of constant time step size. But variable step-size methods applied to date have relied upon empirical approaches to control step size which do not explicitly control truncation error of the solution. In {{the first part of this}} thesis, we solve Richards' equation using the method of lines with a finite difference approach. We show how a differential algebraic equation implementation of the method of lines can give solutions to Richards' equation that are accurate, have good mass balance properties, and are more economical for a wide range of solution accuracy. We have implemented the method of lines using the higher order time integration ode solver ode 15 s to (i) assure robustness for difficult nonlinear problems and computational efficiency; (ii) develop high order adaptive methods for the time discretization taking into account the different time scales that may appear in the process; (iii) investigate the advantage of using higher-order methods in time. The numerical results demonstrate that the proposed method provides a robust and efficientalternative to standard approaches for simulating variably saturated flow in one spatial dimension. In the 2 nd part of the thesis, we to investigate the convergence and mass balance behavior in Richards' equation-based solvers. As hydrological models become increasingly sophisticated (e. g., coupling with various meteorological, ecological, or biogeochemical components) and are applied in ever more computationally demanding contexts (e. g., the many realizations that are typically generated in parameter estimation, uncertainty analysis, data assimilation, or scenario studies), the need for robust, accurate, and efficient codes is greater than ever. The Richards equation for subsurface flow is highly nonlinear and requires iterative schemes for its solution. These schemes have been the subject of much research over the past two decades, but an effective all-purpose algorithm has thus far proven elusive. Ideally, rapid (quadratic as opposed to linear) and global (insensitive to initial guess) convergence is sought, in addition to applicability over a range of conditions (dry soils, storm-interstorm simulations, geological heterogeneity, 3 D domains with complex boundary conditions, etc). Richards’ equation can be mathematically formulated and numerically discretized in a variety of manners, and the specific form and scheme chosen will affect the mass balance behavior of the model. We implement and test a promising nested Newton-type algorithm for solving Richards' equation. In the current state of the art, the Picard iteration method is widely used for solving the nonlinear equation governing flow in variably saturated porous media because this method is simple to code and computationally cheap. But the convergence is slow and sometimes fails. On the other hand, the Newton method is more complex and expensive than Picard. As a result the Picard method is more attractive than Newton. However, especially for strongly nonlinear flow problems, the robustness and higher rate of convergence makes Newton an attractive alternative in some cases. In this work the Picard and Newton schemes are compared with results of a nested Newton-type algorithm. Three test cases are presented and each problem is solved over a wide range of vertical discretization. The results highlight the different aspects of the performance of the iterative methods and the different factors that can affect their convergence and efficiency, including problem size, spatial and temporal discretization, convergence error norm, mass lumping, time weighting, conductivity, moisture content characteristics, and boundary conditions. It is suggested that nested Newton-type methods can be effectively implemented and used alongside Picard and Newton and numerical models of Richards' equation. In the final part of the thesis, we studied the performance of a lookup table option, as an alternative to analytical calculation, for evaluating the nonlinear soil characteristics needed in the Picard and Newton schemes. This assessment is conducted for the CATHY (CATchment Hydrology) 3 D Richards-based subsurface flow solver. The lookup table method can be a cost-effective alternative to analytical evaluation in the case of heterogeneous soils, but it has not been examined in detail in the hydrological modeling literature. Two layered soil test problems are considered, and the robustness and accuracy of the lookup table method are assessed for uniform and nonuniform distributions of <b>lookup</b> <b>points</b> in the soil retention curve...|$|R

