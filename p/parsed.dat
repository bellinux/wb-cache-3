2741|10000|Public
5|$|Barthel (1971) {{claimed to}} have <b>parsed</b> the corpus of glyphs to 120, of which the other 480 in his {{inventory}} are allographs or ligatures. The evidence was never published, but similar counts have been obtained by other scholars, such as Pozdniakov & Pozdniakov (2007).|$|E
5|$|Perl has a Turing-complete grammar because parsing can be {{affected}} by run-time code executed during the compile phase. Therefore, Perl cannot be <b>parsed</b> by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.|$|E
5|$|At compile time, the {{interpreter}} parses Perl code into a syntax tree. At run time, it executes {{the program by}} walking the tree. Text is <b>parsed</b> only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.|$|E
40|$|This paper explores joint {{syntactic}} and semantic <b>parsing</b> of Chinese {{to further}} improve {{the performance of}} both syntactic and semantic <b>parsing,</b> in particular the performance of semantic <b>parsing</b> (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated <b>parsing</b> approach is proposed to integrate semantic <b>parsing</b> into the syntactic <b>parsing</b> process. Secondly, semantic information generated by semantic <b>parsing</b> is incorporated into the syntactic <b>parsing</b> model to better capture semantic information in syntactic <b>parsing.</b> Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated <b>parsing</b> approach outperforms the pipeline <b>parsing</b> approach on n-best <b>parse</b> trees, {{a natural extension of}} the widely used pipeline <b>parsing</b> approach on the top-best <b>parse</b> tree. Moreover, it shows that incorporating semantic role-related information into the syntactic <b>parsing</b> model significantly improves the performance of both syntactic <b>parsing</b> and semantic <b>parsing.</b> To our best knowledge, this is the first research on exploring syntactic <b>parsing</b> and semantic role labeling for both verbal and nominal predicates in an integrated way. ...|$|R
5000|$|A <b>Parse</b> Thicket is a graph that {{represents}} the syntactic {{structure of a paragraph}} of text in natural language processing. A <b>Parse</b> Thicket includes <b>Parse</b> tree for each sentence for this paragraph plus some arcs for other relations between words other than syntactic. <b>Parse</b> thickets can be constructed for both constituency <b>parse</b> trees and dependency <b>parse</b> trees. The relations which link <b>parse</b> trees within a <b>Parse</b> Thicket are: ...|$|R
40|$|We {{discuss the}} {{computation}} of <b>parse</b> forests, i. e., compact representations of all syntax trees {{for a given}} sentence. Conventional <b>parsing</b> algorithms can be adapted in {{a simple way to}} produce <b>parse</b> forests. Recently, the subject of <b>parse</b> forests was brought up in the context of generalized LR(O) <b>parsing,</b> a new <b>parsing</b> method for general context-free grammars. It is widely held that this <b>parsing</b> method is particularly suited to producing <b>parse</b> forests. The contrary is true: generalized LR(O) parsers are poor tools for producing compact <b>parse</b> forests...|$|R
5|$|The CIA {{finished}} processing {{documents on}} September 28, 1954. By this point, the agents had <b>parsed</b> {{through more than}} 500,000 unique documents. 750 photographs of this material were published {{for the use of}} the media, 50,000 documents were microfilmed, and photostatic copies were taken of 2095 important documents. Copies of a handful of important documents were distributed to the varies agencies that had been a part of PBHistory, as well as the US Federal Bureau of Investigation.|$|E
25|$|Like {{with some}} {{programming}} languages Lojban grammar can be <b>parsed</b> using parsing expression grammars.|$|E
25|$|The {{efficiency}} of context-free grammar parsing {{is determined by}} the automaton that accepts it. Deterministic context-free grammars are accepted by deterministic pushdown automata and can be <b>parsed</b> in linear time, for example by the LR parser. This is a subset of the context-free grammars which are accepted by the pushdown automaton and can be <b>parsed</b> in polynomial time, for example by the CYK algorithm. Unambiguous context-free grammars can be nondeterministic.|$|E
50|$|LR <b>parsing</b> extends LL <b>parsing</b> {{to support}} a larger range of grammars; in turn, {{generalized}} LR <b>parsing</b> extends LR <b>parsing</b> {{to support a}}rbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL <b>parsing</b> and LR <b>parsing,</b> respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR <b>parsing</b> {{was developed in the}} 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR <b>parsing</b> up to the present day.|$|R
40|$|This paper {{proposes a}} new technique, a node-driven <b>parse</b> pruning technique, in pruning the less {{probable}} <b>parses</b> for GLR <b>parsing</b> algorithm. Without decreasing the eciency of GLR <b>parsing,</b> this technique estimates {{the number of}} <b>parses</b> in the GSS (graph-structured stack) {{based on the number}} of expanded nodes during the <b>parse</b> process. We show the evaluation results of various beam width settings for pruning, and compare the <b>parse</b> time and space consumption against full <b>parsing</b> results. Our node-driven <b>parse</b> pruning algorithm allows pruning in a left-to-right manner without modifying the GSS. KEY WORDS | Probabilistic GLR <b>parsing,</b> <b>parse</b> pruning, GSS, beam search. |, GLR. GSS, ".. GSS. - | GLR,, GSS,. 1 Introduction Pruning is an essential paradigm to reduce the search space in <b>parsing.</b> The idea of pruning is to exclude hypotheses from further investigation if the <b>parses</b> turn out to be unlikely, based on evaluation of partial data. The eciency of pruning technique [...] ...|$|R
2500|$|LR <b>parsing</b> extends LL <b>parsing</b> {{to support}} a larger range of grammars; in turn, {{generalized}} LR <b>parsing</b> extends LR <b>parsing</b> {{to support a}}rbitrary context-free grammars. [...] On LL grammars and LR grammars, it essentially performs LL <b>parsing</b> and LR <b>parsing,</b> respectively, while on nondeterministic grammars, it is as efficient as can be expected. [...] Although GLR <b>parsing</b> {{was developed in the}} 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR <b>parsing</b> up to the present day.|$|R
25|$|Distance-hereditary graphs can be recognized, and <b>parsed</b> into a {{sequence}} of pendant vertex and twin operations, in linear time.|$|E
25|$|A {{large body}} of {{scientific}} texts (SGD, OMIM, FlyBase, PubMed) are <b>parsed</b> to search for statistically relevant co-occurrences of gene names.|$|E
25|$|In Gnomish (another of Tolkien's invented languages) Balrog is <b>parsed</b> as balc 'cruel' + graug 'demon', with a Quenya {{equivalent}} Malkarauke. Variant {{forms of}} the latter include Nalkarauke and Valkarauke.|$|E
40|$|This paper {{presents}} {{our work}} {{for participation in}} the 2012 CIPS-SIGHAN shared task of Traditional Chinese <b>Parsing.</b> We have adopted two multilingual <b>parsing</b> models – a factored model (Stanford <b>Parser)</b> and an unlexicalized model (Berkeley <b>Parser)</b> for <b>parsing</b> the Sinica Treebank. This paper also proposes a new Chinese unknown word model and integrates it into the Berkeley <b>Parser.</b> Our experiment gives the first result of adapting existing multilingual <b>parsing</b> models to the Sinica Treebank and shows that the <b>parsing</b> accuracy can be improved by our suggested approach...|$|R
40|$|This paper {{proposes a}} method for {{evaluating}} the validity of partial <b>parse</b> trees constructed in incremental <b>parsing.</b> Our method is based on stochastic incremental <b>parsing,</b> and it incrementally evaluates the validity for each partial <b>parse</b> tree on a wordby-word basis. In our method, incremental parser returns partial <b>parse</b> trees {{at the point where}} the validity for the partial <b>parse</b> tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental <b>parsing.</b> ...|$|R
50|$|In {{computer}} science, top-down <b>parsing</b> is a <b>parsing</b> strategy {{where one}} first {{looks at the}} highest level of the <b>parse</b> tree and works down the <b>parse</b> tree by using the rewriting rules of a formal grammar. LL parsers are a type of parser that uses a top-down <b>parsing</b> strategy.|$|R
25|$|DCFLs are {{of great}} {{practical}} interest, {{as they can}} be <b>parsed</b> in linear time, and various restricted forms of DCFGs admit simple practical parsers. They are thus widely used throughout computer science.|$|E
25|$|The {{history of}} Yi {{diplomacy}} can be <b>parsed</b> in four parts: (a) before the Japanese invasions in 1592–1598; (b) {{in the context}} of the invasion; (c) after the invasion; and (d) in modern times.|$|E
25|$|This {{structure}} {{allows the}} file to be <b>parsed</b> {{even if not}} all parts are understood. A GIF marked 87a may contain extension blocks; the intent is that a decoder can read and display the file without the features covered in extensions it does not understand.|$|E
40|$|We {{present a}} novel {{framework}} that combines strengths from surface syntactic <b>parsing</b> and deep syntactic <b>parsing</b> to increase deep <b>parsing</b> accuracy, specifically by combining dependency and HPSG <b>parsing.</b> We show that by using surface dependencies to constrain {{the application of}} wide-coverage HPSG rules, we can benefit {{from a number of}} <b>parsing</b> techniques designed for highaccuracy dependency <b>parsing,</b> while actually performing deep syntactic analysis. Our framework results in a 1. 4 % absolute improvement over a state-of-the-art approach for wide coverage HPSG <b>parsing.</b> ...|$|R
40|$|Data-driven parsers rely on {{recommendations}} from <b>parse</b> models, which are generated from {{a set of}} training data using a machine learning classifier, to perform <b>parse</b> operations. However, in some cases a <b>parse</b> model cannot recommend a <b>parse</b> action to a parser unless it learns from the training data what <b>parse</b> action(s) to take in every possible situation. Therefore, {{it will be hard}} for a parser to make an informed decision as to what <b>parse</b> operation to perform when a <b>parse</b> model recommends no/several <b>parse</b> actions to a parser. Here we examine the effect of various deterministic choices on a datadriven parser when it is presented with no/several recommendation from a <b>parse</b> model...|$|R
40|$|We propose an {{algebraic}} {{method for}} the design of tabular <b>parsing</b> algorithms which uses <b>parsing</b> schemata [7]. The <b>parsing</b> strategy is expressed in a tree algebra. A <b>parsing</b> schema is derived from the tree algebra by means of algebraic operations such as homomorphic images, direct products, subalgebras and quotient algebras. The latter yields a tabular interpretation of the <b>parsing</b> strategy. The proposed method allows simpler and more elegant correctness proofs by using general theorems and is not limited to left-right <b>parsing</b> strategies, unlike current automaton-based approaches. Furthermore, it allows to derive <b>parsing</b> schemata for linear indexed grammars (LIG) from <b>parsing</b> schemata for context-free grammars by means of a correctness preserving algebraic transformation. A new bottom-up head corner <b>parsing</b> schema for LIG is constructed to demonstrate the method...|$|R
25|$|While HTML, {{prior to}} HTML5, {{was defined as}} an {{application}} of Standard Generalized Markup Language (SGML), a flexible markup language framework, XHTML is an application of XML, a more restrictive subset of SGML. XHTML documents are well-formed and may therefore be <b>parsed</b> using standard XML parsers, unlike HTML, which requires a lenient HTML-specific parser.|$|E
25|$|Multiple-word {{descriptive}} identifiers with embedded spaces such as {{end of file}} or char table {{cannot be}} used in most programming languages because the spaces between the words would be <b>parsed</b> as delimiters between tokens. The alternative of running the words together as in endoffile or chartable is difficult to understand and possibly misleading; for example, chartable is an English word (able to be charted).|$|E
25|$|Infinitary logic generalizes first-order {{logic to}} allow {{formulas}} of infinite length. The most common {{way in which}} formulas can become infinite is through infinite conjunctions and disjunctions. However, {{it is also possible}} to admit generalized signatures in which function and relation symbols are allowed to have infinite arities, or in which quantifiers can bind infinitely many variables. Because an infinite formula cannot be represented by a finite string, it is necessary to choose some other representation of formulas; the usual representation in this context is a tree. Thus formulas are, essentially, identified with their parse trees, rather than with the strings being <b>parsed.</b>|$|E
30|$|For <b>parsing</b> English phrases, {{currently}} Link Grammar <b>Parser</b> [14] and Stanford <b>Parser</b> [15] (a lexicalized Probabilistic Context-Free Grammar (PCFG)) {{are two of}} {{the best}} semantic parsers. Link Grammar <b>Parser</b> is a rule-based analyzer, which is essential to obtain accurate results. However, a statistical analyzer parser like Stanford <b>Parser,</b> which is written in Java, is more tolerant with both words and constructions, which are not grammatically correct. Even if there are grammatical errors (e.g., “Parents always does loves their childs”.), a <b>parse</b> tree still can be created by the Stanford <b>Parser.</b> For this reason, we used Stanford <b>Parser</b> to analyze grammatical structure of input sentences.|$|R
40|$|We {{investigated}} the performance e#cacy of beam search <b>parsing</b> and deep <b>parsing</b> techniques in probabilistic HPSG <b>parsing</b> using the Penn treebank. We first tested the beam thresholding and iterative <b>parsing</b> developed for PCFG <b>parsing</b> with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid <b>parsing</b> with a CFG chunk parser. The {{contributions of the}} large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total <b>parsing</b> performance. The precision, recall and average <b>parsing</b> time for the Penn treebank (Section 23) were 87. 85 %, 86. 85 %, and 360 ms, respectively...|$|R
40|$|GML data {{is widely}} used for model building, data exchanging, etc. The GML data <b>parsing</b> is the base of {{handling}} other operation of GML. The <b>parsing</b> technology of XML {{can be used for}} GML <b>parsing.</b> But the XML <b>parsing</b> technology is deficient in <b>parsing</b> semantic information on geography information. This paper tries to build a semantic information database (SIDB) of GML and design GML core schema-based <b>parsing</b> engine which based on SIDB. Ultimately actualize GML data <b>parsing.</b> The results of the study are verified by GML test data in the paper. And more, this study provides a new way to <b>parsing</b> semantic information in other fields...|$|R
25|$|The {{languages}} of this class have great practical importance {{in computer science}} {{as they can be}} <b>parsed</b> much more efficiently than nondeterministic context-free languages. The complexity of the program and execution time of a deterministic pushdown automaton is vastly less than that of a nondeterministic one. In the naive implementation, the latter must make copies of the stack every time a nondeterministic step occurs. The best known algorithm to test membership in any context-free language is Valiant's algorithm, taking O(n2.378) time, where n is the length of the string. On the other hand, deterministic context-free languages can be accepted in O(n) time by an LR(k) parser. This is very important for computer language translation because many computer languages belong to this class of languages.|$|E
25|$|A pull parser {{creates an}} {{iterator}} that sequentially visits the various elements, attributes, and data in an XML document. Code that uses this iterator can test the current item (to tell, for example, {{whether it is}} a start-tag or end-tag, or text), and inspect its attributes (local name, namespace, values of XML attributes, value of text, etc.), and can also move the iterator to the next item. The code can thus extract information from the document as it traverses it. The recursive-descent approach tends to lend itself to keeping data as typed local variables in the code doing the parsing, while SAX, for instance, typically requires a parser to manually maintain intermediate data within a stack of elements that are parent elements of the element being <b>parsed.</b> Pull-parsing code can be more straightforward to understand and maintain than SAX parsing code.|$|E
25|$|The {{measurement}} of critical application characteristics involves measuring structural {{attributes of the}} application's architecture, coding, and in-line documentation, as displayed in the picture above. Thus, each characteristic is affected by attributes at numerous levels of abstraction in the application and all of which must be included calculating the characteristic's measure {{if it is to}} be a valuable predictor of quality outcomes that affect the business. The layered approach to calculating characteristic measures displayed in the figure above was first proposed by Boehm and his colleagues at TRW (Boehm, 1978) and is the approach taken in the ISO 9126 and 25000 series standards. These attributes can be measured from the <b>parsed</b> results of a static analysis of the application source code. Even dynamic characteristics of applications such as reliability and performance efficiency have their causal roots in the static structure of the application.|$|E
40|$|We present Pro 3 Gres, a fast robust broad-coverage and deep-linguistic parser {{that has}} been applied to and {{evaluated}} on unrestricted amounts of text from unrestricted domains. We show that it is largely cognitively adequate. We argue that Pro 3 Gres contributes to closing the gap between psycholinguistics and language engineering, between probabilistic <b>parsing</b> and formal grammar-based <b>parsing,</b> between shallow <b>parsing</b> and full <b>parsing,</b> and between deterministic <b>parsing</b> and non-deterministic <b>parsing.</b> We also describe a successful application of Pro 3 Gres for <b>parsing</b> research texts from the BioMedical domain...|$|R
40|$|<b>Parsing</b> schemata are {{high-level}} {{descriptions of}} <b>parsing</b> algorithms. This paper {{is concerned with}} <b>parsing</b> schemata for different grammar formalisms. We separate the description of <b>parsing</b> steps from that of grammatical properties by means of abstract <b>parsing</b> schemata. We define an abstract Earley schema and prove it correct. We obtain Earley schemata for several grammar formalisms by specifying the grammatical properties in the abstract Earley schema. Our approach offers a clear and well-defined interface between a <b>parsing</b> algorithm and a grammar. Moreover, it provides a precise criterion for the classification of <b>parsing</b> algorithms...|$|R
40|$|This paper {{suggests}} {{two ways}} of improving transition-based, non-projective dependency <b>parsing.</b> First, we add a transition to an existing non-projective <b>parsing</b> algorithm, so it can perform either projective or non-projective <b>parsing</b> as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic <b>parses</b> used as features. The new addition to the algorithm shows a clear advantage in <b>parsing</b> speed. The bootstrapping technique gives a significant improvement to <b>parsing</b> accuracy, showing near state-of-theart performance with respect to other <b>parsing</b> approaches evaluated on the same data set. ...|$|R
