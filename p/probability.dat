10000|10000|Public
5|$|In 1933 Andrei Kolmogorov {{published}} in German {{his book on}} the foundations of <b>probability</b> theory titled Grundbegriffe der Wahrscheinlichkeitsrechnung, where Kolmogorov used measure theory to develop an axiomatic framework for <b>probability</b> theory. The publication of this book is now widely {{considered to be the}} birth of modern <b>probability</b> theory, when the theories of <b>probability</b> and stochastic processes became parts of mathematics.|$|E
5|$|Radiocarbon dating of {{the human}} remains {{suggested}} that some {{were brought to the}} site between either 3980–3800 calibrated BCE (95% <b>probability)</b> or 3960–3880 cal BCE (68% <b>probability).</b> It further suggested that after an interval of either 60–350 years (95% <b>probability)</b> or 140–290 years (68% <b>probability),</b> additional depositions of human remains were made inside the tomb. This second phase probably began in 3730–3540 cal BCE (95% <b>probability)</b> or 3670–3560 cal BCE (68% <b>probability).</b>|$|E
5|$|The {{basic rules}} of <b>probability</b> amplitudes {{that will be}} used are that a) if an event can happen in a variety of {{different}} ways then its <b>probability</b> amplitude is the sum of the <b>probability</b> amplitudes of the possible ways and b) if a process involves a number of independent sub-processes then its <b>probability</b> amplitude is the product of the component <b>probability</b> amplitudes.|$|E
5000|$|Negative <b>probabilities</b> {{have more}} {{recently}} {{been applied to}} mathematical finance. In quantitative finance most <b>probabilities</b> are not real <b>probabilities</b> but pseudo <b>probabilities,</b> often {{what is known as}} risk neutral <b>probabilities.</b> These are not real <b>probabilities,</b> but theoretical [...] "probabilities" [...] under a series of assumptions that helps simplify calculations by allowing such pseudo <b>probabilities</b> to be negative in certain cases as first pointed out by Espen Gaarder Haug in 2004.|$|R
50|$|Estimates {{of prior}} <b>probabilities</b> are {{particularly}} suspect. Estimates will be constructed {{that do not}} follow any consistent frequency distribution. For this reason prior <b>probabilities</b> are considered as estimates of <b>probabilities</b> rather than <b>probabilities.</b>|$|R
40|$|According to a {{conjecture}} of Hájek (1981), for successive sampling {{the variability}} of the inclusion <b>probabilities</b> is smaller than {{the variability of}} the drawing <b>probabilities,</b> and it exceeds the variability of the inclusion <b>probabilities</b> of rejective sampling. A stronger conjecture is motivated and partly proved. Inclusion <b>probabilities</b> drawing <b>probabilities</b> rejective sampling successive sampling...|$|R
5|$|Assuming {{genetic drift}} {{is the only}} {{evolutionary}} force acting on an allele, {{at any given time}} the <b>probability</b> that an allele will eventually become fixed in the population is simply its frequency in the population at that time. For example, if the frequency p for allele A is 75% and the frequency q for allele B is 25%, then given unlimited time the <b>probability</b> A will ultimately become fixed in the population is 75% and the <b>probability</b> that B will become fixed is 25%.|$|E
5|$|For random {{walks in}} -dimensional integer lattices, George Pólya {{published}} in 1919 and 1921 work, {{where he studied}} the <b>probability</b> of a symmetric random walk returning to a previous position in the lattice. Pólya showed that a symmetric random walk, which has an equal <b>probability</b> to advance in any direction in the lattice, will return to a previous position in the lattice {{an infinite number of}} times with <b>probability</b> one in one and two dimensions, but with <b>probability</b> zero in three or higher dimensions.|$|E
5|$|Quantum {{mechanics}} introduces {{an important}} {{change in the}} way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square of <b>probability</b> amplitudes. <b>Probability</b> amplitudes are complex numbers.|$|E
5000|$|The {{choice of}} <b>probabilities</b> {{for which the}} {{randomized}} Kaczmarz algorithm was originally formulated and analyzed (<b>probabilities</b> proportional to the squares of the row norms) is not optimal. Optimal <b>probabilities</b> are the solution of a certain semidefinite program. The theoretical complexity of randomized Kaczmarz with the optimal <b>probabilities</b> can be arbitrarily better than the complexity for the standard <b>probabilities.</b> However, the amount by which it is better depends on the matrix [...] There are problems for which the standard <b>probabilities</b> are optimal.|$|R
40|$|We {{investigate}} the first-crossing-time problem through unit-slope straight lines for a two-dimensional random walk whose single-step <b>probabilities</b> are symmetrically related. The transition <b>probabilities</b> conditioned by non-absorbtion at unit-slope straight {{lines and the}} first-crossing <b>probabilities</b> through such boundaries are expressed in term of the transition <b>probabilities</b> {{in the absence of}} boundaries. The <b>probabilities</b> of ultimate crossing are also given. An application to population models is finally indicated...|$|R
3000|$|The above <b>probabilities</b> {{are valid}} for our system and 802.11 DCF; however, their {{meanings}} {{are different in}} each MAC. For DCF, these <b>probabilities</b> describe the transition <b>probabilities</b> between successful collision and idle states. Whereas, in our system they only describe the <b>probabilities</b> {{that each of the}} [...]...|$|R
5|$|Random {{matrices}} are matrices whose {{entries are}} random numbers, subject to suitable <b>probability</b> distributions, such as matrix normal distribution. Beyond <b>probability</b> theory, they are applied in domains ranging from number theory to physics.|$|E
5|$|Risk is a {{combination}} of hazard, vulnerability and likelihood of occurrence, which can be the <b>probability</b> of a specific undesirable consequence of a hazard, or the combined <b>probability</b> of undesirable consequences of all the hazards of an activity.|$|E
5|$|The {{inner product}} between two state vectors {{is a complex}} number known as a <b>probability</b> amplitude. During an ideal {{measurement}} of a quantum mechanical system, the <b>probability</b> that a system collapses from a given initial state to a particular eigenstate is given by {{the square of the}} absolute value of the <b>probability</b> amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The <b>probability</b> distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.|$|E
50|$|The {{interplay of}} overweighting of small <b>probabilities</b> and concavity-convexity {{of the value}} {{function}} leads to the so-called fourfold pattern of risk attitudes: risk-averse behavior when gains have moderate <b>probabilities</b> or losses have small probabilities; risk-seeking behavior when losses have moderate <b>probabilities</b> or gains have small <b>probabilities.</b>|$|R
5000|$|The {{logarithm}} {{function is}} not defined for zero, so log <b>probabilities</b> can only represent non-zero <b>probabilities.</b> Since the logarithm {{of a number}} in [...] interval is negative, often the negative log <b>probabilities</b> are used. In that case the log <b>probabilities</b> in the following formulas would be inverted.|$|R
40|$|Quantum {{computation}} {{deals with}} projective measurements and unitary transformations in finite dimensional Hilbert spaces. The paper presents a propositional logic designed to describe quantum computation at an operational level by supporting reasoning about the <b>probabilities</b> associated to such measurements: measurement <b>probabilities,</b> and transition <b>probabilities</b> (a quantum analogue of conditional <b>probabilities).</b> We present two axiomatizations, {{one for the}} logic {{as a whole and}} one for the fragment dealing just with measurement <b>probabilities...</b>|$|R
25|$|In <b>probability</b> theory, a {{standard}} <b>probability</b> space, also called LebesgueRokhlin <b>probability</b> space or just Lebesgue space (the latter term is ambiguous) is a <b>probability</b> space satisfying certain assumptions introduced by Vladimir Rokhlin in 1940. Informally, it is a <b>probability</b> space consisting of an interval and/or a finite or countable number of atoms.|$|E
25|$|Most {{introductions}} to <b>probability</b> theory treat discrete <b>probability</b> distributions {{and continuous}} <b>probability</b> distributions separately. The more mathematically advanced measure theory-based treatment of <b>probability</b> covers the discrete, continuous, {{a mix of}} the two, and more.|$|E
25|$|<b>Probability</b> density, <b>Probability</b> density function, p.d.f., Continuous <b>probability</b> {{distribution}} function: {{most often}} reserved for continuous random variables.|$|E
5000|$|... #Subtitle level 2: Conditional <b>probabilities</b> and <b>probabilities</b> of conditionals ...|$|R
30|$|At this step, OOV <b>probabilities</b> are {{interpolated}} by {{backing off}} to unknown word <b>probabilities.</b> Unknown word <b>probabilities</b> are estimated classically. We tag as unk {{all the words}} in the training set that are out of the recognition vocabulary. Then, unk is viewed as a word and its linguistic <b>probabilities</b> are classically estimated.|$|R
30|$|SPA is a soft {{decision}} algorithm that calculates the a priori <b>probabilities</b> of the received code bits {{and uses a}} posteriori <b>probabilities</b> for decoding operation. These <b>probabilities</b> are known as log-likelihood ratios.|$|R
25|$|<b>Probability</b> {{theory is}} {{the branch of}} {{mathematics}} concerned with <b>probability.</b> Although there are several different <b>probability</b> interpretations, <b>probability</b> theory treats the concept in a rigorous mathematical manner by expressing it {{through a set of}} axioms. Typically these axioms formalise <b>probability</b> in terms of a <b>probability</b> space, which assigns a measure taking values between 0 and 1, termed the <b>probability</b> measure, to a set of outcomes called the sample space. Any specified subset of these outcomes is called an event.|$|E
25|$|To sum up this formula: the {{posterior}} <b>probability</b> of the hypothesis {{is equal to}} the prior <b>probability</b> of the hypothesis multiplied by the conditional <b>probability</b> of the evidence given the hypothesis, divided by the <b>probability</b> of the new evidence.|$|E
25|$|Continuous <b>probability</b> {{distributions}} can {{be described}} in several ways. The <b>probability</b> density function describes the infinitesimal <b>probability</b> of any given value, and the <b>probability</b> that the outcome lies in a given interval can be computed by integrating the <b>probability</b> density function over that interval. On the other hand, the cumulative distribution function describes the <b>probability</b> that the random variable is no larger than a given value; the <b>probability</b> that the outcome lies in a given interval can be computed by taking the difference between the values of the cumulative distribution function at the endpoints of the interval. The cumulative distribution function is the antiderivative of the <b>probability</b> density function provided that the latter function exists.|$|E
40|$|This paper {{proposes to}} use implied <b>probabilities</b> {{resulting}} from estimation methods based on uncon-ditional moment conditions to detect structural change. The class of GEL estimators (Smith 1997) assigns {{a set of}} <b>probabilities</b> to each observation such that moment conditions are satisfied. These restricted <b>probabilities</b> are called implied <b>probabilities.</b> Implied <b>probabilities</b> may also constructed for the standard GMM (Back and Brown 1993). The proposed structural change statistic tests {{are based on the}} information content in these implied <b>probabilities.</b> We consider cases of structural change which can occur in the parameters of interest or in the overidentifying restrictions used to estimate these parameters...|$|R
5000|$|If {{the prior}} <b>probabilities</b> {{are all the}} same the <b>probabilities</b> are, ...|$|R
40|$|In {{this paper}} we derive {{a formula for}} zero-avoiding {{transition}} <b>probabilities</b> of an r-node tandem queue with exponential servers and deterministic input. In particular, we show that these transition <b>probabilities</b> may be interpreted as non-coincidence <b>probabilities</b> {{of a set of}} dissimilar Poisson processes restricted by a time-dependent boundary. Tandem queues Non-coincidence <b>probabilities</b> Poisson process...|$|R
25|$|The <b>probability</b> that A will occur, {{given that}} B has occurred, is the <b>probability</b> of A and B {{occurring}} {{divided by the}} <b>probability</b> ofB.|$|E
25|$|There {{have been}} at least two {{successful}} attempts to formalize <b>probability,</b> namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see <b>probability</b> space), sets are interpreted as events and <b>probability</b> itself as a measure on a class of sets. In Cox's theorem, <b>probability</b> is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of <b>probability</b> values to propositions. In both cases, the laws of <b>probability</b> are the same, except for technical details.|$|E
25|$|If it ate grapes today, {{tomorrow}} it {{will eat}} grapes with <b>probability</b> 1/10, cheese with <b>probability</b> 4/10 and lettuce with <b>probability</b> 5/10.|$|E
40|$|Actions should {{maximize}} expected utility. For any {{proposed action}} and given utility function, inferred <b>probabilities</b> are those <b>probabilities</b> {{that support the}} proposed action. Given that the utilities are smooth functions of the unknown state of nature, inferred <b>probabilities</b> may be correspondingly rough. Some classes of inferred <b>probabilities</b> are developed for general utility functions. A detailed study is made of inferred <b>probabilities</b> when the proposed action is the maximum likelihood estimator in the normal location problem with squared error loss...|$|R
40|$|This paper {{presents}} an experiment which investigates whether asset prices are affected in markets where state <b>probabilities</b> {{are not exactly}} known and traders have to form subjective <b>probabilities</b> of payoffs. Results show {{that the presence of}} vague <b>probabilities</b> leads to higher average prices with respect to assets characterised by known <b>probabilities.</b> However, prices under known and vague <b>probabilities</b> draw closer when traders get a sounder understanding of how to arbitrate between markets. markets, risk, uncertainty, prices, arbitrage,...|$|R
50|$|For {{different}} documents, we {{can build}} their own unigram models, with different hitting <b>probabilities</b> of words in it. And we use <b>probabilities</b> from different documents to generate different hitting <b>probabilities</b> for a query. Then we can rank documents for a query according to the generating <b>probabilities.</b> Next {{is an example of}} two unigram models of two documents.|$|R
