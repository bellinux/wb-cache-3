621|10000|Public
25|$|SAS System {{includes}} univariate <b>probability</b> <b>mass</b> <b>function</b> {{and distribution}} function.|$|E
25|$|This {{equation}} is the <b>probability</b> <b>mass</b> <b>function</b> (PMF) for a Poisson distribution.|$|E
25|$|The <b>probability</b> <b>mass</b> <b>function</b> can be {{calculated}} by various Taylor expansion methods or by numerical integration (Fog, 2008).|$|E
2500|$|The Kullback–Leibler {{divergence}} [...] is convex in {{the pair}} of <b>probability</b> <b>mass</b> <b>functions</b> , i.e. if [...] and [...] are two pairs of <b>probability</b> <b>mass</b> <b>functions,</b> then ...|$|R
2500|$|It {{is common}} for {{probability}} density <b>functions</b> (and <b>probability</b> <b>mass</b> <b>functions)</b> to ...|$|R
50|$|Probability density <b>functions</b> (pdfs) and <b>probability</b> <b>mass</b> <b>functions</b> are {{denoted by}} lower case letters, e.g. f(x).|$|R
25|$|Closed form {{expressions}} for the <b>probability</b> <b>mass</b> <b>function</b> exist (Lyons, 1980), {{but they}} are not very useful for practical calculations because of extreme numerical instability, except in degenerate cases.|$|E
25|$|This {{recursive}} dependency {{gives rise}} to a difference equation with a solution that is given in open form by the integral in the expression of the <b>probability</b> <b>mass</b> <b>function</b> in the table above.|$|E
25|$|Mode: for a {{discrete}} random variable, the value with highest probability (the location {{at which the}} <b>probability</b> <b>mass</b> <b>function</b> has its peak); for a continuous random variable, a location at which the probability density function has a local peak.|$|E
3000|$|... (x) be two cdfs {{with either}} {{probability}} density <b>functions</b> (pdfs) or <b>probability</b> <b>mass</b> <b>functions</b> (pmfs) f [...]...|$|R
50|$|In statistics, the {{probability}} distributions of discrete variables {{can be expressed}} in terms of <b>probability</b> <b>mass</b> <b>functions.</b>|$|R
3000|$|... to all be {{discrete}} and finite alphabets, and {{all probability}} distributions {{are to be}} interpreted as <b>probability</b> <b>mass</b> <b>functions.</b>|$|R
25|$|The mode is not {{necessarily}} unique to a given discrete distribution, since the <b>probability</b> <b>mass</b> <b>function</b> may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently.|$|E
25|$|In the {{continuous}} univariate case above, the reference measure is the Lebesgue measure. The <b>probability</b> <b>mass</b> <b>function</b> of a {{discrete random variable}} is the density {{with respect to the}} counting measure over the sample space (usually the set of integers, or some subset thereof).|$|E
25|$|To {{understand}} the above {{definition of the}} <b>probability</b> <b>mass</b> <b>function,</b> note that the probability for every specific sequence of knbsp&successes and rnbsp&failures is , because the outcomes of the knbsp&+nbsp&r trials are supposed to happen independently. Since the rthnbsp&failure comes last, it remains to choose the knbsp&trials with successes out of the remaining knbsp&+nbsp&rnbsp&−nbsp&1 trials. The above binomial coefficient, due to its combinatorial interpretation, gives precisely the number of all these sequences of length knbsp&+nbsp&rnbsp&−nbsp&1.|$|E
3000|$|Inequalities (39) and (40) are proved analogously to Corollary  8 if {{we observe}} the <b>probability</b> <b>mass</b> <b>functions</b> p_i and q_i as Zipf laws defined by (5). □ [...]...|$|R
50|$|In {{statistics}} and machine learning, discretization {{refers to the}} process of converting continuous features or variables to discretized or nominal features. This can be useful when creating <b>probability</b> <b>mass</b> <b>functions.</b>|$|R
2500|$|An {{implementation}} for the R {{programming language}} is available as the package named [...] Includes univariate and multivariate <b>probability</b> <b>mass</b> <b>functions,</b> distribution functions, quantiles, random variable generating functions, mean and variance.|$|R
25|$|A random {{variable}} has a probability distribution, which specifies {{the probability that}} its value falls in any given interval. Random variables can be discrete, that is, taking any of a specified finite or countable list of values, endowed with a <b>probability</b> <b>mass</b> <b>function</b> characteristic of the {{random variable}}'s probability distribution; or continuous, taking any numerical value in an interval or collection of intervals, via a probability density function that is characteristic of the random variable's probability distribution; or a mixture of both types. Two random variables with the same probability distribution can still differ {{in terms of their}} associations with, or independence from, other random variables. The realizations of a random variable, that is, the results of randomly choosing values according to the variable's probability distribution function, are called random variates.|$|E
25|$|Probability {{distributions}} {{are generally}} {{divided into two}} classes. A discrete probability distribution (applicable to the scenarios where the set of possible outcomes is discrete, such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a <b>probability</b> <b>mass</b> <b>function.</b> On the other hand, a continuous probability distribution (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by probability density functions (with the probability of any individual outcome actually being 0). The normal distribution is a commonly encountered continuous probability distribution. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.|$|E
2500|$|... may be {{considered}} to represent a discrete <b>probability</b> <b>mass</b> <b>function</b> of , with an associated <b>probability</b> <b>mass</b> <b>function</b> constructed from the transformed variable, ...|$|E
5000|$|Among many applications, we {{consider}} here one of broad {{theoretical and practical}} importance. Given a parameterized family of probability density <b>functions</b> (or <b>probability</b> <b>mass</b> <b>functions</b> {{in the case of}} discrete distributions) ...|$|R
5000|$|An {{implementation}} for the R {{programming language}} is available as the package named BiasedUrn. Includes univariate and multivariate <b>probability</b> <b>mass</b> <b>functions,</b> distribution functions, quantiles, random variable generating functions, mean and variance.|$|R
2500|$|... {{length of}} , {{in the model}} class of computable <b>probability</b> <b>mass</b> <b>functions</b> of given maximal Kolmogorov {{complexity}} , the complexity of P upper bounded by , is given by the [...] MDL function or constrained MDL estimator: ...|$|R
2500|$|Under this {{parametrization}} the <b>probability</b> <b>mass</b> <b>function</b> will be ...|$|E
2500|$|The <b>probability</b> <b>mass</b> <b>function</b> of the {{negative}} binomial distribution is ...|$|E
2500|$|The entropy [...] is concave in the <b>probability</b> <b>mass</b> <b>function</b> , i.e.|$|E
5000|$|One-parameter {{exponential}} {{families have}} monotone likelihood-functions. In particular, the one-dimensional exponential family of probability density <b>functions</b> or <b>probability</b> <b>mass</b> <b>functions</b> withhas a monotone non-decreasing likelihood ratio in the sufficient statistic T(x), provided that [...] is non-decreasing.|$|R
3000|$|... determines which road segment {{the target}} will {{follow in the}} next {{sampling}} interval in case more than one alternative exists. We assume the availability of prior probability density <b>functions</b> (or <b>probability</b> <b>mass</b> <b>functions</b> in the discrete case) [...]...|$|R
40|$|An equational axiomatisation of {{probability}} functions for one-dimensional event {{spaces in the}} language of signed meadows is expanded with conditional values and configurations. Assuming the presence of a probability function, equational axioms are provided for expectation value, variance, covariance, and correlation squared, each for conditional values, and for expected utility of configurations. Finite support summation is introduced as a binding operator on meadows which simplifies formulating requirements on <b>probability</b> <b>mass</b> <b>functions</b> with finite support. Conditional values are related to <b>probability</b> <b>mass</b> <b>functions</b> and to random variables. The definitions are reconsidered in a higher dimensional setting. Comment: Modified notion for conditional probability operators is introduced and used. The Paragraph concerning two versions of Bayes' rule has been revised accordingl...|$|R
2500|$|The <b>probability</b> <b>mass</b> <b>function</b> {{satisfies}} the following recurrence relation, for every : ...|$|E
2500|$|... {{hence the}} terms of the <b>probability</b> <b>mass</b> <b>function</b> indeed add up to one.|$|E
2500|$|... and (if {{the dice}} are fair) has a <b>probability</b> <b>mass</b> <b>function</b> ƒ'X given by: ...|$|E
5000|$|This is the {{conditional}} <b>probability</b> <b>mass</b> distribution <b>function</b> of [...]|$|R
40|$|Credal {{networks}} [1, 2] generalize Bayesian networks [3] by {{associating with}} variables (closed convex) sets of conditional <b>probability</b> <b>mass</b> <b>functions,</b> i. e., credal sets 1, {{in place of}} precise conditional probability distributions. Credal networks are models of imprecise probabilities [4], which allow the capturin...|$|R
40|$|Codes {{based on}} SUDOKU puzzles are discussed, and belief {{propagation}} decoding introduced for the erasure channel. Despite the non-linearity {{of the code}} constraints, {{it is argued that}} density evolution can be used to analyse code performance due to the invariance of the code under alphabet permutation. The belief propagation decoder for erasure channels operates by exchanging messages containing sets of possible values. Accordingly, density evolution tracks the <b>probability</b> <b>mass</b> <b>functions</b> of the set cardinalities. The equations governing the mapping of those <b>probability</b> <b>mass</b> <b>functions</b> are derived and calculated for variable and constraint nodes, and decoding thresholds are computed for long SUDOKU codes with random interleavers. Comment: 5 pages, accepted for publication at the 8 th International Symposium on Turbo Codes and Iterative Processing, August 201...|$|R
