1355|295|Public
5|$|In neuroscience, a {{great deal}} of effort has gone into {{investigating}} how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world—Gerald Edelman expressed this point vividly by titling one of his books about consciousness The Remembered Present. In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are <b>probabilistic</b> <b>inference</b> models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.|$|E
25|$|In 1990 {{while working}} at Stanford University on large bioinformatic applications, Greg Cooper proved that exact {{inference}} in Bayesian networks is NP-hard. This result prompted a surge in research on approximation algorithms {{with the aim of}} developing a tractable approximation to <b>probabilistic</b> <b>inference.</b> In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of <b>probabilistic</b> <b>inference</b> in Bayesian networks. First, they proved that there is no tractable deterministic algorithm that can approximate <b>probabilistic</b> <b>inference</b> to within an absolute error ɛ< 1/2. Second, they proved that there is no tractable randomized algorithm that can approximate <b>probabilistic</b> <b>inference</b> to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.|$|E
25|$|In machine learning, tree decompositions {{are also}} called {{junction}} trees, clique trees, or join trees; they {{play an important}} role in problems like <b>probabilistic</b> <b>inference,</b> constraint satisfaction, query optimization, and matrix decomposition.|$|E
30|$|By {{learning}} with csGM-LDA, a unified framework is introduced to infer the hierarchies of multiple modalities and predict tags {{for a new}} image. Benefiting from the exploration of hierarchical <b>probabilistic</b> <b>inferences,</b> the unified framework can be effectively conducted.|$|R
30|$|We {{applied the}} {{multiple}} change point model following signals of the c-chart as the Poisson EWMA and CUSUM mostly signalled before simulating the second {{change in the}} process. Although results here are limited to the replication study, distribution of parameters and <b>probabilistic</b> <b>inferences</b> can easily be constructed.|$|R
40|$|Linear {{response}} formulas for {{the generalized}} belief propagation in approximate inference are derived {{by using a}} cluster variation method. The linear response formulas can give us the marginal probability between every pair of nodes and we obtain good accuracy in some examples of practical <b>probabilistic</b> <b>inferences.</b> ...|$|R
25|$|In {{practical}} terms, these complexity results {{suggested that}} while Bayesian networks were rich representations for AI and machine learning applications, {{their use in}} large real-world applications {{would need to be}} tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm was the first provable fast approximation algorithm to efficiently approximate <b>probabilistic</b> <b>inference</b> in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.|$|E
25|$|Because a Bayesian {{network is}} a {{complete}} model for the variables and their relationships, {{it can be used}} to answer probabilistic queries about them. For example, the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called <b>probabilistic</b> <b>inference.</b> The posterior gives a universal sufficient statistic for detection applications, when one wants to choose values for the variable subset which minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.|$|E
25|$|Quantum {{annealing}} is not {{the only}} technology for sampling. In a prepare and measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in <b>probabilistic</b> <b>inference,</b> and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.|$|E
40|$|Oaksford, Chater, and Larkin (2000) have {{suggested}} that people actually use everyday probabilistic reasoning when making deductive inferences. In two studies, we explicitly compared probabilistic and deductive reasoning with identicalif-then conditional premises with concrete content. In the first, adults were given causal premises with one strongly associated antecedent {{and were asked to}} make standard deductive inferences or to judge the probabilities of conclusions. In the second, reasoners were given scenarios presenting a causal relation with zero to three potential alternative antecedents. The participants responded to each set of problems under both deductive and probabilistic instructions. The results show that deductive and <b>probabilistic</b> <b>inferences</b> are not isomorphic. <b>Probabilistic</b> <b>inferences</b> can model deductive responses only using a limited, very high threshold model, which is equivalent to a simple retrieval model. These results provide a clearer understanding of the relations between <b>probabilistic</b> and deductive <b>inferences</b> and the limitations of trying to consider these two forms of inference as having a single underlying process. 9 page(s...|$|R
5000|$|Edward: A {{library for}} <b>probabilistic</b> modeling, <b>inference,</b> and criticism.|$|R
5000|$|Edward is a Python {{library for}} <b>probabilistic</b> modeling, <b>inference,</b> and criticism.|$|R
50|$|In 1990 {{while working}} at Stanford University on large bioinformatic applications, Greg Cooper proved that exact {{inference}} in Bayesian networks is NP-hard. This result prompted a surge in research on approximation algorithms {{with the aim of}} developing a tractable approximation to <b>probabilistic</b> <b>inference.</b> In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of <b>probabilistic</b> <b>inference</b> in Bayesian networks. First, they proved that there is no tractable deterministic algorithm that can approximate <b>probabilistic</b> <b>inference</b> to within an absolute error ɛ< 1/2. Second, they proved that there is no tractable randomized algorithm that can approximate <b>probabilistic</b> <b>inference</b> to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.|$|E
5000|$|... #Subtitle level 2: Example 2: {{parameter}} estimation and <b>probabilistic</b> <b>inference</b> ...|$|E
50|$|Deep neural {{networks}} are generally interpreted {{in terms of}} the universal approximation theorem or <b>probabilistic</b> <b>inference.</b>|$|E
5000|$|First, RCT in {{some cases}} poses demands on {{cognitive}} abilities that humans cannot satisfy. Many real-world problems are computationally intractable - for example, making <b>probabilistic</b> <b>inferences</b> using Bayesian belief networks is NP-hard. Many theorists agree that accounts of rationality must not require [...] "... capacities, abilities, and skills far beyond those possessed by human beings as they now are." ...|$|R
40|$|Bayesian {{networks}} (BNs) {{have been}} widely used as a model for knowledge representation and <b>probabilistic</b> <b>inferences.</b> However, the single probability representation of conditional dependencies has been proven to be overconstrained in realistic applications. Many efforts have proposed to represent the dependencies using probability intervals instead of single probabilities. In this paper, we move one step further and adopt a probability distribution schema. This results in a higher order representation of uncertainties in a BN. We formulate <b>probabilistic</b> <b>inferences</b> in this context and then propose a mean/covariance propagation algorithm based on the well-known junction tree propagation for standard BNs [1]. For algorithm validation, we develop a two-layered Markov likelihood weighting approach that handles high-order uncertainties and provides “ground-truth ” solutions to inferences, albeit very slowly. Our experiments show that the mean/covariance propagation algorithm can efficiently produce high-quality solutions that compare favorably to results obtained through painstaking sampling. ...|$|R
40|$|We {{propose a}} Bayesian {{approach}} to reasoning under uncertainty in on-line auditing of Statistical Databases. A Bayesian network addresses disclosures based on <b>probabilistic</b> <b>inferences</b> {{that can be}} drawn from released data. In particular, we deal with on-line max and min auditing. Moreover, we show how our model is able to deal with the implicit delivery of information that derives from denying the answer to a query and to manage user prior-knowledge...|$|R
5000|$|Offload computation: Robots can use {{the vast}} {{computational}} infrastructure available on the web for computationally heavy tasks including planning, <b>probabilistic</b> <b>inference,</b> and mapping, among many others.|$|E
50|$|Ongoing {{research}} on mental models and reasoning {{has led the}} theory to be extended to account for <b>probabilistic</b> <b>inference</b> (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).|$|E
50|$|In machine learning, tree decompositions {{are also}} called {{junction}} trees, clique trees, or join trees; they {{play an important}} role in problems like <b>probabilistic</b> <b>inference,</b> constraint satisfaction, query optimization, and matrix decomposition.|$|E
40|$|Due to {{copyright}} restrictions, {{the access}} to {{the full text of}} this article is only available via subscription. It is unclear whether decision makers who receive forecasts expressed as probability distributions over outcomes understand the implications of this form of communication. We suggest a solution {{based on the fact that}} people are effective at estimating the frequency of data accurately in environments that are characterized by plentiful, unbiased feedback. Thus, forecasters should provide decision makers with simulation models that allow them to experience the frequencies of potential outcomes. Before implementing this suggestion, however, it is important to assess whether people can make appropriate <b>probabilistic</b> <b>inferences</b> based on such simulated experience. In an experimental program, we find that statistically sophisticated and naïve individuals relate easily to this presentation mode, they prefer it to analytic descriptions, and their <b>probabilistic</b> <b>inferences</b> improve. We conclude that asking decision makers to use simulations actively is potentially a powerful – and simplifying – method to improve the practice of forecasting. the Spanish Ministerio de Economía y Competitivida...|$|R
40|$|The {{research}} {{proposes a}} “conceptual- ERP-framework ” for a smart factory during technology diffusion “mega-projects”. Application of knowledge discovery and classification algorithms {{is applied to}} draw <b>probabilistic</b> <b>inferences</b> based on FP-Growth-algorithm without ignoring the “balanced scorecard (BSC) -model-driven architecture ” which is embedded with business intelligence. The prime objective being to illuminate research and scholarly understanding for strategic-alignment of design of vital artifacts of Enterprise resource planning systems (ERP) through collaborativemultidiscipline initiatives amongst the disciplines of “Software...|$|R
40|$|Cognition is a {{core subject}} to {{understand}} how humans think and behave. In that sense, {{it is clear that}} Cognition is a great ally to Management, as the later deals with people and is very interested in how they behave, think, and make decisions. However, even though Cognition shows great promise as a field, there are still many topics to be explored and learned in this fairly new area. Kemp & Tenembaum (2008) tried to a model graph-structure problem in which, given a dataset, the best underlying structure and form would emerge from said dataset by using bayesian <b>probabilistic</b> <b>inferences.</b> This work is very interesting because it addresses a key cognition problem: learning. According to the authors, analogous insights and discoveries, understanding the relationships of elements and how they are organized, play a very important part in cognitive development. That is, this are very basic phenomena that allow learning. Human beings minds do not function as computer that uses bayesian <b>probabilistic</b> <b>inferences.</b> People seem to think differently. Thus, we present a cognitively inspired method, KittyCat, based on FARG computer models (like Copycat and Numbo), to solve the proposed problem of discovery the underlying structural-form of a dataset...|$|R
50|$|The goal {{underlying}} the theoretical development of PLN {{has been the}} creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions. PLN {{has been designed to}} allow basic <b>probabilistic</b> <b>inference</b> to interact with other kinds of inference such as intensional inference, fuzzy inference, and higher-order inference using quantifiers, variables, and combinators, and be a more convenient approach than Bayesian networks (or other conventional approaches) for the purpose of interfacing basic <b>probabilistic</b> <b>inference</b> with these other sorts of inference. In addition, the inference rules are formulated {{in such a way as}} to avoid the paradoxes of Dempster-Shafer theory.|$|E
50|$|The basic goal of PLN is {{to provide}} {{reasonably}} accurate <b>probabilistic</b> <b>inference</b> {{in a way that}} is compatible with both term logic and predicate logic, and scales up to operate in real time on large dynamic knowledge bases.|$|E
5000|$|Probabilistic models {{treat the}} process of {{document}} retrieval as a <b>probabilistic</b> <b>inference.</b> Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.|$|E
40|$|The {{assumption}} that people possess {{a repertoire of}} strategies to solve the inference problems they face has been made repeatedly. The experimental findings of two previous studies on strategy selection are reexamined from a learning perspective, which argues that people learn to select strategies for making <b>probabilistic</b> <b>inferences.</b> This learning process is modeled with the strategy selection learning (SSL) theory, which assumes that people develop subjective expectancies for the strategies they have. They select strategies proportional to their expectancies, which are updated {{on the basis of}} experience. For the study by Newell, Weston, and Shanks (2003) it can be shown that people did not anticipate the success of a strategy {{from the beginning of the}} experiment. Instead, the behavior observed at the end of the experiment was the result of a learning process that can be described by the SSL theory. For the second study, by Br"oder and Schiffer (2006), the SSL theory is able to provide an explanation for why participants only slowly adapted to new environments in a dynamic inference situation. The reanalysis of the previous studies illustrates the importance of learning for <b>probabilistic</b> <b>inferences...</b>|$|R
40|$|A {{new type}} Fuzzy Inference System is proposed, a <b>Probabilistic</b> Fuzzy <b>Inference</b> system which model and {{minimizes}} {{the effects of}} statistical uncertainties. The blend of two different concepts, degree of truth and probability of truth in a unique framework leads to this new concept. This combination is carried out both in Fuzzy sets and Fuzzy rules, which gives rise to Probabilistic Fuzzy Sets and Probabilistic Fuzzy Rules. Introducing these probabilistic elements, a distinctive <b>probabilistic</b> fuzzy <b>inference</b> system is developed and this involves fuzzification, inference and output processing. This integrated approach accounts {{for all of the}} uncertainty like rule uncertainties and measurement uncertainties present in the systems and has led to the design which performs optimally after training. In this paper a <b>Probabilistic</b> Fuzzy <b>Inference</b> System is applied for modeling and control of a highly nonlinear, unstable system and also proved its effectiveness...|$|R
30|$|It {{is worth}} noting that the {{probabilistic}} logic feature used in isolation outperforms all other features and allows us to improve the classification model for link prediction on accuracy. It could be argued that such performance stems from evidence used on <b>probabilistic</b> <b>inferences,</b> but a similar analysis could be done for topological and semantic features. They use information that is missing on a probabilistic description logic setting. In conclusion, despite the fact that all features have different approaches, experimental results showed that they can be successfully used together.|$|R
50|$|Tenenbaum {{received}} his undergraduate degree in physics from Yale University in 1993, and his Ph.D. from MIT in 1999. His work primarily focuses on analyzing <b>probabilistic</b> <b>inference</b> as {{the engine of}} human cognition and {{as a means to}} develop machine learning.|$|E
5000|$|... reviews Laplace's method results (univariate and multivariate) and {{presents}} a detailed example showing {{the method used}} in parameter estimation and <b>probabilistic</b> <b>inference</b> under a Bayesian perspective. Laplace's method is applied to a meta-analysis problem from the medical domain, involving experimental data, and compared to other techniques.|$|E
50|$|An {{influence}} diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical {{representation of a}} decision situation. It is a generalization of a Bayesian network, in which not only <b>probabilistic</b> <b>inference</b> problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.|$|E
40|$|In {{this paper}} we propose a method for on-line max {{auditing}} of dynamic statistical databases. The method extends the Bayesian approach presented in [2], [3] and [4] for static databases. A Bayesian network addresses disclosures based on <b>probabilistic</b> <b>inferences</b> that can be drawn from released data; we have developed algorithms to update the network whenever the database changes. In particular, we consider the case in which records are added or deleted, or some sensitive values change their value. The paper introduces the algorithms and discusses results of a preliminary set of of experimental trials...|$|R
40|$|In this paper, {{the basic}} {{frameworks}} and practical schemes of the Bayesian network to the probabilistic image processing are reviewed. Though the Bayesian network {{is one of}} methods for <b>probabilistic</b> <b>inferences</b> in the artificial intelligence, also probabilistic models in the image processing based on the Bayesian statistics are regarded as Bayesian networks[1]. As one of approximate algorithms for <b>probabilistic</b> <b>inferences</b> by using Bayesian networks, belief propagation has been investigated[2]. Recently, the belief propagation {{has been applied to}} the probabilistic image processing[3, 4]. We consider an image on a square lattice Ω≡{ 1, 2, ···,L}. The states at pixel i(∈Ω) in the original image and the observed image are regarded as random variables denoted by Ai and Di, respectively. Then the random fields of states in the original image and the observed image are represented by A = (A 1,A 2,···,AL) and D = (D 1,D 2,···,DL). The actual original image and the observed image are denoted by a =(a 1,a 2,···,aL) and d =(d 1,d 2,···,dL), respectively. The probability that the original image is a, Pr { A = a}, is called the a priori probability of the image. In the Bayes formula, the a posteriori probability Pr { A = a | D = d}, that the original image is a when the given observed image is d, is expressed as Pr { A = a | D = d}...|$|R
40|$|First, {{concepts}} {{of different types}} of resampling will be introduced with simple examples. Next, [...] software applications for resampling are illustrated. Contrary to popular beliefs, many resampling tools are [...] available in standard statistical applications such as SAS and SyStat. Resampling can also be performed in [...] spreadsheet programs such as Excel. Last but not least, arguments for and against resampling are discussed [...] . I propose that there should be {{more than one way to}} construe <b>probabilistic</b> <b>inferences</b> and that counterfactual [...] reasoning is a viable means to justify use of resampling as an inferential tool...|$|R
