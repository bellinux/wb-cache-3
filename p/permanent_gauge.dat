0|35|Public
5000|$|Other {{equipment}} {{commonly used}} at <b>permanent</b> stream <b>gauge</b> include: ...|$|R
50|$|<b>Permanent</b> Downhole <b>gauges</b> or <b>Permanents</b> (for short), are {{installed}} {{in oil and}} gas wells {{for the purposes of}} observation and optimization. The most prolific function of a permanent is to monitor pressure at a single point or multiple points in a well. Temperature is the second most monitored factor. <b>Permanent</b> downhole <b>gauges</b> continue to evolve into many different types of sensors.|$|R
40|$|Long-term {{data from}} <b>permanent</b> <b>gauges</b> have the {{potential}} to provide more information about a reservoir than data from traditional pressure transient tests that last for a relatively small duration. Besides reducing ambiguity and uncertainties in the interpretation, long-term data also provide an insight on how reservoir properties may change as the reservoir is produced. This type of long-term surveillance provides the opportunity to look at the reservoir information in four dimensions rather than obtaining a glimpse or snapshot in time. However, the installation of <b>permanent</b> downhole <b>gauges</b> is only a recent phenomenon, and a methodology for the interpretation of the data has yet to be developed. The use of long-term data requires special handling and interpretation techniques due to the instability of in-situ permanent data acquisition systems, extremely large volume of data, incomplete flow rate history caused by unmeasured and uncertain rate changes, and dynamic changes in reservoir [...] ...|$|R
50|$|Intelligent {{completion}} {{technology is}} designed to optimize well, production and reservoir management processes by enabling operators to remotely monitor and control well inflow or injection downhole, at the reservoir, without physical intervention. Intelligent completions consist of some combination of flow control and zonal isolation devices (typically interval control valves and packers), <b>permanent</b> downhole <b>gauges,</b> downhole control systems, digital infrastructure and fiber optic systems.|$|R
40|$|Technological {{achievements}} {{in the area}} of well testing, such as <b>permanent</b> downhole <b>gauges,</b> demand automated techniques to cope with the large amounts of data acquired. In such an application, the need to interpret large quantities of data with little human intervention suggests the desirability of automated model recognition. Also in some cases the characteristic behavior of the pressure or its derivative curves for specific models may be hidden behind noise or human bias may lead to the selection of an invalid or inappropriate model...|$|R
50|$|A <b>Permanent</b> Downhole <b>Gauge</b> (PDG) is a {{pressure}} and/or temperature gauge permanently installed in an oil or gas well. Typically they are installed in tubing {{in the well}} and can measure the tubing pressure or annulus pressure or both. Systems installed in well casing to read formation pressure directly, suspended systems, and systems built in coil (continuous) tubing are also available. The data that PDGs provide are useful to Reservoir Engineers in determining the quantities of oil or gas contained below the Earth's surface in an oil or gas reservoir and also which method of production is best.|$|R
40|$|International audienceInformation on the {{magnitude}} and variability of low river flows at the river reach scale is central to most aspects of water resource and water quality management. Within the UK, river stretches with <b>permanent</b> <b>gauging</b> stations represent {{less than one percent}} of the total number of river stretches mapped at a scale of 1 : 50, 000 and fewer that 20 % of gauged catchments can be regarded as having natural flow regimes. This has led to the development of simple, multivariate models for predicting average annual natural flow duration statistics through relationships with catchment characteristics. One assumption within these models is that low flows occur at the same time at all points within a catchment, irrespective of the hydrogeological nature and climatic condition of the catchment. This paper discusses the implications of spatial variations in the timing of low flow events for this type of model. Differences in the timing of the mean day of occurrence of the annual Q 95 flow in UK catchments can be identified with low flows occurring earlier in the year within impermeable dry catchments and later in the year for wet permeable catchments. However, any differences in the mean day of occurrence between different catchments are generally masked by {{the magnitude}} of the inter-year variability in the day of occurrence. From analysis of linear combinations of flow statistics from nearest-neighbour gauged catchments, the paper demonstrates that the assumption of temporal coherence of low flows will generally result in an under-estimate of Q 95; these underestimates are more significant for pairs of impermeable catchments than for combinations of permeable catchments and impermeable-permeable catchments...|$|R
40|$|The San Joaquin River Riparian Habitat Restoration Program (SJRRP) has {{recognized}} the potential importance of real-time {{monitoring and management}} {{to the success of}} the San Joaquin River (SJR) restoration endeavor. The first step to realizing making real-time management a reality on the middle San Joaquin River between Friant Dam and the Merced River will be the installation and operation of a network of <b>permanent</b> telemetered <b>gauging</b> stations that will allow optimization of reservoir releases made specifically for fish water temperature management. Given the limited reservoir storage volume available to the SJJRP, this functionality will allow the development of an adaptive management program, similar in concept to the VAMP though with different objectives. The virtue of this approach is that as management of the middle SJR becomes more routine, additional sensors can be added to the sensor network, initially deployed, to continue to improve conditions for anadromous fish...|$|R
30|$|With <b>permanent</b> {{down-hole}} <b>gauges</b> (PDGs) widely {{installed in}} oilfields {{around the world}} in recent years, a continuous stream of transient pressure data in real time is now available, which motivates a new round of research interests in further developing pressure transient processing and analysis techniques. Transient pressure measurements from PDG are characterized by long term and high volume data. These data are recorded under unconstrained circumstances, so effects due to noise, rate fluctuation and interference from other wells cannot be avoided. These effects make the measured pressure trends decline or rise and then obscure or distort the actual flow behavior, which makes subsequent analysis difficult. In this paper, the problems encountered in analysis of PDG transient pressure are investigated. A newly developed workflow for processing and analyzing PDG transient pressure data is proposed. Numerical well testing synthetic studies are performed to demonstrate these procedures. The results prove that this new technique works well and the potential for practical application looks very promising.|$|R
40|$|The {{copyright}} in {{this thesis}} {{is owned by the}} author. Any quotation from the thesis or use of any of the information contained in it must acknowledge this thesis as the source of the quotation or information. i With the <b>permanent</b> down-hole <b>gauge</b> (PDG) widely installed in oilfields around the world in recent years, a continuous stream of transient pressure data in real time are available, which motivates a new round of research interests in further developing pressure transient analysis techniques. PDG data is recorded under the unconstrained circumstances, so that it cannot avoid effects due to noise, rate fluctuation and interference from other wells. These effects make the measured pressure trends declining or rising and then obscure or distort the traditional flow behavior, which makes the following analysis difficult. In this thesis, the problems encountered in analysis of PDG transient pressure are investigated. A new algorithm, multi-well deconvolution, and corresponding compute...|$|R
40|$|Sea level {{variation}} is a main contributor in controlling {{the evolution of}} coastal lagoons as well as determining risks of coastal flooding. This work investigates both long and short-term sea level variability in the Ria de Aveiro, a tide-dominated shallow coastal lagoon located on the western coast of Portugal. The 27 year series of available sea level observations from the <b>permanent</b> tide <b>gauge,</b> {{at the mouth of}} the Lagoon, has been analyzed for changes in the different sea level components: tides, surges and mean sea level. The various tidal constituents show interesting local short-term variations in amplitude and phase but no long-term trends. There is no evidence of an increase in weather effects on sea levels and of mean sea level over the period analyzed. Short (1 - 3 month) sea level data collected during two surveys (1987 / 8 and 2002 / 3) in the lagoon are compared. Results show that there has been a significant change in the tidal characteristics of the Lagoon with a general increase in amplitude and decrease in phase for most harmonic constituents...|$|R
40|$|Long term {{monitoring}} of geologic sites used for CO 2 sequestration is an environmental necessity while {{ensuring that the}} injected CO 2 remains contained in the intended storage formation. An early warning of any seepage or leakage that might require mitigating action {{remains one of the}} most important aspects of CO 2 sequestration projects. <b>Permanent</b> Downhole <b>Gauges</b> (PDG) provide important information for reservoir surveillance and management. The high-frequency pressure data are generally used for reservoir understanding, injection performance, and CO 2 breakthrough. Installation and usage of PDGs has become common in oil and gas field especially in smart fields because of their improved reliability and valuable information they provide. The objective of this study is to introduce a new technique to detect CO 2 seepage in a sequestration project by analyzing the high-frequency pressure data received from PDGs. In this work, the reservoir is being monitored after the sequestration is completed and all the wells have been shut-in. A Surrogate Reservoir Model (SRM) is developed that analyzes the bottom-hole pressure data received from the sensors. By analyzing these data, the SRM is able to approximate the location and predict the amount of the seepage. The developed SRM is also able to perform uncertainty analysis on the reservoir properties and their effect on the predictions...|$|R
40|$|With the <b>permanent</b> {{down-hole}} <b>gauge</b> (PDG) widely {{installed in}} oilfields {{around the world}} in recent years, a continuous stream of transient pressure data in real time are available, which motivates a new round of research interests in further developing pressure transient analysis techniques. PDG data is recorded under the unconstrained circumstances, so that it cannot avoid effects due to noise, rate fluctuation and interference from other wells. These effects make the measured pressure trends declining or rising and then obscure or distort the traditional flow behavior, which makes the following analysis difficult. In this thesis, the problems encountered in analysis of PDG transient pressure are investigated. A new algorithm, multi-well deconvolution, and corresponding computer codes are developed. The algorithm is based on linear recursion with added non-linear least squares optimization to deal with the noise problem in time domain. With this developed algorithm, the inter-well interference effect can be extracted and the variable-rate superposition effect can be solved at the same time. New deconvolution-based rate transient analysis and pressure transient analysis methods are both proposed in this thesis. Numerical well testing synthetic studies are performed to demonstrate these procedures. The results prove that the new method works well in homogeneous reservoirs with two wells flowing at single phase, multiple rates. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Mean {{sea level}} {{measurements}} made at Port Louis in the Falkland Islands in 1981 - 2, 1984 and 2009, together with values {{from the nearby}} <b>permanent</b> tide <b>gauge</b> at Port Stanley, have been compared to measurements made at Port Louis in 1842 by James Clark Ross. The long-term rate of change of sea level is estimated to have been + 0. 75 ± 0. 35 mm/year between 1842 and the early 1980 s, after correction for air pressure effects and for vertical land movement due to Glacial Isostatic Adjustment (GIA). The 2009 Port Louis data set is of particular importance due to the availability of simultaneous information from Port Stanley. The data set has been employed in two ways, by providing a short recent estimate of mean sea level itself, and by enabling the effective combination of measurements at the two sites. The rate of sea level rise observed since 1992, when the modern Stanley gauge was installed, has been larger at 2. 51 ± 0. 58 mm/year, after correction for air pressure and GIA. This rate compares to a value of 2. 79 ± 0. 42 mm/year obtained from satellite altimetry in the region over the same period. Such a relatively recent acceleration {{in the rate of}} sea level rise is consistent with findings from other locations in the southern hemisphere and globall...|$|R
40|$|Understanding the {{relationships}} between frequency of tidal inundation and elevation is vital {{for the design of}} coastal flood protection schemes, cross-site comparisons of intertidal ecology, reconstructing past sea level from palaeoecological data and predicting the likely ecological development of created intertidal habitats. However, very few studies relate intertidal ecology to measured inundation frequencies. Here we describe a versatile, relatively low-cost method to determine the precise relationship between elevation and inundation frequency at multiple locations over periods of several months. We used compact, autonomous depth-sensing data loggers (designed as fish tags) to measure water depths and high-resolution differential GPS to relate water depth to a terrestrial datum. Regression against records from <b>permanent</b> tide <b>gauges</b> at standard ports was then used to determine annual averages such as mean high water of spring or neap tides (MHWS/MHWN) or relationships between elevation and inundation frequency. Measurements of tides at 23 locations showed that differences in the levels of MHWN were large, even over short distances (up to 28 cm over 1. 5 km; 70 cm over 40 km). MHWS was less variable over short distances, but varied substantially between sites further than 10 km apart (up to 90 cm over 40 km). Data from standard ports, therefore, give a rather poor guide to the tidal regime at most locations, particularly for MHWN. However, using our methods, it is straightforward to relate intertidal ecology at multiple locations to actual measurements of the tidal regime at each...|$|R
40|$|We {{consider}} the neutralino mass matrix in a supersymmetric theory based on SU(2) × U(1) gauge group with arbitrary number of singlet, doublet and triplet Higgs superfields. We derive an {{upper bound on}} {{the mass of the}} lightest neutralino, and a lower bound on the mass of the heaviest neutralino, in such a theory. Assuming grand unification of the gauge couplings, the upper bound on the mass of the lightest neutralino can be expressed in terms of the gluino mass. For a gluino mass of 200 GeV, the tree level upper bound on the mass of the lightest neutralino is 92 GeV, which increases to 166. 5 GeV for a 1 TeV gluino. We also discuss the effect of dominant one-loop radiative corrections on these bounds. 1 <b>Permanent</b> addressIn supersymmetric <b>gauge</b> theories, each fermion and boson of the Standard Model is accompanied by its supersymmetric partner, transforming in an identical manner under the gauge group [1]. In supersymmetric theorie...|$|R
40|$|The {{effects of}} {{intravenous}} infusions of dopamine (0. 1 to 1 mg kg- 1 h- 1) and bromocriptine (10 to 40 micrograms kg- 1 h- 1) on colonic motility were investigated in fasted dogs fitted with <b>permanent</b> strain <b>gauges</b> on the ascending, transverse and descending colon. Infused at rates of 0. 5 and 1 mg kg- 1 h- 1 during 1 h, dopamine immediately stimulated the motility of the descending colon; after a delay of 40 to 60 min this effect was balanced by an inhibition of the motility of the ascending and transverse colon. Bromocriptine infused intravenously at doses of 10 to 40 micrograms kg- 1 h- 1 stimulated the motility {{of the whole}} colon but these effects were limited to {{the duration of the}} infusion (60 min). Both propranolol (0. 5 mg kg- 1) and tolazoline (2 mg kg- 1) failed to block the effects of dopamine and bromocriptine whereas phentolamine (0. 1 mg kg- 1) and prazosin (0. 2 mg kg- 1) partially reduced the inhibitory effects of dopamine on the proximal colon. Haloperidol at doses higher than 0. 2 mg kg- 1 and domperidone blocked the bromocriptine-induced stimulation of colonic motility which was unaffected by previous treatment with alpha- and beta-adrenoceptor blocking agents. These results suggest that in the dog, dopamine and bromocriptine stimulate colonic motility through specific dopamine receptors. However, they suggest that the inhibitory effects of dopamine on the proximal colon which are blocked by dopamine antagonists are also partially due to an effect on alpha 1 -adrenoceptors...|$|R
30|$|Since 1990 s, most of wells {{have been}} {{installed}} with <b>permanent</b> downhole <b>gauges.</b> These gauges are equipment that measures well pressure at small time interval, e.g., every second. As a result, the pressure monitoring generates {{large volumes of}} data. The recorded data are always associated with measurement errors due to various reasons such as sudden alteration in flowing temperature and problems in gauge that occur by physical changes in reservoir. Measurement errors are due to systematic and random deviations from the true value of the measured (recorded) data. While systematic errors are introduced by an inaccuracy inherent in the system such as imperfect performance (or calibration) of measurement gauges, random errors are caused by unpredictable fluctuations in the reading of gauges. The collected data are usually associated with noise. There is not a unified definition for noise since this term {{has been used in}} various disciplines. On one hand, noise is clearly related to measurement errors. Hence, it is possible to have systematic and random noises, although the latter is more common. Horne (1994) defined noise as a type of measurement error during a test which may be either unbiased (random noise) or biased (drift error). On the other hand, there are some anomalies in the recorded data which are distinguished from noise. For example, outliers are data points that lie away from the trend of data, while noise is considered as data points around the trend of overall data and in the same neighborhood as the true data (Athichanagorn et al. 2002). Either noise or other artifacts such as outliers may cause high degree of uncertainty in well test interpretations, and hence, it is essential to remove noise and other artifacts or reduce them to acceptable levels before further analysis.|$|R
40|$|Intelligent {{completions}} typically include <b>permanent</b> downhole <b>gauges</b> (PDG's) for continuous, real-time {{pressure and}} temperature monitoring. If adequately applied, such new technologies should allow anticipation of oil production and increase of final recovery {{with respect to}} traditional completions. In fact, pressure data collected from PDG's represent an essential information for understanding the dynamic behavior {{of the field and}} for reservoir surveillance. The potential drawback is that the number of data collected by PDG's can grow enormously, making it very difficult, if not impossible, to handle the entire pressure history as it was recorded. As a consequence it might often be necessary to reduce the pressure measurements to a manageable size, though without losing any potential information contained in the recorded data. As extensively reported in the literature, long-term data might be subject to different kinds of errors and noise, and not be representative of the real system response. Before the data can be used for interpretation purposes, especially if pressure derivatives are to be calculated (for instance, in well test analysis), an adequate filtering process should be applied. Multi-step procedures based on the wavelet analysis were presented in the literature for processing and interpreting long-term pressure data from PDG's. In this paper an improved approach largely based on the wavelet algorithms is proposed and discussed for the treatment of pressure data. All the steps of the procedure, namely outliers removal, denoising, transient identification, and data reduction were applied both to synthetic and to real pressure recordings. Results indicated that the application of the proposed approach allows identification of the actual reservoir response and subsequent interpretation of pressure data for an effective characterization of the reservoir behavior even from very disturbed signal...|$|R
40|$|In {{quantized}} gauge field theories one {{can introduce}} sets of operators that modify the gauge-topological {{structure of the}} fields but whose physical effect is essentially local. In 2 + 1 dimensional non-Abelian gauge theories these operators form scalar fields and {{it is argued that}} when the local gauge symmetry is not spontaneously broken then these topological fields develop a vacuum expectation value and their mutual symmetry breaks spontaneously. It is shown that quarks are then permanently confined. In 3 + 1 dimensional non-Abelian gauge theories one finds that the topological operators and the gauge field operators form a closed algebra from which it is deduced that this system can be in one of the four different phases: (i) spontaneous breakdown via an explicit or composite Higgs field, (ii) no Higgs field but <b>permanent</b> confinement of <b>gauge</b> quantum numbers, (iii) Higgs effect and still confinement, presumably only if there is an unbroken subgroup, and (iv) an intermediate phase (critical point?) with massless particles. Finally, the algebra can be realized in a simple model where phases (i) and (ii) can be obtained from each other by dual transformation...|$|R
40|$|Tidal inlets which link a {{tidal basin}} {{to the sea}} via a {{constricted}} entrance are common on the south-east Australian coast. Closure, or even significant constriction, raises water levels but restricts tidal range within the basin, while open entrances provide regular and significant tidal exchange with the ocean. A rapid assessment procedure with minimal data requirements {{has been shown to}} be informative for monitoring and a useful component of any Decision Support System set up as part of a management structure. Such a system is presented in this paper. It is based on one <b>permanent</b> water level <b>gauge</b> inside the inlet plus the use of a simple, first-order hydrodynamic model to relate the tide range, mean water level and river flow to the inlet cross sectional area. The method is tested against data from the Snowy River Estuary in south-eastern Australia but would be suitable over a range of estuaries. In addition, the framework presented can also provide a mechanism to explore conditions over the range of expected data, thus allowing better selection of model schematization and runs in estuarine systems where the use of 2 or 3 D modeling can be justified...|$|R
30|$|In {{well test}} practice, {{the amount of}} {{collected}} pressure data is tremendous. Practically {{it is impossible to}} consider all data points in the interpretation due to limitation in computational resources. In addition, noisy pressure data may lead to wrong interpretation, and hence, large uncertainties would be expected in parameter estimation. As a result, noise removing {{is an essential part of}} analyzing well test data. In this regard, denoising of downhole gauge data has been the subject of several research papers. Osman and Stewart (1997) employed the Butterworth digital filter method to subtract noise in the data. Kikani and He (1998) showed that the Butterworth filter results in poor performance for some cases. Instead, they used a wavelet analysis to remove noise. Unneland et al. (1998) presented the use of decline curve analysis to describe production from a simple reservoir by pressure and rate measurement. The well flow rates were calculated by resolving the rate convolution integral. The rates that determined by convolution integral are rates which happen if pressure stays constant during production. Athichanagorn et al. (2002) proposed a seven-step procedure for processing of <b>permanent</b> downhole <b>gauge</b> data as follows: (1) outlier removal, (2) denoising, (3) transient identification, (4) data reduction, (5) flow rate history reconstruction, (6) behavioral filtering, and (7) data interpretation. They indicated that the wavelet thresholding method is one of the most effective methods to denoise data. An important application of wavelet in well testing is permuting a pressure signal to compress parameters that named wavelet coefficients (Daubechies 1990). The wavelet transform was also applied by Olsen and Nordtvedt (2005) to denoise pressure data. They proposed automatic filtering methods to estimate appropriate thresholds and pressure data reduction. In another study, Nomura (2006) described a smoothing algorithm based on derivative constraints suitable for pressure transient data. He compared the constraint smoother and unconstrained regression splines and concluded that the former can have better performance when the order of derivative constraints increases.|$|R
40|$|With the {{installation}} of <b>Permanent</b> Down-hole <b>Gauges</b> (PDGs) during oil field development, a large volume of high resolution pressure, temperature and sometimes flow-rate data are available for real-time and continuous reservoir monitoring. In practice, interpretations of these data can optimize well performance, provide information about the reservoir and continuously calibrate the reservoir model. Although the wellbore is in a non-isothermal environment, heat transfer between the fluid in the wellbore and the formation is often ignored and temperature is usually assumed to be constant {{in the process of}} data interpretation, leading to misunderstanding of the pressure profile. Furthermore, the pressure transient analysis (PTA) often fails to determine accurate flow regimes, and may be erroneously applied in nonlinear reservoir-well systems. These problems motivated my detailed analysis of temperature data. In this thesis, firstly, a non-isothermal wellbore model that is capable of predicting the temperature, pressure, and flow-rate profiles under multi-rate and multiphase production scenarios is established. Then this numerical wellbore model is coupled with a reservoir model to reproduce the transient temperature behaviour at gauge locations. Secondly, a new workflow for integrating transient down-hole data processing is introduced. The relationship between temperature change and flow-rate change is interpreted and a new nonlinearity diagnostic function (1 ̆d 4341 ̆d 4471 ̆d 4621 ̆d 45 f 1 ̆d 450) is presented. Thirdly, new procedures of model-independent transient temperature analysis are performed, followed by diagnosing the wellbore storage regime, verifying the PTA interpretation results, and reconstructing the flow-rate history using transient temperature data. Several case studies are conducted to demonstrate how transient temperature analysis, along with the transient pressure analysis can greatly reduce the uncertainties in well testing interpretation. The applications of both synthetic datasets which are simulated by the fully coupled wellbore-reservoir model and real field datasets demonstrated that the temperature data can provide additional constraints for pressure analysis. Additionally, the reliability of the developed methods which reveal complementary reservoir information from transient temperature data has also been verified...|$|R
40|$|<b>Permanent</b> Down-hole <b>Gauge</b> (PDG) is the {{down-hole}} {{measuring device}} installed during the well completion. It {{can provide the}} continuous down-hole transient pressure in real-time. Since 1980 s, PDG has been widely applied in oilfields. The wide field applications have demonstrated that the long-term pressure monitoring with PDG is useful for production optimization, reservoir description and model calibration. Analysing the long-term, noisy and large volume of PDG pressure data and extracting useful reservoir information are very challenging. Although lots of achievement {{has been made in}} PDG data processing, such as denoising, outlier removal and transient identification, analysis of long-term transient pressure from PDG is still difficult due to several challenging problems. The first problem is the dynamic changes in reservoir-well properties, which can cause the linearity assumption for pressure-transient analysis invalid, also the reservoir model needs calibration to match the field performance. The second problem is unknown or incomplete flow rate history. These problems together make it a very challenging task for engineer to interpret long-term transient pressure from PDG. This study investigates novel methods to analyse the long-term transient pressure from PDG with Wavelet Transform (WT). Firstly, a new diagnostic function named as Unit Reservoir System Response Aurc has been developed, and it can effectively diagnose the nonlinearities from PDG pressure due to the changes in reservoir-well properties. The nonlinearity diagnostic and evaluation is an important procedure before pressure analysis. Secondly, a model-independent method of reconstructing unknown rate history has been developed. This method has wide applications, considering the effects of skin, wellbore storage, reservoir heterogeneity and multiphase flow. Thirdly, based on the nonlinearity diagnostic result, sliding window technique is proposed to analyse long-term pressure with nonlinearities and update reservoir model with time-dependent reservoir properties. The synthetic cases and field data application have demonstrated that the developed methods can reveal more useful reservoir information from PDG pressure and realize the potential of PDG as the tool of reservoir management...|$|R
40|$|AbstractCarbon capture, utilization, and {{sequestration}} {{have the}} potential to enable deep reductions in global carbon emissions if high storage efficiency can be achieved. A major hurdle to industrial-scale implementation of geological carbon sequestration (GCS) projects is the potential migration of fluids from the storage formations and the resulting legal and financial liabilities. The capability to accurately identify pathways by which stored CO 2 could leak, has leaked, or is leaking from the targeted storage zone is thus of paramount importance to site licensees and regulators. Many monitoring, verification, and accounting (MVA) techniques have been devised over the years, however, pressure-based leakage detection {{remains one of the most}} sensitive and cost-effective technique for early detection of fluid migration from storage formations. It has consistently received the highest score in terms of benefit/cost ratio and it provides the greatest potential for leakage detection with broad areal coverage. Although much has been done in the area of forward modeling of leakage scenarios, the more challenging problems of pressure inversion for leakage detection, monitoring network design, and applications to operational GCS monitoring deserve more attention. Given the advent of <b>permanent</b> downhole <b>gauges,</b> which are real-time pressure and temperature monitoring systems installed at the bottom hole of reservoir wells, a salient question is how to leverage these infrastructure investments to reduce GCS risks caused by leakage from storage formations. Traditional well testing techniques impose constant injection rates and have been mainly applied to inferring reservoir formation properties. In contrast, the harmonic pulse testing (HPT) technique employed by this study induces sinusoidal flows into the reservoir through a pulser well, and pressure oscillations are collected at one or more observation wells. In this sense, HPT bears similarity to many geophysical techniques. The main difference is its low cost. By systematically varying pulsing frequencies, HPT can stimulate larger reservoir volumes than the conventional well testing techniques do and generate more useful information. Leakage will cause different pressur...|$|R
40|$|The <b>Permanent</b> Downhole <b>Gauge</b> (PDG) {{can monitor}} the {{reservoir}} {{in real time}} {{over a long period}} of time. This produces a huge amount of real time data which can potentially provide more information about wells and reservoirs. However, processing large numbers of data and extracting useful information from these data brings new challenges for industry and engineers. A new workflow for processing the PDG data is proposed in this study. The new approach processes PDG data from the view of gauge, well and reservoir. The gauge information is first filtered with data preprocessing and outlier removal. Then, the well event is identified using an improved wavelet approach. The further processing step of data denoise and data reduction is carried out before analyzing the reservoir information. The accurate production history is very essential for data analysis. However, the accurate production rate is hard to be acquired. Therefore, a new approach is created to recover flow rate history from the accumulated production and PDG pressure data. This new approach is based on the theory that the relation between production rate and the amplitude of detail coefficient are in direct proportion after wavelet transform. With accurate pressure and rate data, traditional well testing is applied to analyze the PDG pressure data to get dynamic reservoir parameters. The numerical well testing approach is also carried out to analyze more complex reservoir model with a new toolbox. However, these two approaches all suffer from the nonlinear problem of PDG pressure. So, a dynamic forward modelling approach is proposed to analyze PDG pressure data. The new approach uses the deconvolution method to diagnose the linear region in the nonlinear system. The nonlinear system can be divided into different linear systems which can be analyzed with the numerical well testing approach. Finally, a toolbox which includes a PDG data processing module and PDG data analysis module is designed with Matlab. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Currently {{many of the}} world’s mature fields {{have entered}} the middle-high {{water-cut}} period. At this stage, reservoir numerical simulation (RNS) becomes {{a powerful tool for}} reservoir management which allows the reservoir engineer to plan and evaluate future development options for the reservoir to carry out the ultimate goal of minimizing costs and maximizing economic returns. So the precision of RNS is crucial to the success of any development plan. A precise reservoir model, including a geological model and a fluid model, is the foundation of RNS. Hence, how to use all available history data (such as <b>permanent</b> down-hole <b>gauges</b> (PDG) data, production data, etc.) to calibrate a reservoir model is a huge challenge for reservoir management. In this thesis, the multiphase flow well testing problems encountered in reservoir management are studied. Based on analytical solutions, traditional multiphase flow well testing approaches are modified to make them suitable for special reservoirs. For non-uniform saturation reservoirs, some improved understandings of transient well testing have been synthetically developed. Before water breaks through, the transient pressure response in the transition zone of two phase flow systems is theoretically studied, and the results show that the pressure response in the transition zone {{is a function of the}} total effective mobility of the fluid. After water breaks through, the derivative of the bottom-hole pressure is a function of the integral of total mobility, which can be approximately replaced by the average total effective mobility. The trend of the BHP curve is controlled by the total mobility. For layered commingled reservoirs, the type curves of transient pressure data are obtained. According to these type curves, some improved understandings are developed. At the same time, the practicality of selective inflow performance methods (SIP) on field scale reservoir models is also discussed. Because of the limitation of analytical solutions, not all existing multiphase flow well testing approaches are fit for evaluating mature field cases. So based on numerical solutions, a new multiphase flow well testing procedure is developed. The overall conclusion to the work is that transient well test data or PDG data with multiphase flow effects should be used to improve the degree of accuracy of reservoir model and fulfill the ultimate goal of well testing of dynamic reservoir monitoring and managemen...|$|R
40|$|This work {{presents}} the development, validation {{and application of}} a novel deconvolution method based on B-splines for analyzing variable-rate reservoir performance data. Variable-rate deconvolution is a mathematically unstable problem which has been under investigation by many researchers over the last 35 years. While many deconvolution methods have been developed, few of these methods perform well in practice - {{and the importance of}} variable-rate deconvolution is increasing due to applications of <b>permanent</b> downhole <b>gauges</b> and large-scale processing/analysis of production data. Under these circumstances, our objective is to create a robust and practical tool which can tolerate reasonable variability and relatively large errors in rate and pressure data without generating instability in the deconvolution process. We propose representing the derivative of unknown unit rate drawdown pressure as a weighted sum of Bsplines (with logarithmically distributed knots). We then apply the convolution theorem in the Laplace domain with the input rate and obtain the sensitivities of the pressure response with respect to individual B-splines after numerical inversion of the Laplace transform. The sensitivity matrix is then used in a regularized least-squares procedure to obtain the unknown coefficients of the B-spline representation of the unit rate response or the well testing pressure derivative function. We have also implemented a physically sound regularization scheme into our deconvolution procedure for handling higher levels of noise and systematic errors. We validate our method with synthetic examples generated with and without errors. The new method can recover the unit rate drawdown pressure response and its derivative to a considerable extent, even when high levels of noise are present in both the rate and pressure observations. We also demonstrate the use of regularization and provide examples of under and over-regularization, and we discuss procedures for ensuring proper regularization. Upon validation, we then demonstrate our deconvolution method using a variety of field cases. Ultimately, the results of our new variable-rate deconvolution technique suggest that this technique has a broad applicability in pressure transient/production data analysis. The goal of this thesis is to demonstrate that the combined approach of B-splines, Laplace domain convolution, least-squares error reduction, and regularization are innovative and robust; therefore, the proposed technique has potential utility in the analysis and interpretation of reservoir performance data...|$|R
40|$|International audienceSea-level rise {{is one of}} {{the most}} {{unavoidable}} consequences of anthropogenic climate change. Coastal managers need to anticipate impacts and adaptation needs at local scale, which implies considering all components of sea-level changes, including vertical ground motions. Subsidence phenomena can exacerbate the local consequences of sea-level rise in terms of hazard (flooding, erosion) and coastal vulnerability. In addition, studies attempting to reconstruct past-sea-level changes or to validate satellite altimetry also need to correct the local values obtained by tide gauges using vertical ground motions. Indeed, local surface deformation can affect the link between the global sea level evolution and its local expression measured by the tide gauges: subsidence enhance local sea level, upheaval reduces it. In all these cases, Differential SAR Interferometry – in combination with other in-situ measurements such as tide <b>gauge,</b> <b>permanent</b> GPS and DORIS Stations – provides useful complementary information on the spatial variability of vertical ground motions. Performances of DInSAR can be consistent (depending on the used processing technique) in terms of precision, with the observation of sea-level rise. In addition, using archived data (since the 1990 ’s using past space-borne SAR missions) SAR interferometry allows to observe surface deformations on areas where no ground based geodetic network were available in the past. We present results on coastal areas - mostly urbanized that are more suitable for DInSAR use – for illustrating the potential of the application of the SAR interferometric techniques to this research domain...|$|R
40|$|This study {{assessed}} the bleaching response (BR) of coral colonies within the central reef complex in Sodwana Bay, South Africa. Bleach surveys were conducted at 16 sites on 8 reefs {{over the period}} of 2007 to 2013. A total of 12 858 coral colonies from 30 taxa were randomly sampled and colonies were placed into 7 categories of bleaching response. This allowed for the calculation of taxon-specific BR as a weighted percentage of coral cover bleached. Continuous temperature records from a <b>permanent</b> temperature <b>gauge</b> on Two Mile Reef {{were used to assess}} thermal stress over this period. The percentage of coral colonies that bleached in 2007, 2008, 2011, 2012 and 2013 were 37. 4 %, 17. 4 %, 23. 8 %, 33. 6 % and 38. 8 % respectively. A binomial GLM model framework was used to separate the effects of year, reef and taxon on the bleaching response. Due to inconsistent sampling of sites over time, only data from the seven sites on Two Mile Reef (TMR) and the two sites on Nine Mile Reef (NMR) were included in the model. A total of 6758 coral colonies from the nine most abundantly sampled taxa were used in the assessment of bleaching response for TMR and NMR over the sample period. Taxon was shown to explain most of the variability in the bleaching response of TMR and NMR over time (40. 9 %). The standardized reef bleaching response of TMR and NMR indicated the same temporal trends with a range of 5 % to 28 % of live coral surface bleached. Standardized reef-specific BR showed periods of high (2007, 2012 and 2013) and low (2008 and 2011) bleaching response. Low BR in 2008 and 2011 did not correspond to thermal stress (≥ 27. 5 °C) suggesting that local upwelling buffered the effects of thermal stress experienced. Standardized taxon-specific bleaching response for TMR and NMR displayed large variability over time and ranged from 2. 5 % to 45 % of live coral surface bleached, with Montipora being the most susceptible and Galaxea and Playgyra being the least. Mean site-specific BR averaged over all years, including all sites, found that Coscinaraea, Montipora, Astreopora and Anomastrea were the most susceptible taxa. This study and other recent studies draw attention to a trend of either episodic or an increasing frequency and intensity of bleaching in southern African reefs...|$|R
40|$|This study {{describes}} a post-project {{evaluation of the}} Baxter Creek Gateway Restoration Project located in a small, urbanized section of creek in the City of El Cerrito, Contra Costa County, California. The project was conducted to restore sinuosity, provide aquatic and riparian habitat, and enhance public access to a 700 -foot section of channelized stream. Our assessment of this project’s performance (completed {{less than a year}} after the project was constructed) evaluated the restoration effort’s progress and provides a baseline for future assessments of the project as it matures. Assessment approaches and techniques included physical surveys of the creek’s longitudinal (long) profile and several cross sections, facies mapping of the creek’s bed structure, estimation of a sediment budget for the site’s drainage basin, observations of site users, interviews with community stakeholders, visual evaluation of vegetation success rates, and photo documentation of current site conditions. Although the restoration project is new and some intended features will take years to develop, we evaluated the project based on the proposed goals that we could measure and interpret with only two to three days of field work. We found that the creek’s planform was similar to the plans but detected some bank erosion as well as bed material transport from the upstream to downstream end of the project. The current sediment yield for the urban catchment is much less than during historical conditions. Based on bed structure measurements, sediments will need to be managed at the site as common flows were predicted to move gravels and cobbles in the restored reach. The quality of potential vegetative and aquatic habitat within the project site had increased as a result of the restoration, but the site’s surrounding culverted creek and urbanized landscape limit the feasibility of fish and amphibian colonization. Exotic riparian vegetation grew prolifically despite the fact that crews removed all vegetation during construction, planted only natives, and a citizen group frequently removed invasives. The multi-use trail and interpretive signs contributed to a successful recreational community space, but the trail will need improvements in connectivity beyond the project site before it can serve as a viable segment of the intercity Ohlone Greenway trail system. The current monitoring plan for the restoration is well written and includes important abiotic and biotic factors, but we also recommend installing a <b>permanent</b> stream <b>gauge</b> at the site, monitoring flood overflow conveyance in adjacent streets, conducting physical surveys of the long profile and established cross sections, and regular facies mapping...|$|R
40|$|International audienceThe Corsica {{site has}} been {{established}} in 1996 to perform altimeter calibration on TOPEX/Poseidon and then on its successors Jason- 1 and Jason- 2. The first chosen location was under the # 85 ground track that overflight the Senetosa Cape. In 2005, it was decided to develop another location close to Ajaccio, to be able to perform the calibration of Envisat and in a next future of SARAL/AltiKa that will flight over the same ground tracks. Equipped with various instruments (tide <b>gauges,</b> <b>permanent</b> GPS, GPS buoy, weather station [...] .) the Corsica calibration site is able to quantify the altimeter Sea Surface Height bias but also to give an input on the origin of this bias (range, corrections, orbits, [...] .). Due to the size of Corsica (not a tiny island), the altimeter measurement system (range and corrections) can be contaminated by land. The aim {{of this paper is to}} evaluate this land contamination by using GPS measurements from a fixed receiver on land and from another receiver onboard a life buoy. Concerning the altimeter land contamination, we have quantify that this effect can reach 8 mm/km and then affects the Sea Surface Height bias values already published in the framework of the Corsica calibration site by 5 - 8 mm for TOPEX and Jason missions. On the other hand, the radiometer measurements (wet troposphere correction) are also sensitive to land and we have been able to quantify the level of improvement of a dedicated coastal algorithm that reconciles our results with those coming from other calibration sites. Finally, we have also shown that the standard deviation of the GPS buoy sea level measurements is highly correlated (∼ 87 %) with the Significant Wave Height derived from the altimeters and can be used to validate such paramete...|$|R
40|$|Norway’s {{national}} {{sea level}} observing system {{consists of an}} extensive array of tide <b>gauges,</b> <b>permanent</b> GNSS stations, and lines of repeated levelling. Here, we make use of this observation system to calculate relative sea-level rates and rates corrected for glacial isostatic adjustment (GIA) along the Norwegian coast for three different periods, i. e., 1960 to 2010, 1984 to 2014, and 1993 to 2016. For all periods, the relative sea-level rates show considerable spatial variations that are largely {{due to differences in}} vertical land motion due to GIA. The variation is reduced by applying corrections for vertical land motion and associated gravitational effects on sea level. For 1960 to 2010 and 1984 to 2014, the coastal average GIA-corrected rates for Norway are 2. 0 ± 0. 6 mm/year and 2. 2 ± 0. 6 mm/year, respectively. This is close to the rate of global sea-level rise for the same periods. For the most recent period, 1993 to 2016, the GIA-corrected coastal average is 3. 5 ± 0. 6 mm/year and 3. 2 ± 0. 6 mm/year with and without inverse barometer (IB) corrections, respectively, which is significantly higher than for the two earlier periods. For 1993 to 2016, the coastal average IB-corrected rates show broad agreement with two independent sets of altimetry. This suggests that there is no systematic error in the vertical land motion corrections applied to the tide-gauge data. At the same time, altimetry does not capture the spatial variation identified in the tide-gauge records. This could be an effect of using altimetry observations off the coast instead of directly at each tide gauge. Finally, we note that, owing to natural variability in the climate system, our estimates are highly sensitive to the selected study period. For example, using a 30 -year moving window, we find that the estimated rates may change by up to 1 mm/year when shifting the start epoch by only one year...|$|R
40|$|The phenomenology, {{generation}} and associated dynamics of short period seiche oscillations observed {{along the northern}} coast of the Maltese Islands are studied from a set of densely sampled, long term hydro-meteorobgical observations made at a <b>permanent</b> sea level <b>gauge,</b> together with simultaneous observations of bottom pressure recordings at offshore positions and across the Malta Channel. This coastal seiche, known locally as the 'milghiibd, manifests itself {{in conjunction with the}} occurrence of mesoscale atmospheric gravity waves travelling in the lower troposphere. The associated open sea waves excite the water bodies of the various inlets, bays and harbours into resonant osculations which reach a range ofup to lmin Melfeha Bay. Numerical experiments based on the free surface, non-linear Princeton Ocean Model (POM) in 2 D mode explain the response characteristics of two adjacent wide-mouthed open embayments. The seiche-induced barotropic circulation and impact on the flushing ofMellieha Bay are studied by means of an advection-diffusion scheme implemented within POM. Sea level signals on the synoptic, planetary wave and seasonal scales dominate the residual spectrum A multiple regression model and a novel analytic technique based on the wavelet transform provide in combination a very effective means of studying the composition ofthe sea level signal and the dependence of its variability in time on one or more correlated parameters. Atmospheric pressure is the predominant factor determining the sea level variability at frequencies lower than 0. 75 cpd. In the upper synoptic frequency (0. 3 < f < 0. 5 cpd) the response is very close to isostatic, with an average gain of 0. 7. At other frequencies the overall response is non-isostatic implying that other factors besides mesoscale atmospheric pressure forcing contribute to the sea level variability. On account of the station's position close to the latitudinal axis of the Strait of Sicily, these signals are important in understanding the control ofthe Strait on intra-basin exchanges. Seasonal changes in the mean sea level show a major minimumin March and a major maximum towards the last months ofthe year. Besides the usual steric and direct meteorological effects, this variability is attributed to adjustments in the mass balance ofthe whole Mediterranean basin...|$|R

