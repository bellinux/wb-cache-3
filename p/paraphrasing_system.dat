6|26|Public
40|$|We {{demonstrate}} {{a method of}} improving a seed sentiment lexicon developed on essay data by using a pivot-based <b>paraphrasing</b> <b>system</b> for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15 % improvement in {{the accuracy of the}} seed lexicon on 3 -way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7 % improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews). ...|$|E
40|$|Identifying Textual Entailment is {{the task}} of finding the {{relationship}} between the given hypothesis and text fragments. Developing a high-performance text <b>paraphrasing</b> <b>system</b> usually requires rich external knowledge such as syntactic parsing, thesaurus which is limited in Chinese since the Chinese word segmentation problem should be resolve first. By following last year, in this year, we continue adopting the created RITE system and combine with multiple online available thesaurus. We derive two exclusive feature sets for learners. One is the operations between the text pairs, while the other adopted the traditional bag-of-words model. Finally, we train the classifier with the above features. The official results indicate the effectiveness of our method. 1...|$|E
40|$|In {{this paper}} we {{investigate}} the automatic collection, generation {{and evaluation of}} sentential paraphrases. Valuable sources of paraphrases are news article headlines; they tend to describe the same event in various different ways, and can easily {{be obtained from the}} web. We describe a method for generating paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT <b>paraphrasing</b> <b>system</b> over the word substitution system. We compare human judgements to automatic judgement measures and demonstrate that the BLEU metric correlates well with human judgements provided that the generated paraphrase is sufficiently different from the source sentence...|$|E
40|$|This paper {{describes}} a universal model for paraphrasing that transforms according to defined criteria. We showed that by using different criteria we could construct {{different kinds of}} <b>paraphrasing</b> <b>systems</b> including one for answering questions, one for compressing sentences, one for polishing up, and one for transforming written language to spoken language. ...|$|R
40|$|This paper {{proposes a}} new method of ranking near-synonyms ordered by their {{suitability}} of nuances {{in a particular}} context. Our method distincts near-synonyms by semantic features extracted from their definition statements in an ordinary dictionary, and ranks them by the types of features and a particular context. Our method is an initial step to achieve a semantic <b>paraphrase</b> <b>system</b> for authoring support. ...|$|R
40|$|Previous work on {{paraphrase}} extraction {{and application}} {{has relied on}} either parallel datasets, or on distributional similarity metrics over large text corpora. Our approach combines these two orthogonal sources of information and directly integrates them into our <b>paraphrasing</b> <b>system’s</b> log-linear model. We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving stateof-the-art quality. ...|$|R
40|$|The {{ability to}} {{generate}} or to recognize paraphrases {{is key to the}} vast majority of NLP applications. As correctly exploiting context during translation {{has been shown to be}} successful, using context information for paraphrasing could also lead to improved performance. In this article, we adopt the pivot approach based on parallel multilingual corpora proposed by (Bannard and Callison-Burch, 2005), which finds short paraphrases by finding appropriate pivot phrases in one or several auxiliary languages and back-translating these pivot phrases into the original language. We show how context can be exploited both when attempting to find pivot phrases, and when looking for the most appropriate paraphrase in the original subsentential “envelope”. This framework allows the use of paraphrasing units ranging from words to large sub-sentential fragments for which context information from the sentence can be successfully exploited. We report experiments on a text revision task, and show that in these experiments our contextual sub-sentential <b>paraphrasing</b> <b>system</b> outperforms a strong baseline system. ...|$|E
40|$|Recognizing {{inference}} in text is {{the task}} of finding the textual entailment relation between the given hypothesis and text fragments. Developing a high-performance text <b>paraphrasing</b> <b>system</b> usually requires rich external knowledge such as syntactic parsing, thesaurus which is limited in Chinese since the Chinese word segmentation problem should be resolve first. In this paper, we go different line. We propose a pattern-based and support vector machine-based trainable text entailment tagging framework under the condition of part-of-speech tagging information is available. We derive two exclusive feature sets for learners. One is the operations between the text pairs, while the other adopted the traditional bag-of-words model. Finally, we train the classifier with the above features. The official results indicate the effectiveness of our method. In terms of accuracy, our method achieves 53. 6 % for Traditional Chinese MC task (second place) and 55. 4 % for Traditional Chinese BC task. After the correction, our method in BC task is 67. 9 % with the same setting. 1...|$|E
40|$|Word sense {{disambiguation}} (WSD) is {{the process}} of computationally identifying and labeling poly- semous words in context with their correct meaning, known as a sense. WSD is riddled with various obstacles that must be overcome in order to reach its full potential. One of these problems is the aspect of the representation of word meaning. Traditional WSD algorithms make the assumption that a word in a given context has only one meaning and therfore can return only one discrete sense. On the other hand, a novel approach is that a given word can have multiple senses. Studies on graded word sense assignment (Erk et al., 2009) as well as in cognitive science (Hampton, 2007; Murphy, 2002) support this theory. It has therefore been adopted in a novel, <b>paraphrasing</b> <b>system</b> which performs word sense disambiguation by returning a probability distribution over potential paraphrases (in this case synonyms) of a given word. However, it is unknown how well this type of algorithm fares against the traditional one. The current study thus examines if and how it is possible to make a comparison of the two. A method of comparison is evaluated and subsequently rejected. Reasons for this as well as suggestions for a fair and accurate comparison are presented...|$|E
40|$|Automatic paraphrasing is an {{important}} component in many natural language processing tasks. In this article we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating <b>paraphrase</b> <b>systems</b> automatically (e. g., by measuring precision, recall, and F 1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1...|$|R
40|$|We present initial {{investigation}} into the task of paraphrasing language while targeting a particular writing style. The plays of William Shakespeare and their modern translations are used as a testbed for evaluating <b>paraphrase</b> <b>systems</b> targeting a specific style of writing. We show that even with a relatively small amount of parallel training data, {{it is possible to}} learn paraphrase models which capture stylistic phenomena, and these models outperform baselines based on dictionaries and out-of-domain parallel text. In addition we present an initial {{investigation into}} automatic evaluation metrics for paraphrasing writing style. To {{the best of our knowledge}} this is the first work to investigate the task of paraphrasing text with the goal of targeting a specific style of writing. 9 2012 The COLING. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Existing <b>paraphrase</b> <b>systems</b> either create {{successive}} drafts from {{an underlying}} representation, or they aim to correct texts. A {{different kind of}} paraphrase, also reflecting a real-world task, involves imposing, on an original text, external constraints in terms of length, readability, etc. This sort of paraphrase requires a new framework which allows production of a new text satisfying the constraints, but with minimal change from the original. In order for such a system to be feasible when used on a large scale, searching the space of candidate solution texts {{has to be made}} tractable. This paper examines a method for pruning the search space via branch-and-bound, evaluates three variants to find the most efficient model, and discusses its relation to standard heuristic methods such as genetic algorithms. 1 Introduction The paraphrasing framework of this paper is one where a text is modified `reluctantly' to conform to external surface constraints, such as length or readability requi [...] ...|$|R
40|$|This {{paper is}} {{concerned}} with paraphrase detection. The ability to detect similar sentences written in natural language is crucial for several applications, such as text mining, text summarization, plagiarism detection, authorship authentication and question answering. Given two sentences, {{the objective is to}} detect whether they are semantically identical. An important insight from this work is that existing <b>paraphrase</b> <b>systems</b> perform well when applied on clean texts, but they do not necessarily deliver good performance against noisy texts. Challenges with paraphrase detection on user generated short texts, such as Twitter, include language irregularity and noise. To cope with these challenges, we propose a novel deep neural network-based approach that relies on coarse-grained sentence modeling using a convolutional neural network and a long short-term memory model, combined with a specific fine-grained word-level similarity matching model. Our experimental results show that the proposed approach outperforms existing state-of-the-art approaches on user-generated noisy social media data, such as Twitter texts, and achieves highly competitive performance on a cleaner corpus...|$|R
40|$|This paper {{develops}} a computational model of paraphrase under which text modification {{is carried out}} reluctantly; that is, there are external constraints, such as length or readability, on an otherwise ideal text, and modifications to the text are necessary to ensure conformance to these constraints. This problem is analogous to a mathematical optimisation problem: the textual constraints {{can be described as}} a set of constraint equations, and the requirement for minimal change to the text can be expressed as a function to be minimised; so techniques from this domain can be used to solve the problem. The work is done as part of a computational <b>paraphrase</b> <b>system</b> using the XTAG system [5] as a base. The paper will present a theoretical computational framework for working within the Reluctant Paraphrase paradigm: three types of textual constraints are specified, effects of paraphrase on text are described, and a model incorporating mathematical optimisation techniques is outlined. The work this paper describes is done as part of...|$|R
40|$|PP Restricted {{to other}} {{programme}} participants (including the Commission Services) RE Restricted {{to a group}} specified by the consortium (including the Commission Services) CO Confidential, only {{for members of the}} consortium (including the Commission Services) Executive Summary This is the first deliverable of Work Package 2 Parallel patterns. Its main aim is to describe an initial set of parallel patterns provided to ParaPhrase users–the application programmers–to support the development of efficient parallel applications targeting heterogeneous architectures. This initial set of parallel patterns has been explicitly designed to be minimal, with two distinct but related goals in mind. On the one hand, it provides use case developers with an insight into the parallel application development methodology, without the need for them to learn a large set of parallel patterns: rather, they are presented with details of the most common parallelism exploitation patterns found in typical parallel applications. On the other hand, it provides <b>ParaPhrase</b> <b>system</b> programmers with a core set of patterns with which they can experiment with design, implementation and optimisatio...|$|R
40|$|We present PEM, {{the first}} fully {{automatic}} metric {{to evaluate the}} quality of paraphrases, and consequently, that of <b>paraphrase</b> generation <b>systems.</b> Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams {{that allows us to}} approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. ...|$|R
40|$|This paper {{presents}} {{a system to}} find automatically words from a definition or a <b>paraphrase.</b> The <b>system</b> uses a lexical database of French words that is comparable in its size to WordNet and an algorithm that evaluates distances in the semantic graph between hypernyms and hyponyms of {{the words in the}} definition. The paper first outlines the structure of the lexical network on which the method is based. It then describes the algorithm. Finally, it concludes with examples of results we have obtained. 1...|$|R
40|$|We {{present an}} {{approach}} for automatically learning syn-onyms from a corpus of paraphrased tweets. The syn-onyms are learned by using shallow parse chunks to cre-ate candidate synonyms and their context windows, and the synonyms are substituted {{back into a}} <b>paraphrase</b> de-tection <b>system</b> that uses machine translation metrics as features for a classifier. We find a 2. 29 % improvement in F 1 when we train and test on the paraphrase training set, demonstrating the importance of discovering high quality synonyms. We also find 9. 8 % better coverage of the paraphrase corpus using our synonyms rather than larger, existing synonym resources, demonstrating the power of extracting synonyms that {{are representative of the}} topics in the test set...|$|R
40|$|Abstract. While {{there are}} a number of {{subjectivity}} lexicons available for research purposes, none can be used commercially. We describe the process of constructing subjectivity lexicon(s) for recognizing sentiment polarity in essays written by test-takers, to be used within a commercial essay-scoring system. We discuss ways of expanding a manually-built seed lexicon using dictionary-based, distributional indomain and out-of-domain information, as well as using Amazon Mechanical Turk to help “clean up ” the expansions. We show the feasibility of constructing a family of subjectivity lexicons from scratch using a combination of methods to attain competitive performance with state-of-art research-only lexicons. Furthermore, this is the first use, to our knowledge, of a <b>paraphrase</b> generation <b>system</b> for expanding a subjectivity lexicon...|$|R
40|$|We {{present a}} new and unique {{paraphrase}} resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on Twitter which often express semantically identical information through distinct surface forms. We demonstrate the utility of this new resource {{on the task of}} paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art <b>paraphrase</b> and normalization <b>systems</b> 1. ...|$|R
40|$|We {{present the}} first fully {{unsupervised}} approach to metaphor interpretation, {{and a system}} that produces literal paraphrases for metaphorical expressions. Such a form of interpretation is directly transferable to other NLP applications that can benefit from a metaphor processing component. Our method is different from previous work in {{that it does not}} rely on any manually annotated data or lexical resources. First, our method computes candidate paraphrases according to {{the context in which the}} metaphor appears, using a vector space model. It then uses a selectional preference model to measure the degree of literalness of the <b>paraphrases.</b> The <b>system</b> identifies correct <b>paraphrases</b> with a precision of 0. 52 at top rank, which is a promising result for a fully unsupervised approach...|$|R
40|$|We {{address the}} text-to-text {{generation}} problem of sentence-level paraphrasing — a phenomenon distinct from {{and more difficult}} than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the <b>system</b> derives accurate <b>paraphrases,</b> outperforming baseline <b>systems.</b> ...|$|R
40|$|We sketch the {{architecture}} of a sentence gen- eration module that maps a languagesneutral "deep" rep'resentation to a language-specific sentence-semantic specification, which is given to a front-end generator. Lexicalizat, ion is the main instrmnent [br the maplfing step, and we examine the role o wM. semantics in the process. [n particular, we propose {{a set of rules}} that derive a range of verb alternations from a single base form, which is one source of lexical <b>paraphrasing</b> in the <b>system...</b>|$|R
40|$|SrRACT. The MARGIE {{system is}} a set of three {{programs}} that attempt to understand natural language. They are based on the Conceptual Dependency system for meaning representation. The analysis program maps sentences into conceptual structures. The memory program makes inferences from input conceptual structures. The generator codes conceptual structures back into natural language. Together the programs function as a <b>paraphrase</b> and inference <b>system.</b> tv. y WORDS AND PHRASES: natural language understanding, artificial intelligence, computational linguistics, semantic...|$|R
40|$|This paper {{describes}} FUN-NRC group’s {{machine translation}} sys-tems {{that participated in}} the NTCIR- 10 PatentMT task. The central motivation of this participation was to clarify the potential of auto-matically compiled collections of sub-sentential <b>paraphrases.</b> Our <b>systems</b> were built using our baseline phrase-based SMT system by augmenting its phrase table with novel translation pairs gener-ated by combining paraphrases with translation pairs learned di-rectly from the training bilingual data. We investigated two meth-ods for phrase table augmentation: source-side augmentation and target-side augmentation. Among the systems we submitted, the two that worked best were (a) the one that paraphrased only unseen phrases into translatable phrases at the source side and (b) the one that paraphrased target phrases only into phrases that were seen in the original phrase table. Both these systems were trained on not only bilingual, but also monolingual data. The other two systems were trained using only bilingual data. This paper also reports on our follow-up experiments focusing {{on the relationship between}} re-ordering restriction and system performance...|$|R
40|$|Paraphrase {{plagiarism}} is {{a significant}} and widespread problem and research shows {{that it is hard}} to detect. Several methods and automatic systems have been proposed to deal with it. However, evaluation and comparison of such solutions is not possible because of the unavailability of benchmark corpora with manual examples of paraphrase plagiarism. To deal with this issue, we present the novel development of a paraphrase plagiarism corpus containing simulated (manually created) examples in the Urdu language - a language widely spoken around the world. This resource is the first of its kind developed for the Urdu language and we believe that it will be a valuable contribution to the evaluation of <b>paraphrase</b> plagiarism detection <b>systems...</b>|$|R
5000|$|Harris {{factored}} {{the set of}} transformations into elementary sentence-differences, {{which could}} then be employed as operations in generative processes for decomposing or synthesizing sentences. These are of two kinds, the incremental operations which add words, and the paraphrastic operations which change the phonemic shapes of words. The latter, Harris termed [...] "extended morphophonemics". This led to a partition of the set of sentences into two sublanguages: an informationally complete sublanguage with neither ambiguity nor paraphrase, vs. the set of its more conventional and usable <b>paraphrases</b> ("The Two <b>Systems</b> of Grammar: Report and Paraphrase" [...] 1969). In the paraphrastic set, morphemes may be present in reduced form, even reduced to zero; their fully explicit forms are recoverable by undoing deformations and reductions of phonemic shape.|$|R
40|$|A lexicon is an {{essential}} component in a generation system but few e#orts {{have been made to}} build a rich, large-scale lexicon and make it reusable for di#erent generation applications. In this paper, we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes. Novel transformation and integration of resources is required to reuse them for generation. We also applied the lexicon to the lexical choice and realization component of a practical generation application by using a multi-level feedback architecture. The integration of the lexicon and the architecture is able to e#ectively improve the <b>system</b> <b>paraphrasing</b> power, minimize the chance of grammatical errors, and simplify the development process substantially. 1 Introduction Every generation system needs a lexicon, and in almost every case, it is acquired anew. Few efforts in building a rich, large-scale, and reusable generation lexicon have been [...] ...|$|R
40|$|Paraphrases and paraphrasing {{algorithms}} {{have been}} found of great importance in various natural language processing tasks. While most paraphrase extraction approaches extract equivalent sentences, sentences are an inconvenient unit for further processing, because they are too specific, and often not exact paraphrases. Paraphrase fragment extraction is a technique that post-processes sentential paraphrases and prunes them to more convenient phrase-level units. We present a new approach that uses semantic roles to extract paraphrase fragments from sentence pairs that share semantic content to varying degrees, including full paraphrases. In contrast to previous systems, the use of semantic parses allows for extracting paraphrases with high wording variance and different syntactic categories. The approach is tested on four different input corpora and compared to two previous <b>systems</b> for extracting <b>paraphrase</b> fragments. Our <b>system</b> finds {{three times as many}} good paraphrase fragments per sentence pair as the baselines, {{and at the same time}} outputs 30 % fewer unrelated fragment pairs...|$|R
40|$|International audienceWe have designed, {{implemented}} and assessed an EBMT {{system that can}} be dubbed the 'purest ever built': it strictly does not make any use of variables, templates or patterns, {{does not have any}} explicit transfer component, and does not require any preprocessing or training of the aligned examples. It only uses a specific operation, proportional analogy, that implicitly neutralises divergences between languages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on different tasks and language pairs. To begin with, we compared our system on two tasks of a previous MT evaluation campaign to rank it among other current state-of-the-art systems. Then, we illustrated the 'universality' of our system by participating in a recent MT evaluation campaign, with exactly the same core engine, {{for a wide variety of}} language pairs. Finally, we studied the in uence of extra data like dictionaries and <b>paraphrases</b> on the <b>system</b> performance...|$|R
40|$|Paraphrase {{plagiarism}} {{is one of}} {{the difficult}} challenges facing plagiarism detection <b>systems.</b> <b>Paraphrasing</b> occur when texts are lexically or syntactically altered to look different, but retain their original meaning. Most plagiarism detection systems (many of which are commercial based) are designed to detect word co-occurrences and light modifications, but are unable to detect severe semantic and structural alterations such as what is seen in many academic documents. Hence many paraphrase plagiarism cases go undetected. In this paper, we approached the problem of paraphrase plagiarism by proposing methods for detecting the most common techniques (phenomena) used in paraphrasing texts (namely; lexical substitution, insertion/deletion and word and phrase reordering), and combined the methods into a paraphrase detection model. We evaluated our proposed methods and model on collections containing paraphrase texts. Experimental results show significant improvement in performance when the methods were combined (the proposed model) as opposed to running them individually. The results also show that the proposed paraphrase detection model outperformed a standard baseline (based on greedy string tilling), and previous studies...|$|R
40|$|Thesis (Master's) [...] University of Washington, 2014 The goal of synonym {{extraction}} is {{to automatically}} gather synsets (groups of synonyms) from a corpus. This task {{is related to}} the tasks of normalization and paraphrase detection. We present a series of approaches for synonym extraction on Twitter, which contains unique synonyms (e. g. slang, acronyms, and colloquialisms) for which no traditional resources exist. Because Twitter contains so much variation, we focus our extraction on certain topics. We show that this focus on topics yields significantly higher coverage on a corpus of paraphrases than previous work which was topic-insensitive. We demonstrate improvement on the task of paraphrase detection when we substitute our extracted synonyms into the paraphrase training set. The synonyms are learned by using chunks from a shallow parse to create candidate synonyms and their context windows, and the synonyms are incorporated into a <b>paraphrase</b> detection <b>system</b> that uses machine translation metrics as features for a classifier. When we train and test on the paraphrase training set and use synonyms extracted from the same paraphrase training set, we find a 2. 29 % improvement in F 1 and demonstrate better coverage than previous systems. This shows the potential of synonyms that are representative of a specific topic. We also find an improvement in F 1 score of 0. 81 points when we train on the paraphrase training set and test on the test set and use synonyms extracted with an unsupervised method on a corpus whose topics match those of the paraphrase test set. We also demonstrate an approach that uses distant supervision, creating a silver standard training and test set, which we use both to evaluate our synonyms and to demonstrate a supervised approach to synonym extraction...|$|R

