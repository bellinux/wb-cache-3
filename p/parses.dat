1589|10000|Public
5|$|At compile time, the {{interpreter}} <b>parses</b> Perl code into a syntax tree. At run time, it executes {{the program by}} walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.|$|E
25|$|When AutoRun is disabled, Windows {{should not}} proceed further through the {{activation}} sequence than the Registry check. However, it <b>parses</b> any autorun.inf found and does {{everything except the}} final action to invoke AutoPlay or execute an application.|$|E
25|$|RowGen is {{designed}} to generate test data in production table, file, and report formats for prototype database population, compliance, outsourcing, and application prototyping projects. RowGen's GUI <b>parses</b> data models to define table layouts and relationships so database test sets are structurally and referentially correct. RowGen can also transform and format test data during its generation.|$|E
40|$|This paper explores joint {{syntactic}} and semantic <b>parsing</b> of Chinese {{to further}} improve {{the performance of}} both syntactic and semantic <b>parsing,</b> in particular the performance of semantic <b>parsing</b> (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated <b>parsing</b> approach is proposed to integrate semantic <b>parsing</b> into the syntactic <b>parsing</b> process. Secondly, semantic information generated by semantic <b>parsing</b> is incorporated into the syntactic <b>parsing</b> model to better capture semantic information in syntactic <b>parsing.</b> Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated <b>parsing</b> approach outperforms the pipeline <b>parsing</b> approach on n-best <b>parse</b> trees, {{a natural extension of}} the widely used pipeline <b>parsing</b> approach on the top-best <b>parse</b> tree. Moreover, it shows that incorporating semantic role-related information into the syntactic <b>parsing</b> model significantly improves the performance of both syntactic <b>parsing</b> and semantic <b>parsing.</b> To our best knowledge, this is the first research on exploring syntactic <b>parsing</b> and semantic role labeling for both verbal and nominal predicates in an integrated way. ...|$|R
5000|$|A <b>Parse</b> Thicket is a graph that {{represents}} the syntactic {{structure of a paragraph}} of text in natural language processing. A <b>Parse</b> Thicket includes <b>Parse</b> tree for each sentence for this paragraph plus some arcs for other relations between words other than syntactic. <b>Parse</b> thickets can be constructed for both constituency <b>parse</b> trees and dependency <b>parse</b> trees. The relations which link <b>parse</b> trees within a <b>Parse</b> Thicket are: ...|$|R
40|$|We {{discuss the}} {{computation}} of <b>parse</b> forests, i. e., compact representations of all syntax trees {{for a given}} sentence. Conventional <b>parsing</b> algorithms can be adapted in {{a simple way to}} produce <b>parse</b> forests. Recently, the subject of <b>parse</b> forests was brought up in the context of generalized LR(O) <b>parsing,</b> a new <b>parsing</b> method for general context-free grammars. It is widely held that this <b>parsing</b> method is particularly suited to producing <b>parse</b> forests. The contrary is true: generalized LR(O) parsers are poor tools for producing compact <b>parse</b> forests...|$|R
25|$|One {{strategy}} {{of dealing with}} ambiguous <b>parses</b> (originating with grammarians as early as Pāṇini) is to add yet more rules, or prioritize them so that one rule takes precedence over others. This, however, has the drawback of proliferating the rules, often {{to the point where}} they become difficult to manage. Another difficulty is overgeneration, where unlicensed structures are also generated.|$|E
25|$|Researchers {{have shown}} that this problem is intimately linked {{with the ability to}} parse language, and that those words that are easy to segment due to their high {{transitional}} probabilities are also easier to map to an appropriate referent. This serves as further evidence of the developmental progression of language acquisition, with children requiring an understanding of the sound distributions of natural languages to form phonetic categories, parse words based on these categories, and then use these <b>parses</b> to map them to objects as labels.|$|E
25|$|The Windows Search API {{can also}} be used to convert a search query written using Advanced Query Syntax (or Natural Query Syntax, the natural {{language}} version of AQS) to SQL queries. It exposes a method GenerateSQLFromUserQuery method of the ISearchQueryHelper interface. Searches {{can also be}} performed using the search-ms: protocol, which is a pseudo protocol that lets searches be exposed as an URI. It contains all the operators and search terms specified in AQS. It can refer to saved search folders as well. When such a URI is activated, Windows Search, which is registered as a handler for the protocol, <b>parses</b> the URI to extract the parameters and perform the search.|$|E
50|$|LR <b>parsing</b> extends LL <b>parsing</b> {{to support}} a larger range of grammars; in turn, {{generalized}} LR <b>parsing</b> extends LR <b>parsing</b> {{to support a}}rbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL <b>parsing</b> and LR <b>parsing,</b> respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR <b>parsing</b> {{was developed in the}} 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR <b>parsing</b> up to the present day.|$|R
40|$|This paper {{presents}} {{our work}} {{for participation in}} the 2012 CIPS-SIGHAN shared task of Traditional Chinese <b>Parsing.</b> We have adopted two multilingual <b>parsing</b> models – a factored model (Stanford <b>Parser)</b> and an unlexicalized model (Berkeley <b>Parser)</b> for <b>parsing</b> the Sinica Treebank. This paper also proposes a new Chinese unknown word model and integrates it into the Berkeley <b>Parser.</b> Our experiment gives the first result of adapting existing multilingual <b>parsing</b> models to the Sinica Treebank and shows that the <b>parsing</b> accuracy can be improved by our suggested approach...|$|R
50|$|The {{generated}} parser can <b>parse</b> inputs very efficiently by packrat <b>parsing.</b> The packrat <b>parsing</b> is the recursive descent <b>parsing</b> algorithm that is accelerated using memoization. By using packrat <b>parsing,</b> any input can be <b>parsed</b> in linear time. Without it, {{however, the}} resulting parser could exhibit exponential time {{performance in the}} worst case due to the unlimited look-ahead capability.|$|R
500|$|PokerTracker Software, LLC is {{the name}} of a poker tool {{software}} company that produces the popular PokerTracker line of poker tracking and analysis software. PokerTracker's software imports and <b>parses</b> the hand histories that poker sites create during online play and stores the resulting statistics/information about historical play into a local database library for self-analysis, and for in-game opponent analysis using a real-time Head-up display.|$|E
500|$|... <b>parses</b> and executes {{successfully}} in some environments (e.g. SQLite or PostgreSQL) which unify a NULL boolean with Unknown but fails to parse in others (e.g. in SQL Server Compact). MySQL behaves similarly to PostgreSQL {{in this regard}} (with the minor exception that MySQL regards TRUE and FALSE as {{no different from the}} ordinary integers 1 and 0). PostgreSQL additionally implements a IS UNKNOWN predicate, which can be used to test whether a three-value logical outcome is Unknown, although this is merely syntactic sugar.|$|E
500|$|Watson <b>parses</b> {{questions}} {{into different}} keywords and sentence fragments {{in order to}} find statistically related phrases. Watson's main innovation was not {{in the creation of a}} new algorithm for this operation but rather its ability to quickly execute hundreds of proven language analysis algorithms simultaneously. [...] The more algorithms that find the same answer independently the more likely Watson is to be correct. Once Watson has a small number of potential solutions, it is able to check against its database to ascertain whether the solution makes sense or not.|$|E
40|$|This paper {{proposes a}} method for {{evaluating}} the validity of partial <b>parse</b> trees constructed in incremental <b>parsing.</b> Our method is based on stochastic incremental <b>parsing,</b> and it incrementally evaluates the validity for each partial <b>parse</b> tree on a wordby-word basis. In our method, incremental parser returns partial <b>parse</b> trees {{at the point where}} the validity for the partial <b>parse</b> tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental <b>parsing.</b> ...|$|R
50|$|In {{computer}} science, top-down <b>parsing</b> is a <b>parsing</b> strategy {{where one}} first {{looks at the}} highest level of the <b>parse</b> tree and works down the <b>parse</b> tree by using the rewriting rules of a formal grammar. LL parsers are a type of parser that uses a top-down <b>parsing</b> strategy.|$|R
40|$|We {{present a}} novel {{framework}} that combines strengths from surface syntactic <b>parsing</b> and deep syntactic <b>parsing</b> to increase deep <b>parsing</b> accuracy, specifically by combining dependency and HPSG <b>parsing.</b> We show that by using surface dependencies to constrain {{the application of}} wide-coverage HPSG rules, we can benefit {{from a number of}} <b>parsing</b> techniques designed for highaccuracy dependency <b>parsing,</b> while actually performing deep syntactic analysis. Our framework results in a 1. 4 % absolute improvement over a state-of-the-art approach for wide coverage HPSG <b>parsing.</b> ...|$|R
500|$|In healthcare, Watson's natural language, {{hypothesis}} generation, and evidence-based learning {{capabilities are}} being investigated {{to see how}} Watson may contribute to clinical decision support systems for use by medical professionals. To aid physicians {{in the treatment of}} their patients, once a physician has posed a query to the system describing symptoms and other related factors, Watson first <b>parses</b> the input to identify the most important pieces of information; then mines patient data to find facts relevant to the patient's medical and hereditary history; then examines available data sources to form and test hypotheses; and finally provides a list of individualized, confidence-scored recommendations. The sources of data that Watson uses for analysis can include treatment guidelines, electronic medical record data, notes from physicians and nurses, research materials, clinical studies, journal articles, and patient information. Despite being developed and marketed as a [...] "diagnosis and treatment advisor", Watson has never been actually involved in the medical diagnosis process, only in assisting with identifying treatment options for patients who have already been diagnosed.|$|E
500|$|In {{modeling}} {{protein structure}} as of CASP6, Robetta first searches for structural homologs using BLAST, PSI-BLAST, and 3D-Jury, then <b>parses</b> the target sequence into its individual domains, or independently folding units of proteins, by matching the sequence to structural {{families in the}} Pfam database. [...] Domains with structural homologs then follow a [...] "template-based model" [...] (i.e., homology modeling) protocol. [...] Here, the Baker laboratory's in-house alignment program, K*sync, produces a group of sequence homologs, and each of these is modeled by the Rosetta de novo method to produce a decoy (possible structure). [...] The final structure prediction is selected by taking the lowest energy model as determined by a low-resolution Rosetta energy function. [...] For domains that have no detected structural homologs, a de novo protocol is followed in which the lowest energy model from a set of generated decoys is selected as the final prediction. [...] These domain predictions are then connected together to investigate inter-domain, tertiary-level interactions within the protein. [...] Finally, side-chain contributions are modeled using a protocol for Monte Carlo conformational search.|$|E
2500|$|Ambiguous grammar {{may result}} in {{ambiguous}} parsing if applied on homographs since the same word sequence can {{have more than one}} interpretation. Pun sentences such as the newspaper headline [...] "Iraqi Head Seeks Arms" [...] are an example of ambiguous <b>parses.</b>|$|E
40|$|Data-driven parsers rely on {{recommendations}} from <b>parse</b> models, which are generated from {{a set of}} training data using a machine learning classifier, to perform <b>parse</b> operations. However, in some cases a <b>parse</b> model cannot recommend a <b>parse</b> action to a parser unless it learns from the training data what <b>parse</b> action(s) to take in every possible situation. Therefore, {{it will be hard}} for a parser to make an informed decision as to what <b>parse</b> operation to perform when a <b>parse</b> model recommends no/several <b>parse</b> actions to a parser. Here we examine the effect of various deterministic choices on a datadriven parser when it is presented with no/several recommendation from a <b>parse</b> model...|$|R
40|$|Most {{previous}} studies {{need to learn}} a complex object model for <b>parsing</b> a specific object instance. This paper directly learns the general <b>parsing</b> patterns from the set of <b>parsed</b> objects and formalizes the <b>parsing</b> patterns {{as a series of}} <b>parsing</b> templates instead of learning the complex object model. Moreover, a novel hierarchical structure is presented to represent an object by using the <b>parsing</b> templates, which implicitly contains the multi-scale object parts and their relationships. For a single object, the <b>parsing</b> process is equivalent to establishing its hierarchical representation and determining the <b>parsing</b> template for each node. We combine the top-down decomposing scheme and the bottom-up composing scheme to infer the <b>parsing</b> process and formalize the inference as an energy minimization problem. The effect of our method is demonstrated by <b>parsing</b> the human body with aggressive pose variations. Compared with the state-of-the-art methods, the <b>parsing</b> results are more satisfying...|$|R
40|$|We propose an {{algebraic}} {{method for}} the design of tabular <b>parsing</b> algorithms which uses <b>parsing</b> schemata [7]. The <b>parsing</b> strategy is expressed in a tree algebra. A <b>parsing</b> schema is derived from the tree algebra by means of algebraic operations such as homomorphic images, direct products, subalgebras and quotient algebras. The latter yields a tabular interpretation of the <b>parsing</b> strategy. The proposed method allows simpler and more elegant correctness proofs by using general theorems and is not limited to left-right <b>parsing</b> strategies, unlike current automaton-based approaches. Furthermore, it allows to derive <b>parsing</b> schemata for linear indexed grammars (LIG) from <b>parsing</b> schemata for context-free grammars by means of a correctness preserving algebraic transformation. A new bottom-up head corner <b>parsing</b> schema for LIG is constructed to demonstrate the method...|$|R
2500|$|... <b>parses</b> this chord (in bars 206 and 208) as a “diminished nineteenth… a searingly dissonant {{dominant}} harmony containing {{nine different}} pitches. [...] Who knows what Guido Adler, {{for whom the}} second and Third Symphonies already contained ‘unprecedented cacophonies’, might have called it?” ...|$|E
2500|$|Each of the {{language}} compilers is a separate program that reads source code and outputs machine code. All have a common internal structure. A per-language front end <b>parses</b> the source code in that language and produces an abstract syntax tree ("tree" [...] for short).|$|E
2500|$|An {{example of}} a parser for PCFG grammars is the pushdown automaton. The {{algorithm}} <b>parses</b> grammar nonterminals {{from left to right}} in a stack-like manner. This brute-force approach is not very efficient. In RNA secondary structure prediction variants of the Cocke–Younger–Kasami (CYK) algorithm provide more efficient alternatives to grammar parsing than pushdown automata. Another {{example of a}} [...] PCFG parser is the Stanford Statistical Parser which has been trained using Treebank,.|$|E
30|$|For <b>parsing</b> English phrases, {{currently}} Link Grammar <b>Parser</b> [14] and Stanford <b>Parser</b> [15] (a lexicalized Probabilistic Context-Free Grammar (PCFG)) {{are two of}} {{the best}} semantic parsers. Link Grammar <b>Parser</b> is a rule-based analyzer, which is essential to obtain accurate results. However, a statistical analyzer parser like Stanford <b>Parser,</b> which is written in Java, is more tolerant with both words and constructions, which are not grammatically correct. Even if there are grammatical errors (e.g., “Parents always does loves their childs”.), a <b>parse</b> tree still can be created by the Stanford <b>Parser.</b> For this reason, we used Stanford <b>Parser</b> to analyze grammatical structure of input sentences.|$|R
40|$|We {{investigated}} the performance e#cacy of beam search <b>parsing</b> and deep <b>parsing</b> techniques in probabilistic HPSG <b>parsing</b> using the Penn treebank. We first tested the beam thresholding and iterative <b>parsing</b> developed for PCFG <b>parsing</b> with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid <b>parsing</b> with a CFG chunk parser. The {{contributions of the}} large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total <b>parsing</b> performance. The precision, recall and average <b>parsing</b> time for the Penn treebank (Section 23) were 87. 85 %, 86. 85 %, and 360 ms, respectively...|$|R
40|$|GML data {{is widely}} used for model building, data exchanging, etc. The GML data <b>parsing</b> is the base of {{handling}} other operation of GML. The <b>parsing</b> technology of XML {{can be used for}} GML <b>parsing.</b> But the XML <b>parsing</b> technology is deficient in <b>parsing</b> semantic information on geography information. This paper tries to build a semantic information database (SIDB) of GML and design GML core schema-based <b>parsing</b> engine which based on SIDB. Ultimately actualize GML data <b>parsing.</b> The results of the study are verified by GML test data in the paper. And more, this study provides a new way to <b>parsing</b> semantic information in other fields...|$|R
2500|$|The [...] {{function}} accepts textual S-expressions as input, and <b>parses</b> {{them into}} an internal data structure. For instance, if you type the text [...] at the prompt, [...] translates this into a linked list with three elements: the symbol , the number 1, and the number 2. It so happens that this list is also a valid piece of Lisp code; that is, it can be evaluated. This is because the car of the list names a function—the addition operation.|$|E
2500|$|This {{statement}} {{summarizes the}} {{role for the}} court envisioned by Originalists, that is, that the Court <b>parses</b> what the general law and constitution says of a particular case or controversy, and when questions arise as {{to the meaning of}} a given constitutional provision, that provision should be given the meaning it was understood to mean when ratified. Reviewing Steven D Smith's book Law's Quandary, Justice Scalia applied this formulation to some controversial topics routinely brought before the Court: ...|$|E
2500|$|MSHTML.dll {{houses the}} Trident {{rendering}} engine [...] introduced in Internet Explorer 4, {{which is responsible}} for displaying the pages on-screen and handling the Document Object Model of the web pages. MSHTML.dll <b>parses</b> the HTML/CSS file and creates the internal DOM tree representation of it. It also exposes a set of APIs for runtime inspection and modification of the DOM tree. [...] The DOM tree is further processed by a layout engine which then renders the internal representation on screen.|$|E
40|$|We present Pro 3 Gres, a fast robust broad-coverage and deep-linguistic parser {{that has}} been applied to and {{evaluated}} on unrestricted amounts of text from unrestricted domains. We show that it is largely cognitively adequate. We argue that Pro 3 Gres contributes to closing the gap between psycholinguistics and language engineering, between probabilistic <b>parsing</b> and formal grammar-based <b>parsing,</b> between shallow <b>parsing</b> and full <b>parsing,</b> and between deterministic <b>parsing</b> and non-deterministic <b>parsing.</b> We also describe a successful application of Pro 3 Gres for <b>parsing</b> research texts from the BioMedical domain...|$|R
40|$|<b>Parsing</b> schemata are {{high-level}} {{descriptions of}} <b>parsing</b> algorithms. This paper {{is concerned with}} <b>parsing</b> schemata for different grammar formalisms. We separate the description of <b>parsing</b> steps from that of grammatical properties by means of abstract <b>parsing</b> schemata. We define an abstract Earley schema and prove it correct. We obtain Earley schemata for several grammar formalisms by specifying the grammatical properties in the abstract Earley schema. Our approach offers a clear and well-defined interface between a <b>parsing</b> algorithm and a grammar. Moreover, it provides a precise criterion for the classification of <b>parsing</b> algorithms...|$|R
40|$|Given a <b>parse</b> {{tree for}} a {{sentence}} xzy {{and a string}} Z, an incremental parser builds the <b>parse</b> tree for the sentence xZy by reusing {{as much of the}} <b>parse</b> tree for xzy as possible. The incremental LL(1) <b>parsing</b> algorithm in this paper makes use of a break-point table to identify reusable subtrees of the original <b>parse</b> tree in building the new <b>parse</b> tree. The break-point table may be computed from the grammar...|$|R
