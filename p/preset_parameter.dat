8|92|Public
40|$|Education is {{the only}} major weapon that enables nations to {{progress}} socio-economically and gives a huge rise to their human development index. More so, it {{is considered to be}} the most fundamental requirement for poverty eradication, for addressing socio-political issues and for stabilizing peace and harmony among nations. Education is a lifelong process and does not have a <b>preset</b> <b>parameter</b> for its accessibility and provision (Sharkey, 1998). Learning is innate and natural and the human mind is set to learn since its conception. Additionally, in the recent era, investigations are focused on education and learning even before birth and extensive emphasis is now paid on experimenting on prenatal education by providing pre-birth interventions for babies inside the womb (Hepper, 1991; Kleindorfer & Robertson, 2013; Partanen, et al., 2013; Sharkey, 1998). ...|$|E
40|$|A {{process is}} {{declared}} for {{the recognition of}} persons located on a continuous conveyor used in underground mines, in particular a conveyor belt, by which a running conveyor belt is monitored by means of video signals recorded by a stationary video camera, by means of which a person on the conveyor belt can be reliably and automatically recognized in a monitored danger area. Said process {{is characterized by the}} fact that video signals comprising the load condition of the conveyor belt (11) are generated by means of a light source (13) arranged vertically above the conveyor belt (11) for the emission of line light and a video camera (14) in the light section inclined at a lateral angle to the conveyor belt (11) and that, after the conversion of said video signals in an evaluation unit (17) into a suitable form for their processing by a computer, they are compared with each other over the consecutive length sections of the conveyor belt (11), and that, on deviation of the actual values abo ve a <b>preset</b> <b>parameter,</b> a signal is triggered to stop the conveyor belt (11) ...|$|E
40|$|International audienceVoltammetry is {{a method}} able to {{distinguish}} in certain degree the speciation of dissolved metals. An analysis {{of its ability to}} discern composite and more complex dissolved metal–ligand systems has been carried out by simulating the experiments for determination of metal–ligand complexing parameters. Logarithmic equidistant addition of metal was presumed, covering 2. 5 decades. The data obtained with the <b>preset</b> <b>parameter</b> values were subsequently fitted to the presumed models. Data points under the detection limit DL = 10 − 10 mol L − 1 were eliminated and random noise following a realistic shape was added to the points to approach them to the real experiment. Four models were applied for simulation and up to five models for fitting. The analysis of the results shows that with the nowadays state-of-the-art measurement and data treatment techniques, in most of the cases it was possible to distinguish more complex and also more probable bi-ligand and mixed metal–ligand complexes from the simpler 1 : 1 metal–ligand systems. Statistical evidences to validate the right model were given. Its applicability has been confirmed by generating a similar data mining server (DMS) rule...|$|E
50|$|In {{a pledge}} fund, the investors provide a loose {{commitment}} of capital to an investment team, {{the manager of}} the fund, to make investments within certain <b>preset</b> <b>parameters.</b> Thereafter, the investors must approve each transaction and will decide whether to pursue each transaction independently.|$|R
40|$|This paper {{illustrates}} {{the process of}} study and choice regarding a sensor for motioning of walking robots. Typical for all walking machines is the need of a effective force-to-weight ratio. Based on <b>preset</b> <b>parameters</b> a sensor prototype for a six-legged mobile robot joint had been chosen. To meet the requirements all possibilities for reaching the goal by using optrons with air entrance, integral illumination-frequency commuter and photodiode lineal had been investigated. Explicated are the characteristics and the basic conceivable parameters of diverse variants...|$|R
40|$|This study aims {{to develop}} some models to aid in making {{decisions}} on the combined fleet size and vehicle assignment in working service network where the demands include two types (minimum demands and maximum demands), and vehicles themselves can act like a facility to provide services when they are stationary at one location. This type of problem is named as the dynamic working vehicle scheduling with dual demands (DWVS-DD) and formulated as a mixed integer programming (MIP). Instead of a large integer program, the problem is decomposed into small local problems that are guided by <b>preset</b> control <b>parameters.</b> The approach for <b>preset</b> control <b>parameters</b> is given. By introducing them into the MIP formulation, the model is reformulated as a piecewise form. Further, a piecewise method by updating <b>preset</b> control <b>parameters</b> is proposed for solving the reformulated model. Numerical experiments show that the proposed method produces better solution within reasonable computing time...|$|R
40|$|The, {{developed}} in this study, simple model and numerical solution of diffusion {{growth of the}} solid phase under the conditions of directional solidification allow for the effect of constituent diffusion in both liquid and solid phase and assume the process run in which (like in reality) the <b>preset</b> <b>parameter</b> is the velocity of sample (pulling velocity) at a preset temperature gradient. The solid/liquid interface velocity is not the process parameter (like it is in numerous other solutions proposed so far) but a function of this process. The effect of convection outside the diffusion layer has been included in mass balance {{under the assumption that}} in the zone of convection the mixing is complete. The above assumptions enabled solving the kinetics of growth of the solid phase (along with the diffusion field in solid and liquid phase) under the conditions of diffusion well reflecting the process run starting with the initial transient state, going through the steady state period in central part of the casting, and ending in a terminal transient state. In the numerical solution obtained by the finite difference method with variable grid dimensions, the error of the mass control balance over the whole process range was 1 - 2 %...|$|E
40|$|The aim of {{the study}} was to {{calculate}} the effective dose during fluoroscopy-guided pediatric interventional procedures of the liver in a phantom model before and after adjustment of preset parameters. Organ doses were measured in three anthropomorphic Rando-Alderson phantoms representing children at various age and body weight (newborn 3. 5 kg, toddler 10 kg, child 19 kg). Collimation was performed focusing on the upper abdomen representing mock interventional radiology procedures such as percutaneous transhepatic cholangiography and drainage placement (PTCD). Fluoroscopy and digital subtraction angiography (DSA) acquisitions were performed in a posterior-anterior geometry using a state of the art flat-panel detector. Effective dose was directly measured from multiple incorporated thermoluminescent dosimeters (TLDs) using two different parameter settings. Effective dose values for each pediatric phantom were below 0. 1 mSv per minute fluoroscopy, and below 1 mSv for a 1 minute DSA acquisition with a frame rate of 2 f/s. Lowering the values for the detector entrance dose enabled a reduction of the applied effective dose from 12 to 27 % for fluoroscopy and 22 to 63 % for DSA acquisitions. Similarly, organ doses of radiosensitive organs could be reduced by over 50 %, especially when close to the primary x-ray beam. Modification of <b>preset</b> <b>parameter</b> settings enabled to decrease the effective dose for pediatric interventional procedures, as determined by effective dose calculations using dedicated pediatric Rando-Alderson phantoms...|$|E
40|$|There is continual {{pressure}} on the radiology department to increase its productivity. Two important links to productivity in the computed/digital radiography (CR/DR) workflow chain are the postprocessing step by technologists and the primary diagnosis step by radiologists, who may apply additional image enhancements to aid them in diagnosis. With the large matrix size of CR and DR images and the computational complexity of these algorithms, it has been challenging to provide interactive image enhancement, particularly on full-resolution images. We have used a new programmable processor as the main computing engine of enhancement algorithms for CR or DR images. We have mapped these algorithms to the processor, maximally utilizing its architecture. On a 12 -bit 2688 × 2688 image, we have achieved the execution time of 465 Â ms for adaptive unsharp masking, window/level, image rotate, and lookup table operations using a single processor, which represents at least {{an order of magnitude}} improvement compared to the response time of current systems. This kind of performance facilitates rapid computation with <b>preset</b> <b>parameter</b> values and/or enables truly interactive QA processing on radiographs by technologists. The fast response time of these algorithms would be especially useful in a real-time radiology setting, where the radiologist’s waiting time in performing image enhancements before making diagnosis can be greatly reduced. We believe that the use of these processors for fast CR/DR image computing coupled with the seamless flow of images and patient data will enable the radiology department to achieve higher productivity...|$|E
40|$|Analysis and {{comparison}} of mutational spectra represents a burning question in molecular biology. We report an algorithm {{based upon the}} SEM subclass approach (SEM- stochastique, estimations, maximi-zations). Any real mutational spectrum {{is regarded as a}} mixture of standard binomial distributions. The separation proc~ure is run by rounds. Each iteration includes simulation, maximization and estimation. The algorithm has been checked on random spectra with the <b>preset</b> <b>parameters</b> and on real mutational spectra. As has been shown, any real mutational specmlrn can be represented as a mixture oftwo and more binomial distributions, f which one contains hotspots ofmutation...|$|R
40|$|The method {{involves}} the deposition of individual or numerous NANO-particles on the substrate after which selected particles are fused {{with each other}} and/or with the substrate. The fusion takes places preferably after positioning selected NANO-particles defining a functional structure, or an electronic component. Typically the selective positioning is carried out by using a raster or grid probe microscopy, whose voltage pulse duration, amplitude, and polarity is adjusted according to <b>preset</b> <b>parameters.</b> A raster probe, or power microscope may be used. The fusing may comprise controlled voltage and current increase. ADVANTAGE - Rational manufacturing facility with reduced capability to produce flaw...|$|R
40|$|Background: With the {{emerging}} role of digital imaging in pathology {{and the application}} of automated image-based algorithms to a number of quantitative tasks, {{there is a need to}} examine factors that may affect the reproducibility of results. These factors include the imaging properties of whole slide imaging (WSI) systems and their effect on the performance of quantitative tools. This manuscript examines inter-scanner and inter-algorithm variability in the assessment of the commonly used HER 2 /neu tissue-based biomarker for breast cancer with emphasis on the effect of algorithm training. Materials and Methods: A total of 241 regions of interest from 64 breast cancer tissue glass slides were scanned using three different whole-slide images and were analyzed using two different automated image analysis algorithms, one with <b>preset</b> <b>parameters</b> and another incorporating a procedure for objective parameter optimization. Ground truth from a panel of seven pathologists was available from a previous study. Agreement analysis was used to compare the resulting HER 2 /neu scores. Results: The results of our study showed that inter-scanner agreement in the assessment of HER 2 /neu for breast cancer in selected fields of view when analyzed with any of the two algorithms examined in this study was equal or better than the inter-observer agreement previously reported on the same set of data. Results also showed that discrepancies observed between algorithm results on data from different scanners were significantly reduced when the alternative algorithm that incorporated an objective re-training procedure was used, compared to the commercial algorithm with <b>preset</b> <b>parameters.</b> Conclusion: Our study supports the use of objective procedures for algorithm training to account for differences in image properties between WSI systems...|$|R
40|$|Introduction. In {{production}} and repairs of pipeline armature grinding (debugging) is {{considered as one}} of the major technological operations. The main task is the providing of impermeability of breech-block. Whatever problems did not arise up in the achievement of impermeability, diagnosis of reason, practically, always one - the process of grinding in of fine surfaces is well not enough conducted. There is a large stake of truth in such answer, however, its not all and problem not only in grinding in. Grinding in is the finish operation of polishing of compressions and effective of its application depends not only on the exact observance of the recommended terms and modes of process. A major value of the the stages is the forming of quality and preceding to grinding in of the operation of treatment of compressions. If prior actions are executed off grade, then efficiency of realization of portable radio operations of grinding in will be. Materials and Methods. To the article a growing requirement is driven in the improvement of quality, increment of productivity and increment of longevity and reliability of machines and wares. The process of grinding (polishing) in allows to get the surfaces of processed details with high quality descriptions. Quality of implementation of finishing operation is estimated on following criteria: it is exactly in size, it is an error of form, they are indices of waviness of surface, indices of roughness of surface, the light reflect¬ing ability and quality descriptions of surface layer. For renewal of corps of wedge bolt by a main task providing of impermeability of breech-block. For its implementation hard requirements are produced, namely; a small roughness of surface, form and location. Thus fine surface of corps of wedge bolt must be homogeneous. Results. In order to attain the set roughness of fine surface, the trajectory of motion of instrument must have certain character. Because on this machine-tool a receipt of <b>preset</b> <b>parameter</b> of quality on all area of the processed surface is the main task of instrument accomplishing difficult rotatory motion that consists of: main motion is this rotation of disk on that instrument, is envisaged, directly from an engine; motion providing even treatment is this rotation of instrument about the axis. For the decision of this task the trajectory of motion of instrument is determined and a calculation is executed on determination of angulator of instrumental head. Discussion and Conclusions. Results conducted theoretical and experimental research method of treatment of sealing surfaces, and also developed on the basis of recommendation on the choice of the effective mode and conditions treatment allowed to modernize the technological process of the abrasive polishing. Thus, application of the modernized technology (finishing operation with difficult motion of instrument relatively processed surfaces) as compared to base provides the achievement of parameter roughness of Ra = 0, 08 micrometer. The obtained data testify the possibility of wide application method of finishing treatment of surfaces for the once-personal types of repair plug-forming...|$|E
40|$|The main {{aim of the}} {{bachelor}} thesis is to design a mobile horizontal woodlogs splitter according to <b>preset</b> <b>parameters.</b> The part before the main design of a splitter is focused on different types of mobile splitters with the subsequent focus on the individual structural design of horizontal splitters. The next part is dedicated to design and calculation of a hydraulic circuit of the splitter that generates the required cutting force. There is a calculation of final splitting force and other working properties for all individual components of hydraulic circuit. The individual components of the splitter are stressed by the splitting force. There is a strength check of exposed parts in {{the last part of}} the thesis. The work also includes drawings of selected parts...|$|R
40|$|Physically based dynamic {{models are}} able to {{describe}} variable shapes without prior training. Their behaviour to find an object is intuitive, which facilitates corrections of false results. Expressing shape variation as physical feature, however, may be difficult because the physics of the model {{has little to do}} with the shape variation of instances of a class of objects. We present a dynamic model, which automatically adapts model parameters based on results of previous segmentations. The model was applied to artificial data and to images of leaves. Results show that the adapted model finds the correct shape more accurate than a model with <b>preset</b> <b>parameters.</b> Investigation of the parameterisation from adaptation also showed that they may be interpreted in terms of the semantics of the shape class represented. 1...|$|R
40|$|In this paper, we {{introduce}} a new dataset, Kimia Path 24, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1, 325 test patches of size 1000 × 1000 (0. 5 mm× 0. 5 mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27, 000 to over 50, 000 patches if the <b>preset</b> <b>parameters</b> are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41. 80 % for CNN. Comment: Accepted for presentation at Workshop for Computer Vision for Microscopy Image Analysis (CVMI 2017) @ CVPR 2017, Honolulu, Hawai...|$|R
5|$|When {{the time}} came {{for the company to}} develop a new broiler, it turned to its {{equipment}} manufacturer, Nieco, and St. Louis, Missouri-based Duke Manufacturing. Burger King's goal was to maintain the company's trademark flame broiling method while allowing more product options on a flexible cooking platform. The solutions that Nieco and Duke devised met that goal by using control features during cooking. The cooking methods employed by the two manufacturing companies vary in their methods; Nieco employed two chains, one that maintained a single speed and cooking temperature, and another that had a flexible speed setting and variable temperature control. Duke's solution utilizes an oven that cooks according to <b>preset</b> <b>parameters</b> for time and temperature, one heterogeneous product batch at a time. The first batch-style broiler was introduced in April 1999 and was tested in-store in central Wisconsin during the summer of 1999.|$|R
40|$|The goal of {{this paper}} is to define the {{processes}} that enable the simulation of an autonomous pedestrian’s journeys in confined spaces; and to determine their paths beginning with random processes under <b>preset</b> <b>parameters</b> in a generic fashion as well as doing so in a manner independent from walking movement. We present a conceptual model designed to generate pedestrian journeys, taking into account the focuses of attraction and their influence in the paths taken. In order to implement this model, we add to an architectural blueprint the elements necessary to recognize the geometric characteristics of the World (Limits, Focuses and Portals) that affect the displacements of pedestrians. These characteristics enable us to determine the paths to be taken in the World. The issues of the simulation (the position that each pedestrian occupies for unit of time) can be used by other applications, to generate animations or to verify critical situations...|$|R
40|$|A {{global network}} of small {{automated}} telescopes, the Taiwan Automated Telescope (TAT) network, dedicated to photometric measurements of stellar pulsations, is under construction. Two telescopes have been installed in Teide Observatory, Tenerife, Spain and Maidanak Observatory, Uzbekistan. The third telescope will be installed at Mauna Loa Observatory, Hawaii, USA. Each system uses a 9 -cm Maksutov-type telescope. The effective focal length is 225 [*]cm, corresponding to an f-ratio of 25. The {{field of view}} is 0. 62 degree square. The images are taken with a 16 -bit 1024 × 1024 CCD camera. The telescope is equipped with UBVRI filters. Each telescope is fully automated. The telescope can be operated either interactively or fully automatically. In the interactive mode, it can be controlled through the Internet. In the fully automatic mode, the telescope operates with <b>preset</b> <b>parameters</b> without any human care, including taking dark frames and flat frames. The network {{can also be used}} for studies that require continuous observations for selected objects...|$|R
40|$|We {{present in}} this paper a two-years long {{experience}} of cooperation between industry and research for the resolution of a real world problem. After {{the presentation of the}} industrial context, we develop the theoretical and experimental studies and results in order to find the best architecture of neural networks to <b>preset</b> <b>parameters</b> of a temper mill machine in steel industry. We discuss on these results and compare with the results obtained by a the cascade-correlation learning architecture [2] results and the physical model results. Then we conclude on future work. Submitted category: 1 st choice: techniques & methods. 2 nd choice: applications. A Perceptron with Optimized Backpropagation Learning Algorithm to preset a Temper mill machine: NEUROSKIN Nicolas PICAN 1, Patrick BRESSON 2, Frédéric ALEXANDRE 1, Jean-Paul HATON 1,Eric COURIOT 2 1 CRIN-CNRS / INRIA Lorraine BP 239 F- 54506 VANDOEUVRE-les-NANCY Cedex pican@loria. fr % 83. 59. 20. 53 falex@loria. fr % 83. 59. 20 [...] ...|$|R
40|$|Steady-state {{and dynamic}} {{simulation}} of Preflash and Atmospheric column (Pipestill) {{in a real}} crudeoil distillation plant was performed using ASPEN simulations. Steady-state simulation resultsobtained by ASPEN plus were compared to real experimental data. Experimental ASTM D 86 curvesof different products were compared to those obtained by simulations. Influence of the steam flowrate in the side stripers and of the heat flow removed in pumparounds was analyzed. Significantinfluence of the heat flow removed in pumparounds on the flow conditions in the column and onthe product composition is explained. Steady-state flowsheet was completed by dynamic simulation requirements and exported to ASPENDynamics for simulations in dynamic mode. The behavior of the products flow rate in dynamicregime was observed after changing the crude oil feed by 10 %. Two different control methodsbased on composition (ASTM D 86 95 % boiling point) and temperature at the second stage wereapplied in dynamic simulation. The time needed to reach a new steady-state, deviation, and abilityof reaching the <b>preset</b> <b>parameters</b> of both methods were compared...|$|R
40|$|Introduction.  In {{standard}} {{planning for}} corrective hip osteotomy, a surgical intervention scheme is created on a uniplanar paper medium {{on the basis}} of X-ray images. However, uniplanar skiagrams are unable to render real spatial configuration of the femoral bone. When combining three-dimensional and uniplanar models of bone, human errors inevitably occur, causing the distortion of <b>preset</b> <b>parameters,</b> which may lead to glaring errors and, as a result, to repeated operations. Aims.  To develop a new three-dimensional method for planning and performing corrective osteotomy of the femoral bone, using visualizing computer technologies. Materials and methods.  A new method of planning for corrective hip osteotomy in children with various hip joint pathologies was developed. We examined the method using 27 patients [aged 5 – 18 years (32 hip joints) ] with congenital and acquired femoral bone deformation. The efficiency of the proposed method was assessed in comparison with uniplanar planning using roentgenograms. Conclusions.  Computerized operation planning using three-dimensional modeling improves treatment results by minimizing the likelihood of human errors and increasing planning and surgical intervention  accuracy...|$|R
40|$|The work {{is aimed}} at {{studying}} {{the role of the}} major proteins of the cellular walls of yeast Candida utilis is supporting the entire structure of this organelle. Isolated and partially characterized have been two new proteins of the cellular wall of yeast C. utilis. For the first time, the data have been received, which indicate of the possibility of existence of the yeast structural proteins, attaching the separate molecules of polysaccharides or even the large polysaccharide units into a single structure of the cellular wall. The new model of the yeast cellular wall structere has been offered. The obtained data are useful for solving the practical questions: cultivating the yeast with the <b>preset</b> <b>parameters</b> of the cellular wall, the treatment and prophylaxis of the mycoses of animals and man, the creation of the new immunostimulating preparations, containing the components of the cellular wall of yeast for treatment of the different diseases, including the oncological onesAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
3000|$|... where β {{controls}} {{the weight of}} the penalty term and ρ is a <b>parameter</b> <b>preset</b> before training, typically very small [37]. More details about the sparse autoencoder can be found in online lecture notes [38, 39].|$|R
40|$|Urban {{industrial}} water system parameter fluctuation producing uncertainty may {{not occur in}} a control input channel, can be applied mismatching uncertain system to describe. Based on Lyapunov direct method and linear matrix inequality, design the urban {{industrial water}} mismatching uncertain system feedback stabilization robust control scheme. Avoid the defects that the feedback stabilization control method based on the matrix Riccati equation need to <b>preset</b> equation <b>parameters,</b> easier to solve and can reduce the conservative...|$|R
50|$|The memory, which stores all of {{the user}} <b>presets</b> and <b>parameters,</b> can be dumped via MIDI to any sequencer. The dump is not MIDI System Exclusive (SysEx); it is encoded into {{controller}} changes and other common MIDI data, which any sequencer can handle. This may seem somewhat unusual, but in {{the day of the}} Voyetra, MIDI was in its infancy, and SysEx was not yet a proven strategy for transmitting and storing system information.|$|R
40|$|National Defense Basic Scientific Research {{program of}} China [B 1420110155]; National Natural Science Foundations of China under Natural Science Foundation of Fujian {{province}} [61202144, 2009 J 05153]; Fundamental Research Funds for the Central Universities [2010121065]; Foundation of Key Laboratory of System Control and Information Processing; Ministry of Education, P. R. China [SCIP 2011004]Gridding {{is the first}} and most important step to separate the spots into distinct areas in microarray image analysis. Human intervention is necessary for most gridding methods, even if some so-called fully automatic approaches also need <b>preset</b> <b>parameters.</b> The applicability of these methods is limited in certain domains and will cause variations in the gene expression results. In addition, improper gridding, which is influenced by both the misalignment and high noise level, will affect the high throughput analysis. In this paper, we have presented a fully automatic gridding technique to break through the limitation of traditional mathematical morphology gridding methods. First, a preprocessing algorithm was applied for noise reduction. Subsequently, the optimal threshold was gained by using the improved Otsu method to actually locate each spot. In order to diminish the error, the original gridding result was optimized according to the heuristic techniques by estimating the distribution of the spots. Intensive experiments on six different data sets indicate that our method is superior to the traditional morphology one and is robust in the presence of noise. More importantly, the algorithm involved in our method is simple. Furthermore, human intervention and <b>parameters</b> <b>presetting</b> are unnecessary when the algorithm is applied in different types of microarray images...|$|R
40|$|Motivated by {{emerging}} cooperative P 2 P {{applications that}} go beyond file-sharing, we study new uplink allocation algorithms for substituting the rate-based choke/unchoke algorithm of BitTorrent which becomes inefficient in these cases. Our goal is to reduce further the download times. We do so by improving the uplink utilization when it is mostly challenged: in young torrents, and when there exist downlink and network bottlenecks. We develop a new family of uplink allocation algorithms which we call BitMax, to stress {{the fact that they}} allocate to each unchoked node the maximum rate it can sustain, instead of an 1 /(k+ 1) equal share as done in the existing BitTorrent. BitMax computes in each interval the number of nodes to be unchoked, and the corresponding allocations, and thus does not need any empirically <b>preset</b> <b>parameters</b> like k. We demonstrate experimentally that Bit-Max can reduce significantly the download time in a typical reference scenario involving mostly ADSL nodes. We also consider scenarios involving network bottlenecks caused by filtering of P 2 P traffic at ISP peering points and show that BitMax retains its gains also in these cases...|$|R
40|$|On 11 August 1994, strong {{thunderstorms}} developed along {{a stationary}} surface boundary in northern Virginia, resulting in substantial flooding. This study primarily {{focuses on the}} performance of the Precipitation Processing Subsystem (PPS) of the Weather Surveillance Radars- 1988 Doppler (WSR- 88 D) in Wakefield, Virginia and Dovel; Delaware. The event occurred equidistant (90 - 120 nm) from both radars. Base reflectivity, velocity products and rain gage reports were compared with archived precipitation products from both radars to assess the performance of the PPS. The analysis indicated that the overall performance of the PPS output was very reliable but showed some inaccuracies. These inaccuracies may have been due to the distance of the thunderstorms from the radar, the <b>preset</b> <b>parameters</b> within the PPS, and possibly the methods by which the radar processes the data. Comparison of final storm totals and adjustable parameters, specifically maximum reflec-tivity in dBZ for conversion to rainfall rate and the maxi-mum reflectivity allowed for being labeled as an outlier from the respective Doppler radar systems, illustrate the need for adjustment of these parameters based on the char-acteristics of a given precipitation system. 1...|$|R
40|$|The {{distribution}} of environmental {{features of the}} internal ruins which are formed by a randomly seismic disaster is unpredictable. Therefore, the existing methods of map segmentation, which need to <b>preset</b> <b>parameters,</b> cannot be directly used. Considering the lack of prior knowledge, a map segmentation method based on the spectral clustering is proposed {{in the framework of}} hierarchical simultaneous localization and mapping (SLAM) algorithm. The method solves the problem of incremental complexity of SLAM algorithm using the division of environment. In accordance with the similarity of observed environment, a weighted graph is established. The nodes in the graph are generated by measuring the expected information gain and position redundancy. Then, the graph is partitioned into subjective results of map segment based on the criterion of minimum normalized cut. On the basis of the inherent sparse of SLAM, the proposed algorithm not only reduces the cost of calculation, but also minimizes the loss of information in order to ensure the global consistency. Finally, the feasibility and effectiveness of the algorithm are verified by simulation and experiment. © 2016 Taylor and Francis and The Robotics Society of Japan...|$|R
40|$|The {{model of}} {{communication}} aims {{to improve the}} internal perception of the employees about their working institutions, and improve the external perception of the institution {{in the eyes of}} co-workers and the general population. The analysis of the model of communication in the political world and the business world has its common aspects and some differences. Namely, it is a process that is the basis of creating good relationships and accomplishment in the institution of the working process at a high level. Any gap in communication directly reflects the quality of decision making and creating inappropriate image of the institution. Political institutions are exposed to further analysis by government institutions and the media, based on that, the flow of information it is required to be at a high level and made the basis of <b>preset</b> <b>parameters</b> of successful communication. Business world, on the other hand, is also necessary to take care of selecting the appropriate model of communication because it is subject to regular monitoring by the target groups according to their interest. The success of communication indicates the successful realization of the mission and vision of the institutions (political and business). ...|$|R
40|$|A {{model of}} traffic {{generation}} with the <b>preset</b> Hurst <b>parameter</b> (selfsimilarity) followed by its estimation in Matlab using wavelet transformation {{and a filter}} banks is described. The error in esti mating the given parameter is analyzed depending {{on the value of}} this parameter and the number of wavelet transformation scales used in the estimation procedure. This study is experimental, aimed at realtime estimation of the Hurst parameter and the accuracy of this estimation in the case of wavelet transformation with the help of filter banks...|$|R
40|$|Abstract. <b>Presetting</b> control <b>parameters</b> of {{algorithms}} {{are important}} to ant colony optimization (ACO). This paper presents {{an investigation into the}} relationship of algorithms performance and the different control parameter settings. Two tour building methods are used in this paper including the max probability selection and the roulette wheel selection. Four parameters are used, which are two control parameters of transition probability α and β, pheromone decrease factor ρ, and proportion factor q 0 in building methods. By simulated result analysis, the parameter selection rule will be given. ...|$|R
40|$|Abstract. One of {{the prime}} tool in {{non-invasive}} cardiac electrophysiology is the recording of an electrocardiographic signal (ECG) which analysis is greatly useful in the screening and diagnosis of cardiovascular diseases. However, {{one of the greatest}} problems is that usually recording an electrical activity of the heart is performed in the presence of noise. The paper presents Bayesian and empirical Bayesian approach to problem of weighted signal averaging in time domain which is commonly used to extract a useful signal distorted by a noise. The averaging is especially useful for biomedical signal such as ECG signal, where the spectra of the signal and noise significantly overlap. Using the methods of weighted averaging are motivated by variability of noise power from cycle to cycle, often observed in reality. It is demonstrated that exploiting a probabilistic Bayesian learning framework leads to accurate prediction models. Additionally, even in the presence of nuisance parameters the empirical Bayesian approach offers the method of theirs automatic estimation which reduces number of <b>preset</b> <b>parameters.</b> Performance of the new method is experimentally compared to the traditional averaging by using arithmetic mean and weighted averaging method based on criterion function minimization. Key words: ECG signal, weighted averaging, Bayesian inference. 1...|$|R
40|$|Fingerprint is a {{very vital}} index {{in the field of}} security. Series of Automatic Fingerprint Identification Systems (AFIS) have been {{developed}} for human identification. These systems compare each of the features of a template fingerprint image with each of the features in the feature sets in the reference database to determine whether the template and each of the reference images are from the same source. Comparison is done on the basis of <b>preset</b> <b>parameters</b> such as feature type, location, orientation and so on. Getting the features used {{for the construction of a}} reference database from the images involve the implementation of a sound fingerprint feature detection, validation and extraction algorithm. In this paper, the process of detecting and extracting true and false feature points in a fingerprint image is discussed. Attention is also given to the elimination of the false feature points through the process of validation. Some of the existing fingerprint feature extraction and validation algorithms were firstly modified and the resulting algorithms were implemented. The implementation was carried out in an environment characterized by Window Vista Home Basic as platform and Matrix Laboratory (MatLab) as frontend engine. Fingerprints images of different qualities obtained from the manual (ink an...|$|R
40|$|Discovery-based {{learning}} designs incorporating active exploration {{are common}} within instructional software. However, researchers have highlighted empirical evidence showing that 2 ̆ 2 pure 2 ̆ 2 discovery learning is of limited value and strategies which reduce complexity and provide guidance to learners are important if potential learning benefits {{are to be}} achieved. One approach to reducing complexity in discovery learning is limiting the range of possible actions for the learner {{to ensure that they}} do not undertake exploratory activities leading to confusion. This article reports on a study in which the learning outcomes from two learning conditions using computer-based simulations were compared. One condition allowed exploration through manipulation of simulation parameters, while the other allowed observation of simulation output from <b>preset</b> <b>parameters,</b> the latter condition designed to limit the complexity of the task. Learning outcomes for the 158 university student participants were assessed via pre-tests and post-tests of conceptual understanding. Students 2 ̆ 7 exploration activities were recorded and their strategies subsequently coded as either systematic or unsystematic. The results showed that when compared with observation, systematic exploration resulted in learning benefits, while unsystematic exploration did not. These results have implications for the design of discovery learning tasks and instructional guidance within computer-based simulations...|$|R
