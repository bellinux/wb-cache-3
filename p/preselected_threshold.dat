12|7|Public
40|$|Abstract: Guaranteed {{performance}} {{control is}} considered {{in this paper}} for discrete-time LQG problems by minimizing {{the probability that the}} performance index is over a <b>preselected</b> <b>threshold.</b> It is proven that this guaranteed performance control problem can be converted into a mean-variance control problem which can be solved by using an embedding scheme. An optimal open-loop feedback control law is derived for the guaranteed performance control of discrete-time LQG problems. Copyright c© 2005 IFA...|$|E
40|$|Abstract Traditional {{models in}} {{multistage}} stochastic programming are directed to minimizing the expected value of random optimal costs arising in a multistage, non-anticipative decision process under uncertainty. Motivated by risk aversion, we consider minimization of {{the probability that}} the random optimal costs exceed some <b>preselected</b> <b>threshold</b> value. For the two-stage case, we analyse structural properties and propose algorithms both for models with integer decisions and for those without. Extension of the modeling to the multistage situation concludes the paper...|$|E
40|$|We use direct Lyapunov exponents (DLE) to {{identify}} Lagrangian coherent structures {{in two different}} three-dimensional flows, including a single isolated hairpin vortex, and a fully developed turbulent flow. These results are compared with commonly used Eulerian criteria for coherent vortices. We find that, despite additional computational cost, the DLE method has several advantages over Eulerian methods, including greater detail {{and the ability to}} define structure boundaries without relying on a <b>preselected</b> <b>threshold.</b> As a further advantage, the DLE method requires no velocity derivatives, which are often too noisy to be useful in the study of a turbulent flow. We study the evolution of a single hairpin vortex into a packet of similar structures, and show that the birth of a secondary vortex corresponds to a loss of hyperbolicity of the Lagrangian coherent structures. 1...|$|E
30|$|To {{calculate}} (23) and (24), {{we first}} need to derive the cumulative distribution functions (CDFs) of γ _e 2 e,R_ 1 ^DF and γ _e 2 e,R_ 2 ^DF evaluated at the <b>preselected</b> switching <b>threshold</b> κ.|$|R
30|$|We derived the {{performance}} of our proposed system {{in terms of the}} outage probability (OP) over Rayleigh fading channels for both two typical relaying protocols, AF and DF protocols. Through some simulation results, it is shown that our proposed protocol can attain the full spatial diversity if the <b>preselected</b> switching <b>threshold</b> is chosen equal or higher than the outage threshold. Otherwise, the diversity of one is obtained. This is verified by numerical results and diversity analysis.|$|R
40|$|We {{consider}} linear two-stage stochastic {{programs with}} mixed-integer recourse. Instead of basing {{the selection of}} an optimal first-stage solution on expected costs alone, we include into the objective a risk term reflecting {{the probability that a}} <b>preselected</b> cost <b>threshold</b> is exceeded. After we have put the resulting mean-risk model into perspective with stochastic dominance, we study further structural properties of the model and derive some basic stability results. In the algorithmic part of the paper, we propose a scenario decomposition method and report initial computational experience...|$|R
40|$|Several on {{line and}} off line data {{processing}} techniques {{are used to}} remove interfering signals due to ground clutter, aircraft, instrumental effects, and external transmissions from the desired atmospheric echoes of Mesosphere Stratosphere, Troposphere (MST) radar. The on line, real time techniques are necessarily simple {{in order to minimize}} processing delays. This algorithm examines the individual Doppler spectra which are computed every two to four seconds (for oblique antenna beams). The total spectral power in each individual spectrum is computed by summing all the spectral points. If this integrated power increases from one spectrum to the next by a factor greater than a <b>preselected</b> <b>threshold,</b> then that spectrum is not added to the spectral sum. Succeeding spectra are compared to the last acceptable spectrum. Only a certain maximum number of spectra are allowed to be rejected in succession...|$|E
40|$|This paper {{presents}} a vector quantization system that limits the maximum distortion {{introduced to a}} pre-selected threshold value. This system uses a recently introduced variation of the L 1 distortion measure that attempts to minimize the occurrences of quantization errors above a <b>preselected</b> <b>threshold.</b> The vectors are first coded using the new distortion measure. The quantization error vectors in which at least one entry is above the threshold are again coded using a residual vector quantizer. The pixels of the input image where the quantization errors are still above the threshold are scalar quantized to force all the errors under the specified threshold. An experimental result is included {{in which all the}} error magnitudes are constrained to be below fifteen. This image was coded using 0. 85 bits per pixel with a relatively simple multiplier-free vector quantizer and without entropy coding. 1. INTRODUCTION In many applications involving image compression using vector quantization, it [...] ...|$|E
40|$|We use direct Lyapunov exponents to {{identify}} Lagrangian coherent structures (LCSs) in a bioinspired fluid flow: the wakes of rigid pitching panels with a trapezoidal planform geometry chosen to model idealized fish caudal fins. When compared with commonly used Eulerian criteria, the Lagrangian method has previously exhibited {{the ability to}} define structure boundaries without relying on a <b>preselected</b> <b>threshold.</b> In addition, qualitative changes in the LCS have previously been shown to correspond to physical changes in the vortex structure. For this paper, digital particle image velocimetry experiments were performed to obtain the time-resolved velocity fields for Strouhal numbers of 0. 17 and 0. 27. A classic reverse von Kármán vortex street pattern was observed along the midspan of the near wake at low Strouhal number, but at higher Strouhal number {{the complexity of the}} wake increased downstream of the trailing edge. The spanwise vortices spread transversely across the wake and lose coherence, and this event was shown to correspond to a qualitative change in the LCS {{at the same time and}} location...|$|E
30|$|In this paper, we {{investigated}} {{the performance of}} DSSC technique in dual-hop full-duplex relaying systems with wireless information and power transfer (WIPT). We obtained tight approximate expression and analytical expression of the system outage probability for DF and AF protocols over Rayleigh fading channels, respectively. Compared to the DSSC half-duplex relaying and conventional full-duplex relaying with WIPT system, our results show that DSSC technique can substantially boost the operational performance of full-duplex relaying systems and achieves the full spatial diversity in cases of the <b>preselected</b> switching <b>threshold</b> is chosen equal or higher than the outage threshold.|$|R
40|$|Background and Purpose: Atrial {{fibrillation}} (AF) is {{a leading}} cause of cardioembolic stroke, but the relationship between AF and noncardioembolic stroke subtypes are unclear. Because AF may be unrecognized, and because AF has a substantial genetic basis, we assessed for predisposition to AF across ischemic stroke subtypes. Methods: We examined associations between AF genetic risk and Trial of Org 10172 in Acute Stroke Treatment stroke subtypes in 2374 ambulatory individuals with ischemic stroke and 5175 without from the Wellcome Trust Case-Control Consortium 2 using logistic regression. We calculated AF genetic risk scores using single-nucleotide polymorphisms associated with AF in a previous independent analysis across a range of <b>preselected</b> significance <b>thresholds.</b> Results: There were 460 (19. 4...|$|R
40|$|This paper {{focuses on}} the issue of outlier {{detection}} for time series in the process industry. Considering the characteristics of time series in process control systems, such as high non-linearity, strong noise and the special relationship between the input and output of the controlled object, a new outlier detection algorithm is proposed. The algorithm adopts an improved Radial Basis Function Network to construct the model of the controlled object and an Auto-Regression Hidden Markov Model to detect outliers. Unlike many conventional outlier detection methods, this algorithm does not need any prior data and can detect outliers accurately without <b>preselecting</b> the <b>threshold.</b> The proposed detection algorithm is validated by the application to the electrode regulator system of an arc furnace and comparison with Takeuchi&# 39;s auto-regressive model detection approach. Gratitude is extended to the National High-Tech R&D Program of China, under Grant No. 2007 AA 04 Z 194 and No. 2007 AA 041401...|$|R
40|$|In {{a target}} {{detection}} communication system, apparatus and method {{for determining the}} presence of probable targets based on contacts (which can indicate {{the presence of a}} target, noise, chatter, or objects not of interest) detected within a predefined position sector or sectors over a specified number of scans. The position of each detected contact, as a contact of interest, is compared with the positions of contacts detected at previous times or scans. Velocity profiles indicate which previous contacts support the likelihood that the contact of interest represents a target having a velocity within a defined band. The likelihood, which can be represented by a quality value, may be a function of number of contacts, timing of contacts, or both the number and timing of contacts in a given velocity profile. A <b>preselected</b> <b>threshold</b> value, which is related to false alarm rate, is compared to the most likely, or highest quality, velocity profile associated with a contact of interest. If the highest quality value exceeds the threshold value, an output is provided indicating that the contact of interest represents a probable target having a velocity within the band defined by the highest quality velocity profile...|$|E
40|$|Multiparameter radars measure {{one or more}} {{additional}} parameters {{in addition}} to the coventional reflectivity factor. The combination of radar observations from a multiparameter radar is used to study the time evolution of rainstorms. A technique is presented to self-consistently compare the area-time integral (ATI) and rainfall volume estimates from convective storms, using two different measurements from a multiparameter radar. Rainfall volumes for the lifetime of individual storms are computed using the reflectivity at S band (10 -cm wavelength) as well as one-way specific attenuation at X band (3 -cm wavelength). Area-time integrals are computed by summing all areas in each radar snapshot having reflectivities (S band) in excess of a <b>preselected</b> <b>threshold.</b> The multiparameter radar data used in this study were acquired by the National Center for Atmospheric Research (NCAR) CP- 2 radar during the Cooperative Huntsville Meteorological Experiment (COHMEX) and the Convection and Precipitation/Electrification Experiment (CaPE), respectively. ATI studies were accomplished in this work using multiparameter radar data acquired during the lifetime of six convective events that occurred in the COHMEX radar coverage area. A case study from the COMHEX field campaign (20 July 1986) was selected to depict the various stages in the evolution of a storm over which the ATI and rainfall volume computations were performed using multiparameter radar data. Another case study from the CaPE field campaign (12 August 1991) was used to demonstrate the evolution of a convective cell based on differential reflectivity observations...|$|E
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from Elsevier via the DOI in this record. Water discolouration is an increasingly important and expensive issue due to rising customer expectations, tighter regulatory demands and ageing Water Distribution Systems (WDSs) in the UK and abroad. This paper presents a new turbidity forecasting methodology capable of aiding operational staff and enabling proactive management strategies. The turbidity forecasting methodology developed here is completely data-driven and does not require hydraulic or water quality network model that is expensive to build and maintain. The methodology is tested and verified on a real trunk main network with observed turbidity measurement data. Results obtained show that the methodology can detect if discolouration material is mobilised, estimate if sufficient turbidity will be generated to exceed a <b>preselected</b> <b>threshold</b> and approximate how long the material will take to reach the downstream meter. Classification based forecasts of turbidity can be reliably made up to 5 hours ahead although {{at the expense of}} increased false alarm rates. The methodology presented here could be used as an early warning system that can enable a multitude of cost beneficial proactive management strategies to be implemented as an alternative to expensive trunk mains cleaning programsThe authors are grateful to the Engineering and Physical Sciences Research Council (EPSRC) for providing the financial support as part of the STREAM EngD project and to Julian Collingbourne of South West Water for supplying the data used in this paper...|$|E
40|$|Atrial {{fibrillation}} (AF) is {{a leading}} cause of cardioembolic stroke, but the relationship between AF and noncardioembolic stroke subtypes are unclear. Because AF may be unrecognized, and because AF has a substantial genetic basis, we assessed for predisposition to AF across ischemic stroke subtypes. We examined associations between AF genetic risk and Trial of Org 10172 in Acute Stroke Treatment stroke subtypes in 2374 ambulatory individuals with ischemic stroke and 5175 without from the Wellcome Trust Case-Control Consortium 2 using logistic regression. We calculated AF genetic risk scores using single-nucleotide polymorphisms associated with AF in a previous independent analysis across a range of <b>preselected</b> significance <b>thresholds.</b> There were 460 (19. 4 %) individuals with cardioembolic stroke, 498 (21. 0 %) with large vessel, 474 (20. 0 %) with small vessel, and 814 (32. 3 %) individuals with strokes of undetermined cause. Most AF genetic risk scores were associated with stroke, with the strongest association (P= 6 × 10 - 4) attributed to scores of 944 single-nucleotide polymorphisms (each associated with AF at P< 1 × 10 - 3 in a previous analysis). Associations between AF genetic risk and stroke were enriched in the cardioembolic stroke subset (strongest P= 1. 2 × 10 - 9, 944 single-nucleotide polymorphism score). In contrast, AF genetic risk was not significantly associated with noncardioembolic stroke subtypes. Comprehensive AF genetic risk scores were specific for cardioembolic stroke. Incomplete workups and subtype misclassification may have limited the power to detect associations with strokes of undetermined pathogenesis. Future studies are warranted to determine whether AF genetic risk is a useful biomarker to enhance clinical discrimination of stroke pathogeneses...|$|R
40|$|History {{matching}} is {{an inverse}} problem {{in which an}} engineer calibrates key geological/fluid-flow parameters by fitting a simulator's output to the real reservoir production history. It has no unique solution because of insufficient constraints. History-match solutions are obtained by searching for minima of an objective function below a <b>preselected</b> <b>threshold</b> value. Experimental design and response surface methodologies provide an efficient approach to build proxies of objective functions (OF) for history matching. The search for minima can then be easily performed on the proxies of OF as long as its accuracy is acceptable. In this paper, we first introduce a novel experimental design methodology for semi -automatically selecting the sampling points, which are used to improve the accuracy of constructed proxies of the nonlinear OF. This method is based on derivatives of constructed proxies. We propose an iterative procedure for history matching, applying this new design methodology. To obtain the global optima, the proxies of an OF are initially constructed on the global parameter space. They are iteratively improved until adequate accuracy is achieved. We locate subspaces {{in the vicinity of}} the optima regions using a clustering technique to improve the accuracy of the reconstructed OF in these subspaces. We test this novel methodology and history-matching procedure with two waterflooded reservoir models. One model is the Imperial College fault model (Tavassoli et al. 2004). It contains a large bank of simulation runs. The other is a modified version of the SPE 9 (Killough 1995) benchmark problem. We demonstrate the efficiency of this newly developed history-matching technique...|$|E
40|$|We {{consider}} a clustering problem where we observe feature vectors Xi ∈ Rp, i = 1, 2, [...] ., n, from several possible classes. The class labels are unknown {{and the main}} interest is to estimate these labels. We propose a three-step clustering procedure where we first evaluate the significance of each feature by the Kolmogorov-Smirnov statistic, then we select the small fraction of features for which the Kolmogorov-Smirnov scores exceed a <b>preselected</b> <b>threshold</b> t 3 ̆e 0, and then use only the selected features for clustering by one version of the Principal Component Analysis (PCA). In this procedure, {{one of the main}} challenges is how to set the threshold t. We propose a new approach to set the threshold, where the core is the so-called Signal-to-Noise Ratio (SNR) in post-selection PCA. SNR is reminiscent of the recent innovation of Higher Criticism; for this reason, we call the proposed threshold the Higher Criticism Threshold (HCT), despite that it is significantly different from the HCT proposed earlier by [Donoho 2008] in the context of classification. Motivated by many examples in Big Data, we study the spectral clustering with HCT for a model where the signals are both rare and weak for two-classes clustering case. Through delicate PCA, we forge a close link between the HCT and the ideal threshold choice, and show that the HCT yields optimal results in the spectral clustering approach. The approach is successfully applied to three gene microarray data sets, where it compares favorably with existing clustering methods. Our analysis is subtle and requires new development in the Random Matrix Theory (RMT). One challenge we face is that most results in the RMT can not be applied directly to our case: existing results are usually for matrices with i. i. d. entries, but the object of interest in the current case is the post-selection data matrix, where (due to feature selection) the columns are non-independent and have hard-to-track distributions. We develop intricate new RMT to overcome this problem. We also find the theoretical approximation for the tail distribution of Kolmogorov-Smirnov Statistic under null hypothesis and alternative hypothesis. With the theoretical approximation, we can claim the effectiveness of KS statistic. Besides, we also find the fundamental limits for clustering problem, signal recovery problem, and detection problem under the Asymptotic Rare and Weak model. We find the boundary such that when the model parameters are beyond the boundary, then the inference is unavailable, otherwise there are some methods (usually exhausted search) to achieve the inference...|$|E
40|$|Abstract Locomotion scoring {{systems are}} {{procedures}} {{used to evaluate}} the quality of cows’ locomotion. When scoring locomotion, raters focus their attention on gait and posture traits that are described in the protocol. Using these traits, raters assign a locomotion score to cows according to a pre-determined scale. Locomotion scoring systems are mostly used to classify cows as lame or non-lame. A <b>preselected</b> <b>threshold</b> within the scale determines whether a cow is classified as lame or non-lame. Since lameness is considered an important problem in modern dairy farming evaluation of locomotion scoring systems is utmost important. The objective of this thesis was to evaluate the performance of raters to assess locomotion in dairy cattle in terms of reliability (defined as the ability of a measuring device to differentiate among subjects) and agreement (defined as the degree to which scores or ratings are identical). This thesis also explores possibilities for the practical application of locomotion scoring systems. In a literature review comprising 244 peer-reviewed articles, twenty-five locomotion scoring systems were found. Most locomotion scoring systems varied in the scale used and traits observed. Some of the most used locomotion scoring systems were poorly evaluated and, when evaluated, raters showed an important variation in reliability and agreement estimates. The variation in reliability and agreement estimates was confirmed in different experiments aiming to estimate the performance of raters for scoring locomotion and traits under different practical conditions. For instance, experienced raters obtained better intrarater reliability and agreement when locomotion scoring was performed from video than by live observation. In another experiment, ten experienced raters scored 58 video records for locomotion and for five different gait and posture traits in two sessions. A similar number of cows was allocated in each level of the five-level scale for locomotion scoring. Raters showed a wide variation in intra- and interrater reliability and agreement estimates for scoring locomotion and traits, even under the same practical conditions. When agreement was calculated for specific levels when scoring locomotion and traits, the lowest agreement tended to be in level 3 of a five-level scale. When a multilevel scale was transformed into a two-level scale, agreement increased, however, this increment was likely due to chance. The variation in reliability and agreement is explained by different factors such as the lack of a standard procedure for assessing locomotion or the characteristics of the population sample that is assessed. The factor affecting reliability and agreement most, however, is the rater him/herself. Although the probability for obtaining acceptable reliability and agreement levels increases with training and experience, {{it is not possible to}} assure that raters score cows consistently in every scoring session. Given the large variation in reliability and agreement, it can be concluded that raters have a moderate performance to assess consistently locomotion in dairy cows. The variable performance of raters when assessing locomotion limits the practical utility of locomotion scoring systems as part of animal welfare assessment protocols or as golden standard for automatic locomotion scoring systems.                                  ...|$|E

