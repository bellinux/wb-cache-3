50|202|Public
50|$|The Kelvin-Varley voltage divider, {{named after}} its inventors William Thomson, 1st Baron Kelvin and Cromwell Fleetwood Varley, is an {{electronic}} circuit used to divide voltages, i.e. to generate an output voltage as a <b>precision</b> <b>ratio</b> of an input voltage, with several decades of resolution. In effect, the Kelvin-Varley divider is an electromechanical precision digital-to-analog converter.|$|E
50|$|ICP-MS is {{also used}} widely in the {{geochemistry}} field for radiometric dating, {{in which it is}} used to analyze relative abundance of different isotopes, in particular uranium and lead. ICP-MS is more suitable for this application than the previously used thermal ionization mass spectrometry, as species with high ionization energy such as osmium and tungsten can be easily ionized. For high <b>precision</b> <b>ratio</b> work, multiple collector instruments are normally used to reduce the effect noise on the calculated ratios.|$|E
40|$|Abstract. Recall {{ratio and}} <b>Precision</b> <b>ratio</b> are two {{important}} indices {{of the effect}} evaluation of computer information retrieval. This paper analyzed factors impacting Recall ratio and <b>Precision</b> <b>ratio,</b> then discussed the improving of computer information retrieval efficiency from retrieval approach, database selecting and retrieval pattern, etc...|$|E
40|$|In this paper, Internet {{search and}} image search {{performances}} of the popular search engines Google, Yahoo, Yandex, and Bing were investigated by using twenty four Turkish queries from eight categories. These queries were run on each search engine for each of four consecutive weeks and mean weighted <b>precision</b> <b>ratios</b> of the engines were calculated by using common lists. This was done for Internet search and image search separately. For both Internet search and image search, Yahoo was the best search engine based on the average of mean weighted <b>precision</b> <b>ratios,</b> followed by Bing, Google, and Yandex, respectively. Internet search performances of Google and Yandex were better than their image search performances. On the other hand, image search performances of Bing and Yahoo were better except for the fourth week...|$|R
40|$|Investigations into {{realization}} of high <b>precision</b> <b>ratioed</b> resistors in standard CMOS and BiCMOS processes {{have been carried}} out. The {{results indicate that the}} layout of the resistors can be optimized with respect to area and matching requirements to yield relative accuracy better than 0. 25 %. Using an intermeshed ladder architecture, fast converters with resolution up to 10 b are realizable without trimming...|$|R
40|$|Bringing {{semantics}} to Web Services {{description and}} matching remarkably improves the recall and <b>precision</b> <b>ratios</b> of Service discovery. Though {{there are many}} effective semantic based Web Services matchmakers, they have poor time efficiency for consuming time in the semantic reasoning step. To shorten service matching time, this paper presents a fast matchmaker for OWL-S services, called XServices Semantic Service Discovery (XSSD). It executes the Semantics Pretreatment Algorithm (SPA) in service publishing phase and implements a multi-feature based Semantic Web services matching method when queries coming. SPA extracts part of semantic description in the OWL-S files and creates matrixes which we call service feature matrixes to express service semantics. By shifting part of the reasoning process from service matching stage to service publishing stage, XSSD reduces the waiting time for the matchmaker clients. The results of the evaluation on a well-annotated Semantic Web Services set OWLS-TC 4 show that while XSSD offers the comparable recall and <b>precision</b> <b>ratios,</b> the time efficiency of XSSD outperforms existing similar matchmakers...|$|R
40|$|AbstractMost {{of current}} {{ontology}} mapping methods can not treat different mapping tasks {{in different ways}} referred to {{the features of the}} input ontology. And they combine different features of ontology without full consideration of the influences on mapping results caused mapping features. In view of the above questions, this paper proposes mapping method which can use entropy decision-making method to determine the combined weight of the different features of ontology. Experiments show that this method can maintain the stability and the commonality, and improve the recall ratio and the <b>precision</b> <b>ratio</b> at the same time...|$|E
40|$|Abstract: In this paper, the {{recognition}} system of fuzzy clustering based on BP feature screening was put out. The figure specimens of experiment were filtered through BP network, {{and the result}} of screening was fit into the clustering source. At last fuzzy clustering was carried out by constituting the fuzzy relation matrix. The result of experiment demonstrates that this method has very high noise immunity capacity and overcame the limitation of traditional algorithm with single factor recognition. The recognition rate and <b>precision</b> <b>ratio</b> were greatly improved at the same time...|$|E
40|$|Abstract. In this paper, {{we present}} an {{automatic}} system for bloody image detection on the internet. A quick blood model in HIS color space is employed {{to identify the}} blood regions and then the fractal dimension and the information entropy of these regions are extracted. At last all the features are fed to a SVM-classifier to tell whether the image is bloody or not. Our experiments on real-world web image data indicate that this system can detect the bloody images with high recall ratio and high <b>precision</b> <b>ratio...</b>|$|E
40|$|In {{this paper}} we present preliminar {{large-scale}} experimental {{results on the}} effect of logical negations {{in the context of a}} feedback cycle. Terms are selected from judged non-relevant documents and included in the new query as negated literals. This inclusion yields similar average <b>precision</b> <b>ratios</b> but improves <b>precision</b> at low recall levels. For negative term selection purposes, we also ran some experiments in which the set of judged non-relevant document is not considered in its entirely. This evaluation scenario revealed that negations perform better if selected from a few top ranked nonrelevant documents. KEYWORDS: Logic-based Information Retrieval, Relevance feedbac...|$|R
40|$|This {{chapter is}} devoted to the {{analytical}} methods employed for making high <b>precision</b> isotope <b>ratio</b> measurements that preserve naturally occurring mass-dependent isotopic variations. The biggest challenge in making these types of measurements is deconvolving mass-dependent isotopic fractionation produced in the laboratory and mass spectrometer...|$|R
50|$|Number types include integers, ratios, floating-point numbers, {{and complex}} numbers. Common Lisp uses bignums to {{represent}} numerical values of arbitrary size and <b>precision.</b> The <b>ratio</b> type represents fractions exactly, a facility {{not available in}} many languages. Common Lisp automatically coerces numeric values among these types as appropriate.|$|R
40|$|The {{option of}} {{combining}} multiple model responses to reduce predictive uncertainty {{has been investigated}} {{in the field of}} economic forecasting and more lately in climate prediction. The current combination method in hydroclimatology is mainly limited to mixing weights that remain static over time. Recently (Chowdhury and Sharma, 2006) and (Chowdhury et al., 2007) have proposed using time variant mixing weights as an improvement over the static weight combination. The method involves a dynamic hierarchical pair wise combination tree for outputs from multiple predictive models. The model pairs are first matched based on the sample error covariance. Then the pairs are combined by ascertaining a target weight for each time step. This process provides a low dimensional setting for investigating any predictive structure of the relative model strengths. The weights are predicted using a mixed distribution which is a product of a ‘precision ratio ’ and a ‘bias direction’. The <b>precision</b> <b>ratio</b> is the fraction of the squared residual error associated with each of the paired models, and the bias direction represents an indicator of the sign of the two residual errors. The <b>precision</b> <b>ratio</b> is projected forward using a generalised linear autoregressive model and the bias direction is projected by ordered logistic regression. The method is extended here to combine three climate models, the variables of interest being the monthly global sea surface temperature anomalie...|$|E
40|$|Most {{of current}} {{ontology}} mapping methods can not treat different mapping tasks {{in different ways}} referred to {{the features of the}} input ontology. And they combine different mapping strategies without full consideration of the influences on mapping results caused mapping strategies. In view of the above questions, this paper proposes a dynamic mapping policy selection method which can pre-select mapping strategy and use entropy decision-making method to determine the combined weight of the selected strategy. Experiments show that this method can maintain the stability and the commonality, and improve the recall ratio and the <b>precision</b> <b>ratio</b> at the same time...|$|E
30|$|The {{values of}} {{coefficient}} of determination (R 2) used to express the ‘fitness’ of the model’s regression equations (Eqs.  2 – 8) are 0.947 for Y 1, 0.969 for Y 2, 0.959 for Y 3, 0.942 for Y 4, 0.929 for Y 5, 0.932 for Y 6, 0.933 for Y 7 which further pointed out that Eqs. (2)–(8) may be utilized for representing the true relationships among the variables. Since the calculated values of R 2 and adjusted-R 2 vary negligibly, models are composed of significant terms. An adequate <b>precision</b> <b>ratio</b> {{was determined by the}} software highlighting the signal (information of interest) to noise (random errors) ratio arising out of the data set. Adequate <b>precision</b> <b>ratio</b> greater than 4 is desirable and its values in the current investigation was 26.9 for survival rate, 30.9 for VCAL, 27.6 for VCAL 1  month, 22.7 for VCAL 6  months, 20.6 for VCAL 12  months, 21.2 for VCAL 18  months and 21.3 for VCAL 24  months, respectively. The above value shows that the originating model can be used to navigate in the design space. Further, low coefficient of variation (CV) values (2.63 for survival rate, 3.71 for VCAL, 4.91 for VCAL 1  month, 6.75 for VCAL 6  months, 8.49 for VCAL 12  months, 8.32 for VCAL 18  months and 8.3 for VCAL 24  months) showed the precision and reliability of the experiments. Above results indicated that these models will be helpful in further optimization by CCRD.|$|E
40|$|The {{isotope ratio}} {{performance}} of an axial time-of-flight ICP mass spectrometer (Renaissance TOF-ICPMS, LECO Corp.) was evaluated. The isotope <b>ratio</b> <b>precision,</b> expressed as the {{relative standard deviation}} (RSD) for 10 successive measurements, was evaluated using multielement standard solutions with analyte concentrations of 50 - 500 mu g/L. The influence of the acquisition time per replicate measurement was studied by varying it between 0. 5 and 300 s. For an acquisition time of 30 s per replicate and an elemental concentration of 500 mu g/L, typical isotope <b>ratio</b> <b>precisions</b> of {{less than or equal}} to 0. 05 % RSD were obtained. The fact that this isotope <b>ratio</b> <b>precision</b> can be obtained for many ratios simultaneously is an especially attractive feature of TOF-ICPMS, In contrast to what was expected, increasing the acquisition time per replicate to values of > 30 s resulted in a slightly deteriorated isotope <b>ratio</b> <b>precision.</b> At short acquisition times (< 10 s), isotope <b>ratio</b> <b>precisions</b> similar to, or better than, the best values ever reported for quadrupole-based instruments were obtained. The latter observation remained valid when working with transient signals of corresponding duration. Mass discrimination was observed to be analogous to that observed with other types of ICPMS instrumentation (similar to 1 % per mass unit at midmass), The accuracy attainable was evaluated by comparing Pb isotopic results for a "natural" Pb standard solution obtained by TOF-ICPMS with those obtained by thermal ionization mass spectrometry...|$|R
30|$|<b>Precision</b> is the <b>ratio</b> of {{the number}} of {{correctly}} detected clones to the total number of detecting clones by the proposed tool.|$|R
3000|$|<b>Precision</b> is the <b>ratio</b> of {{the number}} of {{relevant}} records retrieved to the total number of relevant and irrelevant records retrieved. It is given by [...]...|$|R
40|$|Abstract. Secure {{index is}} the core {{technology}} in the full-ciphertext retrieval. In order to appliance the efficient full-text retrieval in the ciphertext state, a streamline dynamic successive trees of ciphertext(SDSTC) index model is proposed. The SDSTC index model which supports queries of substring and dynamic updating of index, has a high recall and <b>precision</b> <b>ratio</b> in retrieval. Giving algorithms of model index creation, retrieval and ciphertext index updating and the analysis of its security and efficiency. Compared with other existing index models, experiments show that the SDSTC index model has good time efficiency and more suitable for areas of ciphertext security of full-text retrieval...|$|E
40|$|With today's {{advances}} in Peer-to-Peer (P 2 P) searching technology, {{a lot of}} non-document content has become searchable and usable. In the near future, since {{a huge amount of}} content is distributed over the networks, not only index server searching but also P 2 P searching will become important because of its scalability and robustness. Typical P 2 P contents sharing services have some problems, such as low search <b>precision</b> <b>ratio,</b> significant increase in traffic and inundations of malicious content such as virus. In this paper, with ideas of the CAN (Content Addressable Network) topology and a vector space method where vectors have a variable length, we propose a new P 2 P content searching method in which a query is effectively forwarded only to peers that have indices of content semantically similar to the desired one but not forwarded to the same peer repeatedly. The main part of our proposal is to map non-document content to a vector space based on users' evaluation and to manage vector space or to route queries using the CAN topology control. The effectiveness of the proposal is shown both by analytical estimations and simulation experiments. Our simulation experiments clarify that the proposed method is effective in improving the precision and recall ratios while reducing the amount of traffic compared with the Gnutella flooding and the vector space method in which vector lengths are fixed (close to pSearch method). In particular, when {{there is a lot of}} malicious content, the proposed method exhibited a higher <b>precision</b> <b>ratio</b> than other methods. </p...|$|E
40|$|A {{comparative}} evaluation {{was made}} of the use of natural language versus two specialized indexing languages, aiming to demonstrate the influence of the availability of indexing languages on the functioning of information retrieval systems. The study was conducted within the ambit of the construction of search strategies by subject in online university library catalogs. The <b>precision</b> <b>ratio</b> was calculated to determine the accuracy of each indexing language in subjectbased information retrieval. From the comparative evaluation of the use of indexing languages, it was concluded that the term specificity required by the user during retrieval was more satisfactory when the query was made through controlled languages, whose availability and simplicity is also an indispensable requisite...|$|E
40|$|Gathering data on {{molecular}} interactions to be {{fed into}} a specialized database has motivated {{the development of a}} computer system to help extracting pertinent information from texts, relying on advanced linguistic tools, completed with object-oriented knowledge modeling capabilities. As a first step toward this challenging objective, a program for the identification of gene symbols and names inside sentences has been devised. The main di#culty is that these names and symbols do not appear to follow construction rules. The program is thus made up of a series of sieves of di#erent natures, lexical, morphological and semantic, to distinguish among the words of a sentence those which can only be potential gene symbols or names. Its performance has been evaluated, in terms of coverage and <b>precision</b> <b>ratios,</b> on a corpus of texts concerning D. melanogaster for which the list of names of known genes is available for checking. ...|$|R
40|$|We {{present a}} new {{approach}} to extracting information from unstructured documents based on an application ontology that describes a domain of interest. Starting with such an ontology, we formulate rules to extract constants and context keywords from unstructured documents. For each unstructured document of interest, we extract its constants and keywords and apply a recognizer to organize extracted constants as attribute values of tuples in a generated database schema. To make our approach general, we fix all the processes and change only the ontological description for a different application domain. In experiments we conducted on two di#erent types of unstructured documents taken from the Web, our approach attained recall ratios in the 80 % and 90 % range and <b>precision</b> <b>ratios</b> near 98 %. Keywords: unstructured data, semistructured data, information extraction, information structuring, ontology. 1 Introduction A relation in a structured database consists of a set of n- tuples. Each n-tupl [...] ...|$|R
40|$|Hydrogen and helium {{emission}} {{lines in}} nebulae form by radiative recombination. This {{is a simple}} process which, in principle, can be described to very high <b>precision.</b> <b>Ratios</b> of He I and H I emission lines {{can be used to}} measure the He^+/H^+ abundance ratio to the same precision as the recombination rate coefficients. This paper investigates the controversy over the correct theory to describe dipole l-changing collisions (nl→ nl'=l± 1) between energy-degenerate states within an n-shell. The work of Pengelly & Seaton (1964) has, for half-a-century, been considered the definitive study which "solved" the problem. Recent work by Vrinceanu et al. (2012) recommended the use of rate coefficients from a semi-classical approximation which are nearly an order of magnitude smaller than those of Pengelly & Seaton (1964), with the result that significantly higher densities are needed for the nl populations to come into local thermodynamic equilibrium. Here, we compare predicted H I emissivities from the two works and find widespread differences, of up to ≈ 10...|$|R
40|$|We {{analyze the}} tendencies in {{choosing}} cut-edit points in personal video content edited by users on the Internet, {{and develop a}} method to automatically estimate cut-edit points based on the results. When we investigated the relationship among cut-edit points in personal videos using a space-time patch feature(STpatch feature), we realized that cut-edits were done in frames with a low Continuous Rank-Increase Measure (CRIM) value and a high Motion Correlation (MC) value calculated from the ST-patch feature. Therefore, we propose a method for estimating cut-edit points based on CRIM and MC values. Experimental results indicate that we obtained a recall ratio of 61. 3 % and a <b>precision</b> <b>ratio</b> of 50. 4 %. ...|$|E
40|$|Abstract. Session {{segmentation}} can {{not only}} contribute {{a lot to the}} further and deeper analysis of user’s search behavior but also act as the foundation of other retrieval process researches based on users ’ complicated search behaviors. This paper proposes a session boundary discrimination model utilizing time interval and query likelihood on the basis of Naive Bayes Model. Compared with previous study, the model proposed in this paper shows a prominent improvement through experiment in three aspects, which is: recall ratio, <b>precision</b> <b>ratio</b> and value F. Owing to its advantage in session boundary discrimination, the application of the model can serve as a tool in fields like personalized information retrieval, query suggestion, search activity analysis and other fields which is related to search results improvement...|$|E
40|$|This paper {{presents}} an interactive query expansion method with association thesaurus, which is mined from the ‘selected web pages ’ of {{users in the}} query logs. The ‘selected web pages ’ of users are transferred into ‘sets of query terms ’ and then used for term correlation mining. Accordingly, various association thesauruses concerning different query terms are constructed from these term correlations. Consequently, the proposed method combines the original query term specified by a user with the corresponding thesaurus to offer the user more precise results. The query expansion mechanism is implemented within the Query Agent of a course recommendation system, Coursebot. Experimental results {{have shown that the}} performance, <b>precision</b> <b>ratio</b> and recall ratio, of the system is increased when the proposed method is applied...|$|E
40|$|This paper {{describes}} the rule-based classification of numerals and strings that include numerals, {{composed of a}} number and semantic unit(s) that indicate a SPEED, NUMBER, or other measure, at three levels: morphological, syntactic, and semantic. The approach employs three interpretation processes: word trigram construction with tokeniser, rule-based processing of number strings, and n-gram based classification. We extracted numeral strings from 378 online newspaper articles, finding that, on average, they comprised about 2. 2 % of {{the words in the}} articles. To manually extract n-gram rules to disambiguate the number strings’ meanings, our approach was trained on 886 numeral strings and tested on the remaining 3251 strings. We implemented two heuristic disambiguation methods based on each category’s frequency statistics collected from the sample data, and <b>precision</b> <b>ratios</b> of both methods were 86. 8 % and 86. 3 % respectively. This paper focuses on the acquisition and performance of different types of rules applied to numeral strings classification...|$|R
40|$|The {{presence}} of obstructing obstacles severely degrades {{the efficiency of}} routing protocols in MANETs. To mitigate {{the effect of these}} obstructing obstacles, routing in MANETs is usually based on the a priori knowledge of the obstacle map. In this paper, we investigate rather the dynamic and autonomic detection of obstacles that might stand within the network. This is accomplished using the enhanced cartography optimized link state routing CE-OLSR with no extra signaling overhead. The evaluation of the performance of our proposed detection scheme is accomplished through extensive simulations using OMNET++. Results clearly show the ability of our proposed scheme to accurately delimit the obstacle area with high coverage and efficient <b>precision</b> <b>ratios.</b> Furthermore, we integrated the proposed scheme into CE-OLSR to make it capable of autonomously detecting and avoiding obstacles. Simulation results show the effectiveness of such an integrated protocol that provides the same route validity as that of CE-OLSR-OA which is based on the a priori knowledge of the obstructing obstacle map...|$|R
40|$|We {{propose to}} map the {{transverse}} momentum (Pt) dependence for semi-inclusive electroproduction of charged pions (π ±) and, simultaneously albeit with a factor of ten lower rates, kaons (K ±) from both proton and deuteron targets. The proposed measurements cover the range 0. 2 < x < 0. 5, 2 < Q 2 < 5 GeV 2, 0. 3 < z < 0. 5, and Pt < 0. 5 GeV. The Hall C HMS spectrometer and the projected SHMS with its first-generation detector package {{will be used for}} electron and meson detection. The chosen setup of highly-focusing magnetic spectrometers with well-understood acceptances and redundant detector packages will allow precise determination of the Pt dependence of the ratios of π + to π − cross sections. The proposed measurements correspond to a beam time request of 32 days, and assume a polarized electron beam to also include azimuthal asymmetry measurements, beam energies of 8. 8 and (predominantly) 11. 0 GeV, and varying beam currents of up to 75 µA. The <b>precision</b> <b>ratios</b> will be combined with maps of the azimuthal asymmetries in semiinclusiv...|$|R
30|$|Carbaryl {{removal from}} aqueous {{solutions}} by P. stratiotes biomass was studied with a CCD. According to the ANOVA analysis, carbaryl removal is highly affected by biomass dose, pH and {{the interactions between}} pH with the other factors (biomass dose, initial concentration and time). The hierarchical quadratic model represents adequately the response surface space based on the adjusted determination coefficient (R 2 adj =  0.9868) and the adequate <b>precision</b> <b>ratio</b> (40.483). By using this model, at the optimal conditions (pH 2.1, 0.72  g of biomass, 30  min contact time and 15.57  mg L− 1 of carbaryl concentration), the predicted removal efficiency achieved near 100  % of carbaryl from aqueous solutions when using P. stratiotes biomass. Finally, the reported results in this research demonstrate the feasibility of employing P. stratiotes biomass as a low-cost biosorbent for carbaryl removal.|$|E
40|$|In {{order to}} solve the problem of medical {{equipment}} image retrieval，an image retrieval method based on multi feature layer fusion is proposed. Process:This method using color moments to calculate the characteristics of color image，the LBP Operator is used to calculate the texture features of the image，using Fourier descriptors calculated image of the characters，the stratified weighted fusion method of the characteristics of three types of fusion within a similarity measure，and ultimately the formation of retrieval based on. Results and conclusions:According to the experimental study of the endoscopic image and CT image，the experimental results confirm the validity of the proposed method. With continuous improvement of the precision requirements of the proposed method in <b>precision</b> <b>ratio</b> was significantly higher than that of two single feature retrieval method for medical image retrieval...|$|E
40|$|Spam is {{so widely}} speared {{that has a}} bad effect on daily use of E-mail. Nowadays, among the primary {{technologies}} of spam filtering, support vector machine (SVM) is applied widely, because it is efficient and has high separating accuracy. The main problem of support vector machine arithmetic is how to choose the kernel function. To solve this problem people propose spam filtering arithmetic of support vector machine based on Boolean kernel. The arithmetic uses filtering methods based on attributes, such as IP address, subject words, keywords in content, enclosure information, etc. These attributes compose the feature vectors, and the vectors are classified by SVM-MDNF based on Boolean kernel. The experiment results show that this arithmetic has high separating accuracy, high recall ratio and <b>precision</b> <b>ratio.</b> The arithmetic has its value in theory and application. </p...|$|E
40|$|An {{evaluation}} {{test was}} made of an experimental data-base prepared by the Space Documentation Service of the European Space Agency, consisting of some 44, 000 items from NASA STAR for 1973 and 1974. With this data-base {{it was possible to}} search on natural language terms in the titles and abstracts, in addition to the normal searches on controlled language index terms. The on-line searches were carried out at four centres, each centre being responsible for ten questions, with two searches in the alternative search modes being made by different people for each question. Up to twenty-five documents retrieved in the two searches for each question were sent to the originator of the question for relevance assessment. The results are presented {{in a number of different}} ways, but in every case the natural language searches showed a significantly higher recall ratio than the controlled language, with little difference in the <b>precision</b> <b>ratios.</b> It is suggested that the main reason for the superiority of natural language searching is the greater exhaustivity of the abstracts as compared to the indexing. European Space Agency Contract Report, ESA contract number I/ 43...|$|R
30|$|Summary: Feedback is {{the ratio}} of queries where NextBug makes at least k recommendations. <b>Precision</b> is the <b>ratio</b> of {{relevant}} recommendations among the top-k recommendations provided by NextBug. Finally, Likelihood indicates whether at least one relevant recommendation is included among NextBug’s top-k suggestions.|$|R
30|$|The <b>{{precision}}</b> is the <b>ratio</b> tp/(tp + fp) where tp is {{the number}} of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.|$|R
