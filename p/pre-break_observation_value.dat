0|1007|Public
30|$|Based on the quale, the {{observer}} produces an <b>observation</b> <b>value.</b> The function qualeToMeasure introduced below establishes a mapping between a quale and an <b>observation</b> <b>value</b> (resulting from a measurement process).|$|R
30|$|Step 4 (also called ‘expression’): {{convert the}} signals to <b>observation</b> <b>values.</b>|$|R
30|$|The {{compressed}} sensing theory {{tells us}} that, if a signal is sparse or compressible through an orthogonal transformation, the signal {{can be observed}} in a lower frequency, and can be represented with least numbers of <b>observation</b> <b>values.</b> Moreover, the original signal can be estimated well by these sparse <b>observation</b> <b>values</b> (Donoho 2006; Candès and Wakin 2008).|$|R
40|$|Abstract. This paper uses total {{radiation}} instrument and scattered radiation instruments to test illumination irradiance {{in the center}} point of sunshine greenhouse, and compares the test data with the software model predicted data, the results show that: the radiation of fine day indoor sunshine greenhouse is generally larger than that in cloudy. The average values in fine day of outdoor <b>observation</b> <b>value,</b> outdoor simulation <b>value,</b> indoor <b>observation</b> <b>value,</b> indoor simulation value are 347. 6 2 /mW, 292. 3 2 /mW, 153. 8 2 /mW and 151. 7 2 /mW, while that in cloudy day are 261. 1 2 /mW, 262. 7 2 /mW, 110. 2 2 /mW and 123. 9 2 /mW. The trend of total indoor radiation simulation value {{is consistent with the}} <b>observation</b> <b>value...</b>|$|R
50|$|Notice that in each case, the {{parameters}} {{which must be}} fixed determine a limit {{on the size of}} <b>observation</b> <b>values.</b>|$|R
3000|$|... {{lead to the}} KF trusting {{the initial}} state {{estimate}} too little and not adapting to the new initial <b>observations.</b> <b>Values</b> of [...]...|$|R
30|$|The spatial {{resolution}} and the temporal {{resolution of the}} <b>observation</b> <b>value</b> are now equated with the {{spatial resolution}} and temporal resolution of the quale respectively 1.|$|R
30|$|Proof As {{shown in}} Fig.  1, the {{sampling}} time is treal[*]=[*]tideal[*]+[*]Δt, while the <b>observation</b> <b>value</b> is val[*]=[*]valreal[*]+[*]Δ, where Δ is the error {{caused by the}} jitter Δt.|$|R
40|$|Abstract. Based on {{the choice}} {{characteristic}} of least absolute estimation,and {{the principle of}} the minimum of summation of residual absolute value,the GM(1, 1) model is proposed to distortion inspecting and forecasting. Exemples show the precision of least absolute estimation is uncertain to be higher a little than that of least squares estimation when <b>observation</b> <b>value</b> contains gross error,and the precision of least absolute estimation is lower a little than that of least squares estimation when <b>observation</b> <b>value</b> doesn’t contain gross error. So least absolute estimation isn’t beter to GM(1, 1) ...|$|R
30|$|In fact, the {{following}} equations hold: spatialResolution(observation) ≤ spatialResolution(quale); temporalResolution(observation) ≤ temporalResolution(quale); thematicResolution(observation) ≤ thematicResolution(quale), since {{the transformation of}} the quale into an <b>observation</b> <b>value</b> (through the expression operation mentioned in Section The receptor-centric approach) might involve another loss of spatial/temporal/thematic detail. The example introduced here assumes no loss of spatial/temporal detail during the expression operation, and equates the spatial/temporal resolution of the observation with the spatial/temporal resolution of the quale. A thorough investigation of the interplay between resolution of quale and resolution of <b>observation</b> <b>value</b> (for the spatial, temporal and thematic dimensions) is deferred to future work.|$|R
30|$|<b>Observation</b> <b>values</b> {{from the}} weather sensors get {{transmitted}} to the LAU (Local Acquisition Unit) Server after it gets converted into a digital signal through the data logger system. At this point, the data logger system and the LAU Server perform an encrypted communication of symmetry key types using the exchanged security key through the mutual authentication, and the data logger system transmits the <b>observation</b> <b>value</b> to the LAU server every minute. If data cannot be transmitted every minute, either a fault has arisen in the data logger system or an unexpected hijacking has happened. Therefore, an inspection for physical hijacking and responsive actions to the failure need to be performed.|$|R
3000|$|The LAU server (signer) {{requests}} {{a private}} key from the PKG and receives the private key D_ID = sQ_ID. The LAU server performs a digital {{signature of the}} meteorological <b>observation</b> <b>value</b> by computing U, V and sends them with the <b>observation</b> <b>value</b> to the DGS. At this point, U is U = rQ_ID, V is V = ([...] r + h)D_ID, and r chooses a random number from Z_q^ * and h is denoted as h = H_ 2 ([...] M, U). The DGS (Data Gathering Server) server (verifier) performs verification by identifying the signature ([...] U, V) of the LAU server by ê([...] P,V) = ê([...] U + h,P_PKG,Q_ID).|$|R
30|$|An {{observer}} has an id and {{a number}} of receptors of a certain type. It carries a quale and an <b>observation</b> <b>value.</b> The measurement unit used below for <b>observation</b> <b>values</b> is “ppm” standing for parts per million. For simplicity, it is assumed here that all receptors (with a similar function) have the same size, and there is no malfunction during the observation process (i.e., either all the receptors detecting the stimulus are stimulated or none of them). The assumption that all receptors have the same size is in line with Quine [59] who states: “The subject’s sensory receptors are fixed in position, limited in number, and substantially alike”. A COA has one measuring probe.|$|R
40|$|Abstract. The {{investigation}} {{presents a}} new approach based on SIR particle filtering state estimation and smoothed residual for GPS receiver autonomous integrity monitoring (RAIM), which adopted the difference value between the ideal <b>observation</b> <b>values</b> acquired by state estimation and the actual state <b>observation</b> <b>values,</b> and the log likelihood ratio (LLR) test based on probability density function of state-measurement was set up. Experimental results based on real GNSS data demonstrate that the algorithm can estimate the state precisely under non-Gaussian measurement noise, detect and isolate GPS satellite failures successfully and solve the performance degradation problem of RAIM algorithm based on Kalman filter. Therefore, experimental results validate the validity of SIR particle filtering state estimation and smoothed residual for RAIM...|$|R
30|$|To {{overcome}} above problems, Rosenblatt [32] and Parzen [33] make improvements on histogram estimation methods. Firstly, replace {{indicator function}} in histogram by smooth kernel function, {{and then set}} estimation interval center as sample <b>observation</b> <b>value.</b> These improvements lead to method {{commonly referred to as}} KDE.|$|R
30|$|In this example, {{the goal}} is to model a {{one-dimensional}} <b>observation</b> <b>value</b> using both ME-based modeling and a contextual additive structure. Due to the prime importance of mean parameters in HMM-based speech synthesis [47], we investigate the difference between mean values predicted by two systems.|$|R
30|$|Listing 1 {{introduces}} three relevant datatypes for the scenario: Magnitude (to {{represent the}} magnitude of a quality), Quale (entity evoked in a cognitive agent’s mind when observing a quality), and ObsValue (to represent <b>observation</b> <b>values).</b> For a detailed discussion of these notions, see [74].|$|R
30|$|According to F(I)[*]≈[*] 14.5123 [*]>[*]F 0.95 and (2, 30 – 3 – 2) = 3.39, and RI is larger, it {{is obvious}} that the <b>observation</b> <b>values</b> of 29 and 30 are {{abnormal}} points. This is consistent with the results obtained by Atkinson and Cheng.|$|R
40|$|Abstract. A new {{approach}} is proposed {{to detect and}} track the moving object. The affine motion model and the non-parameter distribution model are utilized to represent the object firstly. Then the motion region of the object is detected by background difference while Kalman filter estimating its affine motion in next frame. Center association and mean shift are adopted to obtain the <b>observation</b> <b>values.</b> Finally, the distance variance and scale variance between the estimated and detected regions are used to fuse the <b>observation</b> <b>values</b> to acquire the measurement value. To correct fusion errors, the observable edges are employed. Experimental {{results show that the}} new method can successfully track the object under such case as merging, splitting, scale variation and scene noise...|$|R
40|$|At present, {{the number}} of orbit {{satellites}} is limited for the Beidou satellite navigation system. In special terrain like “city Canyon” and other special circumstances, the signal of Beidou is easy to be blocked, which reduces positioning accuracy or interrupts positioning continuity of users. Joining pseudolites can effectively {{solve the problem of}} the insufficient number of BeiDou satellites visible to users. During cooperative positioning of Beidou satellites and pseudolites, the multi-path problem in pseudo-range <b>observation</b> <b>values</b> of the BeiDou satellite and the pseudolite shall be properly handled to obtain better positioning accuracy. The CNMC method (Code Noise and Multi-path Correction) can effectively reduce the multi-path effect of the BeiDou satellite's pseudo-range <b>observation</b> <b>value,</b> but cannot be applied directly to pseudo-range data processing of the pseudolite due to different observation error characteristics caused by different signal propagation paths. To solve the multi-path processing problem of pseudolites, the CNMC method is improved in this paper. The BeiDou/Pseudolite dynamic cooperative positioning experiments were conducted in the actual field of pseudolites. The test results show that the three-dimensional positioning accuracy is increased from 2. 326 m to 1. 936 m with enhanced positioning stability after pseudo-range <b>observation</b> <b>values</b> of the BeiDou satellite and the ground pseudolite are processed by the CNMC method...|$|R
3000|$|... • The triplification {{engine is}} a {{software}} component responsible for consuming and “homogenising” {{the representation of}} incoming raw <b>observation</b> <b>values.</b> The use of time-stamped RDF triples, incorporating OWL-based subjects, predicates and objects, promotes human-readability {{while at the same}} time allowing us to exploit the extensive capabilities of SPARQL query languages.|$|R
40|$|Mathematical {{modeling}} {{for continuous}} prediction of solar energy is inevitable for energy systems. General, existing models are mostly empirical and data dependent. This article has {{to present a}} variable model for predicting global and diffuse solar radiation on the horizontal plane. The model considered in this study has applied Hidden Markov Model (HMM) with two observations. The databases were measured according CIE standard since 2004 to 2010 for model synthetics. The new data of 2011 year used for testing model. Training, take data in time sequence and clustering. After that, create transition probability in each state have two observations matrix; Sky Ratio (SR) and solar altitude angle (Î±) are <b>observation</b> <b>value</b> of the model. Predicting, values of solar radiation are considered as the hidden events by will been calculate probability of observation from P [SR â© Î±], state as highest probability was selected for predict state and convert to solar radiation quantity. Model evaluation, three statistical namely MBD, RMSD and R 2 were used for model evaluations. The results show that, this model is appropriate for predicting sky quantities. Conclusion, the variable model from the synthesis was suitable for predicting long-term data with <b>observation</b> <b>value.</b> The advantage of {{this study showed that}} we could predict sky the quantity value when we only knew the <b>observation</b> <b>values...</b>|$|R
50|$|In {{statistical}} modelling the MSE, representing {{the difference between}} the actual observations and the <b>observation</b> <b>values</b> predicted by the model, is used {{to determine the extent to}} which the model fits the data and whether the removal or some explanatory variables, simplifying the model, is possible without significantly harming the model's predictive ability.|$|R
3000|$|... with DBH the {{diameter}} of the tree and DBHq the quadratic mean diameter of all trees on the plot at the first <b>observation.</b> <b>Values</b> smaller than zero indicate that the tree is relatively small and more likely to be suppressed, while values larger than zero indicate that the tree {{is more likely to be}} dominant.|$|R
40|$|This paper {{presents}} a novel feature for {{remote sensing image}} analysis, called multi-scale relative salience (MsRS) feature. It is constructed by modeling the process of feature value changing with scales. Firstly, the multi-scale <b>observation</b> <b>values</b> at each site are obtained by convolved with recursive Gaussian filters for efficiency. Secondly, the multi-scale <b>observation</b> <b>values</b> are compared with the initial value to generate the relative salience. Lastly, the relative salience between multi-scales are embed into a single feature called the MsRS. The scale in MsRS has explicit spatial meaning which is convenient to choose appropriate scale for specified object. In the MsRS map, the inner of each object become more consistent, while the contrast between object and background is enlarged. The MsRS {{can be used as}} preprocessing step of many applications, such as segmentation. Two state-of-art segmentations (the mean shift and the statistical region merging) are taken into experiments and the results proved that it brings improvement obviously. © 2013 Elsevier GmbH. This paper {{presents a}} novel feature for remote sensing image analysis, called multi-scale relative salience (MsRS) feature. It is constructed by modeling the process of feature value changing with scales. Firstly, the multi-scale <b>observation</b> <b>values</b> at each site are obtained by convolved with recursive Gaussian filters for efficiency. Secondly, the multi-scale <b>observation</b> <b>values</b> are compared with the initial value to generate the relative salience. Lastly, the relative salience between multi-scales are embed into a single feature called the MsRS. The scale in MsRS has explicit spatial meaning which is convenient to choose appropriate scale for specified object. In the MsRS map, the inner of each object become more consistent, while the contrast between object and background is enlarged. The MsRS can be used as preprocessing step of many applications, such as segmentation. Two state-of-art segmentations (the mean shift and the statistical region merging) are taken into experiments and the results proved that it brings improvement obviously. © 2013 Elsevier GmbH...|$|R
40|$|A {{comprehensive}} {{evaluation is}} conducted {{of the numerous}} attempts to estimate the mass of Neptune. It is noted that the two primary methods to mass-determination, respectively based on planetary perturbations and satellite motions, yield results of virtually equal accuracy for the mass of this planet. The attempts discussed encompass Triton <b>observations,</b> the <b>values</b> of Newcomb (1874), photographic <b>observations,</b> and <b>values</b> recently obtained from planetary and spacecraft observations...|$|R
30|$|Both the {{transition}} to the digital age, and the rise of Volunteered Geographic Information (VGI) call for a rethinking of traditional criteria to describe the resolution of data in GIScience. The ideas presented in this paper have shown one way to redefine resolution of observation and observation collections in order to accommodate both technical and human sensors. The article gave examples illustrating the applicability of a receptor-centric approach to observation resolution description. An immediate direction for future work is to extend the theory’s applicability to account for the thematic resolution of observations and observation collections. In addition, since current metadata documentation practices limit themselves to the documentation of the characteristics of <b>observation</b> <b>values,</b> further tests of the applicability of the theory can only be done as the practice of metadata documentation evolves towards a more explicit documentation of the quale’s contribution to the observation process. Finally, it became clear {{during the course of this}} work that a better understanding of the notion of quale (and especially its relationship with <b>observation</b> <b>value)</b> would help advance observation ontology.|$|R
30|$|A PKG gets {{distributed}} to the LAU server after the generation of private keys with Media Access Control (MAC) information. The LAU server transmits the weather <b>observation</b> <b>values</b> to the Data Gathering Server (DGS) signing it with a private key, and the transmitted signature gets inspected in the DGS. If the transmitted signature from the LAU server coincides with it, then the value is confirmed. If not, then the value is discarded.|$|R
40|$|IR {{absorption}} intensities {{are presented}} for thin crystalline films of HCN, HC 3 N, and C 4 N 2, together with n and k complex refractive indices determined {{on the basis}} of an iterative program for the Kramers-Konig integral via a least-squares, point-by-point fitting of the experimental transmission data. It is established that the transmission spectra generated by means of these n and k values can reproduce the experimental transmission <b>observation</b> <b>values</b> to within + or - 2 percent...|$|R
3000|$|... in (3) were set. Like before, {{the aim was}} to {{minimize}} the distances with respect to human <b>observations.</b> The <b>value</b> of r [...]...|$|R
30|$|In this research, we {{performed}} the registration {{of a new}} meteorological observation system using the mutual authentication. We utilized the Identity Based Signature to transmit the <b>observation</b> <b>value,</b> and verify {{the integrity of the}} transmitting result. In a case where a new weather sensor is added, we differentiated the area performing the mutual authentication between the data logger and the LAU Server from the area transmitting the observed data of the LAU server to the Data Gathering Server (DGS) utilizing the Identity Based Signature.|$|R
40|$|AbstractThe {{verification}} system aims at {{monitoring the}} forecast quality over time. Verification helps improving the forecast quality by knowing {{the strengths and}} weaknesses of the existing forecasting system and by comparing the quality of different forecasting methodologies. Thus, the web-based verification system has been developed for verification of forecast results that is produced by International Center for Theoretical Physics Regional Climate Model v 4 for our country. The forecasters and analysts can analyze the data in real-time with this web-based system. In this study, model values obtained from the system provided by ULAKBIM High Performance and Grid Computing Center. Model and station values were compared with each other for verification of model results with <b>observation</b> <b>values.</b> Therefore model grid values are transferred to station by using bi-linear and nearest neighbor (k-NN) (proximal) interpolation methods. This process in meteorological literature is called grid to point technique. Verification methods for forecasts of continuous variables are used to verify forecast <b>values</b> with <b>observation</b> <b>values.</b> Some verification methods; Mean Error, Mean Absolute Error and Root Mean Square Error, are calculated for validation. Verification results are shown as table and graphics on web-based system which is developed by the power of PHP (PHP: Hypertext Preprocessor) ...|$|R
40|$|Kriging as optimal spatial {{interpolation}} {{can produce}} less precise predictive value {{if there are}} outliers among the data. Outliers defined as extreme <b>observation</b> <b>value</b> of the other <b>observation</b> <b>values</b> that {{may be caused by}} faulty record keeping, improper calibration equipment or other posibbilities. Development of Ordinary Kriging method is Robust Kriging which transforms weight of clasic variogram thus become variogram that robust to outlier. The spatial data that used in this research is the spatial data that contains outliers and meet the assumptions of Ordinary Kriging. The analysis showed that the estimation value of Ordinary Kriging and Robust Kriging method is not much different in terms of Mean Absolute Deviation values which generated by both methods. An increase value of Mean Absolute Deviation on Robust Kriging estimation does not indicate that the Ordinary Kriging method is more precise than Robust Kriging method in the rainfall estimates of Amlapura control point remind that Robust Kriging does not eliminate the data of observation such as the Ordinary Kriging method. In general, Ordinary Kriging and Robust Kriging method can estimate the rainfall value of Amlapura control point quite well although it is not able to cover the changes in rainfall value that occurs due to the behavior geographic data. </p...|$|R
40|$|This paper {{develops}} a computational framework for optimizing {{the parameters of}} data assimilation systems {{in order to improve}} their performance. The approach formulates a continuous meta-optimization problem for parameters; the meta-optimization is constrained by the original data assimilation problem. The numerical solution process employs adjoint models and iterative solvers. The proposed framework is applied to optimize <b>observation</b> <b>values,</b> data weighting coefficients, and the location of sensors for a test problem. The ability to optimize a distributed measurement network is crucial for cutting down operating costs and detecting malfunctions...|$|R
40|$|The {{purpose of}} this study is to develop a {{decision}} support model for selecting the most appropriate players in Badminton via Fuzzy Multi Attribute Decision Making (FMADM) algorithm. Developed model has been applied on active Badminton players in age between 9 and 11 among Muğla Province by using the data of player’s physical appropriateness scores and technical <b>observation</b> <b>values</b> in computer environment. In developed model, it has been ensured that using both significant criteria and the weights of criteria make results more sensitive. Finally, it is proved that model’s results are appropriate and consistent...|$|R
50|$|Post-autistic {{economics}} {{criticizes the}} focus on formal models {{at the expense of}} <b>observation</b> and <b>values,</b> arguing for a return to the moral philosophy in which Adam Smith founded this human science.|$|R
