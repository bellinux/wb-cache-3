0|10000|Public
50|$|Rounding/truncation errors due to {{the limited}} <b>precision</b> <b>of</b> the <b>registers.</b>|$|R
5000|$|... where K is a scaling {{constant}} {{determined by}} the <b>precision</b> (size) <b>of</b> the <b>registers</b> as follows: ...|$|R
40|$|I report {{evidence}} {{which raises}} {{doubts about the}} reliability and validity <b>of</b> age-sex <b>registers</b> as true population denominators in general practice in the UK. These have potentially disturbing implications for the interpretation of data based on the presumed <b>precision</b> <b>of</b> age-sex <b>registers.</b> I am undertaking a prospective study to identify and quantify sources of inaccuracy to try to establish a method of estimating the true population at risk and its true age-sex characteristics. This would greatly enhance the utility <b>of</b> the age-sex <b>register</b> as the most valuable tool in general-practice research...|$|R
40|$|Reducing the <b>precision</b> <b>of</b> floating-point values {{can improve}} {{performance}} and/or reduce energy expenditure in computer graphics, among other, applications. However, reducing the <b>precision</b> level <b>of</b> floating-point values {{in a controlled}} fashion needs support both at the compiler and at the microarchitecture level. At the compiler level, a method is needed to automate the reduction <b>of</b> <b>precision</b> <b>of</b> each floating-point value. At the microarchitecture level, a lower <b>precision</b> <b>of</b> each floating-point <b>register</b> can allow more floating-point values to be packed into a register file. This, however, calls for new register file organizations. This article proposes an automated precision-selection method and a novel GPU register file organization that can store floating-point register values at arbitrary precisions densely. The automated precision-selection method uses a data-driven approach for setting the <b>precision</b> level <b>of</b> floating-point values, given a quality threshold and a representative set of input data. By allowing a small, but acceptable, degradation in output quality, our method can remove {{a significant amount of}} the bits needed to represent floating-point values in the investigated kernels (between 28 % and 60 %). Our proposed register file organization exploits these lower-precision floating-point values by packing several of them into the same physical register. This reduces the register pressure per thread by up to 48 %, and by 27 % on average, for a negligible output-quality degradation. This can enable GPUs to keep up to twice as many threads in flight simultaneously...|$|R
40|$|Monitoring {{continuous}} queries over distributed {{data sources}} typically requires replicating data continuously at a central location for query monitoring, incurring significant communication overhead when source data is updated. We propose {{a new technique}} that reduces communication overhead by using approximate caching. Our approach enables users to register continuous queries with precision constraints. The system caches approximate data values with just enough accuracy to meet the <b>precision</b> constraints <b>of</b> all <b>registered</b> continuous queries at all times, while dynamically and adaptively allocating imprecision among cached values using an algorithm that minimizes data refreshes. Through experimental simulation over synthetic and real-world data, we demonstrate the effectiveness of our approach in reducing communication costs significantly compared with other approaches. Most importantly, we show that our algorithm enables users to trade precision for communication cost at a fine granularity by individually adjusting the <b>precision</b> constraints <b>of</b> continuous queries in a large multiquery workload, a feature that no known previous algorithm can provide. ...|$|R
3000|$|... 20 Notice {{that the}} small sample size for certain type of {{transitions}} could affect the <b>precision</b> <b>of</b> the estimates. We have performed a similar analysis using a larger data set (0.5 % <b>of</b> <b>register</b> data for Denmark and the Continuous Family Expenditure Survey for Spain) and the results are very similar to those found with the ECHP.|$|R
30|$|The minimum <b>precision</b> <b>of</b> BAN time {{combines}} {{both the}} <b>precision</b> <b>of</b> SPS-SE and RATS.|$|R
6000|$|... "No, mon ami, I {{am not in}} {{my second}} childhood! I steady my nerves, that is all. This {{employment}} requires <b>precision</b> <b>of</b> the fingers. With <b>precision</b> <b>of</b> the fingers goes <b>precision</b> <b>of</b> the brain. And never have I needed that more than now!" ...|$|R
3000|$|... {{does not}} {{significantly}} affect <b>precision</b> <b>of</b> the clock prediction in Indoor and Outdoor II. Indoor achieves a <b>precision</b> <b>of</b> 20.96 μ s with [...]...|$|R
40|$|AbstractIt {{is shown}} that the ratio <b>of</b> the <b>precision</b> <b>of</b> the stable Kronrod {{extension}} to the <b>precision</b> <b>of</b> the stable version of the Gauss rule for Cauchy principal-value integrals is approximately {{the same as the}} ratio <b>of</b> the <b>precision</b> <b>of</b> the Kronrod extension to that of the Gauss rule for ordinary integrals...|$|R
40|$|Gravity {{anomalies}} {{and vertical}} deflections are important products of altimetry satellites. The <b>precision</b> indexes <b>of</b> them {{are essential for}} the design of future altimetry satellites. In this paper, the spherical harmonic function is used to discuss the <b>precisions</b> <b>of</b> gravity anomaly and vertical deflections. Firstly, the approximate matching relationship between gravity anomaly and vertical deflection error is deduced theoretically. Then, six ultra-high degree gravity field models are used to verify the correctness of the conclusions. The results of numerical experiments show that the errors of vertical defections and gravity anomaly satisfy the approximate proportional relation, that is, if the <b>precision</b> <b>of</b> vertical deflection is 1 μ rad, the <b>precision</b> <b>of</b> gravity anomaly is about 1. 4 mGal. Conversely, if the <b>precision</b> <b>of</b> the gravity anomaly is 1 mGal, the <b>precision</b> <b>of</b> the corresponding vertical deflection is about 0. 7 μ rad...|$|R
30|$|Depth {{discrimination}} thresholds {{were used}} as a measure <b>of</b> the <b>precision</b> <b>of</b> relative depth judgments. Depth realism {{might be expected to}} increase as the <b>precision</b> <b>of</b> relative depth judgments increases.|$|R
50|$|More formally, this is downsampling in {{the time}} dimension, as it is {{reducing}} the resolution (<b>precision</b> <b>of</b> the input), not the bit rate (<b>precision</b> <b>of</b> the output, as in posterization).|$|R
25|$|The {{accuracy}} of an analog computer {{is limited by}} its computing elements as well as quality of the internal power and electrical interconnections. The <b>precision</b> <b>of</b> the analog computer readout was limited chiefly by the <b>precision</b> <b>of</b> the readout equipment used, generally three or four significant figures. The <b>precision</b> <b>of</b> a digital computer {{is limited by the}} word size; arbitrary-precision arithmetic, while relatively slow, provides any practical degree <b>of</b> <b>precision</b> that might be needed.|$|R
30|$|The <b>precision</b> <b>of</b> {{the density}} meter was ± 1 %. The <b>precision</b> <b>of</b> the RTD was ± 0.5 °C. Hence, the {{uncertainty}} of the density experiment was less than ± 2.7 %.|$|R
40|$|Abstract. Using Chebyshev {{polynomial}} to fit precise ephemeris of GPS, the nodes selection has {{a certain}} influence on the precision. In this paper we use 3 kinds of precise ephemeris (IGF, IGR, IGU) to analyze the difference <b>precision</b> <b>of</b> randomly selected interpolation node and Chebyshev points fitting orbit and compare the difference and <b>precision</b> <b>of</b> fitting orbit by 3 kinds of ephemeris and orbit provided by IGS. The result shows that using Chebyshev points to fit precise ephemeris, the <b>precision</b> <b>of</b> IGF and IGR can achieve mm levels, the <b>precision</b> <b>of</b> IGU can achieve cm levels...|$|R
30|$|The <b>precision</b> <b>of</b> the {{electric}} conductivity meter was ± 3 %. The <b>precision</b> <b>of</b> the RTD was ± 0.5 °C. Hence, {{the uncertainty of}} {{the electric}} conductivity experiment was less than ± 3.9 %.|$|R
30|$|The <b>precision</b> <b>of</b> {{the thermal}} {{property}} analyzer was ± 5 %. The <b>precision</b> <b>of</b> the RTD was ± 0.5 °C. Hence, {{the uncertainty of}} the thermal conductivity experiment was less than ± 5.6 %.|$|R
40|$|A fast reversed-phase UPLC {{method was}} {{developed}} for squalene determination in Sicilian pistachio samples that entry in the European <b>register</b> <b>of</b> the products with P. D. O. In the present study the SPE procedure was optimized for the squalene extraction prior to the UPLC/PDA analysis. The <b>precision</b> <b>of</b> the full analytical procedure was satisfactory and the mean recoveries were 92. 8 + 0. 3...|$|R
3000|$|The {{results show}} that the <b>precision</b> <b>of</b> a {{realistic}} multihop network with 4 hops is about 3 [*] milliseconds and even maintains the calculated worst case <b>precision</b> <b>of</b> a fully connected network multiplied by [...]...|$|R
40|$|AbstractRecent studies {{demonstrate}} the importance <b>of</b> computational <b>precision</b> <b>of</b> user equilibrium traffic assignment so- lutions for scenario comparisons. When traffic assignment is hierarchically {{embedded in a}} model for network design and/or road pricing, not only the <b>precision</b> <b>of</b> the solution itself becomes more important, but also the <b>precision</b> <b>of</b> its derivatives {{with respect to the}} design parameters should be considered. The main {{purpose of this paper is}} to present a method for precise computations of equilibrium derivatives. Numeri- cal experiments on a small network are used for two evaluations: 1. <b>precision</b> <b>of</b> computed equilibrium derivatives; and 2. the impact of precise derivatives on the quality of network design solutions...|$|R
5000|$|... { INCrement the {{contents}} <b>of</b> <b>register</b> r, CLeaR {{the contents}} <b>of</b> <b>register</b> r, IF contents <b>of</b> <b>register</b> rj Equals the contents <b>of</b> <b>register</b> rk THEN Jump to instruction Iz ELSE goto to next instruction } ...|$|R
50|$|Note {{that these}} {{precision}} numbers {{are for the}} interpolated values relative original tabulated coordinates. The overall <b>precision</b> and accuracy <b>of</b> interpolated values for describing the actual motions of the planets will be a function <b>of</b> both the <b>precision</b> <b>of</b> the ephemeris tabulated coordinates and the <b>precision</b> <b>of</b> the interpolation.|$|R
5000|$|... { INCrement the {{contents}} <b>of</b> <b>register</b> r, COPY {{the contents}} <b>of</b> <b>register</b> rj to register rk, IF contents <b>of</b> <b>register</b> rj Equals the contents <b>of</b> <b>register</b> rk then Jump to instruction Iz ELSE goto to next instruction } ...|$|R
5000|$|Demonstration of the {{validity}} of those corrections through eclipses, <b>precision</b> <b>of</b> equinoxes, calculation and the correction, correction of the periphery of the manda epicycle, discussion on the <b>precision</b> <b>of</b> the equinoxes, corrections due to the precession of the equinoxes ...|$|R
30|$|Furthermore, the <b>precision</b> <b>of</b> {{timestamps}} in HINT {{depends on}} the logic function of FPGA {{and the frequency of}} crystal oscillator. In our implementation, the <b>precision</b> <b>of</b> timestamp is about tens of nanoseconds, which is far superior to the existing testbed.|$|R
30|$|The <b>precision</b> <b>of</b> {{the strain}} {{measurement}} data at ISA is 1 × 10 − 9 in the E 1 and E 3 directions. The <b>precision</b> <b>of</b> the E 2 component, obtained during the 2011 eruption of Shinmoe-dake, is somewhat poorer than the <b>precision</b> <b>of</b> the E 1 and E 3 components, at around 2 – 3 × 10 − 9, primarily because the electrical {{system in this}} part of the vault was damaged during a lightning strike one month before the eruption.|$|R
5000|$|The program HALTs {{with the}} {{contents}} <b>of</b> <b>register</b> #2 at its original count and the contents <b>of</b> <b>register</b> #3 {{equal to the}} original contents <b>of</b> <b>register</b> #2, i.e., ...|$|R
30|$|The <b>precision</b> <b>of</b> the {{rheometer}} is ± 1 %. The {{accuracy of}} the precise electric balance is ± 0.01 g. The <b>precision</b> <b>of</b> the RTD is ± 0.5 °C Hence, {{the uncertainty of the}} viscosity experiment was calculated to be less than ± 2.7 %.|$|R
40|$|AbstractThe {{statistical}} {{tools such}} as descriptive statistics, full factorial design and analysis of source of variation were used to identify the potential factors that impact the validity of testing method for determining the strength of cement. The results showed that personal error impacted both accuracy and <b>precision</b> <b>of</b> test greatly. Experimental time associated with temperature fluctuation resulted in strength variation but did not impact the <b>precision</b> <b>of</b> test in all curing ages. Different compactions did not impact the <b>precision</b> <b>of</b> test but resulted in the strength variation on 3 d and 28 d significantly. Different methods for the initial moist air curing significantly impacted the <b>precision</b> <b>of</b> testing method and resulted in the strength variation of cement on 1 d...|$|R
40|$|Accurate GPS-derived ZTD (zenith tropospheric delay) plays a {{key role}} in near {{real-time}} weather forecasting, especially in improving the <b>precision</b> <b>of</b> Numerical Weather Prediction (NWP) models. However, ZTD is usually estimated under the assumption that all the GPS measurements, carrier phases or pseudo-ranges, have the same accuracy. These assumptions are unrealistic, which will inevitably degrade the <b>precision</b> <b>of</b> ZTD estimation. This communication aims to further improve the <b>precision</b> <b>of</b> ZTD estimation by stochastic modelling. The results show that the <b>precision</b> <b>of</b> GPS-derived ZTD can be obviously improved using a suitable stochastic model for GPS measurements. The stochastic model using satellite elevation angle-based cosine function is better than other investigated stochastic models. This improvement of ZTD estimation is certainly critical for reliable NWP and other tropospheric delay corrections...|$|R
40|$|For {{combining}} {{results from}} independent experiments, {{it is essential}} that information about the <b>precision</b> <b>of</b> the estimates of treatment effects is available. In publications of horticultural experiments, the results of multiple comparisons tests are often reported without suffi cient information about the <b>precision</b> <b>of</b> the experiments. Based on limited information <b>of</b> the <b>precision</b> <b>of</b> an experiment such as treatments with the same letter are not signifi cantly different, we develop a method for extracting a possible range <b>of</b> the <b>precision</b> <b>of</b> the experiment which can then be used for meta-analysis. The procedure is demonstrated using a real data example where alternatives to methyl bromide are studied in pre-plant soil fumigation. We also provide an R program which computes the possible range <b>of</b> the <b>precision...</b>|$|R
5000|$|... { INCrement {{contents}} <b>of</b> <b>register</b> r, DECrement contents <b>of</b> <b>register</b> r, IF contents <b>of</b> <b>register</b> r is Zero THEN Jump {{to instruction}} Iz ELSE continue to next instruction }: ...|$|R
40|$|In {{order to}} improve the <b>precision</b> <b>of</b> service {{composition}}, a QoS-based and similarity-based service composition method is proposed, which introduces the similarity, which is calculated by service discovery phase, into Web service composition phase. First, normalize the individual service QoS. Secondly, polymerize the workflow and calculate the QoS of individual service. Make combination between QoS and service discovery phase of its similarity to analysis the final score of a single service. Finally, fetch the workflow execution path by using backtracking algorithm. Experimental {{results show that the}} <b>precision</b> <b>of</b> QoS-based and similarity-based service composition algorithm precision is better than the <b>precision</b> <b>of</b> traditional QoS-based service composition algorithm. In {{order to improve}} the <b>precision</b> <b>of</b> service composition, a QoS-based and similarity-based service composition method is proposed, which introduces the similarity, which is calculated by service discovery phase, into Web service composition phase. First, normalize the individual service QoS. Secondly, polymerize the workflow and calculate the QoS of individual service. Make combination between QoS and service discovery phase of its similarity to analysis the final score of a single service. Finally, fetch the workflow execution path by using backtracking algorithm. Experimental results show that the <b>precision</b> <b>of</b> QoS-based and similarity-based service composition algorithm precision is better than the <b>precision</b> <b>of</b> traditional QoS-based service composition algorithm...|$|R
5000|$|For every non-leaf node n, {{assign the}} number <b>of</b> <b>registers</b> needed to {{evaluate}} the respective subtrees of n. If the number <b>of</b> <b>registers</b> needed in the left subtree (l) are not equal to the number <b>of</b> <b>registers</b> needed in the right subtree (r), the number <b>of</b> <b>registers</b> needed for the current node n is max(l, r). If l == r, then the number <b>of</b> <b>registers</b> needed for the current node is r + 1.|$|R
30|$|The figure shows that, {{when the}} number of {{training}} instance is small, the <b>precision</b> <b>of</b> the ensemble training strategy in the presented parallel BPNN outperforms that of a standalone BPNN algorithm. The figure also tells that the <b>precision</b> <b>of</b> ensemble training strategy increases stably without fluctuations.|$|R
