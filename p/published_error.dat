26|205|Public
50|$|In 1986, British Labour {{politician}} and journalist Chris Mullin <b>published</b> <b>Error</b> of Judgement: Truth About the Birmingham Bombings, which provided {{further evidence that}} the men had been wrongly convicted. The book included anonymous interviews {{with some of those}} who claimed to {{have been involved in the}} bombings, and who claimed the protocol 30-minute warning bomb warning had been delayed because the preselected telephone box had been vandalised, and that by the time another telephone box was found, the advance warning had been significantly delayed.|$|E
50|$|It is also {{possible}} for laboratories to have systematic errors, caused by weaknesses in their methodologies. For example, if 1% of the benzene in a modern reference sample is allowed to evaporate, scintillation counting will give a radiocarbon age that is too young by about 80 years. Laboratories work to detect these errors both by testing their own procedures, and by periodic inter-laboratory comparisons {{of a variety of}} different samples; any laboratories whose results differ from the consensus radiocarbon age by too great an amount may be suffering from systematic errors. Even if the systematic errors are not corrected, the laboratory can estimate the magnitude of the effect and include this in the <b>published</b> <b>error</b> estimates for their results.|$|E
40|$|Real time {{digital audio}} {{delivery}} over Wireless Local Area Networks (WLANs) represents an attractive, flexible and cost effective framework for realizing high-quality, multichannel home audio applications. However, the unreliable nature of WLANs IP link frequently imposes significant playback quality degradation, due to delay or permanent {{loss of a}} number of transmitted digital audio packets. In this paper, a novel packet error concealment technique is presented, based on the spectral reconstruction of the statistical equivalent of a previously successfully received audio data packet. It is shown that the proposed data reconstruction scheme outperforms previously <b>published</b> <b>error</b> concealment strategies, in both terms of objective and perceptual criteria...|$|E
5000|$|Due {{to either}} a <b>{{publishing}}</b> <b>error</b> or the vagaries of publishing issue number 9 of volume one {{does not seem to}} exist, but issues 8 and 10 do.|$|R
30|$|Following {{publication}} {{of this article}} [1], {{it has come to}} our attention that the minus sign “-” throughout the paper is missing or not displayed in the PDF version. This is due to a <b>publishing</b> <b>error,</b> and the mistake is not caused by the authors.|$|R
50|$|The Los Angeles County Metropolitan Transportation Authority {{has criticized}} the Courier for <b>publishing</b> <b>errors</b> and {{misleading}} statements {{with regards to}} the Westside Subway Extension. Damien Newton of LA streetsblog has accused the Courier of publishing libel with its coverage of seismology experts who weighed in on the geotechnical issues facing the subway extension.|$|R
40|$|Software {{maintenance}} {{forms an}} essential component of software development. Its planning includes estimation of maintenance effort, duration, personnel and costs. Adequate information regarding size, complexity and maintainability is however often unavailable. In the present work, a Neural Network (NN) based effort estimator is developed using Matlab. A feed forward back-propagation NN employing Bayesian regularization training is selected and trained for one dataset. Various categories of software maintenance cost drivers and their effect on maintenance effort have been analyzed using different combinations of number of hidden layers and hidden neurons etc. The NN is able to successfully model the maintenance effort as the obtained results are well within the previously <b>published</b> <b>error</b> limits...|$|E
40|$|We study Krylov {{subspace}} {{methods for}} approximating the matrix-function vector product φ(tA) b where φ(z) = [exp(z) - 1]/z. This product arises in the numerical integration of large stiff systems of differential equations by the Exponential Euler Method, where A is the Jacobian matrix of the system. Recently, this method has found application in the simulation of transport phenomena in porous media within mathematical models of wood drying and groundwater flow. We develop an a posteriori {{upper bound on}} the Krylov subspace approximation error and provide a new interpretation of a previously <b>published</b> <b>error</b> estimate. This leads to an alternative Krylov approximation to φ(tA) b, the so-called Harmonic Ritz approximant, which we find does not exhibit oscillatory behaviour of the residual error...|$|E
40|$|BACKGROUND: Mislabeled {{samples are}} a serious prob-lem in most {{clinical}} laboratories. <b>Published</b> <b>error</b> rates range from 0. 39 / 1000 to {{as high as}} 1. 12 %. Standardiza-tion of bar codes and label formats has not yet achieved the needed improvement. The mislabel rate in our lab-oratory, although low compared with published rates, prompted us to seek a solution to achieve zero errors. METHODS: To reduce or eliminate our mislabeled sam-ples, we invented an automated device using 4 cameras to photograph the outside of a sample tube. The system uses optical character recognition (OCR) to look for discrepancies between the patient name in our labora-tory information system (LIS) vs the patient name on the customer label. All discrepancies detected by the system’s software then require human inspection. Th...|$|E
50|$|American Girl Place (photographs), <b>published</b> Domain <b>Errors,</b> Autonomedia, New York, NY; 2002.|$|R
30|$|Point {{accuracy}} {{was also}} expressed using the recently <b>published</b> surveillance <b>error</b> grid [13].|$|R
40|$|Announcement: Having {{written a}} bunch of {{articles}} in this vein for Word Ways, I 2 ̆ 7 m now {{working on a book}} with the same theme. If you encounter any amusing <b>published</b> <b>errors</b> or other content that lends itself to this treatment, I would appreciate receiving them. To document authenticity, I need originals or copies, sourced with the publication 2 ̆ 7 s name and date...|$|R
40|$|Since {{they often}} embody compact but mathematically {{sophisticated}} algorithms, operations for computing the common transcendental functions in {{floating point arithmetic}} seem good targets for formal verification using a mechanical theorem prover. We discuss some of the general issues that arise in verifications of this class, and then present a machine-checked verification of an algorithm for computing the exponential function in IEEE- 754 standard binary floating point arithmetic. We confirm (indeed strengthen) the main result of a previously <b>published</b> <b>error</b> analysis, though we uncover a minor error in the hand proof and are forced to confront several subtle issues that might easily be overlooked informally. 1 Introduction Algorithms for performing floating point operations are often rather complex. It is difficult {{to make sure they}} are correct [...] - for example, a bug in the floatingpoint division instruction of the Intel Pentium gained widespread publicity quite recently (Pratt 1995). W [...] ...|$|E
40|$|Previously <b>published</b> <b>error</b> budgets {{have focused}} on {{spacecraft}} error sources for pointing error and have tended to include only spacecraft pointing rather than the ultimate geolocation of each pixel of dam onto a well-defined spot {{on the surface of}} the Earth. A systematic approach to geolocation error budgeting, including all contributors in the geolocation process is presented. Its structure allows simultaneous expression of the needs of instrument teams as well as spacecraft design teams. It allows explicit acknowledgement of approximations made for on-board control as well as the ultimate geolocation accuracy achievable after ground processing and exploits the commonality inherent in the on-board and post-processing error budgets. It also includes the uncontrolled and unmeasured spacecraft jitter. This approach can be used to investigate the mission-wide benefit of a variety of design choices, (such as on-board sensing and ground correction of measurements contrasted with on-board correction of measurements). Additionally, in light of increasing accuracy requirements as sensor resolution improves, numerous non-spacecraft contributors to geolocation error are quantified...|$|E
40|$|This paper {{describes}} an empirical study on typing errors made by children during a text copy exercise. The literature on text input errors is first examined, focussing on studies of errors that occur during keyboard typing. A study of errors made by children during typing is described {{and the results}} from this study are analysed using visual inspection and already <b>published</b> <b>error</b> categorisation methods. These methods are compared with respect to the types and number of errors categorised and uncategorised. We identify and define new kinds of typing errors and use these, together with previously defined error types, to outline an expanded and more detailed method (ExpECT) for the classification of typing errors. ExpECT is compared with the previously examined categorisation methods and is shown to be a more thorough and broader method for the analysis of typing errors. Categories and Subject Descriptors H. 5. 5 [Information Interfaces and Presentation (e. g., HCI) ]: User Interfaces- input devices and strategies, interaction styles, evaluation/methodology...|$|E
5000|$|Returning to {{teaching}} and continuing {{to work on a}} book she would later <b>publish,</b> <b>Errors</b> and Expectations, left her with little time to spend outside of the basic writing field. She did have some family concerns which needed her attention, however. Her father’s arthritis worsened and her mother suffered several angina attacks and died in 1974. A close friend also died during this period.|$|R
40|$|We present {{principles}} {{for the design}} of cryptographic protocols. The principles are neither necessary nor sufficient for correctness. They are however helpful, in that adherence to them would have avoided a considerable number of <b>published</b> <b>errors.</b> Our principles are informal guidelines. They complement formal methods, but do not assume them. In order to demonstrate the actual applicability of these guidelines, we discuss some instructive examples from the literature. ...|$|R
50|$|E.F. Riek synonymized Eoses triassica with M. proavita in 1955, {{regarding}} it as {{a second}} specimen. In doing so, he identified the fossil as a mecopteran rather than a lepidopteran as it was originally described as. Citing morphological differences in wing venation and <b>publishing</b> <b>errors,</b> Norman B. Tindale challenged this conclusion in 1980. He maintains that the three known specimens of Eoses triassica belong to the lepidopteran family Eocoronidae.|$|R
40|$|Abstract—This paper {{addresses}} {{the prediction of}} error floors of low-density parity-check (LDPC) codes with variable nodes of constant degree in the additive white Gaussian noise (AWGN) channel. Specifically, {{we focus on the}} performance of the sum-product algorithm (SPA) decoder formulated in the log-likelihood ratio (LLR) domain. We hypothesize that several <b>published</b> <b>error</b> floor levels are due to the manner in which decoder implementations handled the LLRs at high SNRs. We employ an LLR-domain SPA decoder that does not saturate near-certain messages and find the error rates of our decoder to be lower by at least several orders of magnitude. We study the behavior of trapping sets (or near-codewords) that are the dominant cause of the reported error floors. We develop a refined linear model, {{based on the work of}} Sun and others, that accurately predicts error floors caused by elementary tapping sets for saturating decoders. Performance results of several codes at several levels of decoder saturation are presented. Index Terms—Low-density parity-check (LDPC) code, belief propagation (BP), error floor, linear analysis, Margulis code, absorbing set, near-codeword, trapping set. I...|$|E
40|$|Abstract—This paper {{addresses}} {{the prediction of}} error floors of variable-regular Low Density Parity Check (LDPC) codes in the Additive White Gaussian Noise (AWGN) channel. Specifically, {{we focus on the}} Sum-Product Algorithm (SPA) decoder in the log-domain at high SNRs. We hypothesize that several <b>published</b> <b>error</b> floor levels are due to numerical saturation within their decoders when handling high SNRs. We take care to develop a log-domain SPA decoder that does not saturate near-certain messages and find the error rates of our decoder to be lower by at least several orders of magnitude. We study the behavior of near-codewords / trapping sets that dominate the reported error floors. J. Sun, in his Ph. D. thesis, used a linear system model to show that error floors due to elementary trapping sets don’t exist under certain conditions, assuming that the SPA decoder is non-saturating [1]. We develop a refined linear model which we find to be capable of predicting the error floors caused by elementary trapping sets for saturating decoders. Performance results of several codes at several levels of decoder saturation are presented. I...|$|E
40|$|This paper {{addresses}} {{the prediction of}} error floors of low-density parity-check (LDPC) codes with variable nodes of constant degree in the additive white Gaussian noise (AWGN) channel. Specifically, {{we focus on the}} performance of the sum-product algorithm (SPA) decoder formulated in the log-likelihood ratio (LLR) domain. We hypothesize that several <b>published</b> <b>error</b> floor levels are due to the manner in which decoder implementations handled the LLRs at high SNRs. We employ an LLR-domain SPA decoder that does not saturate near-certain messages and find the error rates of our decoder to be lower by at least several orders of magnitude. We study the behavior of trapping sets (or near-codewords) that are the dominant cause of the reported error floors. We develop a refined linear model, {{based on the work of}} Sun and others, that accurately predicts error floors caused by elementary tapping sets for saturating decoders. Performance results of several codes at several levels of decoder saturation are presented. Comment: 24 pages, 15 figures, submitted to IEEE Transactions on Information Theor...|$|E
50|$|The {{remaining}} {{chapters of}} the book follow {{the fortunes of the}} experimental subjects over the next 12 months. In each case the prediction comes true, though in an unexpected way. After a year of anticipation, Charles Ottery discovers that, {{as a result of a}} <b>publishing</b> <b>error,</b> the report he took to be of his own death is in fact a report of the death of another man of the same name.|$|R
5000|$|... #Caption: Hondius {{made use}} of Wrights {{calculations}} without acknowledgment in his [...] "Christian Knight Map" [...] of 1597, prompting Wright to <b>publish</b> Certaine <b>Errors</b> in Navigation in 1599.|$|R
5000|$|The {{final two}} tracks, [...] "Asik Vaysel" [...] and [...] "Andalusia", were both {{inspired}} by the late Aşık Veysel, a critically acclaimed figure of Turkish folk literature. [...] "Andalusia" [...] features a melody (from 1:40 to 1:53) which was previously played by Satriani on his 1993 video The Satch Tapes, during an acoustic guitar segment. On a podcast prior to the album's release, Satriani explained that a <b>publishing</b> <b>error</b> was the reason why Aşık Veysel's name was misspelled [...] "Asik Vaysel" [...] on the back cover, {{and that it would}} be corrected on subsequent pressings.|$|R
40|$|The Newark-APTS {{established}} a high-resolution {{framework for the}} Late Triassic and Early Jurassic. Palaeomagnetic polarity correlations to marine sections show that stage-level correlations of continental sequences were off {{by as much as}} 10 million years. New Uâ€“Pb ages show the new correlations and the Newark basin astrochronology to be accurate. Correlation of Newark-APTS to the Chinle Formation/Dockum Group, Glen Canyon Group, Fleming Fjord Formation and Ischigualasto Formation led to the following conclusions: (1) there are no unequivocal Carnian-age dinosaurs; (2) the Norian Age was characterised by a slowly increasing saurischian diversity but no unequivocal ornithischians; (3) there was profound Norian and Rhaetian continental provinciality; (4) the classic Chinle-, Germanic- and Los Colorados-type assemblages may have persisted to the close of the Rhaetian; (5) the distinct genus-level biotic transition traditionally correlated with the marine Carnianâ€“Norian is in fact mid-Norian in age and within <b>published</b> <b>error</b> of the Manicouagan impact; (6) the end-Triassic marine and continental extinctions as seen in eastern North America were contemporaneous; and (7) compared to Triassic communities, Hettangian and Sinemurian age terrestrial communities were nearly globally homogenous and of low diversity. Consequently, the complex emerging picture of dinosaur diversification demands biostratigraphically-independent geochronologies in each of the faunally-important regions...|$|E
40|$|Reluctance {{to adopt}} fine-needle {{aspiration}} (FNA) of dominant thyroid nodules stems largely from fear of overlooking a malignancy in a nodule diagnosed as benign on FNA (false-negative error). <b>Published</b> <b>error</b> {{rates have been}} derived from surgical series {{without regard to the}} outcome of those who were followed without operation. In order to ascertain the overall false-negative error rate, the authors conducted a prospective study in 600 patients who underwent FNA. Among the 482 study patients who had a benign FNA diagnosis or inadequate specimens, 117 underwent surgery because of the concurrent large-needle biopsy result or a clinical suspicion of malignancy. Eight false-negative errors were identified in this group, 5 of which were detected by large-needle biopsy. Among the remaining 365 patients who were followed for an average of 2 1 / 2 years, 2 patients were found to have well-differentiated carcinomas in recurrent cysts. The overall false-negative error rate of FNA alone in all 482 patients was 2. 1 %. This was reduced to 1. 0 % by the use of concurrent large-needle biopsy. Properly applied, FNA can reduce unnecessary surgery among patients with clinically benign nodules without incurring an unacceptably high false-negative error rate. Furthermore, this error rate may be reduced substantially by combining large-needle biopsy with FNA and by close follow-up with surgery performed later in patients who manifest clinical features suggestive of malignancy. link_to_subscribed_fulltex...|$|E
40|$|AbstractThe only <b>published</b> <b>error</b> {{analysis}} for an approximation algorithm computing the Riemann zeta-function ζ(s), due to Henri Cohen and Michel Olivier, does evaluate {{the error of}} the approximation, but is not concerned {{by the fact that}} the computations required to calculate this approximation will be carried on with a finite precision arithmetic (by a computer), and thus produce other (rounding) errors. As a first step towards clearing this matter we provide a complete error analysis of the Cohen–Olivier algorithm when s is real with s⩾ 1 / 2, s≠ 1. We prove that, if s can be written with Ds bits in base 2, then in order to compute ζ(s) in any relative precision P⩾ 11, that is, in order to compute a P-bit number ζP(s) such that |ζP(s) −ζ(s) | is certified to be smaller than the number represented by a “ 1 ” at the Pth and last significant bit-place of |ζP(s) |, it is sufficient to perform all the computations (i. e. additions, subtractions, multiplications, divisions, and computation of k−s for integers k⩾ 2) with an internal precisionD=max(Ds,P+max(14,⌈ 3 logP 2 log 2 + 2. 71 ⌉)), and then to round to the nearest P-bits number. For instance if the wanted precision is P= 1000 (and if s has no more than 1018 significant bits), then an internal precision D= 1018 is sufficient...|$|E
50|$|Iris kemaonensis (or Kumaon iris) is {{a species}} in the genus Iris, it {{is also in the}} {{subgenus}} of Iris and in the Pseudoregelia section. It is a rhizomatous perennial, from Tibetan China, Bhutan, India, Kashmir and Nepal. It has It has light green or yellowish green leaves, that extend after flowering time. It has a short stem, 1-2 fragrant flowers that are purple, lilac, lilac-purple or pale purple. They also have darker coloured blotches or spots.It is cultivated as an ornamental plant in temperate regions. It is often known as Iris kumaonensis, due to a <b>publishing</b> <b>error.</b>|$|R
40|$|Abstract. The {{discussion}} in the preceding paper is restricted to the uncertainties in magnetic-field-line tracing in the magnetosphere resulting from <b>published</b> standard <b>errors</b> in the spherical harmonic coefficients that define the axisymmetric part of the internal geomagnetic field (i. e. g 0 n 6 δg 0 n). Numerical estimates of these uncertainties based on an analytic equation for axisymmetric field lines are in excellent agreement with independent computational estimates based on stepwise numerical integration along magnetic field lines. This comparison confirms {{the accuracy of the}} computer program used in the present paper to estimate the uncertainties in magnetic-field-line tracing that arise from <b>published</b> standard <b>errors</b> in the full set of spherical harmonic coefficients, which define the complet...|$|R
30|$|In the {{original}} {{version of this article}} (Ahmed et al. 2014), the authors noted that the author list was <b>published</b> with <b>errors.</b> The correct author list can be found in this erratum. The publisher would like to apologise to the authors for this error.|$|R
40|$|With {{transistor}} budgets ever expanding, microprocessor architects are steadily integrat-ing new {{and more}} sophisticated mechanisms into their designs to boost performance. To cope with this increase in complexity, successful processor verification efforts must employ a vari-ety of complementary verification technologies to achieve an acceptable level of functional cor-rectness in the final product. Research on prac-tical verification techniques for microprocessors has long been impeded {{by the lack of}} <b>published</b> <b>error</b> data, despite the abundance of design errors in large-scale projects. It is common indus-try practice to record design errors, but this infor-mation is considered proprietary and, perhaps, embarrassing, so it rarely appears in public. Detailed error data is especially valuable to veri-fication approaches that use error models to direct test generation. 1, 2 Furthermore, sets of designs and corresponding errors can serve as benchmarks to compare different verification methods. Finally, statistical reliability analysis methods rely heavily on this type of data. 3 These considerations led us to turn to acad-emia as a source of error data from micro-processor design. We first report on the modest amount of error data published by industry. We then describe our method to systematical-ly collect design error data from university design projects. After presenting and analyzing the data we collected, we offer some advice on the collection process based on lessons painfully learned. Industrial Error Data Although design errors that make their way into final products are common, microproces-sor manufacturers have not always been forth-coming about them. This has changed sinc...|$|E
40|$|Single {{nucleotide}} polymorphisms (SNP) {{may be used}} in case-control {{designs to}} test for association between a SNP marker and a disease. Such designs may assume that the genotype data are reported without error. Our goal is quantifying the effects that errors have on sample size for case-control studies with haplotypes formed by a disease locus and a SNP marker locus in the presence of linkage disequilibrium (LD). We consider the effects of a recently <b>published</b> <b>error</b> model on 2 × 3 chi-square analysis. We study the joint relation of LD and errors with sample size for three specific genetic disease models and two settings each of marker allele frequencies (total of 6 studies). Minimal sample size necessary for fixed asymptotic power is estimated as a 4 th degree polynomial in the variables S (error) and D ’ (LD measure) via a backward step-wise regression. We find that increased error rates lower power. In all studies, we observe that LD and errors interact in a non-linear fashion. In particular, regression analyses shows that several higher order interaction terms have coefficients significantly different from 0 in each study, with fraction of variance explained greater than 0. 9999. Finally, the increase in sample size necessary to maintain constant asymptotic power and level of significance as a function of S is smallest when D ’ = 1 (perfect LD). The increase grows monotonically as D ’ decreases to 0. 5 for all studies. ...|$|E
40|$|BACKGROUND: Historically, only partial {{assessments}} of data quality {{have been performed}} in clinical trials, for which the most common method of measuring database error rates has been to compare the case report form (CRF) to database entries and count discrepancies. Importantly, errors arising from medical record abstraction and transcription are rarely evaluated as part of such quality assessments. Electronic Data Capture (EDC) technology has had a further impact, as paper CRFs typically leveraged for quality measurement are not used in EDC processes. METHODS AND PRINCIPAL FINDINGS: The National Institute on Drug Abuse Treatment Clinical Trials Network has developed, implemented, and evaluated methodology for holistically assessing data quality on EDC trials. We characterize the average source-to-database error rate (14. 3 errors per 10, 000 fields) {{for the first year}} of use of the new evaluation method. This error rate was significantly lower than the average of <b>published</b> <b>error</b> rates for source-to-database audits, and was similar to CRF-to-database error rates reported in the published literature. We attribute this largely to an absence of medical record abstraction on the trials we examined, and to an outpatient setting characterized by less acute patient conditions. CONCLUSIONS: Historically, medical record abstraction is the most significant source of error by an order of magnitude, and should be measured and managed during the course of clinical trials. Source-to-database error rates are highly dependent on the amount of structured data collection in the clinical setting and on the complexity of the medical record, dependencies that should be considered when developing data quality benchmarks...|$|E
50|$|In January 2009 Anime News Network {{reported}} that the series would also begin serialization in Gentosha's seinen manga magazine Comic Birz starting in the April issue, but this was later announced to be a <b>publishing</b> <b>error.</b> The Gentosha press release on which ANN had based their article actually referred to a new series by Himaruya, Chibisan Date. Amazon.com has listed Hetalia: Axis Powers as being licensed by Tokyopop for a North American release, with the first volume released on September 21, 2010. After Tokyopop withdrew from the English language market in 2011, Right Stuf has been publishing the Hetalia manga in English since May 2012. On July 7, 2013, Tokyopop announced via their Facebook page that Tokyopop and Right Stuf are teaming up to being out Hetalia Volumes 4 and 5 in Winter 2013.|$|R
30|$|The Editor-in-Chief has {{retracted}} {{this article}} (Umer, 2018), {{because it was}} <b>published</b> in <b>error</b> before the peer review process was completed. Further post publication peer review determined that the article is not suitable for publication in the International Journal of Anthropology and Ethnology. The author does not agree to this retraction.|$|R
50|$|If a {{build on}} Gump is successful, then a report {{is placed on}} the site, and all {{projects}} that declare themselves dependencies are eligible to be built. If a project fails to build, <b>error</b> reports are <b>published,</b> an <b>error</b> email is sent, and all dependent projects are blocked from building.|$|R
