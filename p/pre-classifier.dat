13|3|Public
40|$|I {{argue in}} this paper that the <b>pre-classifier</b> {{adjectival}} modification of big/small is a distinctive linguistic phenomenon. Classifiers are ambiguous between a counting reading and a measure reading. The <b>pre-classifier</b> big/small modifies the quantity denoted by classifiers when they are characterized with the measure reading. From another perspective, the <b>pre-classifier</b> big/small deviates from adnominal and predicative big/small and they measure quantity only. This proposal correctly predicts that only a restricted set of adjectives like big/small can function as <b>pre-classifier</b> modifiers. Besides, I also claim that big/small quantifies over single atoms but not the plurality of atoms...|$|E
40|$|Ternary Content-Addressable Memories (TCAMs) {{has become}} the {{industrial}} standard for high-throughput packet classification. However, one major drawback of TCAMs is their high power consumption, which is becoming critical with the boom of data centers, the growing classifiers and the deployment of IPv 6. In this paper, we propose a practical and efficient solution which introduces a smart <b>pre-classifier</b> to reduce power consumption of TCAMs for multidimensional packet classification. We reduce the dimension of the problem through the <b>pre-classifier</b> which pre-classifies a packet on two header fields, source and destination IP addresses. We {{then return to the}} high dimension problem where {{only a small portion of}} a TCAM is activated and searched for a given packet. The smart preclassifier is built in a way such that a given packet matches at most one entry in the <b>pre-classifier,</b> which make commodity TCAMs sufficient to implement the <b>pre-classifier.</b> Furthermore, each rule is stored only once in one of the TCAM blocks, which avoids rule replication. The presented solution uses commodity TCAMs, and the proposed algorithms are easy to implement. Our scheme achieves a median power reduction of 91 % and an average power reduction of 88 % on real and synthetic classifiers respectively. Categories andSubject Descriptors C. 2. 6 [Computer-CommunicationNetworks]: Internetworking...|$|E
40|$|Introduction: A 16 m 3 whole-body {{exposure}} chamber for human exposure to aerosols is described. Several modifications of the aerosol generation and distribution system {{were needed to}} ensure a stable aerosol concentration in the chamber, especially when a cyclone <b>pre-classifier</b> was used. Results: After these modifications, stable aerosol concentrations of aluminium oxide with avolum...|$|E
40|$|Abstract. The cast {{shadows on}} the {{background}} of the object will distinctly affect the recognition of the foreground objects. Due to the limitation of shadow removal methods utilizing texture, a novel algorithm based on Gaussian Mixture Model (GMM) and HSV color space is proposed. Firstly, moving regions are detected using GMM. Secondly, we make two <b>pre-classifiers</b> accurate and adaptive to the change of shadow by using the features of shadow in RGB and HSV color space. Experimental results show that the proposed method is efficient and robust...|$|R
40|$|Machine {{learning}} {{techniques can}} be used to extract knowledge from data stored in medical databases. In our application, various machine learning algorithms were used to extract diagnostic knowledge to support the diagnosis of sport injuries. The applied methods include variants of the Assistant algorithm for top-down induction of decision trees, and variants of the Bayesian classifier. The available dataset was insufficent for reliable diagnosis of all sport injuries considered by the system. Consequently, expert-defined diagnostic rules were added and used as <b>pre-classifiers</b> or as generators of additional training instances for injuries with few training examples. Experimental results show that the classification accuracy and the explanation capability of the naive Bayesian classifier with the fuzzy discretization of numerical attributes was superior to other methods and was estimated as the most appropriate for practical use. 1 Introduction Machine learning technology is well suited [...] ...|$|R
40|$|International audienceIn this paper, a new {{technique}} for power quality disturbances classification is proposed. It focuses on voltage sags and swells that are first pre-classified into four classes that depend {{on the number of}} non-zero symmetrical components and can contain different types of sag and swell. Using the estimated symmetrical component values, we can afterward classify the corresponding sag or swell signature. In this study, we show that the pre-classification can be reformulated as a pure model order selection problem. To solve this problem, we propose two <b>pre-classifiers</b> based on Information Theoretical Criteria. The former yields the highest statistical performances, while the latter has a lower computation complexity. The performances of the proposed classification algorithms are evaluated using Monte Carlo simulations on synthetic signals and using real power system data obtained from the DOE/EPRI National Database of Power System Events. The achieved simulations and experimental results clearly illustrate the effectiveness of the proposed algorithms for voltage sag and swell classification...|$|R
40|$|As {{research}} {{instruments of}} large information capacities become a reality, automated systems for {{intelligent data analysis}} become a necessity. Scientific archives containing huge volumes of data preclude manual manipulation or intervention and require automated exploration and mining that can at least pre-classify information in categories. The large dataset from the radio plasma imager (RPI) instrument onboard the IMAGE satellite shows a critical need for such exploration {{in order to identify}} and archive features of interest in the volumes of visual information. In this research we have developed such a <b>pre-classifier</b> through a model of pre-attentive vision capable of detecting and extracting traces of echoes from the RPI plasmagrams. The overall design of our model complies with Marr’s paradigm of vision where elements of increasing perceptual strength are built bottom up under the Gestalt constraints of good continuation and smoothness. The specifics of the RPI data, however, demanded extension of this paradigm to achieve greater robustness for signature analysis. Our pre-attentive model now employs a feedback neural network that refines alignmen...|$|E
40|$|The {{automated}} tracing of the carotid layers on ultrasound {{images is}} complicated by noise, different morphology and pathology of the carotid artery. In this study we benchmarked four methods for feature selection {{on a set of}} variables extracted from ultrasound carotid images. The main goal was to select those parameters containing the highest amount of information useful to classify the pixels in the carotid regions they belong to. Six different classes of pixels were identified: lumen, lumenintima interface, intima-media complex, media-adventitia interface, adventitia and adventitia far boundary. The performances of QuickReduct Algorithm (QRA), Entropy-Based Algorithm (EBR), Improved QuickReduct Algorithm (IQRA) and Genetic Algorithm (GA) were compared using Artificial Neural Networks (ANNs). All methods returned subsets with a high dependency degree, even if the average classification accuracy was about 50 %. Among all classes, the best results were obtained for the lumen. Overall, the four methods for feature selection assessed in this study return comparable results. Despite the need for accuracy improvement, this study could be useful to build a <b>pre-classifier</b> stage for the optimization of segmentation performance in ultrasound automated carotid segmentatio...|$|E
40|$|In this paper, a pre-classification stage {{based on}} global {{features}} is incorporated to an online signature verification {{system for the}} purposes of improving its performance. The <b>pre-classifier</b> makes use of the discriminative power of some global features to discard (by declaring them as forgeries) those signatures for which the associated global feature is far away from its respective mean. For the remaining signatures, features based on a wavelet approximation of the time functions associated with the signing process, are extracted, and a Random Forest based classification is performed. The experimental results show that the proposed pre-classification approach, when based on the apppropriate global feature, is capable of getting error rate improvements with respect to the case where no pre-classification is performed. The approach also has the advantages of simplifying and speeding up the verification process. Fil: Parodi, Marianela. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico - CONICET - Rosario. Centro Internacional Franco Argentino de Ciencias de la Información y Sistemas; Argentina;Fil: Gomez, Juan Carlos. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico - CONICET - Rosario. Centro Internacional Franco Argentino de Ciencias de la Información y Sistemas; Argentina...|$|E
40|$|The aim of {{this paper}} is to {{construct}} a practical forensic steganalysis tool for JPEG images that can properly analyze both single- and double-compressed stego images and classify them to selected current steganographic methods. Although some of the individual modules of the steganalyzer were previously published by the authors, they were never tested as a complete system. The fusion of the modules brings its own challenges and problems whose analysis and solution is one of the goals of this paper. By determining the stego algorithm, this tool provides the first step needed for extracting the secret message. Given a JPEG image, the detector assigns it to 6 popular steganographic algorithms. The detection is based on feature extraction and supervised training of two banks of multi-classifiers realized using support vector machines. For accurate classification of single-compressed images, a separate multi-classifier is trained for each JPEG quality factor from a certain range. Another bank of multiclassifiers is trained for double-compressed images for the same range of primary quality factors. The image under investigation is first analyzed using a <b>pre-classifier</b> that detects selected cases of double-compression and estimates the primary quantization table. It then sends the image to the appropriate single- or double-compression multiclassifier. The error is estimated from more than 2. 6 million images. The steganalyzer is also tested on two previously unseen methods to examine its ability to generalize...|$|E
40|$|Measuring the {{similarity}} between 3 D shapes is a fundamental problem, with applications in computer graphics, computer vision, molecular biology, {{and a variety of}} other fields. A challenging aspect of this problem is to find a suitable shape signature that can be constructed and compared quickly, while still discriminating between similar and dissimilar shapes. In this paper, we propose and analyze a method for computing shape signatures for arbitrary (possibly degenerate) 3 D polygonal models. The key idea is to represent the signature of an object as a shape distribution sampled from a shape function measuring global geometric properties of an object. The primary motivation for this approach is to reduce the shape matching problem to the comparison of probability distributions, which is simpler than traditional shape matching methods that require pose registration, feature correspondence, or model fitting. We find that the dissimilarities between sampled distributions of simple shape functions (e. g., the distance between two random points on a surface) provide a robust method for discriminating between classes of objects (e. g., cars versus airplanes) in a moderately sized database, despite the presence of arbitrary translations, rotations, scales, mirrors, tessellations, simplifications, and model degeneracies. They can be evaluated quickly, and thus the proposed method could be applied as a <b>pre-classifier</b> in a complete shape-based retrieval or analysis system concerned with finding similar whole objects. The paper describes our early experiences using shape distributions for object classification and for interactive web-based retrieval of 3 D models. 1...|$|E
40|$|Abstract This paper {{presents}} a comparative analysis of novel supervised fuzzy {{adaptive resonance theory}} (SF-ART), multilayer perceptron (MLP) and Multi Layer Perceptrons (MLP) neural networks over Ballistocardiogram (BCG) signal recognition. To extract essential features of the BCG signal, we applied Biorthogonal wavelets. SF-ART performs classification on two levels. At first level, <b>pre-classifier</b> which is self-organized fuzzy ART tuned for fast learning classifies the input data roughly to arbitrary (M) classes. At the second level, post-classification level, a special array called Affine Look-up Table (ALT) with M elements stores the labels of corresponding input samples in the address equal to the index of fuzzy ART winner. However, in running (testing) mode, the content of an ALT cell with address equal to the index of fuzzy ART winner output will be read. The read value declares the final class that input data belongs to. In this paper, we used two well-known patterns (IRIS and Vowel data) and a medical application (Ballistocardiogram data) to evaluate and check SF-ART stability, reliability, learning speed and computational load. Initial tests with BCG from six subjects (both healthy and unhealthy people) indicate that the SF-ART is capable to perform with a high classification performance, high learning speed (elapsed time for learning around half second), and very low computational load compared to the well-known neural networks such as MLP which needs minutes to learn the training material. Moreover, to extract essential features of the BCG signal, we applie...|$|E
40|$|This paper {{considers}} the online localization of sick animals in pig houses. It presents an automated online recognition and localization procedure for sick pig cough sounds. The instantaneous {{energy of the}} signal is initially used to detect and extract individual sounds from a continuous recording and their duration {{is used as a}} <b>pre–classifier.</b> Auto–regression (AR) analysis is then employed to calculate an estimate of the sound signal and the parameters of the estimated signal are subsequently evaluated to identify the sick cough sounds. It is shown that the distribution of just 3 AR parameters provides an ade-quate classifier for sick pig coughs. A localization technique based on the time difference of arrival is evaluated on field data and is shown that it is of acceptable accuracy for this particular application. The algorithm is applied on continuous recordings from a pig house to evaluate its effectiveness. The correct identification ratio ranged from 73 % (27 % false positive identifications) to 93 % (7 % false positive identifications) depending on the position of the microphone that was used for the recording. Although the false negative identifications are about 50 % it is shown that this accuracy can be enough {{for the purpose of this}} tool. Finally, it is suggested that the presented application can be used to online monitor the welfare in a pig house, and provide early diagnosis of a cough hazard and faster treatment of sick animals...|$|E
40|$|A Canvass steganalyzer for double-compressed JPEG images Steganography is the {{practice}} of hiding a secret message in innocent objects such that {{the very existence of}} the message is undetectable. Steganalysis, on the other hand, deals with finding the presence of such hidden messages. `Canvass 2 ̆ 7 is software developed to perform JPEG image steganalysis. This software uses pattern recognizer to classify unknown images into cover (innocent) or stego (containing hidden message). The pattern recognizer, a support vector machine, is trained using the underlying statistical information in the cover and stego images. Some of the popular steganographic algorithms produce double-compressed JPEG images. A blind steganalyzer built on the assumption that it will see only single-compressed images gives misleading results of classification for such images. The goal of the current work is to develop a double-compression detector for JPEG images that extends the existing Canvass software. We develop a double-compression detector based on Partially Ordered Markov Models (POMMs) that can act as a <b>pre-classifier</b> to the blind steganalyzer. We also use the patterns of relative histogram values of the quantized DCT coefficients for improved accuracy of detection. After detecting the double- compression, we carry out cover Vs. stego detection and primary quality factor estimation. We compare our double-compression detector with two other state-of-the-art detectors. Our detector is found to have better performance compared to the state-of-the-art detectors. The current work considers a limited set of quality factors for double-compression but this novel method for steganalysis of double-compressed data looks promising and could be generalized for any combination of primary and secondary quality factors...|$|E
40|$|In this paper, {{we address}} the issue of feature {{encoding}} for distributed image classification systems. Such systems often extract a set of features such as color, texture and shape from the raw multimedia data automatically and store them as content descriptors. This content-based metadata supports a wider variety of queries than text-based metadata and thus provides a promising approach for efficient database access and management. When the size of the database becomes large and the number of clients connected to the server increases, the feature data requires a significant amount of storage space and transmission bandwidth. Thus it is useful to devise techniques to compress the features. In this paper, we propose an optimal design of a classified quantizer in a rate-distortioncomplexity optimization framework. A Decision Tree Classifier (DTC) is applied to classify the compressed data. We employ the Generalized Breiman, Freidman, Olshen, and Stone (G-BFOS) algorithm to design the optimal <b>pre-classifier,</b> which is a pruned subtree of the decision tree, and to perform the optimal bit allocation among classes. The optimization is carried out based not only on a rate budget, but also on a coding complexity constraint. We illustrate this framework by showing a texture classification example. Our results show that by using a classified quantizer to encode the features, we are able to improve the percentage of correct classification by, as compared to using a quantizer without preclassification at the same rate. This improvement in classification also leads to a reduction of the number of images transmitted between server and client. I...|$|E
40|$|The {{segmentation}} of the {{carotid artery}} wall {{is an important}} aid to sonographers when measuring intima-media thickness (IMT). Automated and completely user-independent segmentation techniques are gaining increasing importance, because they avoid the bias coming from human interactions. However, automated techniques still underperform semi-automated IMT measurement methods. Automated techniques cannot reproduce human expertise in selecting the optimal point where IMT should be measured. Hence, superior intelligence must be embedded into automated techniques in order to overcome the performance limitations. A possible solution is to extract more information from the image, which could be obtained by an accurate analysis of the image at pixel level. In this study, we applied a feature selection and reduction approach to ultrasound carotid images, and measured 141 features for each image pixel and supposed that a pixel could belong {{to one of three}} classes: artery lumen, intima or media layer, or the adventitia layer. Among several approaches that are available for dimensional reduction, we chose to test three based on the Rough-Set Theory (RST) : the QuickReduct Algorithm (QRA), the Entropy-Based Algorithm (EBR) and the Improved QuickReduct Algorithm (IQRA). QRA achieved the best performance and correctly classified 97. 5 % of the pixels on a reduced testing image dataset and about 91. 5 % for a large validation dataset. On average, QRA reduced the complexity of the system from 141 to 8 or 9 features. This result could represent a pilot study for developing an intelligent <b>pre-classifier</b> to improve the image segmentation performance of automated techniques in carotid ultrasound imaging...|$|E

