18|9389|Public
5000|$|In 1987 Dr. Yilmaz {{published}} Seismic Data Processing {{through the}} Society of Exploration Geophysicists. In the preface {{he points out}} the three main steps in exploring {{for oil and gas}} using reflection seismology; data acquisition, data processing, and interpretation. The intended purpose of the book was to help seismic processors understand the fundamentals of <b>processing</b> <b>seismic</b> <b>data.</b> He pointed out the main challenges in seismic processing: ...|$|E
40|$|The curvelet {{transform}} is a multiscale and multidirectional transform, {{which allows}} an almost optimal non-adaptive sparse representation for curve-like features and edges. Applications of curvelets have quicken increasing {{interest in the}} community of applied mathematics, signal processing and seismic geology over the past years. In this paper, we describe some recent applications involving image <b>processing,</b> <b>seismic</b> <b>data</b> exploration, turbulent flows, and compressed sensing. ...|$|E
40|$|Some {{recent studies}} on a {{physical}} understanding of volcanic phenomena have necessitated studies of volcanic subsurface structures, especially to understand processes going on beneath volcanoes. During the last decades, researchers {{have tried to}} model subsurface structures using various seismic exploration techniques. In these studies, numerous formats for recording or <b>processing</b> <b>seismic</b> <b>data</b> have been independently used and no formats to exchange data have been proposed as they were not required...|$|E
5000|$|... #Subtitle level 2: <b>Seismic</b> <b>Data</b> <b>Processing</b> and <b>Seismic</b> <b>Data</b> Analysis ...|$|R
5000|$|... {{worked on}} the {{development}} of Spatial Quality Index and on smart filtering tools for the <b>processing</b> of <b>seismic</b> <b>data</b> ...|$|R
50|$|Öz Yilmaz is the Chief Technology Officer of GeoTomo LLC and {{the founder}} of Anatolian Geophysical. He is the author of <b>Seismic</b> <b>Data</b> <b>Processing</b> and <b>Seismic</b> <b>Data</b> Analysis, the {{principle}} reference volumes for the <b>seismic</b> <b>processing</b> industry.|$|R
30|$|This is {{a special}} issue on FPGA {{supercomputing}} platforms, architectures, and techniques for accelerating computationally complex algorithms. This issue covers {{a broad range of}} applications in which field programmable gate arrays (FPGAs) are successfully used to accelerate processing. It also provides researcher's insights on the challenges in successfully using FPGAs. The applications discussed include motor control, radar processing, face recognition, <b>processing</b> <b>seismic</b> <b>data,</b> and accelerating random number generation. Techniques discussed by the authors include partitioning between a CPU and FPGA hardware, reducing bitwidth to improve performance, interfacing to analog signals, and using high level tools to develop applications.|$|E
40|$|In many {{engineering}} applications {{ranging from}} engineering seismology to petroleum engineering and civil engineering, {{it is important}} to process seismic data. In <b>processing</b> <b>seismic</b> <b>data,</b> {{it turns out to be}} very efficient to describe the signal 2 ̆ 7 s spectrum as a linear combination of Ricker wavelet spectra. In this paper, we provide a possible theoretical explanation for this empirical efficiency. Specifically, signal propagation through several layers is discussed, and it is shown that the Ricker wavelet is the simplest non-trivial solution for the corresponding data processing problem, under the condition that the described properties of the approximation family are satisfied...|$|E
40|$|Seismic {{data are}} usually {{acquired}} and processed for imaging reflections. This paper describes {{a method of}} <b>processing</b> <b>seismic</b> <b>data</b> for imaging discontinuities (e. g., faults and stratigraphic features). One application of this nontraditional process is a 3 -D volume, or cube, of coherence coefficients within which faults are revealed as numerically separated surfaces. Figure 1 compares a traditional 3 -D reflection amplitude time slice {{with the results of}} the new method. To our knowledge, this is the first published method of revealing fault surfaces within a 3 -D volume for which no fault reflections have been recorded...|$|E
5000|$|Prior to the {{availability}} of digital <b>processing</b> of <b>seismic</b> <b>data</b> in the late 1970s, the records were done in a few different forms on different types of media.|$|R
5000|$|Seismic Unix, {{open source}} {{software}} for <b>processing</b> of <b>seismic</b> reflection <b>data</b> ...|$|R
5000|$|Petroleum Geo-Services has an XC40 {{supercomputer}} {{used for}} the <b>processing</b> of complex <b>seismic</b> <b>data</b> sets.|$|R
40|$|In various embodiments, {{the present}} {{disclosure}} describes methods for <b>processing</b> <b>seismic</b> <b>data</b> to concurrently produce a velocity model and a depth image. Various embodiments {{of the methods}} include: a) acquiring seismic data; b) generating a shallow velocity model from the seismic data; c) generating a stacking velocity model using the shallow velocity model as a guide; d) generating an initial interval velocity model from the stacking velocity model; and e) generating an initial depth image using the initial interval velocity model. The methods also include iterative improvement of the initial depth image and the initial interval velocity model to produce improved depth images and improved interval velocity models. Improvement of the depth images and the interval velocity models is evaluated by using a congruency test...|$|E
40|$|Abstract—In many {{engineering}} applications {{ranging from}} en-gineering seismology to petroleum engineering and civil engineer-ing, {{it is important}} to process seismic data. In <b>processing</b> <b>seismic</b> <b>data,</b> {{it turns out to be}} very efficient to describe the signal’s spectrum as a linear combination of Ricker wavelet spectra. In this paper, we provide a possible theoretical explanation for this empirical efficiency. Specifically, signal propagation through several layers is discussed, and it is shown that the Ricker wavelet is the simplest non-trivial solution for the corresponding data processing problem, under the condition that the described properties of the approximation family are satisfied. I. FORMULATION OF THE PROBLEM: SEISMIC WAVES AND THE EMPIRICAL SUCCESS OF RICKER WAVELETS Seismic data is very useful. Already ancient scientists noticed that earthquakes generate waves which can be detected a...|$|E
40|$|Sea {{surface profile}} and {{reflection}} coefficient estimates are vital input parameters to various seismic data processing applications. The common assumption {{of a flat}} sea surface when <b>processing</b> <b>seismic</b> <b>data</b> can lead to misinterpretations and mislocations of events. A new method of imaging the sea surface from decomposed wavefields is presented. Wavefield separation {{is applied to the}} data acquired by a towed dual sensor streamer containing collocated hydrophones and geophones to obtain the up- and down-going wavefields of the related sensors. The up- and down-going wavefields of a given sensor are extrapolated to the sea surface where an imaging condition is applied in order to obtain the sea surface profile. The image points values obtained at the sea surface are the reflection coefficient values of these points. Ray tracing and finite difference methods are used to generate different controlled data sets employed in this feasibility study to demonstrate the imaging principle and to test the image accuracy. Finally, a first field data example of a marginal weather line from the Norwegian North-Sea is presented...|$|E
50|$|In June 2012 Yandex {{acquired}} {{share in}} Seismotech, Ltd. that provides {{services in the}} area of interpretative <b>processing</b> of <b>seismic</b> <b>data</b> and software development (Prime package), and provided its equipment and distributed computing technologies for processing of geological-geophysical data.|$|R
5000|$|... 2004 - GX Technology Corporation, <b>seismic</b> imaging <b>processing</b> and multi-client <b>seismic</b> <b>data</b> libraries, {{including}} {{a portfolio of}} BasinSPAN data libraries ...|$|R
5000|$|Bridges {{began his}} career with Chevron Corporation, then founded Advance Geophysical in 1980. He {{achieved}} success with the software products MicroMAX and ProMAX, both used for the <b>processing</b> of <b>seismic</b> <b>data</b> for the petroleum exploration industry. In recognition of his business accomplishments, {{he was awarded the}} Enterprise Award in 1991 by the Society of Exploration Geophysicists [...] He is also the chairman of Quest Capital, a private venture capital fund ...|$|R
40|$|Signals {{obtained}} in land seismic surveys are usually contaminated with coherent noise, among which the ground roll (Rayleigh surface waves) is of major concern {{for it can}} severely degrade {{the quality of the}} information obtained from the seismic record. Properly suppressing the ground roll from seismic data is not only of great practical importance but also remains a scientific challenge. Here we propose an optimized filter based on the Karhunen [...] Loéve transform for <b>processing</b> <b>seismic</b> <b>data</b> contaminated with ground roll. In our method, the contaminated region of the seismic record, to be processed by the filter, is selected in such way so as to correspond to the maximum of a properly defined coherence index. The main advantages of the method are that the ground roll is suppressed with negligible distortion of the remanent reflection signals and that the filtering can be performed on the computer in a largely unsupervised manner. The method has been devised to filter seismic data, however it could also be relevant for other applications where localized coherent structures, embedded in a complex spatiotemporal dynamics, need to be identified in a more refined way. Comment: 9 pages, 12 figure...|$|E
40|$|Migration is {{the final}} stage of <b>processing</b> <b>seismic</b> <b>data</b> to move the {{position}} of the reflector apparent to a precise location based on the trajectory of the waves. One method is the finite difference migration, where the algorithm is looking for a differential operator, either explicit or implicit, to determine the trajectory discrete seismic wave. Finite difference Migration used the wave equation that can be known upcoming wave models to high approximation used the Muir square root expansion. From this expansion, it is known upcoming wave approximation based on the angle of the field review the downward continuation of the wave as approximation models 150, 450, 600, to other types of approximation. Based on this approximation, applied to various types of approximations used in the algorithm finite difference migration, time domain and depth domain, at ProMAX like Steep Dip Explicit FD Time Migration, Fast Explicit FD Time Migration, Explicit FD Depth Migration, Implicit FD Time Migration, and Implicit FD Depth Migration. Explicit finite difference migration used the Leapfrog method and implicit finite difference used the Crank-Nicolson method. Finite difference migration method is then applied to the real seismic 2 D marine data with the location of acquisition in the continental shelf which has a complex geological structure. Migration results obtained show that the Fast Explicit FD Time Migration algorithm showed better results than other finite difference migration algorithm...|$|E
40|$|Over {{the last}} years, seismic images have {{increasingly}} played {{a vital role}} {{to the study of}} earthquakes. The large volume of seismic data that has been accumulated has created the need to develop sophisticated systems to manage this kind of data. Seismic interpretation can play a much more active role in the evaluation of large volumes of data by providing at an early stage vital information relating to the framework of potential producing levels. [1] This work presents a novel method to manage and analyse seismic data. The data is initially turned into clustering maps using clustering techniques [2] [3] [4] [5] [6], in order to be analysed on the platform. These clustering maps can then be analysed with the friendly-user interface of Seismic 1 which is based on. Net framework architecture [7]. This feature permits the porting of the application in any Windows – based computer as also to many other Linux based environments, using the Mono project functionality [8], so it can run an application using the No-Touch Deployment [7]. The platform supports two ways of <b>processing</b> <b>seismic</b> <b>data.</b> Firstly, a fast multifunctional version of the classical region-growing segmentation algorithm [9], [10] is applied to various areas of interest permitting their precise definition and labelling. Moreover, this algorithm is assigned to automatically allocate new earthquakes to a particular cluster based upon the magnitude of the centre of gravity of the existing clusters; or create a new cluster if all centers of gravity are above a predefined by the user upper threshold point...|$|E
40|$|Large {{velocity}} contrasts {{are regularly}} encountered in geothermal fields due to poorly consolidated and hydro-thermally altered rocks. The appropriate <b>processing</b> of <b>seismic</b> <b>data</b> is therefore crucial to delineate the geological structure. To assess {{the benefits of}} surface seismic surveys in such settings, we applied different migration procedures to image a synthetic reservoir model and <b>seismic</b> <b>data</b> from the Coso Geothermal Field. We {{have shown that the}} two-dimensional migration of synthetic <b>seismic</b> <b>data</b> from a typical reservoir model resolves the geological structure very well despite the extremely strong and sharp velocity contrasts. In addition, small fracture zones can b...|$|R
40|$|Summary. A {{non-linear}} N-th Root {{method of}} <b>processing</b> <b>seismic</b> array <b>data</b> is investigated for both event detection and event processing. For event detec-tion {{it has been}} found that this process is much better at handling non-Gaussian noise and is only very marginally worse than linear processing when the noise is Gaussian. For event processing its chief advantage is the suppres-sion of sections of the record where the signal-to-noise ratio is low, enabling more precise azimuth and slowness measurements to be made...|$|R
40|$|Applications of {{aerospace}} technology to petroleum exploration are described. Attention {{is given to}} seismic reflection techniques, sea-floor mapping, remote geochemical sensing, improved drilling methods and down-hole acoustic concepts, such as down-hole seismic tomography. The seismic reflection techniques include monitoring of swept-frequency explosive or solid-propellant seismic sources, as well as aerial seismic surveys. Telemetry and <b>processing</b> of <b>seismic</b> <b>data</b> may also be performed through use of {{aerospace technology}}. Sea-floor sonor imaging and a computer-aided system of geologic analogies for petroleum exploration are also considered...|$|R
40|$|Master of ScienceDepartment of GeologyMatthew W. TottenNess County {{has contributed}} 30 billion barrels to Kansas oil {{production}} since 1995, {{and has been}} an actively developing county in oil activity. The focus of this research project is to identify the reservoir qualities that make Mississippian-aged production favorable. Modern day logging techniques and seismic data allow specialists to seek out subtle heterogeneities to an oil producing formation {{once thought to be}} homogenous. Having success with horizontal drilling in other locations worldwide, large oil companies have acquired tens of thousands of acres with the intentions of drilling into the Mississippian, although some have recently backed out of the area. While some horizontal wells are producing today, complications with the compartmentalized, relatively thin Mississippian producing zones and short production longevities make horizontal drilling a high risk technique. Better understanding favorable reservoir qualities are essential for future production and development of oil fields in Ness County. This case study utilizes different variations of post and pre-stack 3 D and 2 D seismic data shot on about 3, 200 acres spanning over 8 sections located in northwestern Ness County. The physical and chemical properties associated with the Mississippian formation in this area can be better analyzed with different methods for <b>processing</b> <b>seismic</b> <b>data.</b> Raw seismic signatures show little variation within the Mississippi Lime/Dolomite. Utilizing Seismic attributes derived from raw data may bring certain featured hydrocarbon bearing zones into view. Attributes such as curvature and coherency aid in interpreting physical features within the study area while spectral decomposition, amplitude, instantaneous frequency, and instantaneous Q hold detailed signatures dependent upon rock properties...|$|E
40|$|Installation {{of local}} high {{resolution}} microseismic {{networks in the}} vicinity of hazardous mining fronts, and completing a pre-existing mine scale seismic network, is supposed to help detecting much smaller events and <b>processing</b> <b>seismic</b> <b>data</b> with much better accuracy, then gaining insight in the quantification of the rock mass response versus time, space and mine production, all critical parameters for rating the seismic hazard on a continuous basis. However, the detection and processing of small magnitude events require some precautions due to near-field conditions adverse to the detection of significant changes in the seismic pattern. High resolution monitoring raises new issues related to the magnified complexity of the rock mass when dealing with higher frequency seismic waves travelling through the host rock, intersected by faults and geological disturbances, adjacent backfilled works intertwined with multilevel fast advancing mining works. This magnified complexity may introduce artefacts in the accurate location of the small magnitude sources, blurring the interpretation of the seismicity. To face such situations of complex mining underground conditions, the authors intend to develop a new approach based on the implementation of 3 D dynamic velocity model enabling to take into account not only the geological features surrounding a mining block, but the dynamic mining process itself, i. e. the creation of mining voids and surrounding disturbed zones. Besides synthetic numerical tests that have been run to assess the relevance of this issue, the research work is tested through the back analysis of an intense microseismic swarm recorded during the brutal caving process of a solution mined cavern. This paper presents this numerical procedure currently being developed for operational implementation in the near future in deep mining works within the strategy of deployments of mobile local acoustic/micro seismic arrays...|$|E
40|$|The North Caspian Basin (NCB) {{contains}} {{a significant number}} of major oil fields, some of which are yet to be put into production. The reason why some of these fields are not yet put into production is the exploration challenge that the NCB poses. In particular, the complex geological structure of this region makes it quite difficult to image its oil fields with conventional seismic techniques. This thesis sheds more light on difficulties associated with acquiring and <b>processing</b> <b>seismic</b> <b>data</b> in the NCB. The two central tools for investigation of these imaging challenges were the construction of a geological model of the NCB and the use of an accurate elastic wave-propagation technique to analyze the capability of seismic to illuminate the geological structures of the NCB. Using all available regional and local studies and my knowledge gained with oil companies, where I worked on subsalt and suprasalt 2 D and 3 D seismic data from the North Caspian Basin, I constructed a 2 D elastic isotropic 10 -by- 6 km geological model of a typical oil field located on the shelf of the Caspian Sea in the southeastern part of the North Caspian Basin, which has the largest oil fields. We have propagated seismic waves through this model. The technique we used to compute wave propagation is known as the Finite-Difference Modeling (FDM) technique. Generating 314 shot gathers with stationary multicomponent OBS receivers that were spread over 10 km took two weeks of CPU time using two parallel computers (8 CPU V 880 Sun Microsystems and 24 CPU Sun Enterprise). We have made the data available to the public. The dataset can be uploaded at [URL] in the SEGY format. The key conclusions of the analysis of these data are as follows: - Combined usage of P- and S-waves allows us to illuminate subsalt reef, clastics and complex salt structures despite the 4 -km overburden. - Free-surface multiples and guided waves are one of the key processing challenges in NCB, despite relatively shallow (less than 15 m) shelf water...|$|E
40|$|The <b>processing</b> of <b>seismic</b> <b>data,</b> for {{the imaging}} of the earth's subsurface, is pushing current {{computational}} possibilities to the limit. In this paper {{results are presented}} obtained by optimisation and parallelisation of two innovative seismic algorithms {{with the use of}} PVM and FORGE. It shows that where the algorithm is CPU bound, a very significant speedup can be acquired, whereas IO-bound algorithms cannot take advantage of the parallel implementation. This leads to two distinct approaches for optimisation. © Springer-Verlag Berlin Heidelberg 1996...|$|R
40|$|In this paper, we {{consider}} 3 D wave-packet transform that {{is useful in}} 3 D data processing. This transform is computationally intensive {{even though it has}} a computational complexity of O(N 3 log N). Here we present its implementation on GPUs using NVIDIA CUDA technology. The code was tested on different types of graphical processors achieving the average speedup up to 46 times on Tesla M 2050 compared to CPU sequential code. Also, we analyzed its scalability for several GPUs. The code was tested for <b>processing</b> synthetic <b>seismic</b> <b>data</b> set: data compression, de-noising, and interpolation...|$|R
50|$|ION Geophysical {{provides}} acquisition equipment, software, {{planning and}} <b>seismic</b> <b>processing</b> services, and <b>seismic</b> <b>data</b> libraries {{to the global}} oil & gas industry. The company’s technologies and services are used by E&P operators and seismic acquisition contractors to generate high-resolution images of the subsurface during exploration, exploitation and production operations. Headquartered in Houston, Texas, ION has offices in the United States, Canada, Latin America, Europe, Africa, Russia, China and the Middle East.|$|R
40|$|Seismic {{surveys are}} well {{established}} in hydrocarbon prospecting, and technology for <b>processing</b> <b>seismic</b> <b>data</b> {{has been developed}} through decades. The first electromagnetic survey for hydrocarbon prospecting however, was performed in 2000, {{and as a consequence}} of the short time span the technology is not as well developed as in seismics. For instance, the need for efficient and robust forward modelling software and inversion schemes for collected data is urgent. In this thesis forward modelling using integral equations and the Contrast Source Inversion (CSI) method is investigated for forward and inverse 3 D electromagnetic scattering experiments in hydrocarbon prospecting, respectively. The mathematical model is developed in an arbitrary isotropic, conductive medium, with contrast in electric permittivity and electric conductivity between the scattering object and the background, while in the numerical examples the background model is restricted to a horizontally layered medium with variations in the $z$-direction only and contrast in electric conductivity. The difference in electric conductivity is considered the backbone of electromagnetic hydrocarbon prospecting. The main result concerning forward modelling in this thesis is the establishment of a, to my knowledge, previously unpublished method for solving the electric Lippmann-Schwinger equation in a conductive medium by fixed point iteration. In the inversion part of this thesis the previously scalar CSI method is extended to a full vector valued method. A new CSI method for inversion with respect to all the electromagnetic parameters (the electric permittivity, electric conductivity and magnetic permeability) is also presented, which I have yet to find treated elsewhere. Only the former method is tested numerically, using synthetic data, due to the computational complexity of the latter. The numerical results from the forward modelling show the numerical validity of integral equation modelling and the fixed point iteration, whereas the results from the inversion show some promise. With several source and receiver lines present the lateral position of the scattering object is reconstructed well, whereas the vertical position causes problems. When textit{a priori} information about the position of the scattering object is introduced to further regularise the problem, the approximate position of it is successfully inverted, which illustrates the essential part additional regularisation plays in this inverse scattering problem. Thus the CSI method could be useful in petroleum geophysics, and should be developed further for the purpose of locating hydrocarbons in the subsurface. Several possibilities for further work is noted. This work was performed for StatoilHydro ASA. </p...|$|E
40|$|Two mud-mounds {{have been}} {{reported}} in the Ullin limestone near, but not in, the Aden oil field in Hamilton County, Illinois. One mud-mound is in the Broughton oil field of Hamilton County 25 miles to the south of Aden. The second mud-mound is in the Johnsonville oil field in Wayne County 20 miles to the north of Aden. Seismic reflection profiles were shot in 2012 adjacent to the Aden oil field to evaluate the oil prospects and to investigate the possibility of detecting Mississippian mud-mounds near the Aden field. A feature on one of the seismic profiles was interpreted to be a mud-mound or carbonate buildup. A well drilled at the location of this interpreted structure provided digital geophysical logs and geological logs used to refine the interpretation of the seismic profiles. Geological data from the new well at Aden, in the form of drill cuttings, have been used to essentially confirm the existence of a mud-mound in the Ullin limestone at a depth of 4300 feet. Geophysical well logs from the new well near Aden were used to create 1 -D computer models and synthetic seismograms for comparison to the seismic data. The reflection seismic method is widely used to aid interpreting subsurface geology. <b>Processing</b> <b>seismic</b> <b>data</b> is an important step in the method as a properly processed seismic section can give a better image of the subsurface geology whereas a poorly processed section could mislead the interpretation. Seismic reflections will be more accurately depicted with careful determination of seismic velocities and by carefully choosing the processing steps and parameters. Various data processing steps have been applied and parameters refined to produce improved stacked seismic records. The resulting seismic records from the Aden field area indicate a seismic response similar to what is expected from a carbonate mud-mound. One-dimensional synthetic seismograms were created using the available sonic and density logs from the well drilled near the Aden seismic lines. The 1 -D synthetics were used by Cory Cantrell of Royal Drilling and Producing Company to identify various reflections on the seismic records. Seismic data was compared with the modeled synthetic seismograms to identify what appears to be a carbonate mud-mound within the Aden study area. No mud-mounds have been previously found in the Aden oil field. Average and interval velocities obtained from the geophysical logs from the wells drilled in the Aden area was compared with the same type of well velocities from the Broughton known mud-mound area to observe the significance of velocity variation related to the un-known mud-mound in the Aden study area. The results of the velocity study shows a similar trends in the wells from both areas and are higher at the bottom of the wells. Another approach was used to observe the variation of root mean square velocities calculated from the sonic log from the well velocity from the Aden area and the stacking velocities obtained from the seismic data adjacent to the well...|$|E
40|$|Makoil, Inc., of Orange, California, {{with the}} support of the U. S. Department of Energy has {{reprocessed}} and reinterpreted the 3 D seismic survey of the Grant Canyon area, Railroad Valley, Nye County, Nevada. The project was supported by Dept. of Energy Grant DE-FG 26 - 00 BC 15257. The Grant Canyon survey covers an area of 11 square miles, and includes Grant Canyon and Bacon Flat oil fields. These fields have produced over 20 million barrels of oil since 1981, from debris slides of Devonian rocks that are beneath 3, 500 to 5, 000 ft of Tertiary syntectonic deposits that fill the basin of Railroad Valley. High-angle and low-angle normal faults complicate the trap geometry of the fields, and there is great variability in the acoustic characteristics of the overlying valley fill. These factors combine to create an area that is challenging to interpret from seismic reflection data. A 3 D seismic survey acquired in 1992 - 93 by the operator of the fields has been used to identify development and wildcat locations with mixed success. Makoil believed that improved techniques of <b>processing</b> <b>seismic</b> <b>data</b> and additional well control could enhance the interpretation enough to improve the chances of success in the survey area. The project involved the acquisition of hardware and software for survey interpretation, survey reprocessing, and reinterpretation of the survey. SeisX, published by Paradigm Geophysical Ltd., was chosen as the interpretation software, and it was installed on a Dell Precision 610 computer work station with the Windows NT operating system. The hardware and software were selected based on cost, possible addition of compatible modeling software in the future, and the experience of consulting geophysicists in the Billings area. Installation of the software and integration of the hardware into the local office network was difficult at times but was accomplished with some technical support from Paradigm and Hewlett Packard, manufacturer of some of the network equipment. A number of improvements in the processing of the survey were made compared to the original work. Pre-stack migration was employed, and some errors in muting in the original processing were found and corrected. In addition, improvements in computer hardware allowed interactive monitoring of the processing steps, so that parameters could be adjusted before completion of each step. The reprocessed survey was then loaded into SeisX, v. 3. 5, for interpretation work. Interpretation was done on 2, 21 -inch monitors connected to the work station. SeisX was prone to crashing, but little work was lost because of this. The program was developed for use under the Unix operating system, and some aspects of the design of the user interface betray that heritage. For example, printing is a 2 -stage operation that involves creation of a graphic file using SeisX and printing the file with printer utility software. Because of problems inherent in using graphics files with different software, a significant amount of trial and error is introduced in getting printed output. Most of the interpretation work was done using vertical profiles. The interpretation tools used with time slices are limited and hard to use, but a number to tools and techniques are available to use with vertical profiles. Although this project encountered a number of delays and difficulties, some unavoidable and some self-inflicted, the result is an improved 3 D survey and greater confidence in the interpretation. The experiences described in this report will be useful to those that are embarking on a 3 D seismic interpretation project...|$|E
40|$|Automatic <b>processing</b> of <b>seismic</b> <b>data</b> {{is today}} {{a key element}} in the efforts to achieve high quality seismic systems. Automated {{procedures}} for locating seismic events with a network including arrays and single element seismometers usually incorporate back-azimuth estimates, arrival-time data, and associated uncertainties into a least-squares-inverse location algorithm. Such an algorithm is quite cumbersome and requires expanding a set of non-linear equations in a Taylor series. Second-order terms usually not included in the algorithm can be important if the initial estimate is far from the solution...|$|R
40|$|AbstractIn tomographic image <b>processing</b> of <b>seismic</b> <b>data,</b> the first-arrival traveltime (FATT) {{is often}} {{different}} from those of more energetic wavefronts in realistic media. Since the traveltime of most-energetic wavefront (METT) dominates the data, computing the METT is recognized as an essential element in modern seismic imaging techniques. Solving the full wave equation is extremely expensive to be impractical even for large-size computers to carry out; the solution of the eikonal equation for which the corresponding amplitude is continuous is conjectured to be the METT...|$|R
40|$|Basic 4 {{stages of}} {{interpretation}} of seismic waves are outlined and methods for editing andpreliminary <b>processing</b> of <b>seismic</b> <b>data</b> are examined. <b>Seismic</b> <b>data</b> are filed on magnetic library tape and index-files for libraries are provided ondisk. pack for searching proper data according to various conditions for detailed interpretations. Using the data libraries methods for automated detection and identification of seismic wavesfrom local small earthquakes are developed as follows 1) determination of initial parts of P waveswith time varing spectra, 2) determination of P onset using amplitudes, directions and rectilinearitiesof particle motions and 3) identification of S phases from non-linear process {{based on the}} abovequantities of particle motions...|$|R
