16|128|Public
5000|$|The Khazar University Dictionary and Encyclopedia Center, {{located on}} the campus of Khazar University in Baku, Azerbaijan, has been in {{operation}} since April 1996. [...] The center is directed by Professor Tofik Abaskuliyev. The center’s main functions are to <b>perform</b> <b>linguistic</b> and lexicological research and to prepare, evaluate and publish bilingual, multilingual and encyclopedic dictionaries.|$|E
40|$|This paper {{presents}} a project whose main {{goal is to}} construct a corpus of clinical text manually annotated for part-of-speech information. We describe and discuss the process of training three domain experts to <b>perform</b> <b>linguistic</b> annotation. We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation. We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text. ...|$|E
40|$|We {{present a}} method for {{aggregating}} information from an internal, machine representation and building a text structure {{that allows us to}} express aggregations in natural language. Features of the knowledge representation system, a semantic network, allow us to produce an initial aggregation based on domain information and the competing aggregate structures. In the £nal stages of realization, the network representation allows us to use low-level (below the level of a clause) information to <b>perform</b> <b>linguistic</b> aggregation. The test bed for this work is an interactive tutoring system on home heating systems...|$|E
40|$|Blog posts {{are often}} informally written, poorly structured, rife with {{spelling}} and grammatical errors, and feature non-traditional content. These characteristics make them difficult to process with standard language analysis tools. <b>Performing</b> <b>linguistic</b> analysis on blogs is plagued by two additional problems: (i) {{the presence of}} spam blogs and spam comments and (ii) extraneous non-content including blog-rolls, link-rolls, advertisements and sidebars. We describe techniques designed to eliminate noisy blog data developed {{as part of the}} BlogVox system- a blog analytics engine we developed for the 2006 TREC Blog Track. The findings in this paper underscore the importance of removing spurious content from blog collections. ...|$|R
40|$|We {{report on}} the SUM project which applies {{automatic}} summarisation techniques to the legal domain. We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. We describe some experiments with judgments of the House of Lords where we have <b>performed</b> automatic <b>linguistic</b> annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to <b>perform</b> the <b>linguistic</b> annotation using XML-based tools and a combination of rulebased and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classifier...|$|R
40|$|We {{introduce}} a facial animation system that produces real-time animation sequences including speech synchronization and non-verbal speech-related facial expressions from plain text input. A state-of-the-art text-to-speech synthesis component <b>performs</b> <b>linguistic</b> {{analysis of the}} text input and creates a speech signal from phonetic and intonation information. The phonetic transcription is additionally used to drive a speech synchronization method for the physically based facial animation. Further high-level information from the linguistic analysis such as different types of accents or pauses {{as well as the}} type of the sentence is used to generate non-verbal speech-related facial expressions such as movement of head, eyes, and eyebrows or voluntary eye blinks. Moreover, emoticons are translated into XML markup that triggers emotional facial expressions...|$|R
40|$|Answer Extraction (AE) systems {{retrieve}} phrases in textual {{documents that}} directly answer natural language questions. AE over technical manuals requires very high recall and precision, and yet small text units must be retrieved. It is therefore important to <b>perform</b> <b>linguistic</b> analysis in detail. We present ExtrAns, an AE system over Unix manuals that uses full parsing, partial disambiguation, and anaphora resolution {{to generate the}} minimal logical forms of the documents and the query. The search procedure uses a proof algorithm of the user query over the Horn clause representation of the minimal logical forms. Remaining ambiguities in the retrieved sentences are dealt with by graded highlighting...|$|E
40|$|This paper {{explores the}} {{applications}} of fuzzy logic inference systems {{as an instrument}} to <b>perform</b> <b>linguistic</b> analysis {{in the domain of}} prosodic prominence. Understanding how acoustic features interact to make a linguistic unit be perceived as more relevant than the surrounding ones is generally needed to study the cognitive processes needed for speech understanding. It also has technological applications in the field of speech recognition and synthesis. We present a first experiment to show how fuzzy inference systems, being characterised by their capability to provide detailed insight about the models obtained through supervised learning can help investigate the complex relationships among acoustic features linked to prominence perception...|$|E
40|$|Micro {{blogging}} {{today has}} become a very popular communication tool among Internet users. Millions of users share opinions on different aspects of life every day. Therefore micro blogging web-sites are rich sources of data for opinion mining and sentiment analysis. Because micro blogging has appeared relatively recently, {{there are a few}} research works that are devoted to this topic. In this paper, we are focusing on using Twitter, the most popular micro blogging platform, for the task of Emotion analysis. We will show how to automatically collect a corpus for Emotion analysis and opinion mining purposes and then <b>perform</b> <b>linguistic</b> analysis of the collected corpus and explain discovered phenomena. Using the corpus, we will build a Emotion classifier that will be able to determine the emotion class of the person writing...|$|E
40|$|This paper {{demonstrates}} {{the implementation of}} a small monolingual lexical database for German (currently 7000 entries) for the purpose of manual and automated lexical queries. The lexicon is part of a web based text analysis application which serves to systematically analyze patient’s narratives from psychotherapy sessions. The narratives, small stories from everyday life, are conceived as dramaturgically constructed and <b>performed</b> <b>linguistic</b> productions (Boothe, 2004). The specific function of the lexicon in this context is to provide the means for the lexical coding of the story vocabulary, which is an important step in the narrative analysis 1. The interdisciplinary project is located in psychology, but involves likewise linguistics, corpus linguistics, lexicography, interactional linguistics and conversation analysis...|$|R
40|$|This article {{examines}} the rhetoric of Mexican-American hip hop through {{the analysis of the}} music by the multicultural hip hop fusion band, Ozomatli, from Los Angeles. The objective is to examine how Ozomatli <b>performs</b> <b>linguistic,</b> epistemic, and musical-rhetorical border crossing that provokes cultural and social consciousness. As a cross-cultural site of analysis, Ozomatli embodies cultural mestizaje, mestiza consciousness, and mestiz@ rhetoric that illuminates social justice issues beneath surface-level beats and rhythms. Appointed cultural ambassadors by the U. S. government, Ozomatli navigates dominant systems of power while performing music that contests hegemony. Their mestiz@ hip hop draws from diverse musical traditions like banda, cumbia, merengue, ranchera and others while addressing transnational social justice issues of immigration, inequality, and revolution...|$|R
50|$|Lakoff's The Language War <b>performs</b> a <b>linguistic</b> {{analysis}} of discourse on contemporary issues. She covers topics including the Hill-Thomas hearings, the O.J. Simpson trial, the Lewinsky scandal, {{and the political}} correctness phenomenon. Lakoff discusses each topic while arguing a general thesis that language itself constitutes a political battleground.|$|R
40|$|In a study {{published}} in 1987, Penelope Brown and Stephen Levinson proposed that an abstract sociolinguistic principle guides and constrains a speaker's choice of language, and that this principle explains the politeness phenomenon in conversation. Moreover, central to this principle is the concept of "face, " or personal self-image, implying that any given conversation begins with two conflicting "face wants, " one negative, i. e., the desire to act unimpeded by others; and the other positive, i. e., the desire to be liked and respected by others. In the resulting "balancing act,' ' the legitimate pursuit of a speaker's face needs often leads him/her to <b>perform</b> <b>linguistic</b> acts that threaten the face needs of others, necessitating the use of various strategies to overcome the problem. A major criticism of this model is its failure to reveal how face-threatening acts and politeness strategies interact sequentiall...|$|E
40|$|Microblogging {{today has}} become a very popular {{communication}} tool among Internet users. Millions of users share opinions on different aspects of life everyday. Therefore microblogging web-sites are rich sources of data for opinion mining and sentiment analysis. Because microblogging has appeared relatively recently, {{there are a few}} research works that were devoted to this topic. In our paper, we focus on using Twitter, the most popular microblogging platform, for the task of sentiment analysis. We show how to automatically collect a corpus for sentiment analysis and opinion mining purposes. We <b>perform</b> <b>linguistic</b> analysis of the collected corpus and explain discovered phenomena. Using the corpus, we build a sentiment classifier, that is able to determine positive, negative and neutral sentiments for a document. Experimental evaluations show that our proposed techniques are efficient and performs better than previously proposed methods. In our research, we worked with English, however, the proposed technique can be used with any other language. 1...|$|E
40|$|An {{overwhelming}} {{number of}} consumers are active in social media platforms. Within these platforms consumers are sharing their true feelings about a particular brand/product, its features, customer service and how it stands the competition. With the booming of microblogs on the Web, people have begun to express their opinions {{on a wide variety}} of topics on Twitter and other similar services. In a world where information can bias public opinion it is essential to analyse the propagation and influence of information in large-scale networks. Recent research studying social media data to rank users by topical relevance have largely focused on the “retweet", “following" and “mention" relations. We also <b>perform</b> <b>linguistic</b> analysis of the collected corpus and explain discovered phenomena. Using the corpus, we build a sentiment classifier, that is able to determine positive, negative and neutral sentiments for a document. This paper discusses how Twitter data is used as a corpus for analysis by the application of sentiment analysis and a study of different algorithms and methods that help to track influence and impact of a particular user/brand active on the social network...|$|E
40|$|This One major fact in today's {{technical}} world, {{people are}} very active users of Online Social Networks. They share every details of their day to day life and are {{in touch with their}} loved ones no matter in which part of the world they live. The main issue is the ability to control the messages that are posted in the user's private message or walls to detect and negotiate unwanted messages. This work focus on predicting the emotions of a particular message or post in various OSN like twitter, blogs etc for emotion analysis so as to filter the messages which are inappropriate. This paper focuses on collecting corpus for sentimental analysis and <b>performs</b> <b>linguistic</b> analysis and machine learning techniques for predicting emotions accurately. Using the corpus we define distinct emotions and filter unwanted messages...|$|R
40|$|The Language Muse TM system (LM) 1 is an {{application}} that supports classroom teachers in text modification for middle- and highschool learners who are non-native English speakers (NNES). The application <b>performs</b> <b>linguistic</b> analysis on classroom texts- highlighting lexical, syntactic, and pragmatic features, indicating potential areas of linguistic complexity. One of the lexical feature options that LM offers are synonym candidates for {{words in a}} text. This study investigates how to improve the current synonym detection feature, using a distributional method that identifies similar words [Lin 98, LC 03] and WordNet. For single words and multi-word expressions, human judges annotated “acceptability ” of synonym candidates from both resources. Humans attained high levels of agreement. Outcomes indicated that the distributional method and WordNet provide complementary options that together could provide improved resources. ...|$|R
40|$|We <b>perform</b> a <b>linguistic</b> {{analysis}} of documents during indexing for information retrieval. By eliminating index terms that occur only in subordinate clauses, index size is reduced by approximately 30 % without adversely affecting precision or recall. These results hold for two corpora: {{a sample of the}} world wide web and an electronic encyclopedia...|$|R
40|$|Retextualisation is a {{mechanism}} {{used to produce}} a new text based on one or more source-texts and involves deploying resources from a given language in order to <b>perform</b> <b>linguistic,</b> textual and discursive operations. The {{purpose of this article}} is to analyse and understand operations of retextualisation in the written production of learners of Portuguese as an Additional Language in multilingual contexts in which Spanish is one of the recurring FLs. The hypothesis is that the operations of retextualisation these learners deploy come from the FL they claim to know that is the closest to Portuguese. Data was generated by learners who study Portuguese as an Additional Language in a British university context and results from writing tasks in which students watched parts of a tv miniseries and produced a new text. The results show different operations of retextualisation in the learners’ writing production, among them regularisation, idealisation and transformation. The practical contribution reveals the perception of learners regarding the functioning of language and that their learning occurs through social practices of use of reading and writing...|$|E
40|$|Hidden Markov model (HMM) -based speech {{synthesis}} systems possess several advantages over concatenative synthesis systems. One such advantage is the relative {{ease with which}} HMM-based systems are adapted to speakers not present in the training dataset. Speaker adaptation methods used {{in the field of}} HMM-based automatic speech recognition (ASR) are adopted for this task. In the case of unsupervised speaker adaptation, previous work has used a supplementary set of acoustic models to estimate the transcription of the adaptation data. This paper first presents an approach to the unsupervised speaker adaptation task for HMM-based {{speech synthesis}} models which avoids the need for such supplementary acoustic models. This is achieved by defining a mapping between HMM-based synthesis models and ASR-style models, via a two-pass decision tree construction process. Second, it is shown that this mapping also enables unsupervised adaptation of HMM-based speech synthesis models without the need to <b>perform</b> <b>linguistic</b> analysis of the estimated transcription of the adaptation data. Third, this paper demonstrates how this technique lends itself to the task of unsupervised cross-lingual adaptation of HMM-based speech synthesis models, and explains the advantages of such an approach. Finally, listener evaluations reveal that the proposed unsupervised adaptation methods deliver performance approaching that of supervised adaptation...|$|E
40|$|The {{empirical}} {{study of}} {{language is a}} young field in contemporary linguistics. This being the case, and following a natural development process, the field is currently at a stage where different research methods and experimental approaches are being put into question {{in terms of their}} validity. Without pretending to provide an answer with respect to the best way to conduct linguistics related experimental research, in this article we aim at examining the process that researchers follow in the design and implementation of experimental linguistics research with a goal to validate specific theoretical linguistic analyses. First, we discuss the general challenges that experimental work faces in finding a compromise between addressing theoretically relevant questions and being able to implement these questions in a specific controlled experimental paradigm. We discuss the Granularity Mismatch Problem (Poeppel and Embick, 2005) which addresses the challenges that research that is trying to bridge the representations and computations of language and their psycholinguistic/neurolinguistic evidence faces, and the basic assumptions that interdisciplinary research needs to consider due to the different conceptual granularity of the objects under study. To illustrate the practical implications of the points addressed, we compare two approaches to <b>perform</b> <b>linguistic</b> experimental research by reviewing a number of our own studies strongly grounded on theoretically informed questions. First, we show how linguistic phenomena similar at a conceptual level can be tested within the same language using measurement of event-related potentials (ERP) by discussing results from two ERP experiments on the processing of long-distance backward dependencies that involve coreference and negative polarity items respectively in Dutch. Second, we examine how the same linguistic phenomenon can be tested in different languages using reading time measures by discussing the outcome of four self-paced reading experiments on the processing of in-situ wh-questions in Mandarin Chinese and French. Finally, we review the implications that our findings have for the specific theoretical linguistics questions that we originally aimed to address. We conclude with an overview of the general insights that can be gained from the role of structural hierarchy and grammatical constraints in processing and the existing limitations on the generalization of results...|$|E
40|$|The heading figurative {{language}} subsumes multiple phenomena {{that can be}} used to <b>perform</b> most <b>linguistic</b> functions including predication, modification, and reference. Figurative language can tap into conceptual and linguistic knowledge (as in the case of idioms, metaphor, and some metonymies) as well as evoke pragmatic factors in interpretation (as in indirect speech acts, humor, irony, o...|$|R
40|$|Behavioral {{models of}} {{computer}} systems {{are required for}} their synthesis, for verification and validation. The system behavior is usually described in requirements specifications. However, most specifications are provided in natural language or in a semi-formal way. Incompleteness and ambiguity inhibit their successful exploitation by tools. In this paper an approach for stepwise refinement and formalization of natural-language or semiformal descriptions is presented. Based {{on the structure of}} Use Case descriptions, the formalization of behavioral information is reached by a stepwise transformation of an input text to structured sentences with a well-defined syntax by <b>performing</b> <b>linguistic</b> analyses. The terms of the text are replaced by references to other items, i. e. of the glossary. By providing a tool-supported text output with hypertext-like navigation facilities, a verification of the result by the human experts is provided. The resulting behavioral description is suitable for a derivation of dynamic models, e. g. message-sequence charts and state diagrams...|$|R
40|$|Language and {{concepts}} are intimately linked, {{but how do}} they interact? In the study reported here, we probed the relation between conceptual and linguistic processing at the earliest processing stages. We presented observers with sequences of visual scenes lasting 200 or 250 ms per picture. Results showed that observers understood and remembered the scenes’ abstract gist and, therefore, their conceptual meaning. However, observers remembered the scenes at least as well when they simultaneously <b>performed</b> a <b>linguistic</b> secondary task (i. e., reading and retaining sentences); in contrast, a nonlinguistic secondary task (equated for difficulty with the linguistic task) impaired scene recognition. Further, encoding scenes interfered with performance on the nonlinguistic task and vice versa, but scene processing and <b>performing</b> the <b>linguistic</b> task did not affect each other. At the earliest stages of conceptual processing, the extraction of meaning from visually presented linguistic stimuli and the extraction of conceptual information from the world take place in remarkably independent channels...|$|R
40|$|Poor {{adherence}} to continuous positive airway pressure (CPAP) treatment {{in patients with}} obstructive sleep apnoea (OSA) limits its therapeutic effectiveness and has {{a major impact on}} clinical outcomes. Effective education programme is important to enhance CPAP use. However, existing education programmes are either manpower or resource demanding and may not be feasible in clinical practice. Moreover, the Self-Efficacy Measure for Sleep Apnoea (SEMSA) has been widely adopted for assessing adherence-related cognitions on CPAP therapy in OSA patients, but it was not available for Chinese. The aims of this thesis are: (i) to <b>perform</b> <b>linguistic</b> and psychometric evaluation of a Chinese version of SEMSA (SEMSA-C); (ii) to examine the efficacy of brief motivational enhancement education programme in addition to standard care versus standard care only on improving {{adherence to}} CPAP treatment in patients with OSA. The SEMSA-C was obtained after the standard forward-backward translation process. A randomised controlled trial was then conducted on newly diagnosed OSA patients. Patients in the control group received standard care (SC) comprising advice on the importance of CPAP therapy and its care while those in the intervention group received SC plus motivational enhancement education programme (ME). ME focused to enhance subjects’ knowledge, motivation and self-efficacy to use CPAP, comprising one 45 -minute session on the day after CPAP titration and one 10 -minute telephone follow-up shortly after commencing CPAP treatment. Epworth Sleepiness Scale (ESS), SEMSA-C, and quality of life were assessed. CPAP usage data were downloaded at the completion of this 3 -month study. The primary outcome was the CPAP adherence. Furthermore, 21 patients were randomly sampled at baseline and completed the SEMSA-C at one week. 100 patients (Men : Women, 84 : 16) with OSA indicated for CPAP treatment were recruited, with an average age of 52 ± 10 years, and apnoea hypopnoea index (AHI) of 36. 2 ± 22 events/hour. Factor analysis of SEMSA-C identified three factors: risk perception, outcome expectancies and treatment self-efficacy. Their corresponding internal consistency was high with Cronbach’s alpha > 0. 88, which were larger than all correlations between subscales (Range: 0. 14 to 0. 58). The correlations between items and their hypothesized subscale (Range: 0. 58 to 0. 85) were generally higher than the correlations between items and their competing subscales (Range: - 0. 10 to 0. 58). One-week test-retest intra-class correlation ranged from 0. 70 to 0. 82. CPAP adherence was associated with outcome expectancies and treatment self-efficacy at 3 -month assessment. Furthermore, SEMSA-C demonstrated an improvement in self-efficacy (standardised response mean = 0. 33, p =. 044) but no significant changes were observed in the other two factors, after CPAP use. The 100 patients were followed for 3 months. The interventional effects maintained during the 3 -month study period. There were a better CPAP use [higher daily CPAP usage of 2 hours/day (Cohen d = 1. 33, p <. 001), four-fold the number of subjects using CPAP for ≥ 70 % of days with ≥ 4 hours per day (p < 0. 001) ], and greater improvements in ESS by 2. 2 (p = 0. 001) and treatment self-efficacy by 0. 2 (p = 0. 012) in the intervention group, relative to the control group. The traditional Chinese SEMSA-C possesses satisfactory psychometric properties. It is a reliable and responsive instrument to measure perceived risks, outcome expectancies and treatment self-efficacy in Chinese patients with OSA. Moreover, the newly developed brief motivational enhancement education programme in addition to standard care is effective in improving adherence to CPAP treatment, treatment self-efficacy and daytime sleepiness. published_or_final_versionNursing StudiesDoctoralDoctor of Nursin...|$|E
40|$|This thesis {{concerns}} translation from English into Danish. Specifically, {{it concerns}} {{the practice of}} translation and the linguistic and textual competences of the translator. It has particular relevance to the academic disciplines of English linguistics and Translation Studies. Within Translation Studies, the thesis generally takes a linguistic approach and may be located within what Manfredi calls the 'contextual turn' (Manfredi 2008 : 47) or what Munday calls the discourse and register analysis approach (Munday 2001). I observe three problems in the translation process. First, translators, including myself, {{have a tendency to}} rely too much on intuition. Second, translators, including myself, have a tendency to make use of linguistic and textual analysis in a random way, often treating each clause in isolation. Third, in situations where several adequate translation solutions present themselves, it can be difficult for translators to establish a basis for choosing one solution over the other. In the application of Systemic Functional Linguistics to the development of translators' linguistic and textual competences, I see a possible solution to all three problems. Systemic Functional Linguistics seems particularly relevant to translation because it is not "concerned with a static or prescriptive kind of language study, but rather describes language in actual use and centres around texts and their contexts" (Manfredi 2008 : 49). At the heart of Systemic Functional Linguistics is the idea that language is structured to make three different kinds of meaning at the same time, experiential, interpersonal and textual. This thesis investigates to what extent the ability to map out systematically these three kinds of meaning in original texts, with the purpose of recreating them in translations, can enable the translator to make more informed translation decisions. The method I employ is (1) to <b>perform</b> <b>linguistic</b> and textual analyses based on SFL on three English texts and (2) to discuss to what extent such analyses are helpful to the translator. In my analytical approach, I draw on the work of German translation scholar Juliane House. In her model of translation quality assessment, she argues that the fundamental criterion of translation quality is equivalence on the level of textual function, that is to say, that the overall purpose(s) of the translated text must to a certain extent be the same as the overall purpose(s) of the original text. House applies elements of Systemic Functional Linguistics to analyse original texts with the purpose of establishing their textual function. She focuses especially on the levels of register and genre. Arguing that for translation production purposes, a more thorough and "close" analysis on the level of text/language is desirable, I focus especially on the level of lexicogrammar, building on M. A. K. Halliday's Introduction to Functional Grammar. The results of my investigation demonstrate five key functions of the application of Systemic Functional Linguistics to translation production, specifically that linguistic analysis may be used TRANSLATING WITH SFL JUNE 2015 PER BUDTZ-JØRGENSEN 53302 1. as a basis for selecting macrostrategy; 2. as a basis for selecting microstrategies; 3. to establish practical translation guidelines; 4. to suggest strategies for dealing with recurrent structural dissimilarities between English and Danish; 5. to assess translation solutions to specific words, wordings, meanings, etc., with reference to register, textual function. I argue that all five of these functions are relevant and valuable to the translation process, thus concluding that the application of Systemic Functional Linguistics to translation production can, to a certain extent, enable the translator to make more informed decisions. I go on to suggest that linguistic and textual analysis on the level of register and textual function is particularly relevant to translation while analysis on the level of lexicogrammar needs to be further developed {{if it is to be}} unequivocally relevant. This supports House's overall analytical approach, but goes somewhat against my own hypothesis. In a concluding discussion, I call for further research in the application of Systemic Functional Linguistics to translation so that, eventually, a model or theory might be developed that can indicate systematically how linguistic and textual analysis of experiential, interpersonal and textual meanings at the level of both lexicogrammar, discourse-semantics, register and genre might be approached. Furthermore, I call for a concept of register that is designed specifically for application to written texts, and which can account in greater detail for differences in the Field, Tenor and Mode variables...|$|E
40|$|The {{competency}} retention rate of Advanced Cardiac Life Support (ACLS) among nurses was poor. The knowledge retention rate {{among those who}} have completed an ACLS course rapidly declined to 37 % at three months, and 14 % at one year. Extending the course duration did not show any improvement in the retention rate. Structured debriefing may be a possible solution, but its efficacy on knowledge retention among nurses has not been examined. The aims of this thesis are to <b>perform</b> <b>linguistic</b> and psychometric evaluation of a Chinese version of Student Satisfaction and Self-confidence (SSSC), and to assess the effects in knowledge retention of nurses in a simulation-based resuscitation training, using the Gather-Analysis- Summary (GAS) model of structured debriefing. In this thesis, the reliability and scale structure of the Chinese SSSC were assessed in 161 nurses when they attended an ACLS course. Confirmatory factor analysis showed that a two-factor structure had satisfactory with χ^ 2 = 92. 12 (df = 54), RMSEA =. 07 (90 % CI =. 04 to. 09), CFI =. 98 and NNFI = 0. 96. The two factors are the satisfaction and self-confidence which had high internal reliability with Cronbach's alpha as. 95 and. 97, respectively. The Chinese SSSC is a reliable and valid tool to assess student satisfaction and self-confidence in simulation training. A cluster randomized controlled trial (RCT) was conducted to assess the efficacy of the GAS model. The primary outcome in this study was knowledge competency. It was assessed by the written test of the American Heart Association ACLS provider course. The secondary outcomes in this study were perceived skills, and self-efficacy. They were assessed using our validated Chinese SSSC and the General Self-Efficacy scale. In the intervention arm, the instructor conducted 18 scenarios and structured debriefing using the GAS model. Each GAS model of debriefing session consisted of three phases, namely: "Gather", "Analysis", and "Summary". The debriefing session was conducted to help participants to self-reflect on the learning objectives. Each debriefing session lasted about 5 to 10 minutes. In the control arm, participants had 36 scenario as usual teaching plan instead of debriefing {{at the end of each}} scenario. A follow-up assessment were conducted at Day Two and Week 24. A total of 161 nurses were recruited in this study. They were divided into 32 groups and randomized by groups to either the intervention arm (n= 85, 15 groups) or the control arm (n= 76, 17 groups). The effects of the GAS model were examined using a linear mixed effects model to take account of the extra-covariance among participants in the same group. The test showed that the GAS model debriefing improved knowledge retention after Day Two follow-up. However, it was marginally significant (estimated coefficient= 2. 56, 95 % CI=. 17 to 4. 95, d=. 22, p=. 036). Perceived skills (estimated coefficient=. 01, 95 % CI= -. 24 to. 27, d=. 15, p=. 918), and self-efficacy (estimated coefficient= -. 01 95 % CI= - 1. 27 to 1. 24, d=. 06, p=. 984) were also not significant. After adjusting the baseline by adding three significant different baseline factor, namely: education level, rank, and prior ACLS certification, the knowledge (estimated coefficient= 1. 54, 95 % CI=- 1. 06 to 4. 16, d=. 22, p=. 242). Perceived skills (estimated coefficient=. 01, 95 % CI= -. 26 to. 28, d=. 15, p=. 967), and self-efficacy (estimated coefficient= -. 31, 95 % CI=- 1. 64 to 1. 02, d=. 06, p=. 648) were insignificant. There were 73 nurses (45. 3 %) dropped out before 24 weeks. Overall, only 22 out of the remaining 88 (25 %) nurses passed the follow-up written test. Although debriefing based on the GAS model showed a sizable effect on knowledge retention at Week 24. the improvement was not significant (estimated coefficient= 5. 01, 95 % CI= - 1. 02 to 11. 04, d=. 17, p=. 101). The effects of debriefing on perceived skills (estimated coefficient=. 09, 95 % CI= -. 89 to 2. 7, d=. 22, p=. 317), and self-efficacy (estimated coefficient=. 51, 95 % CI= - 2. 19 to 3. 2, d=. 12, p=. 707) were also not significant. After the baseline adjustment, the debriefing effects on knowledge (estimated coefficient= 2. 15, 95 % CI= - 4. 26 to 8. 56, d=. 17, p=. 503), perceived skills (estimated coefficient=. 92, 95 % CI= -. 98 to 2. 82, d=. 22, p=. 336), and self-efficacy (estimated coefficient= -. 11, 95 % CI= - 3. 02 to 2. 79, d=. 12, p=. 938) remained insignificant. This is the first RCT to assess the effect of structured debriefing in ACLS simulation. The study showed that the GAS model of structured debriefing has potential effect on knowledge retention after 24 weeks. However, further larger-scale studies with stronger incentives are needed. published_or_final_versionNursing StudiesDoctoralDoctor of Nursin...|$|E
40|$|This {{paper is}} {{concerned}} with the application of Kohonen's self-organising feature map to legal knowledge acquisition. More precisely, the map is used for analysis of legal thesauri which are obtained by means of connotation analysis of the individual document descriptors. The outcome of the learning process of the artificial neural network is further used to distinguish between precise legal terms and terms with rather fuzzy meaning. INTRODUCTION The formalisation of legal data, e. g. statutes, court decisions or treaties, constitutes a necessary prerequisite for advanced legal information processing. Common approaches to cope with this problem are first, rewriting the law as a logic program and second, <b>performing</b> <b>linguistic</b> analysis of the legal language. The former approach represents a harsh simplification of the complexity of legal systems and thus leads to severe limitations as for example the open texture problem. Inherent to the latter approach is the obvious fact that so far [...] ...|$|R
40|$|International audienceThis paper {{presents}} {{a way to}} express linguistic knowledge independently of any algorithmic machinery and of any particular grammatical formalism. This is <b>performed</b> through <b>Linguistic</b> Properties, that will be presented. First, the status of linguistic knowledge in grammars is discussed, then the Linguistic Properties are presented and two experiments are mentioned. They illustrate the reusability of the linguistic information enclosed in these Properties...|$|R
40|$|This paper {{presents}} CAT- CELCT Annotation Tool, a new general-purpose web-based {{tool for}} text annotation developed by CELCT (Center for the Evaluation of Language and Communication Technologies). The aim of CAT {{is to make}} text annotation an intuitive, easy and fast process. In particular, CAT was created to support human annotators in <b>performing</b> <b>linguistic</b> and semantic text annotation and was designed to improve productivity and reduce time spent on this task. Manual text annotation is, in fact, a time-consuming activity, and conflicts may arise with the strict deadlines annotation projects are frequently subject to. Thanks to its adaptability and user-friendly interface, CAT can positively contribute to improve time management in annotation project. Further, the tool {{has a number of}} features which make it an easy-to-use tool for many types of annotations. Even if the first prototype of CAT has been used to perform temporal and event annotation following the It-TimeML specifications, the tool is general enough to be used for annotating a broad range of linguistic and semantic phenomena. CAT is freely available for research purposes...|$|R
40|$|Sentiment Analysis (SA) is {{the study}} of {{opinions}} and emotions that are conveyed by text. This field of study has commercial applications for example in market research (e. g., “What do customers like and dislike about a product?”) and consumer behavior (e. g., “Which book will a customer buy next when he wrote a positive review about book X?”). A private person can benefit from SA by automatic movie or restaurant recommendations, or from applications on the computer or smart phone that adapt to the user’s current mood. In this thesis we will put forward research on artificial Neural Network (NN) methods applied to SA. Many challenges arise, such as sarcasm, domain dependency, and data scarcity, {{that need to be addressed}} by a successful system. In the first part of this thesis we <b>perform</b> <b>linguistic</b> analysis of a word (“hard”) under the light of SA. We show that sentiment-specific word sense disambiguation is necessary to distinguish fine nuances of polarity. Commonly available resources are not sufficient for this. The introduced Contextually Enhanced Sentiment Lexicon (CESL) is used to label occurrences of “hard” in a real dataset with its sense. That allows us to train a Support Vector Machine (SVM) with deep learning features that predicts the polarity of a single occurrence of the word, just given its context words. We show that the features we propose improve the result compared to existing standard features. Since the labeling effort is not negligible, we propose a clustering approach that reduces the manual effort to a minimum. The deep learning features that help predicting fine-grained, context-dependent polarity are computed by a Neural Network Language Model (NNLM), namely a variant of the Log-Bilinear Language model (LBL). By improving this model the performance of polarity classification might as well improve. Thus, we propose a non-linear version of the LBL and the vectorized Log-Bilinear Language model (vLBL), because non-linear models are generally considered more powerful. In a parameter study on a language modeling task, we show that the non-linear versions indeed perform better than their linear counterparts. However, the difference is small, except for settings where the model has only few parameters, which might be the case when little training data is available and the model therefore needs to be smaller in order to avoid overfitting. An alternative approach to fine-grained polarity classification as used above is to train classifiers that will do the distinction automatically. Due to the complexity of the task, the challenges of SA in general, and certain domain-specific issues (e. g., when using Twitter text) existing systems have much room to improve. Often statistical classifiers are used with simple Bag-of-Words (BOW) features or count features that stem from sentiment lexicons. We introduce a linguistically-informed Convolutional Neural Network (lingCNN) that builds upon the fact that there has been much research on language in general and sentiment lexicons in particular. lingCNN makes use of two types of linguistic features: word-based and sentence-based. Word-based features comprise features derived from sentiment lexicons, such as polarity or valence and general knowledge about language, such as a negation-based feature. Sentence-based features are also based on lexicon counts and valences. The combination of both types of features is superior to the original model without these features. Especially, when little training data is available (that can be the case for different languages that are underresourced), lingCNN proves to be significantly better (up to 12 macro-F 1 points). Although, linguistic features in terms of sentiment lexicons are beneficial, their usage gives rise to a new set of problems. Most lexicons consist of infinitive forms of words only. Especially, lexicons for low-resource languages. However, the text that needs to be classified is unnormalized. Hence, we want to answer the question if morphological information is necessary for SA or if a system that neglects all this information and therefore can make better use of lexicons actually has an advantage. Our approach is to first stem or lemmatize a dataset and then perform polarity classification on it. On Czech and English datasets we show that better results can be achieved with normalization. As a positive side effect, we can compute better word embeddings by first normalizing the training corpus. This works especially well for languages that have rich morphology. We show on word similarity datasets for English, German, and Spanish that our embeddings improve performance. On a new WordNet-based evaluation we confirm these results on five different languages (Czech, English, German, Hungarian, and Spanish). The benefit of this new evaluation is further that it can be used for many other languages, as the only resource that is required is a WordNet. In the last part of the thesis, we use a recently introduced method to create an ultradense sentiment space out of generic word embeddings. This method allows us to compress 400 dimensional word embeddings down to 40 or even just 4 dimensions and still get similar results on a polarity classification task. While the training speed increases by a factor of 44, the difference in classification performance is not significant. Sentiment Analyse (SA) ist das Untersuchen von Meinungen und Emotionen die durch Text übermittelt werden. Dieses Forschungsgebiet findet kommerzielle Anwendungen in Marktforschung (z. B. : „Was mögen Kunden an einem Produkt (nicht) ?“) und Konsumentenverhalten (z. B. : „Welches Buch wird ein Kunde als nächstes kaufen, nachdem er eine positive Rezension über Buch X geschrieben hat?“). Aber auch als Privatperson kann man von Forschung in SA profitieren. Beispiele hierfür sind automatisch erstellte Film- oder Restaurantempfehlungen oder Anwendungen auf Computer oder Smartphone die sich der aktuellen Stimmungslage des Benutzers anpassen. In dieser Arbeit werden wir Forschung auf dem Gebiet der Neuronen Netze (NN) angewendet auf SA vorantreiben. Dabei ergeben sich viele Herausforderungen, wie Sarkasmus, Domänenabhängigkeit und Datenarmut, die ein erfolgreiches System angehen muss. Im ersten Teil der Arbeit führen wir eine linguistische Analyse des englischen Wortes „hard“ in Hinblick auf SA durch. Wir zeigen, dass sentiment-spezifische Wortbedeutungsdisambiguierung notwendig ist, um feine Nuancen von Polarität (positive vs. negative Stimmung) unterscheiden zu können. Häufig verwendete, frei verfügbare Ressourcen sind dafür nicht ausreichend. Daher stellen wir CESL (Contextually Enhanced Sentiment Lexicon), ein sentiment-spezifisches Bedeutungslexicon vor, welches verwendet wird, um Vorkommen von „hard“ in einem realen Datensatz mit seinen Bedeutungen zu versehen. Das Lexikon erlaubt es eine Support Vector Machine (SVM) mit Features aus dem Deep Learning zu trainieren, die in der Lage ist, die Polarität eines Vorkommens nur anhand seiner Kontextwörter vorherzusagen. Wir zeigen, dass die vorgestellten Features die Ergebnisse der SVM verglichen mit Standard-Features verbessern. Da der Aufwand für das Erstellen von markierten Trainingsdaten nicht zu unterschätzen ist, stellen wir einen Clustering-Ansatz vor, der den manuellen Markierungsaufwand auf ein Minimum reduziert. Die Deep Learning Features, die die Vorhersage von feingranularer, kontextabhängiger Polarität verbessern, werden mittels eines neuronalen Sprachmodells, genauer eines Log-Bilinear Language model (LBL) s, berechnet. Wenn man dieses Modell verbessert, wird vermutlich auch das Ergebnis der Polaritätsklassifikation verbessert. Daher führen wir nichtlineare Versionen des LBL und vectorized Log-Bilinear Language model (vLBL) ein, weil nichtlineare Modelle generell als mächtiger angesehen werden. In einer Parameterstudie zur Sprachmodellierung zeigen wir, dass nichtlineare Modelle tatsächlich besser abschneiden, als ihre linearen Gegenstücke. Allerdings ist der Unterschied gering, es sei denn die Modelle können nur auf wenige Parameter zurückgreifen. So etwas kommt zum Beispiel vor, wenn nur wenige Trainingsdaten verfügbar sind und das Modell deshalb kleiner sein muss, um Überanpassung zu verhindern. Ein alternativer Ansatz zur feingranularen Polaritätsklassifikation wie oben verwendet, ist es, einen Klassifikator zu trainieren, der die Unterscheidung automatisch vornimmt. Durch die Komplexität der Aufgabe, der Herausforderungen von SA im Allgemeinen und speziellen domänenspezifischen Problemen (z. B. : wenn Twitter-Daten verwendet werden) haben existierende Systeme noch immer großes Optimierungspotential. Oftmals verwenden statistische Klassifikatoren einfache Bag-of-Words (BOW) -Features. Alternativ kommen Zähl-Features zum Einsatz, die auf Sentiment-Lexika aufsetzen. Wir stellen linguistically-informed Convolutional Neural Network (lingCNN) vor, dass auf dem Fakt beruht, dass bereits viel Forschung in Sprachen und Sentiment-Lexika geflossen ist. lingCNN macht von zwei linguistischen Feature-Typen Gebrauch: wortbasierte und satzbasierte. Wort-basierte Features umfassen Features die von Sentiment-Lexika, wie Polarität oder Valenz (die Stärke der Polarität) und generellem Wissen über Sprache, z. B. : Verneinung, herrühren. Satzbasierte Features basieren ebenfalls auf Zähl-Features von Lexika und auf Valenzen. Die Kombination beider Feature-Typen ist dem Originalmodell ohne linguistische Features überlegen. Besonders wenn wenige Trainingsdatensätze vorhanden sind (das kann der Fall für Sprachen sein, die weniger erforscht sind als englisch). lingCNN schneidet signifikant besser ab (bis zu 12 macro-F 1 Punkte). Obwohl linguistische Features basierend auf Sentiment-Lexika vorteilhaft sind, führt deren Verwendung zu neuen Problemen. Der Großteil der Lexika enthält nur Infinitivformen der Wörter. Dies gilt insbesondere für Sprachen mit wenigen Ressourcen. Das ist eine Herausforderung, weil der Text der klassifiziert werden soll in der Regel nicht normalisiert ist. Daher wollen wir die Frage beantworten, ob morphologische Information für SA überhaupt notwendig ist oder ob ein System, dass jegliche morphologische Information ignoriert und dadurch bessere Verwendung der Lexika erzielt, einen Vorteil genießt. Unser Ansatz besteht aus Stemming und Lemmatisierung des Datensatzes, bevor dann die Polaritätsklassifikation durchgeführt wird. Auf englischen und tschechischen Daten zeigen wir, dass durch Normalisierung bessere Ergebnisse erzielt werden. Als positiven Nebeneffekt kann man bessere Wortrepresentationen (engl. word embeddings) berechnen, indem das Trainingskorpus zuerst normalisiert wird. Das funktioniert besonders gut für morphologisch reiche Sprachen. Wir zeigen auf Datensätzen zur Wortähnlichkeit für deutsch, englisch und spanisch, dass unsere Wortrepresentationen die Ergebnisse verbessern. In einer neuen WordNet-basierten Evaluation bestätigen wir diese Ergebnisse für fünf verschiedene Sprachen (deutsch, englisch, spanisch, tschechisch und ungarisch). Der Vorteil dieser Evaluation ist weiterhin, dass sie für viele Sprachen angewendet werden kann, weil sie lediglich ein WordNet als Ressource benötigt. Im letzten Teil der Arbeit verwenden wir eine kürzlich vorgestellte Methode zur Erstellen eines ultradichten Sentiment-Raumes aus generischen Wortrepresentationen. Diese Methode erlaubt es uns 400 dimensionale Wortrepresentationen auf 40 oder sogar nur 4 Dimensionen zu komprimieren und weiterhin die gleichen Resultate in Polaritätsklassifikation zu erhalten. Während die Trainingsgeschwindigkeit um einen Faktor von 44 verbessert wird, sind die Unterschiede in der Polaritätsklassifikation nicht signifikant...|$|E
40|$|In {{this paper}} I try to {{identify}} and describe in certain detail a possible avenue of research in machine translation: the use of existing multilingual content on the web and finite-state technology to automatically build and maintain fast web-based direct machine translation systems, especially for language pairs lacking machine translation resources. The term direct is {{used to refer to}} systems <b>performing</b> no <b>linguistic</b> analysis, working similarly to pretranslators based on translation memories. Considering th...|$|R
40|$|This thesis {{deals with}} a {{comparison}} of relative clauses in Czech and Spain language. The first part {{is devoted to the}} theoretical definition of the term "relative clause" in both languages and their comparison. In the second part the author <b>performed</b> a <b>linguistic</b> analysis of the Czech and Spanish comparable text with focuse on identification of the grammatical phenomena described in the theoretical part. This thesis is written in Czech language and includes a summary in Spanish...|$|R
50|$|SIL has {{developed}} widely used software for linguistic research. Adapt It {{is a tool}} for translating text from one language into a related language after <b>performing</b> limited <b>linguistic</b> analysis. In the field of lexicon collection, ShoeBox and the newer ToolBox (Field Linguist's Toolbox) and Lexique Pro, have largely been replaced by FieldWorks Language Explorer (FLEx Windows and Linux) for linguists, and WeSay (also Windows and Linux) for non-professionals. Graphite is a smart-font technology and rendering system.|$|R
40|$|Background. It {{has been}} {{suggested}} that cortical neural systems for language evolved from motor cortical systems, in particular from those fronto-parietal systems responding also to action observation. While previous studies have shown shared cortical systems for action – or action observation- and language, they did not address the question of whether linguistic processing of visual stimuli occurs only within a subset of fronto-parietal areas responding to action observation. If this is true, the hypothesis that language evolved from fronto-parietal systems matching action execution and action observation would be strongly reinforced. Methodology / Principal Findings. We used functional magnetic resonance imaging (fMRI) while subjects watched video stimuli of hand-object-interactions and control photo stimuli of the objects and <b>performed</b> <b>linguistic</b> (conceptual and phonological), and perceptual tasks. Since stimuli were identical for linguistic and perceptual tasks, differential activations had to be related to task demands. The results revealed that the linguistic tasks activated left inferior frontal areas that were subsets of a large bilateral fronto-parietal network activated during action perception. Not a single cortical area demonstrated exclusive – or even simply higher- activation for the linguistic tasks compared to the actio...|$|R
40|$|In this study, {{we present}} an {{automatic}} technique to help segment the Arabic texts while preserving the semantics. The technique {{is based on}} an empirical study on the sentences and clauses connectors. It has evolved from tedious analysis of various Arabic texts and from observations that have been noted {{over a long period of}} time. The analysis made it possible to realize the functionality of each connector in terms of separating standalone segments in the Arabic texts. This has lead to a categorization of active and passive connectors. We used the introduced notion of active and passive connectors to develop an algorithm that respects the semantic of the text to identify the segments of a given Arabic text. The algorithm has been implemented and experimented with. Various Arabic essays were segmented using the algorithm and the results were compared to that of manual segmentations <b>performed</b> by <b>linguistic</b> experts. The performance of the algorithm was in line with the manual segmentations that were <b>performed</b> by the <b>linguistic</b> experts...|$|R
