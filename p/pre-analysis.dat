295|0|Public
5000|$|Andrew Gelman {{references}} [...] "The Garden of Forking Paths" [...] {{to describe}} how scientists can make false discoveries {{when they do not}} pre-specify a data analysis plan and instead choose [...] "one analysis for the particular data they saw." [...] The [...] "Garden of Forking Paths" [...] refers to the near infinite number of choices facing researchers in cleaning and analyzing data, and emphasizes the need for <b>pre-analysis</b> planning and independent replication, an especially relevant consideration in social psychology's recent replication crisis.|$|E
50|$|In 2012 Miguel {{helped launch}} The Berkeley Initiative for Transparency in the Social Sciences (BITSS), {{which aims to}} promote {{transparency}} in empirical social science research. BITSS engages researchers through participatory forums on critical issues surrounding data transparency and encourages the use of study registries, <b>pre-analysis</b> plans, data sharing, and replication. In 2014, Miguel and co-authors published a piece in Science that makes the case for better research transparency practices in the social sciences. In 2015, Miguel and co-authors published another price in Science on the Transparency and Openness Promotion (TOP) guidelines.|$|E
50|$|It is {{dependent}} on the <b>pre-analysis</b> of the entire IP address space. There are more than 4 billion possible IP addresses, and detailed analysis of each of them is a Herculean task, {{especially in light of the}} fact that IP addresses are constantly being assigned, allocated, reallocated, moved and changed due to routers being moved, enterprises being assigned IP addresses or moving, and networks being built or changed. In order to keep up with these changes, complex algorithms, bandwidth measurement and mapping technology, and finely tuned delivery mechanisms are necessary. Once all of the IP space is analyzed, each address must be periodically updated to reflect changes in the IP address information, without invading a user's privacy. This process is similar in scale to the task of Web spidering.|$|E
40|$|A {{safe and}} {{deadlock}} free lock policy is introduced, called <b>pre-analysis</b> locking. <b>Pre-analysis</b> locking {{is based on}} an efficient geometric algorithm which inserts lock and unlock operations into the transactions. <b>Pre-analysis</b> locking is the first safe and deadlock free general locking policy which is not a variant of two-phase locking. It is an approach con-ceptually different from policies following the two-phase locking principle. In general, none of <b>pre-analysis</b> lock-ing and two-phase locking dominates the other: there exist cases in which <b>pre-analysis</b> locking allows for more concur-rency than any two-phase locking policy, but there are also cases in which a two-phase locking policy allows for more concurrency than <b>pre-analysis</b> locking. Keywords: database concurrency control, locking policy, serializability, safety, deadlock 1...|$|E
40|$|A {{safe and}} {{deadlock}} free locking policy is introduced, called <b>pre-analysis</b> locking. A transaction system with no lock and unlock {{operations in the}} transactions is first being analyzed by the <b>pre-analysis</b> locking algorithm. Then, {{the result of this}} analysis is used to insert lock and unlock operations into the transactions with the goal of achieving a degree of concurrency as high as possible. However, <b>pre-analysis</b> locking is merely a heuristic operating in polynomial time; therefore, it is not guaranteed to perform optimally in all cases. In comparison with 2 -phase locking, neither <b>pre-analysis</b> locking nor 2 -phase locking dominates the other; there exist transaction systems in which <b>pre-analysis</b> locking allows for more concurrency than any 2 -phase locking policy, but there are also cases in which a 2 -phase locking policy allows for more concurrency than <b>pre-analysis</b> locking. However, preanalysis locking is free from deadlocks, in general...|$|E
40|$|T he social sciences—including economics—have long {{called for}} {{transparency}} in research to counter threats to producing robust and replicable results (for example, McAleer, Pagan, and Volker 1985; Roth 1994). Recently, {{the push for}} transparency has focused on a few specific policies. In this paper, we discuss {{the pros and cons}} of three of the more prominent proposed approaches: <b>pre-analysis</b> plans, hypothesis registries, and replications. While these policies potentially extend to all different empirical and perhaps also theoretical approaches, they have been primarily discussed for experimental research, both in the field including random-ized control trials and the laboratory, so we focus on these areas. A <b>pre-analysis</b> plan is a credibly fixed plan of how a researcher will collect and analyze data, which is submitted before a project begins. <b>Pre-analysis</b> plans have been lauded in the popular press (for example, Chambers 2014; Nyhan 2014) and across the social sciences (for example, Humphreys, de la Sierra, and van der Windt 2013; Monogan 2013; Miguel et al. 2014). We will argue for tempering such enthusiasm for <b>pre-analysis</b> plans for three reasons. First, recent empirical literature suggests the behavioral problems that <b>pre-analysis</b> plans atten-uate are not a pervasive problem in experimental economics. Second, <b>pre-analysis</b> plans have quite limited value in cases where more than one hypothesis is tested, piloted, or surveyed, and also where null results may not be reported. However, in very costly one-of-a-kind field experiments, including heroic efforts as the Orego...|$|E
40|$|Locking is {{considered}} {{as a means to}} achieve serializable schedules of concurrent transactions. Transactions are assumed to be predeclared such that a <b>pre-analysis</b> for locking becomes feasible to increase concurrency. A condition for safety is introduced which, based on a <b>pre-analysis,</b> allows the design of policies strictly dominating known policies such as 2 -phase locking. The static case, in which the complete set of transactions is known in advance, and the online case, in which a transaction is known when it is started, are considered. It is shown that a policy strictly dominating 2 -phase locking and some other interesting <b>pre-analysis</b> policies can also be applied in an online environment...|$|E
30|$|Several {{authors have}} {{highlighted}} {{the importance of}} performing a <b>pre-analysis</b> in order to evaluate the representativeness, in terms of geographical origin and market share (current or future), of the energy datasets on which the background analysis is being carried out, both for fuels and electricity. The use of authoritative sources for this <b>pre-analysis</b> has been widely recommended (Di et al. 2007; Itten et al. 2014; Treyer and Bauer 2013, 2014).|$|E
40|$|The {{search for}} {{signatures}} of wave and oscillatory {{processes in the}} solar corona in the data obtained with imaging instruments can be automated by using the periodmap method. The method reduces a three-dimensional data cube to a two-dimensional map of the analysed field of view. The map reveals the presence and distribution of the most pronounced frequencies in the power spectrum of the time signal recorded at spatial pixels. We demonstrate the applicability of this method as a <b>pre-analysis</b> tool {{with the use of}} TRACE EUV coronal data, which contain examples of transverse and longitudinal oscillations of coronal loops. The main advantage of using periodmaps over other possible (more sophisticated) <b>pre-analysis</b> tools, such as wavelet analysis, is their robustness and efficiency (both in speed and computational power). The method can be implemented in the Hinode/XRT and SDO/AIA data <b>pre-analysis.</b> ...|$|E
40|$|We {{describe}} a new method for design error diagnosis in digital circuits, that doesn't use any error model. A diagnostic specific <b>pre-analysis</b> of the circuit extracts a subcircuit suspected to be erroneous. Contrary to other published works, here the necessary re-synthesis of the subcircuit {{need not be}} applied to the whole function of an internal signal in terms of primary inputs, but may stop at arbitrary nodes inside the circuit. As the subcircuits to be redesigned are kept as small as possible, the redesign procedure is simple and fast. Experimental data also show the high speed of the diagnostic <b>pre-analysis.</b> TIMA RESEARCH REPORT, 14 JUNE 1999 TIMA-UJF Bat. C de Physique B. P. 53 38041 Grenoble Cedex 9 France 2 Design Error Diagnosis in Digital Circuits without Error Model Raimund Ubar 1, Dominique Borrione Abstract We {{describe a}} new method for design error diagnosis in digital circuits, that doesn't use any error model. A diagnostic specific <b>pre-analysis</b> of the circuit [...] ...|$|E
40|$|We present set-based pre-analysis: a {{virtually}} universal op- timization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how ob- ject values flow through a program’s variables. Set-based <b>pre-analysis</b> {{relies on the}} observation that much of this rea- soning can {{take place at the}} set level rather than the value level. Computing constraints at the set level results in sig- nificant optimization opportunities: we can rewrite the in- put program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the sub- sequent value-based points-to computation. E ectively, set- based <b>pre-analysis</b> puts the program in a normal form opti- mized for points-to analysis. Compared to other techniques for o -line optimization of points-to analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: set-based <b>pre-analysis</b> can be performed on the input just once, e. g., allowing the pre-optimization of libraries that are subsequently reused many times and for di erent analyses. In experiments with Java programs, set-based <b>pre-analysis</b> eliminates 30 % of the program’s local variables and 30 % or more of computed context-sensitive points-to facts, over a wide set of bench- marks and analyses, resulting in a 20 % average speedup (max: 110 %, median: 18 %) ...|$|E
40|$|Abstract: Across {{the social}} sciences, growing {{concerns}} about research transparency {{have led to}} calls for <b>pre-analysis</b> plans, documents that lay out in advance how researchers intend {{to analyze the data}} they are about to gather. Such plans help readers to distinguish between exploratory and confirmatory analysis, thereby improving the credibility of the reported results. <b>Pre-analysis</b> plans, however, impose costs on researchers. They are time-consuming to write, especially if researchers attempt to describe in detail how they would handle the many contingencies that may arise in the course of data collection. In this article, we make the case for “standard operating procedures, ” default practices that researchers can fall back on in the event that their <b>pre-analysis</b> plan fails to address these contingencies. We offer an example of a documented set of standard operating procedures that may be adapted by other researchers seeking to place a safety net beneath their <b>pre-analysis</b> plans. Concerns about data fishing and publication bias have sparked a growing movement to promote transparency in social science research (Miguel et al. 2014; Nosek et al. 2015). One recent innovation is the public archiving of <b>pre-analysis</b> plans (PAPs) that specify details of the analysis (e. g., statistical methods, sample exclusions, outcome measures, covariates, and subgroup definitions) before the researchers see unblinded outcome data. 1 Deviations from the plans are not prohibited, but “when such deviations arise they [should] be highlighted and the effects on results reported ” (Humphreys, Sanchez de la Sierra, and van der Windt 2013, 13). In principle, PAPs have three main advantages. First, pre-specification limits the extent to which researchers can make decisions that consciously or unconsciously tilt a study toward a desired result (Rubin 2007; Tukey 1993). Second, the validity of frequentist statistical inference (standard errors, confidence intervals, p-values, and significance tests) hinges {{on the assumption that the}} analysis follows...|$|E
40|$|Method for {{determining}} the state of an electrochemical device by means of electrochemical impedance spectroscopy (EIS) and equivalent circuit model (ECM) comprising the steps of measuring the system impedance by any EIS device, modeling the measured impedance by an ECM, identifying the ECM parameters through an automatic minimization algorithm, and processing the ECM parameters values to simulate the electrochemical impedance of the device. In particular, the identifications {{of the number of}} parameters (i. e. the ECM model components) and of suitable starting values of these parameters are automatically detected through a geometrical <b>pre-analysis</b> of the shape of the measured impedance spectrum Z', which geometrical <b>pre-analysis</b> will be also called in the following as Geometrical First Guess (GFG) algorithm...|$|E
40|$|The {{purpose of}} this paper is to propose a new histogram-based diflerential current testing {{procedure}} of active defects. This procedure contains three steps: <b>Pre-analysis,</b> Failure Analysis, and Production Testing. The main objehtives of the <b>Pre-analysis</b> are to provide information on the most frequent active current defects and to help ‘setting the limits between faulty and fault-free devices: these limits are then investigated during the Failure Analysis step in’order to set the threshold between faulty and fault-free ICs. During the Production Testing step, this thtieshold is compared to the highest peak value appearing in a histogram built for each tested IC. Some selected results are presented, which confirm the test quality gain increase by using diflerential IDDQ instead o...|$|E
30|$|In this section, we {{describe}} the procedure of extracting cultural similarities from co-editing activity in Wikipedia, and present the network of significant shared interests between 110 language communities. The section begins with summarising our <b>pre-analysis</b> check of whether the language-concept overlap in Wikipedia is random.|$|E
40|$|Holistic Life Cycle Investigations for {{products}} {{and the related}} manufacturing chains require large efforts. The pre-quantification of sensitivities for process inputs underscores their importance for the results and determines the required grade of accuracy for quantification. Those indicators support an expedient construction of process models for the LCA/LCC application. Furthermore, a <b>pre-analysis</b> {{of the differences between}} product systems for comparative investigations support an adequate and tailored setting of system boundaries. The approach for those goal-oriented Life Cycle Investigations is applied in a Use Case describing a manufacturing chain of automotive structural parts made of composites with a focus on environmental impacts. The results of the <b>pre-analysis</b> are used for recommendations and rules for an expedient development of LCA models and later investigations...|$|E
40|$|This paper {{presents}} a new robust global motion estimation method based on <b>pre-analysis</b> {{of the video}} content. The novel idea in the proposed method, compared to classical robust statistics-based estimation methods, is to classify the video sequences into 3 classes based on the analysis of scene content before motion estimation. Different motion models and estimation methods are applied to different classes of image sequences. As a result, outliers can be identified and removed from the dominant motion estimate {{to solve the problem}} of inaccurate initial descending direction estimates associated with classical global motion estimation methods. The <b>pre-analysis</b> of scene content is based on the STGS (Spatial Temporal Gradient Scale) images derived from the original image sequences. The extra computation time for STGS-image-based <b>pre-analysis</b> of scene content is negligible compared to the overall speed and accuracy improvement achieved with the proposed method. Evaluations based on extensive experiments have shown that the proposed method significantly improves the speed of robust global motion estimation methods (saving about 50 % of the execution time of the classical methods). Keywords dominant motion estimation � robust statistics � STGS image � GN method The Global Motion Estimation (GME) technique is important for ‘sprites ’ generation and objec...|$|E
40|$|Aim: To {{discover}} how male students of nursing in Wales, articulate how they {{learn in the}} college pre-registration nursing environment Epistemological framework: The language of hegemony, hegemonic masculinity, complicity, marginalization and subordination from ConnelFs (1995) masculinities framework was revised to create what is called Ryan's model, which guided this study. Ryan's model was developed <b>pre-analysis</b> and post analysis. <b>Pre-analysis,</b> hegemony was interpreted as 'dominance and male positioning' and post analysis became 'dominance reduced/ mutual and neutral appreciation of nursing undergraduate peers'. <b>Pre-analysis</b> hegemonic masculinity was interpreted as 4 Men: their gendered practice in society' and post analysis 4 Men: their gendered practice in undergraduate nursing'. <b>Pre-analysis</b> - complicity was termed as hidden maleness and post analysis 'Maleness surfaces through the articulation of learning needs and demonstration of their learning'. <b>Pre-analysis</b> marginalization was interpreted as 'Imposed feelings of marginalization on learning opportunities' and post analysis 'Exclusion and being excluded feelings of inferiority when learning and visualising procedures in clinical practice. In the <b>pre-analysis,</b> Subordination was interpreted as 'gendered subordination' and post analysis as scenarios that captured the principles behind the 'in and out groups' and scenarios 'looking at inferior and superior knowledge'. The main ethos that gender is socially constructed connects to the analytical approach of Charmaz (2006). Methodology and methods: Thirteen participants {{took part in the}} two pilot studies and eleven in the main study. The research data consisted of tape recorded speech from focus groups and individual interviews. The analysis of fieldnotes contributed to triangulation. The aim was to develop a grounded theory based on how male students articulated how they learnt in the classroom and in clinical practice. Their implicit actions and speech were analysed using the contructivist grounded theory approach by Charmaz. Results: Four categories, along with a core category were developed. The core category, Voice: never hidden, released by masculinity captured the essence of the four categories. The act of coming together allowed a vocal space to discuss learning and how they see themselves in relation to the numerically dominant group. The male students retained their socially perceived masculinity by being able to voice how they learn through the dominance of their speech. Relationships within the classroom were de-gendered and a small number of participants assumed a neutral identity, 'the student nurse'. Limited opportunities resulted through discriminatory attitudes and the disability of dyslexia. Conclusion: The findings of this study suggested hegemonic masculinity was rarely practised through the medium of voice, but male students could openly call on the complicit nature of masculinity to voice how they learnt in college and in the clinical area. Their preferred route to acquire skills was through the visual route however, even the complicit nature of masculinity could not triumph in clinical practice to achieve learning with regards intimate care. Overall, the male student of nursing possesses masculinity, which is conducive with the nature of nursing and a sound ability to voice how they learn...|$|E
40|$|Objectives: to {{determine}} which component items of hospital bills, examined by nurse auditors, were adjusted the most during pre-analysis; to identify the impact upon revenue caused by the adjustments to bills analyzed by physician and nurse auditors; and to identify disallowances related to items checked by the audit team. Method: quantitative, exploratory, descriptive, single-case study. Results: after analysis of 2, 613 bills, {{it was found that}} the item most included by nurses was gas (90. 5 %) and the most excluded was inpatient drugs (41. 2 %). Hemodynamics materials, gases and equipment had the greatest impact on upward adjustments. Downward adjustments were the result of improper entries on bills and did not generate revenue losses. Of total disallowances, 52. 24 % were related to the <b>pre-analysis</b> of nurses and 47. 76 % to that of physicians. Conclusion: this study of the <b>pre-analysis</b> process provides input that enhances knowledge about hospital bill audits...|$|E
40|$|The project {{consists}} {{in the initial}} development of a web based and cloud computing services to allow students and researches to perform fast and very useful cut-based <b>pre-analysis</b> on a browser, using real data and official Monte-Carlo simulations (MC). Several tools are considered: ROOT files filter, JavaScript Multivariable Cross-Filter, JavaScript ROOT browser and JavaScript Scatter-Matrix Libraries. Preliminary but satisfactory results have been deployed online for test and future upgrades...|$|E
40|$|We {{present a}} method for {{selectively}} applying context-sensitivity during interprocedural program analysis. Our method applies context-sensitivity only when and where doing so is likely to improve the precision that matters for resolving given queries. The idea {{is to use a}} <b>pre-analysis</b> to estimate the impact of context-sensitivity on the main analysis’s precision, and to use this information to find out when and where the main analysis should turn on or off its context-sensitivity. We formalize this approach and prove that the analysis always benefits from the <b>pre-analysis</b> [...] guided context-sensitivity. We implemented this selective method for an existing industrial-strength interval analyzer for full C. The method reduced the number of (false) alarms by 24. 4 % while increasing the analysis cost by 27. 8 % on average. The use of the selective method is not limited to context-sensitivity. We demonstrate this generality by following the same principle and developing a selective relational analysis and a selective flow-sensitive analysis. Our experiments show that the method cost-effectively improves the precision in the these analyses as well...|$|E
40|$|ISBN: 0792377311 We {{describe}} a new method for design error diagnosis in digital circuits {{that does not}} use any error model. For representing the information concerning the erroneous signal paths in the circuit, a stuck-at fault model is used. This allows us to adopt the methods and tools of fault diagnosis used in hardware testing for the use in design error diagnosis. A diagnostic specific <b>pre-analysis</b> of the circuit based on stuck-at fault model extracts iteratively subcircuits suspected of being erroneous. Contrary to other published works, here the necessary re-synthesis of the extracted subcircuit need not {{be applied to the}} whole function of an internal signal in terms of primary inputs, but may stop at arbitrary nodes inside the circuit. As the subcircuits to be redesigned are kept as small as possible, the redesign procedure is simple and fast. The search for subcircuits to be rectified is carried out by a systematic procedure. The procedure is guided by a heuristic priority function derived from the results of diagnostic <b>pre-analysis.</b> Experimental data show the high speed of diagnostic preanalysis...|$|E
40|$|AStrion is an {{automatic}} spectrum analyser software, which proposes a new generic and data-driven method without any a priori {{information on the}} measured signals. In order to compute some general characteristics and derive {{the properties of the}} signal, the first step in this approach is to give some insight into the nature of the signal. This <b>pre-analysis,</b> the so-called data validation, contains a number of tests to reveal some of the properties and characteristics of the data, such as the acquisition validity (the absence of saturation and a posteriori in respect of the sampling theorem), the stationarity (or non-stationarity), the periodicity and the signal-to-noise ratio. Based on these characteristics, the proposed method defines indicators and alarm trigger thresholds and also categorises the signal into three classes, which helps to guide the following spectral analysis. The present paper introduces the four tests of this <b>pre-analysis</b> and the signal categorisation rules. Finally, the proposed approach is validated on a set of wind turbine vibration measurements to demonstrate its applicability for a long-term and continuous monitoring of real-world signals...|$|E
40|$|This paper {{focuses on}} a major {{requirement}} of rate control (RC) algorithms when implementing a low-power video coding system: RC must be compatible with a one-pass, sequential and local processing of the frame, data. This requirement prevents direct use of the most efficient RC algorithms, i. e., the ones that rely on rate-distortion (R-D) models whose parameters are computed from a <b>pre-analysis</b> of the input frame. The paper proposes to circumvent the problem by predicting the R-D model parameter(s) without accessing the current input frame data. It avoids the <b>pre-analysis</b> stage while keeping the benefit from R-D based rate control. The method is designed and illustrated for standardized and conventional hybrid coding schemes (H. 26 x, MPEG-x). Specifically, the mean absolute difference (MAD) of the motion prediction error, which is the key R-D model parameter, is predicted before the sequential processing of the input blocks. In order to validate the prediction, the behaviors of. rate control systems using either the actual (computed) or the estimated (predicted) MAD parameter are compared...|$|E
40|$|This {{paper was}} {{presented}} at the 22 nd Annual INCOSE International Symposium, Rome, Italy, 9 – 12 July 2012. The conference website is at: [URL] analysis of perspectives for “capability engineering” has been conducted by the INCOSE UK Capability Working Group (CWG). This paper is a continuation of this study led by the CWG ontology work stream that aims to develop a single shared ontology for the concept of capability engineering to enable semantic interoperability and to support a formal and explicit specification of a shared conceptualisation. Case study material from the different domains of rail, defence and information services was used. The ontology development was executed in three phases; (1) <b>pre-analysis,</b> (2) ontology modelling and (3) post-analysis. The <b>pre-analysis</b> involved literature reviews, requirements specification, systems engineering process utilisation; and resource identification i. e. examination of the case study material. The ontology modelling phase comprised information extraction and classification in addition to modelling and code representation using a mark-up tool, MS Excel and Protégé. The post-analysis involved validation workshops through using expert focus groups...|$|E
40|$|We {{discuss a}} {{statistical}} procedure {{to carry out}} empirical research that combines recent insights about <b>pre-analysis</b> plans and replication. Researchers send their datasets to an independent third party who randomly generates training and testing samples. Researchers perform their analysis on the training sample {{and are able to}} incorporate feedback from both colleagues, editors and referees. Once the paper is accepted for publication the method is applied to the testing sample and it is those results that are published. Simulations indicate that, under empirically relevant settings, the proposed method delivers more power than a <b>pre-analysis</b> plan. The effect mostly operate through a lower likelihood that relevant hypotheses are left untested. The method appears better suited for exploratory analyses where there is significant uncertainty about the outcomes of interest. We do not recommend using the method in situations where the treatment are very costly and thus the available sample size is limited. An interpretation of the method is that it allows researchers to perform direct replication of their work. We also discuss a number of practical issues about the method’s feasibility and implementation...|$|E
40|$|Abstract Binding time {{analysis}} {{has proved to}} be a valuable <b>pre-analysis</b> for partial evaluation. Until now (almost) all binding time analyses have been monovariant, such that binding time analysis could assign only one binding time description to each function definition. This means that if a function f (x) is called once with dynamic data, no reductions on x can be performed in the body of f even when f is called with static data...|$|E
30|$|Within the {{prototype}} experiments, we noted {{an increase of}} delay as the distance from microcontroller/fog devices to cloud instance increases both in scenario 1 and 2, as expected. The delay in scenario 1 was slightly higher than scenario 2 once the fog devices are added. However, the addition of fog devices (scenario 1) enables the <b>pre-analysis</b> of data collected by sensors before sending to the cloud. Thus, simple decisions, such as calling an ambulance, can be made quicker.|$|E
40|$|AbstractThis paper proposes {{the use of}} Stochastic Automata Networks (SAN) {{to develop}} models that can be {{efficiently}} applied to a large class of parallel implementations: master/slave (m/s) programs. We focus our technique {{in the description of}} the communication between master and slave nodes considering two standard behaviors: synchronous and asynchronous interactions. Although the SAN models may help the <b>pre-analysis</b> of implementations, the main contribution {{of this paper is to}} point out advantages and problems of the proposed modeling technique...|$|E
40|$|Abstract. Automatically {{choosing}} suitable native {{storage structures}} for XML documents {{arriving at the}} XML DBMS is a challenging task for its storage manager. While some of the critical parameters require pre-specification, others can be determined by <b>pre-analysis</b> or sampling of the incoming document or by just making experience-driven “educated guesses”. In this paper, we discuss approaches to achieve an adaptive behavior of the storage manager to provide tailor-made native XML storage structures to the extent possible. ...|$|E
40|$|The {{purpose of}} this study was to develop a manual that would {{encourage}} teachers and students to use their school site for educational activities across the curriculum by encouraging a high level of student involvement, identifying real or perceived barriers teachers have toward using school sites for education, and relating the use of school sites to the standards and objectives of the Utah Core Curriculum. A three-phase planning process was outlined in the manual that describes a step by step approach to planning for the use of a school site for education. The three phases in this process include the <b>pre-analysis</b> phase, analysis phase, and synthesis phase. The <b>pre-analysis</b> phase describes how to initiate this process by forming a committee and subcommittees, developing goals and objectives, determining level of commitment, developing a program (needs assessment), and collecting and mapping data. The analysis phase describes how to analyze the site to determine its educational or enhancement potential. The synthesis phase describes and illustrates how to develop a master plan and detailed plans for the use of a school site. Additionally, this phase describes how to implement and maintain an enhanced area of a school site for educational purposes...|$|E
40|$|Many {{emerging}} database applications, such as, automated financial trading, {{network management}} and manufacturing process control involve accessing and manipulating {{large amounts of}} data under time constraints. This {{has led to the}} emergence of active and real-time database as a research area, wherein transactions trigger other transactions and have time deadlines. In this dissertation, three important issues are investigated: the correctness criteria for various classes of transactions; real-time transaction scheduling algorithms for overload situation; and, a concurrency control policy that is sensitive to time deadline and transaction triggering. The first part of the dissertation deals with the issue of consistency of sensor reported data. We formally define sensor data consistency and a new notion of visibility called quasi immediate visibility (QIV) for concurrent execution of write-only and read-only transactions. We propose a protocol for maintaining sensor data consistency that has lower response time and higher throughput. The protocol is validated through simulation. Real-time schedulers must perform well both under underloaded and overloaded situations. In this dissertation, we propose a variation of weighted priority scheduling algorithm called Deadline Access Parameter Ratio (DAPR), that actively considers the I/O requirements and the amount of unprocessed work for "canned" transaction assumption. We show through simulation that DAPR performs significantly better than existing scheduling algorithms under overloaded situations. The limitation of the proposed algorithm is that in underloaded situations DAPR is not an option. The last part of this dissertation proposes a concurrency control (CC), called OCCWB, which is an extension of conventional optimistic CC. OCCWB takes advantage of the "canned" transaction assumption and includes <b>pre-analysis</b> stage, wherein, transactions are selectively blocked from executing if there is a high probability of restarting. The algorithm defines favorable serialization orders considering transaction semantics and tries to achieve such orders through appropriate priority adjustment. OCCWB is shown to perform significantly better than other CC policies under reasonable <b>pre-analysis</b> overhead for underloaded situation, and consistently better in overloaded situations, even with high <b>pre-analysis</b> overhead, {{for a wide range of}} workload and resource parameters...|$|E
40|$|Abstract—Cache {{analysis}} plays {{a crucial}} part when analyzing the WCET of an application. This paper presents ongoing work aiming at a precise cache analysis {{in the presence of}} pointer-based, heap-allocated data structures. The proposed analysis achieves precision by augmenting its abstract cache states with information about the structure of the program’s allocated objects as well as a short term access history of these objects. This additional information is derived from an adapted shape analysis that serves as an <b>pre-analysis</b> phase to our actual cache analysis. I...|$|E
40|$|Embedded systems {{commonly}} execute {{one program}} for their lifetime. Designing embedded system architectures with configurable components, such that those components can be tuned {{to that one}} program based on a program <b>pre-analysis,</b> can yield significant power and performance benefits. We illustrate such benefits by designing a loop cache specifically with tuning in mind. Our results show a 70 % reduction in instruction memory access, for MIPS and 8051 processors [...] representing twice the reduction from a regular loop cache, translating to good power savings...|$|E
40|$|We {{present a}} method for {{automatically}} learning an effective strategy for clustering variables for the Octagon analysis from a given codebase. This learned strategy works as a preprocessor of Octagon. Given a program to be analyzed, the strategy is first applied to the program and clusters variables in it. We then run a partial variant of the Octagon analysis that tracks relationships among variables within the same cluster, but not across different clusters. The notable aspect of our learning method is that although the method is based on supervised learning, {{it does not require}} manually-labeled data. The method does not ask human to indicate which pairs of program variables in the given codebase should be tracked. Instead it uses the impact <b>pre-analysis</b> for Octagon from our previous work and automatically labels variable pairs in the codebase as positive or negative. We implemented our method on top of a static buffer-overflow detector for C programs and tested it against open source benchmarks. Our experiments show that the partial Octagon analysis with the learned strategy scales up to 100 KLOC and is 33 x faster than the one with the impact <b>pre-analysis</b> (which itself is significantly faster than the original Octagon analysis), while increasing false alarms by only 2 %. </p...|$|E
40|$|Carbon (13 C) and {{nitrogen}} (15 N) stable isotope analysis {{has become increasingly}} important {{in the study of}} energy flow and tropho-dynamics in many ecosystems. Prior to analysis, samples are often pre-treated with acids to remove inorganic carbonates which may bias the results. The effects of <b>pre-analysis</b> acidification on isotopic values are, however, still poorly understood for marine producers (e. g. algae and cyanobacteria), and consumers (e. g. molluscs), which may confound the comparability of different studies. In this study, such effects (untreated vs. decalcified samples) were examined at two different sampling periods (summer and winter). Acidification did not seem to affect the isotopic composition of consumers, but reduced both δ 13 C and δ 15 N of producers. This effect was consistent for the two sampling periods, although both producers and consumers had more enriched δ 13 C and δ 15 N values in summer. Acidification had the most distinct, negative effect on the isotopic values of samples which had low carbonate contents. It is, therefore, important to be aware of temporal variations in sample isotopic values and especially the effects of sample treatment when attempting to compare different studies. As an attempt to standardize protocols, it is recommended that only acid-washing carbonate-rich samples is adopted as the most appropriate <b>pre-analysis</b> treatment...|$|E
