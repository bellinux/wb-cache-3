0|10000|Public
40|$|In 1970, M. Gerstenhaber {{introduced}} {{a list of}} axioms defining Moore categories {{in order to develop}} the Baer Extension Theory. In this <b>paper,</b> <b>we</b> study <b>some</b> <b>implications</b> between the axioms and compare them with more recent notions, showing that, apart from size restrictions, a Moore category is a pointed, strongly protomodular and Barr-exact category with cokernels...|$|R
40|$|Abstract- Behavioral {{economics}} is changing our understand-ing of how economic policy operates, including tax policy. In this <b>paper,</b> <b>we</b> consider <b>some</b> <b>implications</b> of behavioral economics for tax policy, {{such as how}} it changes {{our understanding of the}} welfare consequences of taxation, the relative desirability of using the tax system as a platform for policy implementation, and the role of taxes as an element of policy design. We do so by reviewing the logic of specific features of tax policy in light of recent findings in areas such as tax salience, program take-up, and fiscal stimulus...|$|R
40|$|Vol. 26 (2003), 85 - 91 <b>Some</b> <b>implications</b> of {{indivisibility}} {{of special}} values of zeta functions of real quadratic fields Iwao Kimura∗ Abstract. In this <b>paper,</b> <b>we</b> show <b>some</b> <b>implications</b> of Byeon’s result. For example, we prove that, for an odd prime number p, there exist infinitely many real quadratic fields Q(D) which satisfy following properties: For each non-negative integer n, let Q(D) n denote n-th {{layer of the}} cyclotomic Zp-extension over Q(D). Then, for each n ≥ 0, there exist infinitely many CM-fields K whose maximal real subfield is Q(D) n and whose relative Iwasawa λ- and µ-invariants for the cyclotomic Zp-extension over K are zero. 1...|$|R
40|$|In {{distributed}} simulations, such as multi-player distributed virtual environments (DVE), {{power consumption}} traditionally {{has not been}} a major design factor. However, emerging battery-operated mobile computing platforms require revisiting DVE implementation approaches for maximizing power efficiency. In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of power considerations in DVE implementation over mobile handhelds connected by wireless networks. We focus on the state dissemination problem in DVEs and propose a new power-aware dead reckoning framework for power-efficient state dissemination. We highlight a fundamental tradeoff between state consistency and power consumption, and present an adaptive dead reckoning algorithm that attempts to dynamically optimize the tradeoff at runtime. We present a quantitative evaluation of our approach using a synthetic DVE benchmark application. 1...|$|R
40|$|A quiet {{revolution}} is underway. Over the next 5 - 10 years inorganic-semiconductor-based solid-state lighting technology {{is expected to}} outperform first incandescent, and then fluorescent and high-intensity-discharge, lighting. Along the way, many decision points and technical challenges will be faced. To help understand these challenges, the U. S. Department of Energy, the Optoelectronics Industry Development Association and the National Electrical Manufacturers Association recently updated the U. S. Solid-State Lighting Roadmap. In {{the first half of}} this <b>paper,</b> <b>we</b> present an overview of the high-level targets of the inorganic-semiconductor part of that update. In the second half of this <b>paper,</b> <b>we</b> discuss <b>some</b> <b>implications</b> of those high-level targets on the GaN-based semiconductor chips that will be the “engine ” for solid-state lighting...|$|R
40|$|In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of the revived' Bretton Woods {{system for}} {{exchange}} market intervention and reserve management in periphery countries. Financial policies {{in these countries}} {{are seen as a}} component of a more general portfolio management policy in which the formation of an efficient domestic capital stock is a key objective. Because intervention in financial markets {{is an important part of}} their development strategy, intervention in exchange and financial markets has, and we argue will continue to be, large and persistent enough to generate predictable deviations of exchange rates and relative yields in industrial country financial markets from normal cyclical patterns. We argue that management of the currency composition of international reserves by emerging market governments and central banks is unlikely to alter these conclusions. ...|$|R
40|$|Abstract — Today’s Internet is a loose {{federation}} of independent network providers, each acting {{in their own}} self interest. In this <b>paper,</b> <b>we</b> consider <b>some</b> <b>implications</b> of this economic reality. Specifically, we consider how the incentives of the providers might determine where they choose to interconnect with each other; we show that for any given provider, determining an optimal placement of interconnection links is generally NP-complete. However, we present simple solutions for some special cases of this placement problem. We also consider the phenomenon of nearest-exit, or “hotpotato,” routing, where outgoing traffic exits a provider’s network as quickly as possible. If each link in a network is assessed a linear cost per unit flow through the link, we show that {{the total cost of}} nearest exit routing is no worse than three times the optimal cost. I...|$|R
40|$|In this <b>paper</b> <b>we</b> {{investigate}} <b>some</b> <b>implications</b> {{of recent}} results about salience on loan decisions. Using {{the framework of}} focus-weighted utility we show that consumers might take out loans even when that yield them negative utility. We claim however, that consumers are more prudent in their decisions and might {{be less likely to}} take out such loans when the usual fixed- and increasing-installment plans are coupled with a decreasing-installment option. We argue that harmful loan consumption, {{especially in the case of}} loans with increasing-installments (e. g. alternative mortgage loans), could be decreased if a policy would prescribe presentation of loan repayment schedules in a way that employs this effect. Moreover, using the model of focus-weighted utility we give a possible explanation for the unpopularity of decreasing-installment plans, the success of increasing-installment plans and their higher default rate during the financial crisis...|$|R
40|$|In this <b>paper</b> <b>we</b> examine <b>some</b> {{testable}} <b>implications</b> {{of growth}} theories based on threshold externalities and complementarities. Specifically, we use industry data {{for a set}} of eight emerging economies in East Asia and Eastern Europe to perform general tests of the big push industrialization hypothesis of Murphy, Shleifer, and Vishny (1989). The preliminary results reported here are generally supportive of the theory. They also suggest that government policy may {{have played a role in}} moving an economy from a "bad" to a "good" equilibrium. big push industrialization...|$|R
40|$|In this <b>paper</b> <b>we</b> look at <b>some</b> <b>implications</b> of {{the role}} of dynamic {{capabilities}} in enabling strategic decision-making in the firm. It is argued that properly specifying questions relating to dynamic capabilities raises questions of what is involved in strategic decisions, which in turn inevitably involves (or should involve) consideration of the complementary roles of nondecomposability and decision processes. This entails a reappraisal of how and where established frameworks {{may or may not be}} of relevance in this context, and invites consideration of how economics perspective should be framed to deal with such issues. ...|$|R
40|$|In {{terms of}} human interactions, social organization, {{technical}} infrastructures, and the voluntary character of participation, Free/Open Source Software (F/OSS) projects resemble many Community Informatics (CI) efforts. Along with similarities, important differences exist between F/OSS {{and other kinds}} of CI settings. F/OSS development projects may involve thousands of participants (programmers, users, etc.), dispersed in time, space, social status, culture, and skill, interacting via information and computing technologies (ICTs). These features bring F/OSS projects into the realm of Distributed Collective Practices (DCP), broadly understood as the mediated collaboration of large numbers of people, in significant social complexity, across long spans of time and space [Gasser & Ripoche 2003]. These features also mean that discourse necessarily becomes a key constitutive component of F/OSS projects. Many Community Informatics programs, in contrast, are smaller and more "geographically grounded: " the nature of community is significantly defined by shared interests in a localized geographical and geo-political space. In this <b>paper,</b> <b>we</b> examine <b>some</b> <b>implications</b> of the difference in size, scale, mode o...|$|R
40|$|In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> {{of viewing}} graphs as {{geometric}} objects. This approach {{offers a new}} perspective on a number of graph [...] theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the metric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) Keep down the dimension of the host space and (ii) Guarantee a small distortion, i. e., make sure that distances between vertices in G closely match the distances between their geometric images. In this <b>paper</b> <b>we</b> develop efficient algorithms for embedding graphs low [...] dimensionally with a small distortion. Further algorithmic applications include: ffl A simple, unified approach to a number of problems on multicommodity flows, including the Leighton [...] Rao Theorem [36] and some of its extensions. We solve an open question in this area, showing that the max [...] flow vs. min [...] c [...] ...|$|R
40|$|In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of the Miles and Snow (1978, 2003) {{typology}} of strategy types for public organizational adaptation and effectiveness, particularly {{as they are}} linked to organizational ambidexterity and dynamic capabilities. We see ambidexterity as referring to “the synchronous pursuit of both exploration and exploitation via loosely coupled and differentiated subunits or individuals, each of which specializes in either exploration or exploitation ” (Gupta, et al., 2006 : 691). Exploration and exploitation both involve learning, but of different types. Exploitation involves learning along an existing technological and stakeholder trajectory, while exploration involves learning along a trajectory distinct from existing ones. Ambidexterity is consistent with Miles and Snow’s Analyzer strategy type. We argue that to be effective over the longer term, all public organizations need to exhibit {{a certain amount of}} ambidexterity. The paper includes a lengthy discussion of organizational learning and strategic leadership, a crucial feature of successful pursuit of organizational ambidexterity. We pull our argument together {{in the form of a}} set of propositions relating ambidexterity and organizational effectiveness. Finally, we offer a set of conclusions...|$|R
40|$|In this <b>paper,</b> <b>we</b> discuss <b>some</b> {{practical}} <b>implications</b> {{for implementing}} adaptable network algorithms applied to non-stationary time series problems. Using electricity load data and training with the extended Kalman filter, we {{demonstrate that the}} dynamic model-order increment procedure of the resource allocating RBF network (RAN) is highly sensitive to {{the parameters of the}} novelty criterion. We investigate the use of system noise and forgetting factors for increasing the plasticity of the Kalman filter training algorithm, and discuss the consequences for on-line model order selection. We also find that a recently-proposed alternative novelty criterion, found to be more robust in stationary environments, does not fare so well in the non-stationary case due to the need for filter adaptability during training...|$|R
40|$|AbstractA {{proof of}} an {{asymptotic}} {{form of the}} original Goldbach conjecture for odd integers was published in 1937. In 1990, a theorem refining that result was published. In this <b>paper,</b> <b>we</b> describe <b>some</b> <b>implications</b> of that theorem in combinatorial design theory. In particular, we show {{that the existence of}} Paley's conference matrices implies that for any sufficiently large integer k there is (at least) about one third of a complex Hadamard matrix of order 2 k. This implies that, for any ε> 0, the well known bounds for (a) the number of codewords in moderately high distance binary block codes, (b) the number of constraints of two-level orthogonal arrays of strengths 2 and 3 and (c) the number of mutually orthogonal F-squares with certain parameters are asymptotically correct to within a factor of 13 (1 −ε) for cases (a) and (b) and to within a factor of 19 (1 −ε) for case (c). The methods yield constructive algorithms which are polynomial {{in the size of the}} object constructed. An efficient heuristic is described for constructing (for any specified order) about one half of a Hadamard matrix...|$|R
40|$|Power over {{education}} and the upcoming generations {{has always been an}} important instrument in shaping religious and secular values and a vehicle for social mobility. As a consequence, dominance over schooling was – particularly in periods of foreign occupation – an important means for bringing about acceptance of the new regime. In this <b>paper</b> <b>we</b> discuss <b>some</b> <b>implications</b> of the second World War for Belgian secondary schooling and bear on the ways in which Catholic principals and pupils responded to the German strategies of control. Within the institutional context of the installation of a new regime, the challenges pupils faced during the occupation are discussed. Not surprisingly, these were directly linked to the measures taken by the occupier. Apart from this, pupils also dealt with problems caused by the specific war conditions, such as malnutrition or bombardments. Since the occupation cannot be considered as a social, economic and political vacuum, it remains important to investigate whether pre-war confessional norms were equally accepted and what role the war played in this process. By doing so, we hope to illustrate the interrelations between pupils, the Church and the state and elucidate the role of pupils as «active makers of history». status: publishe...|$|R
40|$|It {{has been}} {{suggested}} that observed cognitive limitations may be an expression of the quantum-like structure of the mind. In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of this hypothesis for learning i. e., {{for the construction of a}} representation of the world. For a quantum-like individual, there exists a multiplicity of mentally incompatible (Bohr complementary) but equally valid and complete representations (mental pictures) of the world. The process of learning i. e., of constructing a representation involves two kinds of operations on the mental picture. The acquisition of new data which is modelled as a preparation procedure and the processing of data which is modelled as an introspective measurement operation. This process is shown not converge to a single mental picture but can evolve forever. We define a concept of entropy to capture relative intrinsic uncertainty. The analysis suggests a new perspective on learning. First, it implies that we must turn to double objectification as in Quantum Mechanics: the cognitive process is the primary object of learning. Second, it suggests that a representation of the world arises as the result of creative interplay between the mind and the environment. There is a degree of freedom that modifies the objective of rational learning...|$|R
40|$|This {{paper is}} {{concerned}} with the power of social science and its methods. We first argue that social inquiry and its methods are productive: they (help to) make social realities and social worlds. They do not simply describe the world as it is, but also enact it. Second, we suggest that, if social investigation makes worlds, then it can, in some measure, think about the worlds it wants to help to make. It gets involved in 'ontological politics'. We then go on to show that its methods - and its politics - are still stuck in, and tend to reproduce, nineteenth-century, nation-state-based politics. How might we move social science from the enactment of nineteenth-century realities? We argue that social-and-physical changes in the world are - and need to be - paralleled by changes in the methods of social inquiry. The social sciences need to re-imagine themselves, their methods, and their 'worlds' if they are to work productively in the twenty-first century where social relations appear increasingly complex, elusive, ephemeral, and unpredictable. There are various possibilities: perhaps, for instance, there is need for 'messy' methods. But in the present <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of complexity theory to see whether and how this might provide productive metaphors and theories for enacting twenty-first-century realities...|$|R
40|$|Unilateral or sub-global {{policies}} to combat climate change are potentially sensitive to free-riding and carbon leakage. One {{way of dealing}} with carbon leakage and competitiveness is the imposition of border adjustment measures for competing imports, for example {{in the form of the}} obligation to importers of goods to purchase and surrender emissions allowances to the authorities when importing. In this <b>paper,</b> <b>we</b> explore <b>some</b> <b>implications</b> of border adjustment measures in the EU ETS, for sectors that might be subject to carbon leakage. We examine the implications of two variants of these measures on the competitiveness of these sectors and on the global environment with the help of a multi-sector, multi-region computable general equilibrium (CGE) model of the global economy. Our calculations suggest that border adjustment might reduce the sectoral rate of leakage of the iron and steel industry rather forcefully, but that the reduction would be less for the mineral products sector, including cement. The reduction of the overall or macro rate of leakage would be modest. So, from an environmental point of view border tax adjustments would not be a very effective policy measure, but might mainly be justified by considerations of sectoral competitiveness. © 2009 Elsevier Ltd. All rights reserved...|$|R
40|$|In this <b>paper</b> <b>we</b> explore <b>some</b> <b>implications</b> of view-ing graphs as {{geometric}} objects. This approach of-fers a {{new perspective}} on a number of graph-theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the met-ric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) Keep down the dimension of the host space and (ii) Guarantee a small distortion, i. e., make sure that distances between vertices in G closely match the dis-tances between their geometric images. In this <b>paper</b> <b>we</b> develop efficient algorithms for em-bedding graphs low-dimensionally with a small distor-tion. Further algorithmic applications include: 0 A simple, unified approach to a number of prob-lems on multicommodity flows, including the Leighton-Rae Theorem [29] and some of its ex-tensions. 0 For graphs embeddable in low-dimensional spaces with a small distortion, we can find low-diameter decompositions (in the sense of [4] and [34]). The parameters of the decomposition depend only on the dimension and the distortion and not {{on the size of the}} graph. 0 In graphs embedded this way, small balanced separators can be found efficiently. Faithful low-dimensional representations of statisti-cal data allow for meaningful and efficient cluster-ing, which is one of the most basic tasks in pattern-recognition. For the (mostly heuristic) methods use...|$|R
40|$|In this <b>paper,</b> <b>we</b> discuss <b>some</b> {{practical}} <b>implications</b> {{for implementing}} adaptable network algorithms applied to non-stationary time series problems. Two real world data sets, containing electricity load demands and {{foreign exchange market}} prices, are used to test several different methods, ranging from linear models with fixed parameters, to non-linear models which adapt both parameters and model order on-line. Training with the extended Kalman filter, we demonstrate that the dynamic model-order increment procedure of the resource allocating RBF network (RAN) is highly sensitive to {{the parameters of the}} novelty criterion. We investigate the use of system noise for increasing the plasticity of the Kalman filter training algorithm, and discuss the consequences for on-line model order selection. The results of our experiments show that there are advantages to be gained in tracking real world non-stationary data through the use of more complex adaptive models...|$|R
40|$|Recently, {{both the}} ATLAS and CMS {{experiments}} have observed {{an excess of}} events {{that could be the}} first evidence for a 125 GeV Higgs boson. This is a few GeV below the (absolute) vacuum stability bound on the Higgs mass in the Standard Model (SM), assuming a Planck mass ultraviolet (UV) cutoff. In this <b>paper,</b> <b>we</b> study <b>some</b> <b>implications</b> of a 125 GeV Higgs boson for new physics in terms of the vacuum stability bound. We first consider the seesaw extension of the SM and find that in type III seesaw, the vacuum stability bound on the Higgs mass can be as low as 125 GeV for the seesaw scale around a TeV. Next <b>we</b> dicuss <b>some</b> alternative new physics models which provide an effective ultraviolet cutoff lower than the Planck mass. An effective cutoff Λ≃ 10 ^ 11 GeV leads to a vacuum stability bound on the Higgs mass of 125 GeV. In a gauge-Higgs unification scenario with five-dimensional flat spacetime, the so-called gauge-Higgs condition allows us to predict a Higgs mass of 125 GeV, with the compactification scale of the extra-dimension being identified as the cutoff scale Λ≃ 10 ^ 11 GeV. Identifying the compactification scale with the unification scale of the SM SU(2) gauge coupling and the top quark Yukawa coupling yields a Higgs mass of 121 ± 2 GeV...|$|R
40|$|In this <b>paper</b> <b>we</b> analyze <b>some</b> phenomenological <b>implications</b> of heterotic M-theory with five-branes. Recent {{results for}} the {{effective}} 4 -dimensional action are used to perform a systematic analysis of the parameter space, finding the restrictions that result from requiring {{the volume of the}} Calabi-Yau to remain positive. Then the different scales of the theory, namely, the 11 -dimensional Planck mass, the compactification scale and the orbifold scale, are evaluated. The expressions for the soft supersymmetry-breaking terms are computed and discussed in detail for the whole parameter space. With this information we analyze the theoretical predictions for the supersymmetric contribution to the muon anomalous magnetic moment, using the recent experimental result as a constraint on the parameter space. We finally analyze the viability of the neutralino as a dark matter candidate in this construction. The neutralino-nucleon cross-section is computed and compared with the sensitivities explored by present dark matter detectors...|$|R
40|$|Virtually all {{proposals}} for querying XML include {{a class of}} query we term "containment queries". It {{is also clear that}} in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises {{the question of how to}} support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this <b>paper,</b> <b>we</b> explore <b>some</b> performance <b>implications</b> of both options using native implementations in two comm [...] ...|$|R
40|$|A {{combination}} of recent observational results {{has given rise}} to what is currently known as the dark energy problem. Although several possible candidates have been extensively discussed in the literature to date the nature of this dark energy component is not well understood at present. In this <b>paper</b> <b>we</b> investigate <b>some</b> cosmological <b>implications</b> of another dark energy candidate: an exotic fluid known as the Chaplygin gas, which is characterized by an equation of state p = -A/ρ, where A is a positive constant. By assuming a flat scenario driven by non-relativistic matter plus a Chaplygin gas dark energy we study the influence of such a component on the statistical properties of gravitational lenses. A comparison between the predicted age of the universe and the latest age estimates of globular clusters is also included and the results briefly discussed. In general, we find that the behavior of this class of models may be interpreted as an intermediary case between the standard and ΛCDM scenarios. Comment: 7 pages, 5 figures, to appear in Phys. Rev. ...|$|R
40|$|In this <b>paper</b> <b>we</b> analyze <b>some</b> policy <b>implications</b> of incompleteness {{of markets}} in trade models, {{where there is}} both inter-spatial and intertemporal trade between countries. We {{interpret}} the absence of intertemporal trade as an absence of intermediation services provided by both domestic and foreign service providers. For simplicity, we consider extreme cases where intertemporal intermediation services can only be provided by domestic providers, so that when intertemporal trade in services is not allowed, markets are not complete. To our knowledge, this type of models is not used in the trade literature as general comparative statics results are unavailable. We use numerical simulation methods for insights. We first consider liberalization of financial services trade in a inter-spatial and intertemporal model of two countries, and we show how services liberalization can be welfare worsening {{in the presence of}} a tariff on goods trade spatially. We show that this can hold in a world with financial service trade autarky in which financial service trade liberalization involves both costless intertemporal intermediation provided by foreign service providers and in a more complex (and realistic...|$|R
40|$|A {{labyrinth}} was {{the infinite}} for Leibniz (who declares {{on several occasions}} that,as much from {{the angle of the}} infinite large as from that of the infinite small, infinity is nothing but a fiction in the attempts of thought to explain the phenomena). But another labyrinth was also for him the problem of the evilness, together with the correlative problem of goodness. Used, since he was a young mathematician, to the Leibnizian puzzle of the infinite, Javier Echeverría faces now the second labyrinth. And he approaches the subject by restoring the biblical myth of the tree that hides the mystery. Taken implicitly the opposite point of view that leads Kant to assert the intrinsic division of the reason, Echeverría, from the very title of his book, aims at goodness and evilness as effective goals for science. In this <b>paper</b> <b>we</b> discuss <b>some</b> <b>implications</b> of Echeverría’s bet. Un laberinto era el infinito para Leibniz (quien afirma en varias ocasiones que tanto lo infinitamente grande como lo infinitamente pequeño no son sino ficciones para el pensamiento que busca la explicación de los fenómenos) pero laberinto era asimismo, para él, la cuestión del mal, indisociable de problema del bien. Curtido, desde sus años de joven matemático, en el leibniziano puzzle del infinito y el continuo, Javier Echeverría se confronta ahora al segundo laberinto. Y lo hace restaurando el bíblico relato del árbol cuyo fruto escondería el misterio. Tomando implícitamente una vía opuesta a la que conduce a la kantiana división de la razón, Echeverría apunta a hacer de la polaridad bien-mal objeto de efectiva ciencia. En este escrito discutimos alguna de las implicaciones de su apuesta...|$|R
40|$|I {{study the}} {{relationship}} between portfolio choice and age for the Japanese households by means of micro data and by paying {{particular attention to the}} interaction between decisions to hold stocks and real estate. The major findings are: First, equity shares in financial wealth (S/FW) increase with age among young households, peaking in the fifties age group, then becoming constant. This peak comes in a much later stage of the life cycle compared with Amerkis and Zeldes (2001) report about U. S. households. Second, we observe exactly the same age-related pattern for real estate shares in household total wealth (RE/TW). Third, with respect to both shares, S/FW and RE/TW, the age-related patterns are mostly explained by the decision to hold or not to hold stocks/real estate. Fourth, no age-related pattern in equity holding is observed for households that do not own real estate. These findings suggest that the age-related pattern observed in stock holding will be mostly explained by household's tenure choice of housing. Households who are to purchase and have just purchased houses cannot take risky positions in financial investment because they are saving for down payments or taking heavily leveraged positions by taking out housing loans. Therefore any serious attempt at modeling Japanese households' dynamic portfolio choice should incorporate the effect of housing tenure choice. In {{the second half of the}} <b>paper,</b> <b>we</b> draw <b>some</b> policy <b>implications</b> from these findings. ...|$|R
40|$|First draft: March 2001; This Draft: February 2002 February 28, 2002 I {{study the}} {{relationship}} between portfolio choice and age for the Japanese households by means of micro data and by paying {{particular attention to the}} interaction between decisions to hold stocks and real estate. The major findings are: First, equity shares in financial wealth (S/FW) increase with age among young households, peaking in the fifties age group, then becoming constant. This peak comes in a much later stage of the life cycle compared with Amerkis and Zeldes (2001) report about U. S. households. Second, we observe exactly the same age-related pattern for real estate shares in household total wealth (RE/TW). Third, with respect to both shares, S/FW and RE/TW, the age-related patterns are mostly explained by the decision to hold or not to hold stocks/real estate. Fourth, no age-related pattern in equity holding is observed for households that do not own real estate. These findings suggest that the age-related pattern observed in stock holding will be mostly explained by household's tenure choice of housing. Households who are to purchase and have just purchased houses cannot take risky positions in financial investment because they are saving for down payments or taking heavily leveraged positions by taking out housing loans. Therefore any serious attempt at modeling Japanese households' dynamic portfolio choice should incorporate the effect of housing tenure choice. In {{the second half of the}} <b>paper,</b> <b>we</b> draw <b>some</b> policy <b>implications</b> from these findings. 科学研究費補助金（特定領域研究） = Grant-in-Aid for Scientific Research on Priority Area...|$|R
40|$|In this <b>paper,</b> <b>we</b> review <b>some</b> {{potential}} <b>implications</b> of waterbird ecology {{for their}} role as dispersers of aquatic plants and invertebrates. We focus particularly on internal transport (endozoochory) by the Anatidae (mainly ducks) and shorebirds, groups especially important for dispersal processes owing to their abundance, migratory habitats and diets. We conduct a literature review to assess the seasonal patterns shown by Anatidae in consumption of seeds and plankton, the interspecific patterns in such consumption (including the effects of body size, bill morphology, etc.), and differences in habitat use (e. g., shoreline vs. open water specialists) and migration patterns between species (e. g., true migrants vs. nomads). We show that many shorebirds are important consumers of seeds as well as plankton, and suggest that their role in plant dispersal has been underestimated. This review confirms that Anatidae, shorebirds and other waterbirds have great potential as dispersers of aquatic organisms, but illustrates how closely related, sympatric bird species can have very different roles in dispersal of specific aquatic organisms. Furthermore, great spatial and temporal variation is likely in dispersal patterns realized by a given bird population. We present evidence suggesting that northbound dispersal of aquatic propagules by endozoochory during spring migration is a frequent process in the northern hemisphere. Much more systematic fieldwork and reanalysis of the existing data sets (e. g., from diet studies) are needed before the relative roles of various waterbird species as dispersers can be fully assessed. © 2002 Éditions scientifiques et médicales Elsevier SAS. All rights reserved...|$|R
40|$|The {{literature}} on inventory holdings stresses {{their role in}} smoothing production when costs are convex. Existing empirical evidence suggests that output is more variable than consumption so that production smoothing is not apparently present. One way of explaining this finding is to allow for nonconvex technologies. In this <b>paper,</b> <b>we</b> investigate <b>some</b> macroeconomic <b>implications</b> of the proposition {{that at least some}} firms in the economy produce with non-convex technologies. We begin our analysis by studying a simple Robinson Crusoe economy with a single, storable good which is produced from a non-convex technology. The single agent can produce a finite amount of output simply by incurring a fixed production cost. We demonstrate that the efficient solution to this problem will entail periods of production followed by periods of inactivity: i. e. production will be bunched rather than smoothed. More importantly, inventories will be used to smooth consumption relative to this production path. Still, as long as the agent discounts the future or inventories depreciate over time, consumption will not be totally smooth. Instead, consumption will be highest in periods of production. Thus the non-convex technology will induce fluctuations in both production and consumption. Using this analysis as a starting point, we then consider the implications of a non -convex technology in one sector of the economy for the behavior of other sectors through intersectoral technological linkages for both centralized and decentralized economies. For the centralized setting, the extent to which non- convexities spillover to other sectors depends on the degree to which intermediate and final goods can be inventoried {{and the nature of the}} technological interaction between factors. For the decentralized economy, the production of inputs which are strategic complements (substitutes) will be synchronized (staggered). Thus the presence of strategic complementarities (as in imperfectly competitive markets) will imply that non-convexities will have aggregate implications. ...|$|R
40|$| his writings. Finally, <b>we</b> discuss <b>some</b> <b>implications</b> of Foucaults|$|R
40|$|<b>We</b> show <b>some</b> <b>implications</b> of the {{approach}} to AdS/CFT correspondence based on Type IIB string in the flat space-time with D 3 -branes proposed in our previous <b>paper.</b> <b>We</b> discuss a correspondence for high energy scattering amplitudes of N = 4 super-Yang-Mills proposed recently. We also discuss AdS/CFT correspondence at finite temperature. Our approach provides clear understanding of these issues. ...|$|R
30|$|In this <b>paper,</b> <b>we</b> give <b>some</b> {{interesting}} identities {{which are}} derived from the basis of Genocchi. From our methods which are treated in this <b>paper,</b> <b>we</b> can derive <b>some</b> new identities associated with Bernoulli and Euler polynomials.|$|R
40|$|Only a {{small part}} of the {{research}} which has been carried out to date on the preservation of digital objects has looked specifically at the preservation of software. This is because the preservation of software has been seen as a less urgent problem than the preservation of other digital objects, and also the complexity of software artifacts makes the problem of preserving them a daunting one. Nevertheless, there are good reasons to want to preserve software. In this <b>paper</b> <b>we</b> consider <b>some</b> of the motivations behind software preservation, based on an analysis of software preservation practice. We then go on to consider what it means to preserve software, discussing preservation approaches, and developing a performance model which determines how the adequacy of the a software preservation method. Finally <b>we</b> discuss <b>some</b> <b>implications</b> for preservation analysis for the case of software artifacts...|$|R
40|$|The {{comparison}} of algorithms complexities {{can be reduced}} to the {{comparison of}} complexity functions. In two previous <b>papers,</b> <b>we</b> obtained <b>some</b> results related to the comparison of one-variable complexity functions using complexity classes. In this <b>paper,</b> <b>we</b> extend <b>some</b> of these results to multivariable complexity functions. multivariable complexity function, complexity class, functions comparison...|$|R
