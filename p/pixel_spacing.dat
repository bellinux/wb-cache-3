106|261|Public
25|$|The maximum angular {{resolution}} of the human eye {{at a distance of}} 1km is typically 30 to 60cm. This gives an {{angular resolution}} of between 0.02 and 0.03 degrees, which is roughly 1.2–1.8 arc minutes per line pair, which implies a <b>pixel</b> <b>spacing</b> of 0.6–0.9 arc minutes.|$|E
50|$|The {{prototype}} achieved 15μm depth {{resolution and}} 50μm lateral resolution (limited by the <b>pixel</b> <b>spacing)</b> {{at up to}} 0.5-meter range. It was capable of detecting a 1% equivalent refractive index contrast at 1mm thickness.|$|E
50|$|In remote sensing, spatial {{resolution}} is typically limited by diffraction, {{as well as}} by aberrations, imperfect focus, and atmospheric distortion. The ground sample distance (GSD) of an image, the <b>pixel</b> <b>spacing</b> on the Earth's surface, is typically considerably smaller than the resolvable spot size.|$|E
40|$|In this document, I present {{longitudinal}} research investigating multi-display environments (MDE) in collaborative settings over an eight-week {{period at}} two companies. I also present preliminary laboratory research results exploring {{the effects of}} increasing shared <b>pixel</b> <b>spaces</b> within a collaborative meeting environment {{and its effect on}} performance, collaboration, and satisfaction with the meeting process. This research seeks to explore whether increasing shared <b>pixel</b> <b>spaces</b> contributes to a better comprehension of the meeting, or serves as a distraction or additional element competing for attention resources...|$|R
40|$|One of {{the major}} targets for {{next-generation}} cosmic microwave background (CMB) experiments is the detection of the primordial B-mode signal. Planning is under way for Stage-IV experiments that are projected to have instrumental noise small enough to make lensing and foregrounds the dominant source of uncertainty for estimating the tensor-to-scalar ratio $r$ from polarization maps. This makes delensing {{a crucial part of}} future CMB polarization science. In this paper we present a likelihood method for estimating the tensor-to-scalar ratio $r$ from CMB polarization observations, which combines the benefits of a full-scale likelihood approach with the tractability of the quadratic delensing technique. This method is a <b>pixel</b> <b>space,</b> all order likelihood analysis of the quadratic delensed B modes, and it essentially builds upon the quadratic delenser by taking into account all order lensing and <b>pixel</b> <b>space</b> anomalies. Its tractability relies on a crucial factorization of the <b>pixel</b> <b>space</b> covariance matrix of the polarization observations which allows one to compute the full Gaussian approximate likelihood profile, as a function of $r$, at the same computational cost of a single likelihood evaluation...|$|R
40|$|Szapudi et al (2001) {{introduced}} {{the method of}} estimating angular power spectrum of the CMB sky via heuristically weighted correlation functions. Part of the new technique is that all (co) variances are evaluated by massive Monte Carlo simulations, therefore a fast way to measure correlation functions in a high resolution map is essential. This letter presents a new algorithm to calculate <b>pixel</b> <b>space</b> correlation functions via fast spherical harmonics transforms. Our present implementation of the idea extracts correlations from a MAP-like CMB map (HEALPix resolution of 512, i. e. ≃ 3 × 10 ^ 6 pixels) in about 5 minutes on a 500 MHz computer, including C_ℓ inversion; the analysis of one Planck-like map takes less then one hour. We use heuristic window and noise weighting in <b>pixel</b> <b>space,</b> and include the possibility of additional signal weighting as well, either in ℓ or <b>pixel</b> <b>space.</b> We apply the new code to an ensemble of MAP simulations, to test the response of our method to the inhomogenous sky coverage/noise of MAP. We show that the resulting C_ℓ's {{are very close to}} the theoretical expectations. The HEALPix based implementation of the method, SpICE (Spatially Inhomogenous Correlation Estimator) will be available to the public from the authors. Comment: 6 pages, 4 figures, submitted to ApJ Let...|$|R
50|$|The maximum angular {{resolution}} of the human eye {{at a distance of}} 1 km is typically 30 to 60 cm. This gives an {{angular resolution}} of between 0.02 and 0.03 degrees, which is roughly 1.2-1.8 arc minutes per line pair, which implies a <b>pixel</b> <b>spacing</b> of 0.6-0.9 arc minutes.6/6 vision is defined as the ability to resolve two points of light separated by a visual angle of one minute of arc, or about 320-286 pixels per inch for a display on a device held 25 to 30 cm from the eye.|$|E
50|$|The {{resolution}} of a digital imaging device {{is not only}} limited by the optics, {{but also by the}} number of pixels, more in particular by their separation distance. As explained by the Nyquist-Shannon sampling theorem, to match the optical {{resolution of}} the given example, the pixels of each color channel should be separated by 1 micrometer, half the period of 500 cycles per millimeter. A higher number of pixels on the same sensor size will not allow the resolution of finer detail. On the other hand, when the <b>pixel</b> <b>spacing</b> is larger than 1 micrometer, the resolution will be limited by the separation between pixels; moreover, aliasing may lead to a further reduction of the image fidelity.|$|E
50|$|The pixel {{scale used}} in {{astronomy}} is the angular distance between two objects {{on the sky}} that fall one pixel apart on the detector (CCD or infrared chip). The scale s measured in radians is {{the ratio of the}} <b>pixel</b> <b>spacing</b> p and focal length f of the preceding optics, s=p/f. (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)Because p is usually expressed in units of arcseconds per pixel, because 1 radian equals 180/π*3600≈206,265 arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as s=206p/f.|$|E
5000|$|... #Caption: The {{original}} Lemmings walk-cycle sprite animations from Mike Dailly (left) and Gary Timmons' improved version on the right. The animation {{was done}} using an 8×10 <b>pixel</b> <b>space,</b> with Timmons' version showing a less stiff walk cycle. The animation has eight frames and takes 0.8 seconds to complete one cycle.|$|R
40|$|We {{propose a}} new {{internal}} linear combination (ILC) method in the <b>pixel</b> <b>space,</b> applicable on large angular {{scales of the}} sky, to estimate a foreground minimized Cosmic Microwave Background (CMB) temperature anisotropy map by incorporating prior knowledge about the theoretical CMB covariance matrix. Usual ILC method in <b>pixel</b> <b>space,</b> on the contrary, does not use any information about the underlying CMB covariance matrix. The new approach complements the usual <b>pixel</b> <b>space</b> ILC technique specifically at low multipole region, using global information available from theoretical CMB covariance matrix {{as well as from}} the data. Since we apply our method over the large scale on the sky containing low multipoles we perform foreground minimization globally. We apply our methods on low resolution Planck and WMAP foreground contaminated CMB maps and validate the methodology by performing detailed Monte-Carlo simulations. Our cleaned CMB map and its power spectrum have significantly less error than those obtained following usual ILC technique at low resolution that does not use CMB covariance information. Another very important advantage of our method is that the cleaned power spectrum does not have any negative bias at the low multipoles because of effective suppression of CMB-foreground chance correlations on large angular scales of the sky. Our cleaned CMB map and its power spectrum match well with those estimated by other research groups. Comment: 11 pages, 12 figure...|$|R
30|$|In this subsection, we {{visualize}} the facial features {{that lead to}} the miss-classifications in both tasks. To do this, we apply the process of deconvolution, which provides top–down projections by mapping activations in feature maps generated in intermediate layers back to the input <b>pixel</b> <b>space,</b> showing the patterns that were learned by the feature maps.|$|R
50|$|Trying to kick-start the market, AT&T Corporation {{entered the}} fray, and in May 1981 {{announced}} its own Presentation Layer Protocol (PLP). This was closely {{based on the}} Canadian Telidon system, but added to it some further graphics primitives and a syntax for defining macros, algorithms to define cleaner <b>pixel</b> <b>spacing</b> for the (arbitrarily sizeable) text, and also dynamically redefinable characters and a mosaic block graphic character set, {{so that it could}} reproduce content from the French Antiope. After some further revisions this was adopted in 1983 as ANSI standard X3.110, more commonly called NAPLPS, the North American Presentation Layer Protocol Syntax. It was also adopted in 1988 as the presentation-layer syntax for NABTS, the North American Broadcast Teletext Specification.|$|E
5000|$|The Lucas-Kanade method per se can be {{used only}} when the image flow vector [...] between the two frames is small enough for the {{differential}} equation of the optical flow to hold, which is often less than the <b>pixel</b> <b>spacing.</b> When the flow vector may exceed this limit, such as in stereo matching or warped document registration, the Lucas-Kanade method may still be used to refine some coarse estimate of the same, obtained by other means; for example, by extrapolating the flow vectors computed for previous frames, or by running the Lucas-Kanade algorithm on reduced-scale versions of the images. Indeed, the latter method {{is the basis of}} the popular Kanade-Lucas-Tomasi (KLT) feature matching algorithm.|$|E
50|$|The new LED {{scoreboard}} {{is built}} around four widescreen video displays that were {{the largest in the}} NHL until Bell Centre's upgrades two years later. Measuring 4.13 by 7.3 m they are capable of displaying images in 4.4 trillion colours. Their size combined with their 10 mm <b>pixel</b> <b>spacing</b> gives them an image that is, when viewed from the first row of the upper section at the red line, comparable to watching a 34 in television at 3.1 m. The corners hold 1.67 by 4.13 m displays with two ring displays each capping the top and bottom. The entire scoreboard weighs 22 t, 2% less than the one it replaced. The normally three-week assembly period was completed in only one week and as a result there were some minor technical difficulties during the first home game.|$|E
30|$|Assuming {{that the}} {{geometric}} position of each {{point in the}} track space corresponds to each pixel in the detected image, the track space is completely mapped to the <b>pixel</b> <b>space</b> of the detected image, assuming the camera is calibrated and normalized. The actual width of the rail can be expressed and calculated by the pixel number of the rail image.|$|R
30|$|Despite {{the high}} discriminative {{power of the}} {{extended}} features [11, 22, 25, 26] {{in comparison to the}} rotational LBP feature, the features are the histogram bins of a region. Therefore, in the classification stage, in order to transfer the image from the <b>pixel</b> <b>space</b> into the feature space to be classified, all histograms' regions LBP features have to be computed to obtain the regions' distributions.|$|R
40|$|In {{this paper}} we review some {{recently}} proposed stochastic algorithms which compute optimal probabilities in generating images compressed by IFS or IFSP systems ([12], [13]). We also propose new algorithms, called selection algorithms, which in principle compress an arbitrary image with grey levels on the <b>pixels</b> <b>space,</b> {{by means of}} an IFSP system that is somehow optimal for a given random generator of ane contraction maps...|$|R
40|$|Focused {{ion beam}} (FIB) systems {{are widely used}} as a {{versatile}} tool for nanofabrication prototyping, device modification and ion beam lithography. However, {{there are still many}} unexplored effects due to the different methods that ion implantation could perform during FIB milling using Ga as a liquid metal ion source. In this report we studied the effects of <b>pixel</b> <b>spacing</b> when FIB is used for direct milling of a substrate at different milling currents for constant implantation doses. The experiment consists of FIB milling of a Si substrate at 30 keV using currents of 50 pA and 100 pA for a dose of 5 × 10 16 ions/cm 2. The dwell time was set to be 1 μs and the <b>pixel</b> <b>spacing</b> varied from 6. 2 nm to 34. 2 nm. The surface topographies of machined regions were examined using the atomic force microscope and the quality is described by comparing the intensities of a crystal to amorphous peak of the recorded trace from Raman spectroscopy measurements. This method was introduced by Wagner. In order to more accurately consider the sputtering yield the effect of second order deposition was neglected. It was observed that by increasing the <b>pixel</b> <b>spacing</b> the sputtering yield starts to increase and then gradually decreases for both currents. Ion implantation breaks the crystal structure and the process involves displacement of the atoms from the atomic rows which consequently increases the effect of de-channeling in ion implantation. The increase in sputtering yield could be because of the enhanced de-channeling which is due to the changes of the substrate structure which increases the collisions between implanted ions and the substrate atoms. After a threshold, the sputtering yield gradually decreases, which is due to having less implanted ions per unit of volume for each scan and therefore less applied damage. <b>Pixel</b> <b>spacing</b> at different currents and dose rates can yield different behavior due to the concentration of implanted ions per pixel dwell time. In our study the maximum concentration of implanted ions per pixel dwell time is about 5 × 10 17 ions/cm 3 and 10 18 ions/cm 3 for currents of 50 pA and 100 pA respectively; this concentration is lower than 10 19 ions/cm 3 which is the saturation point of Ga solubility in Si. It was observed that increasing the <b>pixel</b> <b>spacing</b> leads to rougher surfaces. It was also found that the quality of Si is at its highest when the <b>pixel</b> <b>spacing</b> is 14. 8 nm. This is consistent with the topography results which were described by the de-channeling effect. As the de-channeling increases, the depth of implanted ions is decreased, and therefore fewer layers of substrate are damaged. In this study, we investigated the effect of FIB milling <b>pixel</b> <b>spacing</b> on substrate physical and structural changes at a dose of 5 × 10 16 ions/cm 2. We observed the sputtering yield is first increased and then decreased, which is mainly due to structural changes in substrate. The quality of substrate was also studied, revealing less damage when the <b>pixel</b> <b>spacing</b> is 14. 8 nm for both currents...|$|E
30|$|For each participant, MR images {{acquired}} with a three-dimensional, T 1 -weighted, magnetisation-prepared, rapid gradient echo (inversion recovery/gradient recalled) sequence on a 1.5 -T Symphony scanner (Siemens Healthineers, Erlangen, Germany) were available. The acquisition parameters were as follows: inversion time 820  ms; repetition time 1610  ms; echo time 2.38  ms; {{flip angle}} 15 °; {{field of view}} 250 × 203  mm; matrix 512 × 416; acquisition <b>pixel</b> <b>spacing</b> 1.0 × 1.0, reconstruction <b>pixel</b> <b>spacing</b> 0.49 × 0.49  mm; slice thickness 1  mm; spacing between slices 1  mm (no interslice gap); receiver bandwidth 220  Hz/pixel; number of slices 192; acquisition time 1.7 – 2.4  min; body transmit coil type.|$|E
40|$|In this letter, {{we carry}} out a {{comparative}} study of statistical models for multilook synthetic aperture radar amplitude images. Ten state-of-the-art statistical models are selected for comparison. To achieve a fair evaluation, we estimate all model parameters using the method of log-cumulants and apply the method to an image pyramid with varying <b>pixel</b> <b>spacing</b> (and resolution). The pyramid is created by different image product generation options. In addition to <b>pixel</b> <b>spacing</b> and resolution, we also consider the homogeneity of a scene for performance evaluation and we apply three performance measures. Through this study, it was found out that some models perform well for all resolutions, while the performance of other models depends heavily on the image content...|$|E
40|$|We analyse the 145 GHz {{temperature}} map produced from the 2003 flight of BOOMERanG {{in search for}} deviations from Gaussianity. We perform a <b>pixel</b> <b>space</b> analysis computing the map’s skewness, kurtosis and Minkowski functionals, {{as well as a}} Fourier space analysis computing the diagonal part the of angular bispectrum. The preliminary results presented here suggest that the data are fully consistent with the Gaussian hypothesis...|$|R
3000|$|... pixels patches {{computed}} over a grid with 8 <b>pixels</b> <b>spacing</b> {{were employed}} in building the visual vocabulary through K-means clustering. Although the spatial hierarchy we propose in this paper in some sense resembles the work in [10], it introduces a different scheme of splitting the image in the hierarchy, {{a different way to}} weight the contribution of each subregion, as well as a different similarity criterion between histograms.|$|R
5000|$|The Cineon 10 bits per <b>pixel</b> color <b>space</b> {{provides}} 1024 {{levels of}} color {{as opposed to}} 256 levels of color in 8 bits per <b>pixel</b> color <b>space.</b> 10 bit YUV and 10 bit RGB are the industry standard. The standard documented and recognized by the Society Of Motion Picture Television Engineers: SMPTE 259M, SMPTE 292M, SMPTE 296M, SMPTE 372M. A [...] file {{is a type of}} Cineon Graphics Data File format.|$|R
40|$|Abstract: The {{influence}} of pixel’s spatial characteristics on recognition of isolated Chinese character was investigated us-ing simulated prosthestic vision. The accuracy of Chinese character recognition with 4 kinds of pixel number (6 * 6, 8 * 8, 10 * 10, and 12 * 12 pixel array) and 3 kinds of pixel shape (Square, Dot and Gaussian) and different <b>pixel</b> <b>spacing</b> were tested through head-mounted display (HMD). A captured image of Chinese characters in font style of Hei were pixelized with Square, Dot and Gaussian pixel. Results showed that pixel number {{was the most}} important factor which could affect the recognition of isolated pixelized Chinese Chartars and the accuracy of recognition increased with the addition of pixel number. 10 * 10 pixel array could provide enough information for people to recognize an isolated Chinese character. At low resolution (6 * 6 and 8 * 8 pixel array), there were little difference of recognition accuracy between different pixel shape and different <b>pixel</b> <b>spacing.</b> While as for high resolution (10 * 10 and 12 * 12 pixel array), the fluctuation of pixel shape and <b>pixel</b> <b>spacing</b> could not affect the performance of recognition of isolated pixelized Chinese Character...|$|E
3000|$|..., need to {{be found}} in order to align the {{corresponding}} axis of the two reference frames. Imaging parameters such as focal length, primary and secondary rotation angles image <b>pixel</b> <b>spacing</b> and size are obtained from the accompanied DICOM image files. This allows us to have a close enough estimate of the C-arm pose for a particular viewing angle.|$|E
40|$|Large {{scale and}} {{operational}} applications based on contiguous coverage with SAR data require {{a set of}} dedicated techniques in terms of signal handling and product interpretation. The GRFM SAR map of the West African rainforest domain is derived from some 340 JERS- 1 SAR images acquired in a time span of 45 days. The original data with a <b>pixel</b> <b>spacing</b> of 12. 5 m are spatially compressed by wavelet transform resulting in products representing the estimated cross section as well as textural features at different spatial scales. The SAR mosaic is assembled by projecting the derived products into a compatible map projection. A tentative thematic interpretation is presented at a further reduced <b>pixel</b> <b>spacing.</b> A synoptic view on the vegetation gradients in West Africa is obtained and compared to the TREES map derived from NOAA/AVHRR and the sources of coincidence and divergence are discussed...|$|E
40|$|We {{present an}} {{analysis}} of the effects of beam deconvolution on noise properties in CMB measurements. The analysis is built around the artDeco beam deconvolver code. We derive a low-resolution noise covariance matrix that describes the residual noise in deconvolution products, both in harmonic and <b>pixel</b> <b>space.</b> The matrix models the residual correlated noise that remains in time-ordered data after destriping, and the effect of deconvolution on it. To validate the results, we generate noise simulations that mimic the data from the Planck LFI instrument. A χ^ 2 test for the full 70 GHz covariance in multipole range ℓ= 0 - 50 yields a mean reduced χ^ 2 of 1. 0037. We compare two destriping options, full and independent destriping, when deconvolving subsets of available data. Full destriping leaves substantially less residual noise, but leaves data sets intercorrelated. We derive also a white noise covariance matrix that provides an approximation of the full noise at high multipoles, and study the properties on high-resolution noise in <b>pixel</b> <b>space</b> through simulations. Comment: 22 pages, 25 figure...|$|R
40|$|Current {{approaches}} in video forecasting attempt to generate videos directly in <b>pixel</b> <b>space</b> using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all {{the structure and}} scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is to model the forecasting problem {{at a higher level}} of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene [...] -humans [...] -and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in <b>pixel</b> <b>space.</b> By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction. Comment: Project Website: [URL]...|$|R
40|$|We {{present a}} {{stochastic}} algorithm which generates optimal probabilities for the Chaos-Game to decompress an image {{represented by the}} xed point of an IFS operator. The algorithm {{can be seen as}} a sort of time inhomogeneous rigenerative process. We prove that optimal probabilities exist and, by martingale methods, that the algorithm converges almost surely. The method holds for IFS operators associated to any arbitrary number of possibly overlapping ane contraction maps on the <b>pixels</b> <b>space...</b>|$|R
30|$|Image based methods (microscopic model) perform {{normally}} {{better for}} a higher resolution (less than 30  cm <b>pixel</b> <b>spacing</b> [10]), thus the aircraft flight height should be low or equivalently one should {{take into account the}} reduced image coverage. It seems that the proposed model based method should not be so sensitive to the resolution because it is working on the macroscopic model level.|$|E
30|$|Conversely, {{in images}} where both spatial {{resolution}} and RC were too low, {{we chose to}} use more iterations of the algorithm {{in order to enhance}} the spatial resolution of the images and to apply a Gaussian (FWHM between 2 and 4  mm) post-reconstruction smoothing filter to the images. The <b>pixel</b> <b>spacing</b> was between 1 and 3  mm in all optimized images.|$|E
40|$|The main {{objective}} of the proposed research is to evaluate the invariance of the cooccurrence texture model with respect to patient size in Computed Tomography (CT) data. Since patients ’ scans can have high variation in <b>pixel</b> <b>spacing,</b> in order to standardize all patients’ texture descriptors, we investigate the effect of four interpolation techniques in reducing the <b>pixel</b> <b>spacing</b> variance: nearest neighbor, bilinear, cubic and B-spline methods. After applying interpolation, the co-occurrence texture model and nine Haralick texture descriptors are calculated to quantify the texture appearance of the soft tissues. The differences in the texture are evaluated before and after interpolation using Analysis of Variance (ANOVA) and the General Linear Model (GLM). Our preliminary results on liver images from five different patients show that the co-occurrence texture model is not affected by the difference in pixel spacing; therefore, computer-aided segmentation, retrieval and diagnosis tools {{that are based on}} co-occurrence texture models are expected to work on multiple patients of different sizes. ...|$|E
50|$|James was {{inspired}} after seeing Alex Tew's {{success with the}} Million Dollar Homepage and {{wanted to see if}} he could take the pixel phenomenon a step further by integrating a pixel gird directly under the search box of a search engine. Hoping that the search engine would generate recurring visitors and therefore making the <b>pixel</b> <b>space</b> on the homepage valuable. Within the first 24 hours over 5000 pixels at £1 each were sold.|$|R
50|$|The cosmic {{microwave}} background (CMB) is a homogeneous and isotropic random field, and its covariance is therefore diagonal in a spherical harmonics basis. Any given observation of the CMB will be noisy, with the noise typically having different statistical properties than the CMB. It could for example be uncorrelated in <b>pixel</b> <b>space.</b> The generalized Wiener filter exploits this difference in behavior to isolate {{as much as possible}} of the signal from the noise.|$|R
40|$|The {{ability to}} {{automatically}} detect {{certain types of}} cells or cellular subunits in microscopy images is of significant interest {{to a wide range}} of biomedical research and clinical practices. Cell detection methods have evolved from employing hand-crafted features to deep learning-based techniques to locate target cells. The essential idea of these methods is that their cell classifiers or detectors are trained in the <b>pixel</b> <b>space,</b> where the locations of target cells are labeled. In this paper, we seek a different route and propose a convolutional neural network (CNN) -based cell detection method that uses encoding of the output <b>pixel</b> <b>space.</b> For the cell detection problem, the output space is the sparsely labeled pixel locations indicating cell centers. Consequently, we employ random projections to encode the output space to a compressed vector of fixed dimension. Then, CNN regresses this compressed vector from the input pixels. Using $L_ 1 $-norm optimization, we recover sparse cell locations on the output <b>pixel</b> <b>space</b> from the predicted compressed vector. In the past, output space encoding using compressed sensing (CS) has been used in conjunction with linear and non-linear predictors. To the best of our knowledge, this is the first successful use of CNN with CS-based output space encoding. We experimentally demonstrate that proposed CNN + CS framework (referred to as CNNCS) exceeds the accuracy of the state-of-the-art methods on many benchmark datasets for microscopy cell detection. Additionally, we show that CNNCS can exploit ensemble average by using more than one random encodings of the output space. In the AMIDA 13 MICCAI grand competition, we achieve the 3 rd highest F 1 -score in all the 17 participated teams. More ranking details are available at [URL] Implementation of CNNCS is available at [URL]...|$|R
