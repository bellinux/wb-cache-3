4677|2726|Public
25|$|The {{marginal}} <b>posterior</b> <b>distribution</b> of {{the unknown}} mean of a normally distributed variable, with unknown prior mean and variance following the above model.|$|E
25|$|In other words, the <b>posterior</b> <b>{{distribution}}</b> has {{the form}} of a product of a normal distribution over p(μ|σ2) times an inverse gamma distribution over p(σ2), with parameters that are the same as the update equations above.|$|E
25|$|In statistics, the {{t-distribution}} {{was first}} derived as a <b>posterior</b> <b>distribution</b> in 1876 by Helmert and Lüroth. The t-distribution {{also appeared in}} a more general form as Pearson Type IV distribution in Karl Pearson's 1895 paper.|$|E
40|$|An optimal {{adaptive}} design for test-item calibration based on Bayesian optimality criteria is presented. The design adapts {{the choice of}} field-test items to the examinees taking an operational adaptive test using both {{the information in the}} <b>posterior</b> <b>distributions</b> of their ability parameters and the current <b>posterior</b> <b>distributions</b> of the field-test parameters. Different criteria of optimality based on the two types of <b>posterior</b> <b>distributions</b> are possible. The design can be implemented using an MCMC scheme with alternating stages of sampling from the <b>posterior</b> <b>distributions</b> of the test takers’ ability parameters and the parameters of the field-test items while reusing samples from earlier <b>posterior</b> <b>distributions</b> of the other parameters. Results from a simulation study demonstrated the feasibility of the proposed MCMC implementation for operational item calibration. A comparison of performances for different optimality criteria showed faster calibration of substantial numbers of items for the criterion of D-optimality relative to A-optimality, a special case of c-optimality, and random assignment of items to the test takers...|$|R
40|$|Bayesian {{procedures}} for specification analysis or diagnostic checking of modeling assumptions for structural equations of econometric models are developed and applied using Monte Carlo numerical methods. Checks on {{the validity of}} identifying restrictions, exogeneity assumptions and other specifying assumptions are performed using <b>posterior</b> <b>distributions</b> for discrepancy vectors and functions representing departures from specifying assumptions. Several mappings or functions of reduced form coefficients are defined and their <b>posterior</b> <b>distributions</b> are computed. A restricted reduced form approach is used to compute <b>posterior</b> <b>distributions</b> for structural parameters. These procedures are applied in analyses of two econometric models...|$|R
40|$|This paper {{presents}} Carnegie Mellon University’s {{experiments on}} the mixed named-page and homepage finding {{task of the}} TREC 12 Web Track. Our results were strong; we achieved the success using language models estimated from combining information from document text, in-link text, and information present {{in the structure of}} the documents. We also present experiments using expectations about <b>posterior</b> <b>distributions</b> to create class-based prior probabilities. We find that priors do provide a large gain for our official runs, but we do further experiments that show the priors do not always help. Some preliminary analysis shows that the prior probabilities are not providing the desired <b>posterior</b> <b>distributions.</b> In cases where applying the priors harm performance, the observed <b>posterior</b> <b>distributions</b> in the rankings are far off of the desired <b>posterior</b> <b>distributions.</b> 1...|$|R
25|$|In signal {{processing}} and Bayesian inference, particle filters and sequential Monte Carlo techniques are {{a class of}} mean field particle methods for sampling and computing the <b>posterior</b> <b>distribution</b> of a signal process given some noisy and partial observations using interacting empirical measures.|$|E
25|$|A {{more fully}} Bayesian {{approach}} to parameters {{is to treat}} parameters as additional unobserved variables and to compute a full <b>posterior</b> <b>distribution</b> over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, so in practice classical parameter-setting approaches are more common.|$|E
25|$|Methods for the {{identification}} of summary statistics that could also simultaneously assess the influence on the approximation of the posterior would be of substantial value. This is because the choice of summary statistics and the choice of tolerance constitute two sources of error in the resulting <b>posterior</b> <b>distribution.</b> These errors may corrupt the ranking of models and may also lead to incorrect model predictions. Indeed, none of the methods above assesses the choice of summaries {{for the purpose of}} model selection.|$|E
40|$|Abstract. The {{main object}} of Bayesian {{statistical}} inference is {{the determination of}} <b>posterior</b> <b>distributions.</b> Sometimes these laws are given for quantities devoid of empirical value. This serious drawback vanishes when one confines oneself to considering a finite horizon framework. However, assuming infinite exchangeability gives rise to fairly tractable a posteriori quantities, which is very attractive in applications. Hence, {{with a view to}} a reconciliation between these two aspects of the Bayesian way of reasoning, in this paper we provide quantitative comparisons between <b>posterior</b> <b>distributions</b> of finitary parameters and <b>posterior</b> <b>distributions</b> of allied parameters appearing in usual statistical models. 1...|$|R
40|$|In Bayesian theory, calculating a <b>posterior</b> {{probability}} <b>distribution</b> {{is highly}} important but usually difficult. Therefore, some {{methods have been}} put forward {{to deal with such}} problem, among which, the most popular one is the asymptotic expansions for <b>posterior</b> <b>distributions.</b> In this paper, we propose an alternative method, named random weighting method, for scaled <b>posterior</b> <b>distributions,</b> and give an ideal convergence speed, which serves as the theoretical guarantee for methods of numerical simulations. Comment: 17 pages, 0 figur...|$|R
40|$|We {{introduce}} a semi-parametric Bayesian approach based on skewed Dirichlet processes priors for location parameters in the ordinal calibration problem. This approach allows the modeling of asymmetrical error <b>distributions.</b> Conditional <b>posterior</b> <b>distributions</b> are implemented, thus allowing {{the use of}} Markov chains Monte Carlo to generate the <b>posterior</b> <b>distributions.</b> The methodology is applied to both simulated and real data...|$|R
25|$|Because a Bayesian {{network is}} a {{complete}} model for the variables and their relationships, {{it can be used}} to answer probabilistic queries about them. For example, the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the <b>posterior</b> <b>distribution</b> of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when one wants to choose values for the variable subset which minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.|$|E
25|$|Besides {{parameter}} estimation, the ABC-framework {{can be used}} {{to compute}} the posterior probabilities of different candidate models. In such applications, one possibility is to use the rejection-sampling in a hierarchical manner. First, a model is sampled from the prior distribution for the models; then, given the model sampled, the model parameters are sampled from the prior distribution assigned to that model. Finally, a simulation is performed as in the single-model ABC. The relative acceptance frequencies for the different models now approximate the <b>posterior</b> <b>distribution</b> for these models. Again, computational improvements for ABC in the space of models have been proposed, such as constructing a particle filter in the joint space of models and parameters.|$|E
25|$|Bayes {{methods are}} more cost {{effective}} than the traditional frequentist take on marketing research and subsequent decision making. The probability can be assessed from a degree of belief before and after accounting for evidence, instead of calculating the probabilities of a certain decision by carrying out {{a large number of}} trials with each one producing an outcome from a set of possible outcomes. The planning and implementation of trials to see how a decision impacts in the ‘field’ e.g. observing consumers reaction to a relabeling of a product, is time consuming and costly, a method many firms cannot afford. In place of taking the frequentist route in aiming for a universally acceptable conclusion through iteration, it is sometimes more effective to take advantage of all the information available to the firm to work out the ‘best’ decision at the time, and then subsequently when new knowledge is obtained, revise the <b>posterior</b> <b>distribution</b> to be then used as the prior, thus the inferences continue to logically contribute to one another based on Bayes theorem.|$|E
40|$|AbstractWe study {{consistency}} and asymptotic normality of <b>posterior</b> <b>distributions</b> {{of the natural}} parameter for an exponential family when the dimension of the parameter grows with the sample size. Under certain growth restrictions on the dimension, we show that the <b>posterior</b> <b>distributions</b> concentrate in neighbourhoods of the true parameter and can be approximated by an appropriate normal distribution...|$|R
40|$|We {{consider}} {{a sequence of}} <b>posterior</b> <b>distributions</b> based on a data-dependent prior (which we shall refer to as a pseudoposterior distribution) and establish simple conditions under which the sequence is Hellinger consistent. It is shown how investigations into these pseudo posteriors assist with the understanding of some true <b>posterior</b> <b>distributions,</b> including Pólya trees, the infinite dimensional exponential family and mixture models...|$|R
40|$|The {{likelihood}} function plays {{a central role}} in the theory of higher-order asymptotics both for Bayesian and frequentist inference. This theory provides accurate approximations to <b>posterior</b> <b>distributions</b> and to related quantities, even for small sample sizes. Moreover, these approximations give rise to a simple simulation scheme, alternative to MCMC, for Bayesian computation of marginal <b>posterior</b> <b>distributions</b> for a scalar parameter.|$|R
2500|$|The <b>posterior</b> <b>distribution</b> can {{be found}} by {{updating}} the parameters as follows: ...|$|E
2500|$|In Bayesian {{statistics}} the Kullback–Leibler divergence {{can be used}} as {{a measure}} of the information gain in moving from a prior distribution to a posterior distribution: [...] [...] If some new fact [...] is discovered, it can be used to update the <b>posterior</b> <b>distribution</b> for [...] from [...] to a new <b>posterior</b> <b>distribution</b> [...] using Bayes' theorem: ...|$|E
2500|$|Step 5: The <b>posterior</b> <b>distribution</b> is {{approximated}} {{with the}} accepted parameter points. The <b>posterior</b> <b>distribution</b> {{should have a}} non-negligible probability for parameter values in a region around the true value of [...] in the system, if the data are sufficiently informative. In this example, the posterior probability mass is evenly split between the values 0.08 and 0.43.|$|E
40|$|It is not unusual, when {{considering}} conditionally independent hierarchical models (ClliM's) within a Bayesian framework, to assign improper prior distributions to {{the parameters of}} the second stage. Unfortunately, the technical difficulties which lead to the use of Markov chain Monte Carlo techniques (e. g. the Gibbs sampler) for sampling from the resulting <b>posterior</b> <b>distributions</b> also cause problems in establishing the integrability of these <b>posterior</b> <b>distributions.</b> In this work, we give conditions which guarantee the integrability of the resulting <b>posterior</b> <b>distributions</b> for some frequently used CIHM's. These CIHM's include the linear mixed model and those in which the first and second stage distributions can be written as a one-parameter exponential family and its two-parameter conjugate exponential family, respectively...|$|R
40|$|AbstractIn this paper, the {{asymptotic}} behavior of <b>posterior</b> <b>distributions</b> on parameters contained in random processes is examined when the specified {{model for the}} densities is not necessarily correct. Uniform convergence of likelihood functions in some way is {{shown to be a}} sufficient condition for the <b>posterior</b> <b>distributions</b> to be asymptotically confined to a set (Theorem 1). For ergodic stationary Markov processes uniform convergence of likelihood functions is established by the ergodic theorem for Banach-valued stationary processes (Proposition 1). A sufficient condition for the uniform convergence is also shown for general random processes (Proposition 2). These results are used to analyze the {{asymptotic behavior}} of <b>posterior</b> <b>distributions</b> on parameters contained in linear systems under incorrect models (Example 1 and 2) ...|$|R
30|$|The {{following}} proposition establishes {{that the}} two reference priors that we have obtained yield proper <b>posterior</b> <b>distributions.</b>|$|R
2500|$|... {{which shows}} that the mean ± {{standard}} deviation estimate of the <b>posterior</b> <b>distribution</b> for θ is ...|$|E
2500|$|Then, {{given the}} same sample of n {{measured}} values k'i as before, and a prior of Gamma(α, β), the <b>posterior</b> <b>distribution</b> is ...|$|E
2500|$|Empirical Bayes {{methods for}} ancestral {{reconstruction}} require the investigator {{to assume that}} the evolutionary model parameters and tree are known without error. [...] When the size or complexity of the data makes this an unrealistic assumption, it may be more prudent to adopt the fully hierarchical Bayesian approach and infer the joint <b>posterior</b> <b>distribution</b> over the ancestral character states, model, and tree. Huelsenbeck and Bollback first proposed a hierarchical Bayes method to ancestral reconstruction by using Markov chain Monte Carlo (MCMC) methods to sample ancestral sequences from this joint <b>posterior</b> <b>distribution.</b> A similar approach was also used to reconstruct the evolution of symbiosis with algae in fungal species (lichenization). For example, the Metropolis-Hastings algorithm for MCMC explores the joint <b>posterior</b> <b>distribution</b> by accepting or rejecting parameter assignments {{on the basis of the}} ratio of posterior probabilities.|$|E
3000|$|... {{since the}} {{conditional}} <b>posterior</b> <b>distributions</b> for sampling these two hyperparameters {{are similar to}} those for sampling of [...]...|$|R
40|$|A Bayesian {{model for}} {{predicting}} schools' performance in SATs is proposed, in which geographical explanatory variables are averaged over a radius centred on each school. The {{coefficients for the}} explanatory variables, and the averaging (or smoothing) radius are treated as unknown variables, whose prior and <b>posterior</b> <b>distributions</b> will {{be considered in the}} Bayesian analysis. <b>Posterior</b> <b>distributions</b> cannot be determined analytically, and will be investigated using simulation via Gibbs sampling...|$|R
3000|$|... 2,λ,ν,γ and τ in (3)), {{both the}} {{conditional}} marginal filtering and smoothing <b>posterior</b> <b>distributions</b> of the states, p(x [...]...|$|R
2500|$|To {{provide a}} random sample from the <b>posterior</b> <b>distribution</b> in Bayesian inference. This sample then approximates and {{summarizes}} all the essential [...] features of the posterior.|$|E
2500|$|Bayesian linear {{regression}} applies {{the framework of}} Bayesian statistics to {{linear regression}}. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β {{are assumed to be}} random variables with a specified prior distribution. [...] The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression. [...] In addition, the Bayesian estimation process produces not a single point estimate for the [...] "best" [...] values of the regression coefficients but an entire <b>posterior</b> <b>distribution,</b> completely describing the uncertainty surrounding the quantity. [...] This can be used to estimate the [...] "best" [...] coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the <b>posterior</b> <b>distribution.</b>|$|E
2500|$|Some care {{is needed}} when {{choosing}} priors in a hierarchical model, particularly on scale variables {{at higher levels}} of the hierarchy such as the variable [...] in the example. The usual priors such as the Jeffreys prior often do not work, because the <b>posterior</b> <b>distribution</b> will be improper (not normalizable), and estimates made by minimizing the expected loss will be inadmissible.|$|E
40|$|The goal of {{this paper}} is to provide theorems on {{convergence}} rates of <b>posterior</b> <b>distributions</b> that can be applied to obtain good convergence rates in the context of density estimation as well as regression. We show how to choose priors so that the <b>posterior</b> <b>distributions</b> converge at the optimal rate without prior knowledge of the degree of smoothness of the density function or the regression function to be estimated. 1. Introduction. Bayesia...|$|R
50|$|Because {{the prior}} and <b>posterior</b> <b>{{distribution}}s</b> {{are the same}} family, we say the inverse Wishart distribution is conjugate to the multivariate Gaussian.|$|R
40|$|In this article, we {{consider}} {{a family of}} uniform distributions as a statistical model. Assuming that the prior distribution has a smooth, positive density on the parameter space, we prove the large deviation principle of the <b>posterior</b> <b>distributions.</b> We derive the rate functions explicitly. To this end, we apply the Gartner-Ellis theorem to the <b>posterior</b> <b>distributions</b> of the reciprocal of the parameter. 長崎大学経済学部創立 110 周年記念論文集Essays in Commemoration of the 110 th Anniversary of the Faculty of Economics, Nagasaki Universit...|$|R
