6239|10000|Public
25|$|Dimensional reduction: Analysts often {{reduce the}} number of {{dimensions}} (genes) prior to data analysis. This may involve linear approaches such as <b>principal</b> <b>components</b> <b>analysis</b> (PCA), or non-linear manifold learning (distance metric learning) using kernel PCA, diffusion maps, Laplacian eigenmaps, local linear embedding, locally preserving projections, and Sammon's mapping.|$|E
25|$|With the {{development}} of statistical techniques and numerical taxonomy in the 1960s, mathematical methods (including Cluster analysis, <b>Principal</b> <b>components</b> <b>analysis,</b> correspondence analysis and Factor analysis) {{have been used to}} build typologies. These techniques provide a qualitative way to articulate the degrees of consistency among particular attributes. Correlation coefficients created by these methods help archaeologists discern between meaningful and useless similarities between artefacts. During the 1990s archaeologists began to use phylogenetic methods borrowed from Cladistics.|$|E
25|$|A study {{conducted}} by the HUGO Pan-Asian SNP Consortium in 2009 used <b>principal</b> <b>components</b> <b>analysis,</b> which makes no prior population assumptions, on genetic data sampled from a large number of points across Asia. They said that East Asian and South-East Asian populations clustered together, and suggested a common origin for these populations. At the same time they observed a broad discontinuity between this cluster and South Asia, commenting most of the Indian populations showed evidence of shared ancestry with European populations. The study said that genetic ancestry is strongly correlated with linguistic affiliations as well as geography.|$|E
40|$|<b>Principal</b> <b>component</b> <b>analysis</b> {{is one of}} {{the most}} {{important}} and powerful methods in chemometrics as well as in a wealth of other areas. This paper provides a description of how to understand, use, and interpret <b>principal</b> <b>component</b> <b>analysis.</b> The paper focuses on the use of <b>principal</b> <b>component</b> <b>analysis</b> in typical chemometric areas but the results are generally applicable...|$|R
40|$|Face {{recognition}} {{has been}} a very valuable research to pattern recognition and face recognition systems. These years, two-dimensional <b>principal</b> <b>component</b> <b>analysis</b> and kernel <b>principal</b> <b>component</b> <b>analysis</b> have been successfully applied in face recognition systems. However, there is still some space for us to make it better. This study has proposed a novel approach based on Two-dimensional <b>Principal</b> <b>Component</b> <b>Analysis</b> (2 DPCA) and Kernel <b>Principal</b> <b>Component</b> <b>Analysis</b> (KPCA) for face recognition. The proposed approach first performs two-dimensional <b>principal</b> <b>component</b> <b>analysis</b> process to project the faces onto the feature pace and then performs kernel <b>principal</b> <b>component</b> <b>analysis</b> on the projected data. And finally, one nearest neighbor classifier based on Euclidean distance is used for recognizing faces. The experiments on ORL face database, Yale face database and FERET face database show that the proposed approach gives a high recognition rate of 100 % and outperforms state-of-the-art approaches and demonstrates promising applications...|$|R
50|$|Robust <b>principal</b> <b>component</b> <b>analysis</b> (RPCA) is a {{modification}} of the widely used statistical procedure <b>principal</b> <b>component</b> <b>analysis</b> (PCA) which works well with respect to grossly corrupted observations.|$|R
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated <b>principal</b> <b>components</b> <b>analysis</b> of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called <b>principal</b> <b>components</b> <b>analysis</b> (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
2500|$|... data {{transformation}} (incorporating aspects such as data normalization and data analysis) – for example <b>principal</b> <b>components</b> <b>analysis</b> dimensionality reduction, mean calculation ...|$|E
40|$|Probabilistic <b>Principal</b> <b>Component</b> <b>Analysis</b> is a {{reformulation}} of {{the common}} multivariate analysis technique known as <b>Principal</b> <b>Component</b> <b>Analysis.</b> It employs a latent variable model framework similar to factor analysis allowing to establish a maximum likelihood solution for the parameters that comprise the model. One of the main assumptions of Probabilistic <b>Principal</b> <b>Component</b> <b>Analysis</b> is that observed data is independent and identically distributed. This assumption is inadequate for many applications, in particular, for modeling sequential data. In this paper, the authors introduce a temporal version of Probabilistic <b>Principal</b> <b>Component</b> <b>Analysis</b> by using a hidden Markov model {{in order to obtain}} optimized representations of observed data through time. Combining Probabilistic <b>Principal</b> <b>Component</b> Analyzers with a hidden Markov model, it is possible to enhance the capabilities of transformation and reduction of time series vectors. In order to find automatically the dimensionality of the principal subspace associated with these Probabilistic <b>Principal</b> <b>Component</b> Analyzers through time, a Bayesian treatment of the <b>Principal</b> <b>Component</b> model is introduced as well. Keywords: Hidden Markov models, <b>principal</b> <b>component</b> <b>analysis,</b> bayesian <b>principal</b> <b>component</b> <b>analysis,</b> EM algorithm, model selection 1...|$|R
40|$|Vertices <b>Principal</b> <b>Component</b> <b>Analysis</b> (V-PCA) and Centers <b>Principal</b> <b>Component</b> <b>Analysis</b> (C-PCA) are {{variants}} of <b>Principal</b> <b>Component</b> <b>Analysis</b> (PCA) {{to deal with}} two-way interval-valued data. In this case the observation units are represented as hyperrectangles instead of points. Tucker 3 and CANDECOMP/PARAFAC are <b>component</b> <b>analysis</b> techniques to analyze the underlying structure of three-way data sets. In the present paper, after recalling the above mentioned methods, we extend the C-PCA and V-PCA methods to deal with three-way interval-valued data by means of Tucker 3 and CANDECOMP/PARAFAC and we describe how to represent the observation units in the obtained low-dimensional space. Furthermore, an application of the extended methods-called Three-way Vertices <b>Principal</b> <b>Component</b> <b>Analysis</b> (3 V-PCA) and Three-way Centers <b>Principal</b> <b>Component</b> <b>Analysis</b> (3 C-PCA) -to three-way interval-valued air pollution data is described. Copyright (C) 2004 John Wiley Sons, Ltd...|$|R
40|$|To {{overcome}} {{the shortcomings of}} traditional dimensionality reduction algorithms, incremental tensor <b>principal</b> <b>component</b> <b>analysis</b> (ITPCA) based on updated-SVD technique algorithm is proposed in this paper. This paper proves the relationship between PCA, 2 DPCA, MPCA, and the graph embedding framework theoretically and derives the incremental learning procedure to add single sample and multiple samples in detail. The experiments on handwritten digit recognition have demonstrated that ITPCA has achieved better recognition performance than that of vector-based <b>principal</b> <b>component</b> <b>analysis</b> (PCA), incremental <b>principal</b> <b>component</b> <b>analysis</b> (IPCA), and multilinear <b>principal</b> <b>component</b> <b>analysis</b> (MPCA) algorithms. At the same time, ITPCA also has lower time and space complexity...|$|R
5000|$|Exploratory factor {{analysis}} versus <b>principal</b> <b>components</b> <b>analysis</b> ...|$|E
5000|$|... #Subtitle level 2: Exploratory factor {{analysis}} versus <b>principal</b> <b>components</b> <b>analysis</b> ...|$|E
50|$|This decorrelation {{is related}} to <b>principal</b> <b>components</b> <b>analysis</b> for multivariate data.|$|E
40|$|Parties are {{the main}} {{vehicles}} of representation in modern, democratic societies. Party systems, that is the number {{and the size of}} all the parties within a country, can vary greatly across countries. There is an ongoing debate in the political science literature about the appropriate way to reduce the dimensionality of the cross-country party system data for comparative purposes. This thesis reviews that literature and offers a new solution: <b>Principal</b> <b>Component</b> <b>Analysis</b> to find the most important information in the data matrix. I use data from 17 advanced democracies from 1970 - 2013. I conduct analyses using various related methods (<b>Principal</b> <b>Component</b> <b>Analysis,</b> <b>Principal</b> <b>Component</b> <b>Analysis</b> on the Residuals, kernel <b>Principal</b> <b>Component</b> <b>Analysis,</b> Non-Linear <b>Principal</b> <b>Component</b> <b>Analysis,</b> <b>Principal</b> <b>Component</b> <b>Analysis</b> on log-ratio transformed variables and <b>Principal</b> <b>component</b> <b>Analysis</b> on non-centered variables). I find that the most important differences across countries are: “the size of the biggest two parties”, “competition between the two biggest parties”, “existence of a third party” and “balanced multipartism. ” I argue that most of the current political science literature uses summary measures that are only correlated with the first of those four dimensions. I suggest a strategy for incorporating a measure of the second dimension that relies on indices of opposition structure...|$|R
40|$|This thesis {{deals with}} using <b>principal</b> <b>component</b> <b>analysis</b> in {{cryptanalysis}} by power side chanel. At first {{in this thesis}} is discussed cryptanalysis, cryptanalysis by power side chanel, <b>principal</b> <b>component</b> <b>analysis</b> method and interpretation received power consumption from performed differential power analysis on cryptographic device with AES algorithm. Practical part contain execution of own <b>principal</b> <b>component</b> <b>analysis</b> on received data and following try of differential power analysis thus adjusted data...|$|R
30|$|Here, {{grouping}} of the parameters into components {{at this stage}} is very difficult. Hence, in the next step, the <b>principal</b> <b>component</b> <b>analysis</b> has been applied. The correlation matrix is subjected to the <b>principal</b> <b>component</b> <b>analysis.</b>|$|R
5000|$|... in <b>principal</b> <b>components</b> <b>analysis</b> (PCA) {{when there}} are many {{supplementary}} variables; ...|$|E
5000|$|Distinctiveness of {{descriptors}} {{is measured}} by summing the eigenvalues of the descriptors, obtained by the <b>Principal</b> <b>components</b> <b>analysis</b> of the descriptors normalized by their variance. This corresponds {{to the amount of}} variance captured by different descriptors, therefore, to their distinctiveness. PCA-SIFT (<b>Principal</b> <b>Components</b> <b>Analysis</b> applied to SIFT descriptors), GLOH and SIFT features give the highest values.|$|E
5000|$|The {{differences}} between <b>principal</b> <b>components</b> <b>analysis</b> and factor analysis are further illustrated by Suhr (2009): ...|$|E
40|$|AbstractScreening out {{sensitivity}} prediction indexes fit for {{the coal}} mine is very important technical work in coal and gas outburst prediction process. The feasibility that <b>principal</b> <b>component</b> <b>analysis</b> method {{been used in the}} coal and gas outburst prediction index screening is discussed. Through using gray correlation <b>analysis</b> method and <b>principal</b> <b>component</b> <b>analysis</b> method to analysis a batch of measured data of a coal mine, the validity of the latter is proved. Through the contrast of process between the gray correlation <b>analysis</b> and the <b>principal</b> <b>component</b> <b>analysis</b> method, the vantage of <b>principal</b> <b>component</b> <b>analysis</b> method is obvious, that is the better maneuverability, simplicity and easier to extend...|$|R
40|$|A {{new process}} {{monitoring}} approach is proposed for handling the nonlinear monitoring {{problem in the}} electrofused magnesia furnace (EFMF). Compared to conventional method, the contributions are as follows: (1) a new kernel <b>principal</b> <b>component</b> <b>analysis</b> is proposed based on loss function in the feature space; (2) the model of kernel <b>principal</b> <b>component</b> <b>analysis</b> based on forgetting factor is updated; (3) a new iterative kernel <b>principal</b> <b>component</b> <b>analysis</b> algorithm is proposed based on penalty factor...|$|R
5000|$|<b>Principal</b> <b>component</b> <b>analysis</b> (PCA) and <b>principal</b> <b>component</b> {{regression}} ...|$|R
50|$|<b>Principal</b> <b>components</b> <b>analysis</b> (PCA) {{and similar}} methods for {{dimension}} reduction are {{not based on}} the sufficiency principle.|$|E
5000|$|Multivariate analysis: Cluster analysis, <b>Principal</b> <b>components</b> <b>analysis,</b> Linear {{discriminant}} analysis, canonical analysis, Multidimensional scaling, Canonical {{correlation analysis}} ...|$|E
50|$|Hebbian {{learning}} {{is performed using}} conditional <b>principal</b> <b>components</b> <b>analysis</b> (CPCA) algorithm with correction factor for sparse expected activity levels.|$|E
30|$|It is very {{difficult}} at this stage to group the parameters into components and attach any physical significance. Hence, in the next, the <b>principal</b> <b>component</b> <b>analysis</b> has been applied. The correlation matrix is subjected to the <b>principal</b> <b>component</b> <b>analysis.</b>|$|R
40|$|International audienceThe {{available}} {{methods to}} handle missing values in <b>principal</b> <b>component</b> <b>analysis</b> only provide point {{estimates of the}} parameters (axes and components) and estimates of the missing values. To {{take into account the}} variability due to missing values a multiple imputation method is proposed. First a method to generate multiple imputed data sets from a <b>principal</b> <b>component</b> <b>analysis</b> model is defined. Then, two ways to visualize the uncertainty due to missing values onto the <b>principal</b> <b>component</b> <b>analysis</b> results are described. The first one consists in projecting the imputed data sets onto a reference configuration as supplementary elements to assess the stability of the individuals (respectively of the variables). The second one consists in performing a <b>principal</b> <b>component</b> <b>analysis</b> on each imputed data set and fitting each obtained configuration onto the reference one with Procrustes rotation. The latter strategy allows to assess the variability of the <b>principal</b> <b>component</b> <b>analysis</b> parameters induced by the missing values. The methodology is then evaluated from a real data set...|$|R
40|$|Abstract — In this paper, {{we propose}} a new {{approach}} of fault detection and diagnosis combining a Neural Nonlinear <b>Principal</b> <b>Component</b> <b>Analysis</b> (NNLPCA) and Partial Least Square (PLS). We have made a comparative study between the Linear <b>Principal</b> <b>Component</b> <b>Analysis</b> (LPCA) and Nonlinear <b>Principal</b> <b>Component</b> <b>Analysis</b> (NLPCA) to monitor a manufacturing process. This study has shown the capability of NLPCA in explaining nonlinear correlations in the process data. The traditional LPCA is limited to complex nonlinear systems; therefore, an adaptive NLPCA based on an improved training auto-associative neural network is presented. The proposed approach is applied to fault detection of a manufacturing process. The performance of the proposed approach is then illustrated and compared to those of classic LPCA. Keywords — Neural Nonlinear <b>Principal</b> <b>Component</b> <b>Analysis...</b>|$|R
5000|$|... data {{transformation}} (incorporating aspects such as data normalization and data analysis) - for example <b>principal</b> <b>components</b> <b>analysis</b> dimensionality reduction, mean calculation ...|$|E
5000|$|NAG Library - <b>Principal</b> <b>components</b> <b>analysis</b> is {{implemented}} via the [...] routine (available {{in both the}} Fortran versions of the Library).|$|E
5000|$|Fabrigar et al. (1999) {{address a}} number of reasons used to suggest that <b>principal</b> <b>components</b> <b>analysis</b> is not {{equivalent}} to factor analysis: ...|$|E
40|$|Abstract—Several linear {{transforms}} with constructions {{more general}} {{than that of}} <b>principal</b> <b>component</b> <b>analysis</b> are considered for spectral decorrelation in the compression of hyperspectral imagery. Specifically, orthogonal nonnegative matrix factorization, generalized <b>principal</b> <b>component</b> <b>analysis,</b> and <b>principal</b> <b>component</b> <b>analysis</b> coupled with explicit segmentation based on spectral angle mapping are considered. These spectraldecorrelation techniques are employed in conjunction with wavelet-based spatial decorrelation for hyperspectral compression using a 3 D version of the well-known SPIHT algorithm. A shape-adaptive wavelet transform and shape-adaptive SPIHT coder {{are used in the}} case of the latter two spectral-decorrelation techniques which segment the hyperspectral dataset into multiple distinct pixel classes. Experimental results reveal that, despite their general formulation, the proposed techniques fail to offer spectral-decorrelation performance superior to that of traditional <b>principal</b> <b>component</b> <b>analysis.</b> I...|$|R
40|$|<b>Principal</b> <b>component</b> <b>analysis</b> {{is one of}} {{the most}} popular machine {{learning}} and data mining techniques. Having its origins in statistics, <b>principal</b> <b>component</b> <b>analysis</b> is used in numerous applications. However, there seems to be not much systematic testing and assessment of <b>principal</b> <b>component</b> <b>analysis</b> for cases with erroneous and incomplete data. The purpose of this article is to propose multiple robust approaches for carrying out <b>principal</b> <b>component</b> <b>analysis</b> and, especially, to estimate the relative importances of the <b>principal</b> <b>components</b> to explain the data variability. Computational experiments are first focused on carefully designed simulated tests where the ground truth is known and can be used to assess the accuracy of the results of the different methods. In addition, a practical application and evaluation of the methods for an educational data set is given...|$|R
30|$|Step 2 Calculate the <b>principal</b> <b>component</b> loading matrix by <b>principal</b> <b>component</b> <b>analysis.</b>|$|R
