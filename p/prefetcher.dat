165|52|Public
50|$|Note {{that the}} Task Scheduler {{is the process}} {{responsible}} for parsing the trace data collected by the <b>prefetcher</b> and writing files to the <b>prefetcher</b> directory. As a result, the <b>prefetcher</b> will not operate correctly if the Task Scheduler service is not started.|$|E
50|$|Windows XP uses <b>prefetcher</b> {{to improve}} startup and {{application}} launch times. It also became possible to revert {{the installation of}} an updated device driver, should the updated driver produce undesirable results.|$|E
50|$|Since Windows Vista, the <b>Prefetcher</b> {{has been}} {{extended}} by SuperFetch and ReadyBoost. SuperFetch attempts to accelerate application launch times by monitoring and adapting to application usage patterns over periods of time, and caching {{the majority of the}} files and data needed by them into memory in advance {{so that they can be}} accessed very quickly when needed. ReadyBoost (when enabled) uses external memory like a USB flash drive to extend the system cache beyond the amount of RAM installed in the computer. ReadyBoost also has a component called ReadyBoot that replaces the <b>Prefetcher</b> for the boot process if the system has 700 MB or more of RAM.|$|E
40|$|Memory access latency {{continues}} {{to pose a}} crucial performance bottleneck for server workloads. 1 <b>Prefetchers</b> bring cache blocks from main memory to on-chip caches prior to explicit processor requests to hide cache miss latency. Prefetching improves throughput and response time by increasing memory-level parallelism 2, 3 and remains an essential strategy to address the processormemory performance gap. Today’s systems primarily employ stride-based <b>prefetchers,</b> which require only simple hardware additions and minimal on-chip area. However, these <b>prefetchers</b> are only partially effective in commercial server workloads, such as onlin...|$|R
5000|$|The <b>Prefetcher's</b> {{configuration}} {{is stored}} in the Windows Registry at [...] The EnablePrefetcher value can set {{to be one of}} the following: ...|$|R
40|$|Data {{prefetching}} in a cache hierarchy {{with high}} bandwidth and capacity In this paper we evaluate four hardware data <b>prefetchers</b> in the con-text of a high-performance three-level on chip cache hierarchy with high bandwidth and capacity. We consider two classic <b>prefetchers</b> (Sequential Tagged and Stride) and two correlating prefetchers: PC/ DC, a recent method with a superior score and low-sized tables, and P-DFCM, a new method. Like PC/DC, P-DFCM focuses on local delta sequences, {{but it is}} based on the DFCM value predictor. We explore different prefetch degrees and distances. Running SPEC 2000, Olden and IAbench applications, results show that this kind of cache hierarchy turns prefetching aggressiveness into suc-cess for the four <b>prefetchers.</b> Sequential Tagged is the best, and deserves further attention to cut it losses in some applications. PC/ DC results are matched or even improved by P-DFCM, using far fewer accesses to tables while keeping sizes low. ...|$|R
50|$|The {{ability to}} boot in 30 seconds was a design goal for Windows XP, and Microsoft's {{developers}} made efforts {{to streamline the}} system as much as possible; The Logical <b>Prefetcher</b> is {{a significant part of}} this; it monitors what files are loaded during boot, optimizes the locations of these files on disk so that less time is spent waiting for the hard drive's heads to move and issues large asynchronous I/O requests that can be overlapped with device detection and initialization that occurs during boot. The <b>prefetcher</b> works by tracing frequently accessed paged data which is then used by the Task Scheduler to create a prefetch-instructions file at %WinDir%\Prefetch. Once the system boots or an application is started, any data and code specified in the trace that is not already in memory is prefetched from the disk. The previous prefetching results determine which scenario benefited more and what should be prefetched at the next boot or launch. The <b>prefetcher</b> also uses the same algorithms to reduce application startup times. To reduce disk seeking even further, the Disk Defragmenter is called in at idle time to optimize the layout of these specific files and metadata in a contiguous area. Boot and resume operations can be traced and analyzed using Bootvis.exe.|$|E
50|$|When a Windows system boots, {{components}} of many files {{need to be}} read into memory and processed. Often {{different parts of the}} same file (e.g. Registry hives) are loaded at different times. As a result, a significant amount of time is spent 'jumping' from file to file and back again multiple times, even though a single access would be more efficient. The <b>prefetcher</b> works by watching what data is accessed during the boot process (including data read from the NTFS Master File Table), and recording a trace file of this activity. The boot <b>prefetcher</b> will continue to watch for such activity until 30 seconds after the user's shell has started, or until 60 seconds after all services have finished initializing, or until 120 seconds after the system has booted, whichever elapses first.|$|E
50|$|Conte {{realized}} in the early 90’s that Flynn’s prediction of the fetch bandwidth being the limit to increasing instruction level parallelism was coming true. His oft-cited International Symposium on Computer Architecture paper and subsequent work on instruction fetch mechanisms have influenced industry and spawned much follow-on research. More recently, Conte and his Ph.D. students invented a technique to predict data values with very high (~90%) accuracy and showed how predicting data values {{can be used to}} scale the memory wall by enabling aggressive prefetching. The work is of great interest to industry design teams who are struggling with performance limitations imposed by the speed gap between microprocessors and memory systems. Conte and his students have also developed a very small yet highly effective <b>prefetcher</b> termed the Spectral <b>Prefetcher.</b> This was published in the ACM Transactions on Computer Systems.|$|E
40|$|An {{important}} issue that affects response time performance in current OODB and hypertext systems is the I/O involved in moving objects from slow memory to cache. A promising way to tackle {{this problem is}} to use prefetching, in which we predict the user's next page requests and get those pages into cache in the background. Current databases perform limited prefetching using techniques derived from older virtual memory systems. A novel idea of using data compression techniques for prefetching was recently advocated in [KrV, ViK], in which <b>prefetchers</b> based on the Lempel-Ziv data compressor (the UNIX compress command) were shown theoretically to be optimal in the limit. In this paper we analyze the practical aspects of using data compression techniques for prefetching. We adapt three well-known data compressors to get three simple, deterministic, and universal <b>prefetchers.</b> We simulate our <b>prefetchers</b> on sequences of page accesses derived from the OO 1 and OO 7 benchmarks and from CAD applica [...] ...|$|R
5000|$|Software {{prefetching}} {{works well}} only with loops {{where there is}} regular array access as programmer has to hand code the prefetch instructions. Whereas, hardware <b>prefetchers</b> work dynamically based on the program's behavior at runtime.|$|R
40|$|Abstract—L 1 instruction-cache misses pose a {{critical}} performance bottleneck in commercial server workloads. Cache access latency constraints preclude L 1 instruction caches {{large enough to}} capture the application, library, and OS instruction working sets of these workloads. To cope with capacity constraints, researchers have proposed instruction <b>prefetchers</b> that use branch predictors to explore future control flow. However, such <b>prefetchers</b> suffer from several fundamental flaws: their lookahead is limited by branch prediction bandwidth, their accuracy suffers from geometrically-compounding branch misprediction probability, and they are ignorant of the cache contents, frequently predicting blocks already present in L 1. Hence, L 1 instruction misses remain a bottleneck...|$|R
50|$|A second myth is {{that the}} user should delete the {{prefetch}} folder contents {{to speed up the}} computer. If this is done, Windows will need to re-create all the prefetch files again, thereby slowing down Windows during boot and program starts until the prefetch files are created—unless the <b>prefetcher</b> is disabled.Windows maintains prefetch files in the Prefetch folder for up to the 128 most recently launched programs.|$|E
50|$|As {{common with}} other {{defragmentation}} programs, DiskTuna uses the Windows defrag API. DiskTuna {{is capable of}} moving the Master File Table (MFT) while the file system is mounted. DiskTuna provides boot and application launch optimization utilizing the Windows layout.ini (see <b>Prefetcher).</b> DiskTuna counters interaction between the Shadow Copy service and defragmentation I/O by offering a VSS safe-mode option. DiskTuna monitors the hard disk's temperature using S.M.A.R.T., and can warn the user if it overheats.|$|E
50|$|The Turion X2 Ultra is a dual-core {{processor}} fabricated on 65 nm technology using 300 mm SOI wafers. It supports DDR2-800 SO-DIMMs {{and features}} a DRAM <b>prefetcher</b> {{to improve performance}} and a mobile-enhanced northbridge (memory controller, HyperTransport controller, and crossbar switch). Each processor core comes with 1 MiB L2 cache {{for a total of}} 2 MiB L2 cache for the entire processor. This is double the L2 cache found on the Turion 64 X2 processor. Clock rates range from 2.0 GHz to 2.4 GHz, and thermal design power (TDP) range from 32 watts to 35 watts.|$|E
40|$|With {{processor}} speeds {{continuing to}} outpace the memory subsystem, cache missing memory operations continue to {{become increasingly important}} to application performance. In response to this continuing trend, most modern processors now support hardware (HW) <b>prefetchers,</b> which act to reduce the missing loads observed by an application...|$|R
40|$|Although caches {{for decades}} {{have been the}} {{backbone}} of the memory system, the speed gap between CPU and main memory suggests their augmentation with prefetching mechanisms. Recently, sophisticated hardware correlating prefetching mechanisms have been proposed, in some cases coupled with some form of dead-block prediction. In many proposals, however, correlating <b>prefetchers</b> demand a significant investment in hardware...|$|R
40|$|Data {{prefetching}} effectively {{reduces the}} negative effects of long load latencies on the performance of modern processors. Hardware <b>prefetchers</b> employ hardware structures to predict future memory addresses based on previous patterns. Thread-based <b>prefetchers</b> use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer cache, which tracks pointer transitions, to aid prefetching. The pointer cache provides, for a given pointer's effective address, the base address of the object pointed to by the pointer. We examine using the pointer cache in a wide issue superscalar processor as a value predictor and to aid prefetching when a chain of pointers is being traversed. When a load misses in the L 1 cache, but hits in the pointer cache, the first two cache blocks of the pointed to object are prefetched. In addition, the load's dependencies are broken by using the pointer cache hit as a value prediction. We also examine using the pointer cache to allow speculative precomputation to run farther ahead of the main thread of execution than in prior studies. Previously proposed thread-based <b>prefetchers</b> are limited in how far they can run ahead of the main thread when traversing a chain of recurrent dependent loads. When combined with the pointer cache, a speculative thread can make better progress ahead of the main thread, rapidly traversing data structures in the face of cache misses caused by pointer transitions...|$|R
5000|$|The <b>prefetcher</b> stores its trace {{files in}} the [...] "Prefetch" [...] folder inside the Windows folder (typically [...] ). The {{name of the}} boot trace file is always , and {{application}} trace files are a concatenation of the application's executable name, a hyphen, a hexadecimal representation of the hash of the path the file resides in, and a [...] ".pf" [...] extension. Applications that host other components (i.e. Microsoft Management Console or Dllhost) have {{the name of the}} loaded component included in the computed hash as well; this results in different trace files being created for each component.|$|E
50|$|For example, a 2.66 GHz Intel Core 2 Duo {{can perform}} {{a maximum of}} 25 GFLOPs (25 billion {{single-precision}} floating-point operations per second) if optimally using SSE and streaming memory access so the <b>prefetcher</b> works perfectly. However, traditionally (due to shader program length limits) most GPGPU kernels tend to perform relatively small amounts of work on large amounts of data in parallel, so the big problem with directly executing GPGPU algorithms on desktop CPUs is vastly lower memory bandwidth as generally speaking the CPU spends most of its time waiting on RAM. As an example, dual-channel PC2-6400 DDR2 RAM can throughput about 11 Gbit/s which is around 1.5 GFLOPs maximum given {{that there is a}} total of 3 GFLOPs total bandwidth and one must both read and write. As a result, if memory bandwidth constrained, Brook's CPU backend won't exceed 2 GFLOPs. In practice, it's even lower than that most especially for anything other than float4 which is the only data type which can be SSE accelerated.|$|E
5000|$|An issue {{inherent}} to indiscriminate link prefetching involves {{the misuse of}} [...] "safe" [...] HTTP methods. The HTTP GET and HEAD requests {{are said to be}} [...] "safe", i.e., a user agent that issues one of these requests should expect that the request results in no change on the recipient server. However, it's not uncommon for website operators to use these requests outside of this constraint. Plain hyperlinks (which almost universally result in GET requests) are often used to implement logout functionality and account verification, e.g., when a user completes an account creation form, and an automated service sends a verification e-mail to the user's given e-mail address. Similarly, it is entirely possible for a hosting service to provide a Web front end to manage files, including links that delete one or more files. Users who visit pages containing these types of links, (whilst using a browser which employs an indiscriminate link <b>prefetcher),</b> might find that they have been logged out or that their files have been deleted.|$|E
40|$|Although caches {{for decades}} {{have been the}} {{backbone}} of the memory system, the speed gap between CPU and main memory suggests their augmentation with prefetching mechanisms. Recently, sophisticated hardware correlating prefetching mechanisms have been proposed, in some cases coupled with some form of dead-block prediction. In many proposals, however, correlating <b>prefetchers</b> demand a significant investment in hardware. In this pape...|$|R
40|$|Translation Lookaside Buffers (TLBs) are {{critical}} to overall system performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as chip multiprocessors (CMPs) become ubiquitous, TLB design and performance must be re-evaluated. Our paper begins by performing a thorough TLB performance evaluation of sequential and parallel benchmarks running on a real-world, modern CMP system using hardware performance counters. This analysis demonstrates {{the need for further}} improvement of TLB hit rates for both classes of application, and it also points out that the data TLB has a significantly higher miss rate than the instruction TLB in both cases. In response to the characterization data, we propose and evaluate both Inter-Core Cooperative (ICC) TLB <b>prefetchers</b> and Shared Last-Level (SLL) TLBs as alternatives to the commercial norm of private, per-core L 2 TLBs. ICC <b>prefetchers</b> eliminate 19 % to 90 % of data TLB (D-TLB) misses across parallel workloads while requiring only modest changes i...|$|R
40|$|Translation Lookaside Buffers (TLBs) are {{critical}} to processor performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as chip multiprocessors (CMPs) become ubiquitous, TLB designmust be re-evaluated. Thispaperisthefirsttoproposeandevaluateshared last-level (SLL) TLBs {{as an alternative to}} the commercial norm of private, per-core L 2 TLBs. SLL TLBs eliminate 7 - 79 % of system-wide misses for parallel workloads. This is an average of 27 % better than conventional private, per-core L 2 TLBs, translating to notable runtimegains. SLLTLBsalsoprovidebenefitscomparable to recently-proposed Inter-Core Cooperative (ICC) TLB <b>prefetchers,</b> but with considerably simpler hardware. Furthermore, unlike these <b>prefetchers,</b> SLL TLBs can aid sequential applications, eliminating 35 - 95 % of the TLB misses for various multiprogrammed combinations of sequential applications. This corresponds to a 21 % average increase in TLB miss eliminations compared toprivate, per-coreL 2 TLBs. Because of their benefits for parallel and sequentialapplications,andtheirreadily-implementablehardware, SLL TLBs hold great promisefor CMPs. ...|$|R
40|$|International audienceHardware {{prefetching}} is {{an important}} feature of modern high-performance processors. When the application working set is too large to fit in on-chip caches, disabling hardware prefetchers may result in severe performance reduction. A new <b>prefetcher</b> was recently introduced, the Sandbox <b>prefetcher,</b> that tries to find dynamically the best prefetch offset using the sandbox method. The Sandbox <b>prefetcher</b> uses simple hardware and was shown to be quite effective. However, the sandbox method {{does not take into}} account prefetch timeliness. We propose an offset <b>prefetcher</b> with a new method for selecting the prefetch offset that takes into account prefetch timeliness. We show that our Best-Offset <b>prefetcher</b> outperforms the Sandbox <b>prefetcher</b> on the SPEC CPU 2006 benchmarks, with equally simple hardware...|$|E
40|$|The aim of {{this project}} {{is to improve the}} {{software}} prefetching in LLVM. The current software <b>prefetcher</b> in LLVM is a rudimentary one and has not been tuned for various architectures. In comparison, the GCC <b>prefetcher</b> is more advanced and performs much better. There is a large scope for improvement in LLVM’s <b>prefetcher</b> using existing techniques in literature. My proposal is to implement these variety of techniques to improve the performance of the stride <b>prefetcher</b> in LLVM...|$|E
40|$|Abstract—Both on-chip {{resource}} contention and off-chip la-tencies {{have a significant}} impact on memory requests in large-scale chip multiprocessors. We propose a memory-side <b>prefetcher,</b> which brings data on-chip from DRAM, but does not pro-actively further push this data to the cores/caches. Sitting close to memory, it avails close knowledge of DRAM state and memory channels to leverage DRAM row buffer locality and channel state to bring data (from the current row buffer) on-chip ahead of need. This not only reduces the number of off-chip accesses for demand requests, but also reduces row buffer conflicts, effectively improving DRAM access times. At the same time, our <b>prefetcher</b> maintains this data in a small buffer at each memory controller instead of pushing it into the caches to avoid on-chip {{resource contention}}. We show that the proposed memory-side <b>prefetcher</b> outperforms a state-of-the-art core-side <b>prefetcher</b> and an existing memory-side <b>prefetcher.</b> More importantly, our <b>prefetcher</b> can also work in tandem with the core-side <b>prefetcher</b> to amplify the benefits. Using a wide range of multiprogrammed and mul-tithreaded workloads, we show that this memory-side <b>prefetcher</b> provides IPC improvements of 6. 2 % (maximum of 33. 6 %), and 10 % (maximum of 49. 6 %), on an average when running alone and when combined with a core-side <b>prefetcher,</b> respectively. By meeting requests midway, our solution reduces the off-chip latencies while avoiding the on-chip resource contention caused by inaccurate and ill-timed prefetches. Index Terms: memory prefetching, NOC, DRAM, CMP I...|$|E
40|$|Abstract—The processor-memory speed gap {{referred}} to as memory wall, has become much wider in multi core processors due {{to a number of}} cores sharing the processor-memory interface. In addition to other cache optimization techniques, the mechanism of prefetching instructions and data has been used effectively to close the processor-memory speed gap and lower the memory wall. A number of issues have emerged when prefetching is used aggressively in multicore processors. The results presented in this paper are an indicator of the problems that need to be taken into consideration while using prefetching as a default technique. This paper also quantifies the amount of degradation that applications face with the aggressive use of prefetching. Another aspect that is investigated is the performance of multicore processors using a multiprogram workload as compared to a single program workload while varying the configuration of the built-in hardware <b>prefetchers.</b> Parallel workloads are also investigated to estimate the speedup and the effect of hardware <b>prefetchers.</b> This paper is the outcome of work that forms a part of the PhD research project currently in progress at NED University of Engineering and Technology, Karachi. Keywords—Multicore; prefetchers; prefetch-sensitive; memory wall; aggressive prefetching; multiprogram workload; parallel workload. I...|$|R
40|$|Translation Lookaside Buffers (TLBs) are {{commonly}} employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), {{it is necessary}} to examine TLB performance in the context of parallel workloads. This work is the first to present TLB <b>prefetchers</b> that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19 % to 90 % of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8 - 46 % for a range of TLB sizes, a hardware/software approach yields improvements of 4 - 32 %. Overall, our work shows that TLB <b>prefetchers</b> exploiting inter-core correlations can effectively eliminate TLB misses...|$|R
40|$|The processor-memory speed gap {{referred}} to as memory wall, has become much wider in multi core processors due {{to a number of}} cores sharing the processor-memory interface. In addition to other cache optimization techniques, the mechanism of prefetching instructions and data has been used effectively to close the processor-memory speed gap and lower the memory wall. A number of issues have emerged when prefetching is used aggressively in multicore processors. The results presented in this paper are an indicator of the problems that need to be taken into consideration while using prefetching as a default technique. This paper also quantifies the amount of degradation that applications face with the aggressive use of prefetching. Another aspect that is investigated is the performance of multicore processors using a multiprogram workload as compared to a single program workload while varying the configuration of the built-in hardware <b>prefetchers.</b> Parallel workloads are also investigated to estimate the speedup and the effect of hardware <b>prefetchers.</b> This paper is the outcome of work that forms a part of the PhD research project currently in progress at NED University of Engineering and Technology, Karachi...|$|R
40|$|High {{performance}} processors employ hardware data prefetching {{to reduce}} the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into {{the design of the}} <b>prefetcher</b> to increase the average performance improvement provided by prefetching as well as {{to reduce the}} negative performance and bandwidth impact of prefetching. Our mechanism estimates <b>prefetcher</b> accuracy, <b>prefetcher</b> timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data <b>prefetcher</b> dynamically. We introduce a new method to track cache pollution caused by the <b>prefetcher</b> at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the <b>prefetcher.</b> Using the proposed dynamic mechanism improves average performance by 6. 5 % on the 17 memory-intensive benchmarks in the SPEC CPU 2000 suite compared to the best-performing conventional stream-based data <b>prefetcher</b> configuration, while it consumes 18. 7 % less memory bandwidth. Compared to a conventional stream-based data <b>prefetcher</b> configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13. 6 % higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on som...|$|E
40|$|In this work, {{we propose}} a new {{technique}} to improve the performance of hardware data prefetching. This technique is based on detecting periods of time and regions of code where the <b>prefetcher</b> is not working properly, thus not providing any speedup or even producing slowdown. Once these periods of time and regions of code are detected, the <b>prefetcher</b> may be switched off and later on, switched on. To efficiently implement such mechanism, we identify three orthogonal issues that must be addressed: the granularity of the code region, when the <b>prefetcher</b> is switched on, and when the <b>prefetcher</b> is switched off...|$|E
40|$|International audienceIn {{multi-core}} systems, prefetch requests of one core {{interfere with}} the demand and prefetch requests of other cores at the shared resources, which causes prefetcher-caused interference. <b>Prefetcher</b> aggressiveness controllers {{play an important role}} in minimizing the prefetcher-caused interference. State-of-the-art controllers such as hierarchical <b>prefetcher</b> aggressiveness control (HPAC) select appropriate throttling levels that can lead to improvement in system performance. However, HPAC does not consider the interactions between the throttling decisions of multiple prefetchers, and loses opportunity to improve system performance further. For multi-core systems, state-of-the-art <b>prefetcher</b> aggressiveness controllers controls the aggressiveness based on prefetch metrics such as accuracy, bandwidth consumption and cache pollution. We propose a synergistic <b>prefetcher</b> aggressiveness controller (SPAC), which explores the interactions between the throttling decisions of prefetchers, and throttles the prefetchers based on the improvement in fair-speedup of multi-core systems...|$|E
40|$|Among the {{techniques}} to hide or tolerate memory latency, data prefetching {{has been shown}} to be quite effective. However, this efficiency is often limited to prefetching into the first-level cache. With more aggressive architectural parameters in current and future processors, prefetching from main memory to the second-level (L 2) cache becomes increasingly more important. In this paper, we examine the impact of hardware-based <b>prefetchers</b> at the L 2 -main memory interface on the performance of an aggressive out of order superscalar processor...|$|R
40|$|We {{argue that}} five recent {{software}} and hardware developments — the AES-NI instructions, multicore processors with per-core caches, complex modern software, sophisticated <b>prefetchers,</b> and physically tagged caches — combine to make it substantially more difficult to mount data-cache side-channel attacks on AES than previously realized. We propose ways in which some of the challenges posed by these developments might be overcome. We also consider scenarios where sidechannel attacks are attractive, and whether our proposed workarounds might be applicable to these scenarios...|$|R
40|$|Hardware <b>prefetchers</b> are {{commonly}} used to hide and tol-erate off-chip memory latency. Prefetching techniques in the literature are designed for multiple independent sequential applications running on a multicore system. In contrast to mul-tiple independent applications, a single parallel application running on a multicore system exhibits different behavior. In case of a parallel application, cores share and communicate data and code among themselves, and there is commonality in the demand miss streams across multiple cores. This gives an opportunity to predict the demand miss streams and communi-cate the predicted streams from one core to another, which we refer as cross-core stream communication. We propose cross-core spatial streaming (XStream), a prac-tical and storage-efficient cross-core prefetching technique. XStream detects and predicts the cross-core spatial streams at the private mid level caches (MLCs) and sends the predicted streams in advance to MLC <b>prefetchers</b> of the predicted cores. We compare the effectiveness of XStream with the ideal cross-core spatial streamer. Experimental results demonstrate that, on an average (geomean), compared to the state-of-the-art spatial memory streaming, storage efficient XStream reduces the execution time by 11. 3 % (as high as 24 %) and 9 % (as high as 29. 09 %) for 4 -core and 8 -core systems respectively. 1...|$|R
