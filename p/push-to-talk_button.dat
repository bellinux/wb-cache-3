22|6|Public
50|$|In some circumstances, {{voice-operated}} transmit (VOX) is used {{in place}} of a <b>push-to-talk</b> <b>button.</b> Possible uses are handicapped users who cannot push a button, amateur radio operators, firefighters, crane operators, or others performing critical tasks where hands must be free but communication is still necessary.|$|E
50|$|In two-way radios with headsets, a <b>push-to-talk</b> <b>button</b> may be {{included}} on a cord or wireless electronics box clipped to the user's clothing. In fire trucks or an ambulance a button {{may be present}} where the corded headset plugs into the radio wiring. Aircraft typically have corded headsets and a separate <b>push-to-talk</b> <b>button</b> on the control yoke or control stick. Dispatch consoles often have a hand-operated push-to-talk buttons along with a foot switch or pedal. If the dispatcher's hands are on a computer keyboard, the user can step on the foot pedal to transmit. Some systems have muting so the dispatcher can be on a telephone call and the caller cannot hear what is said over the radio. Their headset microphone will mute if they transmit. This relieves the dispatcher of explaining every radio message to a caller.|$|E
50|$|Some trunked systems queue calls if a user's {{attempt to}} {{transmit}} gets a busy signal. In other words, if someone presses their <b>push-to-talk</b> <b>button</b> and all trunked radio system channels are busy, some systems will wait-list {{users in the}} same order as their busy signals occur. When a channel becomes available, the system notifies the user. There is disagreement about MultiNet's ability to queue calls when all channels are busy. Usually, the control channel is the path allowing wait-listed users to get in line. One publication says MultiNet communicates using low baud rate data multiplexed under voice if all channels are busy. One report says MultiNet users on a live system who got a busy had to either hold their <b>push-to-talk</b> <b>button</b> down continually until the system assigned them a channel or periodically check for an available channel by repeatedly pressing the push-to-talk.|$|E
5000|$|A yoke is {{very similar}} to a {{steering}} wheel except that it resembles the control yoke found on many aircraft and has two axes of movement: not only rotational movement about the shaft of the yoke, but also a forward-and-backward axis equivalent to that of pitch control on the yoke of an aircraft. Some yokes have additional controls attached directly to the yoke for simulation of aircraft functions such as radio <b>push-to-talk</b> <b>buttons.</b> Yokes, like throttle quadrants and pedals, are popular with serious flight-simulation enthusiasts.|$|R
30|$|Audio {{interfaces}} [20] are {{a type of}} interface that {{is being}} investigated to assist drivers to use in-car systems. Traditional interfaces present information to users by visual means, but for drivers this distraction has safety critical implications. To address this issue audio inputs are common for in-vehicle systems. The low quality of voice recognition technology can limit its effectiveness within this context. Weinberg et al. [21] have shown that multiple <b>push-to-talk</b> <b>buttons</b> can improve the performance of users of such systems. Other types of interaction paradigms in these papers include touch screens [22], pressure based input [23], spatial awareness [24] and gestures [25]. As well as using these new input modalities {{a number of researchers}} are also looking at alternative output modes such as sound [26] and tactile feedback [27].|$|R
40|$|Buttons {{built into}} the {{steering}} wheel are used in many vehicles as <b>push-to-talk</b> (PTT) <b>buttons</b> for in-car speech user interfaces. We {{explore the influence of}} such a fixed PTT button on driver hand position on the steering wheel and on visual attention while driving. We also explore these variables for a wireless PTT glove, which allows drivers to use the entire surface of the steering wheel to operate the PTT button. Participants in our driving simulator-based study were willing {{to take advantage of the}} flexibility in hand position afforded by the glove PTT button. We also found that participants cast glances toward the steering wheel significantly less often when using the PTT glove than they did when operating the fixed PTT button...|$|R
50|$|The Motorola i860 was {{the first}} iDEN phone to feature a camera. The VGA camera is {{equipped}} with a 10-second video record option and built-in ultra-bright spotlight. As it {{was the first}} iDEN phone to feature a camera, it also was first to feature multimedia messaging as well as push-to-send, where contact information can be sent to another compatible device using the phone's <b>push-to-talk</b> <b>button.</b>|$|E
50|$|Units can be individual-called: a {{dispatcher}} {{can call}} a single mobile or hand-held radio and initiate {{a conversation that}} will not be heard by any other radios in the system. A form of individual call is the telephone patch. Some systems allow half-duplex telephone calls to be placed. The radio user must use the <b>push-to-talk</b> <b>button</b> to speak and cannot talk and receive simultaneously.|$|E
50|$|Many radios are {{equipped}} with transmitter time-out timers which limit {{the length of a}} transmission. A bane of push-to-talk systems is the stuck microphone: a radio locked on transmit which disrupts communications on a two-way radio system. One example of this problem occurred in a car with a concealed two-way radio installation where the microphone and coiled cord were hidden inside the glove box. An operator tossed the mike into the glove box and shut it, causing the <b>push-to-talk</b> <b>button</b> to be depressed and locking the transmitter on. On taxi systems, a driver may be upset when a dispatcher assigns a call (s)he wanted to another driver and may deliberately hold the transmit button down (for which the owner can be fined by the FCC). Radios with time-out timers transmit for the preset amount of time, usually 30- or 60-seconds, after which the transmitter automatically turns off and a loud tone comes out of the radio speaker. The volume level of the tone on some radios is loud and cannot be adjusted. As soon as the <b>push-to-talk</b> <b>button</b> is released, the tone stops and the timer resets.|$|E
40|$|One of {{the main}} {{projects}} that was worked on this semester was creating an acoustic model for the Advanced Space Suit in Comsol Multiphysics. The geometry tools built into the software were used to create an accurate model of the helmet and upper torso of the suit. After running the simulation, plots of the sound pressure level within the suit were produced, as seen below in Figure 1. These plots show significant nulls which should be avoided when placing microphones inside the suit. In the future, this model can be easily adapted {{to changes in the}} suit design to determine optimal microphone placements and other acoustic properties. Another major project was creating an acoustic diverter that will potentially be used to route audio into the Space Station's Node 1. The concept of the project was to create geometry to divert sound from a neighboring module, the US Lab, into Node 1. By doing this, no new audio equipment would need to be installed in Node 1. After creating an initial design for the diverter, analysis was performed in Comsol in order to determine how changes in geometry would affect acoustic performance, as shown in Figure 2. These results were used to produce a physical prototype diverter on a 3 D printer. With the physical prototype, testing was conducted in an anechoic chamber to determine the true effectiveness of the design, as seen in Figure 3. The results from this testing have been compared to the Comsol simulation results to analyze how closely the Comsol results are to real-world performance. While the Comsol results do not seem to closely resemble the real world performance, this testing has provided valuable insight into how much trust can be placed in the results of Comsol simulations. A final project that was worked on during this tour was the Audio Interface Unit (AIU) design for the Orion program. The AIU is a small device that will be used for as an audio communication device both during launch and on-orbit. The unit will have functions including <b>push-to-talk</b> <b>buttons</b> and volume control. With this project, an existing design was modified based on prior feedback that had been received. With the modified design, I created a 3 D printed prototype, shown in Figure 4, which was then used in suited evaluations performed by crew members. The feedback received from those evaluations will be utilized to help create the best possible Orion AIU. As a whole, a number of different interesting engineering projects were worked on over the course of this semester. For many of these projects, acoustic simulations provided valuable insight into how different environments would respond to sound. While work is still underway to verify the results of these simulations, the results are fascinating because of the interesting ways that sound waves interact with the environment. Going forward, {{it will be interesting to}} see how closely these results can be matched by real-world test data...|$|R
5000|$|When Mission Control (in Houston, Texas) {{wanted to}} talk to astronauts, the capsule {{communicator}} (CAPCOM) pushed a <b>button</b> (<b>push-to-talk,</b> or PTT) that turned on the transmitter, then spoke, then released the button. When the transmitter is local, this is easy to arrange - the transmitter is connected directly to the PTT button. But to stay in continuous contact with the astronauts as they orbit the Earth, or travel to the Moon, NASA had to use tracking stations all around the world, switching from one station to the next as needed. To get the voice signal to the remote transmitter, dedicated telephone lines connected these stations to Houston. NASA could either build a parallel system for operating the transmitters - one line to carry the audio and another to carry the control signal for the PTT button (out-of-band signalling), or combine these two systems together, using audio tones to turn the transmitter on and off. Since dedicated phone lines were a very expensive measure at the time, NASA chose the use of tones to reduce the operating cost of the network. [...] The same system was used in Project Gemini and was still in use with half duplex UHF Space Shuttle communications for transmitter RF keying.|$|R
50|$|A modern mobile radio {{consists}} of a radio transceiver, housed in a single box, and a microphone with a <b>push-to-talk</b> <b>button.</b> Each installation would also have a vehicle-mounted antenna connected to the transceiver by a coaxial cable. Some models may have an external, separate speaker which can be positioned and oriented facing the driver to overcome ambient road noise present when driving. The installer would have to locate this equipment {{in a way that}} does not interfere with the vehicle's sun roof, electronic engine management system, vehicle stability computer, or air bags.|$|E
50|$|Modat, {{also written}} MODAT, is an {{obsolete}} Motorola data system using {{a sequence of}} seven audio tones similar to the five-tone-sequential Selcall format. Some systems still use Modat today. Modat is used for unit ID and emergency buttons, rather than for selective calling. In a typical installation, each radio in a system is assigned a unique seven-tone code. Each time the radio's <b>push-to-talk</b> <b>button</b> is pressed, the radio transmits the seven tone sequence {{at the beginning of}} the transmission. To prevent the user from talking while the tone sequence is broadcast, the seven-tone sequence is played over the two-way radio receiver's speaker.|$|E
5000|$|The Automatic Transmitter Identification System (ATIS) is {{a marine}} VHF radio system used and {{mandated}} on navigable inland waterways in Europe for identifying the ship or vessel {{that made a}} radio transmission. The identity of the vessel is sent digitally immediately after the ship's radio operator has finished talking and releases their transceiver's <b>push-to-talk</b> <b>button.</b> This contrasts to the Automatic identification system(AIS) used globally on ships that transmit continuously. A short post-transmission message is sent by the radio with the vessel identity {{and is in the}} form of an encoded call sign or Maritime Mobile Service Identity, starting with number [...] "9" [...] and the three country-specific maritime identification digits.|$|E
50|$|In telecommunications, a voice {{operated}} switch, {{also known}} as VOX or Voice Operated eXchange, is a switch that operates when sound over a certain threshold is detected. It is usually used to turn on a transmitter or recorder when someone speaks and turn it off when they stop speaking. It is used instead of a <b>push-to-talk</b> <b>button</b> on transmitters or to save storage space on recording devices. On cell phones, {{it is used to}} save battery life. Intercom systems that use a speaker in a room as both a speaker and a microphone will often use VOX on the main console to switch the audio direction during a conversation. The circuit usually includes a delay between the sound stopping and switching direction, to avoid the circuit turning off during short pauses in speech.|$|E
5000|$|Consoles {{serve as}} a human {{interface}} and connect to push-to-talk dispatch radio systems. Audio from all channels is processed through audio level compression circuits and is routed to two separate speakers identified as select and unselect. Each has a volume control. The select channel or channels carry the highest priority communications. To prevent missed messages on critical channels, the select volume may be configured so it cannot be set to an inaudible level. Unselect channels {{may be used for}} special events, other agencies, or purposes that do not involve dispatch and may be inaudible. By pressing a button, any channel on the console can be toggled between select and unselect status. Each channel has an independent <b>push-to-talk</b> <b>button,</b> allowing the dispatcher to talk over one channel at a time. For broadcast messages, a single button transmits over all selected channels at the same time. A digital clock and an LED bar-graph or VU meter are included.|$|E
5000|$|In the US, Federal Communications Commission rules require {{users of}} {{selective}} calling {{to monitor the}} channel (i.e. switch to carrier squelch) before transmitting. In other words, the user must monitor (listen) {{to make sure the}} channel is not in use by someone on another selective calling code before transmitting. To enforce this rule, base stations often have a monitor switch on the microphone. The <b>push-to-talk</b> <b>button</b> is split into two segments. One segment turns the selective calling off. The other segment of the button transmits. A mechanical interlock prevents the transmit button from being pressed until the monitor button is down. This is called, [...] "compulsory monitor before transmit." [...] In mobile radios, microphones are stored in a hang-up box. When the microphone is pulled out of the hang-up, the radio reverts to carrier squelch, (the selective calling feature is disabled). The user automatically monitors—verifies no one else is using the channel—by pulling the mike out of the hang-up box. Hand-held radios sometimes have LED indicators that show when the channel is in use.|$|E
50|$|Unit ID {{can be sent}} {{as leading}} or {{trailing}} a voice message. In the leading option, the data burst is sent at the moment a user presses the radio's <b>push-to-talk</b> <b>button.</b> An option can be set to make the radio's speaker emit a tone {{for the length of}} the unit ID data, (about 1-1.5 seconds). This reminds a user to wait until the data has been sent before talking. The leading unit ID takes slightly more air time (is longer) than a trailing ID because of a header tone and the need to delay the data burst to allow time for CTCSS decoders and voting comparators to open an audio path to the decoder. A default delay is defined with the unit ID option. To adjust for time delay variations in each individual system, radios can be programmed to delay the sending of a radio's unit ID data by up to hundreds of milliseconds within a range. In the trailing option, the data packet is sent at the moment the microphone button is released. This avoids timing issues because the audio path to the base station is already open.|$|E
5000|$|Some {{professional}} systems use a phase-reversal of the CTCSS tone {{at the end}} of {{a transmission}} to eliminate the squelch crash or squelch tail. This is common with General Electric Mobile Radio and Motorola systems. When the user releases his <b>push-to-talk</b> <b>button</b> the CTCSS tone does a phase shift for about 200 milliseconds. In older systems, the tone decoders used mechanical reeds to decode CTCSS tones. When audio at a resonant pitch was fed into the reed, it would resonate / vibrate, which would turn on the speaker audio. The end-of-transmission phase reversal (called [...] "reverse burst" [...] by Motorola and [...] "squelch tail elimination" [...] or [...] "STE" [...] by GE [...] ) caused the reed to abruptly stop vibrating which would cause the receive audio to instantly mute. Initially, a phase shift of 180 degrees was used, but experience showed that a shift of ±120 to 135 degrees was optimal in halting the mechanical reeds. These systems often have audio muting logic set for CTCSS only. If a transmitter without the phase reversal feature is used, the squelch can remain unmuted for as long as the reed continues to vibrate—up to 1.5 seconds {{at the end of}} a transmission as it coasts to a stop (sometimes referred to as the [...] "flywheel effect" [...] or called [...] "freewheeling"). Thus, there is one caveat about all CTCSS being interchangeable—if the phase changing system exists then the shift angle must match. Note that the hardware used to implement the [...] "reverse burst" [...] / [...] "squelch tail elimination" [...] system is all contained in the transmitter.|$|E
5000|$|The {{ability of}} a {{receiver}} to mute the audio until it detects a carrier with the correct CTCSS tone is called decoding. Receivers are equipped with features to allow the CTCSS [...] "lock" [...] to be disabled. On USA licensed systems, Federal Communications Commission rules require CTCSS users on shared channels to disable their receiver's CTCSS to check if co-channel users are talking before transmitting. On a base station console, a microphone may have a split <b>push-to-talk</b> <b>button.</b> Pressing {{one half of the}} button, (often marked with a speaker icon or the letters [...] "MON", short for [...] "MONitor") disables the CTCSS decoder and reverts the receiver to hearing any signal on the channel. This is called the monitor function. There is sometimes a mechanical interlock: the user must push down and hold the monitor button or the transmit button is locked and cannot be pressed. This interlock option is referred to as compulsory monitor before transmit (the user is forced to monitor by the hardware design of the equipment itself). On mobile radios, the microphone is usually stored in a hang-up clip or a hang-up box containing a microphone clip. When the user pulls the microphone out of the hang-up clip to make a call, a switch in the clip (box) forces the receiver to revert to conventional carrier squelch mode ("monitor"). Some designs relocate the switch into the body of the microphone itself. In hand-held radios, an LED indicator may glow green, yellow, or orange to indicate another user is talking on the channel. Hand-held radios usually have a switch or push-button to monitor. Some modern radios have a feature called [...] "Busy Channel Lockout", which will not allow the user to transmit as long as the radio is receiving another signal.|$|E
40|$|In {{this article}} we compare two large scale {{dialogue}} corpora recorded in different settings. The main differences are unrestricted turntaking vs. <b>push-to-talk</b> <b>button</b> and complex vs. simple negotiation task. In our investigation we found that vocabulary, durations of turns, words and sounds as well as prosodical features are influenced by differences in the setting...|$|E
40|$|Unit {{containing}} two hand-operated switches water-tight {{and designed}} for use with protective suit. One switch is toggle switch used to select communication by wire or radio. Other switch is press-to-talk button. User grasps and operates switches without looking at them. Fences on top of new unit protects toggle switch from inadvertent operation and spaced to accommodate gloved thumb. <b>Push-to-talk</b> <b>button</b> protected by longitudinal bar, springy and compressed inward to actuate button...|$|E
40|$|With the {{proliferation}} of cell phones around the world, governments have been enacting legislation prohibiting the use of cell phones during driving without a “hands-free ” kit, bringing automotive speech recognition {{to the forefront of}} public safety. At the same time, the trend in cell phone hardware has been to create smaller and thinner devices with greater computational power and functional complexity, making speech the most viable modality for user input. Given the important role that automotive speech recognition is likely to play in consumer lives, we explore how the accuracy of the speech engine, the use of the <b>push-to-talk</b> <b>button,</b> and the type of dialog repair employed by the interface influences driving performance. In experiments conducted with a driving simulator, we found that the accuracy of the speech engine and its interaction {{with the use of the}} <b>push-to-talk</b> <b>button</b> does impact driving performance significantly, but the type of dialog repair employed does not. We discuss the implications of these findings on the design of automotive speech recognition systems. Index Terms: automotive speech recognition 1...|$|E
40|$|We {{present a}} driving simulator-based {{evaluation}} of a new technique for simplifying in-vehicle device interactions and thereby improving driver safety. We show {{that the use of}} multiple, contextually linked push-to-talk buttons (Multi-PTT) shortens voice dialog duration versus the use of a conventional, single <b>push-to-talk</b> <b>button</b> (Single-PTT). This benefit comes without detriment to driving performance or visual attention to the forward roadway. Test subjects also preferred the Multi-PTT approach over the conventional approach, and reported that it imposed a lower cognitive workload...|$|E
40|$|The Project 54 system {{integrates}} multiple {{electronic devices}} in police cruisers {{into a single}} system with a speech user interface (SUI). The system has been deployed in over 240 cruisers {{in the state of}} New Hampshire. We collected over 49, 000 samples of speech commands, along with the corresponding SUI responses, from 27 officers using the Project 54 system during their everyday work. We found that the system recognized officer utterances around 85 % of the time. About one third of recognition errors were due to speech recognizer errors and about two thirds were due to user errors. Three types of user errors were identified: issuing a command that is not valid in any context of the SUI (54 % of user errors), issuing a command that is valid in some context but not in the current one (34 %) and operating the <b>push-to-talk</b> <b>button</b> incorrectly (12 %). 1...|$|E
40|$|Public access mobile radio systems (PAMR) using {{trunking}} technology {{allow more}} {{efficient use of}} the frequency spectrum, as all the users of the system jointly use a common pool of channels. Trunked PAMR systems can operate on two different levels of management, {{on the basis of}} messages (message trunking) and transmissions (transmission trunking). When using message trunking, the system allocates the radio channel from the beginning {{to the end of the}} conversation. However, with transmission trunking, the system allocates a radio channel when one partner presses his or her <b>push-to-talk</b> <b>button</b> (PTT) and deallocates the radio channel when he or she releases the PTT button to change talking partners. Thus, during the silence produced by this change, no channel is occupied. The advantage of transmission trunking is that there is no waste of channel occupancy during the pauses in the conversation. The drawback is that a more complex control system with more signalling is needed, where the distribution of the transmission occupancy is more crucial in the performance than in the distribution of the message holding time when trunking by message. In message trunking, the lack of radio channel only causes a delay and the user has to wait to begin the conversation, while in transmission trunking, it can produce an unpleasant effect of clipped words. This paper verifies that voice calls, generally assumed to have an exponential holding time, are better modelled with other distributions, and analyses the statistical properties of the conversation time in two different PAMR systems (13 refs.) Peer Reviewe...|$|E
40|$|Many {{current and}} future NASA {{missions}} make extreme demands on mission personnel {{both in terms of}} work load and in performing under difficult environmental conditions. In situations where hands are impeded or needed for other tasks, eyes are busy attending to the environment, or tasks are sufficiently complex that ease of use of the interface becomes critical, spoken natural language dialog systems offer unique input and output modalities that can improve efficiency and safety. They also offer new capabilities that would not otherwise be available. For example, many NASA applications require astronauts to use computers in micro-gravity or while wearing space suits. Under these circumstances, command and control systems that allow users to issue commands or enter data in hands-and eyes-busy situations become critical. Speech recognition technology designed for current commercial applications limits the performance of the open-ended state-of-the-art dialog systems being developed at NASA. For example, today's recognition systems typically listen to user input only during short segments of the dialog, and user input outside of these short time windows is lost. Mistakes detecting the start and end times of user utterances can lead to mistakes in the recognition output, and the dialog system as a whole has no way to recover from this, or any other, recognition error. Systems also often require the user to signal when that user is going to speak, which is impractical in a hands-free environment, or only allow a system-initiated dialog requiring the user to speak immediately following a system prompt. In this project, SRI has developed software to enable speech recognition in a hands-free, open-microphone environment, eliminating the need for a <b>push-to-talk</b> <b>button</b> or other signaling mechanism. The software continuously captures a user's speech and makes it available to one or more recognizers. By constantly monitoring and storing the audio stream, it provides the spoken dialog manager extra flexibility to recognize the signal with no audio gaps between recognition requests, as well as to rerecognize portions of the signal, or to rerecognize speech with different grammars, acoustic models, recognizers, start times, and so on. SRI expects that this new open-mic functionality will enable NASA to develop better error-correction mechanisms for spoken dialog systems, and may also enable new interaction strategies...|$|E

