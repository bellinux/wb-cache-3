600|10|Public
25|$|The RIKEN MDGRAPE-3 for {{molecular}} dynamics simulations of proteins {{is a special}} purpose <b>petascale</b> supercomputer at the Advanced Center for Computing and Communication, RIKEN in Wako, Saitama, just outside Tokyo. It uses over 4,800 custom MDGRAPE-3 chips, as well as Intel Xeon processors. However, given {{that it is a}} special purpose computer, it can not appear on the TOP500 list which requires Linpack benchmarking.|$|E
25|$|The Computer Centre, {{established}} in 1970 {{as a central}} computing facility, became Supercomputer Education and Research Centre (SERC) in 1990 to provide state-of-the-art computing facility to the faculty and students of the Institute. SERC is created and fully funded by the Ministry of Human Resource Development (MHRD) to commemorate the platinum jubilee of the Institute. It houses India's first <b>petascale</b> supercomputer CrayXC-40 and also the fastest supercomputer in India.|$|E
50|$|The National Science Foundation is {{responsible}} for initiating and funding several <b>petascale</b> computers in the United States, as well as DARPA who gave IBM the contract to develop the <b>petascale</b> PERCS (Productive, Easy-to-use, Reliable Computer System) platform.|$|E
40|$|State-of-the-art job schedulers have master/slave architecture, {{where the}} master becomes a {{performance}} bottleneck and {{is susceptible to}} single point of failure, especially at <b>petascales</b> • Develop a dynamic distributed scalable job scheduling system at the granularity of node/core levels with distributed load balancing algorithm (work stealing) leading to high throughput and system utilization. Building Block...|$|R
40|$|The {{following}} report outlines work undertaken for PRACE- 2 IP. The {{report will}} outline the computational {{methods used to}} examine <b>petascaling</b> of OpenFOAM on the French Tier- 0 system CURIE. The case study used has been provided by the National University of Ireland, Galway (NUIG). The profiling techniques utilised to uncover bottlenecks, specifically in communication and file I/O within the code, will provide an insight into the behaviour of OpenFOAM and highlight practices that will be of benefit to the user community...|$|R
40|$|The work aims at {{evaluating}} {{the performance of}} GROMACS on different platforms and and determine the optimal set of conditions for given architectures for <b>petascaling</b> molecular dynamics simulations. The activities have been organized into three tasks within PRACE project: (i) Optimization of GROMACS performance on Blue Gene systems; (ii) Parallel scaling of the OpenMP implementation; (iii) Development of a multiple step-size symplectic integrator adapted to the large biomolecule systems. Part of the results reported here has been achieved through the collaboration with ScalaLife project...|$|R
50|$|Russia has {{developed}} the Lomonosov (rus) <b>petascale</b> computer.|$|E
5000|$|... #Caption: The National <b>Petascale</b> Computing Facility, home of Blue Waters ...|$|E
50|$|The first 20 {{supercomputers}} on the June 2012 {{list are}} <b>petascale.</b>|$|E
40|$|In {{this study}} we report the load-balancing {{performance}} issues that are observed during the <b>petascaling</b> of a space plasma simulation code developed at the Finnish Meteorological Institute (FMI). The code models the communication pattern as a hypergraph, and partitions the computational grid using the parallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning framework. The result of partitioning determines the distribution of grid cells to processors. It is observed that the initial partitioning and data distribution phases take a substantial percentage of the overall computation time. Alternative (graph-partitioning-based) schemes that provide better balance are investigated. Comparisons in terms of effect on running time and load-balancing quality are presented. Test results on Juelich BlueGene/P cluster are reported. © 2013 Springer-Verlag...|$|R
40|$|Efficiently {{scheduling}} {{large number}} of jobs over large-scale distributed systems is critical in achieving high system utilization and throughput. Most of current job management systems (JMS) have centralized Master/Slaves architecture that has inherent limitations, such as scalability issues at extreme scales (e. g. <b>petascales</b> and beyond), and single point of failure. In designing the next generation distributed JMS, we must address new challenges such as load balancing. This paper presents MATRIX, a many-task computing execution fabric at exascale. MATRIX utilizes adaptive work stealing algorithm for distributed load balancing, and distributed hash tables for managing task metadata. MATRIX supports many-task computing (MTC) workloads with or without task dependencies in the execution of complex largescale workflows. MATRIX has shown throughput as high as 54. 4 K tasks/sec at 4 K-core scales running on an IBM Blue Gene/P supercomputer with sub-second sleep tasks (64 ms) ...|$|R
40|$|The work aims at {{evaluating}} {{the performance of}} DALTON on different platforms and implementing new strategies to enable the code for <b>petascaling.</b> The activities have been organized into four tasks within PRACE project: (i) Analysis of {{the current status of}} the DALTON quantum mechanics (QM) code and identification of bottlenecks, implementation of several performance improvements of DALTON QM and first attempt of hybrid parallelization; (ii) Implementation of MPI integral components into LSDALTON, improvements of optimization and scalability, interface of matrix operations to PBLAS and ScaLAPACK numerical library routines; (iii) Interfacing the DALTON and LSDALTON QM codes to the ChemShell quantum mechanics/molecular mechanics (QM/MM) package and benchmarking of QM/MM calculations using this approach; (vi) Analysis of the impact of DALTON QM system components with Dimemas. Part of the results reported here has been achieved through the collaboration with ScalaLife project...|$|R
50|$|China has {{developed}} four <b>petascale</b> computers, Nebulae, Tianhe-I, Tianhe-2, and the Sunway TaihuLight.|$|E
50|$|As of 2012, {{these are}} the known active <b>petascale</b> {{computers}} in the world.|$|E
50|$|Other countries, such as Germany and Japan, {{have plans}} {{of their own}} for <b>petascale</b> computers.|$|E
40|$|This whitepaper {{describes}} the load-balancing performance {{issues that are}} observed and tackled during the <b>petascaling</b> of the Vlasiator codes. Vlasiator is a Vlasov-hybrid simulation code developed in Finnish Meteorological Institute (FMI). Vlasiator models the communications associated with the spatial grid operated on as a hypergraph and partitions the grid using the parallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning framework. The result of partitioning determines the distribution of grid cells to processors. It is observed that the partitioning phase takes a substantial percentage of the overall computation time. Alternative (graph-partitioning-based) schemes that perform almost {{as well as the}} hypergraph partitioning scheme and that require less preprocessing overhead and better balance are proposed and investigated. A comparison in terms of effect on running time, preprocessing overhead and load-balancing quality of Zoltan's PHG, ParMeTiS, and PT-SCOTCH are presented. Test results on Jüelich BlueGene/P cluster are presented...|$|R
40|$|Abstract- Efficiently {{scheduling}} {{large number}} of jobs over large scale distributed systems is very critical {{in order to achieve}} high system utilization and throughput. Today's state-of-the-art job schedulers mostly follow a centralized architecture that is master/slave architecture. The problem with this architecture is that it cannot scale efficiently upto even <b>petascales</b> and is always vulnerable to single point of failure. This is over come by the distributed job management system called MATRIX (MAny-Task computing execution fabRIc at eXascale) which adopts a work stealing algorithm which aims at load balancing throughout the distributed system. The MATRIX currently supports Many Task Computing (MTC) workloads. This project aims at extending MATRIX in order to support the High Performance Computing (HPC) workloads. The HPC workloads are nothing but long jobs which needs multiple nodes/cores to run the tasks. It is a challenge to support HPC on the framework which supports MTC jobs. The framework is focused at efficiently scheduling sub-second jobs on available workers. The design of scheduling HPC jobs should be efficient enough in order to not hamper the efficient working of MTC tasks. I...|$|R
40|$|High Performance Computing (HPC) {{is the key}} {{to solving}} many scientific, financial, and {{engineering}} problems. Computer clusters are now the dominant architecture for HPC. The scale of clusters, both in terms of processor per node and the number of nodes, is increasing rapidly, reaching <b>petascales</b> these days and soon to exascales. Inter-process communication plays {{a significant role in the}} overall performance of HPC applications. With the continuous enhancements in interconnection technologies and node architectures, the Message Passing Interface (MPI) needs to be improved to effectively utilize the modern technologies for higher performance. After providing a background, I present a deep analysis of the user level and MPI libraries over modern cluster interconnects: InfiniBand, iWARP Ethernet, and Myrinet. Using novel techniques, I assess characteristics such as overlap and communication progress ability, buffer reuse effect on latency, and multiple-connection scalability. The outcome highlights some of the inefficiencies that exist in the communication libraries. To improve communication progress and overlap in large message transfers, a method is proposed which uses speculative communication to overlap communication with computation i...|$|R
50|$|Sun Constellation System {{is an open}} <b>petascale</b> {{computing}} environment introduced by Sun Microsystems in 2007.|$|E
50|$|<b>Petascale</b> {{can also}} refer to very large storage systems where the {{capacity}} exceeds one petabyte (PB).|$|E
5000|$|In computing, <b>petascale</b> {{refers to}} a {{computer}} system capable of reaching performance in excess of one petaflops, i.e. one quadrillion floating point operations per second. The standard benchmark tool is LINPACK and Top500.org is the organization which tracks the fastest supercomputers. Some uniquely specialized <b>petascale</b> computers do not rank on the Top500 list since they cannot run LINPACK. This makes comparisons to ordinary supercomputers hard.|$|E
40|$|Efficiently {{scheduling}} {{large number}} of jobs over large-scale distributed systems is critical in achieving high system utilization and throughput. Today’s state-of-the-art job management systems have predominantly Master/Slaves architectures, which have inherent limitations, such as scalability issues at extreme scales (e. g. <b>petascales</b> and beyond) and single point of failure. In designing the nextgeneration distributed job management system, we must address new challenges such as load balancing. This paper presents the design, analysis and implementation of a distributed execution fabric called MATRIX (MAny-Task computing execution fabRIc at eXascale). MATRIX utilizes an adaptive work stealing algorithm for distributed load balancing, and distributed hash tables for managing task metadata. MATRIX supports both high-performance computing (HPC) and many-task computing (MTC) workloads, as well as task dependencies in the execution of complex large-scale workflows. We have evaluated it using synthetic workloads up to 4 K-cores on an IBM Blue Gene/P supercomputer, and have shown high efficiency rates (e. g. 85 %+) are possible with certain workloads with task granularities as low as 64 ms. MATRIX has shown throughput rates as high as 13 K tasks/sec at 4 K-core scales (one to two orders of magnitude higher than existing centralized systems). We also explore the feasibility of adaptive work stealing up to 1 M-node scale through simulations. 1...|$|R
50|$|<b>Petascale</b> {{computers}} are under development from manufacturers such as Sun Microsystems, Cray, IBM, Dawning, SGI, and NEC.|$|E
5000|$|TGCC {{is a new}} [...] "green infrastructure" [...] {{for high}} {{computing}} performance, able to host <b>petascale</b> supercomputers.|$|E
5000|$|SGI Pleiades {{which went}} online in 2008 with a {{performance}} of 600 TFLOPS, reached <b>petascale</b> in 2012.|$|E
5000|$|There {{are three}} main {{components}} to MADNESS. At the lowest level is a <b>petascale</b> parallel programming environment ...|$|E
50|$|RIKEN MDGRAPE-3 in Japan {{which went}} online in 2006 reaches <b>petascale</b> {{performance}} but can't run LINPACK, so comparisons to regular supercomputers are hard.|$|E
50|$|He was the {{institutional}} lead {{and is a}} technical area lead on Illinois’ Blue Waters <b>Petascale</b> project funded by a $200M NSF grant.|$|E
50|$|In 2011, the {{challenges}} and difficulties in pushing the envelope in supercomputing were underscored by IBM's abandonment of the Blue Waters <b>petascale</b> project.|$|E
5000|$|Nebulae {{built by}} Dawning, {{was the third}} <b>petascale</b> {{computer}} and the first built by China with a performance of 1.271 petaflops in 2010.|$|E
5000|$|Roadrunner, {{built by}} IBM, {{was the first}} {{computer}} to go <b>petascale,</b> and did so on May 25, 2008, with sustained performance of 1.026 petaflops.|$|E
50|$|In February 2009 it was {{announced}} that JUGENE would be upgraded to reach petaflops performance in June 2009, making it the first <b>petascale</b> supercomputer in Europe.|$|E
50|$|<b>Petascale</b> {{computing}} {{is being}} used to do advanced computations in fields such as weather and climate simulation, nuclear simulations, cosmology, quantum chemistry, lower-level organism brain simulation, and fusion science.|$|E
50|$|MDGRAPE-3 is an ultra-high {{performance}} <b>petascale</b> supercomputer system {{developed by}} the RIKEN research institute in Japan. It is a special purpose system built for molecular dynamics simulations, especially protein structure prediction.|$|E
50|$|This {{supercomputing}} {{center has}} been planned {{to welcome the}} first French <b>Petascale</b> machine Curie, funded by GENCI for the PRACE Research Infrastructure, and {{the next generation of}} the CCRT Computing Center.|$|E
50|$|In June 2011, France's Tera 100 was {{certified}} {{the fastest}} supercomputer in Europe, and ranked 9th {{in the world}} at the time. It was the first <b>petascale</b> supercomputer designed andbuilt in Europe.|$|E
