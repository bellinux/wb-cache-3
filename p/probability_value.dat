750|2014|Public
25|$|This {{variation}} of the birthday problem is interesting because {{there is not a}} unique solution for the total number of people m+n. For example, the usual 0.5 <b>probability</b> <b>value</b> is realized for both a 32-member group of 16 men and 16 women and a 49-member group of 43 women and 6 men.|$|E
25|$|Amerindian {{linguist}} Lyle Campbell also assigned different percentage {{values of}} probability and confidence for various proposals of macro-families and language relationships, depending on his {{views of the}} proposals' strengths. For example, the Germanic language family would receive probability and confidence percentage values of +100% and 100%, respectively. However, if Turkish and Quechua were compared, the <b>probability</b> <b>value</b> might be −95%, while the confidence value might be 95%. 0% probability or confidence would mean complete uncertainty.|$|E
2500|$|In {{statistical}} hypothesis testing, the p-value or <b>probability</b> <b>value</b> is the probability {{for a given}} statistical model that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two compared groups) {{would be the same}} as or of greater magnitude than the actual observed results. The use of p-values in {{statistical hypothesis}} testing is common in many fields of research such as physics, economics, finance, political science, psychology, biology, criminal justice, criminology, and sociology. [...] Their misuse has been a matter of considerable controversy.|$|E
40|$|In {{this paper}} we propose a general {{approach}} based on Genetic Algorithms (GAs) to evolve Hidden Markov Models (HMM). The problem appears when experts assign <b>probability</b> <b>values</b> for HMM, they use only some limited inputs. The assigned <b>probability</b> <b>values</b> might not be accurate to serve in other cases related to the same domain. We introduce an approach based on GAs to findout the suitable <b>probability</b> <b>values</b> for the HMM to be mostly correct in more cases than what have been used to assign the <b>probability</b> <b>values...</b>|$|R
30|$|The other {{ongoing process}} during the LRW {{procedure}} is the contracting process. During this process, the <b>probability</b> <b>values</b> of the visited vertices contract to some vertices. Since the graph is usually heterogeneous, some vertices (and groups of vertices) will receive higher <b>probability</b> <b>values</b> as the procedure continues. The inflation operation further enhances this contracting effect. The degrees of a vertex {{and its surrounding}} vertices determine whether the <b>probability</b> <b>values</b> concentrate to or diffuse from these vertices. Some vertices, normally {{the center of a}} star structure, receive larger <b>probability</b> <b>values</b> than others. We call these vertices attractor vertices and they can be used to represent the structure of a graph.|$|R
40|$|In this work, {{a simpler}} {{algorithm}} for computing <b>probability</b> <b>values</b> of a Chi-square (χ 2) random variable X is provided. This algorithm accurately provides comprehensive χ 2 <b>probability</b> <b>values</b> {{for all the}} range of X at specified degrees of freedom without further need to extrapolate before some desired χ 2 <b>probability</b> <b>values</b> are obtained {{as a result of}} their non-availability in some of the currently available statistical tables. It is further demonstrated that, the p-values for any χ 2 -tests could be readily obtained from the χ 2 <b>probability</b> <b>values</b> computed by our method. Key Words:χ 2 -tests; P-value; Normal approximation; power divergent goodness-of-fit test; Quarti...|$|R
2500|$|Turns {{are also}} {{evaluated}} in four-residue windows, but are calculated using a multi-step procedure because many turn regions contain amino acids that could also appear in helix or sheet regions. Four-residue turns also {{have their own}} characteristic amino acids; proline and glycine are both common in turns. A turn is predicted only if the turn probability {{is greater than the}} helix or sheet probabilities and a <b>probability</b> <b>value</b> based on the positions of particular amino acids in the turn exceeds a predetermined threshold. The turn probability p(t) is determined as: ...|$|E
50|$|For {{experimental}} setups of chemisorption, {{the amount}} of adsorption of a particular system is quantified by a sticking <b>probability</b> <b>value.</b>|$|E
50|$|Using the {{scientific}} method of falsification, the <b>probability</b> <b>value</b> that the sample statistic is sufficiently different from the null-model than {{can be explained by}} chance alone is given prior to the test. Most statisticians set the prior <b>probability</b> <b>value</b> at 0.05 or 0.1, which means if the sample statistics diverge from the parametric model more than 5 (or 10) times out of 100, then the discrepancy is unlikely to be explained by chance alone and the null-hypothesis is rejected. Statistical models provide exact outcomes of the parametric and estimates of the sample statistics. Hence, the burden of proof rests in the sample statistics that provide estimates of a statistical model. Statistical models contain the mathematical proof of the parametric values and their probability distributions.|$|E
40|$|Exact, resampling, and Pearson type III {{permutation}} {{methods are}} provided to compute <b>probability</b> <b>values</b> for Piccarreta's nominal-ordinal index of association. The resampling permutation method provides good approximate <b>probability</b> <b>values</b> {{based on the}} proportion of resampled test statistic values {{equal to or greater}} than the observed test statistic value. contingency tables, nominal-ordinal association, resampling, permutation, probability,...|$|R
3000|$|The {{results show}} that the Gaussian {{assumption}} is not realistic. <b>Probability</b> <b>values</b> below 0.15 were obtained on the average. Moreover, even in the best measures, where the Gaussianity fits the most, <b>probability</b> <b>values</b> range from 0.582 to 0.884 depending on the significance level α. For the sake of completeness, Figure 4 b plots the ordered values of [...]...|$|R
40|$|Drawing Description Claims Patent number: 7236629 Filing date: 8 Apr 2003 Issue date: 26 Jun 2007 Application number: 10 / 408, 316 A {{method of}} {{detecting}} a region having predetermined colour characteristics in an image comprises transforming colour values of pixels {{in the image}} from a first colour space to a second colour space, using the colour values in the second colour space to determine <b>probability</b> <b>values</b> expressing a match between pixels and the predetermined colour characteristics, where the <b>probability</b> <b>values</b> range over a multiplicity of <b>values,</b> using said <b>probability</b> <b>values</b> to identify pixels at least approximating to said predetermined colour characteristics, grouping pixels which at least approximate to said predetermined colour characteristics, and extracting information about each group, wherein pixels are weighted according to the respective multiplicity of <b>probability</b> <b>values,</b> and the weightings are used when grouping the pixels and/or when extracting information about a group...|$|R
5000|$|In {{the late}} 1950s, he invented {{probabilistic}} languages and their associated grammars. [...] A probabilistic language assigns a <b>probability</b> <b>value</b> to every possible string. [...] Generalizing {{the concept of}} probabilistic grammars led him to his discovery in 1960 of Algorithmic Probability and General Theory of Inductive Inference.|$|E
5000|$|Different {{psychological}} {{models have}} tried to predict people's beliefs {{and some of them}} try to estimate the exact probabilities of beliefs. For example, Robert Wyer developed a model of subjective probabilities. When people rate the likelihood of a certain statement (e.g., [...] "It will rain tomorrow"), this rating {{can be seen as a}} subjective <b>probability</b> <b>value.</b> The subjective probability model posits that these subjective probabilities follow the same rules as objective probabilities. For example, the law of total probability might be applied to predict a subjective <b>probability</b> <b>value.</b> Wyer found that this model produces relatively accurate predictions for probabilities of single events and for changes in these probabilities, but that the probabilities of several beliefs linked by [...] "and" [...] or [...] "or" [...] do not follow the model as well.|$|E
50|$|A key {{advantage}} of the AHM is that it supports individualized diagnostic score reporting using the attribute probability results. The score reports produced by the AHM have not only a total score but also detailed information about what cognitive attributes were measured by the test {{and the degree to}} which the examinees have mastered these cognitive attributes. This diagnostic information is directly linked to the attribute descriptions, individualized for each student, and easily presented. Hence, these reports provide specific diagnostic feedback which may direct instructional decisions. To demonstrate how the AHM can be used to report test scores and provide diagnostic feedback, a sample report is presented next.In the example to the right, the examinee mastered attributes A1 and A4 to A6. Three performance levels were selected for reporting attribute mastery: non-mastery (attribute <b>probability</b> <b>value</b> between 0.00 and 0.35), partial mastery (attribute <b>probability</b> <b>value</b> between 0.36 and 0.70), and mastery (attribute <b>probability</b> <b>value</b> between 0.71 and 1.00). The results in the score report reveal that the examinee has clearly mastered four attributes, A1 (basic arithmetic operations), A4 (skills required for substituting values into algebraic expressions), A5 (the skills of mapping a graph of a familiar function with its corresponding function), and A6 (abstract properties of functions). The examinee has not mastered the skills associated with the remaining five attributes.|$|E
30|$|Use Laplace {{mechanism}} to assign proper <b>probability</b> <b>values</b> {{in the final}} published tree [12].|$|R
40|$|Monte Carlo {{resampling}} {{methods to}} obtain <b>probability</b> <b>values</b> for chi-squared and likelihood-ratio test statistics for multiway contingency tables are presented. A resampling algorithm provides random arrangements of cell frequencies in a multiway contingency table, given fixed marginal frequency totals. <b>Probability</b> <b>values</b> are {{obtained from the}} proportion of resampled test statistic values {{equal to or greater}} than the observed test statistic value...|$|R
40|$|AbstractSet <b>valued</b> <b>probability</b> {{and fuzzy}} <b>valued</b> <b>probability</b> theory {{is used for}} {{analyzing}} and modeling highly uncertain probability systems. In this paper the set <b>valued</b> <b>probability</b> and fuzzy <b>valued</b> <b>probability</b> are defined over the measurable space. They are derived from a set and fuzzy valued measure using restricted arithmetics. The range of set <b>valued</b> <b>probability</b> is the set of subsets of the unit interval {{and the range of}} fuzzy <b>valued</b> <b>probability</b> is the set of fuzzy sets of the unit interval. The expectation with respect to set valued and fuzzy <b>valued</b> <b>probability</b> is defined and some properties are discussed. Also, the fuzzy model is applied to binomial model {{for the price of a}} risky security...|$|R
50|$|Algorithmic {{probability}} is a mathematically formalized {{combination of}} Occam's razor, and the Principle of Multiple Explanations.It is a machine independent method of assigning a <b>probability</b> <b>value</b> to each hypothesis (algorithm/program) that explains a given observation, with the simplest hypothesis (the shortest program) having the highest probability and the increasingly complex hypotheses receiving increasingly small probabilities.|$|E
50|$|This {{variation}} of the birthday problem is interesting because {{there is not a}} unique solution for the total number of people m + n. For example, the usual 0.5 <b>probability</b> <b>value</b> is realized for both a 32-member group of 16 men and 16 women and a 49-member group of 43 women and 6 men.|$|E
5000|$|Fuzzy logic. One of {{the first}} {{extensions}} of simply using rules to represent knowledge was also to associate a probability with each rule. So, not to assert that Socrates is mortal, but to assert Socrates may be mortal with some <b>probability</b> <b>value.</b> Simple probabilities were extended in some systems with sophisticated mechanisms for uncertain reasoning and combination of probabilities.|$|E
3000|$|... is a {{normalizing}} constant. Because the <b>probability</b> <b>values</b> {{are only}} compared in the slice sampling, we can simply set Z [...]...|$|R
40|$|Probabilistic {{reasoning}} systems combine different probabilistic {{rules and}} probabilistic facts {{to arrive at}} the desired <b>probability</b> <b>values</b> of consequences. In this paper we describe the MESA-algorithm (Maximum Entropy by Simulated Annealing) that derives a joint distribution of variables or propositions. It takes into account the reliability of <b>probability</b> <b>values</b> and can resolve conflicts between contradictory statements. The joint distribution is represented in terms of marginal distributions and therefore allows to process large inference networks and to determine desired <b>probability</b> <b>values</b> with high precision. The procedure derives a maximum entropy distribution subject to the given constraints. It can be applied to inference networks of arbitrary topology and may be extended into a number of directions. Comment: Appears in Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (UAI 1992...|$|R
3000|$|Finally, s {{values and}} t values are defined for each methods as logodds which {{are derived from}} these (approximated) <b>probability</b> <b>values,</b> [...]...|$|R
5000|$|... {{seems to}} capture the notion of {{allowable}} generator neighbors. That is, Regularity is {{the law of the}} stimulus domain defining, via a bond table, what neighbors are acceptable for a generator. It is the laws of the stimulus domain. Later, we will relax regularity from a boolean function to a <b>probability</b> <b>value,</b> it would capture what stimulus neighbors are probable.|$|E
5000|$|The use of Beta {{distributions}} in Bayesian inference {{is due to}} {{the fact}} that they provide a family of conjugate prior probability distributions for binomial (including Bernoulli) and geometric distributions. The domain of the beta distribution can be viewed as a probability, and in fact the beta distribution is often used to describe the distribution of a <b>probability</b> <b>value</b> p: ...|$|E
50|$|Yet another {{explanation}} {{offered by the}} authors {{is the fact that}} in the case of frequency, people often come across them multiple times and have a sequential input, compared to a <b>probability</b> <b>value,</b> which is given in one time. From John Medina’s Brain Rules, sequential input can lead to a stronger memory than a onetime input. This can be a primary reason why humans choose frequency encounters over probability.|$|E
40|$|Abstract. — Assessment of the {{reliability}} of a given phylogenetic hypothesis {{is an important step}} in phylogenetic analysis. Historically, the nonparametric bootstrap procedure has been the most frequently used method for assessing the support for specific phylogenetic relationships. The recent employment of Bayesian methods for phylogenetic inference problems has resulted in clade support being expressed in terms of posterior probabilities. We used simulated data and the four-taxon case to explore the relationship between nonparametric bootstrap values (as inferred by maximum likelihood) and posterior probabilities (as inferred by Bayesian analysis). The results suggest a complex association between the two measures. Three general regions of tree space can be identified: (1) the neutral zone, where differences between mean bootstrap and mean posterior <b>probability</b> <b>values</b> are not significant, (2) near the two-branch corner, and (3) deep in the two-branch corner. In the last two regions, significant differences occur between mean bootstrap and mean posterior <b>probability</b> <b>values.</b> Whether bootstrap or posterior <b>probability</b> <b>values</b> are higher depends on the data in support of alternative topologies. Examination of star topologies revealed that both bootstrap and posterior <b>probability</b> <b>values</b> differ significantly from theoretical expectations...|$|R
40|$|AbstractVoxel-based morphometry (VBM) is {{commonly}} used to study systematic differences in brain morphology from patients with various disorders, usually by comparisons with control subjects. It has often been suggested, however, that VBM is also sensitive to variations in composition in grey matter. The nature of the grey matter changes identified with VBM is still poorly understood. The aim {{of the current study}} was to determine whether grey matter histopathological measurements of neuronal tissue or gliosis influenced grey matter <b>probability</b> <b>values</b> that are used for VBM analyses. Grey matter <b>probability</b> <b>values</b> (obtained using both SPM 5 and FSL-FAST) were correlated with neuronal density, and field fraction of NeuN and GFAP immunopositivity in a grey matter region of interest in the middle temporal gyrus, in 19 patients undergoing temporal lobe resection for refractory epilepsy. There were no significant correlations between any quantitative neuropathological measure and grey matter <b>probability</b> <b>values</b> in normal-appearing grey matter using either segmentation program. The lack of correlation between grey matter <b>probability</b> <b>values</b> and the cortical neuropathological measures in normal-appearing grey matter suggests that intrinsic cortical changes of the type we have measured do not influence grey matter probability maps used for VBM analyses...|$|R
30|$|The agent selects {{an action}} {{probabilistically}} based on Boltzmann distribution defined using Q <b>value</b> <b>probabilities</b> associating states with actions [139]. The Q values {{are associated with}} policy of actions and thus justifying the use of provenance path <b>probability</b> <b>values</b> in determining action probabilities.|$|R
5000|$|Bookmark Method (item-centered): Items {{in a test}} (or a {{representative}} subset of items) are ordered by difficulty (e.g., IRT response <b>probability</b> <b>value)</b> from easiest to hardest. SMEs place a [...] "bookmark" [...] in the [...] "ordered item booklet" [...] such that {{a student at the}} threshold of a performance level would be expected to respond successfully to the items prior to the bookmark with a likelihood equal to or greater than the specified response <b>probability</b> <b>value</b> (and with a likelihood less than that value for items after the bookmark). For example, for a response probability of [...]67 (RP67) SMEs would place a bookmark such that an examinee at the threshold of the performance level would have at least a 2/3 likelihood of success on items prior to the bookmark and less than a 2/3 likelihood of success on the items after the bookmark“ This method is considered efficient with respect to setting multiple cut scores on a single test and can be used with tests composed of multiple item types (e.g., multiple-choice, construct response, etc.).|$|E
50|$|Amerindian {{linguist}} Lyle Campbell also assigned different percentage {{values of}} probability and confidence for various proposals of macro-families and language relationships, depending on his {{views of the}} proposals strengths. For example, the Germanic language family would receive probability and confidence percentage values of +100% and 100%, respectively. However, if Turkish and Quechua were compared, the <b>probability</b> <b>value</b> might be −95%, while the confidence value might be 95%. 0% probability or confidence would mean complete uncertainty.|$|E
50|$|The next theorem {{will be for}} AVCs with {{randomized}} code. For such AVCs {{the code}} is a random variable with values {{from a family of}} length-n block codes, and these codes are not allowed to depend/rely on the actual value of the codeword. These codes have the same maximum and average error <b>probability</b> <b>value</b> for any channel because of its random nature. These types of codes also help to make certain properties of the AVC more clear.|$|E
50|$|Probabilistic value sets is {{a natural}} {{extension}} of value sets to deductive <b>probability.</b> The <b>value</b> set construct holds the information required to calculate <b>probabilities</b> of calculated <b>values</b> based on <b>probabilities</b> of initial <b>values.</b>|$|R
30|$|In {{order to}} ensure {{comparability}} of method results, their scores are converted to <b>probabilities</b> as <b>probability</b> <b>values</b> are comparable and combinable with each other.|$|R
5000|$|Plausible {{reasoning}} admits {{of degrees}} by testing, {{but of a}} kind different from those the standard <b>probability</b> <b>values</b> and Bayesian rules used in Pascalian probability ...|$|R
