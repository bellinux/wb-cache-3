27|3|Public
50|$|ODRs {{have also}} been shown to be {{effective}} in determining where students fall within a three-leveled model (Sugai et al., 2000), developing professional development as well as helping coordinate school efforts with other community agencies (Tobin & Sugai, 1997; Tobin, Sugai, & Colvin, 2000), predicting school failure in older grades as well as delinquency (Sprague et al., 2001), indicating types of behavior resulting in referrals (Putnam et al., 2003), and determination of the effectiveness of <b>precorrection</b> techniques (Oswald, Safran, & Johanson, 2005). Analyzing discipline referral data can also help school personnel identify where to improve ecological arrangements within a school and to recognize how to increase active supervision in common areas (Nelson, Martella, & Galand, 1998; Nelson et al., 2002). A limitation of only using ODRs to measure behavior problems is that they {{have been found to be}} ineffective at measuring internalizing behavior problems such as anxiety, depression, and withdrawal.|$|E
50|$|Secondary {{prevention}} strategies involve students (i.e., 10-15% of {{the school}} population) who {{do not respond to}} the primary prevention strategies and are at risk for academic failure or behavior problems but are not in need of individual support (Nelson, et al., 2002). Interventions at the secondary level often are delivered in small groups to maximize time and effort and should be developed with the unique needs of the students within the group. Examples of these interventions include social support such as social skills training (e.g., explicit instruction in skill-deficit areas, friendship clubs, check in/check out, role playing) or academic support (i.e., use of research-validated intervention programs and tutoring). Additionally, secondary programs could include behavioral support approaches (e.g., simple Functional Behavioral Assessments FBA, <b>precorrection,</b> self-management training). Even with the heightened support within secondary level interventions, some students (1-7%) will need the additional assistance at the tertiary level (Walker et al., 1996).|$|E
5000|$|Primary {{prevention}} strategies {{focus on}} interventions {{used on a}} school-wide basis for all students (Sugai & Horner, 2002). PBS (positive behavioral supports) use for other than a designated population group has neither {{been approved by the}} professions or the public-at-large.This level of prevention is considered [...] "primary" [...] because all students are exposed in the same way, and at the same level, to the intervention. The primary prevention level is the largest by number. Approximately 80-85% of students who are not at risk for behavior problems respond in a positive manner to this prevention level. Primary prevention strategies include, but are not limited to, using effective teaching practices and curricula, explicitly teaching behavior that is acceptable within the school environment, focusing on ecological arrangement and systems within the school, consistent use of <b>precorrection</b> procedures, using active supervision of common areas, and creating reinforcement systems that are used on a school-wide basis (Lewis, Sugai, & Colvin, 1998; Martella & Nelson, 2003; Nelson, Crabtree, Marchand-Martella & Martella, 1998; Nelson, Martella, & Marchand-Martella, 2002).|$|E
30|$|Specific {{procedures}} {{useful to}} enhance the performance of such stereo-correlation analyses have been developed. In particular, the combination of <b>precorrections</b> and global digital image correlation algorithm provides both robust and accurate 2 D displacement fields that are suitable to stereo-correlation to get three-dimensional displacement fields. Other directions for improving the experiment itself are currently being investigated, namely, surface marking, which can sustain the very strain rates of the specimen, submicrosecond lighting, and digital cameras [27, 28] that will in a near future replace the film camera {{used in the present}} study. In the same spirit, combining more than two views of the same scene is a stimulating direction for increasing the accuracy of the reconstruction even with images having a low contrast or specular reflections that may always occur unexpectedly [14, 29, 30]. Stereovision is thus a very powerful tool to analyze high-speed experiments.|$|R
40|$|Teachers' {{classroom}} management skills {{have a significant}} impact on students' short and long-term outcomes. Unfortunately, research has demonstrated that teachers, particularly novice teachers, often struggle to effectively manage classrooms and student behavior. Teacher consultation, which is defined as working one-on-one with a teacher in order to build a specific set of skills in order to best meet the needs of students, has emerged in the literature as method for enhancing teacher skills. This study examined the integration of a {{classroom management}} consultation model, the Classroom Check-up, into the student teaching component of a pre-service education program for four participants. Impacts on teacher and student behaviors were examined using a multiple baseline design. Results indicated positive teacher behavior change related to praise, opportunities to respond, <b>precorrections,</b> reprimands, and time spent teaching and decreased student disruptive behaviors. Overall, both student teacher and student behaviors were positively impacted by participation in the Classroom Check-up consultation model...|$|R
40|$|Abstract—In {{computed}} tomography (CT), the nonlinear at-tenuation characteristics of polychromatic X-rays cause beam hardening artifacts in the reconstructed images. State-of-the-art methods {{to correct the}} beam hardening effect are mostly single material <b>precorrections</b> (e. g. water-precorrection), which are far less efficient when more than one material {{is present in the}} field of measurement. The use of those techniques is limited by specific restrictions to the objects, computational loads, and inaccurate segmentations. In this paper, we present a practical multi-material beam hardening correction(MMBHC) approach that employs material decomposition technique maintaining CT values from dual-energy CT. This separates single energy CT images into spatial density images and images containing material information. The segmentation maintains the original X-ray attenuation coefficients, such that the original CT attenuation image can be exactly recovered. Therefore, segmentation errors, which result in invalid material properties to the voxel, only have minor effects on the beam hardening correction and do not cause an atypical image impression or introduce additional artifacts. A multi-material beam hardening correction procedure is formulated to iteratively correct the artifacts but shows satisfactory image quality after the first iteration. Based on experiments with simulated CT data, it is shown that the proposed method can efficiently reduce beam hardening artifacts. In addition to the performance benefits, our approach can be flexibly applied to imaging geometries and achieve efficient, fully 3 D reconstructions. Index Terms—CT reconstruction, beam hardening, artifact reduction, segmentation, spatial resolution, image qualit...|$|R
40|$|There is an {{evidence}} base {{supporting the}} use of positive behavior supports in schools; however effectively and efficiently transferring these interventions into classroom settings remains a challenge. <b>Precorrection</b> is a highly-regarded behavior support strategy that relies on antecedent prompting to reduce problem behavior and teach socially appropriate skills. This study examined how a brief training in <b>precorrection</b> and praise paired with regular feedback impacted the behavior of four Title I elementary school teachers and students. As {{a result of the}} intervention, the four teachers increased use of <b>precorrection</b> and praise, while concomitantly reducing their use of reprimands. Limitations and suggestions for future research are provided...|$|E
40|$|Abstract — In this paper, {{methods for}} the design of {{time-varying}} finite impulse response (FIR) filters which facilitate the correction of time-varying systems are presented. The two possible cases where an undesired time-varying system requires either prepro-cessing or postprocessing are considered. In particular, we show how the methods from [1], [2] can be used for the <b>precorrection</b> of time-varying systems. Though, these presented methods allow for the correction of systems exhibiting periodically recurring time-varying behavior, these techniques can also be applied to the non-periodic case. A mixed-signal application is presented to verify the viability of the presented <b>precorrection</b> case. To this end, a system model for non-uniform zero-order-hold (ZOH) signals in a digital-to-analog converter (DAC) is introduced. Finally, numerical simulations are performed to illustrate the increase of performance when the proposed <b>precorrection</b> is employed. I...|$|E
40|$|Three {{methods of}} color {{processing}} for HDTV-conventional, linear, and CIELAB coding-have been investigated, considering different color primaries, different characteristics for <b>precorrection,</b> and {{the visibility of}} noise and quantization errors. The CIELAB codec with REGEBE primaries situated on the spectrum locus, <b>precorrection</b> with a gamma gamma = 0. 33, and a contrast ratio KS= 100 {{turned out to be}} the best solution among the three systems because it provides a wide color gamut, a good uniformity for both noise and quantization errors, and constant luminance...|$|E
40|$|In {{dairy cattle}} {{evaluation}} procedures models which directly consider records of individual testdays have become of interest. In a broad sense, all models which incorporate records from single testdays {{can be defined}} as testday models. This incorporation may consist of a <b>precorrection</b> of individual testday records for fixed effects and then combining these records for the use i...|$|E
40|$|Abstract—This paper {{presents}} correction structures compen-sating for {{the impact}} of linear weakly time-varying systems on bandlimited signals. The proposed structures can be used for the postprocessing and preprocessing of linear weakly time-varying systems which complements and generalizes the literature on iterative postcorrection methods. Furthermore, the <b>precorrection</b> of mixed-signal systems is discussed by investigating the compu-tationally efficient preprocessing of nonuniform zero-order-hold signals in digital-to-analog converters. Index Terms—ADC, correction structures, DAC, time-varying system. I...|$|E
40|$|International audience—The passive time-reversal {{beamformer}} is a 1 ports {{device with}} orthogonal channels. Beamforming is achieved using simultaneous time reversal {{focusing on the}} output ports with the only input waveform. Utilizing this method, an antenna array can be supplied with focused signals of controlled complex weights and achieve beam steering. A <b>precorrection</b> method is developed in this letter to normalize the weights of the output signals at the focusing time, avoiding a long optimization step of beamformer designing. The theory is developed in this letter, followed by experimental validation...|$|E
40|$|Objectives. To {{assess whether}} audio taping {{simulated}} patient interactions {{can improve the}} reliability of manually documented data and result in more accurate assessments. Methods. Over a 3 -month period, 1340 simulated patient visits were made to community pharmacies. Following the encounters, data gathered by the simulated patient were relayed to a coordinator who completed a rating form. Data recorded on the forms were later compared to an audiotape of the interaction. Corrections were tallied and reasons for making them were coded. Results. Approximately 10 % of cases required corrections, resulting in a 10 %- 20 % modification in the pharmacy’s total score. The difference between postcorrection and <b>precorrection</b> scores was significant. Conclusions. Audio taping simulated patient visits enhances data integrity. Most corrections were re-quired because of the simulated patients ’ poor recall abilities...|$|E
40|$|Abstract—A {{quantitative}} {{model for}} interferogram data {{collected in a}} quantum-limited hyperspectral imaging system is derived. This model accounts for the geometry of the interferometer, the Poisson noise, and the parameterization of {{the mean of the}} noise in terms of the autocorrelation function of the incident optical signal. The Cramér–Rao bound on the variance of unbiased spectrum estimates is derived and provides an explanation for what is often called the “multiplex disadvantage ” in interferometer-based methods. Three spectrum estimation algorithms are studied: maximum likelihood via the expectation-maximization (EM) algorithm, least squares (LS), and the fast Fourier transform (FFT) with data <b>precorrection.</b> Extensive simulation results reveal advantages and disadvantages with all three methods in different signal-to-noise ratio (SNR) regimes. Index Terms—Cramer–Rao bound, EM algorithm, interferograms, interferometry, multiplex disadvantage, spectrum estimation. I...|$|E
40|$|Abstract — Many {{conventional}} PET emission scans are corrected for accidental coincidence (AC) events, or randoms, by real-time subtraction of delayed-window coincidences, {{leaving only}} the randoms-precorrected data available for image reconstruction. The real-time <b>precorrection</b> compensates in mean for AC events but destroys Poisson statistics. Since the exact log-likelihood for randoms-precorrected data is inconvenient to maximize, practical approximations are desirable for statistical image reconstruction. Conventional approximations involve setting negative sinogram values to zero, which can induce positive systematic biases, particularly for scans with low counts per ray. We propose new likelihood approximations that allow negative sinogram values without requiring zero-thresholding. We also develop monotonic algorithms for the new models by using “optimization transfer ” principles. Simulation results show that our new model, SP −,isfreeofsystematicbiasyetkeepslow variance. Despite its simpler implementation, the new model performs comparably to the saddle-point (SD) model which has previously shown the best performance (as to systematic bias and variance) in randoms-precorrected PET emission reconstruction. I...|$|E
40|$|Orthogonal Frequency DivisionMulon- 4 (OFDM) modul-E used in DAB, DVB-T {{and future}} 4 G {{communications}} standards arehighl {{sensitive to the}} presence of non-l-E 4 distortion and synchronization errors between the transmitted and receivedsignal 4 Digital <b>precorrection</b> schemes can beappl 4 for the compensation of the AM/AM and AM/PM distortion introduced by High Power AmplS-E (HPA). However, thisl- 4 -E 4 process cannot be real successful unlce the path del y introduced by the anal chains, required for the observation of the HPA output, is previousl estimated. A coarse time del y estimation (TDE) alE) - robust {{to the presence of}} unknown non-l-E 4 Sis presented in this paper. The proposed technique provides the basis for a fast time ale- 4 t of base-bandsignal within a range of several samplm with a discrete resol-E Anyresidual timing o#set can be then estimated and compensated by defining some anal 4 criteria over the obtained TDE spectrum...|$|E
40|$|Many {{conventional}} PET emission scans are corrected for accidental coincidence (AC) events, or randoms, by real-time subtraction of delayed-window coincidences, {{leaving only}} the randoms-precorrected data available for image reconstruction. The real-time <b>precorrection</b> compensates in mean for AC events but destroys Poisson statistics. Since the exact log-likelihood for randoms-precorrected data is inconvenient to maximize, practical approximations are desirable for statistical image reconstruction. Conventional approximations involve setting negative sinogram values to zero, which can induce positive systematic biases, particularly for scans with low counts per ray. We propose new likelihood approximations that allow negative sinogram values without requiring zero-thresholding. We also develop monotonic algorithms for the new models by using "optimization transfer" principles. Simulation results show that our new model, SP-, is free of systematic bias yet keeps low variance. Despite its simpler implementation, the new model performs comparably to the saddle-point (SD) model which has previously shown the best performance (as to systematic bias and variance) in randoms-precorrected PET emission reconstruction...|$|E
40|$|This {{dissertation}} {{investigated the}} effects of a peer coaching relationship between a special education teacher and two general education teachers. More specifically, a two-tier multiple baseline design across subjects was used to evaluate {{the effects of}} peer coaching on the general education teachers 2 ̆ 7 use of effective instructional practices (EIPs) and subsequent effects on the engagement rate and academic performance of students with and without disabilities. The peer coaching process included modeling, direct support, and feedback on the use of effective instructional practices including getting student attention, giving specific directions, asking specific questions with wait time, contingent positive reinforcement, positive error correction, <b>precorrection,</b> prompting, and proximity control. A 30 -second partial interval recording procedure was used to observe the general education teachers 2 ̆ 7 use of effective instructional practices and student engagement rates. Student participants 2 ̆ 7 academic performance was measured using weekly quizzes. ^ Peer coaching resulted in an overall increase in the teachers 2 ̆ 7 use of EIPs. One general education teacher had a 30...|$|E
40|$|We {{propose a}} novel {{segmentation}} method based on regional and nonlocal information {{to overcome the}} impact of image intensity inhomogeneities and noise in human brain magnetic resonance images. With the consideration of the spatial distribution of different tissues in brain images, our method does not need preestimation or <b>precorrection</b> procedures for intensity inhomogeneities and noise. A nonlocal information based Gaussian mixture model (NGMM) is proposed to reduce the effect of noise. To reduce the effect of intensity inhomogeneity, the multigrid nonlocal Gaussian mixture model (MNGMM) is proposed to segment brain MR images in each nonoverlapping multigrid generated by using a new multigrid generation method. Therefore the proposed model can simultaneously overcome the impact of noise and intensity inhomogeneity and automatically classify 2 D and 3 D MR data into tissues of white matter, gray matter, and cerebral spinal fluid. To maintain the statistical reliability and spatial continuity of the segmentation, a fusion strategy is adopted to integrate the clustering results from different grid. The experiments on synthetic and clinical brain MR images demonstrate the superior performance of the proposed model comparing with several state-of-the-art algorithms...|$|E
40|$|Most {{positron}} {{emission tomography}} (PET) emission scans are corrected for accidental coincidence (AC) events by real-time subtraction of delayed-window coincidences, leaving only the randoms-precorrected data available for image reconstruction. The real-time randoms <b>precorrection</b> compensates in mean for AC events but destroys the Poisson statistics. The exact log-likelihood for randoms-precorrected data is inconvenient, so practical approximations are needed for maximum likelihood or penalized-likelihood image reconstruction. Conventional approximations involve setting negative sinogram values to zero, which can induce positive systematic biases, particularly for scans with low counts per ray. We propose new likelihood approximations that allow negative sinogram values without requiring zero-thresholding. With negative sinogram values, the log-likelihood functions can be nonconcave, complicating maximization; nevertheless, we develop monotonic algorithms for the new models by modifying the separable paraboloidal surrogates and the maximum-likelihood expectation-maximization (ML-EM) methods. These algorithms ascend to local maximizers of the objective function. Analysis and simulation {{results show that the}} new shifted Poisson (SP) model is nearly free of systematic bias yet keeps low variance. Despite its simpler implementation, the new SP performs comparably to the saddle-point model which has shown the best performance (as to systematic bias and variance) in randoms-precorrected PET emission reconstruction...|$|E
40|$|Purpose. To reduce beam {{hardening}} {{artifacts in}} CT {{in case of}} an unknown X-ray spectrum and unknown material properties. Methods. We assume that the object can be segmented into a few materials with different attenuation coefficients, and parameterize the spectrum using a small num-ber of energy bins. The corresponding unknown spectrum parameters and material attenuation values are estimated by minimizing the difference between the measured sinogram data and a simulated polychromatic sinogram. Three iterative algorithms are derived from this approach: two reconstruction algorithms IGR and IFR, and one sinogram <b>precorrection</b> method ISP. Results. The methods are applied on real X-ray data of a high and a low-contrast phantom. All three methods successfully reduce the cupping artifacts caused by the beam polychromaticity {{in such a way that}} the reconstruction of each homogeneous region is to good accuracy homogeneous, even in case the segmentation of the pre-liminary reconstruction image is poor. In addition, the results show that the three methods tolerate relatively large variations in uniformity within the segments. Conclusions. We show that even without prior knowledge about materials or spec-trum, effective beam hardening correction can be obtained. 1 I...|$|E
40|$|Abstract — Most PET {{emission}} scans are corrected for accidental coincidence (AC) events by real-time subtraction of delayedwindow coincidences, {{leaving only}} the randoms-precorrected data available for image reconstruction. The real-time randoms <b>precorrection</b> compensates in mean for AC events but destroys the Poisson statistics. The exact log-likelihood for randomsprecorrected data is inconvenient, so practical approximations are needed for maximum likelihood or penalized-likelihood image reconstruction. Conventional approximations involve setting negative sinogram values to zero, which can induce positive systematic biases, particularly for scans with low counts per ray. We propose new likelihood approximations that allow negative sinogram values without requiring zero-thresholding. With negative sinogram values, the log-likelihood functions can be non-concave, complicating maximization; nevertheless, we develop monotonic algorithms for the new models by modifying the separable paraboloidal surrogates (SPS) and the maximum likelihood expectation maximization (ML-EM) methods. These algorithms ascend to local maximizers of the objective function. Analysis and simulation {{results show that the}} new shifted Poisson (SP) model is nearly free of systematic bias yet keeps low variance. Despite its simpler implementation, the new SP performs comparably to the saddle-point (SD) model which has shown the best performance (as to systematic bias and variance) in randoms-precorrected PET emission reconstruction. Index Terms — positron emission tomography (PET), randomsprecorrected PET, accidental coincidences, maximum likelihood reconstruction I...|$|E
40|$|Spatially {{structured}} optical fields {{have been}} used to enhance the functionality {{of a wide variety of}} systems that use light for sensing or information transfer. As higher-dimensional modes become a solution of choice in optical systems, it is important to develop channel models that suitably predict the effect of atmospheric turbulence on these modes. We investigate the propagation of a set of orthogonal spatial modes across a free-space channel between two buildings separated by 1. 6 km. Given the circular geometry of a common optical lens, the orthogonal mode set we choose to implement is that described by the Laguerre-Gaussian (LG) field equations. Our study focuses on the preservation of phase purity, which is vital for spatial multiplexing and any system requiring full quantumstate tomography. We present experimental data for the modal degradation in a real urban environment and draw a comparison to recognized theoretical predictions of the link. Our findings indicate that adaptations to channel models are required to simulate the effects of atmospheric turbulence placed on high-dimensional structured modes that propagate over a long distance. Our study indicates that with mitigation of vortex splitting, potentially through <b>precorrection</b> techniques, one could overcome the challenges in a real point-to-point free-space channel in an urban environment...|$|E
30|$|In this part, the stereovision {{technique}} {{is applied to}} analyze a cylinder expansion caused by blast loading. It {{is worth noting that}} other types of loading configurations have been used in the literature [13, 20]. First, a synthetic case representative of the experiment is studied to estimate the performances of the technique and in particular the resolution of the reconstruction. This enables for the evaluation of the minimum size of observable and quantifiable defects. In the present experiments, the observed surface undergoes important deformations (beyond 100 % strain). This is the reason why the computation is not carried out with the initial reference but rather with an updated reference that causes a cumulation of measurement errors. A reduction {{in the size of the}} reconstructed surface is observed since the points that leave the initial region of interest are not taken into account. To improve the performances of the approach, a <b>precorrection</b> for large displacements is performed. It consists in seeking a uniform translation to apply to the images so that, on average, the region of interest is motionless. Then the DIC algorithm is run using the prior translation as an initialization of the displacement field. This procedure makes the computation faster, more stable and more accurate. Finally, the stereovision {{technique is}} applied to the experiment itself [21 – 24].|$|E
40|$|Abstract: e-Learning {{has become}} a major field of {{interest}} in recent years, and multiple approaches and solutions have been developed. A typical form of e-learning application comprises exercise submission and assessment systems that allow students to work on assignments whenever and where they want (i. e., dislocated, asynchronous work). In basic computer science courses, programming exercises are widely used and courses usually have {{a very large number of}} participants. However, there is still no efficient way for supporting tutors to correct these exercises, as experience has shown that correction (and, beyond that, automatic grading) are difficult and time consuming. In this paper we present an enhancement of the xLx platform developed at the University of Muenster to efficiently support tutors in handling Java programming exercises electronically. The new component is based on concepts of automatic static and dynamic testing approaches, well known from software engineering, and provides an automatic <b>precorrection</b> of submitted solutions. In addition, a tutor is able to annotate solutions manually, by adding comments that are associated with the source code of the solution in an intelligent way. Static tests are based on a compilation of the sources to find syntactical errors, while dynamic tests use test cases defined by tutors during the creation of the exercises and have to be executed correctly on the solutions in order to receive credits for the exercises...|$|E
40|$|Digital breast tomosynthesis is {{a recent}} three {{dimensional}} imaging modality that allows visualization of the breast as a stack of parallel slices. When compared to projection mammography, tomosynthesis is preferred for visualizing mass lesions while mammography is preferred for microcalcifications. In clinical evaluations, the diagnostic accuracy of tomosynthesis {{is at least as}} good as that of mammography, and both modalities combined outperform mammography used alone. Technical evaluations show that iterative reconstruction methods perform better than filtered backprojection reconstruction, which was used in most of the clinical evaluations. Therefore, the goal of this work was to design and evaluate a maximum a posteriori reconstruction algorithm for digital breast tomosynthesis with a focus on the visualization of microcalcifications. The first step was to implement a sequence of preprocessing steps to account for the typical assumptions of mono-energetic and scatter-free data acquisition used in iterative reconstruction. With this <b>precorrection,</b> reconstructed attenuation values of adipose breast tissue was found to be close to the expected theoretical value. A further examination of the difference between scatter <b>precorrection</b> and model based scatter correction during reconstruction was performed by evaluating the contrast to noise ratio of simulated masses in patient data. Results showed that the application of either correction method resulted in a similar contrast to noise ratio, which meant <b>precorrection</b> was preferred due to the lower computational cost. The second part of the work concentrates on developing a maximum a posteriori reconstruction algorithm for breast tomosynthesis. To improve visualization of microcalcifications, a resolution model based on the motion of the x-ray source during image acquisition was combined with a grouped coordinate ascent algorithm that sequentially updated planes parallel to the detector, each with their own position dependent parameters for the resolution model. This new method was evaluated in reconstructions of a simulated power law background containing microcalcifications and resulted in higher contrast to noise ratio when compared to iterative reconstruction without resolution model and improved detectability in a free search observer experiment when compared to filtered backprojection. One drawback of the plane-by-plane updates in this method was the need for careful initialization of the reconstruction volume in order to avoid severe limited angle artifacts. To remedy this problem and to further accelerate convergence, multigrid updates were implemented, and an update scheme was selected that combined the least amount of artifacts and the best convergence after a fixed computational cost. A further comparison was made with a popular alternative update method using ordered subsets rather than plane-by-plane updates, and found that when using an optimal multigrid sequence, both update methods resulted in similar performance. The final parts of this work focused on the evaluation of reconstruction methods. A channelized Hotelling observer was designed to detect groups of five microcalcifications in a background of acrylic spheres, and was applied to optimize the detectability of these microcalcifications {{as a function of the}} smoothing prior. The model observer correlated well with human observer evaluations of the same data, and found that detectability only varied slightly over a large range of strengths for the quadratic and combined quadratic and total variation priors. Therefore, it was not possible to pick an optimal smoothness based only on this criterion. On the other hand, with this information, the smoothing in the reconstruction could be set according to radiologist preference without worrying about calcification detectability. This model observer was then applied together with evaluations by a group of expert and non-expert human observers to compare three reconstruction algorithms for breast tomosynthesis. These were the iterative reconstruction developed in this work, the existing filtered backprojection of the Siemens Mammomat Inspiration system, and a new super-resolution filtered backprojection with post-reconstruction denoising. The evaluation consisted of a four-alternative forced-choice experiment to determine microcalcification and mass detectability in phantom data, and a visual grading study on patient data. Both new reconstruction methods showed improved performance on the lesion detection task compared to the system filtered backprojection, but resulted in significantly different overall appreciation of image quality in the visual grading study. From these results and the feedback from the radiologists that participated in the study, we can conclude that the new super-resolution filtered backprojection can replace the original system reconstruction in the clinic. Although the new iterative reconstruction improved the detectability of microcalcifications significantly, the unfamiliar properties of the images were not received as positive by the radiologists. Therefore further development of the iterative reconstruction should focus on artifact reduction and improving image contrast, and should use frequent clinical input in order to obtain a more familiar look and feel (noise pattern, contrast, …) for the radiologists. status: publishe...|$|E
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. This thesis presents an investigation into the actual electrical energy and demand use of a large metal fabrication facility located in Houston, Texas. Plant selection and the monitoring system are covered. The influence of a low power factor on energy consumption and demand is covered, including installation of correction and the effect of increasing the power factor on demand and energy consumption block sizes. The installation of capacitance correction has increased the low power factor of this facility from the low 60 % range to the mid-to-high 70 % range. A method has been developed to predict savings based on <b>precorrection</b> monitored data in the event the exact amount of capacitance installed is unknown. Savings for the month of February, 1994, are found to be $ 1327. 56. This method {{can be used as a}} diagnostic tool to determine the amount of active capacitance. In this plant, that amount was found to be 315 KVAC, which correlates reasonably well with the amount active in the plant. The monitoring installation is described, and other uses (besides that dealing with power factor correction) are covered. Those uses include monitoring plant and equipment performance and productivity, and savings due to missed opportunities for equipment turn off...|$|E
30|$|In {{this paper}} we {{consider}} communication between a source and a destination, {{and the source}} is possibly assisted with a relay node. All the channels (source to destination, source to relay and relay to destination) {{are assumed to be}} frequency selective and in order to cope with that, OFDM modulation with proper cyclic extension is used. The relay operates in a DF mode. This mode is known to be suboptimum [4, 5]. Decode-and-forward is adopted here as a relaying strategy for its simplicity and its mathematical tractability. Two protocols (P 1 and P 2) are considered. Each protocol is made of two signaling periods, named time slots. The first time slot is identical for both protocols. During this period, on each carrier, the source broadcasts a symbol. This symbol (affected by the proper channel gain) is received by the destination and the relay. The relay may retransmit the same carrier-specific symbol to the destination during the second time slot. Whether the relay does it or not will be indicated by the optimization problem which is formulated and solved in this paper. The protocol P 2 differs from the protocol P 1 in that, in the latter, the source does not transmit during the second time slot, irrespective to whether the relay is active or not during the second time slot. For P 2, on a per carrier basis, the source sends a new symbol if the relay is inactive. The reason for not having the source and the relay transmitting at the same time is to avoid the interference that would occur in this case, thus rendering the optimization problem somewhat tedious. Moreover in practice source and relay will have different carrier frequency offsets which is likely to require involved <b>precorrection</b> mechanisms. A scenario with interference will be investigated in the future.|$|E
40|$|The authors thank The Carnegie Trust for the Universities of Scotland for {{providing}} funding for Miloš Rydval’s Ph. D. The Scottish pine network expansion {{has been an}} ongoing task since 2006, and funding must be acknowledged to the following projects: EU project “Millennium” (017008 - 2), Leverhulme Trust project “RELiC: Reconstructing 8000 years of Environmental and Landscape change in the Cairngorms (F/ 00 268 /BG) ”, and the NERC project “SCOT 2 K: Reconstructing 2000 years of Scottish climate from tree rings (NE/K 003097 / 1) ”. Kevin J. Anchukaitis was also {{supported by a grant}} for the US National Science Foundation (ARC- 0902051). Nonclimatic disturbance events are an integral element in the history of forests. Although the identification of the occurrence and duration of such events may help to understand environmental history and landscape change, from a dendroclimatic perspective, disturbance can obscure the climate signal in tree rings. However, existing detrending methods are unable to remove disturbance trends without affecting the retention of long-term climate trends. Here, we address this issue by using a novel method for the detection and removal of disturbance events in tree-ring width data to assess their spatiotemporal occurrence in a network of Scots pine (Pinus sylvestris L.) trees from Scotland. Disturbance trends “superimposed” on the tree-ring record are removed before detrending and the climate signals in the <b>precorrection</b> and postcorrection chronologies are evaluated using regional climate data, proxy system model simulations, and maximum latewood density (MXD) data. Analysis of subregional chronologies from the West Highlands and the Cairngorms in the east reveals a higher intensity and more systematic disturbance history in the western subregion, likely a result of extensive timber exploitation. The method improves the climate signal in the two subregional chronologies, particularly in the more disturbed western sites. Our application of this method demonstrates {{that it is possible to}} minimise the effects of disturbance in tree-ring width chronologies to enhance the climate signal. PostprintPeer reviewe...|$|E
40|$|The {{purpose of}} the {{research}} work described in this dissertation was a explore available methods for the synthesis of networks from non-ideal elements and {{to find a new}} theory upon which a practically tractable synthesis procedure can be founded for circuits with arbitrarily distributed losses. Since the theory providing the basis for this novel design method turned out to be closely related to the insertion loss synthesis technique, a detailed explanation of the latter was required. It has been provided in Chapter I. Apart from some new derivations involving the relation between the form of the characteristic function and the circuit configuration, and the approximating synthesis procedure for filter-combinations, the fundamental theory described in the first chapter is available in the literature. The construction of realizable transfer functions for some practical requirements is treated in Chapter II. There the proof of the non-realizability of a quasielliptic response, some realizability-considerations involving elliptic filters, the Section on harmonic suppression filter-sets and the step-by-step design method for Chebyshev pass-band filters are original; the rest of Chapter II recapitulates existing theory. Chapter III provides a summary of miscellaneous analysis and synthesis methods which take into account the effects of non-ideal elements. The improved temperature-compensation procedure is new; the other results have been previously published. The derivations contained in Para. III. 2. c. have been worked out by the writer, at least one of them, however, probably duplicates Darlington's original (unpublished) calculations. The new synthesis method and its supporting theorems are described in Chapter IV. First, a large variety of formulae are obtained for the derivatives of various network parameters. These formulae are utilized in the derivation of the main theory; they are also directly applicable to circuit analysis. The formulae can be deduced from the determinant forms of the driving-point and transfer immittances. The effects of losses on the transfer characteristics are then studied indirectly by examining the changes in various parameters of the transfer function (natural frequencies, attenuation poles, proportionality factor) due to the added dissipation. Using series expansions, from the variation of these parameters the increment of the transfer function itself could be found to a first-order approximation. These results were used to find expressions, in which contributions of individual lossy elements to the distortion of the ideal characteristics were displayed. Even more important, they led to a relatively simple synthesis procedure allowing <b>precorrection</b> (often called predistortion) for the detrimental effects of individual losses. Since the calculations giving rise to the final formulae involve series expansions and terms containing powers of loss factors greater than unity are omitted, the accuracy decreases with increasing losses. Even for quality-factors in the order of 10, however, examples proved the results to be sufficiently exact for any practical purposes. Using some special properties of the lossy ladder-networks found {{in the course of this}} investigation the general expressions were simplified for this important configuration. For the import-art case of semi-homogeneous loss distribution (all coils have identical quality-factors, and so do all capacitors, but the quality-factors of the two groups differ), topological considerations were applied to achieve extremely compact formulae. Procedures to improve the accuracy for extreme cases (large losses, sharply changing response) have been developed; however these are seldom needed. Some alternative procedures applicable in special cases have also been derived. Finally, the theory was tested with excellent results by applying it to a large number of design problems, a representative cross-section of which has been included in Chapter IV to illustrate the simplicity and usefulness of the method...|$|E

