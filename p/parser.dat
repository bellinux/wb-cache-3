7773|1637|Public
5|$|Perl has a Turing-complete grammar because parsing can be {{affected}} by run-time code executed during the compile phase. Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison <b>parser</b> to resolve ambiguities in the language.|$|E
5|$|PHP 3.0 is the {{successor}} of PHP/FI 2.0. Zeev Suraski and Andi Gutmans rewrote the <b>parser</b> in 1997 and formed {{the base of}} PHP 3, changing the language's name to the recursive acronym PHP: Hypertext Preprocessor. Afterwards, public testing of PHP 3 began, and the official launch came in June 1998. Suraski and Gutmans then started a new rewrite of PHP's core, producing the Zend Engine in 1999. They also founded Zend Technologies in Ramat Gan, Israel.|$|E
5|$|PHP acts {{primarily}} as a filter, taking input from a file or stream containing text and/or PHP instructions and outputting another stream of data. Most commonly the output will be HTML, although it could be JSON, XML or binary data such as image or audio formats. Since PHP 4, the PHP <b>parser</b> compiles input to produce bytecode for processing by the Zend Engine, giving improved performance over its interpreter predecessor.|$|E
5000|$|Syntax {{analysis}} (including context-free grammars, LL <b>parsers,</b> bottom-up <b>parsers,</b> and LR <b>parsers)</b> ...|$|R
50|$|ANTLR can {{generate}} lexers, <b>parsers,</b> tree <b>parsers,</b> and combined lexer-parsers. <b>Parsers</b> can automatically generate parse trees or abstract syntax trees {{which can be}} further processed with tree <b>parsers.</b> ANTLR provides a single consistent notation for specifying lexers, <b>parsers,</b> and tree <b>parsers.</b> This is in contrast with other parser/lexer generators and adds greatly to the tool's ease of use.|$|R
50|$|LL grammars, {{particularly}} LL(1) grammars, are {{of great}} practical interest, as <b>parsers</b> for these grammars {{are easy to}} construct, and many computer languages {{are designed to be}} LL(1) for this reason. LL <b>parsers</b> are table-based <b>parsers,</b> similar to LR <b>parsers.</b> LL grammars can also be parsed by recursive descent <b>parsers.</b>|$|R
5|$|The Portopia Serial Murder Incident {{follows a}} first-person {{perspective}} and narrative. The various events are described with still pictures and text messages. The player {{interacts with the}} game using a verb-noun <b>parser</b> which requires typing precise commands with the keyboard. Finding the exact words to type is {{considered part of the}} riddles that must be solved. While sound effects are present, the game lacks music and a save function. It features a conversation system with branching dialogue choices, where the story develops through entering commands and receiving answers to them from the player's sidekick or non-player characters.|$|E
5|$|Alan Brigman was the {{technical}} director and co-producer. He and Mike Woodroffe developed a game creation system, Adventure Graphic Operating System (AGOS) II, which facilitated {{the development of}} Simon the Sorcerer and enabled the team {{to focus on the}} gameplay and story without worrying about {{the technical}} aspects. The system allowed the developers to input text commands on a separate monitor, and the engine could be ported to other platforms. Other features of the engine included translating actions performed by the mouse into text commands (a sentence <b>parser</b> carries them out), the loading of data as-needed, and functions could be implemented by the simple addition of commands. The game was built as a database, which contained tables for rooms and objects. These tables contained animation code and information about what is supposed to happen. Alan Cox was also involved {{in the development of the}} AGOS engine, which is based on AberMUD.|$|E
5|$|With no keyboard, the Famicom version {{replaces the}} verb-noun <b>parser</b> with a menu list of {{fourteen}} set commands selectable with the gamepad. This {{is similar to}} the command selection menu system introduced in Yuji Horii's murder mystery adventure game , which was released in 1984, in between the PC and Famicom releases of Portopia. One of the commands on the menu allowed the player to use a point-and-click interface, using the D-pad to move a cursor on the screen in order to look for clues and hotspots. The Famicom version of Portopia also features branching menu selections, which includes using the pointer as a magnifying glass to investigate objects, which is needed to find hidden clues, and as a fist or hammer to hit anything or anyone, which could be used to carry out beatings during suspect interrogations. Additional sequences were also added, notably an underground dungeon maze, with a style similar to role-playing video games.|$|E
5000|$|LR <b>parsers</b> {{were invented}} by Donald Knuth in 1965 as an {{efficient}} generalization of precedence <b>parsers.</b> Knuth proved that LR <b>parsers</b> {{were the most}} general-purpose <b>parsers</b> possible that would still be efficient in the worst cases.|$|R
30|$|Custom-made <b>parsers.</b> These are <b>parsers</b> {{implemented}} {{in an ad}} hoc manner using a general purpose programming language. The composition of such <b>parsers</b> often requires adding “glue code” to adapt the data and interactions between the individual <b>parsers.</b>|$|R
50|$|In {{computer}} science, tail recursive <b>parsers</b> are a derivation {{from the}} more common recursive descent <b>parsers.</b> Tail recursive <b>parsers</b> are commonly used to parse left recursive grammars. They use a smaller amount of stack space than regular recursive descent <b>parsers.</b> They are also easy to write. Typical recursive descent <b>parsers</b> make parsing left recursive grammars impossible (because of an infinite loop problem). Tail recursive <b>parsers</b> use a node reparenting technique that makes this allowable.|$|R
25|$|The LALR(1) <b>parser</b> is less {{powerful}} than the LR(1) <b>parser,</b> and more {{powerful than the}} SLR(1) <b>parser,</b> though they all use the same production rules. The simplification that the LALR <b>parser</b> introduces consists in merging rules that have identical kernel item sets, because during the LR(0) state-construction process the lookaheads are not known. This reduces the power of the <b>parser</b> because not knowing the lookahead symbols can confuse the <b>parser</b> as to which grammar rule to pick next, resulting in reduce/reduce conflicts. All conflicts that arise in applying a LALR(1) <b>parser</b> to an unambiguous LR(1) grammar are reduce/reduce conflicts. The SLR(1) <b>parser</b> performs further merging, which introduces additional conflicts.|$|E
25|$|In 1965, Donald Knuth {{invented the}} LR <b>parser</b> (Left to Right, Rightmost derivation). The LR <b>parser</b> can {{recognize}} any deterministic context-free language in linear-bounded time. Rightmost derivation has very large memory requirements and implementing an LR <b>parser</b> was impractical {{due to the}} limited memory of computers at that time. To address this shortcoming, in 1969, Frank DeRemer proposed two simplified versions of the LR <b>parser,</b> namely the Look-Ahead LR (LALR) and the Simple LR <b>parser</b> that had much lower memory requirements {{at the cost of}} less language-recognition power, with the LALR <b>parser</b> being the most-powerful alternative. In 1977, memory optimizations for the LR <b>parser</b> were invented but still the LR <b>parser</b> was less memory-efficient than the simplified alternatives.|$|E
25|$|The LALR <b>parser</b> {{was invented}} by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages, in his {{treatment}} of the practical difficulties {{at that time of}} implementing LR(1) parsers. He showed that the LALR <b>parser</b> has more language recognition power than the LR(0) <b>parser,</b> while requiring the same number of states as the LR(0) <b>parser</b> for a language that can be recognized by both parsers. This makes the LALR <b>parser</b> a memory-efficient alternative to the LR(1) <b>parser</b> for languages that are LALR. It was also proved that there exist LR(1) languages that are not LALR. Despite this weakness, the power of the LALR <b>parser</b> is enough for many mainstream computer languages, including Java, though the reference grammars for many languages fail to be LALR due to being ambiguous.|$|E
40|$|We {{evaluate}} two dependency <b>parsers,</b> MSTParser and MaltParser, {{with respect}} to their capacity to recover unbounded de-pendencies in English, a type of evalu-ation that has been applied to grammar-based <b>parsers</b> and statistical phrase struc-ture <b>parsers</b> but not to dependency <b>parsers.</b> The evaluation shows that when combined with simple post-processing heuristics, the <b>parsers</b> correctly recall unbounded dependencies roughly 50 % of the time, which is only slightly worse than two grammar-based <b>parsers</b> specifically de-signed to cope with such dependencies. ...|$|R
50|$|As well as lexers and <b>parsers,</b> ANTLR {{can be used}} to {{generate}} tree <b>parsers.</b> These are recognizers that process abstract syntax trees which can be automatically generated by <b>parsers.</b> These tree <b>parsers</b> are unique to ANTLR and greatly simplify the processing of abstract syntax trees.|$|R
40|$|International audienceSupporting {{standard}} text-based protocols in {{embedded systems}} is challenging {{because of the}} often limited computational resources that embedded systems provide. To overcome this issue, a promising approach is to build <b>parsers</b> directly in hardware. Unfortunately, developing such <b>parsers</b> is a daunting task for most developers as {{it is at the}} crossroads of several areas of expertise, such as low-level network programming, or hardware design. In this paper, we propose Zebra, a generative approach to drastically ease the development of hardware <b>parsers</b> and their use in network applications. To validate our approach, we have used Zebra to generate hardware <b>parsers</b> for widely used protocols, namely HTTP, SMTP, SIP, and RTSP. Our experiments show that Zebra-based <b>parsers</b> systematically outperform software-based <b>parsers...</b>|$|R
25|$|The {{original}} dissertation gave no algorithm {{for constructing}} such a <b>parser</b> given some formal grammar. The first algorithms for LALR <b>parser</b> generation {{were published in}} 1973. In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers. LALR parsers can be automatically generated from some grammar by an LALR <b>parser</b> generator such as Yacc or GNU Bison. The automatically generated code may be augmented by hand-written code to augment {{the power of the}} resulting <b>parser.</b>|$|E
25|$|Starting from a {{sequence}} of characters, the lexical analyzer builds {{a sequence}} of language tokens (such as reserved words, literals, and identifiers) from which the <b>parser</b> builds a syntax tree. The lexical analyzer and the <b>parser</b> handle the regular and context-free parts of the programming language's grammar.|$|E
25|$|Various JSON <b>parser</b> {{implementations}} {{have suffered}} from denial-of-service attack and mass assignment vulnerability.|$|E
40|$|This paper {{discusses}} the performance difference of wide-coverage <b>parsers</b> on small-domain speech transcripts. Two <b>parsers</b> (C&C CCG and RASP) are tested on the speech transcripts {{of two different}} domains (parent-child language, and picture descriptions). The performance difference between the domain-independent <b>parsers</b> and two domain-trained <b>parsers</b> (MSTParser and MEGRASP) is substantial, with a difference of at least 30 percent point in accuracy. Despite this gap, some of the grammatical relations can still be recovered reliably. ...|$|R
5000|$|The {{parse tree}} {{generated}} is correct and simply more efficient [...] than non-lookahead <b>parsers.</b> This is the strategy followed in LALR <b>parsers.</b>|$|R
40|$|Deterministic {{dependency}} parsing {{has often}} been regarded as an efficient algorithm while its parsing accuracy is a little lower than the best results reported by more complex methods. In this paper, we compare deterministic dependency <b>parsers</b> with complex parsing methods such as generative and discriminative <b>parsers</b> on the standard data set of Penn Chinese Treebank. The results show that, for Chinese dependency parsing, deterministic <b>parsers</b> outperform generative and discriminative <b>parsers.</b> Furthermore, basing on the observation that deterministic parsing is a greedy algorithm which chooses the most probable parsing action at every step, we propose three kinds of ungreedy deterministic dependency parsing algorithms to globally model parsing actions. We take the original deterministic <b>parsers</b> as baseline systems. Results show that ungreedy deterministic dependency <b>parsers</b> perform better than the baseline systems while maintaining the same time complexity, and our best result improve much over baseline...|$|R
25|$|Deterministic pushdown automata can {{recognize}} all deterministic context-free languages while nondeterministic ones {{can recognize}} all context-free languages, {{with the former}} often used in <b>parser</b> design.|$|E
25|$|In 1979, Frank DeRemer and Tom Pennello {{announced}} {{a series of}} optimizations for the LALR <b>parser</b> that would further improve its memory efficiency. Their work was published in 1982.|$|E
25|$|Most content {{requiring}} namespaces {{will not}} work in HTML, except the built-in support for SVG and MathML in the HTML5 <b>parser</b> along with certain magic prefixes such as xlink.|$|E
40|$|In this paper, {{we look at}} {{comparing}} highaccuracy context-free <b>parsers</b> with highaccuracy finite-state (shallow) <b>parsers</b> {{on several}} shallow parsing tasks. We show that previously reported comparisons greatly under-estimated the performance of context-free <b>parsers</b> for these tasks. We also demonstrate that contextfree <b>parsers</b> can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported. Finally, we establish that combining the output of context-free and finitestate <b>parsers</b> gives much higher results than the previous-best published results, on several common tasks. While the efficiency benefit of finite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought. ...|$|R
40|$|Most recent {{statistical}} <b>parsers</b> {{fall into}} one of two groups. The largest group consists of <b>parsers</b> which are based on some variation of a probabilistic context-free grammar, use joint probability models, and use tabular methods to find the most probable parse. <b>Parsers</b> in the second group are based on probabilistic push-down automata, use conditional probability models, and use some form of state-space search to find the most probable parse. ^ This thesis is a study of natural language parsing as a control problem. This view leads to <b>parsers</b> of the second type. We show that search can be done very efficiently for such <b>parsers.</b> The control approach leads to a particular interpretation of the history-based parsing tradition, in which history is equated with state. The corresponding probability model is called a Markov parsing model, which can be used both for syntactic disambiguation and for search. The resulting <b>parsers</b> are simple, fast, have excellent coverage, and are reasonably accurate. ^ Using treebanks (collections of text, which are expert-annotated with syntactic structure), we learn controllers for <b>parsers</b> that can be applied with little or no search. We call these greedy or nearly-greedy policies. Thus we are studying <b>parsers</b> which are constrained to operate efficiently. ...|$|R
50|$|In practice, LALR {{offers a}} good solution, because LALR(1) grammars are more {{powerful}} than SLR(1), and can parse most practical LL(1) grammars. LR(1) grammars are {{more powerful than}} LALR(1), however, canonical LR(1) <b>parsers</b> can be extremely large in size and are considered not practical. Minimal LR(1) <b>parsers</b> are small in size and comparable to LALR(1) <b>parsers.</b>|$|R
25|$|Finite automata {{are often}} used in the {{frontend}} of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a <b>parser.</b>|$|E
25|$|A {{directive}} is {{a special}} instruction on how ASP.NET should process the page. The most common directive is <%@ Page %>, which can specify many attributes used by the ASP.NET page <b>parser</b> and compiler.|$|E
25|$|In 1981, CE Software {{published}} SwordThrust as {{a commercial}} successor to the Eamon gaming system for the Apple II. SwordThrust and Eamon were simple two-word <b>parser</b> games with many role-playing elements not available in other interactive fiction.|$|E
50|$|Left {{recursion}} often poses {{problems for}} <b>parsers,</b> either because it leads them into infinite recursion (as {{in the case}} of most top-down <b>parsers)</b> or because they expect rules in a normal form that forbids it (as {{in the case of}} many bottom-up <b>parsers,</b> including the CYK algorithm). Therefore, a grammar is often preprocessed to eliminate the left recursion.|$|R
5000|$|Earley <b>parsers</b> {{apply the}} {{techniques}} and [...] notation of LR <b>parsers</b> {{to the task}} of generating all possible parses for ambiguous grammars such as for human languages.|$|R
40|$|GLR <b>parsers</b> {{have been}} criticized by various authors for their {{potentially}} large sizes. Other <b>parsers</b> also have individual weaknesses and strengths. Our heterogeneous parsing algorithm is based on GLR <b>parsers</b> that handle the size and other problems by partitioning the grammar during compiling and assembling the partitioned sub-grammars during parsing. We discuss different <b>parsers</b> for different grammars and present some intuitive considerations for the partitioning. 1 Introduction Various authors [9, 18, 19, 11] have criticized the size of (G) LR <b>parsers</b> as being too big for programming languages and natural languages. In addition, there exist context-free grammars G whose collection of LR(0) items is exponentially larger than jGj. Other <b>parsers</b> also have their own weaknesses and strengths. A number of {{attempts have been made}} to counter these problems[9, 12], and in this paper we present a new approach. The main idea is to look for certain partitions of a grammar. Instead of having compi [...] ...|$|R
