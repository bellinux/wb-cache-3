4|10000|Public
40|$|Many {{articles}} {{have been written}} concerning Moses and scholarship generally regards him as a divinely ‘elected’ leader of Judaism. The principle of parsimony has contributed a great deal towards united and simplified explanations {{of what kind of}} leader Moses was. For many biblical scholars, Moses possessed a wide array of skills and thus stands out as the foremost personality of the pre-Christian world. He headed a race comprising slaves and led them out of servitude in Egypt in what were exceedingly trying circumstances, to their promised land. However, if we apply the thinking of Plato, Schopenhauer or even Wittgenstein, we should apply a <b>principle</b> <b>of</b> <b>specification</b> and look at different types of discourse. This paper consequently approaches and examines the role of Moses as a leader from both theological and managerial perspectives. It is proposed that while Moses {{is considered to be a}} great leader of God’s elect people, the Hebrews, his managerial skills informs many of the modern day management principles despite his actions being embedded in the theological and cultural world in which he operate...|$|E
40|$|This article {{seeks to}} show how the {{elements}} of a (flexible and adaptable) model of territorial development, whose roots are anchored in the analysis of coordination among pre-capitalist actors, can take on a more concrete form in developing economies. Our hypothesis is that the application of this model functions as a resurgence of pre-capitalist production relations, revalidated by local practices and renewed by territorial dynamics. Is such a return to pre-capitalist ways possible and realistic despite the obstacles and, if so, under what conditions? These are the questions this paper seeks to address, by the way of a specifically economic analysis focused on the dynamics of productive systems. On the first part, we review the fundamental principles of a model of territorial development based on a local system of actors, which assumes that the territory is “constructed” and is founded, according to our approach, on the <b>principle</b> <b>of</b> <b>specification.</b> On the second part, we assess the feasibility of this model in developing economies, remembering that we are starting from practices that are already old; practices involving risks and that assume certain conditions of implementation...|$|E
40|$|URL] article {{seeks to}} show how the {{elements}} of a (flexible and adaptable) model of territorial development, whose roots are anchored in the analysis of coordination among pre-capitalist actors, can take on a more concrete form in developing economies. Our hypothesis is that the application of this model functions as a resurgence of pre-capitalist production relations, revalidated by local practices and renewed by territorial dynamics. Is such a return to pre-capitalist ways possible and realistic despite the obstacles and, if so, under what conditions? These are the questions this paper seeks to address, by the way of a specifically economic analysis focused on the dynamics of productive systems. On the first part, we review the fundamental principles of a model of territorial development based on a local system of actors, which assumes that the territory is “constructed” and is founded, according to our approach, on the <b>principle</b> <b>of</b> <b>specification.</b> On the second part, we assess the feasibility of this model in developing economies, remembering that we are starting from practices that are already old; practices involving risks and that assume certain conditions of implementation. This article seeks {{to show how}} the elements of a (flexible and adaptable) model of territorial development, whose roots are anchored in the analysis of coordination among pre-capitalist actors, can take on a more concrete form in developing economies. Our hypothesis is that the application of this model functions as a resurgence of pre-capitalist production relations, revalidated by local practices and renewed by territorial dynamics. Is such a return to pre-capitalist ways possible and realistic despite the obstacles and, if so, under what conditions? These are the questions this paper seeks to address, by the way of a specifically economic analysis focused on the dynamics of productive systems. On the first part, we review the fundamental principles of a model of territorial development based on a local system of actors, which assumes that the territory is “constructed” and is founded, according to our approach, on the <b>principle</b> <b>of</b> <b>specification.</b> On the second part, we assess the feasibility of this model in developing economies, remembering that we are starting from practices that are already old; practices involving risks and that assume certain conditions of implementation. Este artículo busca mostrar de qué manera los elementos de un modelo (flexible y adaptable) de desarrollo territorial, cuyos orígenes están anclados en el análisis de la coordinación de acciones de actores pre-capitalistas, pueden adquirir una forma más concreta en los países en desarrollo. Se plantea que esta aplicación presupone un resurgimiento de las relaciones de producción pre-capitalistas, pero revalidadas por medio de prácticas novedosas a nivel local y renovadas a través de dinámicas territoriales. El eje de la línea de argumentación, apoyada en un análisis económico, apunta hacia las condiciones de posibilidad de este rescate actual de padrones pre-capitalistas. En la primera parte son examinados los principios fundamentales del modelo, basado en procesos de “construcción” del territorio por los actores locales. Y en la segunda parte se analiza la viabilidad de este modelo en el contexto actual de los países en desarrollo. En este sentido, se presupone que estamos partiendo de prácticas antiguas que incluyen riesgos y que exigen condiciones específicas para su implementación de forma consistente. ...|$|E
5000|$|Giving {{an index}} of ISO {{standards}} involved with different <b>principles</b> <b>of</b> technical product <b>specification</b> (TPS); ...|$|R
40|$|This paper {{develops}} a methodology {{for the design}} of audiovideo data corpora of the speaking face. Existing corpora are surveyed and the <b>principles</b> <b>of</b> data <b>specification,</b> data description and statistical representation are analysed both from an application-driven and from a scientifically motivated perspective. Furthermore, the possibility of "opportunistic" design of speaking-face data corpora is considered...|$|R
5000|$|ELMER (Easier {{and more}} Efficient Reporting), is a {{comprehensive}} set <b>of</b> <b>principles</b> and <b>specifications</b> {{for the design of}} Internet-based forms.|$|R
40|$|From the {{viewpoint}} of ensuring complex business security, {{the relevance of the}} present work is associated with the rationale of multilevel hierarchical approach to the classification of security threats in the age of globalization. The specificity of the threats specific to one or another level of the economy, helps to better understand and consequently to build an effective system of ensuring complex business security. For each of the nine hierarchical levels of the economy the author identifies the main threats to the business, as well as the objects and subjects of this study. It is noted that the performance of the business {{in the form of a}} complex hierarchical system depends on the <b>principle</b> <b>of</b> <b>specification.</b> The author gives examples of the use of the basic principles of specification. It is noted that the decomposition of the economic system from {{the viewpoint}} of its hierarchical nature is of great importance not only to the distribution of the goals and objectives of security of business levels of the system, but their subordination corresponding to each level. The result is the development of specific recommendations and elaboration of the main directions to ensure complex business security for mega-, macro-, micro-, mini-, nano - and mesoeconomic levels. Although the priority of action in multi-level hierarchical system is directed from the upper to the lower levels, the success of the system as a whole depends on the behavior of all system components. It is stated that the interaction with the environment in business occurs mainly in the lower levels of the hierarchy. The quality system of ensuring complex business security which deals with hierarchical positions, will depend not so much on top-level elements, but on response to intervention on the part of lower level, more precisely from their total effect. In other words, the quality of the system of integrated safety management business provides organized feedbacks in the system...|$|E
40|$|Formal methods {{should be}} taught as part of any degree in {{computing}} science or software engineering. We believe that discrete mathematics is the foundation upon which software development can be lifted up to the heights of a true engineering discipline. The transfer of formal methods to industry cannot be expected to occur without first transferring, from academia to industry, graduates who are well grounded in such mathematical techniques. These graduates must bring a positive, yet realistic, view on the application of formal methods. Our goal is to produce software engineers who will go out into industry understanding the <b>principles</b> <b>of</b> <b>specification,</b> design and implementation. As these graduates develop their engineering skills, in an industrial setting, they should have the means, and the motivation, to integrate formality and rigour into any {{environment in which they}} are found. In this way, the formal methods should start to `sell themselves'...|$|R
40|$|Since a {{dispersion}} interferometer is {{free from}} mechanical vibrations, {{it does not}} need a vibration compensation system even for a probe beam with a short wavelength. This paper describes a new signal processing of the dispersion interferometer using a ratio of modulation amplitudes with a photoelastic modulator. The proposed method is immune to changes in detected signal intensities, thus making the signal processing system simple. Designs of the optical system of the dispersion interferometer for proof <b>of</b> <b>principle,</b> especially <b>specification</b> <b>of</b> a nonlinear optical crystal, are also shown...|$|R
40|$|The {{goal of this}} Creative Project was {{to write}} a model text on the <b>principles</b> and {{practices}} <b>of</b> <b>specifications</b> writing for landscape architecture students. The goals of the text were to provide: 1) an overview <b>of</b> the <b>principles</b> and practices <b>of</b> <b>specifications</b> writing as advocated by the Construction Specifications Institute; 2) examples of contract documents used by the landscape architect; and 3) a source of reference information specifically for landscape architects. The text consists of nine chapters. Each chapter begins {{with a set of}} goals, followed by the text content, and ends with a set of review questions. The text contains figures and appendices which provide examples and sources of information useful to <b>specifications</b> writers. Department <b>of</b> Landscape ArchitectureThesis (M. L. A. ...|$|R
40|$|So far, several {{implementations}} <b>of</b> the Fractal <b>specifications</b> {{have been}} proposed. These implementations propose frameworks for programming with Fractal in a target language (Java, C, Smalltalk, C++). The general <b>principles</b> <b>of</b> implementing the <b>specifications</b> {{are common to}} all these frameworks. However, {{as far as we}} know, no concrete piece of software or no common set of interna...|$|R
40|$|Essential {{concepts}} <b>of</b> algebraic <b>specification</b> refinement are {{translated into}} a type-theoretic setting involving System F and Reynolds' relational parametricity assertion as expressed in Plotkin and Abadi's logic for parametric polymorphism. At first order, the type-theoretic setting provides a canonical picture <b>of</b> algebraic <b>specification</b> refinement. At higher order, the type-theoretic setting allows future generalisation <b>of</b> the <b>principles</b> <b>of</b> algebraic <b>specification</b> refinement to higher order and polymorphism. We show the equivalence of the acquired type-theoretic notion <b>of</b> <b>specification</b> refinement with that from algebraic specification. To do this, a generic algebraic-specification strategy for behavioural refinement proofs is mirrored in the type-theoretic setting. 1 Introduction This paper aims to express in type theory certain essential concepts <b>of</b> algebraic <b>specification</b> refinement. The benefit to algebraic specification is that inherently first-order concepts are tra [...] ...|$|R
40|$|Primordial {{germ cells}} (PGCs) are the {{precursors}} of sperm and eggs, which generate a new organism that {{is capable of}} creating endless new generations through germ cells. PGCs are specified during early mammalian postimplantation development, and are uniquely programmed for transmission of genetic and epigenetic information to subsequent generations. In this Primer, we summarise the establishment <b>of</b> the fundamental <b>principles</b> <b>of</b> PGC <b>specification</b> during early development and discuss how it is now possible to make mouse PGCs from pluripotent embryonic stem cells, and indeed somatic cells if they are first rendered pluripotent in culture...|$|R
40|$|As {{complexity}} and scale of design processes in architecture and in engineering increase, {{as well as}} the demands on these processes with respect to costs, throughput time and quality, traditional approaches to organise and plan these processes may no longer suffice. In this conceptual article it is argued that more innovative approaches may be needed. The content and nature of process designs is discussed, {{as well as the}} design knowledge to make them, and ideas are presented on research approaches to further develop design knowledge that can support more innovative process design. An important type of such design knowledge is the technological rule, {{to be based on the}} <b>principle</b> <b>of</b> minimal <b>specification...</b>|$|R
40|$|FRSM (Formal Requirements Specification Method) is a {{structured}} formal language and method for requirements analysis and specification construction {{based on data}} flow analysis. It uses a formalized DeMarco data flow diagrams to describe the overall structure of systems and a VDM-SL like formal notation to describe precisely the functionality of components in the diagrams. This paper first describes the formal syntax and semantics of FRSM and then presents an example of using the axiom and inference rules given {{in the definition of}} the formal semantics for checking consistency <b>of</b> <b>specifications.</b> A case study of applying FRSM to a practical example is described to demonstrate the <b>principle</b> <b>of</b> constructing requirements <b>specifications</b> and to uncover the benefits and deficiencies of FRSM...|$|R
40|$|AbstractWe {{address the}} problem of {{unification}} modulo a set of equations, using the narrowing relation. We propose some syntactical criteria on algebraic specifications that ensure the completeness of narrowing strategies. We then prove a theorem relating narrowing and reduction relations. The completeness of narrowing strategies is proved and conditions for the computation of a “minimal” ground complete set of E-unifiers are given, as well as an algorithm transforming specifications satisfying Huet and Hullot's <b>principle</b> <b>of</b> definition into <b>specifications</b> fulfilling the proposed criteria...|$|R
40|$|Renshaw cells {{provide a}} {{convenient}} model to study spinal circuit development during {{the emergence of}} motor behaviors with the goal <b>of</b> capturing <b>principles</b> <b>of</b> interneuron <b>specification</b> and circuit construction. This work is facilitated by {{a long history of}} research that generated essential knowledge about the characteristics that define Renshaw cells and the recurrent inhibitory circuit they form with motoneurons. In this review, we summarize recent data on the <b>specification</b> <b>of</b> Renshaw cells and their connections. A major insight from these studies is that the basic Renshaw cell phenotype is specified before circuit assembly, a result of their early neurogenesis and migration. Connectivity is later added, constrained by their placement in the spinal cord. Finally, different rates of synapse proliferation alter the relative weights of different inputs on postnatal Renshaw cells. Based on this work some general principles on the integration of spinal interneurons in developing motor circuits are derived...|$|R
40|$|This {{thesis is}} focused {{on some of the}} new {{instruments}} on capital markets [...] specifically on Investment Certificates, Warrants, Exchange Traded Funds and Contracts for Difference. It shows fundamental <b>principle</b> <b>of</b> behavior, important <b>specifications</b> and comparison to other investment products. The text further examine overall instruments' offering especially on the European markets and compare trading possibilities of the main brokers in the Czech Republic. The work analyses price changes of the instruments and related underlying assets and examine risk and profitability with respect to the latest global financial crisis...|$|R
40|$|We {{present a}} {{mechanised}} semantics for higher-order logic (HOL), and a proof of soundness for the inference system, including {{the rules for}} making definitions, implemented by the kernel of the HOL Light theorem prover. Our work extends Harrison’s verification of the inference system without definitions. Soundness of the logic extends to soundness of a theorem prover, because we also show that a synthesised implementation of the kernel in CakeML refines the inference system. Apart from adding support for definitions and synthesising an implementation, we improve on Harrison’s work by making our model of HOL parametric on the universe of sets, and we prove soundness for an improved <b>principle</b> <b>of</b> constant <b>specification</b> {{in the hope of}} encouraging its adoption. Our semantics supports defined constants directly via a context, and we find this approach cleaner than our previous work formalising Wiedijk’s Stateless HOL...|$|R
50|$|In 2005 the ELMER 2 project {{developed}} the example towards a comprehensive set <b>of</b> <b>principles</b> and <b>specifications</b> {{for the design}} of Internet-based forms. Business organizations, governmental bodies, usability experts and form developers were invited to submit suggestions and to take part in debates during the process. Open workshops were held, and a number of authorities and experts wrote, read and commented contributions posted on the project discussion forum.|$|R
40|$|Abstract. We {{present a}} {{mechanised}} semantics and soundness proof for the HOL Light kernel including its definitional principles, extending Har-rison’s verification of the kernel without definitions. Soundness of the logic extends to soundness of a theorem prover, because we {{also show that}} a synthesised implementation of the kernel in CakeML refines the inference system. Our semantics is the first for Wiedijk’s stateless HOL; our implementation, however, is stateful: we give semantics to the stateful inference system by translation to the stateless. We improve on Harri-son’s approach by making our model of HOL parametric on the universe of sets. Finally, we prove soundness for an improved <b>principle</b> <b>of</b> con-stant <b>specification,</b> {{in the hope of}} encouraging its adoption. This paper represents the logical kernel aspect of our work on verified HOL imple-mentations; the production of a verified machine-code implementation of the whole system with the kernel as a module will appear separately. ...|$|R
40|$|Lakowski, R. (1969). Brit. J. industr. Med., 26, 173 - 189. Theory and {{practice}} of colour vision testing: A review Part I. It is the concern of this paper to examine not only the effectiveness of tests for detecting colour confusion but also their usefulness in assessing colour vision generally. In part 1, problems of administration and age {{and the question of}} the basic elements of such tests are discussed. The existing theory of colour vision and colour defect is outlined and the <b>principle</b> <b>of</b> objective colour <b>specification,</b> which is believed to be valuable for understanding these tests, is introduced...|$|R
40|$|Abstract: According to each {{subsystem}} {{of its own}} energy-consumption {{characteristics in}} paper machine dryer section, taking heat transfer in the dryer section as paramount, the stepwise recovery and optimal use of steam as the target to decompose the multi-Agent energy consumption network. It’s combined with optimal strategies of decomposition and coordination {{in the process of}} energy integrated optimization, the divisional <b>principle</b> <b>of</b> Agent functional <b>specifications,</b> the structure and performance of computer network systems. The paper discussed the energy network topology of multi Agent overall nodes in paper machine dryer section that provide the foundation for researc...|$|R
40|$|The main design <b>principles</b> and <b>specifications</b> <b>of</b> an {{experimental}} vertical boring mill built for high speed turning tests (up to over 100 m/s) are described, and some {{results of an}} investigation on turning steel with ceramic tool within a 15 to 50 m/s cutting speed range are discussed. Data on single and combined effects of machining parameters on surface finish, tool life, chip formation and cutting power were obtained in the tests, and their significance is assesse...|$|R
40|$|An {{advantage}} of conceptual structures is {{their capacity to}} act as a convenient humancomputer intermediary in the graphical form. When we compute with conceptual structures the canonical formation rules can repeatedly be applied to a graph changing both the structure and content of the original graph. The layout of the deduced graph will be critical to its comprehensibility. The conceptual structure layout problem is non-trivial both in terms of the computational cost for the general layout problem and the problem of deriving a metric for the understandability of the resulting graph. In this paper we discuss these issues and present a spatial script language based on the <b>principle</b> <b>of</b> layout <b>specification</b> described by Kamada et al. [3]. The language preserves spatial structure when translated into the linear form and is used by the conceptual graph editor[2] to reconstitute graphs from their linear to their graphical form. The language is presented as a conceptual graph cannon with a pa [...] ...|$|R
40|$|The scoped-memory {{feature is}} central to the Real-Time Specification for Java. It allows greater control over memory management, in {{particular}} the deallocation of objects without the use of a garbage collector. To preserve the safety of storage references associated with Java since its inception, the use of scoped memory is constrained by a set of rules in the specification. While a program’s adherence to the rules can be partially checked at compile-time, undecidability issues imply that some—perhaps, many—checks may be required at run-time. Poor implementations of those run-time checks could adversely affect overall performance and predictability, the latter being a founding <b>principle</b> <b>of</b> the <b>specification.</b> In this paper we present efficient algorithms for managing scoped memories and the checks they impose on programs. Implementations and results published to date require time linear in the depth of scope nesting; our algorithms operate in constant time. We describe our approach and present experiments quantifying the gains in efficiency...|$|R
40|$|ABSTRACT-This {{study is}} based on the radar-evaluated {{rainfall}} data from 52 south Florida cumulus clouds, 26 seeded and 26 control clouds, selected by a randomization procedure. The fourth root of the rainfall for both seeded and control populations was well fitted by a gamma distribution for probability density. The gamma distribution is prescribed by two parameters, one for scale and one for shape. Since the coefficient of variation of seeded and control cloud populations was the same, the shape parameters for the two gamma distributions were the same, while the seeded population’s scale parameter was such as to shift the distribution to higher rainfall values than the control distribution. The best-fit gamma functions were found by application <b>of</b> the <b>principle</b> <b>of</b> maximum entropy. <b>Specification</b> <b>of</b> tractable distributions for natural an...|$|R
40|$|The {{widespread}} {{conviction that}} perceiving another person must rest on ambiguous and fakeable information is challenged. Arguing from biomechanical necessities inherent in maintaining balance and coping with reactive impulses, {{we show that}} the detailed kinematic pattern is specific to an acting person's anatomical makeup and to the working {{of his or her}} motor control system. In this way information is potentially available about gender, identity, expectations, intentions, and what the person is in fact doing. We invoke the lawfulness of human movement, as elucidated by recent advances in motor control theory, to demonstrate the virtual impossibility of performing truly deceptive movements and to argue in general terms for the specification power inherent in human kinematics. The outcome of the analysis is subsumed under a <b>principle</b> <b>of</b> kinematic <b>specification</b> <b>of</b> dynamics (KSD), which states that movements specify the causal factors of events. Generally, a linked multiple degrees-of-freedom system does not exhibit substitutability; a change in one of its "input " factors cannot substitute for, or cancel, the multivariable effects of a change in another factor. Six explorative experiments are reported. Displaying humans in action with Johansson'...|$|R
40|$|A {{prominent}} {{figure in}} twentieth-century physics, Gregor Wentzel made major {{contributions to the}} development of quantum field theory, first in Europe and later at the University of Chicago. His Quantum Theory of Fields offers a knowledgeable view of the original literature of elementary quantum mechanics and helps make these works accessible to interested readers. An introductory volume rather than an all-inclusive account, the text opens with an examination <b>of</b> general <b>principles,</b> without <b>specification</b> <b>of</b> the field equations of the Lagrange function. The following chapters deal with particula...|$|R
40|$|This is {{the final}} version of the article. It first {{appeared}} from Springer via [URL] present a mechanised semantics for higher-order logic (HOL), and a proof of soundness for the inference system, including the rules for making definitions, implemented by the kernel of the HOL Light theorem prover. Our work extends Harrison’s verification of the inference system without definitions. Soundness of the logic extends to soundness of a theorem prover, because we also show that a synthesised implementation of the kernel in CakeML refines the inference system. Apart from adding support for definitions and synthesising an implementation, we improve on Harrison’s work by making our model of HOL parametric on the universe of sets, and we prove soundness for an improved <b>principle</b> <b>of</b> constant <b>specification</b> in the hope of encouraging its adoption. Our semantics supports defined constants directly via a context, and we find this approach cleaner than our previous work formalising Wiedijk’s Stateless HOL. The first author was supported by the Gates Cambridge Trust. The second author was funded in part by the EPSRC (grant number EP/K 503769 / 1). The third author was partially supported by the Royal Society UK and the Swedish Research Council...|$|R
40|$|International audienceSo far, several {{implementations}} <b>of</b> the Fractal <b>specifications</b> {{have been}} proposed. These implementations propose frameworks for programming with Fractal in a target language (Java, C, Smalltalk, C++). The general <b>principles</b> <b>of</b> implementing the <b>specifications</b> {{are common to}} all these frameworks. However, {{as far as we}} know, no concrete piece of software or no common set of internal interfaces have ever been shared between them. In this paper, we report on a reference model which has been set up to support several implementations on the Fractal Specifications. This model has been derived to build a Java personality called AOKell, and FractNet, a personality for the languages of the. NET framework. The originality of this model is {{to be based on the}} concepts of Aspect-Oriented Programming (AOP). Section 2 presents the general architecture of the reference architecture. Section 3 reports on the two personalities, AOKell and FractNet. Section 4 shows some performance measurements. Section 5 concludes this paper...|$|R
40|$|International audienceIn {{this paper}} we propose a new {{strategy}} of watermarking which extends the <b>principle</b> <b>of</b> histogram <b>specification</b> to color histogram. The proposed scheme embeds into a color image a color watermark from either the xy chromatic plane or the xyY color space. The scheme resists geometric attacks (e. g., rotation, scaling, etc.,) and, within some limits, JPEG compression. The scheme uses a secret binary pattern, or combines some patterns generated by a secret key in order to modify the chromatic distribution of an image. By using the inverse pattern, the watermark is detected without knowing the original image. Examples of images and attacks are given to illustrate the relevance of the proposed approach, i. e., its invisibility and its robustness. In the second part of this paper we investigate the usefulness of our watermarking approach for color image authentication. Several experiments are presented to show that our scheme ensures image authentication, detects tampered regions in case of malicious attacks and ensures a certain degree of robustness to common image manipulations like JPEG compression, etc. Compared with other blind authentication schemes, the experiments show that, the detection ability, the invisibility, as well as the robustness to some common image processing are improved...|$|R
40|$|AbstractFormalization in {{a logical}} theory can {{contribute}} to the foundational understanding of interactive systems in two ways. One is to provide language and <b>principles</b> for <b>specification</b> <b>of</b> and reasoning about such systems. The other is to better understand the distinction between sequential (turing equivalent) computation and interactive computation using techniques and results from recursion theory and proof theory. In this paper we briefly review the notion of interaction semantics for actor systems, and report on work in progress to formalize this interaction model. In particular we have shown that the set theoretic models of the formal interaction theory have greater recursion theoretic complexity than analogous models of theories of sequential computation, using a well-known result from recursion theory...|$|R
40|$|This {{work was}} {{motivated}} by two problems we experienced while teaching the introductory computer programming courses CS 1 and CS 2. Firstly, the quality of code written by our students was poor and showed little improvement over time. The norm was overly complex code that exhibited about 80 % of the specified functionality. Secondly, it was difficult and labour intensive to evaluate student code effectively. To address these problems, we integrated systematic verification into both our CS 1 and CS 2 courses. We found that systematic verification facilitated the key engineering <b>principles</b> <b>of</b> implementation to <b>specification</b> and quality evaluation. We observed {{an increase in the}} quality of student code and a decrease in the effort required to evaluate student code. 1 INTRODUCTION We have over ten years experience teaching introductory computer programming at the CS 1 and CS 2 levels. While teaching these courses, we observed student code that was overly complex and exhibited about 80 % o [...] ...|$|R
50|$|The CoreASM {{language}} emphasizes {{freedom of}} experimentation, and supports the evolutionary nature of design {{as a product}} of creativity. It is particularly suited to Exploring the problem space for the purpose of writing an initial specification. The CoreASM language allows writing of highly abstract and concise specifications by minimizing the need for encoding in mapping the problem space to a formal model, and by allowing explicit declaration of the parts <b>of</b> the <b>specification</b> that are purposely left abstract. The <b>principle</b> <b>of</b> minimality, in combination with robustness of the underlying mathematical framework, improves modifiability <b>of</b> <b>specifications,</b> while effectively supporting the highly iterative nature <b>of</b> <b>specification</b> and design.|$|R
40|$|By means <b>of</b> three design <b>principles</b> (the sociotechnical criterion, the <b>principle</b> <b>of</b> minimal {{critical}} <b>specification</b> and the <b>principle</b> <b>of</b> joint optimization of {{the technical}} and social system), STS as a design theory is related to four organizational performance indicators (price, quality, flexibility and innovation). As a diagnostic theory, STS helps to find contingencies between environmental demands and work design. The diagnoses result in sets of STS practices. It is argued {{that as long as}} price and quality are the only important performance criteria, STS practices have little to offer and their contributions will be only at the job level. If flexibility is of importance, STS has much more to offer, on the job level as well as the organizational level. The same is true for when innovation is a relevant indicator, in which case STS practices may also help to 'design' processes, such as mutual trust among workers and diversity with respect to attitudes, abilities and cognitions. It is argued that the dominant performance indicators have changed in a cumulative way from efficiency, via quality and flexibility towards innovation and learning. In accordance with these changes, the STS principles are extended with the concept of organizational learning. (C) 2001 Elsevier Science B. V. All rights reserved...|$|R
