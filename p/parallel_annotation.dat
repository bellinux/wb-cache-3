6|30|Public
40|$|We {{describe}} a <b>parallel</b> <b>annotation</b> approach for PubMed abstracts. It includes both entity/relation annotation and a treebank containing syntactic structure, {{with a goal}} of mapping entities to constituents in the treebank. Crucial to this approach is a modification of the Penn Treebank guidelines and the characterization of entities as relation components, which allows {{the integration of the}} entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events...|$|E
40|$|Summary: Digital Extractor is {{a program}} for the highthroughput {{processing}} of data sets derived from digital differential display-based comparisons of EST libraries. These comparisons can be utilized to identify discrete subsets of genes whose expression is restricted to distinct tissue types. The program facilitates these investigations by permitting <b>parallel</b> <b>annotation</b> of genes identified as being differentially expressed. Availability: The executable program, suitable for use on all UNIX-based platforms is freely available to non-profit users Contact...|$|E
40|$|Abstract- An {{image is}} logically divided into {{clusters}} and their texture features are given as inputs to the ARTMAP for concepts identification. Concepts are associated using bidirectional ARTMAP of mirror neurons and these associations are carried {{over in a}} parallel manner avoiding the problem encountered in hierarchical annotation that of training many components. Applying of 2 / 3 rule is done to the <b>parallel</b> <b>annotation</b> to bring out the annotated result. Experiments were conducted for about 500 natural scenes. Experimental results show that the new proposed approach makes better in not only concept identification but also for efficient annotation of the image. It also has given better result when executed in parallel i. e. in a committee machine system than in a hierarchical manner...|$|E
40|$|We {{present a}} tool for {{annotation}} of semantic intersentential discourse relations on the tectogrammatical layer of the Prague Dependency Treebank (PDT). We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, a possibility to define, display and connect arbitrary groups of nodes, a clausebased compact depiction of trees, etc. For studying differences among <b>parallel</b> <b>annotations,</b> the tool offers a simultaneous depiction of <b>parallel</b> <b>annotations</b> of the data...|$|R
40|$|We have {{previously}} reported on ProPOSEL, a purpose-built Prosody and PoS English Lexicon {{compatible with the}} Python Natural Language ToolKit. ProPOSEC is a new corpus research resource built using this lexicon, intended for distribution with the Aix-MARSEC dataset. ProPOSEC comprises multi-level <b>parallel</b> <b>annotations,</b> juxtaposing prosodic and syntactic information from {{different versions of the}} Spoken English Corpus, with canonical dictionary forms, in a query format optimized for Perl, Python, and text processing programs. The order and content of fields in the text file is as follows: (1) Aix-MARSEC file number; (2) word; (3...|$|R
40|$|VALLEX is a valency lexicon of Czech verbs. We briefly {{introduce}} VALLEX {{and then}} describe {{and evaluate the}} VALEVAL experiment: annotation of 10256 corpus instances of 109 Czech verbs with valency frames. The inter-annotator agreement of three <b>parallel</b> <b>annotations</b> ranges from 61 % to 74 % and κ from 0. 52 to 0. 62. More than 8000 sentences are now available as the “golden VALEVAL ” for word-sense disambiguation experiments. In out first attempts using morphological and syntax-based information, {{we achieve the accuracy}} of 70 % to 80 %. ...|$|R
40|$|The {{referential}} transparancy {{of functional}} languages imposes restraints on the communication between parallel tasks. This seriously hampers {{the implementation of}} parallel branch-and-bound algorithms, because the parallel tasks cannot update a shared bound asynchronously. Maintaining local bounds only causes enormous performance loss, because parallel branchand -bound algorithms exploit speculative parallelism. Therefore, the parallel tasks have to exchange their bounds synchronously, but this lowers the average processor utilization. Three parallel functional implementations of a generally applicable branch-and-bound algorithm {{have been developed to}} investigate this trade-off. Experiments have been performed with the cargo loading problem. Two functional equivalents of imperative parallel branch-and-bound approaches are outperformed by a dedicated parallel functional branch-and-bound implementation. A new speculative <b>parallel</b> <b>annotation</b> is proposed to tackle the trade-off between specu [...] ...|$|E
40|$|In {{this paper}} {{describes}} {{the effects of the}} evolution of an Italian dependency grammar on a task of multilingual FrameNet acquisition. The task is based on the creation of virtual English/Italian <b>parallel</b> <b>annotation</b> corpora, which are then aligned at dependency level by using two manually encoded grammar based dependency parsers. We show how the evolution of the LAS (Labeled Attachment Score) metric for the considered grammar has a direct impact {{on the quality of the}} induced FrameNet, thus proving that the evolution of the quality of syntactic resources is mirrored by an analogous evolution in semantic ones. In particular we show that an improvement of 30 % in LAS causes an improvement of precision for the induced resource ranging from 5 % to 10 %, depending on the type of evaluation...|$|E
40|$|A highly {{accurate}} Named Entity (NE) corpus for Hungarian that is {{publicly available}} {{for research purposes}} is introduced in the paper, along with its main properties. The results of experiments that apply various Machine Learning models and classifier combination schemes are also presented {{to serve as a}} benchmark for further research based on the corpus. The data is a segment of the Szeged Corpus (Csendes et al., 2004), consisting of short business news articles collected from MTI (Hungarian News Agency, www. mti. hu). The annotation procedure was carried out paying special attention to annotation accuracy. The corpus went through a <b>parallel</b> <b>annotation</b> phase done by two annotators, resulting in a tagging with inter-annotator agreement rate of 99. 89 %. Controversial taggings were collected and discussed by the two annotators and a linguist with several years of experience in corpus annotation. These examples were tagged following the decision they made together, and finally all entities that had suspicious or dubious annotations were collected and checked for consistency. We consider the result of this correcting process virtually be free of errors. Our best performing Named Entity Recognizer (NER) model attained an accuracy of 92. 86 % F measure on the corpus. 1...|$|E
40|$|Motivated by {{the recent}} {{hardware}} evolution towards multi-core machines, we investigate parallel planning techniques in a shared-memory environment. We consider, more specifically, parallel versions of a best-first search algorithm that run K threads, each expanding the next best node from the open list. We show that the proposed technique {{has a number of}} advantages. First, it is (reasonably) simple: we show how the algorithm can be obtained from a sequential version mostly by adding <b>parallel</b> <b>annotations.</b> Second, we conduct an extensive empirical study that shows that this approach is quite effective. It is also dynamic {{in the sense that the}} number of nodes expanded in parallel is adapted during the search. Overall we show that the approach is promising for parallel domain-independent, suboptimal planning...|$|R
40|$|The {{annotation}} of the Prague Dependency Treebank (PDT) {{is conceived}} of as a multilayered scenario that comprises also dependency representations (tectogrammatical tree structures, TGTS’s) of the underlying structure of the sentences. TGTS’s capture three basic aspects of the underlying structure of sentences: (a) the dependency tree structure, (b) the kinds of dependency syntactic relations, and (c) the basic characteristics of the topic-focus articulation (TFA). Since the PDT is a large collection and the annotations on the deepest layer are {{to a large extent}} performed by several human annotators (based on an automatic preprocessing module), it is more than necessary to observe the consistence of annotators and the agreement among them. In the present paper, we summarize the results of the evaluation of <b>parallel</b> <b>annotations</b> of several samples taken from PDT and the measures accepted to improve the consistency of annotations...|$|R
5000|$|A source-to-source {{compiler}} {{is a type}} of compiler {{that takes}} a high-level language as its input and outputs a high-level language. For example, an automatic parallelizing compiler will frequently take in a high-level language program as an input and then transform the code and annotate it with <b>parallel</b> code <b>annotations</b> (e.g. OpenMP) or language constructs (e.g. Fortran's [...] statements).|$|R
40|$|We {{present an}} {{annotation}} {{tool for the}} extended textual coreference and the bridging anaphora in the Prague Dependency Treebank 2. 0 (PDT 2. 0). After we very briefly describe the annotation scheme, we focus on details of the annotation process from the technical point of view. We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, an automatic maintaining of the coreferential chain, underlining candidates for antecedents, etc. For studying differences among <b>parallel</b> <b>annotations,</b> the tool offers a simultaneous depiction of several annotations of the same data. The annotation tool {{can be used for}} other corpora too, {{as long as they have}} been transformed to the PML format. We present modifications of the tool for working with the coreference relations on other layers of language description, namely on the analytical layer and the morphological layer of PDT...|$|R
40|$|To {{facilitate}} {{the application of}} semantics in statistical machine translation, we propose a broad-coverage predicate-argument structure mapping technique using automated resources. Our approach utilizes automatic syntactic and semantic parsers to generate Chinese-English predicate-argument structures. The system produced a many-to-many argument mapping for all PropBank argument types by computing argument similarity based on automatic word alignment, achieving 80. 5 % F-score on numbered argument mapping and 64. 6 % F-score on all arguments. By measuring predicate-argument structure similarity based on the argument mapping, and formulating the predicate-argument structure mapping problem as a linear-assignment problem, the system achieved 84. 9 % F-score using automatic SRL, only 3. 7 % F-score lower than using gold standard SRL. The mapping output covered 49. 6 % of the annotated Chinese predicates (which contains predicateadjectives that often have no <b>parallel</b> <b>annotations</b> in English) and 80. 7 % of annotated English predicates, suggesting its potential as a valuable resource for improving word alignment and reranking MT output. ...|$|R
50|$|In proteogenomics, proteomic {{technologies}} such as mass spectrometry are used for improving gene <b>annotations.</b> <b>Parallel</b> analysis of the genome and the proteome facilitates discovery of post-translational modifications and proteolytic events, especially when comparing multiple species (comparative proteogenomics).|$|R
40|$|Abstract. Conventional {{iterative}} solvers for partial {{differential equations}} impose strict data dependencies between each solution point and its neighbors. When implemented in OpenMP, they repeatedly execute barrier synchronization in each iterative step {{to ensure that}} data dependencies are strictly satisfied. We propose new <b>parallel</b> <b>annotations</b> to support an asynchronous computation model for iterative solvers. At the outermost level, the ASYNC REDUCTION keyword is used to annotate the iterative loop {{as a candidate for}} asynchronous execution. The ASYNC REGION may contain inner loops annotated by ASYNC DO or ASYNC REDUCTION. If the compiler accepts the ASYNC REGION designation, it converts the iterative loop into a parallel section executed by multiple threads which divide the iterations of each ASYNC DO or ASYNC REDUCTION loop and execute them without having to synchronize through a conventional barrier. Comparing to directly implementing asynchronous algorithm using P-threads or existing OpenMP loop constructs, the iterative solver written with the new constructs gives the compiler the flexility to decide whether to implement the annotated candidates in the asynchronous manner. We present experimental results to show the benefit of using ASYNC loop constructs in 2 D and 3 D multigrid methods as well as an SOR-preconditioned conjugate gradient linear system solver. ...|$|R
40|$|We have {{previously}} reported on ProPOSEL, a purpose-built Prosody and PoS English Lexicon {{compatible with the}} Python Natural Language ToolKit. ProPOSEC is a new corpus research resource built using this lexicon, intended for distribution with the Aix-MARSEC dataset. ProPOSEC comprises multi-level <b>parallel</b> <b>annotations,</b> juxtaposing prosodic and syntactic information from {{different versions of the}} Spoken English Corpus, with canonical dictionary forms, in a query format optimized for Perl, Python, and text processing programs. The order and content of fields in the text file is as follows: (1) Aix-MARSEC file number; (2) word; (3) LOB PoS-tag; (4) C 5 PoS-tag; (5) Aix SAM-PA phonetic transcription; (6) SAM-PA phonetic transcription from ProPOSEL; (7) syllable count; (8) lexical stress pattern; (9) default content or function word tag; (10) DISC stressed and syllabified phonetic transcription; (11) alternative DISC representation, incorporating lexical stress pattern; (12) nested arrays of phonemes and tonic stress marks from Aix. As an experimental dataset, ProPOSEC can be used to study correlations between these annotation tiers, where significant findings are then expressed as additional features for phrasing models integral to Text-to-Speech and Speech Recognition. As a training set, ProPOSEC can be used for machine learning tasks in Information Retrieval and Speech Understanding systems...|$|R
50|$|In {{what is now}} {{commonly}} referred to as proteogenomics, peptides identified with mass spectrometry are used for improving gene annotations (for example, gene start sites) and protein <b>annotations.</b> <b>Parallel</b> analysis of the genome and the proteome facilitates discovery of post-translational modifications and proteolytic events, especially when comparing multiple species.|$|R
5000|$|An {{annotated}} edition is a literary work where marginal comments {{have been added}} to explain, interpret, or illuminate words, phrases, themes, or other elements of the text. The {{annotated edition}} is often something pursued by historical or literary scholars, as a secular <b>parallel</b> to exegesis <b>annotations</b> of the Bible.|$|R
40|$|Abstract. Conventional {{iterative}} solvers for partial {{differential equations}} impose strict data dependencies between each solution point and its neighbors. When implemented in OpenMP, they repeatedly execute barrier synchronization in each iterative step {{to ensure that}} data dependencies are strictly satisfied. We propose new <b>parallel</b> <b>annotations</b> to support an asynchronous computation model for iterative solvers. ASYNC DO annotates a loop whose iterations can be executed by multiple processors, as OpenMP parallel DO loops in Fortran (or parallel for loops in C), {{but it does not}} require barrier synchronization. ASYNC REDUCTION annotates a loop which performs parallel reduction operations but uses a relaxed tree barrier, instead of the conventional barrier, to synchronize the processors. When a number of ASYNC DO and ASYNC REDUCTION loops are embedded in an iterative loop annotated by ASYNC REGION, the iterative solver allows each data point to be updated using the value of its neighbors which {{may not be the most}} current, instead of forcing the processor to wait for the new value to arrive. We discuss how the compiler can transform an ASYNC REGION (with embedded ASYNC DO and ASYNC REDUCTION) into an OpenMP parallel section with relaxed synchronization. We present experimental results to show the benefit of using ASYNC loop constructs in 2 D and 3 D multigrid methods as well as an SOR-preconditioned conjugate gradient linear system solver. ...|$|R
5000|$|A source-to-source {{compiler}}, transcompiler or transpiler {{is a type}} of compiler {{that takes}} the source code of a program written in one programming language as its input and produces the equivalent source code in another programming language. A source-to-source compiler translates between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language. For example, a source-to-source compiler may perform a translation of a program from Pascal to C. An automatic parallelizing compiler will frequently take in a high level language program as an input and then transform the code and annotate it with <b>parallel</b> code <b>annotations</b> (e.g., OpenMP) or language constructs (e.g. Fortran's [...] statements).|$|R
40|$|High-level {{parallel}} languages {{relieve the}} programmer of low-level details of parallel programming, and several visualisation tools {{are available for}} performance evaluation of these languages. However, the abstraction level of the languages {{does not seem to}} be reflected by current visualisation tools. Much of the information the latter convey is the same as that for low-level parallel languages; e. g., suspensions and resumptions, interprocessor communication, and processor utilisation. Even higher level forms of display such as program graphs can be very detailed and difficult to interpret. Our thesis is that much of this information is too detailed for application programmers. This paper describes the visualisation tool ParSee which provides a high-level view of a parallel computation. This view is a static picture utilising colour to portray several performance metrics, each metric diagnosing the influence of a <b>parallel</b> program <b>annotation</b> on performance. The view is scalable over the [...] ...|$|R
40|$|PARFORMAN (<b>PARallel</b> FORMal <b>ANnotation</b> language) is a {{high-level}} specification language for expressing intended behavior or known types of error conditions when debugging or testing parallel programs. Models of intended or faulty target program {{behavior can be}} succinctly specified in PARFORMAN. These models are then compared with the actual behavior in terms of execution traces of events, in order to localize possible bugs. PARFORMAN {{can also be used}} as a general language for expressing computations over target program execution histories. PARFORMAN is based on a precise axiomatic model of target program behavior. This model, called H-space (History-space), is formally defined through a set of general axioms about three basic relations, {{which may or may not}} hold between two arbitrary events: they may be sequentially ordered (SEQ), they may be parallel (PAR), or one of them might be included in another composite event (IN). The general notion of composite event is exploited systematic [...] ...|$|R
40|$|<b>Annotation</b> {{interfaces}} for <b>parallel</b> corpora which fit in {{well with}} other tools {{can be very}} useful. We describe a set of annotation interfaces which fulfill this criterion. This set includes a sentence alignment interface, two different word or word group alignment interfaces and an initial version of a <b>parallel</b> syntactic <b>annotation</b> alignment interface. These tools {{can be used for}} manual alignment, or they can be used to correct automatic alignments. Manual alignment can be performed in combination with certain kinds of linguistic annotation. Most of these interfaces use a representation called the Shakti Standard Format that {{has been found to be}} very robust and has been used for large and successful projects. It ties together the different interfaces, so that the data created by them is portable across all tools which support this representation. The existence of a query language for data stored in this representation makes it possible to build tools that allow easy search and modification of annotated parallel data...|$|R
5000|$|The alpha {{version of}} lime 2.0 was {{announced}} on the Symfony blog on November 10, 2009. The second version of lime {{was built to}} be as backward compatible with the first version as was possible - the two parts of lime 2.0 that are not compatible with lime 1.0 are {{the configuration of the}} test harness and the [...] class. lime 2.0 includes support for xUnit output, source code <b>annotations,</b> <b>parallel</b> execution of tests, automatic generation of mock and stub objects, and operator overloading for data within tests. Unlike the first version of lime, lime 2.0 does have some dependencies on Symfony.|$|R
40|$|Word Sense Disambiguation aims {{to label}} {{the sense of}} a word that best applies in a given context. Graded word sense {{disambiguation}} relaxes the single label assumption, allowing for multiple sense labels with varying degrees of applicability. Training multi-label classifiers for such a task requires substantial amounts of annotated data, which is currently not available. We consider an alternate method of annotating graded senses using Word Sense Induction, which automatically learns the senses and their features from corpus properties. Our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using <b>parallel</b> graded sense <b>annotations.</b> We demonstrate that sense induction offers significant promise for accurate graded sense annotation. ...|$|R
40|$|Rich mark-up can {{considerably}} {{benefit the}} process of establishing bitext correspondences, that is, the task of providing correct identification and align-ment methods for text segments that are transla-tion equivalences {{of each other in}} a parallel corpus. We present a sentence alignment algorithm that, by taking advantage of previously annotated texts, ob-tains accuracy rates close to 100 %. The algorithm evaluates the similarity of the linguistic and extra-linguistic mark-up in both sides of a bitext. Given that annotations are neutral with respect to typolog-ical, grammatical and orthographical differences be-tween languages, rich mark-up becomes an optimal foundation to support bitext correspondences. The main originality of this approach is that it makes maximal use of annotations, which is a very sensible and efficient method for the exploitation of <b>parallel</b> corpora when <b>annotations</b> exist. ...|$|R
40|$|We present OGER, an {{annotation}} service {{built on}} top of OntoGene’s biomedical entity recognition system, which participates in the TIPS task (technical interoperability and performance of annotation servers) of the BeCalm (biomedical annotation metaserver) challenge. The annotation server is a web application tailored {{to the needs of}} the task, using an existing biomedical entity recognition suite. The core annotation module uses a knowledge-based strategy for term matching and entity linking. The server’s architecture allows <b>parallel</b> processing of <b>annotation</b> requests for an arbitrary number of documents from mixed sources. In the discussion, we show that network latency is responsible for significant overhead in the measurement of processing time. We compare the preliminary key performance indicators with an analysis drawn from the server’s log messages. We conclude that our annotation server is ready for the upcoming phases of the TIPS task...|$|R
40|$|In {{this paper}} we address methodological issues in the {{evaluation}} of a projectionbased framework for dependency parsing in which annotations for a source language are transfered to a target language using word alignments in a parallel corpus. The projected trees then constitute the training data for a data-driven parser in the target language. We discuss two problems that arise {{in the evaluation of}} such cross-lingual approaches. First, the annotation scheme underlying the source language annotations – and hence the projected target annotations and predictions of the parser derived from them – is likely to differ from previously existing gold standard test sets devised specifically for the target language. Second, the standard procedure of cross-validation cannot be performed in the absence of <b>parallel</b> gold standard <b>annotations,</b> so an alternative method has to be used to assess the generalization capabilities of the projected parsers. ...|$|R
40|$|This paper {{presents}} a novel, top-down, high-level approach to parallelizing file I/O. Each parallel file descriptor is annotated with a high-level specification, or template, {{of the expected}} <b>parallel</b> behaviour. The <b>annotations</b> are external to and independent of the source code. At run-time, all I/O using a parallel file descriptor adheres to the semantics of the selected template. By separating the parallel I/O specifications from the code, a user can quickly change the I/O behaviour without rewriting code. Templates can be composed hierarchically to construct more complex access patterns. Two sample parallel programs using these templates are compared against versions implemented in an existing parallel I/O system (PIOUS). The sample programs show {{that the use of}} parallel I/O templates are beneficial from both the performance and software engineering points of view. 1. Introduction The development of parallel applications has focused on computational parallelism. However, the corres [...] ...|$|R
40|$|The {{creation}} of parallel corpora {{has been very}} active especially since 90 s. The globalization, the extension of EU with new countries {{as well as the}} availability of open-source places for information, such as Wikipedia, DBPeadia, etc. required a multilingual approach towards the interpersonal and official communication. This status quo produced a lot of parallel data – especially administrative and political documents in several languages (EuroParl), but also news (SETIMES) and texts on various topics (wikipedia, bi- and multilingual web sites). However, the fast compilation of large amounts of data very often compromised in lower quality of paralleling texts. Here comes the challenge to discover the inconsistencies in these huge quantities of parallel data, to process them in adequate ways, and to exploit them for various applications: QA, Information Retrieval, Machine Translation, etc. The parallel corpora go beyond word-to-word alignments. They rely on dependency, constituent or semantic pairings. There appeared guidelines and tools for aligning linguistic structures, which raised the issue of transferability of aligning schemes from one language to another, and also for the compatibility among various resources. The topics, which fall within the scope of the workshop, include: Strategies for {{creation of}} annotated <b>parallel</b> corpora; <b>Annotation</b> guidelines for alignment; Annotation alignment transfer over languages; Tools for manual and automatic processing and exploitation of parallel corpora; Problems in manual and automatic alignment; Syntax-based and semantic-based approaches to using parallel corpora in MT...|$|R
40|$|It {{has been}} widely shown that GPGPU {{architectures}} offer large performance gains compared to their traditional CPU counterparts for many applications. The downside to these architectures is that the current programming models present numerous challenges to the programmer: lower-level languages, explicit data movement, loss of portability, and challenges in performance optimization. In this paper, we present novel methods and compiler transfor-mations that increase productivity by enabling users to easily pro-gram GPGPU architectures using the high productivity program-ming language Chapel. Rather than resorting to different <b>parallel</b> libraries or <b>annotations</b> for a given parallel platform, we leverage a language that has been designed from first principles to address the challenge of programming for parallelism and locality. This also {{has the advantage of}} being portable across distinct classes of paral-lel architectures, including desktop multicores, distributed memory clusters, large-scale shared memory, and now CPU-GPU hybrids. We present experimental results from the Parboil benchmark suite which demonstrate that codes written in Chapel achieve perfor-mance comparable to the original versions implemented in CUDA. 1...|$|R
40|$|This paper {{describes}} a computational architecture for accessing implicit {{information about the}} grammar of the languages included in a parallel corpus and exploiting it in an Optimality Theorystyle learning approach. Previous work on OT learning presupposes the existence of training data in which the underlying input has been annotated. This is an idealization that {{does not reflect the}} natural learning situation; and it also requires considerable effort to produce such training data for learning experiments with syntactic/semantic grammar models. In the proposed bootstrapping architecture, which will be underlying in the new PTOLEMAIOS project, the training data are sentences from a <b>parallel</b> corpus; manual <b>annotations</b> are only provided for a small set of seed sentences. The translations of the sentence into the other languages serve as clues for zeroing in on the assumed underlying meaning representations, which can be used as the input in OT-style learning. 1 Introduction: the goal for the PTOLEMAIOS project In this programmatic paper, an architecture for grammar learning based on parallel corpora is outlined. This proposed architecture is the target for the PTOLEMAIOS project. 2 More concretely, the projec...|$|R
40|$|Abstract—It {{has been}} widely shown that {{high-throughput}} computing architectures such as GPUs offer large performance gains compared to their traditional low-latency counterparts for many applications. The downside to these architectures is that the current programming models present numerous challenges to the programmer: lower-level languages, loss of portability across different architectures, explicit data movement, and challenges in performance optimization. In this paper, we present novel methods and compiler transformations that increase programmer productivity by enabling users of the language Chapel to provide just a single code implementation, that the compiler can then use to target not only conventional multiprocessors, but also highthroughput, as well as hybrid machines. Rather than resorting to different <b>parallel</b> libraries or <b>annotations</b> for a given parallel platform, we leverage a language that has been designed from first principles to address the challenge of programming for parallelism and locality. This also {{has the advantage of}} providing portability across different parallel architectures. Finally, we present experimental results from the Parboil benchmark suite which demonstrate that codes written in Chapel achieve performance comparable to the original versions implemented in CUDA on both GPUs and multicore platforms. I...|$|R
40|$|The lambda calculus, {{subject to}} typing restrictions, {{provides}} a syntax for the internal language of cartesian closed categories. This paper establishes a <b>parallel</b> result: staging <b>annotations,</b> subject to named level restrictions, provide a syntax for the internal language of Freyd categories, which {{are known to}} be in bijective correspondence with Arrows. The connection is made by interpreting multi-stage type systems as indexed functors from polynomial categories to their reindexings. This result applies only to multi-stage languages which are (1) homogeneous, (2) allow cross-stage persistence and (3) place no restrictions on the use of structural rules in typing derivations. Removing these restrictions and repeating the construction yields generalized arrows, of which Arrows are a particular case. A translation from well-typed multi-stage programs to single-stage GArrow terms is provided. The translation is defined by induction on the structure of the proof that the multi-stage program is well-typed, relying on information encoded in the proof's use of structural rules. Metalanguage designers can now factor out the syntactic machinery of metaprogramming by providing a single translation from staging syntax into expressions of generalized arrow type. Object language providers need only implement the functions of the generalized arrow type class in point-free style. Object language users may write metaprograms over these object languages in a point-ful style, using the same binding, scoping, abstraction, and application mechanisms in both the object language and metalanguage. This paper's principal contributions are the GArrow definition of Figures 2 and 3, the translation in Figure 5 and the category-theoretic semantics of Definition 16. An accompanying Coq proof formalizes the type system, translation procedure, and key theorems. Comment: This paper is obsolete and has been superceded by Multi-Level Programs are Generalized Arrows available here: [URL]...|$|R

