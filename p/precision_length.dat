19|71|Public
500|$|Interferometers {{are widely}} used in science and {{industry}} for the measurement of small displacements, refractive index changes and surface irregularities. [...] In an interferometer, light from a single source is split into two beams that travel different optical paths, then combined again to produce interference. [...] The resulting interference fringes give information about the difference in optical path length. [...] In analytical science, interferometers are used to measure lengths and the shape of optical components with nanometer precision; they are the highest <b>precision</b> <b>length</b> measuring instruments existing. [...] In Fourier transform spectroscopy {{they are used to}} analyze light containing features of absorption or emission associated with a substance or mixture. [...] An astronomical interferometer consists of two or more separate telescopes that combine their signals, offering a resolution equivalent to that of a telescope of diameter equal to the largest separation between its individual elements.|$|E
2500|$|Invariable pendulums: [...] Kater {{introduced}} {{the idea of}} relative gravity measurements, to supplement the absolute measurements made by a Kater's pendulum. [...] Comparing the gravity at two different points was an easier process than measuring it absolutely by the Kater method. All that was necessary was to time the period of an ordinary (single pivot) pendulum at the first point, then transport the pendulum to the other point and time its period there. Since the pendulum's length was constant, from (1) {{the ratio of the}} gravitational accelerations was equal to the inverse of the ratio of the periods squared, and no <b>precision</b> <b>length</b> measurements were necessary. So once the gravity had been measured absolutely at some central station, by the Kater or other accurate method, the gravity at other points could be found by swinging pendulums at the central station and then taking them to the other location and timing their swing there. Kater made up a set of [...] "invariable" [...] pendulums, with only one knife edge pivot, which were taken to many countries after first being swung at a central station at Kew Observatory, UK.|$|E
50|$|Pratt & Whitney Measurement Systems is an American {{manufacturer}} of <b>precision</b> <b>length</b> measuring metrology instruments.  Main product lines include universal comparators, bench micrometers, and inspection gaging systems.  These instruments primarily use laser interferometers, encoders and LVDT's and are primarily used in quality departments, calibration laboratories, and in manufacturing environments.|$|E
5000|$|Tungsten carbide is {{a common}} {{material}} used {{in the manufacture of}} gauge blocks, used as a system for producing <b>precision</b> <b>lengths</b> in dimensional metrology.|$|R
5000|$|Gauge blocks (also {{known as}} gage blocks, Johansson gauges, slip gauges, or Jo blocks) are {{a system for}} {{producing}} <b>precision</b> <b>lengths.</b> The individual gauge block is a metal or ceramic block that has been precision ground and lapped to a specific thickness. Gauge blocks come in sets of blocks {{with a range of}} standard lengths. In use, the blocks are stacked to make up a desired length.|$|R
30|$|As expected, {{precision}} consistently {{improved with}} increasing cumulative transect length, and locations with lower DWD volumes required longer transects {{to achieve a}} given level of precision. We developed models relating <b>precision,</b> transect <b>length,</b> and DWD volume {{that allows us to}} gauge a suitable LIS transect <b>length</b> for desired <b>precision</b> levels.|$|R
5000|$|Due to thermal expansion, <b>precision</b> <b>length</b> {{measurements}} {{need to be}} made at (or converted to) {{a defined}} temperature. ISO 1 helps in comparing measurements by defining such a reference temperature. The reference temperature of 20 °C was adopted by the CIPM on 15 April 1931, and became ISO recommendation number 1 in 1951. [...] It soon replaced worldwide other reference temperatures for length measurements that manufacturers of precision equipment had used before, including 0 °C, 62 °F, and 25 °C. Among the reasons for choosing 20 °C was that this was a comfortable and practical workshop temperature and that it resulted in an integer value on both the Celsius and Fahrenheit scales.|$|E
50|$|Interferometers {{are widely}} used in science and {{industry}} for the measurement of small displacements, refractive index changes and surface irregularities. In an interferometer, light from a single source is split into two beams that travel different optical paths, then combined again to produce interference. The resulting interference fringes give information about the difference in optical path length. In analytical science, interferometers are used to measure lengths and the shape of optical components with nanometer precision; they are the highest <b>precision</b> <b>length</b> measuring instruments existing. In Fourier transform spectroscopy {{they are used to}} analyze light containing features of absorption or emission associated with a substance or mixture. An astronomical interferometer consists of two or more separate telescopes that combine their signals, offering a resolution equivalent to that of a telescope of diameter equal to the largest separation between its individual elements.|$|E
5000|$|Invariable pendulums: Kater {{introduced}} {{the idea of}} relative gravity measurements, to supplement the absolute measurements made by a Kater's pendulum. [...] Comparing the gravity at two different points was an easier process than measuring it absolutely by the Kater method. All that was necessary was to time the period of an ordinary (single pivot) pendulum at the first point, then transport the pendulum to the other point and time its period there. Since the pendulum's length was constant, from (1) {{the ratio of the}} gravitational accelerations was equal to the inverse of the ratio of the periods squared, and no <b>precision</b> <b>length</b> measurements were necessary. So once the gravity had been measured absolutely at some central station, by the Kater or other accurate method, the gravity at other points could be found by swinging pendulums at the central station and then taking them to the other location and timing their swing there. Kater made up a set of [...] "invariable" [...] pendulums, with only one knife edge pivot, which were taken to many countries after first being swung at a central station at Kew Observatory, UK.|$|E
50|$|After eight {{iterations}} {{the method}} produced a quadratic factor {{that contains the}} roots −1/3 and −3 within the represented <b>precision.</b> The step <b>length</b> from the fourth iteration on demonstrates the superlinear speed of convergence.|$|R
40|$|Abstract. We {{show that}} quantum {{mechanics}} and general relativity imply {{the existence of}} a minimal length. To be more precise, we show that no operational device subject to quantum mechanics, general relativity and causality could exclude the discreteness of spacetime on lengths shorter than the Planck length. We then consider the fundamental limit coming from quantum mechanics, general relativity and causality on the <b>precision</b> of <b>length</b> measurement. PACS. 04. 20. -q; 03. 65. -w...|$|R
40|$|We {{show that}} quantum {{mechanics}} and general relativity imply {{the existence of}} a minimal length. To be more precise, we show that no operational device subject to quantum mechanics, general relativity and causality could exclude the discreteness of spacetime on lengths shorter than the Planck length. We then consider the fundamental limit coming from quantum mechanics, general relativity and causality on the <b>precision</b> of <b>length</b> measurement. © 2008 Springer-Verlag / Società Italiana di Fisica. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Abstract − In <b>precision</b> <b>length</b> {{measurements}} {{the accurate}} and traceable {{value of the}} linear thermal expansion coefficient (LTEC) is needed. A device for interferometric determination of the LTEC of gauge blocks has been constructed. Minimum temperature gradients in a gauge block with 500 mm maximum length and relatively fast operation were the objectives of this project. Based on uncertainty analysis LTEC can be measured with a standard uncertainty of 0, 02 × 10 - 6 1 /K for 100 mm gauge blocks...|$|E
40|$|Investigations on {{standing}} wave (SW) interferometry come in focus {{of interest in}} the course of ongoing miniaturization of high <b>precision</b> <b>length</b> measurement systems. A key problem within these efforts is the development of a transparent ultra-thin photodetector for sampling the intensity profile of the generated SW. Group III-materials are promising candidates to ensure a good photodetector performance combined with the required optical transparency. In this work, we report on the interrelation of strain and dislocation density along with the influence of the structural properties on the sensitivity of double-heterostructure III-nitride photodetectors grown by molecular beam and metal organic vapour phase epitaxy...|$|E
40|$|<b>Precision</b> <b>length</b> {{measurements}} {{provide valuable}} insights about the fundamental properties of space-time. The Holometer {{is a research}} program to both experimentally probe signatures of the Planck scale and to extend the accessible frequency range from kHz up to MHz for gravitational wave searches. The instrument consists of separate yet identical 39 -meter Michelson interferometers operated at Fermi National Accelerator Laboratory, which can reach length sensitivities better than 10 ^(- 20) m/√Hz within the 1 - 10 MHz frequency range. The Holometer is fully operational with 130 of hours of science quality data obtained during the first observational campaign...|$|E
40|$|A iimdamentdly new moIec&r-biology {{technology}} in constructing restriction maps, Optical Mapping, {{has been developed}} by Schwartz et al. (1993). Using this method restriction maps are constructed by measuring the relevant fluorescence jnt&ty and length measurements. However, it is &&tilt to directly estimate the restriction site lo-C&XI. S of single DNA molecules based on these optical mapping data because of the <b>precision</b> of <b>length</b> measurements and the unknown number of true restriction sites in the data. We propose {{the use of a}} hierarchical Bayes model based on a mixture model with normals and random noise. In this model, we explicitly consider the missing observatio...|$|R
40|$|Abstract – It {{is shown}} that {{symbolic}} sequence {{can be used}} to reconstruct a trajectory of a chaotic system with high precision. Examples of construction of symbolic sequence and trajectory reconstruction are given. A method that demonstrates exponential increase in <b>precision</b> with the <b>length</b> of processing is developed. Index terms – Dynamic chaos, chaotic oscillator, symbolic dynamics, information theory. I...|$|R
50|$|In 2005 {{the minimum}} length of these boats was reduced from 4 meters down to 3.5 meters, causing {{a flurry of}} new, faster boat designs which are able to {{navigate}} courses with more speed and <b>precision.</b> The shorter <b>length</b> also allows for easier navigation and less boat damage in the smaller man made river beds that are prevalent in current elite competitions.|$|R
40|$|Several km-scale gravitational-wave {{detectors}} {{have been}} constructed world wide. These instruments combine a number of advanced technologies to push the limits of <b>precision</b> <b>length</b> measurement. The core devices are laser interferometers of a new kind; developed from the classical Michelson topology these interferometers integrate additional optical elements, which significantly change {{the properties of the}} optical system. Much of the design and analy-sis of these laser interferometers can be performed using well-known classical optical tech-niques, however, the complex optical layouts provide a new challenge. In this review we give a textbook-style introduction to the optical science required for the understanding of modern gravitational wave detectors, as well as other high-precision laser interferometers. In addition, we provide a number of examples for a freely available interferometer simulation software and encourage the reader to use these examples to gain hands-on experience with the discussed optical methods. 1 a...|$|E
40|$|An {{evaluation}} function is presented, which tests {{the results of}} an object boundary segmentation algorithm to choose an optimal approximation using lines and circular arcs. An angle detection procedure is used to find sharp corners of the object boundary. Because this procedure does not detect smooth changes, the boundary segments defined by the sharp corners are further processed with a Gauss-filter-based method to find the possibly remaining connection points. Between these points and the sharp corners the boundary is approximated by means of lines or circular arcs. Because the processing steps have different control parameters, different parameter sets are automatically tested. Using a function which evaluates the <b>precision,</b> <b>length,</b> and quantity of line segments and circular arcs, an optimal segmentation is chosen. 1 Introduction To recognize an object, normally a symbolic description of its boundary is used. If the description consists of primitives like line segments [...] ...|$|E
40|$|Planners {{of surveys}} and {{experiments}} that partially identify parameters of interest face trade offs between using limited resources to reduce sampling error {{and using them}} to reduce the extent of partial identification. I evaluate these trade offs in a simple statistical problem with normally distributed sample data and interval partial identification using different frequentist measures of inference <b>precision</b> (<b>length</b> of confidence intervals, minimax mean sqaured error and mean absolute deviation, minimax regret for treatment choice) and analogous Bayes measures with a flat prior. The relative value of collecting data with better identification properties (e. g., increasing response rates in surveys) depends crucially on {{the choice of the}} measure of precision. When the extent of partial identification is significant in comparison to sampling error, the length of confidence intervals, which has been used most often, assigns the lowest value to improving identification among the measures considered. statistical treatment choice; survey planning; nonresponse; mean squared error; mean absolute deviation; minimax regret...|$|E
40|$|We {{show that}} quantum {{mechanics}} and general relativity imply {{the existence of}} a minimal length. To be more precise, we show that no operational device subject to quantum mechanics, general relativity and causality could exclude the discreteness of spacetime on lengths shorter than the Plan& length. We then consider the fundamental limit coming from quantum mechanics, general relativity and causality on the <b>precision</b> of <b>length</b> measurement. © 2008 by World Scientific Publishing Co. Pte. Ltd. SCOPUS: cp. p 44 th International School of Subnuclear Physics - The Logic of Nature, Complexity and New Physics: From Quark-Gluon Plasma to Superstrings, Quantum Gravity and Beyond, ISSP 2006; Erice, Sicily; Italy; 29 August 2006 through 7 September 2006;info:eu-repo/semantics/publishe...|$|R
40|$|Abstract. In {{order to}} obtain spectra from EFPI {{interference}} with the noise in the signal spectrum which can get demodulation <b>precision</b> cavity <b>length,</b> HHT method is proposed based gas detection algorithm processing EFPI fiber optic signals. The principle of HHT algorithm is {{to deal with the}} noise spectral signal EFPI through empirical mode decomposition (EMD), a group based on the signal obtained their intrinsic mode function (IMF), through to the IMF for Hilbert transform the Hilbert spectrum for time-frequency analysis. Through simulation and numerical analysis, the algorithm to solve the original non-stationary signal interference spectrum, nonlinear shortcomings, through a series of changes to remove noise, which can reduce the difficulty of data processing and increase the accuracy of the gas signal analysis. 1...|$|R
5000|$|Since {{additional}} {{work is done}} in each [...] "station" [...] of the die, {{it is important that}} the strip be advanced very precisely so that it aligns within a few thousandths of an inch as it moves from station to station. Bullet shaped or conical [...] "pilots" [...] enter previously pierced round holes in the strip to assure this alignment since the feeding mechanism usually cannot provide the necessary <b>precision</b> in feed <b>length.</b>|$|R
40|$|Abstract:- The paper {{introduces}} {{an architecture}} for a {{direct digital synthesizer}} based on algorithmic direct amplitude generation. The circuit implementing the algorithm has a gate count proportional to digital <b>precision</b> <b>length</b> and in its simplest form includes one compare, one addition and several increments per iteration step. A phase compensation method is described to eliminate the nonuniformity in timing of the samples inherent to the algorithm. The paper also presents {{an analysis of the}} proposed architecture focusing on the amplitude samples timing accuracy versus generated signal frequency. A FPGA implementation of the architecture was simulated using a VHDL description to validate the solution as proposed. Key-Words:- DDS amplitude centered architecture, Jordan's nonparametric curve generator, signal generation solutions in FPGA. 1 Introduction. Digital implementation is one of the trends in the design of electronic systems recently. The direct digital synthesis (DDS) is such a example. The analog solution of sinusoidal signal generation using PLL'...|$|E
40|$|Planners {{of surveys}} and {{experiments}} that partially identify parameters of interest face trade o¤s between using limited resources to reduce sampling error {{and using them}} to reduce the extent of partial identi 8 ̆ 5 cation. I evaluate these trade o¤s in a simple statistical problem with normally distributed sample data and interval partial identi 8 ̆ 5 cation using di¤erent frequentist measures of inference <b>precision</b> (<b>length</b> of con 8 ̆ 5 dence intervals, minimax mean sqaured error and mean absolute deviation, minimax regret for treatment choice) and analogous Bayes measures with a at prior. The relative value of collecting data with better identi cation properties (e. g., increasing response rates in surveys) depends crucially on {{the choice of the}} measure of precision. When the extent of partial identi 8 ̆ 5 cation is signi 8 ̆ 5 cant in comparison to sampling error, the length of con 8 ̆ 5 dence intervals, which has been used most often, assigns the lowest value to improving identi 8 ̆ 5 cation among the measures considered...|$|E
40|$|Abstract:-The paper {{presents}} {{an analysis of}} a novel DDS architecture based on direct amplitude algorithmic generation in its capacity to generate pure and high frequency signals. The algorithm implementation requires very few resources and is linear in digital <b>precision</b> <b>length.</b> In its simplest form includes one compare, two additions and several increments per iteration step. A solution to compensate the known major drawback of the algorithm, sample phase non uniformity in time, is presented. The versatility of direct digital synthesizers in frequency switching and phase modulation applications is shown to be preserved in the new architecture. The paper also {{presents an}} analysis of the potential of the proposed architecture performance as reflected in the low harmonic spectrum of the quadrature output signals. A FPGA implementation of the architecture was synthesized and simulated using a VHDL description to validate the solution as proposed. Synthesis reports are compared with other FPGA direct frequency synthesizer implementations as reported in recent literature. Key-Words:- amplitude centered DDS architecture, generated sine/cosine spectrum, FPGA implementation...|$|E
40|$|We {{determine}} {{the improvement in}} baseline <b>length</b> <b>precision</b> and accuracy using new atmospheric delay mapping functions and MTT by analyzing the NASA Crustal Dynamics Project research and development (R&D) experiments and the International Radio Interferometric Surveying (IRIS) A experiments. These mapping functions reduce baseline length scatter by about 20 % below that using the CfA 2. 2 dry and Chao wet mapping functions. With the newer mapping functions, average station vertical scatter inferred from observed <b>length</b> <b>precision</b> (given by <b>length</b> repeatabilites) is 11. 4 mm for the 1987 - 1990 monthly R&D series of experiments and 5. 6 mm for the 3 -week-long extended research and development experiment (ERDE) series. The inferred monthly R&D station vertical scatter is reduced by 2 mm or by 7 mm is a root-sum-square (rss) sense. Length repeatabilities are optimum when observations below a 7 - 8 deg elevation cutoff are removed from the geodetic solution. Analyses of IRIS-A data from 1984 through 1991 and the monthly R&D experiments both yielded a nonatmospheric unmodeled station vertical error or about 8 mm. In addition, analysis of the IRIS-A exeriments revealed systematic effects {{in the evolution of}} some baseline length measurements. The length rate of change has an apparent acceleration, and the length evolution has a quasi-annual signature. We show that the origin of these effects is unlikely to be related to atmospheric modeling errors. Rates of change of the transatlantic Westford-Wettzell and Richmond-Wettzell baseline lengths calculated from 1988 through 1991 agree with the NUVEL- 1 plate motion model (Argus and Gordon, 1991) to within 1 mm/yr. Short-term (less than 90 days) variations of IRIS-A baseline length measurements contribute more than 90 % of the observed scatter about a best fit line, and this short-term scatter has large variations on an annual time scale...|$|R
40|$|Discrete Cosine Transform (DCT) is {{the main}} {{transform}} used in image/video compression standards. Many fast algorithms are available to reduce the computation in realizing DCT/IDCT, and these can be implemented either in fixed-point or floating-point implementation. IEEE- 1180 standard stipulates the maximum possible error for any IDCT implementation. The minimum word length required to implement IDCT needs to be reduced to the possible extent as cost and energy consumption are directly proportional to the word length and are crucial for a mobile device. Current paper describes the ways of improving the accuracy of IDCT by exploiting the characterstics of image and video sequences, when the available word length is limited. In this paper, the results are presented for a 12 -bit fixed-point processor, on which decoders are developed. Same algorithms {{can be used to}} improve the accuracy with processors having different <b>precision</b> (word <b>length)</b> also...|$|R
30|$|PRO {{assessment}} still mostly {{relies on}} traditional questionnaires that present {{the same set}} of questions at every assessment to all patients and therefore are frequently referred to as being “static questionnaires”. The use of static PRO measures has shortcomings in terms of measurement <b>precision</b> and <b>length</b> as well as with regard to the comparability of scores across assessment instruments. Static questionnaires mostly are based on classical test theory and require a considerable number of items to cover the whole range of an assessed construct. Consequently, patients may be presented with a high proportion of items inappropriate for their current health state that, at the same time, do not increase measurement precision. For example, a patient who has difficulty taking a short walk will not provide additional information to the clinician or researcher when being asked questions about strenuous sports activities [3]. Moreover, answering such uninformative and inappropriate questions may be frustrating and affect patient compliance with questionnaire completion.|$|R
40|$|Calibration {{machines}} up to a {{torque range}} of 20 kN·m need extensive design if the force is performed by dead weights. A machine {{based on two}} force reference standards and a vertical torque axis may serve to avoid these specific problems. The precision force transducers work at a lever arm of <b>precision</b> <b>length.</b> The test piece is mounted in vertical arrangement between a hydraulically operating rotary-actuator and the torque measuring system. Such a calibration machine allows additionally performing easily continuous calibration. Regarding an overall relative uncertainty not greater than 2 · 10 - 4 there are some significant problems. The estimation of the uncertainty has been proved by comparative tests with the national torque standard by the PTB according to the guidelines EA- 10 / 14 and EA- 2 / 03. Boundary conditions The best measuring capability is given by a system with a lever-arm (beam) with direct effect of mass {{and the influence of}} gravity and earth-acceleration (Figure 1). To compare this type of calibration machine with the introduced type we will discuss the most important points: Figure 1 : Principially Torque Primary Standar...|$|E
40|$|OBJECTIVE: Acupuncture is {{a complex}} {{intervention}} consisting of specific and non-specific components. Acupuncture studies more frequently focus on collecting data from the patients’ perspective and response, but the acupuncturist’s role remains relatively unclear. In order to investigate potential non-mechanical active factors originating from the acupuncturist and transmitted to the patient during treatment, two novel devices for basic research in acupuncture were designed. The Acuplicator allows the researcher to insert needles without touching the needles themselves, while the Veliusator locks the needle in its place so that no mechanical movement can be transferred. METHODS: The Acuplicator was used to insert needles at Neiguan (PC 6) on the right forearm of 23 volunteers. The insertion depth was measured using a depth gauge. The transfer of mechanical movements from the handle to the tip was detected with a <b>precision</b> <b>length</b> gauge with a motoric-tactile sensor. RESULTS: The mean insertion depth was (12. 3 ± 1. 5) mm (range 9. 5 to 15. 0 mm). Even with intense manipulation of the needle handle, no movements within ± 1 μm could be detected at the tip when the needle was locked. CONCLUSION: With these two devices {{it will be possible}} to investigate the influence of non-mechanical components such as therapeutic qi in acupuncture...|$|E
40|$|ATLAS is {{the largest}} {{particle}} detector under construction at CERN Geneva. Frequency scanning interferometry (FSI), also known as absolute distance interferometry, {{will be used to}} monitor shape changes of the SCT (semiconductor tracker), a particle tracker in the inaccessible, high radiation environment at the centre of ATLAS. Geodetic grids with several hundred fibre-coupled interferometers (30 mm to 1. 5 m long) will be measured simultaneously. These lengths will be measured by tuning two lasers and comparing the resulting phase shifts in grid line interferometers, (GLIs) with phase shifts in a reference interferometer. The novel inexpensive GLI design uses diverging beams to reduce sensitivity to misalignment, albeit with weaker signals. One micrometre <b>precision</b> <b>length</b> measurements of grid lines will allow 10 μm precision tracker shape corrections to be fed into ATLAS particle tracking analysis. The technique was demonstrated by measuring a 400 mm interferometer to better than 400 nm and a 1195 mm interferometer to better than 250 nm. Precise measurements were possible, even with poor quality signals, using numerical analysis of thousands of intensity samples. Errors due to drifts in interferometer length were substantially reduced using two lasers tuned in opposite directions and the precision was further improved by linking measurements made at widely separated laser frequencies. © 2004 IOP Publishing Ltd...|$|E
40|$|Advanced accelerators {{as well as}} 4 th {{generation}} light sources require extremely precise timing distribution. Energy recovery linacs, for example, require precise tim-ing of {{the electron}} bunches reentering the linac {{in order to minimize}} the disturbance of the RF amplitude and phase to evade resonant effects on the subsequent acceler-ated bunch. The timing distribution system has to maintain femtosecond <b>precision</b> over <b>lengths</b> ranging from several hundred meters to a few kilometers. We discuss potential optical master oscillators with exceptionally low timing jitter {{that can be used for}} ultra-precise timing distribution in such facilities. A promising approach is the use of a mode-locked laser that generates pulses with an ultra-stable repetition rate, distributed through fiber links. A good candidate is a mode-locked Erbium-doped fiber laser, featuring very low high-frequency noise in comparison to presently available microwave sources. Laser systems locked to atomic references are discussed, which eventually enable synchronization of independent lasers in the facility with potentially few-femtosecond precision as well as pump-probe measurements with attosecond time resolution...|$|R
40|$|In a {{previous}} paper Neerchal, Lacayo and Nussbaum (2007) explored {{the behavior of}} the well-known problem of finding the optimal sample size for obtaining a confidence interval of a pre-assigned <b>precision</b> (or <b>length)</b> for the proportion parameter of a finite or infinite binary population. We illustrated some special problems that arise due to the discreteness of the population distribution and precision that is measured by the length of the interval rather than by the variance. Specifically, the confidence level of an interval of fixed length does not necessarily increase as the sample size increases. However, when such confidence levels are computed using normal approximations, we see a monotonic behavior. In this paper, we consider the corresponding problem under the Poisson approximation and show that for this distribution monotonicity does not hold and one should be beware of this seeming peculiarity in recommending sample sizes for studies involving estimation of means or proportions...|$|R
40|$|The next {{generation}} of electron linacs will fill two different roles: 1. ultra-low emittance, very high power accelerators for linear colliders and 2. ultra-short bunch, high stability accelerators for SASE X-ray production. In either case, precision control based on non-invasive, reliable, beam instrumentation will be required. For the linear collider, low emittance transport is an important concern for both warm and superconducting linacs. Instrumentation {{will be used for}} control and diagnostics will be used to validate emittance preserving strategies, such as beam based alignment and dispersion- free steering. Tests at the KEK ATF and the SLAC FFTB have demonstrated the required performance of beam position and beam size monitors. Linacs intended for FEL's will require <b>precision</b> bunch <b>length</b> diagnostics because of expected non-linear micro-bunching processes. A wide variety of devices are now in development at FEL prototypes, including TTF 2 at DESY and SPPS at SLAC. We present a review of the new diagnostic systems...|$|R
