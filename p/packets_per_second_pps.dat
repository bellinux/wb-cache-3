3|10000|Public
40|$|This paper {{presents}} traffic {{analyses for}} Xbox System Link traffic. The {{goal of this}} analyses {{is to create a}} traffic model for Xbox traffic {{that can be used for}} network evaluation purposes. Traffic characteristics observed were: packet length, packet inter-arrival times, <b>packets</b> <b>per</b> <b>second</b> (<b>PPS)</b> and data rates. We found that System Link traffic is very periodic. Servers send update packets every 40 ms to all clients. Clients transmit two types of packets to the server. One every 40 ms and the other one every 201 ms. Server to Client PPS was 25 and Client to Server PPS 30 for all experiments. Packet lengths depend on the number of players participating in a game. Based on these observations a simple ns 2 simulation model for Server and Client Xboxes was developed...|$|E
40|$|Abstract- This paper {{presents}} traffic {{analyses for}} Xbox System Link traffic. The {{goal of this}} analyses {{is to create a}} traffic model for Xbox traffic {{that can be used for}} network evaluation purposes. Traffic characteristics observed were: packet length, packet inter-arrival times, <b>packets</b> <b>per</b> <b>second</b> (<b>PPS)</b> and data rates. We found that System Link traffic is very periodic. Servers send update packets every 40 ms to all clients. Clients transmit two types of packets to the server. One every 40 ms and the other one every 201 ms. Server to Client PPS was 25 and Client to Server PPS 30 for all experiments. Packet lengths depend on the number of players participating in a game. Based on these observations a simple ns 2 simulation model for Server and Client Xboxes was developed. Keywords- Traffic analyses, network games, Xbox, Ethernet, LAN, Halo,System Link I...|$|E
40|$|This work {{deals with}} the {{security}} of computer networks based on TCP/IP protocol stack. The main objective {{is to develop a}} generator of DoS flooding attacks which carries out attacks such SYN flood, RST flood, UDP flood, ICMP flood, ARP flood, DNS flood and DHCP starvation. The theoretical part describes the features of the mentioned attacks and protocols or mechanisms associated with them. Next part {{deals with the}} comparison of selected tools (Hping 3, Mausezahn, Trafgen) in terms of number of <b>packets</b> <b>per</b> <b>second</b> (<b>pps)</b> and the link utilization (MB/s). The practical part describes design and implementation of the new attacking tool. There is explained the importance of it’s individual modules, it’s installation and usage options. New tool is also being tested. Then there is described the development, options and installation of control interface which {{is in the form of}} web application...|$|E
50|$|When used in {{the context}} of {{communication}} networks, such as Ethernet or packet radio, throughput or network throughput is the rate of successful message delivery over a communication channel. The data these messages belong to may be delivered over a physical or logical link, or it can pass through a certain network node. Throughput is usually measured in bits <b>per</b> <b>second</b> (bit/s or bps), and sometimes in data <b>packets</b> <b>per</b> <b>second</b> (p/s or <b>pps)</b> or data <b>packets</b> <b>per</b> time slot.|$|R
3000|$|... of n pair of {{features}} which represent bidirectional behavior of flows, such as: bytes <b>per</b> <b>second</b> (forward) & bytes <b>per</b> <b>second</b> (backward), <b>packets</b> <b>per</b> <b>second</b> (forward) & <b>packets</b> <b>per</b> <b>second</b> (backward), avg. payload (forward) & avg. payload (backward), etc. We computed the classification error obtained from classes-to-cluster evaluation with S [...]...|$|R
30|$|For {{the purpose}} of clustering, we extract a five-feature vector for every flow: Protocol, <b>Packets</b> <b>per</b> <b>second</b> (f/w), <b>Packets</b> <b>per</b> <b>second</b> (b/w), Avg. Payload size (f/w), and Avg. Payload size (b/w)Protocol, <b>Packets</b> <b>per</b> <b>second</b> (f/w), <b>Packets</b> <b>per</b> <b>second</b> (b/w), Avg. Payload size (f/w), and Avg. Payload size (b/w), with ‘f/w’ and ‘b/w’ {{signifying}} the forward and the backward direction of the flow, respectively. The primary motivation behind the choice of these features is to exploit the bidirectional nature of P 2 P traffic and separate flows based on their ‘behavior’ {{in terms of the}} transport layer protocol used and packets and payload exchanged. A more detailed discussion on the choice of features and clustering algorithm will follow in Section 4.|$|R
3000|$|... ○ The {{transmission}} rate for every communication {{is presented in}} {{transmission rate}} field and every data packet. Its values are said in terms of <b>packets</b> <b>per</b> <b>second,</b> typically, 1, 20, 20, 30, and 40 <b>packets</b> <b>per</b> <b>second</b> for communication (flow) 5, 4, 2, 3, and 1, respectively.|$|R
40|$|SMP Click is a {{software}} router that provides both flexibility and high performance on stock multiprocessor PC hardware. It achieves high performance using device, buffer, and queue management techniques optimized for multiprocessor rout-ing. It allows vendors or network administrators to configure the router {{in a way}} that indicates parallelizable packet pro-cessing tasks, and adaptively load-balances those tasks across the available CPUs. SMP Click’s absolute performance is high: it can forward 494, 000 64 -byte IP <b>packets</b> <b>per</b> <b>second</b> on a 2 -CPU 500 MHz Intel Xeon machine, compared to 302, 000 <b>packets</b> <b>per</b> <b>second</b> for uniprocessor Click. SMP Click also scales well for CPU intensive tasks: 4 -CPU SMP Click can encrypt and forward 87, 000 64 -byte <b>packets</b> <b>per</b> <b>second</b> using IPSec 3 DES, com-pared to 23, 000 <b>packets</b> <b>per</b> <b>second</b> for uniprocessor Click. ...|$|R
50|$|Up to 450,000 <b>packets</b> <b>per</b> <b>second</b> {{aggregate}} packet-forwarding rate.|$|R
5000|$|A {{very high}} packet {{transmission}} rate (approximately 100,000 <b>packets</b> <b>per</b> <b>second)</b> ...|$|R
5000|$|... a is {{the average}} arrival rate of packets (e.g. in <b>packets</b> <b>per</b> <b>second)</b> ...|$|R
30|$|The {{size of the}} each {{transmitted}} M 2 M <b>packet</b> <b>per</b> <b>second</b> is P_size= 656 bits.|$|R
3000|$|... (Feasible {{end-to-end}} throughput) The end-to-end throughput of λ <b>packets</b> <b>per</b> <b>second</b> for a node {{is feasible}} when the end-to-end delay is finite.|$|R
40|$|Rights to {{individual}} papers {{remain with the}} author or the author's employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must {{be included in the}} reproduced paper. USENIX acknowledges all trademarks herein. Flexible Control of Parallelism in a Multiprocessor PC Router SMP Click is a software router that provides both flexibility and high performance on stock multiprocessor PC hardware. It achieves high performance using device, buffer, and queue management techniques optimized for multiprocessor routing. It allows vendors or network administrators to configure the router in a way that indicates parallelizable packet processing tasks, and adaptively load-balances those tasks across the available CPUs. SMP Click’s absolute performance is high: it can forward 494, 000 64 -byte IP <b>packets</b> <b>per</b> <b>second</b> on a 2 -CPU 500 MHz Intel Xeon machine, compared to 302, 000 <b>packets</b> <b>per</b> <b>second</b> for uniprocessor Click. SMP Click also scales well for CPU intensive tasks: 4 -CPU SMP Click can encrypt and forward 87, 000 64 -byte <b>packets</b> <b>per</b> <b>second</b> using IPSec 3 DES, compared to 23, 000 <b>packets</b> <b>per</b> <b>second</b> for uniprocessor Click. ...|$|R
3000|$|Assuming {{the voice}} codec generates 33 <b>packets</b> <b>per</b> <b>second</b> at a data rate of 6.3 kbps, the packet interarrival time in _p [...]...|$|R
3000|$|... aThe main {{parameters}} of the simulation are summarized in Table  1 (see Section 4). The frequency of Hello messages is one <b>packet</b> <b>per</b> <b>second.</b>|$|R
50|$|Eventually, {{the shared}} {{resource}} became a bottleneck, with {{the limit of}} shared bus speed being roughly 2 million <b>packets</b> <b>per</b> <b>second</b> (Mpps). Crossbar fabrics broke through this bottleneck.|$|R
30|$|Threshold: This {{marks the}} cutoff {{frequency}} after which any particular IP is marked as attacker. We have used two threshold values of 500 and 1000 <b>packets</b> <b>per</b> <b>second</b> for our evaluations.|$|R
30|$|Goodput: {{total number}} of useful (data) packets {{received}} at the application layer divided by the total duration of the flow and averaged for every flow, giving a ratio of <b>packets</b> <b>per</b> <b>second.</b>|$|R
50|$|The {{asymptotic}} bandwidth (formally asymptotic throughput) for {{a network}} {{is the measure}} of maximum throughput for a greedy source, for example when the message size (the number of <b>packets</b> <b>per</b> <b>second</b> from a source) approaches infinity.|$|R
30|$|Regarding the {{evaluation}} of the admission control module, two scenarios were considered. For the first, each sensor node within the network was set to generate constant bit rate traffic of about 0.5 <b>packets</b> <b>per</b> <b>second,</b> where the network is considered uncongested and able to admit a new sensor node while maintaining a PRR of, at least, 98 %. In the second scenario, the sensor nodes were set to send 1 data <b>packet</b> <b>per</b> <b>second</b> to the network, and it has to fulfil a PRR of, at least, 91 %. Herein, the network is congested and unable to admit a new sensor node while maintaining the requested QoS.|$|R
30|$|Experiment A - We {{change the}} average {{velocity}} of the vehicles in the third lane only from 60 to 140 km/h. The UDP packet size is 1, 024 bytes. The transmission data rate is 10 <b>packets</b> <b>per</b> <b>second.</b>|$|R
30|$|Experiment B - We {{change the}} data packet size from 500 to 3, 000 bytes. The {{transmission}} data rate is 10 <b>packets</b> <b>per</b> <b>second.</b> The average velocity of vehicles for each lane is 40, 60 and 80 km/h, respectively.|$|R
50|$|Availability—usual DNS has no {{protection}} against {{denial of service}} (DoS) by a sniffing attacker sending a few forged <b>packets</b> <b>per</b> <b>second.</b> DNSCurve recognizes and discards forged DNS packets, providing some protection, though SMTP, HTTP, HTTPS, are also vulnerable to DoS.|$|R
40|$|Routers {{classify}} packets {{to determine}} which flow they belong to, and to decide what service they should receive. Classification may, in general, be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields {{is known to be}} difficult, and has poor worst-case performance. In this paper, we consider a number of classifiers taken from real networks. We find that the classifiers contain considerable structure and redundancy that can be exploited by the classification algorithm. In particular, we find that a simple multi-stage classification algorithm, called RFC (recursive flow classification), can classify 30 million <b>packets</b> <b>per</b> <b>second</b> in pipelined hardware, or one million <b>packets</b> <b>per</b> <b>second</b> in software. ...|$|R
50|$|Dials at user {{stations}} typically produced pulses at {{the rate}} of ten pulses <b>per</b> <b>second</b> (<b>pps),</b> while dials on operator consoles on Crossbar or electronic exchanges often pulsed at 20 pps.|$|R
40|$|Several {{emerging}} network {{trends and}} new architectural ideas are placing increasing demand on forwarding table sizes. From massive-scale datacenter networks running millions of virtual machines to flow-based software-defined networking, many intriguing design options require FIBs that can scale {{well beyond the}} thousands or tens of thousands possible using today’s commodity switching chips. This paper presents CUCKOOSWITCH, a software-based Ethernet switch design built around a memory-efficient, high-performance, and highly-concurrent hash table for compact and fast FIB lookup. We show that CUCKOOSWITCH can process 92. 22 million minimum-sized <b>packets</b> <b>per</b> <b>second</b> on a commodity server equipped with eight 10 Gbps Ethernet interfaces while maintaining a forwarding table of one billion forwarding entries. This rate is the maximum <b>packets</b> <b>per</b> <b>second</b> achievable across the underlying hardware’s PCI buses...|$|R
40|$|Increased {{bandwidth}} in the Internet puts great {{demands on}} network routers; for example, to route minimum sized Gigabit Ethernet packets, an IP router must process about <b>packets</b> <b>per</b> <b>second</b> <b>per</b> port. Using the "rule-of-thumb" {{that it takes}} roughly 1000 <b>packets</b> <b>per</b> <b>second</b> for every 10 6 bits <b>per</b> <b>second</b> of line rate, an OC- 192 line requires routing lookups per second; well above current router capabilities. One limitation of router performance is the route lookup mechanism. IP routing requires that a router perform a longest-prefix-match address lookup for each incoming datagram {{in order to determine}} the datagram's next hop. In this paper, we present a route lookup mechanism that when implemented in a pipelined fashion in hardware, can achieve one route lookup every memory access. With current 50 ns DRAM, this corresponds to approximately <b>packets</b> <b>per</b> second; much faster than current commercially available routing lookup schemes. We also present novel schemes for performing quick updates [...] ...|$|R
30|$|In {{this set}} of simulations, we compare {{multi-channel}} MAC protocols with their corresponding single-channel protocols. To make a fair comparison. We fix the input traffic rate of every node to be 0.1 <b>packet</b> <b>per</b> <b>second</b> and change the bandwidth of the control channel in multi-channel MAC protocols.|$|R
30|$|Consider a 50  m[*]×[*] 50  m {{area with}} one blind node {{at the center}} and three {{reference}} nodes around the blind node. All nodes are stationary and operate based on DALIS. Each reference node broadcasts beacon packets {{at a rate of}} 10 <b>packets</b> <b>per</b> <b>second</b> with σ[*]=[*] 4  dB.|$|R
50|$|The T640 {{supports}} up to 8 OC-768c/STM-256 ports, 32 10-Gbit/s ports (10-Gigabit Ethernet or OC-192/STM-64), 128 OC-48c/STM-16 ports, or 320 Gigabit Ethernet ports. It delivers up to 640 Gbit/s {{of capacity}} (320G bit/s full duplex) {{with the ability}} to forward up to 770 million <b>packets</b> <b>per</b> <b>second</b> (Mpps).|$|R
30|$|We set the {{wireless}} nodes' range to 250 meters. In terms of traffic, we define both UDP and TCP connections between each participating node and node 6. For TCP connections, the maximum transfer size is of 100 [*]MB. UDP flows generate 55 <b>packets</b> <b>per</b> <b>second,</b> and <b>packet</b> size is fixed at 512 bytes.|$|R
30|$|Data {{understanding}} phase implies collecting initial data, describing {{and exploring}} data. In our case, {{the data was}} obtained by monitoring the traffic from 67 BS composing a WiMAX network. The duration of collection is 8 weeks. Our database is formed by numerical values representing {{the total number of}} packets/bytes from uplink and downlink channels, for each BS. The values were recorded every 15 min and the traces are accessible in two formats: bytes <b>per</b> <b>second</b> and <b>packets</b> <b>per</b> <b>second.</b> Supplementary details about the database are presented in [5]. We will analyze the format in <b>packets</b> <b>per</b> <b>second,</b> because it is easier to handle time series with smaller values of samples. For estimating the moment when the traffic of each BS becomes comparable with the BS capacity, the downlink channel is more important. The traffic has a higher volume in downlink. Therefore, the results presented in the following correspond to downlink channel. The risk of saturation of the BS in uplink is considerably smaller.|$|R
50|$|The most {{important}} TRIPOS concepts {{have been the}} non-memory-management approach (meaning no checks are performed to stop programs from using unallocated memory) and message passing by means of passing pointers instead of copying message contents. Those two concepts together allowed for sending and receiving over 1250 <b>packets</b> <b>per</b> <b>second</b> on a 10 MHz Motorola 68010 CPU.|$|R
3000|$|... can be calculated. Let us {{assume that}} the audio part employs G. 726, which generates 50 <b>packets</b> <b>per</b> <b>second</b> and its data rate is 32 kbps. It means that, the packet inter-arrival time is 20 ms and the packet payload is 80 bytes. This {{is similar to the}} audio {{streaming}} service in Equation 6 and it can be calculated in the same way.|$|R
40|$|RPL is the IETF {{candidate}} {{standard for}} IPv 6 routing in low-power wireless sensor networks. We present the first experimental results of RPL {{which we have}} obtained with our ContikiRPL implementation. Our results show that Tmote Sky motes running IPv 6 with RPL routing have a battery lifetime of years, while delivering 0. 6 <b>packets</b> <b>per</b> <b>second</b> to a sink node...|$|R
