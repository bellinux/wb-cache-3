30|0|Public
5|$|In all, Hurricane Mitch caused {{at least}} 3,800 {{fatalities}} in Nicaragua, of {{which more than}} 2,000 {{were killed in the}} towns of El <b>Provenir</b> and Rolando Rodriguez from the landslide at the Casita volcano. The mudslide buried at least four villages completely in several feet of mud. Throughout the entire country, the hurricane left between 500,000 and 800,000 homeless. In all, damage in Nicaragua is estimated at around $1billion (1998USD).|$|E
2500|$|In the South American Championship 1937 (currently Copa América), {{the rivalry}} between both teams was already {{something}} of national pride. There were verbal confrontations between both parties, and Argentine fans often taunted the Brazilians by calling them macaquitos and making monkey sounds. The final match, held in Buenos Aires, was played {{between the two}} sides and was goalless after 90 minutes. In extra time, Argentina scored two goals. Questioning one of the goals and fearful for their own safety, the Brazilian players decided to leave the stadium before the match was officially finished. The Brazilian press has since called this match [...] "jogo da vergonha" [...] ("the shame game"). Argentina won, 2–0, and was South American champion again. Leónidas traveled alone to the World Cups of 1934 and 1938 because he was black. In the same way, a presidential decree of 1921 do not allow any black player in the Brazilian national team by an [...] "issue of national prestige". The Brazilian football was in extreme racist, and black players played first in the Argentine First División and in the National Team than in Brazil, i.e Alejandro de los Santos, idol of El <b>Provenir</b> (second top scorer in his history) and champion of the 1925 Copa América.|$|E
50|$|In all, Hurricane Mitch caused {{at least}} 3,800 {{fatalities}} in Nicaragua, of {{which more than}} 2,000 {{were killed in the}} towns of El <b>Provenir</b> and Rolando Rodriguez from the landslide at the Casita volcano. The mudslide buried at least four villages completely in several feet of mud. Throughout the entire country, the hurricane left between 500,000 and 800,000 homeless. In all, damage in Nicaragua is estimated at around $1 billion (1998 USD).|$|E
5000|$|Gottel {{founded a}} weekly {{newspaper}} in Rivas in 1865, [...] "El Porvenir de Nicaragua" [...] ("The Future of Nicaragua"), {{that was published}} every Saturday and was bilingual; Spanish and English. El Porvenir was the first newspaper that did not depend on protection from a political party or government favors. [...] Gottel ran the newspaper for 9 years, up until 1874. El <b>Provenir</b> was transferred to Managua, where Fabio Carnevalini, an Italian immigrant, served as its editor from the year of Gottel's death to May 21, 1885. That same year Jesús Hernández Somoza, son of the German immigrant Karl Hermann, took over the newspaper, which was then published twice a week, however, publication stopped in 1886. Rubén Darío, considered the Father of Modernism, {{was one of the}} prominent writer to collaborate with the newspaper.|$|E
5000|$|Provenance (from the French <b>provenir,</b> [...] "to come from"), is the {{chronology}} of the ownership, custody or {{location of a}} historical object. The term was originally mostly used in relation to works of art, but is now used in similar senses {{in a wide range}} of fields, including archaeology, paleontology, archives, manuscripts, printed books, and science and computing. The primary purpose of tracing the provenance of an object or entity is normally to provide contextual and circumstantial evidence for its original production or discovery, by establishing, as far as practicable, its later history, especially the sequences of its formal ownership, custody, and places of storage. The practice has a particular value in helping authenticate objects. Comparative techniques, expert opinions, and the results of scientific tests may also be used to these ends, but establishing provenance is essentially a matter of documentation.|$|E
5000|$|In the South American Championship 1937 (currently Copa América), {{the rivalry}} between both teams was already {{something}} of national pride. There were verbal confrontations between both parties, and Argentine fans often taunted the Brazilians by calling them macaquitos and making monkey sounds. The final match, held in Buenos Aires, was played {{between the two}} sides and was goalless after 90 minutes. In extra time, Argentina scored two goals. Questioning one of the goals and fearful for their own safety, the Brazilian players decided to leave the stadium before the match was officially finished. The Brazilian press has since called this match [...] "jogo da vergonha" [...] ("the shame game"). Argentina won, 2-0, and was South American champion again. Leónidas traveled alone to the World Cups of 1934 and 1938 because he was black. In the same way, a presidential decree of 1921 do not allow any black player in the Brazilian national team by an [...] "issue of national prestige". The Brazilian football was in extreme racist, and black players played first in the Argentine First División and in the National Team than in Brazil, i.e Alejandro de los Santos, idol of El <b>Provenir</b> (second top scorer in his history) and champion of the 1925 Copa América.|$|E
5000|$|Provenance (from the French <b>provenir,</b> [...] "to come from"), is {{the place}} of origin or {{earliest}} known history of something. In geology (specifically, in sedimentary petrology), the term provenance deals with the question where sediments originate from. The purpose of sedimentary provenance studies is to reconstruct and to interpret the history of sediment from parent rocks at a source area to detritus at a burial place. The ultimate goal of provenance studies is to investigate {{the characteristics of a}} source area by analyzing the composition and texture of sediments. The studies of provenance involve the following aspects: [...] "(1) the source(s) of the particles that make up the rocks, (2) the erosion and transport mechanisms that moved the particles from source areas to depositional sites, (3) the depositional setting and depositional processes responsible for sedimentation of the particles (the depositional environment), and (4) the physical and chemical conditions of the burial environment and diagenetic changes that occur in siliciclastic sediment during burial and uplift". Provenance studies are conducted to investigate many scientific questions, for example, the growth history of continental crust, collision time of Indian and Asian plates, Asian monsoon intensity, and Himalayan exhumation Meanwhile, the provenance methods are widely used in the oil and gas industry. [...] "Relations between provenance and basin are important for hydrocarbon exploration because sand frameworks of contrasting detrital compositions respond differently to diagenesis, and thus display different trends of porosity reduction with depth of burial." ...|$|E
40|$|The <b>provenir</b> {{ontology}} is an upper-level ontology {{to facilitate}} interoperability of provenance information in scientific applications. The description logic (DL) expressivity of <b>provenir</b> ontology is ALCH, that is, it models role hierarchies (H) (without transitive roles and inverse roles). Even though the complexity results for concept satisfiability for numerous variants of DL such as ALC with transitively closed roles (ALCR+ also called S), inverse roles SI, and role hierarchy SHI have been well-established, similar results for ALCH has been surprisingly {{missing from the}} literature. Here, we show that {{the complexity of the}} concept satisfiability problem for the ALCH variant of DL is PSpace complete. This result contributes towards a complete set of complexity results for DL variants and establishes a lower bound on complexity for domain-specific provenance ontologies that extend <b>provenir</b> ontology...|$|E
40|$|Provenance {{metadata}} {{describes the}} 2 ̆ 7 lineage 2 ̆ 7 or history of an entity and necessary information {{to verify the}} quality of data, validate experiment protocols, and associate trust value with scientific results. eScience projects generate data and the associated provenance metadata in a distributed environment (such as myGrid) and on a very large scale that often precludes manual analysis. Given this scenario, provenance information should be, (a) interoperable across projects, research groups, and application domains, and (b) support analysis over large datasets using reasoning to discover implicit information. In this paper, we introduce an ontology-driven framework for eScience provenance management underpinned by an 2 ̆ 7 upper-level 2 ̆ 7 ontology called <b>provenir</b> defined in OWL-DL. This framework is implemented in a modular fashion by extending <b>provenir</b> ontology to create a suite of domain-specific provenance ontologies that facilitate interoperability and enable reasoning. We demonstrate the application of this framework in two eScience projects domains through creation of, (a) Parasite Experiment ontology to model provenance in parasite research, and (b) Trident ontology to model provenance in the Neptune oceanography project...|$|E
40|$|Provenance metadata, {{describing}} the history or lineage of an entity, {{is essential for}} ensuring data quality, correctness of process execution, and computing trust values. Traditionally, provenance management issues have been {{dealt with in the}} context of workflow or relational database systems. However, existing provenance systems are inadequate to address the requirements of an emerging set of applications in the new eScience or Cyberinfrastructure paradigm and the Semantic Web. Provenance in these applications incorporates complex domain semantics on a large scale with a variety of uses, including accurate interpretation by software agents, trustworthy data integration, reproducibility, attribution for commercial or legal applications, and trust computation. In this dissertation, we introduce the notion of “semantic provenance” to address these requirements for eScience and Semantic Web applications. In addition, we describe a framework for management of semantic provenance by addressing the three issues of, (a) provenance representation, (b) query and analysis, and (c) scalable implementation. First, we introduce a foundational model of provenance called <b>Provenir</b> to serve as an upper-level reference ontology to facilitate provenance interoperability. Second, we define a classification scheme for provenance queries based on the query characteristics and use this scheme to define a set of specialized provenance query operators. Third, we describe the implementation of a highly scalable query engine to support the provenance query operators, which uses a new class of materialized views based on the <b>Provenir</b> ontology, called Materialized Provenance Views (MPV), for query optimization. We also define a novel provenance tracking approach called Provenance Context Entity (PaCE) for the Resource Description Framework (RDF) model used in Semantic Web applications. PaCE, defined in terms of the <b>Provenir</b> ontology, is an effective and scalable approach for RDF provenance tracking in comparison to the currently used RDF reification vocabulary. Finally, we describe the application of the semantic provenance framework in biomedical and oceanography research projects...|$|E
40|$|Background A {{critical}} {{aspect of}} the NIH Translational Research roadmap, which seeks to accelerate the delivery of 2 ̆ 2 bench-side 2 ̆ 2 discoveries to patient 2 ̆ 7 s 2 ̆ 2 bedside, 2 ̆ 2 is {{the management of the}} provenance metadata that keeps track of the origin and history of data resources as they traverse the path from the bench to the bedside and back. A comprehensive provenance framework is essential for researchers to verify the quality of data, reproduce scientific results published in peer-reviewed literature, validate scientific process, and associate trust value with data and results. Traditional approaches to provenance management have focused on only partial sections of the translational research life cycle and they do not incorporate 2 ̆ 2 domain semantics 2 ̆ 2, which is essential to support domain-specific querying and analysis by scientists. Results We identify a common set of challenges in managing provenance information across the pre-publication and post-publication phases of data in the translational research lifecycle. We define the semantic provenance framework (SPF), underpinned by the <b>Provenir</b> upper-level provenance ontology, to address these challenges in the four stages of provenance metadata: (a) Provenance collection - during data generation (b) Provenance representation - to support interoperability, reasoning, and incorporate domain semantics (c) Provenance storage and propagation - to allow efficient storage and seamless propagation of provenance as the data is transferred across applications (d) Provenance query - to support queries with increasing complexity over large data size and also support knowledge discovery applications We apply the SPF to two exemplar translational research projects, namely the Semantic Problem Solving Environment for Trypanosoma cruzi (T. cruzi SPSE) and the Biomedical Knowledge Repository (BKR) project, to demonstrate its effectiveness. Conclusions The SPF provides a unified framework to effectively manage provenance of translational research data during pre and post-publication phases. This framework is underpinned by an upper-level provenance ontology called <b>Provenir</b> that is extended to create domain-specific provenance ontologies to facilitate provenance interoperability, seamless propagation of provenance, automated querying, and analysis...|$|E
40|$|Abstract Background A {{critical}} {{aspect of}} the NIH Translational Research roadmap, which seeks to accelerate the delivery of "bench-side" discoveries to patient's "bedside," is {{the management of the}} provenance metadata that keeps track of the origin and history of data resources as they traverse the path from the bench to the bedside and back. A comprehensive provenance framework is essential for researchers to verify the quality of data, reproduce scientific results published in peer-reviewed literature, validate scientific process, and associate trust value with data and results. Traditional approaches to provenance management have focused on only partial sections of the translational research life cycle and they do not incorporate "domain semantics", which is essential to support domain-specific querying and analysis by scientists. Results We identify a common set of challenges in managing provenance information across the pre-publication and post-publication phases of data in the translational research lifecycle. We define the semantic provenance framework (SPF), underpinned by the <b>Provenir</b> upper-level provenance ontology, to address these challenges in the four stages of provenance metadata: (a) Provenance collection - during data generation (b) Provenance representation - to support interoperability, reasoning, and incorporate domain semantics (c) Provenance storage and propagation - to allow efficient storage and seamless propagation of provenance as the data is transferred across applications (d) Provenance query - to support queries with increasing complexity over large data size and also support knowledge discovery applications We apply the SPF to two exemplar translational research projects, namely the Semantic Problem Solving Environment for Trypanosoma cruzi (T. cruzi SPSE) and the Biomedical Knowledge Repository (BKR) project, to demonstrate its effectiveness. Conclusions The SPF provides a unified framework to effectively manage provenance of translational research data during pre and post-publication phases. This framework is underpinned by an upper-level provenance ontology called <b>Provenir</b> that is extended to create domain-specific provenance ontologies to facilitate provenance interoperability, seamless propagation of provenance, automated querying, and analysis. </p...|$|E
40|$|Provenance, {{from the}} French word “provenir”, {{describes}} the lineage or {{history of a}} data entity. Provenance is critical information in the sensors domain to identify a sensor and analyze the observation data over time and geographical space. In this paper, we present a framework to model and query the provenance information associated with the sensor data exposed {{as part of the}} Web of Data using the Linked Open Data conventions. This is accomplished by developing an ontology-driven provenance management infrastructure that includes a representation model and query infrastructure. This provenance infrastructure, called Sensor Provenance Management System (PMS), is underpinned by a domain specific provenance ontology called Sensor Provenance (SP) ontology. The SP ontology extends the <b>Provenir</b> upper level provenance ontology to model domain-specific provenance in the sensor domain. In this paper, we describe the implementation of the Sensor PMS for provenance tracking in the Linked Sensor Data...|$|E
40|$|International audienceTwo small {{fragments}} of human cranial vaults are inventoried in {{the collections of}} the Field Museum of Natural History as recovered from Le Moustier site (Dordogne). The historical study of the purchase of these remains and the Le Moustier's human fossils disoveries, allow to conclude that these fragments {{do not belong to}} the individual found by Denis Peyrony in 1914 in the inferior rock-shelter of the site. Nevertheless, according to the progress of the researches, nothing opposes to the hypothesis that theses remains could come from the brechia level the classical rock-shelter dug by 0. HauserDeux petits fragments de voûte crânienne humaine sont inventoriés dans les collections du Field Museum of Natural History comme provenant du gisement du Moustier (Dordogne). L'étude historique de l'achat de ces restes et des découvertes qui ont eu lieu au Moustier permet de conclure qu'ils n'appartiennent pas à l'individu mis au jour par Denis Peyrony en 1914 dans l'abri inferieur de ce site. Néanmoins, en fonction de l'avancée actuelle des recherches, rien ne s'oppose à l'hypothèse que ces restes puissent <b>provenir</b> du niveau bréchifié de l'abri supérieur fouillé par 0. Hauser...|$|E
40|$|ABSTRACT. We {{discuss the}} {{possibility}} of the occurrence of thermal instability in ice sheets and glaciers. This may arise from the non-linear viscous heating term, which could provide for the existence of multiple steady states in the flow and temperature fields: a word of warning is given about the applicability of these ideas. RESUME. L'existence de multiples etats d'equilibre dans l'eeoulement des grandes masses de glace. Nous discutons la possibilite que se produise une instabilite thermique dans les calottes glaciaires et les glaciers. Ceci peut <b>provenir</b> du terme non lineaire exprimant la viscosite thermique, qui peut entrainer l'existence de plusieurs etats d'equilibre dans l'ecoulement et les flux de chaleur: un mot d'avertissement est donne sur les perspectives d'application de ces idees. ZUSAMMENFASSUNG. Die Existenz mehrerer stationarer Zustande im Fluss grosser Eismassen. Es wird die Moglichkeit des Auftretens thermischer InstabiliUit in Eisschilden und Gletschern untersucht; eine solche konnte aus dem Ausdruck fur die nicht-lineare viskose Erwarmung hergeleitet werden, mit dem sich die Existenz mehrerer stationarer Zustande in den Fliess- und Temperaturfeldern begrunden liesse. Gegenuber der Anwendbarkeit solcher Ideen scheint jedoch Skepsis angebracht. ROBIN (1955) was the first to propose the possibility that thermal instability could provide a trigge...|$|E
40|$|The World Wide Web Consortium (W 3 C) Provenance Incubator Group has {{the goal}} of {{providing}} “a state-of-the art understanding [...] . {{in the area of}} provenance for Semantic Web technologies. ” To enhance the mutual understanding of language capabilities, the group is attempting to map several of the existing provenance languages such as <b>Provenir,</b> PREMIS, and the Proof Markup Language (PML) into the provenance language the Open Provenance Model (OPM). OPM is intended to be a precise inter-lingua for exchanging provenance information. This article contributes {{to the understanding of the}} capabilities of OPM and PML by establishing a set of six common and relatively simple provenance use cases and comparing the OPM and PML models and implications of those models. A provenance use case consists of a scenario and a provenance question associated with the scenario. Some of the use cases are taken from the OPM specification document. The use cases in this article may be considered essential for any provenance encoding intended to be used for provenance interoperability. The modeling of the use cases exposes a number of substantial difficulties in creating and interpreting OPM specifications for use by machine reasoning systems. Keywords...|$|E
40|$|Trabajo que trata sobre la fascinante figura de Aphra Behn, primera mujer escritora que se puede {{considerar}} que viviera de su pluma. Mujer y autora muy discutida en su época, varios siglos después los “Estudios de Género” han contribuido no a rescatar o sacar su figura y obra del olvido, sino a hacerle justicia por su valía personal e intelectual. La primera enfrentándose en igualdad de condiciones a los hombres, con el mérito añadido de no <b>provenir</b> de una clase social alta; la segunda por escribir de {{una manera}} totalmente libre y desinhibida, expresando no sólo los mismos pensamientos que los escritores del sexo contrario, sino en muchas ocasiones más atrevidos. This paper {{deals with the}} enthralling figure of Aphra Behn, the first English women to earn her living by her writing. During her lifetime she gained the attacks of many critics for her chosen subject matter, often alluding to female sexual desire. Several centuries later, Women‟s Studies have contributed {{not only to the}} current revival of her reputation and works, but also to give her their fair due for her proven worth and intellectual capability. Firstly, she faced male writers under the same conditions, {{despite the fact that she}} didn‟t belong to aristocracy; and secondly she broke cultural barriers by writing in a distinctive poetic voice characterized by audacity and directness in a masculine style assumed as her own...|$|E
40|$|Abstract: The {{recreational}} waters near {{many large}} {{cities in the}} United States and Canada are severely impaired by pathogens that {{are present in the}} storm water runoff. In separated sewers the pathogen sources may be cross-flows between the sanitary and storm water systems. This paper presents the methodology that was used in developing a forecasting model for pathogen indicators for recreational sites in the receiving waters of multiple storm water outfalls. The objective of the model is to give a timelier indicator of beach water quality than conventional beach monitoring, which takes about 2 d for laboratory results. The model used for the study was based on the Princeton Ocean Model. The forecasting system consists of nested hydrodynamic models and a bacteria fate–transport submodel. Calibration and validation is based on 6 years of field studies, laboratory analyses, and experiments. The methodology is illustrated by a case study of the impact of storm water flows on the south shore of Lake Pontchartrain, Louisiana, which has been banned for swimming since 1985. The water quality data included: pathogen indicators (fecal coliform, Enterococci, and E. Coli), water chemistry parameters, turbidity, and nutrients. Key words: modeling, water quality, pathogens, fecal coliform, stormwater runoff. Résumé: L’eau des cours d’eau destinés aux loisirs près des grandes villes des États-Unis et du Canada est gravement détériorée par les pathogènes se trouvant dans le ruissellement des eaux pluviales. Dans des conduites d’égout séparées, les sources de pathogènes peuvent <b>provenir</b> d’une contamination croisée entre les systèmes d’égouts pluviaux et d’égout...|$|E
40|$|Provenance, {{from the}} French word „provenir ‟ meaning "to come from", {{describes}} the lineage of an entity. Provenance is critical information in eScience to accurately interpret scientific results. Though information provenance {{has been recognized}} as a hard problem in computing science (British Computing Society, 2004), many fundamental research issues in provenance {{have yet to be}} addressed. A common provenance model with well-defined formal semantics to facilitate interoperability of provenance metadata from different sources has not been defined. Another important issue is the lack of a systematic study of provenance query characteristics across multiple applications. A classification or taxonomy of the provenance queries will not only help to better understand provenance metadata, but will also enable the definition of provenance query operators. Finally, while provenance for a user or an application is a specific view over all available provenance metadata, a provenance management system that supports provenance storage as views has not been implemented. In this paper we propose a novel provenance algebra consisting of a common provenance model called <b>provenir,</b> defined in description logic based W 3 C Web Ontology Language (OWL-DL), along with a set of provenance query operators derived from the classification of provenance queries. We also introduce a practical provenance storage solution using materialized views over a generic relational database system. Our approach takes advantage of provenance query operators and well-defined indices to efficiently process complex provenance queries over very large datasets. To support our claims we present an evaluation of both performance and scalability aspects of our initial implementation. To {{the best of our knowledge}} this is the first provenance management system that supports the complete process from a formal provenance model and query operators to storage and efficient queries over provenance data. 1...|$|E
40|$|Este artículo trae de nuevo al debate el tema sobre el límite que debe imponerle el Estado alcrecimiento de las firmas en el mercado, poniendo como ejemplo la industria de la generacióneléctrica colombiana. Para ello, propone primero presentar las posibilidades que la legislacióncolombiana ofrece a las firmas, para aumentar su tamaño; continúa con una revisión sobrelos límites que en otros países se le imponen al sector de la generación eléctrica; y finalizamostrando la postura que el neoinstitucionalismo económico tiene frente al tamaño de lasfirmas. La conclusión a la que llega la escuela neoinstitucional es que el control al tamañode la firma debe <b>provenir</b> de la naturaleza interna de la empresa, {{relacionada con}} sus costosde transacción, y propone que el Estado-regulador debería asumir el papel institucional deestablecer normas claras de conducta para que los agentes en el mercado las cumplan, y noun papel de imponer restricciones al crecimiento de las firmas. This article brings back {{the debate on}} the limit that the State must impose to the growth of {{companies}} in the market; the author analyzes a case in the Colombian electrical energy generation industry.  For this purpose, it first presents the possibilities that Colombian legislation offers to the companies to increase their size, and continues with a review on the limits that are imposed in other countries regarding electrical energy generation; the text ends showing the point of view that economic neo-institutionalism has regarding the size of the companies.  The conclusion drawn from this perspective is that the control of the size of the company must come from its internal nature, related to its transaction costs, and proposes that the Regulator-State should assume the institutional role of establishing clear norms of conduct for the agents in the market to follow, as opposed to a role that imposes restrictions to the growth of the companies...|$|E
40|$|El género Agrobacterium incluye especies ftopatógenas que inducen la formación de agallas en el cuello o la proliferación de raíces en cabellera en más de 600 especies de dicotiledóneas, y especies no patógenas cuyo hábitat natural es el suelo. Como {{no es posible}} erradicar a las especies patógenas y habida cuenta de que más del 80 % de las infecciones puede <b>provenir</b> de viveros, es importante evitar la diseminación de la enfermedad. Por ello, el objetivo de este trabajo ha sido desarrollar técnicas sensibles y precisas que, aisladamente o combinadas, permitan detectar la presencia de especies y biovares de Agrobacterium a partir de muestras de {{material}} vegetal, suelo y agua. Se comprobó que con la estrategia combinada de realizar aislamientos en los medios semiselectivos D 1, D 1 -M y YEM-RCT; PCR multiplex sobre el gen 23 S ADNr; PCR específca sobre los genes virC 1 y virC 2 y bioensayos en plántulas de pimiento cv. California Wonder y en hojas cortadas de kalanchoe, se reduce la posibilidad de obtener falsos negativos y/o falsos positivos. Por lo expuesto, esta combinación de técnicas constituye una herramienta adecuada para el diagnóstico de cepas patógenas de Agrobacterium a partir de distintos tipos de muestras. The genus Agrobacterium includes phytopathogenic {{bacteria that}} induce the development of root crown galls and/or aerial galls {{at the base of}} the stem or hairy roots on more than 600 species of plants belonging to 90 dicotyledonous families and non-pathogenic species. These bacteria being natural soil inhabitants are particularly diffcult to eradicate, which is a problem in nurseries where more than 80 % of infections occur. Since early detection is crucial to avoid the inadvertent spread of the disease, the aim of this work was to develop sensitive and precise identifcation techniques by using a set of semi-selective and differential culture media in combination with a specifc PCR to amplify a partial sequence derived from the virC operon, as well as a multiplex PCR on the basis of 23 SrDNA sequences, and biological assays to identify and differentiate species and biovars of Agrobacterium obtained either from soil, water or plant samples. The combination of the different assays allowed us to reduce the number of false positive and negative results from bacteria isolated from any of the three types of samples. Therefore, the combination of multiplex PCR, specifc PCR, isolations in semi-selective D 1, D 1 -M and YEM-RCT media combined with bioassays on cut leaves of Kalanchoe and seedlings of California Wonder pepper cultivar constitute an accurate tool to detect species and biovars of Agrobacterium for diagnostic purposes...|$|E
40|$|OBJETIVO: Estudiar la distribución y frecuencia de los anticuerpos contra el virus de la {{hepatitis}} A en una muestra probabilística en México con representatividad estatal, así como analizar los factores de riesgo y los patrones epidemiológicos. MATERIAL Y MÉTODOS: A partir de la Encuesta Nacional de Salud se estudiaron 4 907 sueros seleccionados de forma aleatoria para anticuerpos contra VHA mediante ensayo inmunoenzimático. Los sueros se recolectaron de noviembre de 1999 a junio del 2000 a nivel nacional. RESULTADOS: Se encontró seroprevalencia general de 81. 3 % (IC 78. 6 - 84. 2), y los resultados expandidos permiten inferir que existen 78. 7 millones de mexicanos infectados. Los factores de riesgo en menores de nueve años incluyen residir en entidades sureñas (RM= 5. 3), localidades rurales (RM= 3. 1), <b>provenir</b> de familia con bajos ingresos (RM= 2. 4) y habitar viviendas con acceso limitado a servicios sanitarios (agua, RM= 2. 5; drenaje, RM= 2. 7). CONCLUSIONES: Los patrones de transmisión heterogéneos y las diferencias en las pre-valencias de infección indican inequidad en las poblaciones estudiadas, explicables por diferencias en las condiciones sanitarias y sociales. Se discuten las ventajas de intervenciones poblacionales tales como la vacunación y el fortalecimiento de las condiciones sanitarias y socioeconómicas. OBJECTIVE: Hepatitis A Virus (HAV) in Mexico {{has traditionally}} been considered a disease with a homogeneous pattern of transmission, high rates of infection at early ages, and infrequent complication rates. The {{purpose of this study}} was to take advantage of the 2000 NHS, a probabilistic population-based survey, in order to describe the seroepidemiology of HAV infection in Mexico. MATERIAL AND METHODS: This study is based on information obtained from the National Health Survey that was conducted in 2000. The present report is based on 4 907 randomly selected samples that were studied to determine the prevalence of HAV antibodies using immunoenzymatic assay. Sera were collected from November 1999 to June 2000. RESULTS: Seroprevalence among the general population was 81. 3 % (CI 95 %: 78. 6 - 84. 2); expanded results allow the inference that 78. 7 million Mexicans have been infected by this agent. Risk factors for HAV among children younger than nine years of age are the following: residence in southern states OR= 5. 3, residence in rural communities OR= 3. 1, low-income family OR= 2. 4 and living in households with limited access to sanitary facilities (water OR= 2. 5 and sewage OR= 2. 7). CONCLUSIONS: Results of this study demonstrate that HAV transmission patterns are heterogeneous and that differentials in the prevalence of infection are due to sanitary and social inequity among studied populations. Finally, the advantages of adopting public health measures such as vaccination and improvement of sanitary and socioeconomic conditions are discussed...|$|E
40|$|La autoevaluación del control es un elemento del Sistema de Control Interno que permiteel diagnóstico y el fortalecimiento organizacional. Se desarrolla en toda la entidad y supone el apoyo de la alta dirección, apoyo representado en recursos físicos y la orientación que lebrinde durante su aplicación. Dicho elemento genera mayor responsabilidad en los empleados, al involucrarlos en el análisisde fortalezas y debilidades del sistema de control, al comprometerlos con la recolección de la información que soporta el juicio sobre el estado del sistema y al permitirles proponer planesde mejoramiento que contribuyan al logro del objetivo del sistema de control, y por ende alde la organización. Las dificultades en su aplicación pueden <b>provenir</b> de aspectos relacionados con la culturaorganizacional: si el {{personal}} no está dispuesto a colaborar con los grupos de apoyo en elsuministro de la información, si existe apatía con el proceso y si no se {{cuenta con}} personas capacitadas para realizarlo. El proceso se efectúa en tres etapas; la primera consiste en la planeación, en la que el Comité de Autoevaluación compromete la gerencia y se asignan actividades y responsabilidades; sedefine el cronograma de trabajo, los recursos necesarios y se capacita a los grupos de apoyo;la segunda se refiere a la ejecución, consistente en el levantamiento de la información que soporta el análisis de cada uno de los elementos del sistema de control y permite emitir juicios objetivos sobre su desempeño; finalmente, la tercera, elaboración del informe, da cuenta delos resultados de la autoevaluación del sistema de control, de su desarrollo y sus dificultades, y establece las propuestas de mejora. Control Self-Assessment {{is an element}} of the Internal Control System which allows the organizational diagnose and strengthening. It is run throughout the entity and supposes the support from the high managerial circle. This support {{is in the form of}} physical resources and the orientation offered throughout its execution. Self-assessment generates enhanced staff responsibility as it involves them in the analysis of the strengths and weaknesses of the system, in the collection of the data supporting the description of the state of the system and as it allows them to propose improvement plans that contribute in the achievement of the Control System goal and thence the organization’s goal. Difficulties in its application may stem from aspects related to the organizational culture: if the staff is not willing to collaborate with the supporting groups in furnishing the information, if there is apathy to the process, and if there are not the skilled people to carry out the process. The process is carried out in three stages: the first one is the planning, in which the SelfAssessment Committee involves the management and assigns activities and responsibilities; it also defines the work time table, the necessary resources and trains the supporting groups.  The second stage refers to the execution, which consists in getting the information that supports the analysis of every element of the Control System and allows the expression of objective statements about their performance. Finally, the third stage is the production of a report, which accounts for the results of the Control System Self-Assessment, its development and difficulties, and status improvement proposals...|$|E
40|$|En 2002, un {{ensemble}} d’objets métalliques a été découvert sur la plage de l’Amélie à Soulac-sur-Mer (Gironde), localité ayant déjà fourni de nombreux dépôts de l’Âge du bronze. Treize objets en alliages à base de cuivre sont attribués à l’époque du Bronze moyen. Si la hache à rebords est classique de la production médocaine, les autres objets (bracelets, épée, anneau) évoquent la culture des Duffaits. La présente contribution propose une étude technologique de l’ensemble des objets : lecture de la surface, examen de la microstructure par la métallographie et analyse élémentaire de la composition. Les résultats permettent une appréhension plus fine des chaînes opératoires de fabrication des haches à rebords et des bracelets massifs incisés. Ils mettent aussi en évidence la série de bracelets comme pouvant <b>provenir</b> d’un même atelier. Enfin, la présence d’objets finis spécifiques et d’un objet à l’état d’ébauche témoigne d’une production locale tout en posant la question des modalités de circulation des objets. This study concerns metallic objects discovered in 2002 on Amélie beach, Soulac-sur-Mer (Gironde; Aquitaine). Many Bronze Age hoards have previously {{been discovered in}} this area. This find contains thirteen copper base alloy artefacts : eight massive bracelets (three of which present line decoration), a flanged axe, a broken palstave axe, an ingot, a sword fragment and a ring. The flanged axe is typical of production in the Médoc region. The other objects (bracelets, sword and ring) suggest the Duffaits culture. The present contribution concerns a technological study of all the objects : surface observations, metallographic examinations and elementary composition analysis. Fourteen metallographic sections were made according to a specific protocol which, after preparation of the samples, allows the polished sections to be examined before they are etched with a ferric chloride solution. Composition analyses were obtained with an EDXS device associated with a scanning electron microscope. The results enable us to understand the chaînes opératoires used for making these types of flanged axe and massive bracelets. The sequence of the flanged axe shaping processes can be described, and certain gestures and tools clarified. An assembled mould (made with stone or terracotta) was used. After the stripping, a grinding operation, using abrasive materials, {{was applied to the}} whole object. Then an annealing and finally a fettling operation – which leaves the metal in a partially hardened state – were applied to the object. No other thermal or mechanical treatment occurred later. This state can be qualified as rough. The technological reading of the set of bracelets confirms their morphological homogeneity by that of the manufacturing processes. The concordance of similar parameters in shape, decoration and shaping techniques allows us to suppose that all the objects were produced in the same workshop. These bracelets were manufactured by a plastic deformation, with the coiling of a bar whose thickness and section shape were certainly conceived from the very moulding. In the very last operation, the decoration was obtained by plastic deformation, on metal softened by annealing. The use of a scriber is very possible. The presence of finished objects and of a rough casting certifies that a local production existed in the area of Soulac-sur-Mer. This technological study allows us to discuss the question of the circulation of metallic objects. The case of the rough axe represents a stage in the chaîne opératoire. This would have been divided into two stages : moulding/ fusion and post-casting. A particular organization would have separated these two stages which would not then have been produced in continuity. We thus submit the hypothesis according to which several types of workshops could have existed, functioning on a «cascade » effect system. An important hierarchical organization of society would in that case be highlighted, with a strong power or authority importing the metallic raw material, centralizing it and, by doing so, being able to control a particular type of workshop making rough-standard axes : the Médoc type axes express rigorous morphometric rules. Lagarde Céline, Pernot Michel. Approche pluridisciplinaire d’{{un ensemble}} d’objets métalliques de l’Âge du bronze découvert à Soulac-sur-Mer (Gironde, France). In: Bulletin de la Société préhistorique française, tome 106, n° 3, 2009. pp. 553 - 567...|$|E
40|$|Introducción: la idea actual de nuestro sistema sanitario es que la asistencia sea ambulatoria y que se utilice la hospitalización cuando sea precisa. En este sentido es de destacar el desarrollo de la {{consulta}} única o de alta resolución. Por ello, se ha realizado una encuesta entre varios hospitales andaluces con la idea de definir y determinar qué aspectos son necesarios para poder desarrollar esta consulta. Material y métodos: la encuesta ha sido contestada por 10 hospitales andaluces. Se trata de un estudio prospectivo descriptivo de las respuestas contestadas por los distintos hospitales. Las preguntas son 27 en las que se reflexiona sobre la existencia de la consulta y la infraestructura para desarrollarla: cuántos pacientes se ven, dónde se pasa esta consulta, de dónde provienen los pacientes, los criterios para derivar los pacientes a esta consulta, las condiciones en las que acude el enfermo, si se realiza ecografía de abdomen, si se dispone de sistema informático integrado hospitalario, enfermera, en cuántas visitas se emite un diagnóstico del paciente y, por último, se pregunta si se cree que es necesaria esta consulta y por qué. Resultados: de los 10 hospitales 5 tienen consulta de alta resolución, aunque todos consideran que la deberían tener. El número de pacientes atendidos debe ser 10 y en el propio hospital. Existen diferencias en considerar si los pacientes deben <b>provenir</b> desde Urgencias o desde el médico de cabecera. Parece lógico pensar que sólo se deben derivar pacientes cuya patología pueda ser diagnosticada mediante ecografía y/o endoscopia. El paciente debería acudir a la consulta en ayunas y con analítica del médico de cabecera, para así poder realizarles la ecografía. La consulta debe constar de sistema informático y de una enfermera propia. Según los encuestados este tipo de consultas es muy útil en nuestro actual sistema, porque permite mayor colaboración entre Atención Primaria y el especialista, y consigue una orientación rápida de la patología del paciente actuando de filtro adecuado para el resto de las consultas. Conclusiones: la consulta debe ser atendida por un facultativo especialista de área (FEA) con conocimientos y experiencia en ecografía, endoscopia digestiva y en el {{manejo de}} programas informáticos a nivel de usuario. Hoy en día puede ser considerada una modalidad de consulta resolutiva y eficaz en nuestro sistema sanitario. Introduction: the present concept in our healthcare {{system is that}} medical care should be given on an outpatient basis with hospitalization occurring only when essential. We therefore put forth {{the development of the}} "all in one" outpatient office or "high resolution" outpatient clinic. For such purpose we administered a questionnaire to various Andalusian hospitals to define and determine those aspects necessary {{in the development of the}} aforementioned outpatient office. Materials and methods: the questionnaire was filled out by 10 Andalusian hospitals. This is a prospective-descriptive study of responses from all 10 participating hospitals. The 27 questions inquired on the existence of such an outpatient office and the infrastructure needed to develop this service: How many patients are seen, where is it physically located, where do patients come from, criteria for assigning patients to this medical office, condition of incoming patients, whether ultrasound scans are performed, whether an integrated hospital computer system exists, nursing staff, how many visits are required before coming to a diagnosis, and finally whether this type of outpatient office is needed, and if so, why. Results: of all 10 hospitals, 5 of them had this type of clinic. All of them considered this type of outpatient service essential. The number of patients treated should be " 10 ", in the hospital itself. There are differences as to whether patients should come from the emergency room or a primary care physician. It seems logical to assume that only patients who can be diagnosed via ultrasounds or endoscopy should be chosen. To allow an ultrasonogram the patient should visit the outpatient office in a state of "fasting" and with standard blood counts from the primary care physician. The outpatient clinic should have a computer system and its own nurse. According to participating hospitals this type of outpatient visits is very useful in our present healthcare system, as it allows higher levels of collaboration between Primary Care and the specialist; it also provides a rapid orientation regarding patient pathology, and acts as a "filter" for the rest of the healthcare system. Conclusions: the outpatient office should be tended to by an attending specialist in the field (FEA) with knowledge and experience in ultrasounds and gastrointestinal endoscopy, as well as user competency with the required computer programs. In our present-day system this can be considered a modality of high-resolution outpatient services and a model of efficiency...|$|E
40|$|Se suele evaluar Insulinorresistencia (IR) mediante "Insulina e índice HOMA-IR" pero hay escasa publicación sobre valores de referencia y/o corte para evaluar IR en el Síndrome Metabólico. Un mismo valor de HOMA-IR puede <b>provenir</b> de diversos pares de glucosa/{{insulina}}; esto aporta información insuficiente si no se consigna el % de &# 946;-secreción (%B) y el % de Sensibilidad (%S). Objetivos: 1 º) Calcular (para Insulina medida por MEIA) los valores de corte a informar para HOMA-IR, %B y %S, obtenidos a partir de la fórmula HOMA. 2 º) ídem, para esos índices obtenidos del programa HOMA 2. 3 º) Dadas las múltiples combinaciones de datos que pueden confluir en un mismo HOMA-IR o un mismo %B, interpretarlos en una gráfica para facilitar su evaluación. Valor de corte: se realizaron 208 TTOG obteniéndose para HOMA-fórmula y HOMA 2 los siguientes: HOMA-IR, 2. 64, HOMA 2, 1. 67; %S: 37. 8 % y HOMA 2 -%S 59. 9 %. %B: 67. 6 % y HOMA 2 -%B: 73. 0 %; Interpretación de datos: si en un gráfico de insulina vs. glucosa se unen todos los puntos correspondientes a un mismo HOMA-IR se obtiene una curva de iso-HOMA, lo mismo para la recta de iso-%B. Repitiendo la operación con varios valores de HOMA-IR y de %B se obtiene una gráfica, en la cual los iso-HOMA e iso-%B de corte delimitan 4 zonas, y la ubicación de los datos de un paciente en c/u de ellas tiene significados diferentes, que se interpretan en este trabajo. Conclusión: debe informarse siempre HOMA-IR, %S, y %B como indicador del status &# 946;-secretor. BACKGROUND: {{insulin resistance}} status is frequently evaluated through the HOMA-IR index, but {{there still is}} a widespread missunderstanding about its interpretation. &# 946;-cell status is evaluated through the %B (&# 946;-secretion). Still, {{there are very few}} papers regarding cut-off values for HOMA-IR and its associated indexes that arise from the original HOMA formula. Indeed, the need of evaluation IR as a main component of the Metabolic Syndrome as defined by the WHO committee (1999), led us to try to calculate the cut-off value for HOMA-IR in case Insulin is measured by MEIA (Abbott Laboratories). A single HOMA-IR value (as well as %S, %-Sensitivity) may come from different glucose-insulin pairs, so that in itself it provides little information, specially regarding &# 946;-secretory status. That lack of information must be assessed through %-B, which also comes from different combinations of glucose-insulin values. OBJECTIVE: The aims of this study are: 1 º) To calculate the cut-off values for HOMA-IR, %-S, %-B, to be reported along with Insulin to the physicians, as obtained by HOMA-formula. 2 º) To calculate the cut-off values for HOMA 2 -IR, %-B, %-S as provided by the HOMA 2 Calculator. 3 º) Considering that each value of HOMA or %-B may come from multiple combinations of glucose-Insulin pairs, to design a graphic in which a patients status migth be evaluated. METHODS AND RESULTS: 208 OGTT were performed according to WHO 1999 recomendations, 110 patientes and 98 controls. Statistics were calculated by using G. Reaven's criteria about upper and lower quartile among people under 30 kg/m² BMI. Data calculated were: upper quartile for HOMA-IR: 2. 64; for HOMA 2 : 1. 67; lower quartile for %S: 37. 8 %, HOMA 2 -%S: 59. 9 %; lower quartile for %B: 67. 6 %, HOMA 2 -%B: 73. 0 %. Study and interpretation of the data: we can calculate the different pairs of data that converge onto a same value of HOMA-IR (table 3); if we then plot on a graphic Insulin vs. Glucose those different points, we could see a curve ranging from "low glucose-high insulin" points to "high glucose-low insulin" others (Fig. 1). We can draw a curve for each HOMA-IR value, but for the evaluation it is enough if we take the cut-off value and some lower and higher ones (Fig. 4). The same can be done for each data of %-B (table 4); in this case, we obtain a straigth line with a positive slope; the highest the %B, the highest the slope. Just like before, we can plot the cut-off value for %B, and some lower and higher (Fig. 5). If we plot all this curves in a unique Insulin vs. Glucose graphic (Fig. 6), then we can see that the intersection of both cut-off lines leaves on the graphic four zones (beyond the uncertainty zones undermentioned), (Fig. 7, 8). The position of the pair of data of a patient on the plot could allow to predict about his insulin-sensitivity and &# 946;-secretory status, in spite of &# 946;-cell pulsatility. CONCLUSIONS: Cut-off data were calculated for MEIA-Insulin: HOMA-IR (2. 64), %B(67. 6) y %S (37. 8). For HOMA 2 -Calculator: HOMA- 2 : 1. 67; HOMA 2 -%B: 73. 0 %; HOMA 2 -%S: 59. 9 %. Given the limited information provided by HOMA-IR alone e suggest that patient´s reports for the physicians include HOMA-IR as well as %S, in order to evaluate the &# 946;-cell status and try to predict &# 946;-claudication as early as possible before it takes place. Nonetheless, in the long-time-evaluation of a patient, the method of Insulin dosage should be, if possible, the same one...|$|E
40|$|L'une des difficultés rencontrées couramment dans la {{conception}} des réseaux de mesure - au moins {{en ce qui concerne}} les micropolluants - porte sur la sélection des paramètres à mesurer. C'est notamment le cas pour les pesticides, dont plusieurs centaines sont utilisées en agriculture, mais qu'il est impossible de surveiller dans les eaux en totalité pour des raisons à la fois techniques et économiques. C'est la raison pour laquelle les autorités françaises ont fait procéder à la mise au point d'une méthode de sélection des matières actives utilisées en agriculture basée sur l'évaluation du risque. Dans cette méthode, l'exposition est figurée par un rang combinant les données relatives aux usages des matières actives (superficie, dose par ha) et leurs caractéristiques physico-chimiques. Le danger est représenté par la toxicité, soit pour l'homme, soit pour les espèces aquatiques. Cette approche a été appliquée à l'échelle nationale et dans un certain nombre de régions françaises, dont l'Alsace et la Lorraine. Les résultats des mesures de pesticides réalisées ensuite pendant un an ont été confrontés aux indices d'exposition obtenus. Les substances détectées le plus fréquemment correspondent effectivement à celles dotées des rangs d'exposition les plus élevés (ajustement exponentiel, r 2 ≈ 0. 82); cependant, le diuron apparaît à une fréquence plus élevée que celle attendue, {{en raison de}} ses usages non agricoles. La corrélation est moins bonne pour les substances dont les rangs d'exposition sont proches de la valeur considérée comme significative pour les eaux superficielles, ce qui peut <b>provenir</b> soit de l'utilisation de données erronées lors de la sélection, soit d'un poids insuffisant attribué à certains facteurs dans la méthode de sélection, soit enfin d'aléas météorologiques. Monitoring of micropollutants is {{a rather}} recent activity (10 - 15 years), at least in surface waters; because of the need for sophisticated analytical methods and of the potential number of analytes, this type of activity is confronted with important economic constraints, which require that one make a selection among the range of substances to monitor. Among organic micropollutants, pesticides constitute a well-identified category, since they are used mainly in agriculture; this use on broad surfaces may have important impacts on the quality of surface water. Various methods have been used to select those pesticides likely to have the greatest impacts on water quality; some of these methods might be considered to be "hazard assessment", whereas others correspond to simplified "risk assessment" methods (this appears particularly true for pesticides, of which several hundreds are used in agriculture). Recently, a French panel of experts mandated by different Ministries designed a selection method called SIRIS, which allows one to define three different lists of pesticides according to the media to be monitored (surface or ground-water) and to the monitoring objectives (ecosystem protection, drinking water production). This paper deals with the application of the SIRIS method at a regional level, {{in the context of a}} permanent survey of river quality. As a simplified risk assessment method, SIRIS combines data on hazard and exposure; hazard is estimated by a single parameter, either toxicity for aquatic species or acceptable daily intake (ADI). Exposure represents the probability that a transfer to water bodies may occur; for surface water, this probability is influenced by the crop acreage, the applied dose (kg/ha), the solubility, the pesticide half-life, the hydrolysis and the distribution coefficient between water and organic matter (Koc). These factors are considered in this hierarchical order, and for each substance a score is assigned to each of these factors among three possible values ("o"=slight, "m"=medium, "d"=high, according to the relative influence on transfer); finally exposure is estimated by a relative rank obtained by a combination of these values following a "penalisation" principle. Two tables are available for applying this approach at a regional level: the first contains the values (o,m,d) assigned to more than 300 substances by the expert panel for solubility, half-life, etc., and should be completed with crop acreage and dose. The second table provides the ranks corresponding to the different combinations of o,m,d values. A final rank of 35 was considered by consensus to be a pragmatic threshold for the transfer to surface water. This method was applied in 1996 in two regions in France (Alsace and Lorraine) separately; most of the selected chemicals (but unfortunately not all, due to technical constraints) were then analysed monthly in surface waters (24 sampling points, yielding 144 samples in Alsace and 169 in Lorraine). Occurrences fell between 0 % and 60 % in Alsace, and between 0 % and 90 % in Lorraine; in both regions, the most frequently detected chemicals were atrazine and diuron. The relevance of the selection method may be discussed under several aspects: the choice of the factors, their order, the position of thresholds corresponding to o,m,d values, the value of the overall threshold, and the availability of the data. Some pesticides are not ranked only because no data were available concerning their solubility, hydrolysis rate or Koc, but the relative importance of such gaps cannot be appreciated with the current set of data. Other items may be assessed through the comparison of the exposure rank versus the occurrence. This relationship takes an exponential shape, with some anomalies: for example, the occurrence of diuron in Alsace is higher than expected, based on its exposure rank. This situation can be explained by the fact that there are non-agricultural uses of this substance, such that the exposure rank appears to be underestimated. For other substances, like aldicarb and chlorpyrifos-ethyl, discrepancies are observed between the exposure rank and occurrences, when comparing with substances with higher exposure ranks. This anomaly may be due to poor data quality. For carbendazime, the occurrence in Lorraine appears underestimated, probably because of a dry period deficit after the application. Finally, chlortoluron received the same rank in the 2 regions, but is more frequently detected in Lorraine; crop acreage may have been overestimated in Alsace. However, the dataset is still limited to one year of sampling; some discrepancies may appear less important when more data are available. For chemicals with ranks > 50, there is a good exponential fit between ranks and occurrences (y= 0. 0235 *e 0. 0739 x; r 2 = 0. 82). This observation means that pesticides with ranks > 50 are systematically encountered in surface waters; however, the current threshold (35) should be maintained, because some substances with ranks < 50 are also detected. Thus, the SIRIS method appears to be a good tool for selecting agricultural pesticides for monitoring purposes at a regional level...|$|E
40|$|With recent {{progress}} in computing, algorithmics and telecommunications, 3 D models are increasingly used in various multimedia applications. Examples include visualization, gaming, entertainment and virtual reality. In the multimedia domain 3 D {{models have been}} traditionally represented as polygonal meshes. This piecewise planar representation {{can be thought of}} as the analogy of bitmap images for 3 D surfaces. As bitmap images, they enjoy great flexibility and are particularly well suited to describing information captured from the real world, through, for instance, scanning processes. They suffer, however, from the same shortcomings, namely limited resolution and large storage size. The compression of polygonal meshes has been a very active field of research in the last decade and rather efficient compression algorithms have been proposed in the literature that greatly mitigate the high storage costs. However, such a low level description of a 3 D shape has a bounded performance. More efficient compression should be reachable through the use of higher level primitives. This idea has been explored to a great extent in the context of model based coding of visual information. In such an approach, when compressing the visual information a higher level representation (e. g., 3 D model of a talking head) is obtained through analysis methods. This can be seen as an inverse projection problem. Once this task is fullled, the resulting parameters of the model are coded instead of the original information. It is believed that if the analysis module is efficient enough, the total cost of coding (in a rate distortion sense) will be greatly reduced. The relatively poor performance and high complexity of currently available analysis methods (except for specific cases where a priori knowledge about the nature of the objects is available), has refrained a large deployment of coding techniques based on such an approach. Progress in computer graphics has however changed this situation. In fact, nowadays, an increasing number of pictures, video and 3 D content are generated by synthesis processing rather than coming from a capture device such as a camera or a scanner. This means that the underlying model in the synthesis stage can be used for their efficient coding without the need for a complex analysis module. In other words it would be a mistake to attempt to compress a low level description (e. g., a polygonal mesh) when a higher level one is available from the synthesis process (e. g., a parametric surface). This is, however, what is usually done in the multimedia domain, where higher level 3 D model descriptions are converted to polygonal meshes, if anything by the lack of standard coded formats for the former. On a parallel but related path, the way we consume audio-visual information is changing. As opposed to recent past and a large part of today's applications, interactivity is becoming a key element in the way we consume information. In the context of interest in this dissertation, this means that when coding visual information (an image or a video for instance), previously obvious considerations such as decision on sampling parameters are not so obvious anymore. In fact, as in an interactive environment the effective display resolution can be controlled by the user through zooming, there is no clear optimal setting for the sampling period. This means that because of interactivity, the representation used to code the scene should allow the display of objects in a variety of resolutions, and ideally up to infinity. One way to resolve this problem would be by extensive over-sampling. But this approach is unrealistic and too expensive to implement in many situations. The alternative would be to use a resolution independent representation. In the realm of 3 D modeling, such representations are usually available when the models are created by an artist on a computer. The scope of this dissertation is precisely the compression of 3 D models in higher level forms. The direct coding in such a form should yield improved rate-distortion performance while providing a large degree of resolution independence. There has not been, so far, any major attempt to efficiently compress these representations, such as parametric surfaces. This thesis proposes a solution to overcome this gap. A variety of higher level 3 D representations exist, of which parametric surfaces are a popular choice among designers. Within parametric surfaces, Non-Uniform Rational B-Splines (NURBS) enjoy great popularity as a wide range of NURBS based modeling tools are readily available. Recently, NURBS has been included in the Virtual Reality Modeling Language (VRML) and its next generation descendant eXtensible 3 D (X 3 D). The nice properties of NURBS and their widespread use has lead us to choose them as the form we use for the coded representation. The primary goal of this dissertation is the definition of a system for coding 3 D NURBS models with guaranteed distortion. The basis of the system is entropy coded differential pulse coded modulation (DPCM). In the case of NURBS, guaranteeing the distortion is not trivial, as some of its parameters (e. g., knots) have a complicated influence on the overall surface distortion. To this end, a detailed distortion analysis is performed. In particular, previously unknown relations between the distortion of knots and the resulting surface distortion are demonstrated. Compression efficiency is pursued at every stage and simple yet efficient entropy coder realizations are defined. The special case of degenerate and closed surfaces with duplicate control points is addressed and an efficient yet simple coding is proposed to compress the duplicate relationships. Encoder aspects are also analyzed. Optimal predictors are found that perform well across a wide class of models. Simplification techniques are also considered for improved compression efficiency at negligible distortion cost. Transmission over error prone channels is also considered and an error resilient extension defined. The data stream is partitioned by independently coding small groups of surfaces and inserting the necessary resynchronization markers. Simple strategies for achieving the desired level of protection are proposed. The same extension also serves the purpose of random access and on-the-fly reordering of the data stream. Avec les récents progrès de l'informatique et des télécommunications, les modèles 3 D sont de plus en plus utilisés dans les applications multimédia. La visualisation, les jeux, le divertissement et la réalité virtuelle comptent parmi les exemples les plus répandus. Dans le domaine du multimédia les modèles 3 D ont été traditionnellement représentés comme des maillages polygonaux. Cette représentation plane par morceaux, peut être vue comme l'analogue des images bitmap pour les surfaces 3 D. Comme les images bitmap, ils jouissent d'une grande flexibilité et sont particulièrement bien adaptés pour décrire des informations acquises depuis le monde réel, comme par exemple, lors d'un processus de balayage. Ils souffrent, cependant, des mêmes limitations, notamment une résolution limitée et un grand espace de stockage. La compression de maillages polygonaux est un domaine de recherche très actif depuis une décennie et des algorithmes de compression efficaces permettant de réduire fortement les besoins en place de stockage, ont été proposés dans la littérature. Cependant, cette description bas-niveau de formes 3 D a une performance limitée. Une compression plus efficace devrait être possible avec l'usage de primitives de plus haut niveau. Cette idée a été extensivement explorée dans le contexte du codage à base de modèles de l'information visuelle. Dans une telle approche, lors de la compression de l'information visuelle une représentation de plus haut niveau (par ex. un modèle 3 D d'une tête parlante) est obtenue par analyse. Ceci peut être vu comme un problème de projection inverse. Une fois cette tâche accomplie, les paramètres du modèle résultants sont codés à la place de l'information originale. Il est communément admis que si le module d'analyse est suffisamment efficace le coût total de codage (dans le sens débit distorsion) en sera largement réduit. La performance relativement basse et la haute complexité des méthodes d'analyse existantes (mis à part des cas spécifiques où une connaissance a priori de la nature des objets est disponible), a empêché un large déploiement des techniques de codage basées sur une telle approche. Le progrès dans le domaine de l'infographie (computer graphics) a néanmoins changé cette situation. En effet, de nos jours, un nombre croissant d'images, vidéos et contenu 3 D sont générés par procédés de synthèse au lieu de <b>provenir</b> d'un appareil de capture, tels une caméra ou un scanner. Cela signifie que le modèle sous-jacent dans le stade de synthèse peut être utilisé pour améliorer les performances de codage sans avoir besoin de recourir à un module d'analyse hautement complexe. En d'autres mots, ce serait une erreur que de vouloir essayer de compresser une description bas-niveau (par ex. un maillage polygonal) alors qu'une description de plus haut niveau est disponible dans le processus de synthèse (par ex. une surface paramétrique). Cela est, cependant, ce qui est couramment fait dans le domaine du multimédia, où des descriptions de modèles 3 D de haut niveau sont convertis en maillage polygonaux, ne serait-ce que par manque d'un format standard pour le codage de ceux-ci. Par ailleurs, la façon dont nous consommons l'information audiovisuelle est en train de changer. A l'opposé des anciennes applications et une grande partie des actuelles, l'interactivité est en train de devenir un élément clé de la manière dont nous consommons l'information. Dans le cadre de la présente dissertation, cela signifie que, lorsque nous codons une information visuelle (par ex. une image ou une vidéo), des considérations évidentes par le passé telles que la sélection des paramètres d'échantillonnage n'est plus aussi évidente qu'avant. En effet, à l'instar d'un environnement interactif où la résolution d'affichage effective peut être contrôlée par l'utilisateur à travers un zoom, il n'y a pas de choix optimal clairement défini pour les paramètres d'échantillonnage. Cela signifie qu'à cause de l'interactivité, la représentation utilisée pour coder la scène devrait permettre l'affichage des objets dans une large gamme de résolutions et, idéalement, jusqu'à l'infini. Une façon de résoudre ce problème serait l'utilisation d'un suréchantillonnage extensif. Néanmoins, cette approche est irréaliste et trop coûteuse à implanter dans beaucoup de situations. L'alternative serait d'utiliser une représentation indépendante de la résolution. Dans le domaine du modelage 3 D, lesdites représentations sont couramment disponibles lorsque les modèles sont crées par un artiste sur un ordinateur. Le sujet de cette dissertation est précisément la compression de modèles 3 D dans une forme de plus haut niveau. Le codage direct sous une telle forme devrait délivrer une performance débit distorsion améliorée tout en procurant un large degré d'indépendance de résolution. Il n'y a pas eu, jusqu'à ce jour, de travaux majeurs sur la compression efficace de telles représentations, telles que les surfaces paramétriques. Cette thèse propose une solution pour combler ce vide. Une variété de représentations 3 D de haut niveau existe, parmi lesquelles les surfaces paramétriques sont un choix répandu parmi les designers. Dans la famille des surfaces paramétriques, les B-Splines rationnelles non-uniformes (NURBS) jouissent d'une grande popularité étant donné qu'une large gamme d'outils basés sur les NURBS sont couramment disponibles. Récemment, les NURBS ont été ajoutées dans le Virtual Reality Modeling Language (VRML) ainsi que son descendant de nouvelle génération le eXtensible 3 D (X 3 D). Les bonnes propriétés des NURBS et leur utilisation largement répandue nous ont conduit à les choisir comme forme sous laquelle les modèles seront codés. Le but principal de cette dissertation est la définition d'un système de codage des modèles 3 D NURBS avec une distorsion garantie. La base du système est la modulation par impulsion et codage différentiel (DPCM) codée entropiquement. Dans le cas de NURBS, garantir la distorsion n'est pas évident, dès lors que certains de ses paramètres (par ex. noeuds) ont une influence compliquée sur la distorsion totale de la surface. A cette fin, une analyse détaillé de la distorsion est effectuée. En particulier, des relations jusqu'alors inconnues entre la distorsion des noeuds et la distorsion de la surface résultante est démontrée. L'efficacité de la compression est recherchée à chaque stade et des codeurs entropiques simples mais néanmoins efficaces sont définis. Le cas particulier de surfaces fermées et dégénérées avec des point de contrôle dupliqués est adressé et un codage simple et efficace est proposé pour compresser les relations de duplication. Les aspects de l'encodeur sont aussi analysés. Des prédicteurs optimaux ayant une bonne performance sur une large classe de modèles sont trouvés. Des techniques de simplification, ayant un coût négligeable sur la distorsion, sont aussi considérées pour une meilleure efficacité de compression. La transmission sur des canaux présentant un taux d'erreur non négligeable est aussi considérée est une extension de résilience aux erreurs est définie. Le train de données est morcelé en codant indépendamment des petits groupes de surfaces et en insérant les marqueurs de resynchronisation nécessaires. Des stratégies simples pour atteindre le niveau désiré de protection sont proposées. La même extension sert aussi pour l'accès aléatoire et le réordonnancement à la demande du train de données...|$|E

