33|10000|Public
5000|$|The one-megabyte limit {{has created}} a {{bottleneck}} in bitcoin, resulting in increasing transaction fees and delayed <b>processing</b> <b>of</b> <b>transactions</b> that cannot be fit into a block. Various proposals have come forth on how to scale bitcoin and a contentious debate has resulted. Business Insider in 2017 characterized this debate as an [...] "ideological battle over bitcoin's future." ...|$|E
5000|$|... == Operation == A {{national}} computerised {{network was}} set up with overnight <b>processing</b> <b>of</b> <b>transactions,</b> initially on IBM System/360 Model 40 computers. Money Transfer Services (MTS) were introduced in February 1969, and all branches of the trading banks were converted to computer processing by November 1969. In 1974/75 chequing accounts were introduced by the Post Office Savings Bank and several trustee savings banks.|$|E
50|$|Every {{recipient}} agency {{participating in}} the scam had employees or officials that maintained contact with Napoles, allowing for the smooth <b>processing</b> <b>of</b> <b>transactions</b> and the expedient release of PDAF funds to her organizations. Most importantly, Napoles was in regular contact with the DBM through Undersecretary for Operations Mario L. Relampagos, who had three employees (identified as Leah, Malou and Lalaine) responsible for the processing of SAROs destined for Napoles' organizations.|$|E
50|$|As {{of today}} jPOS is {{supported}} by more than 2,000 developers and is in production in 5 continents <b>processing</b> millions <b>of</b> <b>transactions</b> including several 1,000 TPS solutions.|$|R
5000|$|The {{sphere of}} {{influence}} described by revenue assurance varies greatly between telecommunications service providers, but is usually closely related to back office functions where small errors may have a disproportionately large impact on revenues or costs. The <b>processing</b> <b>of</b> <b>transaction</b> data in modern telecommunications providers exhibits many attributes akin to a complex system. However, there is significant disagreement about the ultimate aims and legitimate scope of revenue assurance teams. This is in part caused by: ...|$|R
40|$|In {{this report}} {{we present a}} new file {{facility}} architecture where a separate interface is provided to speedup <b>processing</b> <b>of</b> <b>transaction</b> oriented file operations. This interface is event driven and ensures that maximum number <b>of</b> <b>transaction</b> oriented file operations are processed and supervised at a client's workstation. Furthermore, it efficiently coordinates the <b>processing</b> <b>of</b> distributed <b>transactions</b> and relieves a server to take the responsibility of a coordinator or worker. In order to improve the performance we provide caching of file's data in the main memory of a client's workstation and the file server. We demonstrate that our design allows the granularity of locking as fine as a single byte and as coarse as an entire file. Therefore we claim our design addresses the requirements {{of a wide range}} of applications. The use of intentions list in our design ensures that transaction oriented file operations are resilient against system and media failures. We further demonstrate that it [...] ...|$|R
5000|$|In April 2009, {{the company}} made a {{settlement}} with the United States government where they agreed to pay a penalty of $105 {{million over the next}} four years as part of a [...] "non-prosecution agreement". As part of the deal, PartyGaming put its name to a [...] "statement of facts" [...] in which it admitted for the first time that, before October 2006, it had targeted US citizens, resulting in the <b>processing</b> <b>of</b> <b>transactions</b> that were [...] "contrary to certain US laws".|$|E
50|$|The {{blocks in}} the {{blockchain}} are limited to one megabyte in size, which has created problems for bitcoin transaction processing, such as increasing transaction fees and delayed <b>processing</b> <b>of</b> <b>transactions</b> that cannot be fit into a block. Contenders to solve the scalability problem {{are referred to as}} Bitcoin Cash, Bitcoin Classic, Bitcoin Unlimited, and SegWit2x. On 21 July 2017 miners locked-in a software upgrade referred to as Bitcoin Improvement Proposal (BIP) 91, meaning that the controversial Segregated Witness upgrade will activate at block # 477,120.|$|E
40|$|In {{order to}} {{increase}} availability in a distributed system {{some or all of}} the data items are replicated and stored at separate sites. This is an issue of key concern especially since there is such a proliferation of wireless technologies and mobile users. However, the concurrent <b>processing</b> <b>of</b> <b>transactions</b> at separate sites can generate inconsistencies in the stored information. Several replica control techniques have been proposed to deal with the <b>processing</b> <b>of</b> <b>transactions.</b> Some of these techniques include certain consistency checks before processing the transaction, i. e. they are pessimistic in nature. Other techniques process the transaction and do the consistency checking afterwards, i. e. they are optimistic. We have built a distributed service that manages updates to widely deployed counter-like replicas. The service is built on our distributed concurrency control scheme which combines optimism and pessimism in the <b>processing</b> <b>of</b> <b>transactions.</b> The service allows a transaction to be processed immediately (optimistically) at any individual replica as long as the transaction satisfies a cost bound. All transactions are also processed in a concurrent pessimistic manner to ensure mutual consistency. Information system management, distributed system, replica management, transaction processing. 1...|$|E
5000|$|Annually <b>processing</b> billions <b>of</b> <b>transactions</b> {{valued at}} trillions of South African rand; BankservAfrica's clients include banks, corporates, {{government}} and the retail sector. By volume <b>of</b> <b>transactions</b> alone, it is rated as Africaʼs largest automated payments clearing house.|$|R
5000|$|Issuance {{of credit}} cards and <b>processing</b> <b>of</b> credit card <b>transactions</b> and billing ...|$|R
40|$|Several multi-pass {{algorithms}} {{have been}} proposed for Association Rule Mining from static repositories. However, such algorithms are incapable <b>of</b> online <b>processing</b> <b>of</b> <b>transaction</b> streams. In this paper we introduce an efficient single-pass algorithm for mining association rules, given a hierarchical classification amongst items. Processing efficiency is achieved by utilizing two optimizations, hierarchy aware counting and transaction reduction, which become possible {{in the context of}} hierarchical classification. We also propose a modified algorithm for the rule generation phase which avoids the construction of an explicit adjacency lattice...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedElectronic data interchange (EDI) is the intercompany, computer-to-computer exchange of business documents in standard formats. The direct benefits of EDI consist in cost savings, operational accuracy, and speedy <b>processing</b> <b>of</b> <b>transactions.</b> This thesis provides guidelines {{to develop an}} EDI (Electronic Data Interchange) system. It discusses the basic concepts, standards, data mappping, hardware and software requirements, and networking requirements. Also discussed are some auditing and security issues in implementing EDI. [URL] Korean Arm...|$|E
40|$|In a {{distributed}} system {{some or all}} of the data items are replicated and stored at separate sites. This increases the availability of these items and it is then possible to complete transactions faster than in a single site system. However, the concurrent <b>processing</b> <b>of</b> <b>transactions</b> at separate sites can generate inconsistencies in the stored information. Several replica control techniques have been proposed to deal with the <b>processing</b> <b>of</b> <b>transactions.</b> Some of these techniques include certain consistency checks before processing the transaction, i. e. they are pessimistic in nature. Other techniques process the transaction and do the consistency checking afterwards, i. e. they are optimistic. The system discussed here is an attempt to show how the increased processing capabilities provided by (i) multi-threaded object-oriented programming, and by (ii) powerful computing nodes in the {{distributed system}} can be used to combine optimism and pessimism into one system. The system allows a transaction to be processed immediately (optimistically) at any individual replica as long as the transaction satisfies a cost bound. All transactions are also processed in a concurrent pessimistic manner. Only the changes made to the replicas via pessimistic processing are made permanent...|$|E
40|$|The wide {{emergence}} of electronic-commerce has widened the extensive usage {{of credit card}} for online transactions. However, {{there is also a}} high rise in malicious transaction and fraudulent associated with the credit cards. In this study we present several models and algorithm used in data mining for the detection of such malicious fraudulents or thefts. Such algorithm learns the transaction patterns and cluster the pattern of sequences usually involving with the <b>processing</b> <b>of</b> <b>transactions</b> to inhibit such malicious transactions made in the future...|$|E
40|$|Real life {{transaction}} data often miss some occurrences {{of items that}} are actually present. As a consequence some potentially interesting frequent item sets cannot be discovered, since with exact matching the number <b>of</b> supporting <b>transactions</b> may be smaller than the user-specified minimum. In order to allow approximate matching during the mining process, we propose an approach based on transaction editing. Our recursive algorithm relies on a step by step elimination of items from the {{transaction data}}base together with a recursive <b>processing</b> <b>of</b> <b>transaction</b> subsets. This algorithm works without complicated data structures and allows us to find fuzzy frequent item sets easily. 1...|$|R
50|$|Xchanging was foed in 1999, by David Andrews, {{a former}} partner in Andersen Consulting. Andrews {{came up with}} the idea at Andersen Consulting, to create joint ventures with large multinationals to {{outsource}} the <b>processing</b> <b>of</b> back-office <b>transactions.</b>|$|R
5000|$|There is {{high-level}} {{agreement between}} practitioners about {{the goals and}} methods of revenue assurance, though reaching a consensus on defining the boundaries of revenue assurance has proved elusive so far. The goals relate to improving the financial performance by eliminating mistakes in the <b>processing</b> <b>of</b> <b>transaction</b> data. Some take a more encompassing view of what counts as a mistake, which may extend as far as questioning the policy set by executives even when this has been executed correctly. Others take a more open-ended view of the data {{that is the subject}} matter. For example, in decreasing order of frequency, revenue assurance may cover: ...|$|R
30|$|Transaction Processing: most {{organizations}} need {{systems that}} perform and record routine transactions for the company's operations, such as processing sales and reservation requests, dealing with payable and receivable accounts, among others (Laudon and Laudon 2007). This type of system, called transaction processing, supports daily activities and helps organizations add value to their {{products and services}} (Stair and Reynolds 2005). In order {{to contribute to the}} identification and writing of requirements related to the <b>processing</b> <b>of</b> <b>transactions,</b> the pattern Process Transaction was elaborated and is presented in Additional file 2.|$|E
40|$|To {{support the}} highly {{concurrent}} <b>processing</b> <b>of</b> <b>transactions</b> with low price at parallel and distributed database systems, Network Of Workstations(NOW) is utilized. All workstations cooperate {{to perform the}} jobs submitted by database applications. Each job consists of several transactions and these transactions are executed on NOW. Each transaction sent to NOW is allocated to a certain workstation by the coordinator running at a workstation. In this paper, we present a Distributed-Transaction Coordinator (DTC). In DTC, the cost to finish transactions is collected automatically and each transaction is assigned to an appropriate site based on the collected information...|$|E
40|$|An {{important}} problem {{faced by}} auditors is gauging how much reliance {{can be placed}} on the accounting systems that process millions of transactions to produce the numbers summarized in a company’s financial statements. Accounting systems contain internal controls, procedures designed to detect and correct errors and irregularities that may occur in the <b>processing</b> <b>of</b> <b>transactions.</b> In a complex accounting system, it can be an extremely difficult task for the auditor to anticipate the possible errors that can occur and {{to evaluate the effectiveness of}} the controls at detecting them. An accurate analysis must take into account the unique features of each company’s business processes. To cope with this complexity an...|$|E
40|$|Database {{performance}} {{is a very}} important aspect of database usability. The objective {{of this paper is to}} proposed policy to forecast the performance <b>of</b> <b>transaction</b> under real time database system in distributed environment. A real time database system in distributed environment is a transaction processing system design to handle the workload within a deadline. The objective of such scheme is to complete the <b>processing</b> <b>of</b> <b>transaction</b> before the deadline expires. The performance of the system depends on the factors like as database system architectures, underlying processors, various operating conditions, disks speeds and workloads. Our works involves <b>of</b> forecasting the <b>transaction</b> performance depends on the basis of comparing with commit and abort of a transition in the scheme to give the result through simulatio...|$|R
40|$|Abstract: Due {{to various}} reasons {{transaction}} data often lack information about some items. This {{leads to the}} problem that some potentially interesting frequent item sets cannot be discovered, since by exact matching the number <b>of</b> supporting <b>transactions</b> may be smaller than the user-specified minimum. In this study we try to find such frequent item sets nevertheless by inserting missing items into transactions during the mining process {{in order to allow}} approximate matching. We present a recursive elimination algorithm, based on a step by step elimination of items from the transaction database together with a recursive <b>processing</b> <b>of</b> <b>transaction</b> subsets. This algorithm is very simple, works without complicated data structures, and allows us to find fuzzy frequent item sets easily...|$|R
50|$|An {{operational}} {{system is a}} term used in data warehousing {{to refer to a}} system that is used to process the day-to-day <b>transactions</b> <b>of</b> an organization. These systems are designed in a manner that <b>processing</b> <b>of</b> day-to-day <b>transactions</b> is performed efficiently and the integrity of the transactional data is preserved.|$|R
40|$|Databases are {{models of}} the real world. Yet, our {{knowledge}} of the real world is often imperfect, thus challenging our ability to create databases of integrity. To uphold the integrity of a database in situations where knowledge of the real world is imperfect, one may either (1) restrict the model to that portion of the real world about which perfect information is available, or (2) develop formalisms that allow the representation of imperfect information. This paper surveys some of the better-known database formalisms for capturing imperfect information. Imperfections in the specification and <b>processing</b> <b>of</b> <b>transactions</b> also have important impact {{on the quality of the}} information delivered to users, and this survey discusses them as well...|$|E
40|$|In {{order to}} {{increase}} availability in a distributed system {{some or all of}} the data items are replicated and stored at separate sites. This is an issue of key concern especially since there is such a proliferation of wireless technologies and mobile users. We have built a distributed service that manages updates to widely deployed counter-like replicas. The service is built on our distributed concurrency control scheme which combines optimism and pessimism in the <b>processing</b> <b>of</b> <b>transactions.</b> The system is currently used as a prototype and is being tested using data input via an XML file. This is in keeping with the movement towards Web Services and the use of current standard technologies for data representation. KEY WORD...|$|E
40|$|Abstract: Electronic Governance (e-Governance) {{is the use}} of Information and Communication Technologies (ICT) for the planning, implementation, and {{monitoring}} of government programs, projects, and activities. E-Governance is expected to help deliver cost-effective and easy-to-access citizen services, and improve <b>processing</b> <b>of</b> <b>transactions</b> both within the government, and between the government and other agencies. The National e-Governance Plan (NeGP) which is the flagship e-governance programme of the Central Government, was approved by the Government of India in May 2006 comprising with 27 Mission Mode Projects(MMPs) at the Central, State and Local Government level. But the number of mission mode projects is going to be 30 which are under process on the cabinet where education sector is one of them. Under the NeGP, the state ‘Mission Mode Projects ’ ar...|$|E
30|$|As we can see, even a {{small change}} in the CVL limit will bring {{benefits}} related to the <b>processing</b> time <b>of</b> the <b>transaction.</b>|$|R
50|$|Number Default Accounts - If a {{phone number}} or range of phone numbers can often be related to a {{specific}} account among a firm’s clients, some ECRS programs can automatically identify that account with the call to be then charged during the <b>processing</b> <b>of</b> call <b>transactions</b> into the billing system.|$|R
50|$|The {{transaction}} records printed by the 3624 {{and used}} by customers to verify their transactions were approximately 3 inches square and on similar card stock to punch cards. When performing deposits, customers were instructed to place a special <b>transaction</b> record inside <b>of</b> the deposit envelope {{to aid in the}} <b>processing</b> <b>of</b> the <b>transaction</b> by the back office staff.|$|R
40|$|Abstract: 2 ̆ 2 We {{present an}} {{overview}} of the novel aspects of Avalon/Common Lisp: (1) support for remote evaluation through a new evaluator data type; (2) a generalization of the traditional client/server model of computation, allowing clients to extend server interfaces and server writers to hide aspects of distribution, such as caching, from clients; (3) support for automatic commit and abort <b>processing</b> <b>of</b> <b>transactions</b> and automatic crash recovery of atomic data. These capabilities provide programmers with the flexibility to exploit the semantics of an application to enhance its reliability and efficiency. 2 ̆ 2 Though the design of Avalon/Common Lisp exploits some of the features of Common Lisp, e. g., its packaging mechanism, all of the constructs are applicable to any Lisp-like language. 2 ̆...|$|E
40|$|The rise of {{non-bank}} {{trade finance}} {{has been especially}} noticeable in the last decade. Many commodity and e-commerce companies are rapidly entering this arena as sovereign guarantees and collaterals take a backseat {{making it harder for}} banks to apply traditional models while lending to businesses. Non-banks which are more nimble occupy space vacated by banks. Increasingly, though, banks have begun to collaborate with non-banks, particularly fintech companies for rapid <b>processing</b> <b>of</b> <b>transactions.</b> Besides, trade finance field is itself developing its own specialised areas such as commodity finance, power generation and financial services. Growth in each field depends on the extent of specialised knowledge, developments in capital markets, innovations by technological service providers permitting new collaborations and a dynamic regulatory framework including self-regulation for assessing and disclosing emerging risks...|$|E
40|$|Severe data {{contention}} {{may exist}} between short update transactions and long read-only queries if both are {{to access the}} same database through a conventional concurrency control method. In this paper, we present dynamic finite versioning (DFV) schemes to effectively support concurrent <b>processing</b> <b>of</b> <b>transactions</b> and queries. Without acquiring locks, queries read from a small, fixed number of dynamically derived, transaction-consistent, but maybe slightly obsolete, logical snapshots of the database. On the other hand, transactions access the most up-to-date data in the database without data contention from queries. Intermediate versions created between snapshots are automatically discarded. Furthermore, dirty pages updated by active transactions are allowed to be written back into the database before commitment (i. e., the STEAL policy), {{and at the same}} time, consistent logical snapshots can be advanced automatically without quiescing the ongoing transactions or queries. 1 Introduction C [...] ...|$|E
50|$|Real-time posting {{refers to}} a style <b>of</b> <b>processing</b> {{financial}} <b>transactions</b> in a core banking system. It is {{an alternative to the}} older Memo Posting style.|$|R
50|$|EFAMA {{strongly}} {{believes that}} the FPSG recommendations, if embraced by the industry, will serve to converge towards industry-wide standards, thereby removing an important barrier to the development <b>of</b> harmonized <b>processing</b> <b>of</b> investment fund <b>transactions</b> in Europe.|$|R
30|$|As {{shown in}} the results, {{transactions}} incur extra processing time compared to simple ‘Update only’ (or Write) operations but they enforce 100 % consistency. If the number <b>of</b> <b>transactions</b> and clients is smaller (2 clients) then transactions do not incur much processing overhead. However, {{when the number of}} clients increases from 16 to 32, then the transaction execution time also increases. This {{is due to the fact}} that transactions manipulate shared data which generate conflicts between different transactions. Similar to MongoDB, the <b>processing</b> overhead <b>of</b> <b>transactions</b> is incurred due to the coordination <b>of</b> <b>transaction</b> operations on the data.|$|R
