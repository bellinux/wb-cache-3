38|110|Public
50|$|The Threadstorm {{processors}} only execute user code, {{on top of}} {{a simple}} BSD Unix-based microkernel called MTX; system I/O is performed by Opteron processors running Linux. This third generation MTA system improves clock speed from 220 MHz to 500 MHz, the maximal <b>processor</b> <b>count</b> from 256 to 8192, and maximum memory to 512 TB.|$|E
5000|$|... "The {{method for}} {{measuring}} system performance is as follows: {{the ratio of}} time a given benchmark application at a target <b>processor</b> <b>count</b> provides a relative measure of the system's performance on that application test case compared with the DoD standard system, stated in Habu-equivalents. Habu, the first DoD standard system, is an IBM POWER3 formerly located at the US Naval Oceanographic Office (NAVO) Major Shared Resource Center. One Habu-equivalent is the performance of 1,024 system-under-study processors compared with 1,024 Habu processors.|$|E
5000|$|In {{the early}} 1990s, a new {{definition}} of supercomputer {{was needed to}} produce meaningful statistics. After experimenting with metrics based on <b>processor</b> <b>count</b> in 1992, the idea arose at the University of Mannheim to use a detailed listing of installed systems as the basis. In early 1993, Jack Dongarra was persuaded to join the project with his LINPACK benchmarks. A first test version was produced in May 1993, partly based on data available on the Internet, including the following sources: ...|$|E
40|$|As {{applications}} {{scale to}} increasingly large <b>processor</b> <b>counts,</b> the interconnection network is frequently the limiting factor in application performance. In {{order to achieve}} application scalability, the interconnect must maintain high bandwidth while minimizing variation in packet latency. As the offered load in the network increases with growing problem sizes and <b>processor</b> <b>counts,</b> so does the expected maximum packet latency in the network, directly impacting performance of applications with any synchronized communication. Age-based packet arbitration reduces the variance in packet latency as well as average latency. This paper describes the Cray XT router packet aging algorithm which allows globally fair arbitration by incorporating “age” in the packet output arbitration. We describe {{the parameters of the}} aging algorithm and how to arrive at appropriate settings. We show that an efficient aging algorithm reduces both the average packet latency and the variance in packet latency on communication-intensive benchmarks. General terms: design; architecture; performanc...|$|R
40|$|Communication {{has always}} been a {{limiting}} factor in making efficient computing architectures with large <b>processor</b> <b>counts.</b> Reconfigurable interconnects can help in this respect, since they can adapt the interprocessor network to the changing communication requirements imposed by the running application. In this paper, we present a performance evaluation of these reconfigurable interconnection networks in the context of shared-memory multiprocessor (SMP) machines. We look at the effects of architectural parameters such as reconfiguration speed and topological constraints, and analyze how these results scale up with higher <b>processor</b> <b>counts.</b> We find that for 16 processors connected in a torus topology, reconfigurable interconnects with switching speeds in the order of milliseconds can provide up to 20 % reduction in communication delay. For larger networks, up to 64 processors, the expected gain can rise up to 40 %. This shows that reconfigurable networks can help in removing the communication bottleneck from future interconnection designs. 1...|$|R
5000|$|Processors may be {{interconnected}} using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth {{and power}} {{consumption of the}} interconnect among the various processors, the memory, and the disk arrays. Mesh architectures avoid these bottlenecks, and provide nearly linear scalability to much higher <b>processor</b> <b>counts</b> at the sacrifice of programmability: ...|$|R
50|$|Since {{critical}} sections may execute only on {{the processor}} on which they are entered, synchronization is only required within the executing processor. This allows critical sections to be entered and exited at almost zero cost. No inter-processor synchronization is required. Only instruction stream synchronization is needed. Most processors provide the required amount of synchronization by {{the simple act of}} interrupting the current execution state. This allows critical sections in most cases to be nothing more than a per <b>processor</b> <b>count</b> of critical sections entered.|$|E
50|$|One of {{the more}} {{interesting}} applications of the MIPS architecture is its use in massive <b>processor</b> <b>count</b> supercomputers. Silicon Graphics (SGI) refocused its business from desktop graphics workstations to the high-performance computing market in the early 1990s. The success of the company's first foray into server systems, the Challenge series based on the R4400 and R8000, and later R10000, motivated SGI to create a vastly more powerful system. The introduction of the integrated R10000 allowed SGI to produce a system, the Origin 2000, eventually scalable to 1024 CPUs using its NUMAlink cc-NUMA interconnect. The Origin 2000 begat the Origin 3000 series which topped out with the same 1024 maximum CPU count but using the R14000 and R16000 chips up to 700 MHz. Its MIPS-based supercomputers were withdrawn in 2005 when SGI made the strategic decision to move to Intel's IA-64 architecture.|$|E
5000|$|A single {{processor}} or uniprocessor system could disable interrupts by executing currently running code without preemption, {{which is very}} inefficient on multiprocessor systems."The key ability we require to implement synchronization in a multiprocessor {{is a set of}} hardware primitives with the ability to atomically read and modify a memory location. Without such a capability, the cost of building basic synchronization primitives will be too high and will increase as the <b>processor</b> <b>count</b> increases. There are a number of alternative formulations of the basic hardware primitives, all of which provide the ability to atomically read and modify a location, together with some way to tell if the read and write were performed atomically. These hardware primitives are the basic building blocks that are used to build a wide variety of user-level synchronization operations, including things such as locks and barriers. In general, architects do not expect users to employ the basic hardware primitives, but instead expect that the primitives will be used by system programmers to build a synchronization library, a process that is often complex and tricky." [...] Many modern hardware provides special atomic hardware instructions by either test-and-set the memory word or compare-and-swap contents of two memory words.|$|E
40|$|We {{prove the}} {{correctness}} of a recently-proposed cache coherence protocol, Tardis, which is simple, yet scalable to high <b>processor</b> <b>counts,</b> because it only requires O(logN) storage per cacheline for an N-processor system. We prove that Tardis follows the sequential consistency model and is both deadlock- and livelock-free. Our proof {{is based on}} simple and intuitive invariants {{of the system and}} thus applies to any system scale and many variants of Tardis. Comment: 16 pages, 2 figure...|$|R
40|$|Further {{performance}} improvements {{of parallel}} simulation applications {{will not be}} reached by simply scaling today’s simulation algorithms and system software. Rather, they need qualitatively different approaches and new developments that address and reduce the typically non-linearly increasing complexity of algorithms {{with the use of}} increasing <b>processor</b> <b>counts.</b> We presented first results of an activity aimed at improving the performance of collective communication operations of relevance to simulation applications through more efficient implementations of collective communication operations for largescale program executions...|$|R
40|$|Community {{models for}} global climate {{research}} {{such as the}} Community Atmospheric Model must per-form well {{on a variety of}} computing systems. Supporting diverse research interests, these computationally demanding models must be efficient for a range of problem sizes and <b>processor</b> <b>counts.</b> In this paper we describe the data structures and associated infrastructure developed for the physical parameterizations that allow the Community Atmospheric Model (CAM 3) to be tuned for vector or nonvector systems, to provide load balancing while minimizing communication overhead, and to exploit the optimal mix of distributed MPI processes and shared OpenMP threads. ...|$|R
40|$|Abstract—HPC {{applications}} {{are increasingly being}} used in academia and laboratories for scientific research and in industries for business and analytics. Cloud computing offers the benefits of virtualization, elasticity of resources and elimination of cluster setup cost and time to HPC applications users. However, poor network performance, performance variation and OS noise {{are some of the}} challenges for execution of HPC applications on Cloud. In this paper, we propose that Cloud can be viable platform for some HPC applications depending upon application characteristics such as communication volume and pattern and sensitivity to OS noise and scale. We present an evaluation of the performance and cost tradeoffs of HPC applications on a range of platforms varying from Cloud (with and without virtualization) to HPC-optimized cluster. Our results show that Cloud is viable platform for some applications, specifically, non communicationintensive applications such as embarrassingly parallel and tree-structured computations up to high <b>processor</b> <b>count</b> and for communication-intensive applications up to low <b>processor</b> <b>count...</b>|$|E
30|$|Scaling such {{codes to}} large <b>processor</b> <b>count</b> {{requires}} overcoming not only spatial resolution challenges, but also large ranges in timescales. In this paper, we compare two approaches to handling this problem. The first approach involves using different time steps for different particles {{in relation to}} their dynamical time scales, leading to an algorithm that is challenging to parallelize effectively. An alternative approach, using a single, uniformly small time step for all particles, leads to more computation, but is simpler to parallelize.|$|E
40|$|The {{problem of}} {{constructing}} in parallel a maximal independent set {{of a given}} graph is considered. A new deterministic NC-algorithm implemented in the EREW PRAM model is presented. On graphs with n vertices and m edges, it uses O ((n +m) /logn) processors and runs in O (log³ n) time. This reduces {{by a factor of}} logn both the running time and the <b>processor</b> <b>count</b> of the previously fastest deterministic algorithm which solves the problem using a linear number of processors...|$|E
40|$|To date, many {{commercial}} {{applications in}} telecommunications, data transmission and data storage require {{a high level}} ofcryptographical protection. The Asc processor, presented here can be programmed to execute a large set of cryptographical functions, not found in other cryptographical devices. Novel architectures for both data path and controller {{have been designed to}} realize this high degree of programmability, while still reaching a high throughput. The compact <b>processor</b> <b>counts</b> 18 K transistors on 25 mm in a 2. 4 /m CMOS process and yet it reaches a throughput of 30 Mbit/s for every single-encryption mode. It is the fastest DES processor currently available...|$|R
50|$|Generally, large of {{processor}} register {{shows how}} big numbers core of <b>processor</b> can <b>count</b> one time. Number of registers is important too, {{because they can}} connect together for a moment with some instructions.|$|R
40|$|Most space-sharing {{resources}} presently {{operated by}} high performance computing centers employ {{some sort of}} batch queueing system to manage resource allocation to multiple users. In this work, we explore a new method for providing end-users with predictions of the bounds on queuing delay individual jobs will experience when waiting to be scheduled to a machine partition. We evaluate this method using scheduler logs that cover a 10 year period from 10 large HPC systems. Our results show {{that it is possible}} to predict delay bounds with specified confidence levels for jobs in different queues, and for jobs requesting different ranges of <b>processor</b> <b>counts.</b> 1...|$|R
40|$|Parallel {{implementation}} and performance {{assessment of the}} grid assembly function in a chimera grid approach, realized by the Beggar code, are presented. A mixed programming model is used combining message passing with shared-memory programming constructs using standard POSIX calls. The POSIX calls are used to store large, common data structures in shared memory. The associated reduction in the total memory requirements allows more processors to be used. The effective utilization of this increased <b>processor</b> <b>count</b> is based on improvements in load balancing because of a finer decomposition of the work associated with the grid assembly function. Parallel efficiency is demonstrated using a three-store, ripple release, trajectory calculation...|$|E
40|$|A {{deterministic}} parallel algorithm for parallel tree contraction {{is presented}} in this paper. The algorithm takes T time and uses (P processors, where n the number of vertices in a tree using an Exclusive Read and Exclusive Write (EREW) Parallel Random Access Machine (PRAM). This algorithm improves the results of Miller and who use the CRCW randomized PRAM model {{to get the same}} complexity and <b>processor</b> <b>count.</b> The algorithm is optimal {{in the sense that the}} product P is equal to the input size and gives an time algorithm when log Since the algorithm requires space, which is the input size, it is optimal in space as well. Techniques for prudent parallel tree contraction are also discussed, as well as implementation techniques for connection machines...|$|E
40|$|Abstract — The Community Atmospheric Model (CAM) is {{a global}} {{atmosphere}} model developed for the weather and climate research communities. CAM also serves as the atmospheric component of the Community Climate System Model (CCSM). As a community model, {{it is important that}} CAM run efficiently on different architectures, and that it be easily ported to and optimized on new platforms. The current version of CAM contains a number of performance portability features- compile-time or runtime parameters {{that can be used to}} optimize performance for a given platform, problem or <b>processor</b> <b>count.</b> The large number of tuning options can make benchmarking an arduous task. The paper describes these options and how optimization is managed to make it feasible for evaluation of early systems. The paper also describes some of the performance sensitivities of selecte...|$|E
40|$|Abstract. PEPC (Pretty Efficient Parallel Coulomb-solver) is {{a complex}} HPC {{application}} developed at the Jülich Supercomputing Centre, scaling to thousands of processors. This is {{a case study of}} challenges faced when applying the Scalasca parallel performance analysis toolset to this intricate example at relatively high <b>processor</b> <b>counts.</b> The Scalasca version used in this study has been extended to distinguish iteration/timestep phases to provide a better view of the underlying mechanisms of the application execution. The added value of the additional analyses and presentations is then assessed to determine requirements for possible future integration within Scalasca. Keywords:Parallel/distributed systems, performance measurement & analysis tools, application tracing & profiling. ...|$|R
40|$|Abstract—We {{design and}} {{implement}} distributed-memory par-allel algorithms for computing maximal cardinality matching in a bipartite graph. Relying on matrix algebra building blocks, our algorithms expose {{a higher degree}} of parallelism on distributed-memory platforms than existing graph-based algorithms. In contrast to existing parallel algorithms, empirical approximation ratios of the new algorithms are insensitive to concurrency and stay relatively constant with increasing <b>processor</b> <b>counts.</b> On real instances, our algorithms achieve up to 300 × speedup on 1024 cores of a Cray XC 30 supercomputer. Even higher speedups are obtained on larger synthetically generated graphs where our algorithms show good scaling on up to 16, 384 processors. I...|$|R
40|$|This paper {{presents}} a System-on-a-Chip design methodology {{that uses a}} microprocessor subsystem as a building block {{for the development of}} chips for networking applications. The microprocessor subsystem is a self-contained macro that functions as an accelerator for computation-intensive pieces of the application code, and complements the standard components of the SoC. It consists of processor cores, memory banks, and welldefined interfaces that are interconnected via a highperformance switch. The number of processors and memory banks are parameters that can vary depending on the application to be implemented on the chip. Applications such as protocol conversion, TCP/IP offload engine, or firewalls can be implemented with <b>processor</b> <b>counts</b> ranging from 8 to 128...|$|R
40|$|AbstractWe {{present a}} new method for parallelization of {{adaptive}} mesh refinement called Concurrent Structured Adaptive Mesh Refinement (CSAMR). This new method offers the lower computational cost (i. e. wall time×processor count) of subcycling in time, {{but with the}} runtime performance (i. e. smaller wall time) of evolving all levels at once using the time step of the finest level (which does more work than subcycling but has less parallelism). We demonstrate our algorithm's effectiveness using an adaptive mesh refinement code, AMSS-NCKU, and show performance on Blue Waters and other high performance clusters. For the class of problem considered in this paper, our algorithm achieves a speedup of 1. 7 – 1. 9 when the <b>processor</b> <b>count</b> for a given AMR run is doubled, consistent with our theoretical predictions...|$|E
40|$|Abstract — In {{parallel}} applications communication overheads generally {{increase as}} the <b>processor</b> <b>count</b> increases and in particular, collective communication operations {{can become a}} critical limiting factor in achieving high performance. In this paper we explore a novel technique to boost application performance by dedicating some processors in the system to collective operations. We demonstrate the viability and efficiency of this approach for the Allreduce collective operation on a state-of-the-art cluster. Experimental {{results show that the}} collective latency can be reduced by 30 % and that the communication overhead per processor is also very low, at 1. 6 μs, which represents one order of magnitude higher performance than with conventional implementations. Moreover, results on a large-scale scientific application (POP) show that this approach achieves 15 % higher performance on 640 processors than when using the default collective implementation. I...|$|E
40|$|The {{performance}} of many CFD applications, like the lattice Boltzmann method (LBM), on popular cluster computers can fall {{far short of}} the impressive peak performance numbers. Using a large scale LBM application, we demonstrate the different performance characteristics of modern supercomputers. Classical vector systems (NEC SX 8) still combine excellent performance with a well established optimization approach and can break the TFlops application performance barrier at a very low <b>processor</b> <b>count.</b> Although, modern microprocessors offer impressive peak performance numbers and extensive code tuning has been performed, they only achieve 10 % or less of the vector performance for the LBM application. Clusters try to fill this gap by massive parallelization but network capabilities can impose severe performance restrictions if the problem size is not arbitrarily scalable. Tailored HPC servers like the SGI Altix series can relieve these restrictions. 1...|$|E
40|$|Abstract—Recently in {{electronics}} all {{the portable}} devices realized with low power architectures because of power consumption is main consideration along with performance parameters. This proposed paper presents design {{as well as}} the implementation of a 32 -bit low power pipelined Hybrid processor for real-time applications in embedded systems. The Clock gating technique {{is one of the most}} familiar technique used to reduce dynamic power, hence it is considered in the enhancement to optimize the power of the <b>processor.</b> <b>Counting</b> application is implemented in this architecture. VHDL code is simulated and the functionality of the proposed system architecture is implemented on FPGA and is implemented by using Xilinx tools. The power analysis can be analysed by Xilinx’s Xpower analysis tool...|$|R
40|$|Abstract. We {{present a}} {{methodology}} which allows to derive accurate and simple models which {{are able to}} describe the performance of parallel applications without looking at the source code. A trace is obtained and linear models are derived by fitting {{the outcome of a}} set of simulations varying the influential parameters, such as: processor speed, network latency or bandwidth. The simplicity of the linear models allows for natural derivation of interpretations for the corresponding factors of the model, allowing for both prediction accuracy and interpretability to be maintained. We explain how we plan to extend this approach to extrapolate from these models to be apply it to predict for <b>processor</b> <b>counts</b> different to the one of the given traces. ...|$|R
50|$|These {{numbers are}} NOT the IPC {{value of these}} CPUs. These are the {{theoretical}} possible Floating Point performance.Generally, large of processor register shows how big numbers core of <b>processor</b> can <b>count</b> one time. Number of registers is important too, because they can connect together for a moment with some instructions.|$|R
40|$|We {{present a}} new {{approach}} to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrixmatrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1. 4 TFLOPS on 484 processors (cluster jacquard. nersc. gov) and returns a correct result while one process failure has happened. This represents 65 % of the machine peak efficiency and less than 12 % overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the <b>processor</b> <b>count,</b> the overhead of the fault tolerance drops significantly...|$|E
40|$|We {{investigate}} a new hybrid of sort-first and sort-last approach for parallel polygon rendering, using {{as a target}} platform a cluster of PCs. Unlike previous methods that statically partition the 3 D model and/or the 2 D image, our approach performs dynamic, viewdependent and coordinated partitioning of both the 3 D model and the 2 D image. Using a specific algorithm that follows this approach, we show that it performs better than previous approaches and scales better with both <b>processor</b> <b>count</b> and screen resolution. Overall, our algorithm is able to achieve interactive frame rates with efficiencies of 55. 0 % to 70. 5 % during simulations of a system with 64 PCs. While it does have potential disadvantages in client-side processing and in dynamic data management [...] -which also stem from its dynamic, view-dependent nature [...] -these problems are likely to diminish with technology trends in the future. Keywords: Parallel rendering, cluster computing. 1 Introduction The objective of our research is [...] ...|$|E
40|$|Many applied {{scientific}} domains {{are increasingly}} relying on large-scale parallel computation. Consequently, many large clusters now {{have thousands of}} processors. However, the ideal number of pro-cessors to use for these scientific applications varies with both the input variables and the machine under consideration, and predict-ing this <b>processor</b> <b>count</b> is rarely straightforward. Accurate pre-diction mechanisms would provide many benefits, including im-proving cluster efficiency and identifying system configuration or hardware issues that impede performance. We explore novel regression-based approaches to predict parallel program scalability. We use several program executions on a small subset of the processors to predict execution time on larger num-bers of processors. We compare three different regression-based techniques: one based on execution time only; another that uses per-processor information only; and a third one based on the global critical path. These techniques provide accurate scaling predic-tions, with median prediction errors between 6. 2 % and 17. 3 % for seven applications...|$|E
40|$|Parallel {{algorithms}} for sparse matrix-matrix multiplication typically {{spend most}} of their time on inter-processor communication rather than on computation, and hardware trends predict the relative cost of communication will only increase. Thus, sparse matrix multiplication algorithms must minimize communication costs in order to scale to large <b>processor</b> <b>counts.</b> In this paper, we consider multiplying sparse matrices corresponding to Erdős-Rényi random graphs on distributedmemory parallel machines. We prove a new lower bound on the expected communication cost for a wide class of algorithms. Our analysis of existing algorithms shows that, while some are optimal for a limited range of matrix density and number of processors, none is optimal in general. We obtain two new parallel algorithms and prove that they match the expected communication cost lower bound, and hence they are optimal. We acknowledge funding from Microsoft (Award # 024263...|$|R
40|$|Abstract. Reactor {{simulation}} {{depends on}} the coupled solution of various physics types, including neutronics, thermal/hydraulics, and structural mechanics. This paper describes the formulation and implementation of a parallel solution coupling capability being developed for reactor simulation. The coupling process consists of mesh and coupler initialization, point location, field interpolation, and field normalization. This capability is tested on two example problems; {{one of them is}} a reflector assembly from the ABTR. Performance of this coupler in parallel is reasonable for the problem size chosen and the chosen range of <b>processor</b> <b>counts.</b> The runtime is dominated by startup costs, which amortize over the entire coupled simulation. Future efforts will include adding more sophisticated interpolation and normalization methods, to accommodate different numerical solvers used in various physics modules and to obtain better conservation properties for certain field types. 1...|$|R
40|$|High-performance {{computing}} systems {{continue to}} employ {{more and more}} processor cores. Current typical high-end machines in industry, university, and government research laboratory computing centers feature thousands of computing cores. While these machines promise ever more compute power and memory capacity to tackle today's complex simulation problems, they force application developers to greatly enhance the scalability of their codes {{to be able to}} exploit it. To better support them in their porting and tuning process, many parallel-tools research groups have already started to work on scaling their methods, techniques, and tools to extreme <b>processor</b> <b>counts.</b> In this paper, we survey existing profiling and tracing tools, report on our experience in using them in extreme scaling environments, review working and promising new methods and techniques, and discuss strategies for solving open issues and problems. Copyright (C) 2010 John Wiley & Sons, Ltd...|$|R
