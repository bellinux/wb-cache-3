2293|622|Public
5|$|New {{information}} presents {{improvements in}} multithreading, resilency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache <b>prefetching,</b> and data access hints).|$|E
25|$|Swapping in {{of memory}} pages and system cache include <b>prefetching</b> and clustering, to improve performance.|$|E
25|$|Like {{most major}} web browsers, Chrome uses DNS <b>prefetching</b> {{to speed up}} website lookups, as do other browsers like Firefox, Safari, Internet Explorer (called DNS Pre-resolution), and in Opera as a UserScript (not built-in).|$|E
40|$|A new {{conceptual}} cache, NRP (Non-Referenced <b>Prefetch)</b> cache, {{is proposed}} to improve the performance of instruction <b>prefetch</b> mechanisms which try to <b>prefetch</b> both the sequential and non-sequential blocks under the limited memory bandwidth. The NRP cache is used in storing <b>prefetched</b> blocks which were not referenced by the CPU, while these blocks were discarded in other previous <b>prefetch</b> mechanisms. By storing the nonreferenced <b>prefetch</b> blocks in the NRP cache, both cache misses and memory traffics are reduced. A <b>prefetch</b> method to <b>prefetch</b> both the sequential and the non-sequential instruction paths is designed to utilize {{the effectiveness of the}} NRP cache. The results from trace-driven simulation show that this approach provides an improvement in memory access time than other <b>prefetch</b> methods. Particularly, the NRP cache is more effective in a lookahead <b>prefetch</b> mechanism that can hide longer memory latency. Also, the NRP cache reduces 50 % ¸ 112 % of the additional memory traffics requi [...] ...|$|R
40|$|Existing DRAM {{controllers}} employ rigid, non-adaptive {{scheduling and}} buffer management policies when servicing <b>prefetch</b> requests. Some controllers treat <b>prefetch</b> requests {{the same as}} demand requests, others always prioritize demand requests over <b>prefetch</b> requests. However, none of these rigid policies result in the best performance {{because they do not}} take into account the usefulness of <b>prefetch</b> requests. If <b>prefetch</b> requests are useless, treating <b>prefetches</b> and demands equally can lead to significant performance loss and extra bandwidth consumption. In contrast, if <b>prefetch</b> requests are useful, prioritizing demands over <b>prefetches</b> can hurt performance by reducing DRAM throughput and delaying the service of useful requests. This paper proposes a new low-cost memory controller, called Prefetch-Aware DRAM Controller (PADC), that aims to maximize the benefit of useful <b>prefetches</b> and minimize the harm caused by useless <b>prefetches.</b> To accomplish this, PADC estimates the usefulness of <b>prefetch</b> requests and dynamically adapts its scheduling and buffer management policies based on the estimates. The key idea is to 1) adaptively prioritize between demand and <b>prefetch</b> requests, and 2) drop useless <b>prefetches</b> to free up memory system resources, based on the accuracy of the prefetcher. Our evaluation shows that PADC significantly outperforms previous memory controllers with rigid <b>prefetch</b> handling policies. Across a wide range of multiprogrammed SPEC CPU 2000 / 2006 workloads, it improves system performance by 8. 2 % on a 4 -core system and by 9. 9 % on an 8 -cor...|$|R
50|$|A second myth is {{that the}} user should delete the <b>prefetch</b> folder {{contents}} {{to speed up the}} computer. If this is done, Windows will need to re-create all the <b>prefetch</b> files again, thereby slowing down Windows during boot and program starts until the <b>prefetch</b> files are created—unless the prefetcher is disabled.Windows maintains <b>prefetch</b> files in the <b>Prefetch</b> folder for up to the 128 most recently launched programs.|$|R
25|$|Another {{source of}} {{improved}} performance is in microarchitecture techniques exploiting {{the growth of}} available transistor count. Out-of-order execution and on-chip caching and <b>prefetching</b> reduce the memory latency bottleneck {{at the expense of}} using more transistors and increasing the processor complexity. These increases are described empirically by Pollack's Rule, which states that performance increases due to microarchitecture techniques are square root of the number of transistors or the area of a processor.|$|E
25|$|MIPS IV is {{the fourth}} version of the architecture. It is a superset of MIPS III and is {{compatible}} with all existing versions of MIPS. MIPS IV was designed to mainly improve floating-point (FP) performance. To improve access to operands, an indexed addressing mode (base + index, both sourced from GPRs) for FP loads and stores was added, as were prefetch instructions for performing memory <b>prefetching</b> and specifying cache hints (these supported both the base + offset and base + index addressing modes).|$|E
25|$|While the single-tasking DOS had {{provisions}} for multi-sector reads and track blocking/deblocking, {{the operating system}} and the traditional PC hard disk architecture (only one outstanding input/output request {{at a time and}} no DMA transfers) originally did not contain mechanisms which could alleviate fragmentation by asynchronously <b>prefetching</b> next data while the application was processing the previous chunks. Such features became available later. Later DOS versions also provided built-in support for look-ahead sector buffering and came with dynamically loadable disk caching programs working on physical or logical sector level, often utilizing EMS or XMS memory and sometimes providing adaptive caching strategies or even run in protected mode through DPMS or Cloaking to increase performance by gaining direct access to the cached data in linear memory rather than through conventional DOS APIs.|$|E
50|$|Accuracy is the {{fraction}} of total <b>prefetches</b> that were useful - i.e. {{the ratio of the}} number of memory addresses <b>prefetched</b> were actually referenced by the program to the total <b>prefetches</b> done.|$|R
5000|$|AVX-512 <b>Prefetch</b> Instructions (PF) new <b>prefetch</b> capabilities, {{supported}} by Knights Landing ...|$|R
25|$|Another {{benefit is}} its <b>prefetch</b> buffer, which is 8-burst-deep. In contrast, the <b>prefetch</b> buffer of DDR2 is 4-burst-deep, and the <b>prefetch</b> buffer of DDR is 2-burst-deep. This {{advantage}} is an enabling technology in DDR3's transfer speed.|$|R
25|$|All {{applications}} {{have seen}} bugfixes, feature additions and user interface improvements. Dolphin now supports previews of files in toolbars and {{has gained a}} slider to zoom in and out on file item views. It can now also show the full path in the breadcrumb bar. Konqueror offers increased loading speed by <b>prefetching</b> domain name data in KHTML. A find-as-you-type bar improves navigation in webpages. KMail has a new message header list, and reworked attachment view. The KWrite and Kate text editors can now operate in Vi input mode, accommodating those used to the traditional UNIX editor. Ark, the archiving tool has gained support for password-protected archives and is accessible via a context menu from the file managers now. KRDC, the remote desktop client improves support for Microsoft’s Active Directory through LDAP. Kontact has gained a new planner summary and support for drag and drop in the free/busy view. KSnapshot now uses the window title when saving screenshots, {{making it easier to}} index them using search engines.|$|E
5000|$|Data <b>prefetching</b> fetches data {{before it}} is needed. Because data access {{patterns}} show less regularity than instruction patterns, accurate data <b>prefetching</b> is generally more challenging than instruction <b>prefetching.</b>|$|E
5000|$|Hardware <b>prefetching</b> {{also has}} less CPU {{overhead}} {{when compared to}} software <b>prefetching.</b>|$|E
40|$|The paper explores how the {{structure}} of Webspace {{can be used for}} accelerated Web <b>prefetch.</b> We have conducted experiments based on a novel hyperspace aware <b>prefetch</b> proxy and have studied the <b>prefetch</b> performance on several dominant hyperspace patterns. The study assesses the system's responsiveness and background loads for various user interaction duration, surfing and <b>prefetch</b> sequences. This paper also draws attention to the emergence of some regular patterns in contemporary webspace. The results show that webspace awareness can help in improving <b>prefetch</b> performance. This study also provides an interesting insight toward a framework where the professional content developers can gain more control towards authoring <b>prefetch</b> friendly collection for increased site responsiveness...|$|R
5000|$|A {{software}} <b>prefetch</b> {{instruction is}} used as a type of data <b>prefetch.</b> This <b>prefetch</b> increases the chances for a cache hit for loads, and can indicate the degree of temporal locality needed in various levels of the cache.|$|R
40|$|The paper {{presents}} an interesting study that how the user surfing behavior {{with respect to}} the organization of a web space affects the performance of a <b>prefetch</b> enabled proxy. We have conducted experiments based on a novel hyperspace aware <b>prefetch</b> proxy and have studied the <b>prefetch</b> performance on several dominant hyperspace patterns including chain, tree, and complete graph sub-structures. The study assesses the system’s responsiveness as well as the background loads for various user interaction duration, surfing and <b>prefetch</b> sequences. The results show that awareness about the web space and surfing sequence can greatly help in improving <b>prefetch</b> performance. Schemes such as these also conversely may enable professional content developers to create <b>prefetch</b> friendly collection for increased site responsiveness...|$|R
5000|$|While {{software}} <b>prefetching</b> requires programmer or compiler intervention, hardware <b>prefetching</b> requires special hardware mechanisms.|$|E
5000|$|AVX-512 {{prefetch}} instructions contain new prefetch {{operations for}} the new scatter and gather functionality introduced in AVX2 and AVX-512. [...] prefetch means <b>prefetching</b> into level 1 cache and [...] means <b>prefetching</b> into level 2 cache.|$|E
5000|$|Another {{pattern of}} <b>prefetching</b> {{instructions}} is to prefetch addresses that are [...] addresses {{ahead in the}} sequence. It is mainly used when the consecutive blocks {{that are to be}} prefetched are [...] addresses apart. This is termed as Strided <b>Prefetching.</b>|$|E
50|$|The {{objective}} of the <b>prefetch</b> procedure is to <b>prefetch</b> data from NAND based on a given prediction graph such that most data accesses occur over SRAM. The basic idea is to <b>prefetch</b> data by following the LBA order in the graph. In order to efficiently look up a selected page in the cache, a cyclic queue is adopted in the cache management. Data prefethced from NAND flash is enqueued, while those transferred to the host is dequeued, on the other hand. The <b>prefetch</b> procedure is done in a greedy way: Let P1 be the last <b>prefetched</b> page. If P1 corresponds to a regular node, then the page that corresponds to the subsequent LBA is <b>prefetched.</b> If P1 corresponds to a branch node, then the procedure should <b>prefetch</b> pages by following all possible next LBA links in an equal base and a round-robin way.|$|R
40|$|Many modern {{high-performance}} processors <b>prefetch</b> blocks {{into the}} on-chip cache. <b>Prefetched</b> blocks can potentially pollute the cache by evicting more useful blocks. In this work, we observe that both accurate and inaccurate <b>prefetches</b> lead to cache pollution, and propose a comprehensive mechanism to mitigate prefetcher-caused cache pollution. First, we observe that over 95 % of useful <b>prefetches</b> {{in a wide}} variety of applications are not reused after the first demand hit (in secondary caches). Based on this observation, our first mechanism simply demotes a <b>prefetched</b> block to the lowest priority on a demand hit. Second, to address pollution caused by inaccurate <b>prefetches,</b> we propose a self-tuning <b>prefetch</b> accuracy predictor to predict if a <b>prefetch</b> is accurate or inaccurate. Only predicted-accurate <b>prefetches</b> are inserted into the cache with a high priority. Evaluations show that our final mechanism, which combines these two ideas, significantly improves performance compared to both the baseline LRU policy and two state-of-the-art approaches to mitigating prefetcher-caused cache pollution (up to 49 %, and 6 % on average for 157 two-core multiprogrammed work-loads). The performance improvement is consistent across a wide variety of system configurations...|$|R
40|$|Abstract—In {{order to}} bridge the gap of the growing speed {{disparity}} between processors and their memory subsystems, aggressive <b>prefetch</b> mechanisms, either hardware-based or compiler-assisted, are employed to hide memory latencies. As the first-level cache gets smaller in deep submicron processor design for fast cache accesses, data cache pollution caused by overly aggressive <b>prefetch</b> mechanisms will become a major performance concern. Ineffective <b>prefetches</b> not only offset the benefits of benign <b>prefetches</b> due to pollution but also throttle bus bandwidth, leading to an overall performance degradation. In this paper, we propose and analyze a number of hardware-based <b>prefetch</b> pollution filtering mechanisms to differentiate good and bad <b>prefetches</b> dynamically based on history information. We designed three <b>prefetch</b> pollution filters organized as a one-level, two-level, or gshare style. In addition, we examine two table indexing schemes: Per-Address (PA) based and Program Counter (PC) based. Our <b>prefetch</b> pollution filters work in tandem with both hardware and software prefetchers. As our analysis shows, the cache pollution filters can reduce the ineffective <b>prefetches</b> by more than 90 percent and alleviate the excessive memory bandwidth induced by them. Also, the performance can be improved by up to 16 percent when our filtering mechanism is incorporated with aggressive <b>prefetch</b> filters as a result of reduced cache pollution and less competition for the limited number of cache ports. In addition, a number of sensitivity studies are performed to provide more understandings of the <b>prefetch</b> pollution filter design. Index Terms—Prefetch, memory subsystems, microarchitecture. Ç...|$|R
5000|$|Whenever the {{prefetch}} mechanism detects a miss on {{a memory}} block, say A, it allocates a stream to begin <b>prefetching</b> successive {{blocks from the}} missed block onward. If the stream buffer can hold 4 blocks, then we would prefetch A+1, A+2, A+3, A+4 and hold those in the allocated stream buffer. If the processor consumes A+1 next, then it shall be moved [...] "up" [...] from the stream buffer to the processor's cache. The first entry of the stream buffer would now be A+2 and so on. This pattern of <b>prefetching</b> successive blocks is called Sequential <b>Prefetching.</b> It is mainly used when contiguous locations are to be prefetched. For example, it is used when <b>prefetching</b> instructions.|$|E
5000|$|Instruction <b>prefetching</b> fetches {{instructions}} {{before they}} need to be executed. The first mainstream microprocessors to use some form of instruction prefetch were the Intel 8086 (six bytes) and the Motorola 68000 (four bytes). In recent years, all high-performance processors use <b>prefetching</b> techniques.|$|E
50|$|FasterIE - Increases surfing speed {{by using}} <b>prefetching.</b>|$|E
50|$|It is {{possible}} for implementations to <b>prefetch</b> links {{even when they are}} not specified as <b>prefetch</b> links.|$|R
2500|$|The DDR4 SDRAM uses a 8n <b>prefetch</b> {{architecture}} {{to achieve}} high-speed operation. The 8n <b>prefetch</b> architecture {{is combined with}} ...|$|R
40|$|International audienceThe Best-Offset (BO) {{prefetcher}} {{submitted to}} the DPC 2 contest <b>prefetches</b> one line into the level-two (L 2) cache on every cache miss or hit on a <b>prefetched</b> line. The <b>prefetch</b> line address is generated by adding an offset to the demand access address. The BO prefetcher tries to find automatically an offset value that yields timely <b>prefetches</b> with the highest possible coverage and accuracy. It evaluates an offset value by maintaining a table of recent requests addresses and by searching these addresses {{to determine whether the}} line currently requested would have been <b>prefetched</b> in time with that offset...|$|R
50|$|Mozilla Firefox {{supports}} DNS <b>prefetching,</b> as of version 3.5.|$|E
5000|$|Better files <b>prefetching</b> {{which allows}} {{to speed up}} {{compression}} ...|$|E
5000|$|Pipelined {{processing}} of short data streams using data <b>prefetching</b> ...|$|E
40|$|Recently, {{techniques}} for the optimization of microarchitecture and compilers have progressed significantly. However, sometimes these optimizations cause failure in <b>prefetch</b> detection. In general, to generate <b>prefetch</b> requests, <b>prefetch</b> algorithms use (a) data addresses, (b) memory access orderings, and (c) instruction addresses. However, (b) and (c) are often scrambled by optimizations. In this paper, we propose a new optimization-friendly memory-side <b>prefetch</b> algorithm: Access Map Pattern Matching <b>prefetch.</b> In this algorithm, coarse-grained ordering information – recent-zone-access frequency (denoted by (d)) – {{is used instead}} of (b) and (c). The AMPM prefetcher holds the memory access pattern of recent memory access requests and generates <b>prefetch</b> requests by pattern matching. We evaluate the AMPM prefetcher in a DPC framework. The simulation results show that our prefetcher improves performance by 53 %. 1...|$|R
30|$|In [33], {{the authors}} model the 4 -byte-long <b>prefetch</b> queue of an Intel 80188. Even for this simple <b>prefetch</b> queue, {{the authors have}} to perform some {{simplifications}} in their approach to handle the resulting complexity due to {{the interaction between the}} instruction execution and the instruction <b>prefetch</b> (the consuming and the producing ends of the queue).|$|R
40|$|This paper proposes and evaluates {{hardware}} {{mechanisms for}} supporting prescient instruction <b>prefetch</b> [...] -an approach to improving single-threaded application performance by using helper threads to perform instruction <b>prefetch.</b> We demonstrate {{the need for}} enabling store-to-load communication and selective instruction execution when directly pre-executing future regions of an application that suffer I-cache misses. Two novel hardware mechanisms, safe-store and YAT-bits, are introduced that help satisfy these requirements. This paper also proposes and evaluates finite state machine recall, a technique for limiting pre-execution to branches {{that are hard to}} predict by leveraging a counted I-prefetch mechanism. On a research Itanium SMT processor with next line and streaming I-prefetch mechanisms that incurs latencies representative of next generation processors, prescient instruction <b>prefetch</b> can improve performance by an average of 10. 0 % to 22 % on a set of SPEC 2000 benchmarks that suffer significant I-cache misses. Prescient instruction <b>prefetch</b> is found to be competitive against even the most aggressive research hardware instruction <b>prefetch</b> technique: fetch directed instruction <b>prefetch...</b>|$|R
