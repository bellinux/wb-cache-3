27|10000|Public
500|$|Jenna accidentally burns Kenneth's page {{jacket on}} a hot plate, and Kenneth worries that head page Donny Lawson (Paul Scheer) will punish him. Jenna finds Donny {{backstage}} at the studio, who is ecstatic that he finally has a reason to send Kenneth to CNBC in New Jersey. Donny offers Kenneth a choice: go to New Jersey, or compete in a [...] "page off", a contest of physical stamina and NBC trivia; Jenna agrees to the page off. Before the event starts, Pete comes in and yells at the <b>pages</b> <b>to</b> <b>get</b> back to work. He forces Donny to give Kenneth a new jacket, but Donny swears to Jenna and Kenneth that he will get back at them.|$|E
2500|$|The highest praise I {{can give}} this novel {{is that it}} reminds me {{strongly}} of Charles Williams, but it succeeds where Williams always failed: it has believable characters. Not at first, however. It takes Ms. Carl about fifty to seventy-five <b>pages</b> <b>to</b> <b>get</b> into her stride with this story. Until then, the characters feel stiff and wooden and contrived.|$|E
50|$|The {{following}} are web-based live chat applications, which enable website visitors {{to chat with}} the sales or support people of the website in real time. Webmasters only need to paste a piece of code onto the web <b>pages</b> <b>to</b> <b>get</b> them working.|$|E
50|$|A {{list of all}} {{course in}} which the student is {{registered}} is displayed on My Courses Page. The student goes through this <b>page</b> <b>to</b> <b>get</b> <b>to</b> her/his course homepages.|$|R
50|$|Charlene Teters and the NCRSM have {{listed the}} {{following}} schools and teams {{on their home}} <b>page</b> <b>to</b> <b>get</b> certain universities and institutions in America {{to realize that the}} use of indigenous people is wrong as mascots.|$|R
6000|$|The <b>page</b> ran <b>to</b> <b>get</b> the armour; {{but it was}} so uncommonly {{hot that}} he dropped it, and put his fingers in his mouth, crying! ...|$|R
5000|$|Facebook {{offers an}} {{advertising}} tool for <b>pages</b> <b>to</b> <b>get</b> more [...] "likes". According to Business Insider, this advertising tool is called [...] "Suggested Posts" [...] or [...] "Suggested Pages", allowing companies to market their page {{to thousands of}} new users {{for as little as}} US$50.|$|E
5000|$|One critic found Ripley Under Water typical Highsmith: [...] "No flashy or fashionable {{effects are}} allowed to {{interrupt}} the flow of a Highsmith narrative, which often appears to be eventful even when nothing is happening." [...] This novel [...] "takes about 100 <b>pages</b> <b>to</b> <b>get</b> going, and when it does, the pace, paradoxically, seems to slacken." ...|$|E
50|$|It takes five <b>pages</b> <b>to</b> <b>get</b> to Dresden, {{the protagonist}} having joined the RAF as {{an escape from}} rural and then urban poverty. In Carson {{everything}} {{is rooted in the}} everyday, so the destruction of Dresden evokes memories of a particular Dresden shepherdess he had on the mantelpiece as a child and the destruction is described in terms of 'an avalanche of porcelain, sluicing and cascading'.|$|E
6000|$|Luc. [To HIPPOLITA.] Be sure, if the <b>page</b> {{approaches}} you, <b>to</b> <b>get</b> {{out of him}} his master's name. [...] [The Prince and LUCRETIA seem to talk.|$|R
5000|$|In 2014, WeChat {{announced}} {{that according to}} [...] "related regulations", domains of the web <b>pages</b> that want <b>to</b> <b>get</b> shared in WeChat Moments need <b>to</b> <b>get</b> an Internet Content Provider (ICP) licence by Dec 31, 2014 to avoid being restricted by WeChat.|$|R
5000|$|USA Swimming {{decided to}} allow hand signals at the U.S. Olympic trials, thanks to Titus and his fan base, to {{accommodate}} deaf swimmers. Titus, upon hearing USA Swimming {{would not allow}} hand signals to comply with international rules, talked to US Swimming through an attorney. When this didn't work, Titus set up a Facebook <b>page</b> <b>to</b> <b>get</b> fans <b>to</b> e-mail USA swimming officials. Before, they only used strobe lights, which is good for deaf and hearing-impaired swimmers {{to know when to}} start the race, but not any other cue. Titus took this as a form of discrimination against deaf and hearing-impaired swimmers. While other swimming organizations used hand signals to accommodate those who are hearing-impaired, USA Swimming did not. Titus, backed up by U.S. Deaf Swimming and USA Deaf Sports Federation, finally got USA Swimming to reverse this decision in July, 2012 [...]|$|R
50|$|The {{original}} Guardians of the Globe was {{a private}} organization funded by War Woman and Darkwing. This version of the Guardians was a pastiche of DC Comics' Justice League of America (JLA), {{with each of the}} Guardians (with the exception of Black Samson) having an obvious JLA analogue. Robert Kirkman, in his author notes for Invincible: The Ultimate Collection, Volume 1, explains that he went with these archetypes because he had only 18 <b>pages</b> <b>to</b> <b>get</b> the reader to care for the characters before their brutal deaths.|$|E
50|$|With {{the advent}} of the World Wide Web, mouse {{tracking}} was expanded to include click data. Researchers and developers would track and record each time a user used the mouse to click something on the website, as well as the location of the event. Web developers use these mouse clicks to assess what information users are interested in and how they interact with a page. Additionally, advertisers are interested in click data in terms of banner advertisements and where to place their ads on <b>pages</b> <b>to</b> <b>get</b> the most click-throughs.|$|E
5000|$|Jenna accidentally burns Kenneth's page {{jacket on}} a hot plate, and Kenneth worries that head page Donny Lawson (Paul Scheer) will punish him. Jenna finds Donny {{backstage}} at the studio, who is ecstatic that he finally has a reason to send Kenneth to CNBC in New Jersey. Donny offers Kenneth a choice: go to New Jersey, or compete in a [...] "page off", a contest of physical stamina and NBC trivia; Jenna agrees to the page off. Before the event starts, Pete comes in and yells at the <b>pages</b> <b>to</b> <b>get</b> back to work. He forces Donny to give Kenneth a new jacket, but Donny swears to Jenna and Kenneth that he will get back at them.|$|E
50|$|In 2010, Wertheim UK {{launched}} {{a range of}} new machines to the UK market place - http://www.wertheim.uk.com, They are no longer sold in the UK with the website taking you <b>to</b> a landing <b>page</b> advising where <b>to</b> <b>get</b> service.|$|R
30|$|The module {{web crawler}} takes the keyword set by set and issues queries via the Google API. The {{returned}} search {{results are in}} JSON format. Each item contains three properties of the webpage. They are “title,” “snippet,” and “address.” Both the title and snippet can contain a question or parts of a question, so the contents of these two properties are stored as candidates of the final returned questions. The program then goes to the actual web <b>page</b> <b>to</b> <b>get</b> the content in HTML format by the link provided in “address.” The questions may be deeply embedded in the content, so we wrote a parser in terms of HTML tags to extract the possible text and store them as the list of candidates. Specifically, the parser considered the content of every HTML tag as one candidate and left the final decisions to the filters.|$|R
50|$|The Winds of War is Herman Wouk's {{second book}} about World War II, the first being The Caine Mutiny (1951). Published in 1971, it was {{followed}} up seven years later by War and Remembrance; originally conceived as one volume, Wouk decided to break it in two when he realized it took nearly 1000 <b>pages</b> just <b>to</b> <b>get</b> <b>to</b> the attack on Pearl Harbor. In 1983, it became a highly successful miniseries on the ABC television network.|$|R
50|$|In Green Man Review Matthew Scott Winslow wrote:Lillian Stewart Carl's latest fantasy novel, Lucifer's Crown, {{effectively}} combines Arthurian legend, Grail myth, and British folkways {{to create}} a powerful novel. The highest praise I can give this novel is that it reminds me strongly of Charles Williams, but it succeeds where Williams always failed: it has believable characters. Not at first, however. It takes Ms. Carl about fifty to seventy-five <b>pages</b> <b>to</b> <b>get</b> into her stride with this story. Until then, the characters feel stiff and wooden and contrived. Also much like Williams is the theological and philosophical subtext. Ms. Carl takes the ideas of good vs. evil quite seriously and probes deeply into the idea of redemption. She does not, however, take her themes lightly, instead giving them a vigorous shaking down before she's done, resulting in a gripping spiritual thriller. One could easily call this 'in the tradition of Charles Williams' — which it certainly is — but it more importantly moves beyond that master of the spiritual thriller.|$|E
50|$|In a {{disaster}} or emergency situation, {{there is a}} need for hospitals to be able to communicate with each other, and with other members of the emergency response community. The ability to exchange data in regard to hospitals’ bed availability, status, services, and capacity enables both hospitals and other emergency agencies to respond to emergencies and disaster situations with greater efficiency and speed. In particular, it will allow emergency dispatchers and managers to make sound logistics decisions - where to route victims, which hospitals have the ability to provide the needed service. Many hospitals have expressed the need for, and indeed are currently using, commercial or self-developed information technology that allows them to publish this information to other hospitals in a region, as well as EOCs, 9-1-1 centers, and EMS responders via a Web-based tool. Systems that are available today do not record or present data in a standardized format, creating a serious barrier to data sharing between hospitals and emergency response groups. Without data standards, parties of various kinds are unable to view data from hospitals in a state or region that use a different system - unless a specialized interface is developed. Alternatively, such officials must get special passwords and toggle between web <b>pages</b> <b>to</b> <b>get</b> a full picture. Other local emergency responders are unable to get the data imported into the emergency IT tools they use (e.g. a 9-1-1 computer-aided dispatch system or an EOC consequence information management system). They too must get a pass word and go to the appropriate web page. This is very inefficient. A uniform data standard will allow different applications and systems to communicate seamlessly.|$|E
40|$|In this paper, {{we present}} a novel {{backward}} transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web <b>pages.</b> <b>To</b> <b>get</b> a better precision during the re-ranking process, a variety of web-based in...|$|E
40|$|Abstract Danmarks Rockmuseum (The Danish Rock Museum) has {{launched}} a beta version of what hopefully soon shall be a finished homepage, called Rockensdanmarkskort. dk. The site allows private people to upload stories or experiences from their own life in the Danish music environment. The stories will {{be a part of}} an online exhibition but the site is also {{to be seen as a}} community for the persons sharing their stories. This study investigates how to tell what the exact purpose from Danmarks Rockmuseum has been with this homepage and what the potential users will think of such a <b>page.</b> <b>To</b> <b>get</b> there I have used qualitative method by making interviews with both Danmarks Rockmuseum and a group of people that I argue is to be seen as the potential users. In an attempt to see the potential users as more than individuals, I have used Michel Maffesolis theory of neo tribalism and tried to regard the potential users as a tribe, based on the site Rockensdanmarkskort. dk...|$|R
40|$|This PDF is {{designed}} to be read onscreen, two pages at a time. If you want to print a copy, your PDF viewer should have an option for printing two pages on one sheet of paper, but you may need <b>to</b> start with <b>page</b> 2 <b>to</b> <b>get</b> it <b>to</b> print facing <b>pages</b> correctly. (Print this cover page separately.) Alternatively, you can download a free PDF of the printed edition or buy a low-cost printed copy fro...|$|R
5000|$|While applauding {{the tone}} and setting of the novel, some reviews cited the shallowness of the characterizations. The Guardian and Chicago Tribune reviews observe that Nella is drawn more like a worldly, feminist 21st-century girl than a naïve 17th-century one. The Chicago Tribune adds: [...] "main {{characters}} are complex and complicated and suffer terrible tragedies, but Burton doesn't give us a deep enough look into their psyches. I'd read 100 additional <b>pages</b> just <b>to</b> <b>get</b> inside Johannes' head".|$|R
40|$|In {{this paper}} we {{proposed}} a new approach called Extended Fivatech from the problem of multiple input pages. Extended FivaTech can reduce the schema and templates for each individual web page generated from a CGI program. It also merge the multiple input pages into the single page to generate the dynamic web pages Extended FiaTech uses tree templates. Extended FivaTech reduce the schema templates for each individual Deep web site which contains either singleton or multiple data records in one Web page and merging of web pages into a single page. Extended FivaTech applies tree merging, tree alignment and mining techniques, merging of the input <b>pages</b> <b>to</b> <b>get</b> the outstanding task. These experiments show a good result for the web pages used in many web data extractions. 1...|$|E
40|$|With the {{exponentially}} {{growing amount}} of information available on the Internet, an effective technique for users to discern the useful information from the unnecessary information is urgently required. Cleaning web pages for web data extraction becomes critical for improving performance of information retrieval and information extraction. So, we investigate to remove various noise patterns in Web pages instead of extracting relevant content from Web <b>pages</b> <b>to</b> <b>get</b> main content information. To solve this problem, we put forward an extracting main content method which firstly removes the usual noise and the candidate nodes without any main content information from web pages, and makes use of the relation of content text length, the length of anchor text {{and the number of}} punctuation marks to extract the main content. In this paper, we focus on removing noise and utilization of all kinds of content-characteristics, experiments show that this approach can enhance the universality and accuracy in extracting the body text of web page...|$|E
40|$|Abstract- As {{the size}} of the World Wide Web has grown largely, it has become {{difficult}} to retrieve useful information quickly. User looking for information may have to browse lots of <b>pages</b> <b>to</b> <b>get</b> the desired information from the pool of World Wide Web (WWW). A technique is required which organizes documents content efficiently so that information can be easily obtained from largest data repository of WWW. Clustering is an unsupervised classification technique which puts related data in one set (Cluster). Clustering can help user to get interested information quickly from these abundance of information. However clustering methods are suffered from the huge size of documents with the high dimensionality of text features. We have proposed web page clustering scheme that works efficiently in higher dimension. We have presented the method to reduce the dimensionality of the feature vector by selecting the most informative words and still maintaining the quality of the clusters...|$|E
5000|$|Roberts' {{drug and}} alcohol use {{increased}} after the film was released. In 2004, Roberts faced a charge of [...] "causing unnecessary suffering" [...] after his snake, [...] "Damien", was allowed to starve to death in the garage of his London Colney home. In 2007, WWE started a policy that they would pay all expenses for any former WWWF/WWF/WWE performer who needed to enter into any form of drug rehabilitation. According to various wrestling news reports, as well as his own MySpace page, Roberts was placed in a 14-week voluntary rehab program by WWE as of December 10, 2007. In May 2008, Jim Ross reported that, [...] "Jake Roberts has been doing well the past few weeks, after completing a treatment program." [...] Roberts continues to wrestle on the independent circuit, and in 2012 moved in with fellow wrestler Diamond Dallas <b>Page</b> <b>to</b> <b>get</b> help on getting his life back on track. In 2013, Scott Hall joined Roberts' rehabilitative efforts by also moving into Page's home, which has been nicknamed the [...] "accountability crib".; this is documented in the film [...] "The Resurrection of Jake the Snake." ...|$|R
30|$|In {{order to}} rigorously study at-a-glance web page categorization, we need {{properly}} labeled web <b>pages.</b> One way <b>to</b> <b>get</b> labels is <b>to</b> do a categorization experiment with unlimited viewing time. In other words, {{we need to}} collect a corpus of web pages, define a set of unambiguous category labels, and confirm that observers agree on the ground truth labeling of those web pages’ categories.|$|R
5000|$|Despite {{being well}} received, {{the end result}} was deemed insufficiently {{commercial}} by the parties that commissioned it, and it floated around Hollywood until being discovered by independent producer Mary Ann Page. Enthusiastic about the script, originally sent {{to her as a}} writing sample, <b>Page</b> tried <b>to</b> <b>get</b> the project off the ground {{for three and a half}} years. The film was briefly set up at Universal Studios, during which Brian Gilbert was attached as director. In 1988, Michael Klesic was originally cast in the role of Henry Evans. [...] The film was soon after put on hold due to a lack of funding.|$|R
40|$|Cleaning Web pages before mining becomes {{critical}} for improving performance of information retrieval and information extraction. With the exponentially growing {{amount of information}} available on the Internet, an effective technique for users to discern the useful information from the unnecessary information is urgently required. So, we investigate to remove various noisy data patterns in Web pages instead of extracting relevant content from Web <b>pages</b> <b>to</b> <b>get</b> main content information. In this paper, we propose an approachNoiseEliminator that detect multiple noise patterns and remove these noise patterns from Web pages of any Web sites. Our approach {{is based on the}} basic idea of Case-Based Reasoning (CBR) to find noise pattern from mixture (data and noise together) patterns in current Web page by matching similar noise pattern kept in Case-Based. We also apply back propagation neural network algorithm to classify various noise patterns, data patterns and mixture patterns in current Web page. The classification result of neural network is used for removing noise patterns. We have implemented our method on several commercial Web sites and News Web sites to evaluate the performance and improvement of our approach. Experimental results show the effectiveness of the approach...|$|E
40|$|Abstract- The {{semantic}} search {{usually the}} web pages for the required information and filter the pages from semantic web searching unnecessary pages by using advanced algorithms. Web pages are vulnerable in answering intelligent semantic search from the user {{due to the}} confidence of their consequences on information obtainable in web <b>pages.</b> <b>To</b> <b>get</b> the trusted results semantic web search engines require searching for pages that maintain such information at some place including domain knowledge. The layered model of Semantic Web provides {{solution to this problem}} by providing semantic web search based on HMM for optimization of search engines tasks, specialty focusing on how to construct a new model structure to improve the extraction of web pages. We classify the search results using some search engines and some different search keywords provide a significant improvement in search accuracy. Semantic web is segmented from the elicited information of various websites based on their characteristic of semi-structure in order to improve the accuracy and efficiency of the transition matrix. Also, it optimizes the observation probability distribution and the estimation accuracy of state transition sequence by adopting the “voting strategy ” and alter Viterbi algorithm. In this paper, we have presented a hybrid system that includes both hidden Markov models and rich markov model that showed the effectiveness of combining implicit search with rich Markov models for a recommender system...|$|E
40|$|The World Wide Web is a {{rich source}} of {{information}} and continues to expand in size and complexity. To capture the features of the Web at a higher level to realise the information classification and efficient retrieval on the Web is becoming a challenge task. One natural way is to exploit the linkage information among the Web pages. Previous work such as HITS in this area is based on a set of retrieved <b>pages</b> <b>to</b> <b>get</b> a Web community that is a bunch of pages related to the query topics. Since the set of retrieved pages may contain many unrelated pages (noise pages) to the query topics, the obtained Web community sometimes is unsatisfactory. In this paper, we propose an innovative algorithm to eliminate noise pages from the set of retrieved pages and improve its quality. This improvement will enable existing community construction algorithms to construct good quality Web page communities. The proposed algorithm reveals and takes advantage of the relationships among concerned Web pages at a deeper level. The numerical experiment results show the effectiveness and feasibility of the algorithm. This algorithm could also be used solely to filter unnecessary Web pages and reduce the management cost and burden of Web-based data management systems. The ideas in the algorithm can also be applied to other hyperlink analysis...|$|E
50|$|In 1981, {{they started}} working with the adventures of Spirou et Fantasio. This is the title series of Spirou magazine, in {{continuous}} production since 1938 by a succession of authors, most famously André Franquin. The last of those, Jean-Claude Fournier, had left the series {{at the end of}} the 1970s, and the magazine was looking for a solution <b>to</b> <b>get</b> the series back in production. Three teams started making stories, but after a few years only Tome and Janry remained. For them, it was a childhood dream, and they showed their first two pilot <b>pages</b> <b>to</b> Franquin <b>to</b> <b>get</b> some advice. They continued making stories until 1998, when after 14 albums they ended their run.|$|R
2500|$|Josh Brolin {{discussed}} the Coens' directing style in an interview, {{saying that the}} brothers [...] "only really say {{what needs to be}} said. They don't sit there as directors and manipulate you and go into <b>page</b> after <b>page</b> <b>to</b> try <b>to</b> <b>get</b> you <b>to</b> a certain place. They may come in and say one word or two words, so that was nice to be around in order to feed the other thing. 'What should I do right now? I'll just watch Ethan go humming to himself and pacing. Maybe that's what I should do, too.|$|R
50|$|Companies place JavaScript {{files on}} their web servers which {{are linked to}} from a HTML script in the page code. The {{combination}} of the two makes a request from the data collection servers for each page viewed by a user. A cookie is placed on the users computer which contains a unique id. This unique id is passed with the request to the data collection servers. During the processing of the data Hitbox ties together the <b>page</b> views <b>to</b> <b>get</b> a perspective of how the user moved through the site. Other, custom information can be passed in the request as well as information from the HTTP header of the browser, such as referrer and browser details.|$|R
