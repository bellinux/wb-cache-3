48|54|Public
5000|$|Miles Tackett is {{best known}} {{around the world as}} bassist/leader of Breakestra, a funk, soul-jazz band whose records & live shows are a throwback to the <b>pre-sampling</b> era of Bronx DJ performances. [...] "Music Man Miles", as he is known among the {{underground}} DJ crowd, also helms the longest-running funky soul party in Los Angeles spinning an all-vinyl set.|$|E
5000|$|Breakestra is a funk music project {{founded by}} Miles Tackett and based in Los Angeles, California. Breakestra was first formed in 1997 as a {{strictly}} live ensemble playing [...] "covers" [...] of funk, soul, and jazz breaks {{that had been}} sampled in late 1980s & early 1990s hip-hop seamlessly, blended into {{each other in the}} same style that early hip hop DJs would do in the <b>pre-sampling</b> days of the 1970s when they would DJ records at block parties.|$|E
40|$|Abstract Background Since {{it is not}} {{yet clear}} whether it is {{possible}} to satisfactorily avoid sampling-induced stress interference in poultry, more studies on the pattern of physiological response and detailed quantification of stress connected with the first few minutes of capture and <b>pre-sampling</b> handling in poultry are required. This study focused on detection of changes in the corticosterone level and concentrations of other selected biochemical parameters in broilers handled in two different manners during blood sampling (involving catching, carrying, restraint, and blood collection itself) that lasted for various time periods within the interval 30 - 180 seconds. Methods Stress effects of <b>pre-sampling</b> handling were studied in a group (n = 144) of unsexed ROSS 308 broiler chickens aged 42 d. Handling (catching, carrying, restraint, and blood sampling itself) was carried out in a gentle (caught, held and carried carefully in an upright position) or rough (caught by the leg, held and carried with lack of care in inverted position) manner and lasted for 30 s, 60 s, 90 s, 120 s, 150 s, and 180 s. Plasma corticosterone, albumin, glucose, cholesterol, lactate, triglycerides and total protein were measured in order to assess the stress-induced changes to these biochemical indices following handling in the first few minutes of capture. Results <b>Pre-sampling</b> handling in a rough manner resulted in considerably higher plasma concentrations of all biochemical indices monitored when compared with gentle handling. Concentrations of plasma corticosterone after 150 and 180 s of handling were considerably higher (P Conclusions These results indicate that the <b>pre-sampling</b> procedure may be a considerably stressful procedure for broilers, particularly when carried out with lack of care and exceeding 120 seconds. </p...|$|E
40|$|We {{examine the}} neoclassical {{investment}} model using {{a panel of}} U. S. manufacturing firms. The standard model with no financing constraints cannot be rejected for firms with high (<b>pre-sample)</b> dividend payouts. However, it is decisively rejected for firms with low (<b>pre-sample)</b> payouts (firms we expect to face financing constraints). Hem, investment is sensitive to both firm cash flow and macroeconomic credit conditions, holding constant investment opportunities. Sample splits based on firm size or maturity do not produce such distinctions. The latter comparison identifies firms where "free-cash-flow" problems {{might be expected to}} produce correlations between investment and cash flow. ...|$|R
40|$|Over {{the last}} decades, {{computer}} graphics and vision researchers {{have focused on}} developing novel visual effects for computer-generated photo-realistic images. To achieve high-quality output, many state-of-the-art rendering algorithms, which are known as data-driven rendering, pre-process an input three-dimensional scene with complex procedures to obtain necessary data, or pre-capture the real world {{with a set of}} images. The desired visual effects are then recon-structed from the <b>pre-sampled</b> observations for efficient run-time rendering. Nevertheless, with the increasing demand of more and more photo-realistic image synthesis, the amount of <b>pre-sampled</b> data expands accordingly. It not only consumes a great deal of storage space, but also increases data access time and rendering costs at run-time. In order to solve this issue, we can adopt a compact representation to efficiently describe the <b>pre-sampled</b> observations and further apply sophisticated approximation methods {{to reduce the amount of}} data, but achieving real-time performance at the same time is frequently another chal-lenging problem. In this dissertation, we thus focus on data representations and approximation algorithms for real-time rendering of visual data sets. Two novel parametric representations...|$|R
40|$|PHASE is a Monte Carlo event generator, under construction, for all Standard Model {{processes}} {{with six}} fermions {{in the final}} state at the LHC. It employs the full set of tree level Feynman diagrams, taking into account fermion masses for b quarks. The program can generate unweighted events for any subset of all six fermion final states in a single run, by making use of dedicated <b>pre-samples.</b> An interface to hadronization is provided. 1...|$|R
40|$|Background: Since {{it is not}} {{yet clear}} whether it is {{possible}} to satisfactorily avoid sampling-induced stress interference in poultry, more studies on the pattern of physiological response and detailed quantification of stress connected with the first few minutes of capture and <b>pre-sampling</b> handling in poultry are required. This study focused on detection of changes in the corticosterone level and concentrations of other selected biochemical parameters in broilers handled in two different manners during blood sampling (involving catching, carrying, restraint, and blood collection itself) that lasted for various time periods within the interval 30 - 180 seconds. Methods: Stress effects of <b>pre-sampling</b> handling were studied in a group (n = 144) of unsexed ROSS 308 broiler chickens aged 42 d. Handling (catching, carrying, restraint, and blood sampling itself) was carried out in a gentle (caught, held and carried carefully in an upright position) or rough (caught by the leg, held and carried with lac...|$|E
40|$|The {{amount of}} {{information}} lost in sub-Nyquist sampling of a continuous-time Gaussian stationary process is quantified. We consider a combined source coding and sub-Nyquist reconstruction problem in which the input to the encoder is a noisy sub-Nyquist sampled version of the analog source. We first derive an expression for the mean squared error in {{the reconstruction of the}} process from a noisy and information rate-limited version of its samples. This expression {{is a function of the}} sampling frequency and the average number of bits describing each sample. It is given as the sum of two terms: Minimum mean square error in estimating the source from its noisy but otherwise fully observed sub-Nyquist samples, and a second term obtained by reverse waterfilling over an average of spectral densities associated with the polyphase components of the source. We extend this result to multi-branch uniform sampling, where the samples are available through a set of parallel channels with a uniform sampler and a <b>pre-sampling</b> filter in each branch. Further optimization to reduce distortion is then performed over the <b>pre-sampling</b> filters, and an optimal set of <b>pre-sampling</b> filters associated with the statistics of the input signal and the sampling frequency is found. This results in an expression for the minimal possible distortion achievable under any analog to digital conversion scheme involving uniform sampling and linear filtering. These results thus unify the Shannon-Whittaker-Kotelnikov sampling theorem and Shannon rate-distortion theory for Gaussian sources. Comment: Accepted for publication at the IEEE transactions on information theor...|$|E
40|$|The {{topic of}} thesis are reasons for {{discarding}} blood donors. The {{aim of this}} thesis is to evaluace the amount of discarded blood donors {{and the reasons for}} elimination in 2015 and 2016, further implementation of <b>pre-sampling</b> and post-sampling tests in the Blood Bank Department of the Hospital ČB, Ltd. In the theoretical part I attend to important events in the history of transfusion medicine. Next I deal with criteria for receiving blood donors, evaluation of the blood donors and ways, amounts and frequency of blood collections. The important part of my thesis are the reasons for discarding blood donors, which are divided into temporary and permanent. There are registers, in which these donors are registered. In the methodological part I attend to <b>pre-sampling</b> and post-sampling tests, which are performed in the Blood Bank Department of the Hospital ČB, Ltd. At <b>pre-sampling</b> laboratory measurement of hemoglobin amount on HemoCue 201 + is performed. For new blood donors the blood group orientation is examined on the slide. Donors plasma, platelets or in case of chylozity blood at the last sample hematokrit is examined. Further examination post-sampling test, which are include tests of infectious markers and imunohematology. For examination infectious markers are of service analyzer Architect, tests make on this analyzer are: Ab anti HCV, Ag and Ab HIV, Ab against Treponema pallidum and HBsAg. Immunohematology tests are examination with a dispenser Qasar and imagine analyzer Duet Reader. These two devices are jointly involved in the testing of AB 0 blood group and RhD, screening of irregular anti-erythrocyte antibodies and examination of a group of Rh Kell. In 2015 there were 15 048 blood donors and 360 women and 306 men were discarded. In 2016 there were 15 788 blood donors and 286 women and 211 men were discarded. The most blood donors were discarded because of unsufficient amount of hemoglobin...|$|E
40|$|We {{investigate}} {{the effect of}} environmental policies on innovation under different levels of competition. Using information regarding renewable energy policies, competition and green patents for OECD countries since the late 1970 s, we develop a <b>pre-sample</b> mean count-data econometric specification that accounts for the endogeneity of policies. We find that renewable energy policies are more effective in fostering green innovation in countries with liberalized energy markets. We also find that environmental policies are crucial only in the generation of high-quality green patents, whereas competition enhances the generation of low-quality green patents...|$|R
30|$|For each {{respondent}} 30 repeated {{trials were}} observed. The {{motivation for the}} respondent was, at each trial, to reach the destination {{in time for a}} meeting (or to attend a class). The more punctual the respondents were able to be, the higher the assigned score was (50 points maximum). Moreover, a bonus of 10 points was assigned to each trial if the respondent was able to choose the fastest route (see [13]). The bonus sums up to the standard punctuality-related score. The rewards scheme was chosen by means of a pre-pilot study among ten friends of the SP platform developer (who is {{one of the authors of}} this paper). With this rewards scheme, most of the pre-pilot group were able to achieve almost the same score by acting in two different explicitly requested ways. The first was to consider the fastest-route bonus score as the main goal and the punctuality score as the secondary one, and vice versa for the second way. During the balancing of the rewards scheme only a context without ATIS was applied by the SP platform. Half of the <b>pre-sample</b> was asked to first act with the fastest-route goal and then in the more conservative way; vice versa for the other half of the <b>pre-sample.</b> Because of the balancing of the rewards scheme, we are confident that it does not discourage risk-adverse (preferring punctuality) or risk-seeking (preferring the bonus) route choice behaviour.|$|R
40|$|Particle Induced X-ray Emission is a {{well-established}} technique for quantitative elemental analysis down to trace levels. During microbeam analysis, where the beam is collimated and focused {{into a small}} spot, the beam current reduces to nA or less. The generation of characteristic X-rays is reduced in the same proportion, leading to long data-acquisition times. This can partly be compensated for by using detectors with a large solid angle. In this work, the performance of an annular eight-element silicon drift detector with a total solid angle of 261 msr is described. The initial calibration of the detector was performed using thin elemental standards. Charge measurement was carried out both in a Faraday Cup positioned after the sample and by a <b>pre-sample</b> electrostatic deflection system sampling the beam charge into another Faraday Cup. The two methods were used in parallel and compared during the calibration measurements. A recently installed Versa Module Europe (VME) based data acquisition system equipped with, for example, multi-hit time-to-digital converters, amplifiers, and 32 -channel scalers, was used to record data in event-by-event mode for simultaneous data evaluation on multiple computers. Off-line dead time and pile-up corrections were made on the event data that was sorted into spectra and fitted with the GeoPIXE software. The <b>pre-sample</b> deflection charge measurement gave consistent values for the calibration, {{and this is an}} important observation implying that non-conductive and thick samples will be able to quantify without the use of internal standards...|$|R
40|$|In this paper, {{the optimal}} {{sampling}} strategies (uniform or nonuniform) and distortion tradeoffs for Gaussian bandlimited periodic signals with additive white Gaussian noise are studied. Our {{emphasis is on}} characterizing the optimal sampling locations {{as well as the}} optimal <b>pre-sampling</b> filter to minimize the reconstruction distortion. We first show that to achieve the optimal distortion, no <b>pre-sampling</b> filter is necessary for any arbitrary sampling rate. Then, we provide a complete characterization of optimal distortion for low and high sampling rates (with respect to the signal bandwidth). We also provide bounds on the reconstruction distortion for rates in the intermediate region. It is shown that nonuniform sampling outperforms uniform sampling for low sampling rates. In addition, the optimal nonuniform sampling set is robust with respect to missing sampling values. On the other hand, for the sampling rates above the Nyquist rate, the uniform sampling strategy is optimal. An extension of the results for random discrete periodic signals is discussed with simulation results indicating that the intuitions from the continuous domain carry over to the discrete domain. Sparse signals are also considered, where it is shown that uniform sampling is optimal above the Nyquist rate. Comment: Under revie...|$|E
40|$|In this paper, an {{advanced}} direct RF sampling receiver architecture is studied for the GNSS environment. The architecture {{is based on}} sampling the signal directly at RF, which in the GNSS case are in the 1. 5 GHz range. The high-frequencies in the signals to be sampled pose then very high demands for the accuracy {{and quality of the}} sampling process, and thus quantization and especially the timing jitter must be considered in detail. The study shows that the quantization and jitter requirements are, however, feasible when the <b>pre-sampling</b> filtering is done properly...|$|E
40|$|This paper {{deals with}} the {{development}} of a multisensory virtual environment with visual, haptic, and aural feedbacks for simulating the five-axis CNC milling process. The paper focuses on the haptic and au-ral rendering of the virtual milling process. Haptic rendering provides the user with kinesthetic and tactile information. Kinesthetic informa-tion is displayed by the cutting force of a milling machine. The tactile information is conveyed by the haptic texturing. Aural rendering sim-ulates the machine sound and provides the aural feedback to the user. Using ideas from the concepts of image-based rendering, haptic and aural rendering are accelerated by <b>pre-sampling</b> related environment’s parameters in a perception-dependent way. ...|$|E
40|$|The {{characteristics}} of the thematic mapper (TM) and multispectral scanner (MSS) sensors on LANDSATs 4 and 5 affecting their spatial responses are described, and functions defining {{the response of the}} system to an arbitrary input spatial pattern are derived, i. e., transfer functions (TF) and line spread functions (LSF). These design LSF's and TF's were modified based on prelaunch component and system measurements to provide improved estimates. Prelaunch estimates of LSF/FT's are compared to in-orbit estimates. For the MSS instruments, only limited prelaunch scan direction square-wave response (SWR) data were available. Design estimates were modified by convolving in Gaussian blur till the derived LSF/TF's produced SWR's comparable to the measurements. The two MSS instruments were comparable at their temperatures of best focus; separate calculations were performed for bands 1 and 3, band 2 and band 4. The <b>pre-sample</b> nadir effective instantaneous field's of view (EIFOV's) based on the. 5 modulation transfer function (MTF) criteria vary from 70 to 75 meters in the track direction and 79 to 82 meters in the scan direction. For the TM instruments more extensive prelaunch measurements were available. Bands 1 to 4, 5 and 7, and 6 were handled separately as were the two instruments. Derived MTF's indicate nadir <b>pre-sample</b> EIFOV's of 32 to 33 meter track (bands 1 to 5, 7) and 36 meter scan (bands 1 to 5, 7) and 1245 meter track (band 6) and 141 meter scan (band 6) for both TM's...|$|R
40|$|We {{investigate}} {{the effectiveness of}} policies in favor of innovation in renewable energy under different levels of competition. Using information regarding renewable energy policies, product market regulation and high-quality green patents for OECD countries since the late 1970 s, we develop a <b>pre-sample</b> mean count-data econometric specification that also accounts for the endogeneity of policies. We find that renewable energy policies are significantly more effective in fostering green innovation in countries with deregulated energy markets. We also find that public support for renewable energy is crucial only in the generation of high-quality green patents, whereas competition enhances the generation of green patents irrespective of their quality...|$|R
40|$|In {{this paper}} {{we examine the}} panel data {{estimation}} of dynamic models for count data that include correlated fixed effects and predetermined variables. Use of a linear feedback model is proposed. A quasi-differenced GMM estimator is consistent for the parameters in the dynamic model, but when series are highly persistent, {{there is a problem}} of weak instrument bias. An estimator is proposed that utilises <b>pre-sample</b> information of the dependent count variable, which is shown in Monte Carlo simulations to possess desirable small sample properties. The models and estimators are applied to data on US patents and R&D expenditure. (C) 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|In a Digital Matched Filter (DMF) the {{degradation}} of signal quality depends on the <b>pre-sampling</b> filter bandwidth and {{on the number of}} Analogue to Digital Converter (ADC) bits. Digital matched filters are used in the acquisition of Global Navigation Satellite Systems (GNSS) signals. Automatic Gain Control (AGC) in the GNSS receivers is used to determine the maximum level of quantization by the ADC. In [1], the quantization degradation for different pre sampling bandwidths and ADC bits is analysed in the presence of Additive Gaussian white noise (AGWN). Bandwidths of more than once (or twice) the data bit rate contribute only about 0. 4 dB (or (0. 2 dB) to the overall degradation of the DMF. Also 2 bit uniform step quantization with optimum ADC threshold setting is shown to recover most of the digital implementation degradation. In this paper by focusing on <b>pre-sampling</b> filter bandwidth equal to the data bit rate and 2 bit ADC, the effect of quantization on the received GNSS signal quality in the presence of Continuous Wave (CW) radio frequency interference (RFI) is analysed. It is shown that a one bit ADC can have much higher degradation than the 3. 5 dB in the AGWN case. It is also shown that the effect of AGC on the quantization degradation has to do with both power and frequency of CW RFI. In the presence of CW RFI, the optimal value for ratio of maximum threshold of ADC to the effective noise RMS is shown to be about 1. 35 which is higher than the case when RFI does not exist (0. 8) ...|$|E
40|$|National Natural Science Foundation of China [51105309, 51175425]; Aviation Science Foundation [2011 ZA 53015]A novel {{adaptive}} importance {{sampling method}} is proposed {{to estimate the}} structural failure probability. It properly utilizes Markov chain algorithm to form an adaptive importance sampling procedure. The main concept is suggesting the proposal distributions of Markov chain as the importance sampling density. Markov chain states can adaptively populate the important failure regions thus the importance sampling based on them will yield an efficient and accurate estimate of the failure probability. Compared with existent methods, it does not need {{the solution of the}} design point(s) or the <b>pre-sampling</b> in the failure region. Various examples are given to demonstrate the advantages of the proposed method. (C) 2013 Elsevier Masson SAS. All rights reserved...|$|E
40|$|AbstractObjectiveThis study {{aimed to}} check the {{sensitivity}} of multiple newly developed 3 T MRI breast sequences using CAD software, in <b>pre-sampling</b> diagnosis of breast cancer, {{in an attempt to}} minimize unnecessary invasive sampling or surgical procedures. Patients and methodsThis was a prospective study, included 120 female patients, presented with debatable or malignancy suspected mammo-sonographic results. The study protocol was approved by the ethics committee in Al-Mana General Hospital. Results 36 patients’ tumors were reported as benign and 84 were reported as malignant. Biopsy approved 33 tumors as benign and 87 as malignant tumors. These results gave indices of 93. 1 % sensitivity and 90. 9 % specificity. Conclusion 3 T MRI breast with CAD is very sensitive imaging tool, that can help to avoid unnecessary invasive procedures...|$|E
5000|$|... #Caption: X(f) (top blue) and XA(f) (bottom blue) are {{continuous}} Fourier transforms of two [...] functions, x(t) and xA(t) (not shown). When {{the functions}} are sampled at rate fs, the images (green) {{are added to}} the original transforms (blue) when one examines the discrete-time Fourier transforms (DTFT) of the sequences. In this hypothetical example, the DTFTs are identical, which means , even though the original continuous <b>pre-sampled</b> functions are not. If these were audio signals, x(t) and xA(t) might not sound the same. But their samples (taken at rate fs) are identical and would lead to identical reproduced sounds; thus xA(t) is an alias of x(t) at this sample rate.|$|R
40|$|Architectural {{simulation}} {{is extremely}} time-consuming given the {{huge number of}} instructions {{that need to be}} simulated for contemporary benchmarks. Sampled simulation that selects a number of samples from the complete benchmark execution yields substantial speedups. However, there is one major issue that needs to be dealt with in order to minimize non-sampling bias, namely the hardware state {{at the beginning of each}} sample. This is well known in the literature as the cold-start problem. The hardware structures that suffer the most from the cold–start problem are cache hierarchies. In this paper, we propose NSL–BLRL, which combines two previously proposed cache hierarchy warmup approaches, namely: no-state-loss (NSL) and boundary line reuse latency (BLRL). The idea of NSL–BLRL is to warmup the cache hierarchy using a hardware state checkpoint that stores a truncated NSL stream. The NSL stream is a least-recently used stream of (unique) memory references in the <b>pre-sample.</b> This NSL stream is then truncated to form the NSL–BLRL warmup checkpoint; this is done by inspecting the sample for determining how far in the <b>pre-sample</b> one needs to go back to accurately warmup the hardware state for the given sample. We show using SPEC CPU 2000 benchmarks that NSL–BLRL is (i) nearly as accurate as BLRL and NSL for sampled processor simulation, (ii) yields simulation time speedups of several orders of magnitude compared to BLRL and (iii) is more space-efficient than NSL. As such, we conclude that NSL–BLRL is a highly efficient and accurate cache warmup strategy for sampled processor simulation...|$|R
40|$|WPHACT 2. 0 {{is the new}} fully massive {{version of}} a MC program and {{unweighted}} event generator which computes all Standard Model processes with four fermions in the final state at e + e − colliders. The program can now generate unweighted events for any subset of all four fermion final states in a single run, by making use of dedicated <b>pre-samples</b> which can cover the entire phase space. Improvements with respect to WPHACT 1. 0 include the Imaginary Fermion Loop gauge restoring scheme, new phase space mappings, a new input system, the possibility to compute subsets of Feynman diagrams and options for including ISR via QEDPS, running αQED, CKM mixing, resonances in q¯q channels...|$|R
40|$|AbstractObjectiveThis study {{aimed to}} check the {{sensitivity}} of phased array surface coli of 3 T MRI, in <b>pre-sampling</b> diagnosis of prostate cancer, {{in an attempt to}} use it instead of endorectal coil. Patients and methodsThis was a prospective comparative study, included 20 male patients, presented with suspected prostate cancer due to unexplained high PSA. The study protocol was approved by the ethics committee in Al-Mana General Hospital. ResultsProstate cancer was correctly diagnosed by T 2 w sequence within 9 patients, 10 by DW&T 2 w, 13 by T 2 w – DW-DCE and 14 by of T 2 w-DW-DCE-MRS sequences. Conclusion 3 T MRI imaging using phased array surface coil is a useful diagnostic tool for detecting prostate cancer, trustworthy when compared to endorectal approach...|$|E
40|$|Abstract—In this paper, the {{applicability}} of advanced direct RF sampling receiver architecture is studied in the GNSS envi-ronment. The architecture is based on sampling the signal di-rectly at RF, which in the GNSS case is in the 1. 5 GHz range. The high-frequencies in the signal to be sampled pose then very high demands for the accuracy {{and quality of the}} sampling process, and thus quantization and especially the timing jitter aspects must be considered in detail. Both system calculations as well as computer simulations are used to assess the essential requirements for the sampling process. In summary, the study shows that the quantization and jitter requirements are in prin-ciple feasible when the <b>pre-sampling</b> filtering is done properly. Index Terms—Direct RF Sampling, radio receiver design...|$|E
40|$|Objective: This study {{aimed to}} check the {{sensitivity}} of multiple newly developed 3 T MRI breast sequences using CAD software, in <b>pre-sampling</b> diagnosis of breast cancer, {{in an attempt to}} minimize unnecessary invasive sampling or surgical procedures. Patients and methods: This was a prospective study, included 120 female patients, presented with debatable or malignancy suspected mammo-sonographic results. The study protocol was approved by the ethics committee in Al-Mana General Hospital. Results: 36 patients’ tumors were reported as benign and 84 were reported as malignant. Biopsy approved 33 tumors as benign and 87 as malignant tumors. These results gave indices of 93. 1 % sensitivity and 90. 9 % specificity. Conclusion: 3 T MRI breast with CAD is very sensitive imaging tool, that can help to avoid unnecessary invasive procedures...|$|E
40|$|This paper {{examines}} {{the application of}} count data models to firm level panel data on technological innovations. The model the authors propose exhibits dynamic feedback and unobserved heterogeneity. We develop a fixed effects estimator that generalizes the standard Poisson and negative binomial models allowing for dynamic feedback through both the firm's stock of knowledge and its product market power. By using the long <b>pre-sample</b> history of innovation information this 'entry stock' estimator is shown to control for correlated fixed effects and is compared with an alternative nonlinear GMM estimator. We find evidence of history dependence in innovation activity although variables reflecting the company's economic environment are also found {{to play a major}} role. Copyright 1995 by Royal Economic Society. ...|$|R
40|$|We {{show that}} {{universities}} in the United States that provide stronger royalty incentives to faculty scientists generate greater license income, controlling for university characteristics. We use <b>pre-sample</b> data on university patenting to control for the potential endogeneity of royalty shares. Faculty responds to royalties both {{in the form of}} cash and research lab support, indicating both pecuniary and intrinsic research motivations. The impact of incentives is larger in private than in public universities, and we provide new survey evidence on the organization and objectives of university licensing offices to explain this difference. Royalty incentives work both by raising faculty effort and sorting scientists across universities. The primary impact of incentives is to increase the quality rather than the quantity of inventions. Copyright (c) 2008, RAND. ...|$|R
50|$|E.M.A.K.’s {{influences}} include Kraftwerk, Neu!, Klaus Schulze, Tangerine Dream, Neue Musik and musique concrète, {{as well as}} Pink Floyd and the White Noise project {{associated with}} the BBC Radiophonic Workshop. Much of E.M.A.K.'s musical style was directly formed from experimentation with various instruments. These included analogue synthesizers such as a Mini-Moog and a Synthanorma Sequencer (a German sequencer built by Hajo Wiechers of Matthen & Wiechers/ Bonn similar to the sequencer of the large Moog system), a Fender Rhodes piano (customized by Stühlen with external treatments such as distortion, echo chambers, and improvised effects), and a Roland TR-808 drum machine. E.M.AK. and E.M.A.K. 2 were released <b>pre-sampler</b> and pre-midi and so took a manual tape-based approach to looping and musique concrète parts. E.M.A.K. 3 used the AKAI S612 midi sampler and a Commodore 64.|$|R
40|$|Kinetic Analysis {{has been}} {{successful}} for metallic elements in relatively isolated areas. In this study it is applied to a complex organic compound in a geographical area with a large urban component. Ten media compartments are included, with man as the ultimate receptor. Field data were collected for only 6 {{of the media and}} were not used in the analysis but were compared to the calculated steady state concentrations. The greatest differences between calculated and observed values were 4. 8 -fold for soil and 5. 4 -fold for sediment. The field sampling regime for soils was biased towards areas of industrialization and probably explains the higher observed value. The lower observed value for sediment is likely due to unknown variables necessary for the estimation of the compartment size and/or the associated transfer rate constants. This study indicated that the Kinetic Analysis technique can be applied successfully to the <b>pre-sampling</b> estimation of the distribution of organic pollutants in environmental systems...|$|E
40|$|NASA Extreme Environment Mission Operations (NEEMO) is an {{underwater}} spaceflight analog {{that allows a}} true mission-like operational environment and uses buoyancy effects and added weight to simulate different gravity levels. Three missions were undertaken from 2014 - 2015, NEEMO's 18 - 20. All missions were performed at the Aquarius undersea research habitat. During each mission, the effects of communication latencies on operations concepts, timelines, and tasks were studied. METHODS: Twelve subjects (4 per mission) were weighed out to simulate near-zero or partial gravity extravehicular activity (EVA) and evaluated different operations concepts for integration and management of a simulated Earth-based science team (ST) to provide input and direction during exploration activities. Exploration traverses were preplanned based on precursor data. Subjects completed science-related tasks including <b>pre-sampling</b> surveys, geologic-based sampling, and marine-based sampling as {{a portion of their}} tasks on saturation dives up to 4 hours in duration that were designed to simulate extravehicular activity (EVA) on Mars or the moons of Mars. One-way communication latencies, 5 and 10 minutes between space and mission control, were simulated throughout the missions. Objective data included task completion times, total EVA times, crew idle time, translation time, ST assimilation time (defined as time available for ST to discuss data/imagery after data acquisition). Subjective data included acceptability, simulation quality, capability assessment ratings, and comments. RESULTS: Precursor data can be used effectively to plan and execute exploration traverse EVAs (plans included detailed location of science sites, high-fidelity imagery of the sites, and directions to landmarks of interest within a site). Operations concepts that allow for <b>pre-sampling</b> surveys enable efficient traverse execution and meaningful Mission Control Center (MCC) interaction across communication latencies and can be done with minimal crew idle time. Imagery and contextual information from the EVA crew that is transmitted real-time to the intravehicular (IV) crewmember(s) can be used to verify that exploration traverse plans are being executed correctly. That same data can be effectively used by MCC (across comm latency) to provide meaningful feedback and instruction to the crew regarding sampling priorities, additional tasks, and changes to the EVA timeline. Text / data capabilities are preferred over voice capabilities between MCC and IV when executing exploration traverse plans over communication latency...|$|E
40|$|Unclassified region deceases the {{efficiency}} and performance of PLSA and FLDA. The proper selection of feature sub set reduced the unclassified region of PLSA and FLDA. Now a day‟s binary classification are widely used in image classification. The mapping of data one space to another space creates diversity of outlier and noise and generate unclassified region for image classification. For the reduction of unclassified region we used radial basis function for sampling of feature and reduce the noise and outlier for feature space of data and increase the performance and efficiency of image classification. Our proposed method optimized the feature selection process and finally sends data to FLDA classifier for classification of data. Here we used fisher classifier. As a classifier FLDA suffering two problems (1) how to choose optimal feature sub set input and (2) how to set best kernel parameters. These problems influence the performance and accuracy of FLDA. Now the <b>pre-sampling</b> of feature reduced the feature selection process of FLDA for image classification. Keywords: image classification, feature reduction, FLDA, RB...|$|E
40|$|The {{so-called}} type I {{and type}} II fractional Brownian motions are limit distributions {{associated with the}} fractional integration model in which <b>pre-sample</b> shocks are either included in the lag structure, or suppressed. There can be substantial di¤erences between the distributions of these two processes and of functionals derived from them, so that it becomes an important issue to decide which model {{to use as a}} basis for inference. Alternative methods for simulating the type I case are contrasted, and for models close to the nonstationarity boundary, truncating in…nite sums is shown to result in a signi…cant distortion of the distribution. A simple simulation method that overcomes this problem is described and implemented. The approach also has implications for the estimation of type I ARFIMA models, and a new conditional ML estimator is proposed, using the annual Nile minima series for illustration. ...|$|R
40|$|This paper draws {{attention}} to a fundamental problem that occurs in applying importance sampling to ‘high-dimensional’ reliability problems, i. e., those {{with a large number}} of uncertain parameters. This question of applicability carries an important bearing on the potential use of importance sampling for solving dynamic first-excursion problems and static reliability problems for structures with a large number of uncertain structural model parameters. The conditions under which importance sampling is applicable in high dimensions are investigated, where the focus is put on the common case of standard Gaussian uncertain parameters. It is found that importance sampling densities using design points are applicable if the covariance matrix associated with each design point does not deviate significantly from the identity matrix. The study also suggests that importance sampling densities using random <b>pre-samples</b> are generally not applicable in high dimensions...|$|R
40|$|Authors' draft {{published}} as Discussion paperThe so-called type I and type II fractional Brownian motions are limit distributions {{associated with the}} fractional integration model in which <b>pre-sample</b> shocks are either included in the lag structure, or suppressed. There can be substantial differences between the distributions of these two processes and of functionals derived from them, so that it becomes an important issue to decide which model {{to use as a}} basis for inference. Alternative methods for simulating the type I case are contrasted, and for models close to the nonstationarity boundary, truncating infinite sums is shown to result in a significant distortion of the distribution. A simple simulation method that overcomes this problem is described and implemented. The approach also has implications for the estimation of type I ARFIMA models, and a new conditional ML estimator is proposed, using the annual Nile minima series for illustration...|$|R
