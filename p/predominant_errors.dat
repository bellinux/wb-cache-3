3|35|Public
40|$|The use of English {{language}} {{occurs in}} a variety of areas such as politics, economics, technology, and education. Besides our country needs to absorb ideas from western, we must be able to express our ideas to the advanced countries by translating them into English. Petra takes this role. Students of Petra Christian University are required to write the thesis abstract in English. We notice not all Petra students have good skill in English, particularly in doing translation. There are some translation errors that occurred. Therefore, the writer wants to reveal the common problems in terms of content and grammatical errors. In order to find out the content error, the writer uses Newmark?s and Barnet and Stubbs? classifications. For the grammatical error, she uses Richards? classification. The writer takes the data from some Management Department theses in Petra Christian University. From the quantity of the content errors in translation, majority of Management Department students make left out and different sense errors. For the grammatical error, the <b>predominant</b> <b>errors</b> are error in the production of verb group and miscellaneous error...|$|E
40|$|Objective: The {{ability of}} Cantonese {{speakers}} in producing aspirated and unaspirated stops and stops at different {{places of articulation}} using expiratory phonation (EP) and inspiratory phonation (IP) was compared. Interarticulator timing during stop production using EP and IP was examined. Voice onset time (VOT) associated with EP and IP stops was compared with stop identification scores by naïve listeners. Subjects and Methods: Aspirated and unaspirated voiceless stops (/p h, t h, k h, p, t, k/) followed by the vowel /a/ were produced by 15 male and 15 female Cantonese speakers using EP and IP. VOT values were measured and isolated speech samples of stop productions were identified by 10 naïve listeners. Percent correct identification of stops {{was obtained from the}} 10 listeners. Results: Perceptual data showed that production of IP stops were associated with reduced accuracy in stop identification, with <b>predominant</b> <b>errors</b> in aspiration perception. Acoustic analysis showed that IP stops were generally produced with significantly shorter VOT than their EP counterparts. In addition, effect of place of articulation on VOT was also found for both IP and EP stops, notably with velar stops being associated with significantly longer VOT values than bilabial and alveolar stops. Conclusions: The findings that IP stops were produced with shorter VOT as compared with EP stops imply that the articulatory-phonatory coordination during IP was not the same as that during EP, causing a discrepancy in the timing control between articulators. © 2011 The Voice Foundation. link_to_subscribed_fulltex...|$|E
40|$|Deformation {{monitoring}} in an {{open pit}} mine poses some interesting challenges. Due to limitations with classical geodetic techniques, the Canadian Centre for Geodetic Engineering (CCGE) at UNB has decided to pursue augmenting its fully-automated deformation monitoring system with GPS. Just as there are limitations with classical techniques of deformation monitoring {{in an open}} pit mine, so too are there constraints placed {{on the use of}} GPS. High-performance GPS RTK software has been developed within the Geodetic Research Laboratory (GRL) at the University of New Brunswick (UNB). This software was initially designed for gantry crane auto-steering. Naturally, the GRL and CCGE have combined efforts to attempt to achieve the required precision for this application. As a first step, tests of {{a modified version of the}} GPS RTK software have been carried out at Highland Valley Copper Mine in British Columbia, Canada. These results are compared with results attained from Trimble Total Control and UNB’s DIPOP GPS processing software. Because the stability of the benches is of primary concern, the GPS vertical component solution is of particular interest. This paper introduces the approach taken at UNB to address high precision requirements in a constrained environment. Technical and scientific aspects of the UNB RTK software, especially in handling two <b>predominant</b> <b>errors</b> (residual tropospheric zenith delay and multipath) at the mine, are discussed. Remaining issues that need to be addressed in order to meet the needs of this application are also presented...|$|E
40|$|Projections {{regarding}} the rapidly {{increasing number of}} Spanish speakers in the United States combined with the increasing incidence of bilingual aphasia have inspired researchers to collect normative data on confrontation naming tasks {{that can be used}} in bilingual patient populations. Studies that have reported normative data for English/Spanish bilingual speakers have found that relatively more English proficient bilingual speakers do not perform like English monolinguals and, therefore, bilingual performance has to be compared to bilingual norms and not to monolingual norms. Using An Object and Action Naming Battery, this study aims to 1) provide Spanish/English bilingual normative data for nouns and verbs, 2) evaluate performance in naming across three proficiency groups: English dominant, balanced, and Spanish dominant, 3) conduct an error analysis to examine the error patterns according to proficiency group. Consistent with the predictions, overall results showed overall accuracy was higher in English than Spanish, but three proficiency groups did emerge, with each proficiency group scoring better in the stronger language, and the balanced group scoring similarly across languages. The balanced group was the only group that benefited from the use of a composite score, which provides credit for all responses regardless of language. Overall, the <b>predominant</b> <b>error</b> type was no response/I don t know. Within language errors occurred more often than crosslinguistic <b>errors.</b> The <b>predominant</b> <b>error</b> for the two dominant groups was no response/I don t know. The <b>predominant</b> <b>error</b> for the balanced group was semantic. Error patterns and clinical significance are discussed...|$|R
40|$|We {{study the}} {{temperature}} dependence of discretization errors in nuclear lattice simulations. We find that for systems with strong attractive interactions the <b>predominant</b> <b>error</b> {{arises from the}} breaking of Galilean invariance. We propose a local "well-tempered" lattice action which eliminates much of this error. The well-tempered action can be readily implemented in lattice simulations for nuclear systems as well as cold atomic Fermi systems. Comment: 33 pages, 17 figure...|$|R
40|$|The <b>predominant</b> <b>error</b> {{source in}} the final {{navigation}} for the Giotto Comet fly-by of Halley's nucleus is due to uncertainties in the comet's ephemeris. Efforts are being undertaken at ESOC to improve the prediction of the comet's state {{at the time of}} the encounter. Some of the aspects involved in the problem discussed include ways to improve old observations by re-measurement or re-reduction and encouragement to observers to take fine quality astrometric observations in the future, especially at critical times. Difficulties in the mathematical modelling of the non-gravitational forces and possible offsets of the comet's center-of-light from its center-of-mass are addressed. The available estimation techniques are also examined including one based upon selecting optimum observations...|$|R
40|$|Ionospheric {{electron}} density variations are more <b>predominant</b> <b>error</b> sources in precise positioning with Global Positioning System (GPS) based navigation systems over low latitude regions such as India and Brazil. {{spatial and temporal}} variability is more in this region due to Equatorial Ionospheric Anomaly (EIA), Spread F and ionospheric scintillations etc. Short and long term ionospheric changes such as the solar cycle, the annual and semiannual variations of the ionosphere needs to be investigated for improving reliable communication and navigation systems. In this paper, EOF analysis is used to investigate the spatial and temporal distribution of ionospheric {{electron density}}. An empirical model is implemented based on coefficients and basis functions obtained from the EOF analysis. It {{is evident from the}} results that the coefficients of EOF basis functions well signify the solar activity, diurnal variations of electron density...|$|R
40|$|This article {{reports on}} a mildly aphasic patient with major {{disorders}} in reading, writing, and number processing. His <b>predominant</b> <b>error</b> type in reading aloud Arabic numbers and in matching heard numerals with Arabic numbers was the violation of the inversion rule of the German Arabic number reading system. According to this rule most of the two-digit numbers or numbers in the final and prefinal position of longer digit strings have to be read beginning with the final digit (e. g., 26 → sechsundzwanzig (literally translated: six-and-twenty)). It is argued that AT’s inver-sion errors (e. g., 26 → zweiundsechzig (literally translated: two-and-sixty)) are {{not consistent with the}} predictions of single route models of Arabic number reading but are in agreement with proposals of a visually based asemantic reading routine in addition to a semantically mediated reading routine. 1997 Academic Pres...|$|R
40|$|Three {{experiments}} {{investigated the}} ability of eight-year old children with poor language comprehension to produce past tense forms of verbs. Twenty children selected as poor comprehenders were compared to 20 age-matched control children. Although the poor comprehenders performed less well than controls {{on a range of}} tasks considered to tap verbal-semantic abilities, the two groups showed equivalent phonological skills. Poor comprehenders performed as well as control children when asked to inflect novel verbs and regular verbs. In contrast, poor comprehenders were less skilled than controls at inflecting both high frequency and low frequency irregular verbs. Although the <b>predominant</b> <b>error</b> pattern for all children was to over-regularize, this was most marked in the poor comprehenders; control children were more likely to produce errors that contained knowledge of the irregular form than poor comprehenders. In addition, the ability to inflect irregular verbs was related to individual differences in verbal-semantic skills. These findings are discussed within a framework in which verb inflection is related to underlying language skills in both the phonological and semantic domains...|$|R
40|$|Purpose: The {{purpose of}} this {{cross-sectional}} observational {{study was to determine}} the distribution and patterns of refractive errors, strabismus, and amblyopia in children seen at a pediatric eye care. Materials and Methods: The study was conducted in a Private Hospital in Dammam, Kingdom of Saudi Arabia, from March to July 2013. During this period, a total of 1350 children, aged 1 - 15 years were seen at this Center′s Pediatric Ophthalmology Unit. All the children underwent complete ophthalmic examination with cycloplegic refraction. Results: Refractive errors accounted for 44. 4 % of the cases, the <b>predominant</b> refractive <b>error</b> being hypermetropia which represented 83 %. Strabismus and amblyopia were present in 38 % and 9. 1 % of children, respectively. Conclusions: In this clinic-based study, the focus was on the frequency of refractive errors, strabismus, and amblyopia which were considerably high. Hypermetropia was the <b>predominant</b> refractive <b>error</b> in contrast to other studies in which myopia was more common. This could be attributed to the criteria for sample selection since it was clinic-based rather than a population-based study. However, it is important to promote public education on the significance of early detection of refractive errors, and have periodic screening in schools...|$|R
40|$|We {{describe}} a brain-damaged subject, RR, who manifests superior written over spoken naming of concrete entities {{from a wide}} range of conceptual domains. His spoken naming difficulties are due pri-marily to an impairment of lexical-phonological processing, which implies that his successful written naming does not depend on prior access to the sound structures of words. His performance therefore provides further support for the “orthographic autonomy hypothesis, ” which maintains that written word production is not obligatorily mediated by phonological knowledge. The case of RR is especially inter-esting, however, because for him the dissociation between impaired spoken naming and relatively pre-served written naming is significantly greater for two categories of unique concrete entities that are lexicalised as proper nouns—specifically, famous faces and famous landmarks—than for five categories of nonunique (i. e., basic level) concrete entities that are lexicalised as common nouns—specifically, animals, fruits/vegetables, tools/utensils, musical instruments, and vehicles. Furthermore, RR’s <b>predominant</b> <b>error</b> types in the oral modality are different for the two types of stimuli: omissions for unique entities vs. semantic errors for nonunique entities. We consider two alternative explanations for RR’s extreme diffi-culty in producing the spoken forms of proper nouns: (1) a disconnection between the meanings of proper nouns and the corresponding word nodes in the phonological output lexicon; or (2) damage to th...|$|R
40|$|Background: Limited {{data are}} {{available}} on physicians’ accuracy in coding for their services. The {{purpose of this study}} was to determine the current procedural terminology (CPT) evaluation and management coding accuracy of family physicians and define demographic variables associated with coding accuracy. Methods: Six hundred randomly selected active members of the Illinois Academy of Family Physicians were sent six hypothetical progress notes of office visits along with a demographic survey. The study group assigned CPT evaluation and management codes to each of the progress notes and completed the demographic survey. Five expert coders also assigned codes to each of the cases. The accuracy of family physicians in determining CPT E/M codes was determined relative to that of expert coders. Results: Family physicians agreed with the experts’ CPT evaluation and management codes for 52 % of established patient progress notes, the most common error being undercoding. In contrast, for new patient progress notes, family physicians agreed with the experts only 17 % of the time, the <b>predominant</b> <b>error</b> being overcoding. No surveyed demographic variable was associated with coding accuracy. Conclusions: The error rate for physician CPT coding is substantial and occurs more commonly with new patients. The complexity of the CPT coding guidelines, along with limited physician training in CPT coding, likely account for these results...|$|R
40|$|Abstract—Errors are {{inherently}} present in unreliable wireless channels. The primary challenge in designing error control protocols in the MAC or physical layer is to effectively maximize achievable throughput in wireless networks even when unpredictable and time-varying errors exist. Network coding {{has been successfully}} applied to improve throughput in IEEE 802. 11 -based wireless networks with a shared broadcast channel. In state-ofthe-art physical layer designs in multi-channel wireless networks (such as IEEE 802. 16 WiMAX), however, the convenience of a shared wireless broadcast channel to perform opportunistic listening no longer exists, and Hybrid ARQ (HARQ) is the <b>predominant</b> <b>error</b> control protocol in the physical layer, rather than plain ARQ in IEEE 802. 11 MAC. Would network coding be well employed in multi-channel wireless networks and able to bring further improvements over HARQ? This paper proposes Drizzle, a new solution to maximize throughput {{with the presence of}} errors, that takes advantage of network coding at the symbol level in multi-channel wireless networks. By operating at the symbol level and using soft decision values, we show that Drizzle is able to exploit both time and cooperative diversity in realistic multi-channel wireless networks, to adapt to time-varying and bursty channel errors, and to efficiently collect as many correct symbols as possible at the receiver...|$|R
40|$|Abstract Stereoscopic {{particle}} image velocimetry (SPIV) {{is applied}} to measure the instantaneous three component velocity field of pipe flow over the full circular cross-section of the pipe. The light sheet is oriented perpendicular to the main flow direction, and therefore the flow structures are advected through the measurement plane by the mean flow. Applying Taylor’s hypothesis, the 3 D flow field is reconstructed from the sequence of recorded vector fields. The large out-of-plane motion in this configuration puts a strong constraint on the recorded particle displacements, which limits the measurement accuracy. The light sheet thickness becomes an important parameter that determines the balance between the spatial resolution and signal to noise ratio. It is further demonstrated that so-called registration errors, which result from a small misalignment between the laser light sheet and the calibration target, easily become the <b>predominant</b> <b>error</b> in SPIV measurements. Measurements in laminar and turbulent pipe flow are compared to well estab-lished direct numerical simulations, and {{the accuracy of the}} instantaneous velocity vectors is found to be better than 1 % of the mean axial velocity. This is sufficient to resolve the secondary flow patterns in transitional pipe flow, which are an order of magnitude smaller than the mean flow. ...|$|R
40|$|Human {{error is}} present in {{approximately}} 60 to 80 percent of all Naval Aviation (NA) flight mishaps (FMs). This indicates a need to identify the patterns and relationships of human error associated with NA FMs {{in order to develop}} tailored intervention strategies. This study uses the Human Factors Analysis and Classification System (HFACS), a human error oriented accident Investigation and analysis process, to conduct post-hoc analysis of 77 rotary wing and 141 Tactical Aircraft (TACAIR) Class A and B human error FMs from Fiscal Year 90 to 97. This study indicates that Skill-Based Error, Decision Error, Adverse Mental State (AMS) and Crew Resource Management (CRM) are the <b>predominant</b> human <b>error</b> types associated with NA FMs. A nonparametric bootstrap simulation is performed for singular and combinations of human error types to develop the most effective intervention strategies. For the rotary wing community, the CRM human error type represents the best target for selected intervention strategies and potential cost savings. The AMS human error type provides the best target for selected intervention strategies and potential cost savings for the TACAIR community. The use of flight simulators is viewed as the most effective intervention strategy for both <b>predominant</b> human <b>error</b> types identified. U. S. Navy (U. S. N.) author...|$|R
40|$|Studies on the {{acquisition}} of indefinite articles by sequential bilingual (L 2) children have provided mixed results regarding whether L 2 children omit or substitute indefinite articles. In the present paper, we examined whether Turkish-speaking child L 2 learners of English omitted or substituted indefinite articles by using a production task that comprised two different semantic contexts, the referential specific and the non-referential predicational context. We also examined the source of children’s errors by using a self-paced listening task where children heard grammatical sentence where indefinite articles were present and ungrammatical sentences with omitted indefinite articles. L 2 children’s performance was {{compared with that of}} two monolingual (L 1) groups: an age-matched L 1 group and a younger L 1 group. Results showed that all groups of children distinguished between the two semantic contexts in both the production and the on-line processing task. At the same time, both groups of older L 1 and L 2 children’s errors consisted in omission of indefinite articles whereas the <b>predominant</b> <b>error</b> for the younger L 1 children was substitutions. In the on-line processing task, all groups of children detected the ungrammaticality related to article omission. We interpret these results within the Feature Reassembly and the Full Transfer/Full Access Hypotheses...|$|R
40|$|Airborne gravimetry is the {{determination}} of the Earth's gravity field, using aircraft as mobile measurement platforms. For such measurements, there exist two predominant types of instrumentation: 1. Mechanical spring gravimeters, which are mounted on a gimballed platform in order to maintain a constant sensor orientation during the flight, aligned with the local vertical of the gravity field; 2. aircraft body-fixed 'strap-down' Inertial Measurement Units (IMU), containing each one sensor triad of accelerometers and gyroscopes. While IMUs are commonly designed for navigation applications, they also turn out to have several practical advantages also for gravimetric applications, compared to the more established platform-stabilised spring-gravimeters. In particular advantageous are the lower space and energy consumption, the autonomous operation during the flights, the lower sensitivity to turbulence, and the considerably lower acquisition costs. This thesis is a contribution to the improvement of kinematic, IMU-based gravimetry (denoted as strapdown gravimetry). In practice, the <b>predominant</b> source of <b>errors</b> of such systems arises from uncompensated accelerometer drifts. It is shown theoretically, and based on simulations as well, that such drifts are in practice inseparable from the gravity signal which is to be determined. Based on this finding, several accelerometer calibration methods are developed, aiming at the reduction of in-flight accelerometer drifts. In particular, thermal effects are shown to be the <b>predominant</b> <b>error</b> source. The proposed calibration methods are evaluated on real data, taken from five different airborne gravity campaigns. The common airborne gravimetry evaluation methods are summarised and discussed. An IMU-based gravity measurement accuracy of approximately 1 e- 5 m/s^ 2 is verified, being equal or even superior compared to the achievable accuracy of mechanical spring-gravimeters under comparable conditions, which are still the predominant instrumentation for airborne gravimetry...|$|R
40|$|Abstract—Previous decades {{brought about}} a {{revolution}} in ra-dio and microprocessor technology that made possible a plethora of new applications. In particular, {{the possibility of using}} many inexpensive sensor nodes interconnected by wireless networks (WSN) for a number of ends, such as pollution monitoring and defense, draw the attention of the research community. WSN are usually heavily resource-constrained. Of particular relevance is energy, since in many applications nodes should operate during long periods from batteries. The literature on this topic reveals many techniques to improve energy efficiency, one of them being the use of network aggregation. However, the problem of aggregation of duplicate sensitive summaries (e. g. sum, average, histogram, etc.) in multi-path routing networks is not fully resolved. This paper addresses this problem by sending redundant aggregated information through different paths, so data can be reconstruct to obtain the exact summary, provided that {{there is at least one}} feasible path. Two algorithms are pre-sented, one better suited for networks dominated by link errors and another suited to networks where the <b>predominant</b> <b>error</b> source is node failures. The algorithms are light during normal network operation, with the most intensive processing performed during the initialization phase. The approach presented herein outperform previous solutions found in the literature in two key aspects: complete topology independence and aggregation depth independence...|$|R
40|$|Correct {{prescription}} writing habits {{could have}} a great influence {{on the fate of}} drug therapy as well as the health of patients. One of the major types of errors in prescriptions is the “errors of commission”. In this study attempts were made to examine 519 prescriptions of the internal ward of Ayatollah Taleghani teaching hospital over a period of 3 months in terms of {{the nature and extent of}} the errors of commission. In addition these prescriptions were also compared and contrasted with the actual clinical charts of relevant inpatients. Results showed that the most <b>predominant</b> <b>error</b> of commission is that due to ignoring drug interactions, noted in 18 % of all prescriptions examined. The other major errors of commission noted in these prescriptions included mistakes in writing drug names or prescribing the correct dosage form, ignoring drug interactions occurring because of the condition of patients, andignoring the side effects of drugs following administration. Furthermore, when comparing the clinical chart of inpatients with their prescriptions, discrepancies were noted. Here, the majorproblem was not mentioning the name of some of the drugs present within the prescription, in patients' clinical chart. In conclusion, it seems that educational programs are needed to be held for physicians and nursing staff in order to eliminate errors of commission. Careful examination of prescriptions by pharmacists and decisive interventions by them could also be critical in avoiding this type of prescription error...|$|R
40|$|A {{previous}} study of 10 patients with Broca’s aphasia {{demonstrated that the}} advantage for producing the past tense of irregular over regular verbs exhibited by these patients was eliminated when {{the two sets of}} past-tense forms were matched for phonological complexity (Bird, Lambon Ralph, Seidenberg, McClelland, & Patterson, 2003). The interpretation given was that a generalised phonological impairment was central to the patients’ language deficits, including their poor performance on regular past tense verbs. The current paper provides further evidence in favour of this hypothesis, {{on the basis of a}} detailed analysis of the errors produced by these same 10 patients in reading, repetition, and sentence completion for a large number of regular, irregular, and nonce verbs. The patients’ <b>predominant</b> <b>error</b> types in all tasks and for all verb types were close and distant phonologically related responses. The balance between close and distant errors varied along three continua: the severity of the patient (more distant errors produced by the more severely impaired patients); the difficulty of the task (more distant errors in sentence completion > reading > repetition); the difficulty of the item (more distant errors for novel word forms than real verbs). A position analysis for these phonologically related errors revealed that vowels were most likely to be preserved and that consonant onsets and offsets were equally likely to be incorrect. Critically, the patients’ errors exhibited a strong tendency to simplify the phonological form of the target. These results are consistent with the notion that the patients’ relatively greater difficulty with regular past tenses reflects a phonological impairment that is sensitive to the complexity of spoken forms...|$|R
40|$|The pH of tumors and {{surrounding}} tissues {{is a key}} biophysical property of the tumor microenvironment that affects how a tumor survives and how it invades the surrounding space of normal tissue. Research into tumorigenesis and tumor treatment is greatly dependent on accurate, precise, and reproducible measurements. Optical imaging is generally regarded as {{the best choice for}} non-invasive and high spatial resolution measurements. Ratiometric fluorescence imaging and fluorescence lifetime imaging microscopy (FLIM) are two primary ways for measuring tumor pH. pH measurements in a window chamber animal model using a ratiometric fluorescence imaging technique is demonstrated in this dissertation. The experimental setup, imaging protocols, and results are presented. A significantly varying bias was consistently observed in the measured pH. A comprehensive analysis on the possible error sources accounting for this bias is carried out. The result of analysis reveals that accuracy of ratiometric method is most likely limited by biological and physiological factors. FLIM is a promising alternative because the fluorescence lifetime is insensitive to the biological and physiological factors. Photon noise is the <b>predominant</b> <b>error</b> source of FLIM. The Fisher information matrix and the Cramér-Rao lower bound are used to calculate the lowest possible variance of estimated lifetime for time-domain (TD) FLIM. A statistical analysis of frequency-domain (FD) FLIM using homodyne lock-in detection is also performed and the probability density function of the estimated lifetime is derived. The results allow the derivation of the optimum experimental parameters, which yields the lowest variance of the estimated lifetime in a given period of imaging time. The analyses of both TD and FD-FLIM agree with results of corresponding Monte Carlo simulations...|$|R
40|$|Aneuploidy {{frequency}} {{increases with}} advanced female age {{and results in}} infertility or live birth of affected individuals. Aneuploidies occur mainly during female meiosis. Polar bodies biosied from oocytes after first and second meiosis were analyzed using array comparative genomic hybridization testing for all chromosomes. More than a half of tested oocytes were aneuploid. Aneuploidies {{as a result of}} nondisjunction in meio-sis II were slightly more frequent than meiosis I errors. Premature chromatide predivisi-on was absolutely <b>predominant</b> among <b>errors</b> occurring during meiosis I. Despite the fact that aneuploidies were detected for each chromosome, most aneuploidies were detected for small acrocentric chromosomes. Possible mechanisms of aneuploidy formation are discussed in context of information obtained by the means of animal biotechnologies at different species of mammals...|$|R
40|$|Does letter-form {{constrain}} {{errors in}} peripheral dyslexia? In Hebrew, 5 of the 22 letters have two different letter forms, one is used {{only when the}} letter occurs in word-final position, the other form is used in initial and middle positions. Is the information on final-forms encoded in the letter identity information and used for word identification, or is it discarded? The current research explored this question through the effect of final vs. non final letter form on the error pattern in neglect dyslexia (neglexia) and letter position dyslexia (LPD). Left word-based neglexia results in errors of omission, substitution and addition of letters in {{the left side of}} words, which in Hebrew {{is the end of the}} word. We examined whether final letter form blocks the addition of letters to the end of the word and whether omissions of letters after letters in non-final form are avoided. The <b>predominant</b> <b>error</b> type in LPD is migration of letters within words. We tested whether migrations also occur when they cause form change of either final-form letters that move to middle position or middle-form letters that move to final position. These questions were assessed in both acquired and developmental neglexia and LPD. The results indicated a strong effect of final letter-form on acquired neglexia and on acquired and developmental LPD, which almost completely prevented form-changing errors. This effect was not found in developmental neglexia, where words that end in final-form letters were actually more impaired than other words, probably because final-form letters appear only on the neglected side of the word for Hebrew-reading children with left developmental neglexia. These data show that early visuo-orthographic analysis is sensitive to final letter form and that final letter form constrains errors in peripheral dyslexia...|$|R
40|$|The {{present study}} used free-recollection tasks {{exploring}} whether the phonology {{took part in}} the processing when the children express their meanings from the mental semantic system to graphic forms. One hundred and fifteen subjects from 2, 3, and 5 grades participated in this study. We found that the homophonic error is high in recollection writing task and during the development of reading system the homophonic <b>error</b> is still <b>predominant</b> among total <b>errors,</b> which suggested that the phonology {{plays an important role in}} children mental lexicon development and it helps to form the association of semantic system and orthography. IUPsy...|$|R
40|$|BACKGROUND Notions {{about the}} most common errors in {{medicine}} currently rest on conjecture and weak epidemiologic evidence. We sought to determine whether cascade analysis is of value in clarifying the epidemiology and causes of errors and whether physician reports {{are sensitive to the}} impact of errors on patients. METHODS Eighteen US family physicians participating in a 6 -country international study fi led 75 anonymous error reports. The narratives were examined to identify the chain of events and the <b>predominant</b> proximal <b>errors.</b> We tabulated the consequences to patients, both reported by physicians and inferred by investigators. RESULTS A chain of errors was documented in 77 % of incidents. Although 83 % of the errors that ultimately occurred were mistakes in treatment or diagnosis, 2 of 3 were set in motion by errors in communication. Fully 80 % of the errors that initiated cascades involved informational or personal miscommunication. Examples of informational miscommunication included communication breakdowns among colleagues and with patients (44 %), misinformation in the medical record (21 %) ...|$|R
40|$|Objective: A {{systematic}} {{cross-sectional study}} {{conducted in a}} South-Indian Medical College to establish refractive errors as a plebeian problem in young medical students. Background: Increase in myopia prevalence rates posing {{a threat to the}} health and economy of the developing countries. There is ample evidence in the ophthalmic literature to support the classic view of association of myopia with the learned people. However, there are also suggestions in regard to the role of environmental, nutritional, hereditary and work associations for this dramatic increase in myopia. Method: In the present study, the medical students of NRI Medical College were studied for their refractive errors. In addition, their gender distributions, heights, weights, body mass indices were also studied in a batch-wise manner. Information about the refractive errors of eye of their parents are gathered through interviewing these medial students. Results and Conclusion: Our observations suggest that – (a) myopia is the <b>predominant</b> refractive <b>errors</b> among the medical students; (b) the numbers of myopic students in a batch of medical students are increasing year by year; (c) there is no apparent bias towards either gender and (d) majority of the parents of myopic medical students are also found to be myopic...|$|R
40|$|Thesis (MMed.) -Univerity of KwaZulu-Natal, Durban, 2009. Introduction Sputum smears stained by the Ziehl-Neelsen method are {{the least}} {{expensive}} tool for diagnosing patients with infectious tuberculosis. However, false positive and false negative results have serious implications for treatment of patients. Therefore, controlling the quality of sputum microscopy services is important {{to ensure that the}} laboratory produce results that are accurate, reliable and reproducible. Aim The aim {{of the study was to}} determine the quality of tuberculosis smear microscopy in public health laboratories in KwaZulu-Natal between the years 2001 and 2006, and to assess the current knowledge and attitude of laboratory workers and laboratory managers to proficiency testing as a quality assurance technique. Methods A secondary analysis of laboratory proficiency testing results, from the KwaZulu-Natal reference laboratory (2001 to 2004) and from the National Health Laboratory Services reference laboratory (2006), was performed. Key informant interviews were conducted to determine the role proficiency testing played as a quality assurance technique. Results Overall laboratory performance was 93 % from 2001 to 2004 and 98 % in 2006. High false negative results were the <b>predominant</b> <b>error.</b> Sensitivity and specificity improved from 91 % (for both) in 2001 to 2004 to 97 % (for both) in 2006. Overall performance of primary, district and tertiary health care levels were 92 %, 93 % and 73 % respectively in the period 2001 to 2004 and 98 %, 98 % and 94 % respectively in 2006. There was significant (p< 0. 01) improvement in both urban (97 %) and rural (98 %) laboratory performance in 2006. The overall scores by year ranged from 89 % (2002) to 98 % (2006), but the annual overall scores (2001 to 2006) only achieved the acceptable level twice. Key informants indicated that proficiency testing was an essential exercise, however, they reported challenges such as inconsistent feedback, high workload and need for training. Discussion Overall performance improved from an unacceptable level of 93 % (2001 - 2004) to a satisfactory level of 98 % (2006). Likely reasons include improvement in technical skills of microscopists and improvement in preparation of proficiency testing slides. Proficiency testing is considered an essential exercise to improve laboratory performance, however, participants know that they are being tested and may give 'special attention' to proficiency testing slides resulting in a social desirability bias. Recommendations A blinded rechecking programme should be established in conjunction with the use of a standardised checklist during support visits. Feedback, communication and staff training should be improved while the workload should be evaluated...|$|R
40|$|Aim: This study {{aimed at}} {{determining}} the prevalence and pattern of ophthalmic disorders among students of School for the Deaf, Akure, Ondo State, Nigeria. Methodology: This is a cross sectional descriptive {{study was conducted}} in October, 2011 as part of activities marking the Annual Physicians’ week of Nigerian Medical Association (NMA),Ondo State. Ethical clearance was obtained from the Ethical Review Committee of Federal Medical Centre, Owo prior to commencement of this study. The permission of the School Authority was also obtained before the commencement of this study. The respondents were selected by simple random sampling technique. All enrolled participants were interviewed with the aid of the study instrument (questionnaire) by the authors and interpreters (school teachers). Results: The respondents comprised of 91 (56. 9 %) Males and 69 Females (43. 1 %). Nearly all the respondents; 158 (98. 8 %) were deaf and dumb. Most respondents; 116 (72. 5 %) had ocular examination in the past. Few respondents; 118 (73. 75 %) had ophthalmic disorder. The commonest ophthalmic disorder was refractive error which was found in 16 respondents (38. 1 %). Myopia was diagnosed in 9 respondents. CONCLUSION: Most of the respondents were deaf and dumb. Few respondents had ophthalmic disorder. The commonest ophthalmic disorder was refractive error. Myopia was the most <b>predominant</b> refractive <b>error.</b> There is need for periodic ocular screening and treatment atthe School for the Deaf...|$|R
40|$|Two {{different}} approaches {{are considered to}} the problem of determining seismic velocity ratios from P and S arrival data from a group of earthquakes. Each method arises naturally from a different assumption about the sources of error. If the S reading <b>error</b> is the <b>predominant</b> source of <b>error,</b> S versus P regression, an extension of the Wadati Plot method to pooled data, should be used. If, however, S reading errors do not dominate, a symmetric regression should be used. A natural extension of the latter method allows the determination of station-by-station variation of a/ft. The methods are illustrated with data from an aftershock sequence which occurred off the coast of northern California. The average velocity ratio is not as well determined as the station-by-station variation about the average. However, the results show that the method has some utility in the study of regional variation of seismic velocities...|$|R
40|$|Due to the {{increasing}} throughput of current DNA sequencing instruments, sample multiplexing is necessary for making economical use of available sequencing capacities. A widely used multiplexing strategy for the Illumina Genome Analyzer utilizes sample-specific indexes, which are embedded {{in one of the}} library adapters. However, this and similar multiplex approaches come with a risk of sample misidentification. By introducing indexes into both library adapters (double indexing), we have developed a method that reveals the rate of sample misidentification within current multiplex sequencing experiments. With ~ 0. 3 % these rates are orders of magnitude higher than expected and may severely confound applications in cancer genomics and other fields requiring accurate detection of rare variants. We identified the occurrence of mixed clusters on the flow as the <b>predominant</b> source of <b>error.</b> The accuracy of sample identification is further impaired if indexed oligonucleotides are cross-contaminated or if indexed libraries are amplified in bulk. Double-indexing eliminates these problems and increases both the scope and accuracy of multiplex sequencing on the Illumina platform...|$|R
40|$|Icelandic and Norwegian {{past tense}} {{morphology}} contain strong patterns of inflection and two weak patterns of inflection. We report {{the results of}} an elicitation task that tests Icelandic and Norwegian children’s knowledge of the past tense forms of a representative sample of verbs. This cross-sectional study of four-, six- and eight-year-old Icelandic (nfl 92) and Norwegian (nfl 96) children systematically manipulates verb characteristics such as type frequency, token frequency and phonological coherence – factors that are generally considered to have an important impact on the acquisition of inflectional morphology in other languages. Our findings confirm that these factors {{play an important role in}} the acquisition of Icelandic and Norwegian. In addition, the results indicate that the <b>predominant</b> source of <b>errors</b> in children shifts during the later stages of development from one weak verb class to the other. We conclude that these findings are consistent with the view that exemplar-based learning, whereby patterns of categorization and generalization are driven by similarity to known forms, appropriately characterizes the acquisition of inflectional systems by Icelandic and Norwegian children...|$|R
40|$|Satellite {{has been}} {{identified}} as a potential candidate to meet the explosive Internet demand and to evolve the global Internet services. With a view to combat channel <b>errors</b> <b>predominant</b> in satellite based networks, TCP with Adaptive Flow Control and Delayed Fast Recovery (TCP-AFC) has been designed to identify a random loss with the help of selective acknowledgments. TCP-AFC has demonstrated significant performance enhancement in error prone environments through simulations and experiments on an active emulated network. In order to substantiate the improvement, we investigate the performance of TCP-AFC in a real environment consisting of a Ku band satellite link, which is more susceptible to atmospheric conditions. This paper focuses on evaluation of TCP-AFC in real life situations having appreciable channel noise and delay. Results of the extensive experiments conducted on a test bed consisting of a symmetric GEO satellite link for different channel conditions, data volume, data type and data traffic are presented in this paper. Analysis of the results reconfirms the compatibility of TCP-AFC in a heterogeneous network besides the performance improvement over a dedicated satellite link...|$|R
40|$|Telomeric DNA {{consists}} of short, tandemly repeated sequences {{at the ends}} of chromosomes. Telomeric DNA in the ciliate Paramecium tetraurelia is synthesized by an error-prone telomerase with an RNA template specific for GGGGTT repeats. We have previously shown that misincorporation of TTP residues at the telomerase RNA templating nucleotide C 52 accounts for the 30 % GGGTTT repeats randomly distributed in wild-type telomeres. To more completely characterize variable repeat synthesis in P. tetraurelia, telomerase RNA genes mutated at C 52 (A, U, and G) were expressed in vivo. De novo telomeric repeats from transformants indicate that the <b>predominant</b> TTP misincorporation <b>error</b> seen in the wild-type telomerase is dependent on the presence of a C residue at template position 52. Paradoxically, the effects of various other telomerase RNA template and alignment region mutations on de novo telomeres include significant changes in fidelity, as well as the synthesis of aberrant, 5 -nucleotide telomeric repeats. The occurrence of deletion errors and the altered fidelity of mutated P. tetraurelia telomerase, in conjunction with misincorporation by the wild-type enzyme, suggest that the telomerase RNA template domain may be analogous to homopolymeric mutational hot spots that lead to similar errors by the human immunodeficiency virus proofreading-deficient reverse transcriptase...|$|R
40|$|Human error {{remains a}} major cause of several {{accidents}} in the oil and gas (O&G) industry. While human error has been analysed in several industries and has been at the centre of many debates and commentaries, a detailed, systematic and comprehensive analysis of human error in the O&G industry has not yet been conducted. Hence, this report aims to use the Technique for Retrospective and Predictive Analysis of Cognitive Errors (TRACEr) to analyse historical accidents in the O&G industry. The study has reviewed 163 major and/or fatal O&G industry accidents that occurred between 2000 and 2014. The results obtained have shown that the <b>predominant</b> context for <b>errors</b> was internal communication, mostly influenced by factors of perception. Major accident events were crane accidents and falling objects, relating to the most dominant accident type: ‘Struck by’. The main actors in these events were drillers and operators. Generally, TRACEr proved very useful in identifying major task errors. However, the taxonomy was less useful in identifying both equipment errors and errors due to failures in safety critical control barriers and recovery measures. Therefore, {{a modified version of the}} tool named Technique for the Retrospective and Predictive Analysis of Cognitive Errors for the Oil and Gas Industry (TRACEr-OGI) was proposed and used. This modified analytical tool was consequently found to be more effective for accident analysis in the O&G industry...|$|R
40|$|Abstract. High-frequency (HF) {{radar systems}} can provide periodic, two-dimensional, vector current {{estimates}} over an area approaching 1000 km •. As {{the use of}} these HF systems has gained wider acceptance, a number of {{attempts have been made to}} estimate the accuracy of such systems. However, comparisons of HF radar current estimates with in situ sensors are difficult to interpret since HF systems measure currents averaged over an area of- 1 km 2 and to a depth of only- 50 cm while in situ sensors measure currents at a point and somewhat greater depths (- 1 to 10 m). Previous studies of the accuracy of HF radar technology have thus attributed the differences observed between HF radar and in situ sensors to an unknown combination of vertical shear, horizontal inhomogeneity, in situ instrument errors, and HF radar system errors. This study examines the accuracy of HF radar current measurements using data from the 1993 High Resolution Remote Sensing Experiment, conducted off Cape Hatteras, North Carolina. Data from four shipborne in situ current meters are compared with data from an Ocean Surface Current Radar (OSCR), a commercial current-measuring radar. We attempt to discern the <b>predominant</b> sources of <b>error</b> in these data by using multiple simultaneous measurements from different sensors and by examining the variation of observed current differences as a function of location. The results suggest an upper bound on the accuracy of the OSCR-derived radial currents of 7 to 8 cm/s. 1...|$|R
40|$|Aim. The {{study was}} {{aimed to explore}} the {{consciousness}} of medical error among health professionals stratified by clinicians and surgeons, within the Teaching Hospital Gemelli in Rome. Methods. A questionnaire, consisting of two questions about most serious and frequent mistakes, was administered to participants of a course. Data were collected, stratified in macro-categories and analysed applying c 2 -test. Statistical significance was set at P≤ 0. 05. Results. A total number of 988 questionnaires was collected; 2340 errors referred as the most serious and 2111 as the most frequent were indicated. The errors considered as the most serious are represented by: exchanges (18. 7 %); therapy administration (16. 5 %); communication (11. 3 %). The errors considered as the most frequent are represented by: communication (15. 2 %); exchanges (14. 2 %); therapy administration (11. 5 %); prescription (10. 9 %). The analysis of error macro-categories showed significant differences between clinical and surgical departments for both the most serious (P= 0. 038) and the most frequent errors (P= 0. 004). In particular procedures/protocols mistakes represent the errors mostly perceived as frequent with the highest percentage (29 %) among clinicians, but not among surgeons (23. 4 %). Conclusion. Differences between clinicians and surgeons about procedures/protocols mistakes {{may be due to}} an higher standardization of surgical processes than clinical, which contributes to a reduction of the number of related errors and near misses. The weight of communication and information seems to be <b>predominant</b> versus other <b>error</b> classes. The engagement is to promote and encourage clinical risk management culture and to try Clinical Governance tools allowing to monitor, limit and prevent medical errors...|$|R
