219|275|Public
2500|$|A large {{quantity}} of field data is desirable for PVA; some conservatively estimate {{that for a}} precise extinction <b>probability</b> <b>assessment</b> extending T years into the future, five-to-ten times T years of data are needed. [...] Datasets of such magnitude are typically unavailable for rare species; {{it has been estimated}} that suitable data for PVA is available for only 2% of threatened bird species. [...] PVA for threatened and endangered species is particularly a problem as the predictive power of PVA plummets dramatically with minimal datasets. [...] Ellner et al. (2002) argued that PVA has little value in such circumstances and is best replaced by other methods. [...] Others argue that PVA remains the best tool available for estimations of extinction risk, especially with the use of sensitivity model runs.|$|E
5000|$|Hall, J., Held, H., Dawson, R., Kriegler, E. and Schellnhuber, H. J. (2009). Imprecise <b>probability</b> <b>assessment</b> of tipping {{points in}} the climate system. PNAS (special feature) 106, 5041 ...|$|E
50|$|A fourth {{reason to}} embrace science {{is that it}} can help deliver justice in a court of law, via DNA {{fingerprinting}} or even via simple statistical reasoning.Everyone should learn the scientist's art of <b>probability</b> <b>assessment,</b> to make better decisions.|$|E
40|$|In {{this paper}} we {{consider}} numerical and qualitative <b>probability</b> <b>assessments</b> on finite families of conditional events. We assume {{the existence of}} suitable subsets of the set of constituents. We give two results on coherence and generalized coherence of numerical <b>probability</b> <b>assessments.</b> Then, we give some conditions for the coherence of qualitative <b>probability</b> <b>assessments.</b> In the numerical case we verify the solvability of suitable linear systems used for the checking of coherence, or generalized coherence, of numerical <b>probability</b> <b>assessments.</b> In the qualitative case we find precise probabilities which represent the qualitative orderings. We illustrate our results by examining some examples...|$|R
25|$|The {{discovery}} of rigorous methods {{to assess and}} combine <b>probability</b> <b>assessments</b> has changed society. It is important for most citizens to understand how <b>probability</b> <b>assessments</b> are made, and how they contribute to decisions.|$|R
40|$|In {{this paper}} we study a {{probabilistic}} logic {{based on the}} notion of coherence of de Finetti. We first recall the notion of coherence for precise conditional <b>probability</b> <b>assessments,</b> then we consider its generalization to the case of imprecise assessments. We examine conditional probabilistic knowledge bases associated with imprecise <b>probability</b> <b>assessments</b> defined on arbitrary families of conditional events. In our probabilistic logic the notion of probabilistic interpretation is directly defined in terms of precise conditional <b>probability</b> <b>assessments.</b> Then, we give in our approach new proofs of some results obtained in probabilistic default reasoning...|$|R
5000|$|Calibrated <b>probability</b> <b>assessment.</b> This is {{a method}} for {{training}} estimators and experts (who are relied on for the inputs in Monte Carlo methods) to be neutrally confident about their assigned probabilities. [...] That is, their probabilities are neither overconfident (too high) nor underconfident (too low).|$|E
50|$|In attacking or {{defending}} from enemy forces, {{the player}} receives {{an option to}} either go into an RTS game to try to defeat the opposing enemy in real-time, or to let the computer compute a quick resolve based on the chances of victory for the player. The outcome of a real-time strategy resolution is entirely based on a player's skill, while computerized battle resolution is a random roll based on a <b>probability</b> <b>assessment</b> resulting from a comparison between the attacker's and defender's forces situated in the contested territory.|$|E
5000|$|Douglas Hubbard {{has also}} {{conducted}} {{a sample of}} over 400 retired claims which showed that the probability of an event is close to its market price but, more importantly, significantly closer than the average single subjective estimate. However, he also shows that this benefit is partly offset if individuals first undergo calibrated <b>probability</b> <b>assessment</b> training {{so that they are}} good at assessing odds subjectively. The key benefit of the market, Hubbard claims, is that it mostly adjusts for uncalibrated estimates and, at the same time, incentivizes market participants to seek further information.|$|E
40|$|The {{concept of}} totally {{coherent}} set-valued <b>probability</b> <b>assessments</b> {{on a family}} of conditional events has been introduced in a previous paper by the same authors. Here we examine the extendibility of interval-valued <b>probability</b> <b>assessments</b> and in this context some theoretical results are proved. These results are applied {{in the analysis of}} some well known inference rules of interest in the field of nonmonotonic reasoning...|$|R
30|$|The {{presented}} {{model is}} not limited to the formulation and solving of decision problems where (solely) the utility values are ambiguous. Analogously, we can use its basic concept to formalize and solve problems, where, e.g., <b>probability</b> <b>assessments</b> or both, utility and <b>probability</b> <b>assessments</b> are imprecise. The latter case has been examined in detail by Metzger and Spengler (2017). This work also presents a comprehensive discussion on interdependencies between selected fuzzy measures and i-fuzzy values, used as substitutes for probability measures.|$|R
5000|$|When {{dealing with}} {{personal}} <b>probability</b> <b>assessments,</b> or supposed <b>probabilities</b> derived in nonstandard ways, it is a property of self-consistency across {{a whole set}} of such assessments.|$|R
50|$|A large {{quantity}} of field data is desirable for PVA; some conservatively estimate {{that for a}} precise extinction <b>probability</b> <b>assessment</b> extending T years into the future, five-to-ten times T years of data are needed. Datasets of such magnitude are typically unavailable for rare species; {{it has been estimated}} that suitable data for PVA is available for only 2% of threatened bird species. PVA for threatened and endangered species is particularly a problem as the predictive power of PVA plummets dramatically with minimal datasets. Ellner et al. (2002) argued that PVA has little value in such circumstances and is best replaced by other methods. Others argue that PVA remains the best tool available for estimations of extinction risk, especially with the use of sensitivity model runs.|$|E
50|$|For example, if it {{is unknown}} {{whether or not it}} will rain tomorrow, then there is a state of uncertainty. If probabilities are applied to the {{possible}} outcomes using weather forecasts or even just a calibrated <b>probability</b> <b>assessment,</b> the uncertainty has been quantified. Suppose it is quantified as a 90% chance of sunshine. If there is a major, costly, outdoor event planned for tomorrow then there is a risk since there is a 10% chance of rain, and rain would be undesirable. Furthermore, if this is a business event and $100,000 would be lost if it rains, then the risk has been quantified (a 10% chance of losing $100,000). These situations can be made even more realistic by quantifying light rain vs. heavy rain, the cost of delays vs. outright cancellation, etc.|$|E
5000|$|Suppose an {{individual}} wishes {{to make a}} probability assignment among m mutually exclusive propositions. She has some testable information, but is {{not sure how to}} go about including this information in her <b>probability</b> <b>assessment.</b> She therefore conceives of the following random experiment. She will distribute N quanta of probability (each worth 1/N) at random among the m possibilities. (One might imagine that she will throw N balls into m buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, she will check if the probability assignment thus obtained is consistent with her information. (For this step to be successful, the information must be a constraint given by an open set in the space of probability measures). If it is inconsistent, she will reject it and try again. If it is consistent, her assessment will be ...|$|E
40|$|This study {{provides}} exploratory evidence on auditors’ framing {{and evaluation of}} hypotheses, identifies implications for improving audit decision-making and facilitates the interpretation of prior research. Prior studies usually assume hypotheses to be framed as mutually exclusive and exhaustive. However, both verbal protocol evidence and <b>probability</b> <b>assessments</b> reveal that in a realistic case most auditors frame the hypotheses as a non-mutually exclusive and exhaustive set of causes. Further, auditor <b>probability</b> <b>assessments</b> tend to reflect multiple causes. Finally, exploratory analyses indicate auditors have difficulty in updating assessments consistent with the perceived interrelationships between hypotheses...|$|R
40|$|This {{article will}} {{consider}} Hogarth's 1975 assessment that "man is a selective, sequential information processing system with limited capacity,... ill-suited for assessing probability distributions. " Particular attention {{will be paid}} to when people make normatively "good" or "poor" <b>probability</b> <b>assessments,</b> what techniques are effective in eliciting "good," coherent <b>probability</b> <b>assessments,</b> and on how these ideas {{are relevant to the}} practicing Bayesian statistician. While there are situations where experts can make well-calibrated judgments, it will be argued that more research needs to be done into the effects of expertise, training, and feedback...|$|R
5000|$|Expert Judgment (EJ) denotes a {{wide variety}} of {{techniques}} ranging from a single undocumented opinion, through preference surveys, to formal elicitation with external validation of expert <b>probability</b> <b>assessments.</b> Recent books are ...|$|R
5000|$|Single instance. It is {{possible}} to reexamine the procedures governing a specific hiring decision, {{see if they were}} followed, and re-evaluate the selection by asking questions such as: was it fair? were fair procedures followed? was the best applicant selected? This is a judgment call and it {{is possible}} that biases may enter into the minds of decision-makers. The determination of equality of opportunity in such an instance is based on mathematical probability: if equality of opportunity is in effect, then it's seen as fair if each of two applicants has a 50% chance of winning the job, that is, they both have equal chances to succeed (assuming of course that the person making the <b>probability</b> <b>assessment</b> is unaware of all variables - including valid ones such as talent or skill as well as arbitrary ones such as race or gender.) But it is hard to measure whether each applicant had, in fact, a 50% chance, based on the outcome.|$|E
5000|$|After {{independence}} of the country, he found the central government to be exploitative and not as cooperative towards the East Pakistan Province as he had hoped. Thus, he joined the Ooposition by becoming an active leader of the Krishok Proja Party and thereby of the Jukta Front coalition. In the decisive election of 1954, Mohon Mia had played {{a vital role in}} suggesting and selecting candidates for the Jukta Front across whole of East Pakistan. It so happened that in the election, the candidates of Jukto Front landed a landslide victory winning in almost all the seats but a few. Mohon Mia was largely credited for this success as most of the victors were relatively new to politics as compared to their competitors. So, {{the fact that they were}} shortlisted so cleverly, and the accuracy of the winning <b>probability</b> <b>assessment</b> gained Mohon Mia the nickname of the [...] "King Maker of East Pakistan". During this time he also worked closely with Bangabandhu Sheikh Mujibur Rahman as well and the two became rather good friends. Mohon Mia had also become a Provincial Minister in the coalition Provincial Government at that time. However, that unity in the coalition quickly eroded away due to inter party conflicts and that ended up causing embitterment in his relations with Bangabndhu as well. But Mohon Mia remained in the opposition, though not with the mainstream opposition which was gradually being led by the Awami League.|$|E
40|$|We {{introduce}} {{the concept of}} total coherence of a set-valued <b>probability</b> <b>assessment</b> on a family of conditional events. In particular we give sufficient and necessary conditions of total coherence {{in the case of}} interval-valued probability assessments. Some relevant cases in which the set-valued <b>probability</b> <b>assessment</b> is represented by the unitary hypercube are also considered. 1...|$|E
30|$|In this paper, {{we focus}} on the {{formulation}} and solving of decision problems where (solely) the utility values are ambiguous. Analogously we can use its basic concept to formalize and solve problems, where, e.g., <b>probability</b> <b>assessments</b> or both, utility and <b>probability</b> <b>assessments</b> are imprecise. In order to elicit imprecise utility assessments of decision-makers, it is possible to apply an adapted version of the classical Bernoulli game [based on Ramsey’s (1926) work]. Imprecise probability values can be derived, e.g., from interval-valued probability judgements (see e.g., Metzger and Spengler 2017). Respective applications in intuitionistic fuzzy contexts can be addressed in further research projects.|$|R
40|$|Five {{case studies}} are {{analysed}} in depth and other investigations {{are carried out}} in order to answer questions concerned with: {{the nature of the}} distributions which are output from risk evaluation models. the important features of. the distributions which are input to risk evaluation models. the accuracy with which different methods for assessing subjective probability distributions are capable of providing the inputs to risk evaluation models. (iv) the way in which dependencies should be dealt with in risk evaluation models. and (v) {{the extent to which it}} is possible to distinguish important <b>probability</b> <b>assessments</b> from unimportant <b>probability</b> <b>assessments</b> in risk evaluation models...|$|R
40|$|We {{introduce}} a fully parametric approach for updating beliefs regarding correlated binary variables, after marginal <b>probability</b> <b>assessments</b> {{based on information}} of varying quality are provided by an expert. This approach allows for the calculation of a predictive joint density for future assessments. The proposed methodology offers new insight into the parameters that control the dependence of the binary variables, and the relation of these parameters to the joint density of the <b>probability</b> <b>assessments.</b> A comprehensible elicitation procedure for the model parameters is put forward. The approach taken is motivated and illustrated through a practical application. Copyright (c) 2007 Board of the Foundation of the Scandinavian Journal of Statistics [...] ...|$|R
40|$|To assess probabilities in {{decision}} analysis, and for {{decision making in}} general, decision makers must evoke and apply relevant information. Decision analysts have developed a variety of structuring tools to aid decision makers in these tasks, including influence diagrams and knowledge maps. However, despite their pervasive use in practice, {{there have been no}} reported empirical tests of these tools. One goal of the present research was to provide an empirical test of the evocative knowledge map methodology. Second, a theoretical analysis of <b>probability</b> <b>assessment</b> was used to develop a new prescriptive elicitation technique. This technique uses a theoretically-grounded set of directed questions to help decision makers evoke information for <b>probability</b> <b>assessment.</b> Experimental results showed that both the knowledge map and the new directed questions methodology elicited a higher quantity and quality of information from decision makers engaged in <b>probability</b> <b>assessment</b> tasks than did a control condition. Further, the information elicited by the two techniques was qualitatively different, suggesting that the two methods might profitably be used as complementary elicitation techniques. <b>probability</b> <b>assessment,</b> decision analysis, knowledge maps, practical reasoning, knowledge evocation, protocol analysis...|$|E
3000|$|... 1.- Chawla et al. Renal angina: {{concept and}} {{development}} of pretest <b>probability</b> <b>assessment</b> in acute kidney injury. Critical Care (2015) 19 : 93.|$|E
3000|$|... 1. Chawla L, Goldstein S, Kellum J, Ronco C. Renal Agina: {{concept and}} {{development}} pretest <b>probability</b> <b>assessment</b> in acute kidney injury. Critical Care. 2015; 19 : 93.|$|E
40|$|In {{this paper}} the {{concepts}} of partial and complete logical dependence among conditional events are studied and a comparison is made with some definitions given by other authors. Then, some logical and probabilistic relations are considered {{in the framework of}} coherent probabilistic assessments. A particular class of logically dependent conditional events is considered and, based on some results on coherence of precise and imprecise <b>probability</b> <b>assessments,</b> the probabilistic relations implied by logical dependence are studied. It is shown that logical dependence does not necessarily imply probabilistic relations. Finally, given two logically independent events, we examine the coherence of some <b>probability</b> <b>assessments</b> which are relevant to the property of stochastic independence...|$|R
40|$|Standard axiomatizations of expected-utility theory {{envision}} {{an agent}} with fixed <b>probability</b> <b>assessments</b> {{who can be}} observed to choose actions from varying opportunity sets (for instance, pairs of lotteries). These axiomatizations also envision that the agent's preferences among these actions depend {{on the state of}} nature only through clearly defined and observable consequences. This viewpoint may be unnecessarily restrictive as a basis for applying and evaluating the theory. The authors study instead the pattern of choices from a fixed set of actions as <b>probability</b> <b>assessments</b> change. Convexity and integrability conditions characterize maximization of expected state-dependent utility. Copyright 1991 by The Review of Economic Studies Limited. ...|$|R
40|$|The <b>probability</b> <b>assessments</b> of a Bayesian belief network {{generally}} involve inaccuracies. These inaccuracies {{influence the}} reliability of the network's output. An integral part of investigating {{the reliability of}} output is to study robustness. Robustness pertains {{to the extent to which}} varying the <b>probability</b> <b>assessments</b> of a Bayesian belief network influences its output. Output robustness is studied by subjecting the network to a sensitivity analysis. In this paper, we address the issue of robustness of a belief network's output in view of the threshold model for decision making. We present a method for sensitivity analysis that provides for the computation of bounds between which a network's assessments can be varied without inducing a change in recommended decision...|$|R
40|$|ABayesian {{network is}} a {{probabilistic}} representation for uncertain relationships, which {{has proven to}} be useful for modeling real-world problems. When there are many potential causes of a given e ect, however, both <b>probability</b> <b>assessment</b> and inference using a Bayesian network can be di cult. In this paper, we describe causal independence, a collection of conditional independence assertions and functional relationships that are often appropriate to apply to the representation of the uncertain interactions between causes and e ect. We show how the use of causal independence in a Bayesian network can greatly simplify <b>probability</b> <b>assessment</b> aswell as probabilistic inference. ...|$|E
40|$|A <b>probability</b> <b>assessment</b> {{framework}} is outlined for an organizational decision involving a conditioning event (CE). The decision may, for example, involve a new-product launch (strategic decision) {{dependent on the}} outcome of market research (CE). The framework illustrates how Bayesian revision could be employed as related "news" arrives intermittently to revise current probabilities prior to decision implementation. A unique contribution of this paper is its utilization of the analytic hierarchy process to ascertain a set of consistent and coherent probabilities for the event/sample spaces at all stages of the decision process. Strategic decision making <b>Probability</b> <b>assessment</b> Analytic hierarchy process Bayesian revision...|$|E
40|$|Abstract- A Bayesian {{network is}} a {{probabilistic}} representation for uncertain relationships, which {{has proven to}} be useful for modeling realworld problems. When there are many potential causes of a given effect, however, both <b>probability</b> <b>assessment</b> and inference using a Bayesian network can he difficult. In this paper, we describe causal independence, a collection of conditional independence assertions and functional relationships that are often appropriate to apply to the representation of the uncertain interactions between causes and effect. We show how the use of causal independence in a Bayesian network can greatly simplify <b>probability</b> <b>assessment</b> as well as probabilistic inference. I...|$|E
40|$|Assessing the {{likelihood}} of future events is core to technical risk management at Scottish Power Generation (SPG). Events can include failures resulting in unavailability of key assets, or incidents impacting staff safety or the environment. Eliciting probabilities from engineers to quantify {{the likelihood}} of future, uncertain events is challenging given the diversity of assets across the multiple, heterogeneous power plants operated by SPG. Such <b>probability</b> <b>assessments</b> inform investment decisions intended to manage technical risks and support regulatory compliance. Through interviews with engineers we reveal the opportunities for heuristics and bias that {{explain some of the}} historical disparities in assessments intuitively evident to risk managers. We propose better ways of obtaining judgemental <b>probability</b> <b>assessments</b> based on a study involving engineers and a control group of post-experience students. We find that the choice of scale descriptors impacts the probability values of defined events as judged by engineers. Consequently we suggest changes to the risk management system, including new design features to better frame and capture <b>probability</b> <b>assessments.</b> As a consequence of our study, the technical risk management process is being enhanced {{in a number of ways}} including the creation of a single organisational-wide framework, clearer guidelines, and better knowledge management...|$|R
40|$|Abstract. Jonathan Weisberg {{claims that}} certain <b>probability</b> <b>assessments</b> {{constructed}} by Jeffrey conditioning resist subsequent revision {{by a certain}} type of after-the-fact defeater of the reasons supporting those assessments, and that such conditioning is thus “inherently anti-holistic. ” His analysis founders, however, in applying Jeffrey conditioning to a partition for which an essential rigidity condition clearly fails. Applied to an appropriate partition, Jeffrey conditioning is amenable to revision by the sort of after-the-fact defeaters considered by Weisberg in precisely the way that he demands. Key words: defeater, holism, Jeffrey conditioning, rigidity. 1. Holism Denied. Confirmational holism requires that a belief’s empirical justification be sensitive to background belief. For Bayesian epistemology this entails among other things that your <b>probability</b> <b>assessments</b> be amenable to revision in response to the discovery of after-thefact defeaters of reasons supporting those assessments. Jonathan Weisberg (2009) thinks that a certain class of <b>probability</b> <b>assessments</b> constructed by Jeffrey conditioning resist such revision, and that such conditioning is thus “inherently anti-holistic. ” Typifying such cases is, he claims, the following example: Suppose that E asserts that a certain jelly bean is red, and F assert...|$|R
40|$|This paper {{proposes a}} set of axioms for {{combining}} expert <b>probability</b> <b>assessments.</b> The axioms {{provide a framework for}} understanding previous results, and are used to derive a new method for combining event probabilities from different sources into a single estimate. expert, expert resolution, decision analysis...|$|R
