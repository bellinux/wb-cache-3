1|6|Public
40|$|Up-to-date {{progress}} and developments in low-power and low-cost wireless communication tied together with advancement in ad-hoc networking routing and protocols have fashioned WSN’S a hot theme of vigorous attention. Usually, hardware modules in a WSN comprise analog-to-digital converters, sensor circuits, wireless communications transceivers and microprocessors. These hardware modules/components {{ought to be}} planned utilizing software tools to maneuver agreeably {{on the way to}} bring about a user-defined job. The major goals of such a network are data acquirement and Radio Frequency transmission. In this research article, we bring in indispensable impressions in this promising multidisciplinary research area. Two categories of WSN’s applications in real-life are acknowledged. In regards to on-going exertion, a constructed and an all-inclusive devise framework of a smart wireless <b>patient-monitoring</b> <b>system</b> has been illustrated. Our preliminary purpose in this research article is probable to set off prospect research projects with a possible sketch for future work as well...|$|E
50|$|BPL {{initially}} {{expanded its}} medical product ranges to include electrocardiographs and <b>patient-monitoring</b> <b>systems.</b> After the 1982 Asian Games, BPL expanded its range further and manufactured colour televisions and video cassette recorders, and later refrigerators, batteries and other consumer electrical equipment.|$|R
40|$|Traditional {{real-time}} {{systems are}} usually used by very specific, mission-critical applications, {{such as a}} nuclear reactor monitor. For these applications task deadlines must not be missed. However, with the advances of computer and communication technology, and as computers become a commonplace utility, more applications are requiring a casual use of real-time technology. Task deadlines in these applications are soft {{in the sense that}} missing a deadline occasionally is, although undesirable, not a disaster. Example soft real-time applications include 800 -number translation, cellular telecommunication, financial databases, <b>patient-monitoring</b> <b>systems</b> and so on. In these applications, deadlines are used as a tool to express performance goals. For example, a buy-sell action triggered by a stock-price update in a financial database system should be implemented within a specified period of time due to the volatility of the market. To conserve resources, soft real-time systems are usually not [...] ...|$|R
40|$|Abstract Real-time {{systems are}} {{becoming}} increasingly widespread, often in safety-critical applica-tions. It is therefore crucial that these systems be correct; however, there are few automated tools for analyzing concurrency and timing properties of these systems. The PARTS toolset usesa Petri-net-based reachability analysis to analyze program specifications written in an Ada- 83 subset. Our simple time Petri nets are specifically aimed at facilitating real-time analysis. Inorder to control the state-explosion problem, P ARTS employs several optimization techniquesaimed at state-space reduction. In this paper we discuss our approach and we report on extensive experiments with several examples of real-time specifications based on Ada 83. Whenpossible, we also compare our experimental results with results obtained by other approaches to real-time analysis. 1 Introduction Real-time software is becoming increasingly widespread, including in safety-critical applications such as traffic-control systems, navigation <b>systems</b> and <b>patient-monitoring</b> <b>systems.</b> Real-time software {{is characterized by the}} existence of timing constraints on its behavior, such as deadlines on the completion of program computations. In many cases, real-time software is also physically distributed, meaning that different software units (e. g., processes) execute on different processors that communicate through a network. Because real-time software is often used in safety-critical applications, it is essential that this software be dependable; however, the development of reliable real-time programs is extremely difficult...|$|R
40|$|Data {{compression}} {{is important}} in the computing process because it helps to reduce the space occupied by a file, which normally leads to the reduction in the time taken to access the file. Files which may be compressed include text, images, video/speech and sound. Data compression algorithms have been developed and applied to several areas including neural networks, bioinformatics, database management systems, wireless systems, error detection and correction codes, fractals etc. Since computer/communication codes are binary texts, they may be compressed using binary text compression algorithms. By considering two binary texts which can be modeled as subsets of Unicode, namely the numerics of a 7 -bit subset of the Inf or mation Processing Code (IPC) and the numerics of the 7 -bit American Standard Code for Information Interchange (ASCII) respectively, this paper presents a comparative analysis of the compression properties of these two texts using a binary text compression algorithm. The algorithm is a generalization of a storage-based built-in test pattern generation method for reducing storage requirements and test application time in circuits. This algorithm is then applied to the design of an IPC to ASCII code converter. Code converters are circuits which accept input codes in one form and translate them to present equivalent values in a different format as the output code. They are useful in real-time control systems and data acquisition systems, such as aviation <b>systems</b> and <b>patient-monitoring</b> <b>systems</b> in hospitals, which require the use of different sensors to continuously monitor the computer. This paper thus presents an equivalent approach to the design of code converters via the well-known k-map method...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. The use of optical techniques in biomedical monitoring and diagnosis is becoming increasingly widespread, {{primarily because of the}} non-invasive nature of optically derived measurements. Physiological analysis is usually achieved by characterisation of the spectral or temporal properties of the interaction between light and the anatomy. Although some optical measurements require complex instrumentation and protocols, recent technological advances have resulted in robust and compact equipment that is now used routinely in a multitude of clinical contexts. Unfortunately, these measurements are inherently sensitive to corruption from dynamic physical conditions or external sources of light, inducing signal artefact. Artefact is the primary restriction in the applicability of many optical measurements, especially for ambulatory monitoring and tele-medicine. The most widely used optical measurement is photoplethysmography, a technique that registers dynamic changes in blood volume throughout the peripheral vasculature and can be used to screen for a number of venous disorders, as well as monitoring the cardio-vascular pulse wave. Although photoplethysmographic devices are now incorporated into many <b>patient-monitoring</b> <b>systems,</b> the prevalent application is a measurement known as pulse oximetry, which utilises spectral analysis of the peripheral blood to estimate the arterial haernoglobin oxygen saturation. Pulse oximetry is well established as an early warning for hypoxia and is now mandatory under anaesthesia in many countries. The problem of artefact is prominent in these continuous monitoring techniques, where it is often impossible to control the physical conditions during use. This thesis investigates the possibility of reducing artefact corruption of photoplethysmographic signals in real time, using an electronic processing methodology that is based upon inversion of a physical artefact model. The consequences of this non-linear artefact reduction technique for subsequent signal analysis are discussed, culminating in a modified formulation for pulse oximetry that not only has reduced sensitivity to artefact but also possesses increased generality. The design and construction of a practical electronic system is then used to explore both the implementation issues and the scope of this technique. The performance of artefact reduction obtained is then quantified under realistic experimental conditions, demonstrating that this methodology is successful in removing or reducing a large proportion of artefact encountered in clinically relevant situations. It is concluded that non-linear artefact reduction can be applied to any photoplethysmographic technology, reducing interpretation inaccuracies that would otherwise be induced by signal artefact. It is also speculated that this technology could enable the use of photoplethysmographic systems in applications that are currently precluded by the inherent severity of artefact...|$|R
40|$|Action {{recognition}} is crucial {{area of research}} in computer vision with wide range of applications in surveillance, <b>patient-monitoring</b> <b>systems,</b> video indexing, Human- Computer Interaction and many more. These applications require automated action recognition. Robust classification methods are sought-after despite influential {{research in this field}} over past decade. The data resources have grown tremendously owing to the advances in the digital revolution which cannot be compared to the meagre resources in the past. The main limitation on a system when dealing with video data is the computational burden due to large dimensions and data redundancy. Sparse and low rank approximation methods have evolved recently which aim at concise and meaningful representation of data. This thesis explores the application of sparse and low rank approximation methods in the context of video data classification with the following contributions. 1. An approach for solving the problem of action and gesture classification is proposed within the sparse representation domain, effectively dealing with large feature dimensions, 2. Low rank matrix completion approach is proposed to jointly classify more than one action 3. Deep features are proposed for robust classification of multiple actions within matrix completion framework which can handle data deficiencies. This thesis starts with the applicability of sparse representations based classifi- cation methods to the problem of action and gesture recognition. Random projection is used to reduce the dimensionality of the features. These are referred to as compressed features in this thesis. The dictionary formed with compressed features has proved to be efficient for the classification task achieving comparable results to the state of the art. Next, this thesis addresses the more promising problem of simultaneous classifi- cation of multiple actions. This is treated as matrix completion problem under transduction setting. Matrix completion methods are considered as the generic extension to the sparse representation methods from compressed sensing point of view. The features and corresponding labels of the training and test data are concatenated and placed as columns of a matrix. The unknown test labels would be the missing entries in that matrix. This is solved using rank minimization techniques {{based on the assumption that}} the underlying complete matrix would be a low rank one. This approach has achieved results better than the state of the art on datasets with varying complexities. This thesis then extends the matrix completion framework for joint classification of actions to handle the missing features besides missing test labels. In this context, deep features from a convolutional neural network are proposed. A convolutional neural network is trained on the training data and features are extracted from train and test data from the trained network. The performance of the deep features has proved to be promising when compared to the state of the art hand-crafted features...|$|R

