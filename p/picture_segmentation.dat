13|12|Public
40|$|This bachelor’s thesis {{describes}} {{basic methods}} of <b>picture</b> <b>segmentation</b> {{for the purpose}} of separation of text from a background. In first part are described general methods meant for picture processing. There is described verging, edge detection and interference reduction. In second part are defined categories of text in picture. Third part is focused on program which will help with <b>picture</b> <b>segmentation...</b>|$|E
40|$|ABSTRACT. In the past, <b>picture</b> <b>segmentation</b> {{has been}} {{performed}} by merging small primitive regions or by recursively splitting the whole picture. This paper combines {{the two approaches}} with significant increase in processing speed while maintaining small memory requirements. The data structure is described in detail and examples of implementations are given...|$|E
40|$|The Bachelor's {{paper is}} {{concerned}} with processing black-and-white, grey-toned documents, which means the conversion of grey-toned picture of the document into binary one. In the paper, the procedures of separating the variable-brightness text from the background are described. Diverse methods of detaching the foreground from the background, the so-called <b>picture</b> <b>segmentation,</b> are used. Moreover, the outcomes of different methods are compared...|$|E
40|$|In this paper, a novel {{class of}} wavelet transform, {{referred}} to as the multiresolution Hadamard representation (MHR), is proposed as a general framework for document image analysis. The two basis functions of MHR constitute a bar detector, serving as a tool for determining the width/scale of strokes in characters. In this article, we apply MHR to the extraction of half-tone <b>pictures,</b> <b>segmentation</b> of documents into text blocks, and determination of the scale of characters in each text block. The scale information is thus found very useful for document analysis. 1 Introduction Document image analysis is the first stage of the automatic transcription of printed documents into texts. It aims to pick up text information (e. g. strokes, characters, text lines or blocks) from digitized documents, which are usually noisy. To achieve this, a priori knowledge of the structure of signals is required. It has been a common practice in document analysis to extract features that are associated with [...] ...|$|R
40|$|Abstract: – We {{present an}} option for colour image {{segmentation}} applied to printing quality assessment in offset lithographic printing by measuring an average ink dot size in halftone <b>pictures.</b> The <b>segmentation</b> is accomplished in two stages through classification of image pixels. In the first stage, rough image segmentation is performed. The results of the first segmentation stage are then utilized to collect a balanced training data set for learning refined parameters of the decision rules. The developed software is successfully used in a printing shop to assess the ink dot size on paper and printing plates...|$|R
40|$|Despite {{several decades}} of {{research}} into segmentation techniques, automated medical image segmentation is barely usable in a clinical context, and still at vast user time expense. This paper illustrates unsupervised organ segmentation {{through the use of}} a novel automated labelling approximation algorithm followed by a hypersurface front propagation method. The approximation stage relies on a pre-computed image partition forest obtained directly from CT scan data. We have implemented all procedures to operate directly on 3 D volumes, rather than slice–by–slice, because our algorithms are dimensionality–independent. The results <b>picture</b> <b>segmentations</b> which identify kidneys, but can easily be extrapolated to other body parts. Quantitative analysis of our automated segmentation compared against hand–segmented gold standards indicates an average Dice similarity coefficient of 90 %. Results were obtained over volumes of CT data with 9 kidneys, computing both volume–based similarity measures (such as the Dice and Jaccard coefficients, true positive volume fraction) and size–based measures (such as the relative volume difference). The analysis considered both healthy and diseased kidneys, although extreme pathological cases were excluded from the overall count. Such cases are difficult to segment both manually and automatically due to the large amplitude of Hounsfield unit distribution in the scan, and the wide spread of the tumorous tissue inside the abdomen. In the case of kidneys that have maintained their shape, the similarity range lies around the values obtained for inter–operator variability. Whilst the procedure is fully automated, our tools also provide a light level of manual editing...|$|R
40|$|This paper {{describes}} an automatic threshold selection method for <b>picture</b> <b>segmentation.</b> The basic concept {{is the definition}} of an anisotropy coefficient, which is related to the asymmetry of the grey-level histogram. Its use permits the derivation of the entropic thresholds, which has been successfully applied to images having various kinds of histograms. Several experimental results are presented. An extension to multithresholding is also suggested...|$|E
40|$|This paper {{describes}} an automatic threshold selection method for <b>picture</b> <b>segmentation,</b> using {{the entropy of}} the grey level histogram. It is shown that, by an a priori maximation of an entropy determined a posteriori, a picture can successfully be thresholded into a two-level image. Several experimental results are presented to show {{the validity of the}} method. An extension to multithresholding and to multidimensional histogram processing is also discussed...|$|E
40|$|Abstract- In this paper, a {{fast and}} {{flexible}} algorithm for computing watersheds in digital grayscale images is introduced. A review of watersheds and related notion is first presented, {{and the major}} methods to determine watersheds are discussed. The present algorithm {{is based on an}} immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using a queue of pixels. It is described in detail and provided in a pseudo C language. We prove the accuracy of this algorithm is superior to that of the existing implementations. Furthermore, it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images and even to graphs are straightforward. In addition, its strongest point is that it is faster than any other watershed algorithm. Applications of this algorithm with regard to <b>picture</b> <b>segmentation</b> are presented for MR imagery and for digital elevation models. An example of 3 -D watershed is also provided. Lastly, some ideas are given on how to solve complex segmentation tasks using watersheds on graphs. Zndex Terms-Algorithm, digital image, FIFO structure, graph, grid, mathematical morphology, <b>picture</b> <b>segmentation,</b> water...|$|E
40|$|The image {{segmentation}} {{is often used}} to trace the thing and bounds like line and curves in a <b>picture.</b> The <b>segmentation</b> of the text dependability is critical to perform the classification and Recognition. the most aim of segmentation is to partition the document image into varied homogenous regions like text block, image block, line and word. during this paper we've got introduced a cluster based mostly neighbor technique and Direction based line segmentation technique for the {{image segmentation}}. First, scan the input image and take away the noise. Second, apply the highest down phaseation approach to segment a document image into text lines. Third, the results of segmentation is about of segments that conjointly cowl entire pictures...|$|R
40|$|Bayesian {{technique}} {{is a popular}} tool for object detection due to its high efficiency. As it compares pixel by pixel, {{it takes a lot}} of execution time. This paper addresses a novel framework for head detection with minimum time and high accuracy. To detect head from motion <b>pictures,</b> motion <b>segmentation</b> algorithm is employed. The novelty of this paper carried out with the following steps: frame differencing, preprocessing, detecting edge lines and restoration, finding the head area and cutting the head candidate. Moreover, nested K-means algorithm is adopted to find head location and statistical modeling is employed to determine face or non-face class, while Bayesian Discriminating Features (BDF) method is employed to verify the faces. Finally, the proposed system is carried out with a lot of experiments and a recognizable success is notifie...|$|R
40|$|Page {{segmentation}} and classification {{are important}} parts of the document analysis process. The aim is to extract and classify {{different parts of the}} page. This paper proposes an approach in which these two phases are combined. The integration process includes fast feature extraction with rule-based classification and label propagation using connectivity analysis providing classified areas in three categories: background, text and <b>picture.</b> Keywords: <b>segmentation,</b> classification, fast feature extraction, rule-based reasoning, connectivity analysis. 1. Introduction There is an ever growing demand for electronic and computerized document management, storage and circulation. This has been a problem for governments and corporations in the past, but nowadays also considerable consuming masses, ordinary people, have grown to demand these basic tasks. Document structure analysis and understanding are the main processes in reaching this goal: ease of use and availability of documents. The lates [...] ...|$|R
40|$|Work {{is focused}} on usage wavelet {{transform}} in diagnostic of retinae illness. Wavelet transform enables decomposition of eye background pictures. With the following segmentation by force of thresholding it leads to lucidity of picture and simpler diagnostics of morphological and anatomical changes. Part of work is also familiarization with wavelet transform and mother's wavelet family. Marginally is in work analysed <b>picture</b> <b>segmentation</b> according to RGB coloured model, picture filtration and thresholding. In opening chapters are mentioned essential piece of knowledge from anatomy, physiology and pathology...|$|E
40|$|ISBN: 0780373049 A joint algorithm-architecture {{analysis}} {{leads to}} {{a new version of}} <b>picture</b> <b>segmentation</b> system adapted to multimedia mobile terminal constraints. The asynchronous processors network, with a granularity level of one processor per pixel, based on data flow model, takes less than 10 mu s to segment a SQCIF $ 88 * 72 pixels - image (about 2000 times faster than the classical sequential watershed algorithms). The main originality of the proposed algorithm is only one global synchronization point is needed in order to complete the segmentation transformation, instead of the three (or more) classical points: minima detection, labelization and flooding. Our system tends to cope with multimedia mobile phones constraints, i. e. real time computing circuit, low power. We have simulated and validated this system thanks to "SystemC" library; VHDL synchronous prototyping shows up results accordingly...|$|E
40|$|The artistic {{content of}} {{historical}} manuscripts provides {{a lot of}} challenges in terms of automatic text extraction, <b>picture</b> <b>segmentation</b> and retrieval by similarity. In particular this work addresses the problem of automatic extraction of meaningful pictures, distinguishing them from handwritten text and floral and abstract decorations. The proposed solution firstly employs a circular statistics description of a directional histogram in order to extract text. Then visual descriptors are computed over the pictorial regions of the page: the semantic content is distinguished from the decorative parts using color histograms and a novel texture feature called Gradient Spatial Dependency Matrix. The feature vectors are finally processed using an embedding procedure which allows increased performance in later SVM classification. Results for both feature extraction and embedding based classification are reported, supporting {{the effectiveness of the}} proposal on high resolution replicas of artistic manuscripts...|$|E
40|$|To {{evaluate}} {{the sustainability of}} {{an enormous number of}} urban intersections, a novel assessment model is proposed, along with an indicator system and corresponding methods to determine the indicators. Considering mainly the demands and feelings of the urban residents, the three aspects of safety, functionality, and image perception are taken into account in the indicator system. Based on technologies such as street view <b>picture</b> crawling, image <b>segmentation,</b> and edge detection, GIS spatial data analysis, a rapid automated assessment method, and a corresponding multi-source database are built up to determine the indicators. The improved information entropy method is applied to obtain the entropy weights of each indicator. A case study shows the efficiency and applicability of the proposed assessment model, indicator system and algorithm...|$|R
40|$|The authors {{develop an}} {{approach}} to reveal segmentation in response to marketing variables at a brand-level perspective. In the proposed procedure, response segmentation is analyzed separately for each brand instead of jointly across all brands. This yields a <b>segmentation</b> <b>picture</b> oriented toward the potential targeting objectives of the brand manager. Using the multinomial logit and probabilistic mixture models, the procedure first calibrates consumer response in the brand choice decision. Individual-level measures of response for a given marketing variable (e. g., price) are then computed, and brand-level segments are obtained by clustering brand-specific response. Using scanner panel data, the approach is applied to price response segmentation for brands that compete in the ground coffee category. The results illustrate a series of implications for brand strategy, particularly the potential for targeting marketing activity to different response segments...|$|R
40|$|This work {{is focused}} on {{controlling}} the game by using hand gestures. The {{main part of the}} work is image segmentation and detection of the hand in <b>picture.</b> For the <b>segmentation</b> of the image are used techniques of skin detection and the background subtraction with adaptive model of the background. Also the methods of mathematical morphology to eliminate the noise from the image and the appropriate methods for transferring images of gestures to characteristic gestures in numerical form are mentioned. In the context of the work was a simple car race game created which is controlled by hand gestures. At the end there was a testing carried out to identify {{the advantages and disadvantages of}} used methods of image segmentation and to detect the used hand gestures. There were also several sets of gestures tested by which the game is controlled. The two sets which came out of the test most successfully are applicable depending on the quality of the hand gesture recognition...|$|R
40|$|The work {{described}} herin, presents {{my research}} activities over the 1990 - 2009 period. They focus on image analysis, the objective being to develop image processing tools to assist clinical {{decision making in}} the {{diagnosis and treatment of}} coronary artery diseases and congestive heart failure (where therapy relies on a transvenous approach to position the lead in the left ventricle). These works cover most of the topics of a chain <b>picture</b> (<b>segmentation,</b> registration, motion tracking, reconstruction, etc [...] ) And they mainly proceed with generic methods (attributed matching strings, 3 D navigation based on geometrical moments, tracking of deformables strctures, sparse 3 D reconstruction and motion compensation). Les travaux décrits présentent mes activités de recherches sur la période 1990 - 2009. Ils couvrent la plupart des thématiques d'une chaine image (segmentation, recalage, suivi de mouvement, reconstruction, etc.) et ils procèdent en grande partie de méthodes génériques (mise en correspondance par chaines de caractères, navigation 3 D basée sur des moments géométriques et reconstruction par compensation de mouvement) ...|$|E
40|$|In {{this paper}} we define a new {{paradigm}} for 8 -connection labeling, which employes a general approach to improve neighborhood exploration and minimizes the number of memory accesses. Firstly we exploit and extend the decision table formalism introducing OR-decision tables, in which multiple alternative actions are managed. An automatic procedure to synthesize the optimal decision tree from the decision table is used, providing the most effective conditions evaluation order. Secondly we propose a new scanning technique that moves on a 2 x 2 pixel grid over the image, which is optimized by the automatically generated decision tree. An extensive comparison {{with the state of}} art approaches is proposed, both on synthetic and real datasets. The synthetic dataset is composed of different sizes and densities random images, while the real datasets are an artistic image analysis dataset, a document analysis dataset for text detection and recognition, and finally a standard resolution dataset for <b>picture</b> <b>segmentation</b> tasks. The algorithm provides an impressive speedup over {{the state of the art}} algorithms...|$|E
40|$|A joint algorithm-architecture {{study has}} {{resulted}} {{into a new}} version of a <b>picture</b> <b>segmentation</b> system complying with multimedia mobile terminal constraints, i. e., real-time computing, and low power consumption. Previously published watershed segmentation algorithms required at least three global synchronization points: minima detection, labeling and flooding. This paper presents a new fully asynchronous algorithm, where pixels can compute their local data in parallel and independently from one another, and which requires only a unique final global synchronization point. This paper provides a formal demonstration of the convergence and correctness of this new parallel asynchronous algorithm using a mathematical model of data propagation in a graph: the associative net formalism. We demonstrate the simplicity of implementation of this algorithm on parallel processor arrays. We explore, simulate, and validate several configurations of the algorithm-architecture using a “SystemC” model. Simulations reveal an image segmentation rate up to 66, 000 QCIF images/sec, i. e., a speed-up factor of more than 1, 000 times compared with state of the art watershed algorithms. A fine grain processor array design using STmicroelectronics 0 : 18 _m CMOS technology confirms that this new approach is a breakthrough in the domain of real-time image segmentation...|$|E
40|$|There is {{a growing}} body of {{evidence}} that children with dyslexia have problems not just in reading but in a range of skills including several unrelated to reading. In an attempt to compare the severity and individual incidence of deficits across these skills, children with dyslexia (mean ages 8, 12 and 16 years), and control groups of normally achieving children matched for IQ and for age or reading age, were tested on a range of primitive (basic) skills. The children with dyslexia performed significantly worse than the same-age controls on most tasks, and significantly worse even than the reading age controls on phoneme <b>segmentation,</b> <b>picture</b> naming speed, word flash, bead threading and both blindfold and dual task balance. Overall, the performance of the 16 year old children with dyslexia was no better than that of the 8 year old normally-achieving children, with some skills being significantly worse, and some better. The overall performance of the children with dyslexia is interpreted [...] ...|$|R
40|$|This paper {{introduces}} {{an automatic}} tool able to analyse the picture {{according to the}} semantic interest an observer attributes to its content. Its aim is to give a "level of interest" to the distinct areas of the <b>picture</b> extracted byany <b>segmentation</b> tool. For the purpose of dealing with semantic interpretation of images, a single criterion is clearly insufficient because the human brain, due to its a priori knowledge and its huge memory of real-world concrete scenes, combines different subjective criteria {{in order to assess}} its final decision. The developed method permits such combination through a model using assumptions to express some general subjective criteria. Fuzzy Logic enables to encode knowledge in a form that is very close to the way experts think about the decision process. This fuzzy modelling is also well suited to representmultiple collaborating or even conflicting experts opinions. Actually, the assumptions are verified through a non-hierarchical strategy that considers them in a random order, each partial result contributing to the final one. Presented results prove that the tool is effective {{for a wide range of}} natural pictures. It is versatile and flexible in that it can be used stand-alone or can takeinto accountany a priori knowledge about the scene. Keywords: Fuzzy Logic System, Video Analysis, Semantic Interpretation, Segmentation, Compression. ...|$|R
40|$|Otsu {{reference}} {{proposed a}} criterion for maximizing the between-class variance of pixel intensity to perform picture thresholding. However, Otsu’s method for image segmentation is very time-consuming {{because of the}} inefficient formulation of the between-class variance. In this paper, a faster version of Otsu’s method is proposed for improving the efficiency of computation for the optimal thresholds of an image. First, a criterion for maximizing a modified between-class variance that {{is equivalent to the}} criterion of maximizing the usual between-class variance is proposed for image segmentation. Next, in accordance with the new criterion, a recursive algorithm is designed to efficiently find the optimal threshold. This procedure yields the same set of thresholds as the original method. In addition, the modified between-class variance can be pre-computed and stored in a look-up table. Our analysis of the new criterion clearly shows that it takes less computation to compute both the cumulative probability (zeroth order moment) and the mean (first order moment) of a class, and that determining the modified between-class variance by accessing a look-up table is quicker than that by performing mathematical arithmetic operations. For example, the experimental results of a five-level threshold selection show that our proposed method can reduce down the processing time from more than one hour by the conventional Otsu’s method to less than 107 seconds. Keywords: Otsu’s thresholding, image <b>segmentation,</b> <b>picture</b> thresholding, multilevel thresholding, recursive algorithm 1...|$|R
40|$|Several {{types of}} {{algorithms}} are generally used to process digital imagery such as Landsat data. The {{most commonly used}} algorithms perform the task of registration, compression, and classification. Because there are different techniques available for performing registration, compression, and classification, imagery data users need a rationale for selecting a particular approach to meet their particular needs. This collection of registration, compression, and classification algorithms was developed so that different approaches could be evaluated and the best approach for a particular application determined. Routines are included for six registration algorithms, six compression algorithms, and two classification algorithms. The package also includes routines for evaluating the effects of processing on the image data. This collection of routines should be useful to anyone using or developing image processing software. Registration of image data involves the geometrical alteration of the imagery. Registration routines available in the evaluation package include image magnification, mapping functions, partitioning, map overlay, and data interpolation. The compression of image data involves reducing the volume of data needed for a given image. Compression routines available in the package include adaptive differential pulse code modulation, two-dimensional transforms, clustering, vector reduction, and <b>picture</b> <b>segmentation.</b> Classification of image data involves analyzing the uncompressed or compressed image data to produce inventories and maps of areas of similar spectral properties within a scene. The classification routines available include a sequential linear technique and a maximum likelihood technique. The choice of the appropriate evaluation criteria is quite important in evaluating the image processing functions. The user is therefore given a choice of evaluation criteria with which to investigate the available image processing functions. All of the available evaluation criteria basically compare the observed results with the expected results. For the image reconstruction processes of registration and compression, the expected results are usually the original data or some selected characteristics of the original data. For classification processes the expected result is the ground truth of the scene. Thus, the comparison process consists of determining what changes occur in processing, where the changes occur, how much change occurs, and the amplitude of the change. The package includes evaluation routines for performing such comparisons as average uncertainty, average information transfer, chi-square statistics, multidimensional histograms, and computation of contingency matrices. This collection of routines is written in FORTRAN IV for batch execution and has been implemented on an IBM 360 computer with a central memory requirement of approximately 662 K of 8 bit bytes. This collection of image processing and evaluation routines was developed in 1979...|$|E

