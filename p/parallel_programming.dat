4514|4661|Public
5|$|Mainstream <b>parallel</b> <b>programming</b> {{languages}} remain either explicitly parallel or (at best) partially implicit, {{in which}} a programmer gives the compiler directives for parallelization. A few fully implicit <b>parallel</b> <b>programming</b> languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.|$|E
5|$|The {{research}} group of Sabri Pllana developed an assistant for learning <b>parallel</b> <b>programming</b> using the IBM Watson. A survey {{with a number}} of novice parallel programmers at the Linnaeus University indicated that such assistant will be welcome by students that learn <b>parallel</b> <b>programming.</b>|$|E
5|$|<b>Parallel</b> <b>programming</b> {{languages}} and parallel computers {{must have a}} consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.|$|E
40|$|This {{research}} investigated {{an automated}} approach to re-writing traditional sequential computer <b>programs</b> into <b>parallel</b> <b>programs</b> for networked computers. A tool {{was designed and}} developed for generating <b>parallel</b> <b>programs</b> automatically and also executing these <b>parallel</b> <b>programs</b> on a network of computers. Performance is maximized by utilising all idle resources...|$|R
40|$|The {{architecture}} of system for optimization of <b>parallel</b> <b>programs</b> {{that allows the}} development, optimization and preparing of <b>parallel</b> <b>programs</b> is offered. The {{architecture of}} system is constructed as open system {{that it could be}} adjusted easily for use of various support libraries for <b>parallel</b> <b>programs</b> and optimization techniques...|$|R
40|$|Most current {{compiler}} analysis {{techniques are}} unable {{to cope with the}} semantics introduced by explicit parallel and synchronization constructs in <b>parallel</b> <b>programs.</b> In this paper we introduce new analysis and optimization techniques for compiling explicitly <b>parallel</b> <b>programs</b> that use mutual exclusion synchronization. We introduce the CSSAME form, an extension of Concurrent Static Single Assignment (CSSA) that incorporates mutual exclusion into a data flow framework for explicitly <b>parallel</b> <b>programs.</b> We show how this analysis can improve the effectiveness of constant propagation in a <b>parallel</b> <b>program.</b> We also present a modification to a sequential dead code elimination algorithm to work on explicitly <b>parallel</b> <b>programs.</b> Finally, we introduce new optimization techniques specifically targeted at explicitly <b>parallel</b> <b>programs.</b> These techniques apply optimizing transformations to a program by taking advantage of its parallel and synchronization structure. We prove the correctness of these tran [...] ...|$|R
5|$|CAPS {{entreprise}} and Pathscale {{are also}} coordinating {{their effort to}} make hybrid multi-core <b>parallel</b> <b>programming</b> (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.|$|E
5|$|Concurrent {{programming}} languages, libraries, APIs, and <b>parallel</b> <b>programming</b> models (such as algorithmic skeletons) {{have been}} created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP {{are two of the}} most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.|$|E
5|$|In April 1958, S. Gill (Ferranti) {{discussed}} <b>parallel</b> <b>programming</b> and {{the need}} for branching and waiting. Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time. Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch. In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference. It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.|$|E
40|$|Abstract. We {{consider}} a certain class of <b>parallel</b> <b>program</b> segments {{in which the}} order of messages sent affects the completion time. We give characterization of these <b>parallel</b> <b>program</b> segments and propose a solution to minimize the completion time. With a sample <b>parallel</b> <b>program,</b> we experimentally evaluate {{the effect of the}} solution on a PC cluster. ...|$|R
40|$|The {{transformational}} derivation of <b>parallel</b> <b>programs</b> for distributed-memory architectures using skeleton-based approaches {{is one of}} {{the most}} promising methods for <b>parallel</b> <b>program</b> development. These approaches support the derivation of provably correct, efficient and portable <b>parallel</b> <b>programs</b> using a predefined set of encapsulated efficiently implemented parallel base algorithms. Encapsulation requires that compositions of skeletons are explicitly defined by means of transformations. More flexible approaches which enable the compositional development of <b>parallel</b> <b>programs</b> [...] - that is, without reliance on ad hoc transformations [...] - are, however, often advantageous. The researc...|$|R
40|$|A {{serial-parallel}} multiplier {{is developed}} systematically from functional specification to circuit implementation. First, a functional program is derived and, second, a <b>parallel</b> <b>program</b> for a systolic computation is constructed. The <b>parallel</b> <b>program</b> {{is derived from}} the functional program. Both synchronous and asynchronous circuit implementations for the <b>parallel</b> <b>program</b> are discussed. The latter implementation has a pipeline structure with bounded response time...|$|R
2500|$|Visual Studio 2010 {{comes with}} [...]NET Framework 4 and {{supports}} developing applications targeting Windows 7. It supports IBM DB2 and Oracle databases, {{in addition to}} Microsoft SQL Server. It has integrated support for developing Microsoft Silverlight applications, including an interactive designer. Visual Studio 2010 offers several tools to make <b>parallel</b> <b>programming</b> simpler: {{in addition to the}} Parallel Extensions for the [...]NET Framework and the Parallel Patterns Library for native code, Visual Studio 2010 includes tools for debugging parallel applications. The new tools allow the visualization of parallel Tasks and their runtime stacks. Tools for profiling parallel applications can be used for visualization of thread wait-times and thread migrations across processor cores. Intel and Microsoft have jointly pledged support for a new Concurrency Runtime in Visual Studio 2010 ...|$|E
50|$|PPoPP, the ACM SIGPLAN Symposium on Principles and Practice of <b>Parallel</b> <b>Programming,</b> is an {{academic}} {{conference in the}} field of <b>parallel</b> <b>programming.</b> PPoPP is sponsored by the Association for Computing Machinery special interest group SIGPLAN.|$|E
50|$|The main design {{guideline}} for NESL {{was to make}} <b>parallel</b> <b>programming</b> {{easy and}} portable. Algorithms are typically significantly more concise in NESL than in most other <b>parallel</b> <b>programming</b> languages, and the code closely resembles high-level pseudocode.|$|E
40|$|A {{preliminary}} MPI library {{has been}} implemented for the Fujitsu AP 1000 + multicomputer running the AP/Linux operating system. Under this environment, <b>parallel</b> <b>programs</b> may {{be dedicated to}} a fixed partition, or a number of <b>parallel</b> <b>programs</b> may share a partition. Therefore, the MPI library has been constructed so that messaging operations can be driven by polling and/or interrupt techniques. It {{has been found that}} polling works well when a single <b>parallel</b> <b>program</b> is running on a given partition, and that interrupt-driven communication makes far better use of the machine when multiple <b>parallel</b> <b>programs</b> are executing. Gang scheduling of multiple <b>parallel</b> <b>programs</b> which use polling was found to be relatively ineffective. 1 Introduction MPI has previously been implemented on both the Fujitsu AP 1000 [3] and AP 1000 + [2, 4] multicomputers running the CellOS operating system. Under CellOS, when a <b>parallel</b> <b>program</b> is initiated, the machine is reset and the kernel is loaded before launching the pa [...] ...|$|R
40|$|Abstract—A <b>parallel</b> <b>program</b> must execute {{correctly}} even in {{the presence}} of unpredictable thread interleavings. This interleaving makes it hard to write correct <b>parallel</b> <b>programs,</b> and also makes it hard to find bugs in incorrect <b>parallel</b> <b>programs.</b> A range of tools have been developed to help debug <b>parallel</b> <b>programs,</b> ranging from atomicity-violation and data-race detectors to model-checkers and theorem provers. One technique that has been successful for debugging sequential programs, but less effective for <b>parallel</b> <b>programs,</b> is running the program using assertion predicates provided by the developer. These assertions allow programmers to specify and check their assumptions. In a multi-threaded program, the programmer’s assumptions include both the current state, and any actions (e. g. access to shared memory) that other, parallel executing threads might take. We introduce parallel assertions which allow programmers to express these assumptions for <b>parallel</b> <b>programs</b> using simple and intuitive syntax and semantics. We present a proof-of-concept implementation, and demonstrate its value by testing a number of benchmark <b>programs</b> using <b>parallel</b> assertions. I...|$|R
40|$|Parallelism {{has become}} {{the way of life}} for many {{scientific}} programmers. A significant challenge in bringing the power of parallel machines to these programmers is providing them with a suite of software tools similar to the tools that sequential programmers currently utilize. Unfortunately, writing correct <b>parallel</b> <b>programs</b> remains a challenging task. In particular, automatic or semi-automatic testing tools for <b>parallel</b> <b>programs</b> are lacking. This paper takes a first step in developing an approach to providing all-uses coverage for <b>parallel</b> <b>programs.</b> A testing framework and theoretical foundations for structural testing are presented, including test data adequacy criteria and hierarchy, formulation and illlustration of all-uses testing problems, classification of all-uses test cases for <b>parallel</b> <b>programs,</b> and both theoretical and empirical results with regard to what can be achieved with all-uses coverage for <b>parallel</b> <b>programs...</b>|$|R
50|$|Mainstream <b>parallel</b> <b>programming</b> {{languages}} remain either explicitly parallel or (at best) partially implicit, {{in which}} a programmer gives the compiler directives for parallelization. A few fully implicit <b>parallel</b> <b>programming</b> languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.|$|E
50|$|The {{research}} group of Sabri Pllana developed an assistant for learning <b>parallel</b> <b>programming</b> using the IBM Watson. A survey {{with a number}} of novice parallel programmers at the Linnaeus University indicated that such assistant will be welcome by students that learn <b>parallel</b> <b>programming.</b>|$|E
50|$|He {{also has}} led applied {{research}} projects: <b>parallel</b> <b>programming</b> languages (Proteus System for <b>parallel</b> <b>programming),</b> parallel architectures (Blitzen, a massively parallel machine), data compression (massively parallel loss-less compression hardware), and optical computing (free-space holographic routing). His papers on these algorithmic topics {{can be downloaded}} here.|$|E
40|$|One of {{the main}} {{obstacles}} to a more widespread use of parallel computing in computational science is the difficulty of implementing, testing, and maintaining <b>parallel</b> <b>programs.</b> The combination of a simple parallel computation model, BSP, and a high-level programming language, Python, simplifies these tasks significantly. It allows the rapid development facilities of Python {{to be applied to}} <b>parallel</b> <b>programs,</b> providing interactive development as well as interactive debugging of <b>parallel</b> <b>programs...</b>|$|R
40|$|We present Dubstep, a novel {{system that}} uses the findtransform-navigate {{approach}} to automatically explore new parallelization opportunities in already parallelized (fullysynchronized) programs by opportunistically relaxing synchronization primitives. This set of transformations generates a space of alternative, possibly non-deterministic, <b>parallel</b> <b>programs</b> with varying performance and accuracy characteristics. The freedom to generate <b>parallel</b> <b>programs</b> whose output may differ (within statistical accuracy bounds) from {{the output of the}} original program enables a significantly larger optimization space. Dubstep then searches this space to find a <b>parallel</b> <b>program</b> that will, with high likelihood, produce outputs that are acceptably close to the outputs that the original, fully synchronized <b>parallel</b> <b>program</b> would have produced. Initial results from our benchmarked application show that Dubstep can generate acceptably accurate and efficient versions of a <b>parallel</b> <b>program</b> that occupy different positions in performance/accuracy trade off space...|$|R
40|$|Most current {{compiler}} analysis {{techniques are}} unable {{to cope with the}} semantics introduced by explicit parallel and synchronization constructs in <b>parallel</b> <b>programs.</b> In this paper we propose new analysis and optimization techniques for compiling explicitly <b>parallel</b> <b>programs</b> that use mutual exclusion synchronization. We introduce the CSSAME form, an extension of the Concurrent Static Single Assignment (CSSA) form that incorporates mutual exclusion into a data flow framework for explicitly <b>parallel</b> <b>programs.</b> We show how this analysis can improve the effectiveness of constant propagation in a <b>parallel</b> <b>program.</b> We also modify a deadcode elimination algorithm to work on explicitly <b>parallel</b> <b>programs.</b> Finally, we introduce lock independent code motion, a new optimization technique that attempts to minimize the size of critical sections in the program. 1. Introduction Although recent advances in parallelizing compilers and data-parallel languages have been impressive [4, 8], there are importan [...] ...|$|R
5000|$|... #Subtitle level 2: Classification of <b>parallel</b> <b>programming</b> models ...|$|E
5000|$|... #Article: Symposium on Principles and Practice of <b>Parallel</b> <b>Programming</b> ...|$|E
5000|$|Studies in Computational Science: <b>Parallel</b> <b>Programming</b> Paradigms (1995, [...] ) ...|$|E
40|$|Effectively and {{efficiently}} implementing <b>parallel</b> <b>programs</b> for computer systems {{with a large}} number of processing elements involves the investigation of occurring communication between processes. The communication entails messages that are passed between participating processing elements. Analysing, debugging and tuning of <b>parallel</b> <b>programs</b> is very complex due to the dynamic behaviour of the communication. Recording the communication events during the execution of a <b>parallel</b> <b>program</b> together with a time stamp assists a programmer in analysing the <b>parallel</b> <b>program</b> by enabling the reconstruction of the dynamic behaviour of the communication. An automatic instrumentation process changes the <b>parallel</b> <b>program</b> for recording all important events, including the communication events. In association with Cray Research the Zentralinstitut für Angewandte Mathematik at the Forschungszentrum Jülich implements an instrumentation module for the Performance Analysis Tool (PAT) for the Cray T 3 E parallel [...] ...|$|R
40|$|Testing the {{performance}} scalability of <b>parallel</b> <b>programs</b> {{can be a}} time consuming task, involving many performance runs for different computer configurations, processor numbers, and problem sizes. Ideally, scalability issues would be addressed during <b>parallel</b> <b>program</b> design, but tools are not presently available that allow program developers to study the impact of algorithmicchoices under different problem and system scenarios. Hence, scalability analysis is often reserved to existing (and available) parallel machines as well as implemented algorithms. In this paper, we propose techniques for analyzing scaled <b>parallel</b> <b>programs</b> using stochastic modeling approaches. Although allowing more generality and flexibility in analysis, stochastic modeling of large <b>parallel</b> <b>programs</b> is difficult due to solution tractability problems. We observe, however, that the complexity of <b>parallel</b> <b>program</b> models depends significantly {{on the type of}} parallel computation, and we present several computation clas [...] ...|$|R
40|$|Abstract — A <b>parallel</b> <b>program</b> {{should be}} {{evaluated}} to determine its efficiency, accuracy and benefits. This paper defines how <b>parallel</b> <b>programs</b> differ by sequential programs. A brief discussion {{on the effect of}} increasing number of processors on execution time is given. Some of the important measurement units which are used for the purpose of measuring performance of a <b>parallel</b> <b>program</b> are discussed. Various performance laws-Amdahl’s Law, Gustafson’s Law to measure speedup are discussed...|$|R
5000|$|Symposium on Principles and Practice of <b>Parallel</b> <b>Programming</b> (PPoPP) ...|$|E
5000|$|Collective {{operations}} {{are present in}} the following <b>parallel</b> <b>programming</b> frameworks: ...|$|E
5000|$|The Search for Simplicity: Essays in <b>Parallel</b> <b>Programming</b> (1996, [...] ) ...|$|E
40|$|Traditional parallelizing compilers are {{designed}} to generate <b>parallel</b> <b>programs</b> that produce identical outputs as the original sequential program. The difficulty of performing the program analysis required to satisfy this goal and the restricted space of possible target <b>parallel</b> <b>programs</b> have both posed significant obstacles {{to the development of}} effective parallelizing compilers. The QuickStep compiler is instead designed to generate <b>parallel</b> <b>programs</b> that satisfy statistical accuracy guarantees. The freedom to generate <b>parallel</b> <b>programs</b> whose output may differ (within statistical accuracy bounds) from the output of the sequential program enables a dramatic simplification of the compiler and a significant expansion in the range of <b>parallel</b> <b>programs</b> that it can legally generate. QuickStep exploits this flexibility to take a fundamentally different approach from traditional parallelizing compilers. It applies a collection of transformations (loop parallelization, loop scheduling, synchronization introduction, and replication introduction) to generate a search space of parallel versions of the original sequential program. It then searches this space (prioritizing the parallelization of the most time-consuming loops in the application) to find a final parallelization that exhibits good parallel performance and satisfies the statistical accuracy guarantee. At each step in the search it performs a sequence of trial runs on representative inputs to examine the performance, accuracy, and memory accessing characteristics of the current generated <b>parallel</b> <b>program.</b> An analysis of these characteristics guides the steps the compiler takes as it explores the search space of <b>parallel</b> <b>programs.</b> Results from our benchmark set of applications show that QuickStep can automatically generate <b>parallel</b> <b>programs</b> with good performance and statistically accurate outputs. For two of the applications, the parallelization introduces noise into the output, but the noise remains within acceptable statistical bounds. The simplicity of the compilation strategy and the performance and statistical acceptability of the generated <b>parallel</b> <b>programs</b> demonstrate the advantages of the QuickStep approach...|$|R
40|$|<b>Parallel</b> <b>programs</b> on lists {{have been}} intensively studied. It {{is well known}} that {{associativity}} provides a good characterization for divide-and-conquer <b>parallel</b> <b>programs.</b> In particular, the third homomorphism theorem is not only useful for systematic development of <b>parallel</b> <b>programs</b> on lists, but it is also suitable for automatic parallelization. The theorem states that if two sequential programs iterate the same list leftward and rightward, respectively, and compute the same value, then there exists a divide-and-conquer <b>parallel</b> <b>program</b> that computes the same value as the sequential programs. While there have been many studies on lists, few have been done for characterizing and developing of <b>parallel</b> <b>programs</b> on trees. Naive divide-and-conquer programs, which divide a tree at the root and compute independent subtrees in parallel, take time that is proportional to the height of the input tree and have poor scalability with respect to the number of processors when the input tree is ill-balanced. In this paper, we develop a method for systematically constructing scalable divide-and-conquer <b>parallel</b> <b>programs</b> on trees, in which two sequential programs lead to a scalable divide-andconquer <b>parallel</b> <b>program.</b> We focus on paths instead of trees so as to utilize rich results on lists and demonstrate that associativity provides good characterization for scalable divide-and-conquer <b>parallel</b> <b>programs</b> on trees. Moreover, we generalize the third homomorphism theorem from lists to trees. We demonstrate the effectiveness of our method with various examples. Our results, being generalizations of known results for lists, are generic {{in the sense that they}} work well for all polynomial data structures...|$|R
40|$|Abstract. This {{practical}} tutorial {{introduces the}} features available in Haskell for writing <b>parallel</b> and concurrent <b>programs.</b> We first describe {{how to write}} semi-explicit <b>parallel</b> <b>programs</b> by using annotations to express opportunities for parallelism and to help control the granularity of parallelism for effective execution on modern operating systems and processors. We then describe the mechanisms provided by Haskell for writing explicitly <b>parallel</b> <b>programs</b> {{with a focus on}} the use of software transactional memory to help share information between threads. Finally, we show how nested data parallelism can be used to write deterministically <b>parallel</b> <b>programs</b> which allows programmers to use rich data types in data <b>parallel</b> <b>programs</b> which are automatically transformed into flat data parallel versions for efficient execution on multi-core processors. ...|$|R
