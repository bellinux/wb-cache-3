25|35|Public
25|$|The RIKEN MDGRAPE-3 for {{molecular}} dynamics simulations of proteins {{is a special}} purpose <b>petascale</b> <b>supercomputer</b> at the Advanced Center for Computing and Communication, RIKEN in Wako, Saitama, just outside Tokyo. It uses over 4,800 custom MDGRAPE-3 chips, as well as Intel Xeon processors. However, given {{that it is a}} special purpose computer, it can not appear on the TOP500 list which requires Linpack benchmarking.|$|E
25|$|The Computer Centre, {{established}} in 1970 {{as a central}} computing facility, became Supercomputer Education and Research Centre (SERC) in 1990 to provide state-of-the-art computing facility to the faculty and students of the Institute. SERC is created and fully funded by the Ministry of Human Resource Development (MHRD) to commemorate the platinum jubilee of the Institute. It houses India's first <b>petascale</b> <b>supercomputer</b> CrayXC-40 and also the fastest supercomputer in India.|$|E
50|$|In February 2009 it was {{announced}} that JUGENE would be upgraded to reach petaflops performance in June 2009, making it the first <b>petascale</b> <b>supercomputer</b> in Europe.|$|E
5000|$|TGCC {{is a new}} [...] "green infrastructure" [...] {{for high}} {{computing}} performance, able to host <b>petascale</b> <b>supercomputers.</b>|$|R
50|$|FERMI is a Blue Gene/Q system, {{the last}} {{generation}} of the IBM project for designing <b>petascale</b> <b>supercomputers.</b> It consists of 10 racks, two midplanes each, {{for a total of}} 10.240 compute nodes and 163.840 cores.|$|R
40|$|How do {{massive stars}} explode? Progress toward {{the answer is}} driven by {{increases}} in compute power. <b>Petascale</b> <b>supercomputers</b> are enabling detailed 3 D simulations of core-collapse supernovae that are elucidating the role of fluid instabilities, turbulence, and magnetic field amplification in supernova engines...|$|R
50|$|MDGRAPE-3 is an ultra-high {{performance}} <b>petascale</b> <b>supercomputer</b> system {{developed by}} the RIKEN research institute in Japan. It is a special purpose system built for molecular dynamics simulations, especially protein structure prediction.|$|E
50|$|In June 2011, France's Tera 100 was {{certified}} {{the fastest}} supercomputer in Europe, and ranked 9th {{in the world}} at the time. It was the first <b>petascale</b> <b>supercomputer</b> designed andbuilt in Europe.|$|E
5000|$|Pleiades ( [...] or [...] ) is a <b>petascale</b> <b>supercomputer</b> housed at the NASA Advanced Supercomputing (NAS) {{facility}} at NASA Ames Research Center located at Moffett Field near Mountain View, California. It is maintained by NASA and partners Silicon Graphics (SGI) and Intel.|$|E
40|$|<b>Petascale</b> <b>supercomputers</b> rely on highly {{efficient}} Petascale I/O subsystems. This work describes the tuning and scaling {{behavior of the}} GPFS parallel file system on JUGENE, the largest IBM Blue Gene/P installation worldwide and the first PetaFlop/s HPC resource within the European PRACE Research Infrastructure...|$|R
50|$|In March 2015, the Indian {{government}} {{has approved a}} seven-year supercomputing program worth $730 million (Rs. 4,500-crore). The National Supercomputing grid will consist of 73 geographically-distributed high-performance computing centers linked over a high-speed network. The mission involves both capacity and capability machines and includes standing up three <b>petascale</b> <b>supercomputers.</b>|$|R
5000|$|Tianhe-I, Tianhe-1, or TH-1 ( [...] , Sky River Number One) is a {{supercomputer}} {{capable of}} an Rmax (maximum range) of 2.5 petaFLOPS. Located at the National Supercomputing Center of Tianjin, China, {{it was the}} fastest computer in the world from October 2010 to June 2011 {{and is one of}} the few <b>Petascale</b> <b>supercomputers</b> in the world.|$|R
50|$|IBM won a $244 million DARPA {{contract}} in November 2006 {{to develop a}} <b>petascale</b> <b>supercomputer</b> architecture {{before the end of}} 2010 in the HPCS project. The contract also states that the architecture shall be available commercially. IBM's proposal, PERCS (Productive, Easy-to-use, Reliable Computer System), which won them the contract, is based on the POWER7 processor, AIX operating system and General Parallel File System.|$|E
5000|$|Blue Waters is a <b>petascale</b> <b>supercomputer</b> at the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign. On August 8, 2007, the National Science Board {{approved}} a resolution which authorized the National Science Foundation to fund [...] "the acquisition and {{deployment of the}} world's most powerful leadership-class supercomputer." [...] The NSF awarded $208 million for the Blue Waters project.|$|E
50|$|The RIKEN MDGRAPE-3 for {{molecular}} dynamics simulations of proteins {{is a special}} purpose <b>petascale</b> <b>supercomputer</b> at the Advanced Center for Computing and Communication, RIKEN in Wako, Saitama, just outside Tokyo. It uses over 4,800 custom MDGRAPE-3 chips, as well as Intel Xeon processors. However, given {{that it is a}} special purpose computer, it can not appear on the TOP500 list which requires Linpack benchmarking.|$|E
40|$|To {{meet the}} goals of extreme weather event warning, this {{approach}} couples a modeling and visualization system that integrates existing NASA technologies and improves the modeling system's parallel scalability {{to take advantage of}} <b>petascale</b> <b>supercomputers.</b> It also streamlines the data flow for fast processing and 3 D visualizations, and develops visualization modules to fuse NASA satellite data...|$|R
40|$|<b>Petascale</b> <b>supercomputers</b> will be {{available}} by 2008. The largest machine of these complex leadership-class machines will probably have nearly 250 K CPUs. These massively parallel systems {{have a number of}} challenging operating system issues. In this paper, we focus on the issues most important for the system that will first breach the petaflop barrier: synchronization and collective operations, parallel I/O, and fault tolerance...|$|R
40|$|How do {{massive stars}} explode? Progress toward {{the answer is}} driven by {{increases}} in compute power. <b>Petascale</b> <b>supercomputers</b> are enabling detailed three-dimensional simulations of core-collapse supernovae. These are elucidating the role of fluid instabilities, turbulence, and magnetic field amplification in supernova engines. Comment: 12 pages, 8 figures. Refereed overview article published in Computing in Science & Engineering (CiSE; number of references limited due to magazine format). [URL] Non-copyedited version prepared by the autho...|$|R
50|$|Jaguar was a <b>petascale</b> <b>supercomputer</b> {{built by}} Cray at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee. The massively {{parallel}} Jaguar had a peak performance {{of just over}} 1,750 teraFLOPS (1.75 petaFLOPS). It had 224,256 x86-based AMD Opteron processor cores, and operated with a version of Linux called the Cray Linux Environment. Jaguar was a Cray XT5 system, a development from the Cray XT4 supercomputer.|$|E
50|$|The Computer Centre, {{established}} in 1970 {{as a central}} computing facility, became Supercomputer Education and Research Centre (SERC) in 1990 to provide state-of-the-art computing facility to the faculty and students of the Institute. SERC is created and fully funded by the Ministry of Human Resource Development (MHRD) to commemorate the platinum jubilee of the Institute. It houses India's first <b>petascale</b> <b>supercomputer</b> CrayXC-40 and also the fastest supercomputer in India.|$|E
5000|$|Nebulae (...) is a <b>petascale</b> <b>supercomputer</b> {{located at}} the National Supercomputing Center in Shenzhen, Guangdong, China. Built from a Dawning TC3600 Blade system with Intel Xeon X5650 {{processors}} and Nvidia Tesla C2050 GPUs, it has a peak performance of 1.271 petaflops using the LINPACK benchmark suite. Nebulae was ranked the second most powerful computer {{in the world in}} the June 2010 list of the fastest supercomputers according to TOP500. Nebulae has a theoretical peak performance of 2.9843 petaflops. This computer is used for multiple applications requiring advanced processing capabilities. It is ranked 10th among the June 2012 list of top500.org.|$|E
50|$|NSF {{grants for}} computing, data, and {{scientific}} visualization resources are allocated to researchers who investigate the Earth system through simulation. The current HPC environment includes two <b>petascale</b> <b>supercomputers,</b> data analysis and visualization servers, an operational weather forecasting system, an experimental supercomputing architecture platform, a centralized file system, a data storage resource, and an archive of historical research data. All computing and support systems required for scientific workflows {{are attached to}} the shared, high-speed, central file system to improve scientific productivity and reduce costs by analyzing and visualizing their data files in place at the NWSC.|$|R
40|$|Multiscale {{simulations}} {{are essential}} in the biomedical domain to accurately model human physiology. We present a modular approach for designing, constructing and executing multiscale simulations {{on a wide}} range of resources, from desktops to <b>petascale</b> <b>supercomputers,</b> including combinations of these. Our work features two multiscale applications, in-stent restenosis and cerebrovascular bloodflow, which combine multiple existing single-scale applications to create a multiscale simulation. These applications can be efficiently coupled, deployed and executed on computers up to the largest (peta) scale, incurring a coupling overhead of 1 to 10 % of the total execution time. Comment: accepted by Interface Focus. 17 pages, 2 figures, 4 table...|$|R
40|$|Abstract â€” Currently {{deployed}} <b>petascale</b> <b>supercomputers</b> typ-ically use toroidal network topologies {{in three}} to five dimensions. While these networks perform well for topology-agnostic codes on a few thousand nodes, leadership machines with 20, 000 nodes require topology awareness to avoid network contention for communication-intensive codes. Topology adaptation is com-plicated by irregular node allocation shapes and holes due to dedicated input/output nodes or hardware failure. In {{the context of the}} popular molecular dynamics program NAMD, we present methods for mapping a periodic 3 -D grid of fixed-size spatial decomposition domains to 3 -D Cray Gemini and 5 -D IBM Blue Gene/Q toroidal networks to enable hundred-million atom full machine simulations, and to similarly partition node allocations into compact domains for smaller simulations using multiple-copy algorithms. Additional enabling techniques are discussed and performance is reported for NCSA Blue Waters, ORNL Titan, and ANL Mira...|$|R
5000|$|The {{high-performance}} Dawning 6000 {{was announced}} in 2011, at 300 TFLOPS, incorporating 3000 8-core Godson 3B processors (a 64-bit MIPS architecture) at 3.2 GFLOP/W, is the [...] "first supercomputer made exclusively of Chinese components", which has a projected speed of over a PFLOP (one quadrillion operations per second). For comparison, the fastest supercomputer as of June 2014 runs at 33 PFLOPS. Dawning 6000 is currently jointly developed by the Institute of Computing Technology under the Chinese Academy of Sciences and the Dawning Information Industry Company. The same announcement said that a <b>petascale</b> <b>supercomputer</b> was under development and that the launch was anticipated in 2012 or 2013.|$|E
5000|$|Tachyon was {{originally}} {{developed for the}} Intel iPSC/860, a distributed memory parallel computer based on a hypercube interconnect topology based on the Intel i860, an early RISC CPU with VLIW architecture and [...] Tachyon {{was originally}} written using Intel's proprietary NX message passing interface for the iPSC series, but it was ported to the earliest versions of MPI shortly thereafter in 1995. Tachyon was adapted to run on the Intel Paragon platform using the Paragon XP/S 150 MP at Oak Ridge National Laboratory. The ORNL XP/S 150 MP was the first platform Tachyon supported that combined both large-scale distributed memory message passing among nodes, and shared memory multithreading within nodes. Adaptation of Tachyon {{to a variety of}} conventional Unix-based workstation platforms and early clusters followed, including porting to the IBM SP2. Tachyon was incorporated into the PARAFLOW CFD code to allow in-situ volume visualization of supersonic combustor flows performed on the Paragon XP/S at NASA Langley Research Center, providing a significant performance gain over conventional post-processing visualization approaches that had been used previously. [...] Beginning in 1999, support for Tachyon was incorporated into the molecular graphics program VMD, and this began an ongoing period co-development of Tachyon and VMD where many new Tachyon features were added specifically for molecular graphics. Tachyon was used to render the winning image illustration category for the NSF 2004 Visualization Challenge. [...] In 2007, Tachyon added support for ambient occlusion lighting, {{which was one of the}} features that made it increasingly popular for molecular visualization in conjunction with VMD. VMD and Tachyon were gradually adapted to support routine visualization and analysis tasks on clusters, and later for large petascale supercomputers. Tachyon was used to produce figures, movies, and the Nature cover image of the atomic structure of the HIV-1 capsid solved by Zhao et al. in 2013, on the Blue Waters <b>petascale</b> <b>supercomputer</b> at NCSA, U. Illinois.|$|E
40|$|AbstractAs the Pawsey Centre project continues, in 2013 iVEC was {{tasked with}} {{deciding}} which accelerator technology {{to use in}} the <b>petascale</b> <b>supercomputer</b> to be delivered in mid 2014. While accelerators provide impressive performance and efficiency, an important factor in this decision is the usability of the technologies. To assist in the assessment of technologies, iVEC conducted a code sprint where iVEC staff and advanced users were paired to make use of a range of tools to port their codes to two architectures. Results of the sprint indicate that certain subtasks could benefit from using the tools in the code-acceleration process; however, there will be many hurdles users will face in migrating to either of the platforms explored...|$|E
40|$|Procurement and the {{optimized}} {{utilization of}} <b>Petascale</b> <b>supercomputers</b> and centers is a renewed national priority. Sustained performance {{and availability of}} such large centers is a key technical challenge significantly impacting their usability. Storage systems {{are known to be}} the primary fault source leading to data unavailability and job resubmissions. This results in reduced center performance, partially {{due to the lack of}} coordination between I/O activities and job scheduling. In this work, we propose the coordination of job scheduling with data staging/offloading and on-demand staged data reconstruction to address the availability of job input data and to improve centerwide performance. Fundamental to both mechanisms is the efficient management of transient data: in the way it is scheduled and recovered. Collectively, from a centerâ€™s standpoint, these techniques optimize resource usage and increase its data/service availability. From a userâ€™s standpoint, they reduce the job turnaround time and optimize the allocated time usage...|$|R
40|$|AbstractHazardous {{scenarios}} involving explosives {{are difficult}} to experimentally study and simulation is often the only viable approach to study highly reactive phenomena. Explosive simulations are computationally expensive, requiring supercomputing resources for continued scientific discovery in the field. Here an idealized mesoscale simulation of explosive grains under mechanical insult by a high-speed projectile with reaction represented by a novel kinetic model is designed to test the scalability of the Uintah software on <b>petascale</b> <b>supercomputers.</b> Good scalability is found up to 49 K processors. Timing breakdown of compu- tational tasks are determined with relocation of Lagrangian particles and interpolation of those particles to the grid identified as the most expensive operation and ideal for optimization. Potential optimization strategies are identified. Realistic model simulations rather than toy model simulations are found to better represent scalability of a science code on a supercomputer. Estimations for total supercomputer hours necessary to complete the kinetic model validation study are reported...|$|R
40|$|Abstract. Applications that {{generate}} bursty I/O load, like checkpointing, require additional support to perform efficiently on next generation <b>petascale</b> <b>supercomputers.</b> Tens {{of thousands of}} processors, generating terabytes of snapshot data at once at each timestep, can easily overwhelm a storage system. Further, even at the current peak I/O bandwidth rates, offered by parallel file system deployments at leadership class facilities, an application is likely to spend {{a significant portion of}} its runtime checkpointing. To address these issues, we propose a checkpoint storage device, built from memory resources, that acts as an intermediary to the central parallel file system. Our system comprises of a dedicated manager that aggregates memory resources from processors (benefactors) and makes it available as a collective space for checkpointing clients, using a standard POSIX file system interface. We argue that such a system has the potential to alleviate the I/O bandwidth bottleneck for bursty I/O operations like checkpointing by aggregating memory and interprocessor bandwidth...|$|R
40|$|Abstractâ€”In {{this paper}} {{we present a}} {{framework}} to enable data-intensive Spark workloads on MareNostrum, a <b>petascale</b> <b>supercomputer</b> designed mainly for compute-intensive applica-tions. As far as we know, {{this is the first}} attempt to investigate optimized deployment configurations of Spark on a petascale HPC setup. We detail the design of the framework and present some benchmark data to provide insights into the scalability of the system. We examine the impact of different configurations including parallelism, storage and networking alternatives, and we discuss several aspects in executing Big Data workloads on a computing system that is based on the compute-centric paradigm. Further, we derive conclusions aiming to pave the way towards systematic and optimized methodologies for fine-tuning data-intensive application on large clusters emphasizing on parallelism configurations. I...|$|E
40|$|Part 8 : High Performance Computing and BigDataInternational audienceA 3 -year {{investigation}} is underway into {{the performance of}} applications used in the Australian Community Climate and Earth System Simulator on the <b>petascale</b> <b>supercomputer</b> Raijin hosted at the National Computational Infrastructure. Several applications {{have been identified as}} candidates for this investigation including the UK MetOfficeâ€™s Unified Model (UM) atmospheric model and Princeton Universityâ€™s Modular Ocean Model (MOM). In this paper we present initial results of the investigation of the performance and scalability of UM and MOM on Raijin. We also present initial results of a performance study on the data assimilation package (VAR) developed by the UK MetOffice and used by the Australian Bureau of Meteorology in its operational weather forecasting suite. Further investigation and optimization is envisioned for each application investigated and will be discussed...|$|E
40|$|In {{this paper}} {{we present a}} {{framework}} to enable data-intensive Spark workloads on MareNostrum, a <b>petascale</b> <b>supercomputer</b> designed mainly for compute-intensive applications. As far as we know, {{this is the first}} attempt to investigate optimized deployment configurations of Spark on a petascale HPC setup. We detail the design of the framework and present some benchmark data to provide insights into the scalability of the system. We examine the impact of different configurations including parallelism, storage and networking alternatives, and we discuss several aspects in executing Big Data workloads on a computing system that is based on the compute-centric paradigm. Further, we derive conclusions aiming to pave the way towards systematic and optimized methodologies for fine-tuning data-intensive application on large clusters emphasizing on parallelism configurations. Peer ReviewedPostprint (author's final draft...|$|E
40|$|For {{many kinds}} of problem the {{accuracy}} of quantum Monte Carlo (QMC) {{is much better than}} that of density functional theory (DFT), and its scaling with number of atoms is much more favourable than that of high-level quantum chemistry. However, the widespread use of QMC has been hindered {{by the fact that it}} is considerably more expensive than DFT. We show here that QMC is very well placed to exploit the power of <b>petascale</b> <b>supercomputers</b> that are now becoming available, and we explain how this is opening up new scientific areas to investigation with QMC. We describe how we have been able to modify the Cambridge QMC code CASINO so that it runs with almost perfect parallel efficiency on 100000 cores and more on the JaguarPF machine at Oak Ridge. We also present illustrative results showing how QMC calculations run in this way are enabling us to go beyond the limitations of DFT in three important areas: the surface formation energies of materials, the adsorption energies of molecules on surfaces, and the energetics of water systems. ...|$|R
5000|$|The Argonne Leadership Computing Facility, which {{commissioned}} the supercomputer, {{was established by}} the America COMPETES Act, signed by President Bush in 2007, and President Obama in 2011. The United States' emphasis on supercomputing {{has been seen as}} a response to China's progress in the field. China's Tianhe-1A, located at the Tianjin National Supercomputer Center, was ranked the most powerful supercomputer in the world from October 2010 to June 2011. Mira is, along with IBM Sequoia and Blue Waters, one of three American <b>petascale</b> <b>supercomputers</b> deployed in 2012.The cost for building Mira has not been released by IBM. Early reports estimated that construction would cost US$50 million, and Argonne National Laboratory announced that Mira was bought using money from a grant of US$180 million. In a press release, IBM marketed the supercomputer's speed, claiming that [...] "if every man, woman and child in the United States performed one calculation each second, it would take them almost a year to do as many calculations as Mira will do in one second".|$|R
50|$|It {{was slowly}} phased out as its successors at NAS, the <b>petascale</b> Pleiades <b>supercomputer</b> and the Endeavour shared-memory system, {{expanded}} {{to meet with}} NASAâ€™s growing high-end computing needs. At {{the time of its}} decommissioning in March 2013, Columbia was made up of four nodes over 40 SGI Altix 4700 racks, containing Intel Itanium 2 Montecito and Montvale processors to make up a total of 4,608 cores with a theoretical peak of 30 teraflops and total memory of 9 terabytes.|$|R
