166|4|Public
50|$|Designs can be {{optimized}} {{when the}} design-space is constrained, for example, when the mathematical process-space contains factor-settings that are <b>practically</b> <b>infeasible</b> (e.g. due to safety concerns).|$|E
5000|$|Meta-modeling or Surrogate {{model is}} a process to win the {{mathematical}} relationship between design parameters and product characteristics. For each point in the parameter space, there is a corresponding point of the design space. Many model calculations should be performed to show the relationship between input and output systematically (Full Factorial Design). For a high computing effort of the product model, it is <b>practically</b> <b>infeasible.</b> Adaptive response surface methodology {{can be used to}} solve this problem.|$|E
50|$|Helmut Schreyer advised Konrad Zuse to use {{electrical}} circuit technology to implement computers, but he first considered it <b>practically</b> <b>infeasible</b> and then {{could not get}} the necessary funding. Up to 1942 Schreyer himself built an experimental model of a computer using 100 vacuum tubes, which was lost {{at the end of}} the war. Schreyer planned to build a computer memory for 1000 words in 1943, that was to contain several thousand electron tubes, but the course the war took put an end to all larger plans. In 1944 he built an {{electrical circuit}} to convert decimal to binary numbers.|$|E
40|$|The {{intrinsically}} location-aware {{applications of}} sensor networks {{and the need}} for location-based optimized routing make efficient and accurate location determination of sensor nodes essential. It is <b>practically</b> and economically <b>infeasible</b> to equip all sensor nodes with GPS receivers. A few GPS equipped anchors are therefore used to determine the location of sensor nodes in the field. Localization techniques which use lateration require at least k+ 1 anchors to compute the location of all the sensor nodes under their coverage area in a k dimensional plane. In a twodimensional plane, for example, sensor nodes which are not in the range of at least three anchor nodes cannot estimate their location. In sensor networks {{it is often difficult to}} set up three anchors to cover all sensor nodes. We propose a simple technique which requires only a single anchor node to determine the location of sensor nodes in a two-dimensional plane. 1...|$|R
40|$|The formal {{verification}} of embedded systems {{is becoming a}} key research area due to the ever increasing design complexity involved in the modelling and validation of embedded systems. Traditional methods of validation, such as simulation and testing, are <b>practically</b> becoming an <b>infeasible</b> solution for large design models. Nowadays, {{only a small part}} of a real-life embedded system's state space can be explored by such traditional methods and, therefore, alternative ways of reasoning about the correctness of an embedded system are rapidly gaining popularity in hardware [1] and software [2] verification. The model checking [3] approach has been successfully applied to the verification of finite state concurrent systems, and is becoming of much interest in both industry and academia. This approach automatically verifies a system model, given a set of properties expressed in temporal logics [4], e. g. Computation Tree Logic (CTL) or Linear Temporal Logic (LTL) [5]...|$|R
40|$|With the {{proliferation}} of {{wireless local area network}} (WLAN) technologies, wireless Internet access via public hotspots will become a necessity in the near future. In outdoor areas where the installation {{of a large number of}} wired access points is <b>practically</b> or economically <b>infeasible,</b> mobile users located at the edge of the network communicate with the access point at a very low rate and in turn waste network resources. In this work, we promote the use of tetherless relay points (TRPs) to improve the throughput of a WLAN in such environments. We first provide a high level description on how to integrate TRPs in a multi-rate WLAN architecture. We then propose an integer-programming optimization formulation and an iterative approach to compute the best placement of a fixed number of TRPs. Finally, we show in numerical analysis, through a case study based on relay-enabled rate adaptation and IEEE 802. 11 -like multi-rate physical model with Rayleigh fading, that for a wide range of system parameters, significant performance gain can be achieved when TRPs are strategically installed in the network...|$|R
50|$|When {{the user}} logs on, the {{password}} entered {{by the user}} during the log on process is run through the same key derivation function and the resulting hashed version is compared with the saved version. If the hashes are identical, the entered password {{is considered to be}} correct, and the user is authenticated. In theory, it is possible for two different passwords to produce the same hash. However, cryptographic hash functions are designed {{in such a way that}} finding any password that produces the same hash is very difficult and <b>practically</b> <b>infeasible,</b> so if the produced hash matches the stored one, the user can be authenticated.|$|E
50|$|Zuse {{designed}} the Z1 in 1935 to 1936 and built it from 1936 to 1938. The Z1 was wholly mechanical and only {{worked for a}} few minutes at a time at most. Helmut Schreyer advised Zuse to use a different technology. As a doctoral student at the Berlin Institute of Technology in 1937 he worked on the implementation of Boolean operations and (in today's terminology) flip-flops on the basis of vacuum tubes. In 1938 Schreyer demonstrated a circuit on this basis to a small audience, and explained his vision of an electronic computing machine - but since the largest operational electronic devices contained far fewer tubes this was considered <b>practically</b> <b>infeasible.</b> In that year when presenting the plan for a computer with 2,000 electron tubes, Zuse and Schreyer, who was an assistant at Prof. Wilhelm Stäblein's Telecommunication Institute at the Technical University of Berlin, were discouraged by members of the institute who knew about the problems with electron tube technology.|$|E
5000|$|Since the {{abandonment}} of propiska system in 1993, a new legislation on registration was passed instead. Unlike propiska which was a permit to reside in a certain area, registration as worded in the law is merely notification. However, administrative procedures developed [...] "in implementation" [...] of the registration law imposed such conditions on registration which effectively made it depending on the landlord's assent. As landlords, for various reasons, are not interested to register tenants or guests in their properties, many of internal migrants are prevented from executing their legal duty to register. Before 2004, it was common for police to fine those having failed to register within 3 working days at a place of stay. In 2004, the maximum permitted registration lag was raised to 90 days thus making such a prosecution <b>practically</b> <b>infeasible.</b> Thus now there are no obstacles to movement of citizens per se. Nevertheless, since registration is {{the primary source of}} one's address for legal purposes, many internal migrants still are de facto second-class citizens deprived of their right to vote, obtain a passport or driver's license etc.|$|E
40|$|Abstract—Maximum multiflow {{and maximum}} {{concurrent}} mul-tiflow in multi-channel multi-radio (MC-MR) wireless networks have been well-studied in the literature. They are NP-hard even in single-channel single-radio (SC-SR) wireless networks when all nodes have uniform (and fixed) interference radii and {{the positions of}} all nodes are available. While they admit a polynomial-time approximation scheme (PTAS) {{when the number of}} channels is bounded by a constant, such PTAS is quite <b>infeasible</b> <b>practically.</b> Other than the PTAS, all other known approximation algorithms, in both SC-SR wireless networks and MC-MR wireless networks, resorted to solve a polynomial-sized linear program (LP) exactly. The scalability of their running time is fundamentally limited by the general-purposed LP solvers. In this paper, we first introduce the concept of interference costs and prices of a path and explore their relations with the maximum (concurrent) multiflow. Then we develop purely combinatorial approximation algorithms which compute a sequence of least interference-cost routing paths along which the flows are routed. These algorithms are faster and simpler, and achieve nearly the same approximation bounds known in the literature. I...|$|R
5000|$|Clearly, kernel PCR has a {{discrete}} shrinkage {{effect on the}} eigenvectors of K', quite similar to the discrete shrinkage effect of classical PCR on the principal components, as discussed earlier. However, {{it should be noted}} that the feature map associated with the chosen kernel could potentially be infinite-dimensional, and hence the corresponding principal components and principal component directions could be infinite-dimensional as well. Therefore, these quantities are often practically intractable under the kernel machine setting. Kernel PCR essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix. Under the linear regression model (which corresponds to choosing the kernel function as the linear kernel), this amounts to considering a spectral decomposition of the corresponding [...] kernel matrix [...] and then regressing the outcome vector on a selected subset of the eigenvectors of [...] so obtained. It can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components (which are finite-dimensional in this case), as defined in the context of the classical PCR. Thus, for the linear kernel, the kernel PCR based on a dual formulation is exactly equivalent to the classical PCR based on a primal formulation. However, for arbitrary (and possibly non-linear) kernels, this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map. Thus classical PCR becomes <b>practically</b> <b>infeasible</b> in that case, but kernel PCR based on the dual formulation still remains valid and computationally scalable.|$|E
50|$|After the {{appointment}} of Ruge as Commanding General on 10 April, the Norwegian strategy was to fight delaying actions against the Germans advancing northwards from Oslo to link up with the invasion forces at Trondheim. The main aim of the Norwegian effort in Eastern Norway was to give the Allies enough time to recapture Trondheim, and start a counter-offensive against the German main force in the Oslo area. The region surrounding the Oslofjord was defended by the 1st Division, commanded by Major General Carl Johan Erichsen. The rest of the region was covered by the 2nd Division, commanded by Major General Jacob Hvinden Haug. Having been prevented from mobilizing {{in an orderly fashion}} by the German invasion, improvised Norwegian units were sent into action against the Germans. Several of the units facing the German advance were led by officers especially selected by Ruge to replace commanders who had failed to show sufficient initiative and aggression {{in the early days of}} the campaign. The German offensive aimed at linking up their forces in Oslo and Trondheim began on 14 April, with an advance north from Oslo towards the Gudbrandsdalen and Østerdalen valleys. Hønefoss was the first town to fall to the advancing German forces. North of Hønefoss the Germans began meeting Norwegian resistance, first delaying actions and later units fighting organized defensive actions. During intense fighting with heavy casualties on both sides, troops of the Norwegian Infantry Regiment 6 blunted the German advance at the village of Haugsbygd on 15 April. The Germans only broke through the Norwegian lines at Haugsbygd the next day after employing panzers for the first time in Norway. Lacking anti-tank weapons, the Norwegian troops could not hold back the German attack.The basis for the Norwegian strategy started collapsing already on 13 and 14 April, when the 3,000 troops of the 1st Division in Østfold evacuated across the Swedish border without orders, and were interned by the neutral Swedes. The same day that the 1st Division began crossing into Sweden, the two battalions of Infantry Regiment no. 3 at Heistadmoen Army Camp in Kongsberg capitulated. The 3rd Division, commanded by Major General Einar Liljedahl and tasked with defending Southern Norway, surrendered to the Germans in Setesdal on 15 April, having seen no action up to that point. Some 2,000 soldiers marched into captivity in the Setesdal capitulation. With the abandonment on 20 April of the Franco-British plans for recapturing the central Norwegian city of Trondheim, Ruge's strategy became <b>practically</b> <b>infeasible.</b>|$|E
30|$|Second, hash {{functions}} are one-way algorithms. Therefore, finding the original key from the hash results demands high effort {{which can make}} it <b>practically</b> <b>infeasible</b> [4].|$|E
40|$|For {{modelling}} {{and evaluating}} {{the performance of}} modern computer communication systems, quasi-birth-and-death models (QBDs) can often be used. For such QBDs, a variety of highly e#cient numerical solution algorithms have been developed. For models of practically relevant size though, the specification of QBDs {{at the state level}} is cumbersome or <b>practically</b> <b>infeasible.</b> To overcome thi...|$|E
40|$|An {{interesting}} {{issue of}} contemplation amongst researchers working on multi-projector displays is whether spatial superresolution {{can be achieved}} by overlapping images from multiple projectors. This paper presents a thorough theoretical analysis to answer this question using signal processing and perturbation theory. Our analysis is supported by results from a simulated overlapping projector display. This analysis shows that achieving spatial super-resolution using overlapping projectors is <b>practically</b> <b>infeasible...</b>|$|E
40|$|The {{bootstrap}} is {{a convenient}} tool for calculating standard errors of {{the parameters of}} complicated econometric models. Unfortunately, {{the fact that these}} models are complicated often makes the bootstrap extremely slow or even <b>practically</b> <b>infeasible.</b> This paper proposes an alternative to the bootstrap that relies only on the estimation of one-dimensional parameters. The paper contains no new difficult math. But we believe that it can be useful...|$|E
40|$|We {{propose a}} {{framework}} to encrypt Baseline JPEG files directly at bitstream level, i. e., {{without the need}} to recompress them. Our approach enables encrypting more than 25 pictures per second in VGA resolution, allowing real-time operation in typical video surveillance applications. In addition, our approach preserves {{the length of the}} bitstream while being completely format-compliant. Furthermore, we show that an attack on the encryption process, which partly relies on AES, is <b>practically</b> <b>infeasible...</b>|$|E
40|$|This paper {{claims that}} a new field of Software Engineering {{research}} and practice is emerging: Search-Based Software Engineering. The paper argues that Software Engineering is ideal {{for the application of}} metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and sometimes inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or <b>practically</b> <b>infeasible...</b>|$|E
30|$|Harman and Jones (2001) {{stated that}} {{software}} engineering {{is ideal for}} the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. They argued that such search-based techniques could provide solutions to the difficult problems of balancing competing (and sometimes inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or <b>practically</b> <b>infeasible.</b> In their work they briefly set out key ingredients for successful reformulation and evaluation criteria for search-based software engineering.|$|E
40|$|Web {{archives}} {{preserve the}} history of Web sites and have high long-term value for media and business analysts. Such archives are maintained by periodically re-crawling entire Web sites of interest. From an archivist’s point of view, the ideal case to ensure highest possible data quality of the archive would be to “freeze ” the complete contents of an entire Web site during the time span of crawling and capturing the site. Of course, this is <b>practically</b> <b>infeasible.</b> To comply with the politeness specification of a Web site, the crawler needs to pause between subsequen...|$|E
40|$|We {{propose a}} new method to encrypt {{baseline}} JPEG bit streams by selective Huffman code word swapping and coef-ficient value scrambling based on AES encryption. Further-more, {{we show that}} our approach preserves {{the length of the}} bit stream while being completely format-compliant. In con-trast to most existing approaches, no recompression is nec-essary as the encryption is applied directly to the bit stream. In addition, we assess the effort required for brute-force and known-plaintext attacks on pictures encrypted with our ap-proach, showing that both are <b>practically</b> <b>infeasible.</b> Categories and Subject Descriptor...|$|E
40|$|Wireless Sensor Networks (WSNs) {{present a}} new {{generation}} of real-time embedded systems with limited computation, energy and memory resources. They are being used {{in a wide variety of}} applications where traditional networking infrastructure is <b>practically</b> <b>infeasible.</b> Appropriate cluster-head node election can drastically reduce the energy consumption enhancing so the network lifetime. In this paper, a fuzzy logic approach to cluster-head election is proposed based on three descriptors - energy, concentration and centrality of nodes. Simulation shows that depending upon network configuration, a substantial increase in network lifetime can be accomplished as compared to probabilistically selecting the nodes as cluster-heads using only local information...|$|E
40|$|In {{this paper}} we {{investigate}} the continuum limits {{of a class}} of Markov chains. The investigation of such limits is motivated {{by the desire to}} model very large networks. We show that under some conditions, a sequence of Markov chains converges in some sense to the solution of a partial differential equation. Based on such convergence we approximate Markov chains modeling networks with a large number of components by partial differential equations. While traditional Monte Carlo simulation for very large networks is <b>practically</b> <b>infeasible,</b> partial differential equations can be solved with reasonable computational overhead using well-established mathematical tools. Comment: 15 pages, 10 figure...|$|E
40|$|Over {{the last}} 20 years or so, {{a number of}} Bayesian {{researchers}} and groups have invested {{a good deal of}} time, effort and money in parallel computing for Bayesian analysis. The growth of “small research group ” to “institutionally supported ” cluster computational facilities has had a substantial impact on a number of areas of Bayesian analysis, enabling analyses that are otherwise <b>practically</b> <b>infeasible.</b> Parallel computing has also motivated new approaches to simulation and optimisation-based Bayesian computations that aim to maximally exploit the “master-slave ” and “embarrassingly parallel ” computational model [e. g., 3, 4, 6]. In more recent years, increasingly prevalen...|$|E
40|$|This paper {{discusses}} an asymmetric cryptosystem C* {{which consists}} of public transformations of compleyJty O(m 2 n 3) and secret transformations of complexity O((mn) 2 (m + logn)), where each complex- ity is measured in {{the total number of}} bit-operations for processing an ran-bit message block. Each public key of C* is an n-tuple of quadratic n-variate polynomials over GF(2 '") and can be used for both verifying signatures and encrypting plaintexts. This paper also shows that for C* it is <b>practically</b> <b>infeasible</b> to extract the n-tuple of n-variate polynomials representing the inverse of the corresponding public key...|$|E
40|$|Abstract — Finite-Stage Markov Decision Process (MDP) {{supplies}} {{a general}} framework for many practical problems when only the {{performance in a}} finite duration is of interest. Dynamic programming (DP) supplies a general way to find the optimal policies but is usually <b>practically</b> <b>infeasible,</b> due to the exponentially increasing policy space. Approximating the finitestage MDP by an infinite-stage MDP reduces the search space but usually does not find the optimal stationary policy, due to the approximation error. We develop a method that finds the optimal stationary policies for the finite-stage MDP. The method is based on performance potentials, which can be estimated through sample paths and thus suits practical application...|$|E
40|$|Obtaining {{an optimal}} {{schedule}} for assigning tasks onto distributed memory mac hines (DMMs) {{has been proven}} to be NP-Complete. Several heuristic solutions to the scheduling problem have been proposed {{which are based on}} certain assumptions that allow the algorithm to be executed in a polynomial time bound. Even though the heuristics do not guarantee an optimal solution, they have been shown to perform reasonably well for many applications. It is di cult to judge the performance of these heuristics because for agiven directed acyclic graph (DAG), it is <b>practically</b> <b>infeasible</b> to obtain the optimal solution. This paper proposes a method to compute the lowerbound in a polynomial time bound. The ide...|$|E
40|$|Genetic {{algorithms}} {{are commonly}} used for automatically solving complex design problem because exploration using genetic algorithms can consistently deliver good results when the algorithm is given a long enough run-time. However, the exploration time for problems with huge design spaces can be very long, often making exploration using a genetic algorithm <b>practically</b> <b>infeasible.</b> In this work, we present a genetic algorithm for exploring the instruction-set architecture of VLIW ASIPs and demonstrate its effectiveness by comparing it to two heuristic algorithms. We present several optimizations to the genetic algorithm configuration, and demonstrate how caching of intermediate compilation and simulation results can reduce the exploration time by an order of magnitude...|$|E
40|$|This paper {{considers}} optimal {{testing of}} model comparison hypotheses for misspeci…ed unconditional moment restriction models. We adopt the generalized Neyman-Pearson optimality criterion, {{which focuses on}} the convergence rates of the type I and II error probabilities under …xed global alternatives, and derive an optimal but <b>practically</b> <b>infeasible</b> test. We then propose feasible approximation test statistics to the optimal one. For linear instrumental variable regression models, the conventional empirical likelihood ratio test statistic emerges. For general nonlinear moment restrictions, we propose a new test statistic based on an iterative algorithm. We derive the asymptotic properties of these test statistics. JEL classi…cation: C 12; C 14; C 5...|$|E
40|$|The optical power penalty (OPP) {{due to the}} {{artificial}} light interferences (ALIs) can be significantly high in an indoor optical wireless communication (OWC) channel making such link <b>practically</b> <b>infeasible.</b> A discrete wavelet transform (DWT) is an effective technique in reducing the ALI effects. The DWT has the advantage over the high pass filtering (HPF) to reduce ALI in terms of complexity and performance. In this paper, a comprehensive study of the DWT based denoising for the on-off keying (OOK), pulse position modulation (PPM) and digital pulse interval modulation (DPIM) is provided. The OPPs due to ALIs and DWT based denoising for these modulation techniques are presented...|$|E
40|$|Hand-held {{devices have}} rigid {{constraints}} regarding power dissipation and energy consumption. Whether a new functionality can be supported often depends upon its power requirements. Concerns about the area (or cost) are generally addressed after a design {{can meet the}} performance and power requirements. Different micro-architectures have very different area, timing and power characteristics, and these need RTL-level models to be evaluated. In this paper we discuss the microarchitectural exploration of an 802. 11 a transmitter via synthesizable and highly-parametrized descriptions written in Bluespec SystemVerilog (BSV). We also briefly discuss why such architectural exploration would be <b>practically</b> <b>infeasible</b> without appropriate linguistic facilities. No knowledge of 802. 11 a or BSV is needed to read this paper. 1...|$|E
40|$|Designing custom-extensible {{instructions}} for Extensible Processors 1 cess. One {{of the most}} challenging and so far unsolved steps is a computationally complex task because of the large design space. The task of automatically matching candidate instructions in an application (e. g. written in a high-level language) to a pre-designed library of extensible instructions is especially challenging. Previous approaches have focused on identifying extensible instructions (e. g. through profiling), synthesizing extensible instructions, estimating expected performance gains etc. In this paper we introduce our approach of automatically matching extensible instructions as this key step is missing in automating the entire design flow of an ASIP with extensible instruction capabilities. Since matching using simulation is <b>practically</b> <b>infeasible</b> (simulation time), and traditiona...|$|E
40|$|Summary points The ever-growing HIV {{treatment}} programs in sub-Saharan Africa (SSA) present local policy makers with complex decision dilemmas, as international guidelines emphasize {{the need for}} expanded access to antiretroviral therapy (ART), yet funding has flatlined. We argue that the current evidence base for prioritizing ART scale-up strategies results in recommendations that are theoretically optimal but <b>practically</b> <b>infeasible</b> to implement. Cost-effectiveness analyses (CEAs) of scaling up ART in SSA should be made more responsive {{to the needs of}} policy makers by taking into account the local health system. We provide suggestions for a better integration of health system constraints into CEA by integrating supply- and demand-side constraints in mathematical models and improving the dialogue between researchers and policy makers...|$|E
40|$|Wireless sensor Networks {{present a}} new {{generation}} of real time embedded systems with limited computation, energy and memory resources that are being used in wide variety of applications where traditional networking infrastructure is <b>practically</b> <b>infeasible.</b> Legitimate leader selection can drastically improve the lifetime of the sensor network. This paper proposes fuzzy logic methodology for leader election in PEGASIS based protocol PEGASIS-TC [1, 17] based on two descriptors- residual energy of node and its proximity to Base Station. Simulation results show that depending on network configuration, a substantial increase in stability of network lifetime can be accomplished as compared to PEGASIS-TC. General Terms Topology control, fuzzy based leader selection, PEGASIS. Indexing terms Topology controlled PEGASIS, Fuzzy based leader selection, lifetime enhancement, wireless sensor network...|$|E
40|$|Abstract—Genetic {{algorithms}} {{are commonly}} used for auto-matically solving complex design problem because exploration using genetic algorithms can consistently deliver good results when the algorithm is given a long enough run-time. However, the exploration time for problems with huge design spaces can be very long, often making exploration using a genetic algorithm <b>practically</b> <b>infeasible.</b> In this work, we present a genetic algorithm for exploring the instruction-set architecture of VLIW ASIPs and demonstrate its effectiveness by comparing it to two heuristic algorithms. We present several optimizations to the genetic algorithm configuration, and demonstrate how caching of intermediate compilation and simulation results can reduce the exploration time by an order of magnitude. Index Terms—instruction-set, very-long instruction word, ge-netic algorithm, design space exploration I...|$|E
40|$|Gifted education’s most {{pressing}} problem, {{according to its}} critics, {{is a lack of}} racial, cultural, and socioeconomic diversity. This lack of diversity {{can be attributed to the}} fractured nature of gifted education’s historical development, and the also fractured development of its very independent and numerous stakeholders. By the 20 th century, these factors caused an overreaching regulatory structure to be <b>practically</b> <b>infeasible.</b> This policy proposal attempts to push back against historical precedent and begin a process of implementing overarching guidelines for gifted education programs in the United States based on a Controlled Choice model of admissions for gifted and talented pro-grams that receive federal funding. The new federal Special Task Force on Equity in Excellence will be tasked with enforcing and overseeing this policy change...|$|E
