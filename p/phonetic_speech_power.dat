0|465|Public
5000|$|... 2005: <b>Phonetic</b> {{knowledge}} in <b>speech</b> technology - and <b>phonetic</b> knowledge from <b>speech</b> technology (Springer Verlag) ...|$|R
40|$|Methods {{involving}} <b>phonetic</b> <b>speech</b> recognition {{are discussed}} for detecting Persian-accented English. These methods offer promise {{for both the}} identification and mitigation of L 2 pronunciation errors. Pronunciation errors, both segmental and suprasegmental, particular to Persian speakers of English are discussed...|$|R
5000|$|In {{the context}} of spoken {{language}}s, a phone is an unanalyzed sound of a language [...] A phone is a speech segment that possesses distinct physical or perceptual properties and serves as the basic unit of <b>phonetic</b> <b>speech</b> analysis. Phones are generally either vowels or consonants.|$|R
40|$|We {{discuss the}} notion of {{language}} and dialect-specific search {{in the context of}} audio indexing. A system is described where users can find dialect or language-specific pronunciations of Afghan placenames in Dari and Pashto. We explore the efficacy of a <b>phonetic</b> <b>speech</b> recognition system employed in this task. ...|$|R
50|$|Phonetotopy is {{the concept}} that articulatory {{features}} as well as perceptual features of speech sounds are ordered in the brain {{in a similar way}} as tone (tonotopy), articulation and its somatosensory feedback (somatotopy), or visual location of an object (retinotopy). It is assumed that a phonetotopic ordering of speech sounds as well as of syllables can be found at a supramodal speech processing level (i.e. at a <b>phonetic</b> <b>speech</b> processing level) within the brain.|$|R
40|$|The {{truncation}} error for a two-pass decoder is analyzed in {{a problem of}} <b>phonetic</b> <b>speech</b> recognition for very demanding latency constraints (look-ahead length < 100 ms) and for applications where successive refinements of the hypotheses are not allowed. This is done empirically {{in the framework of}} hybrid MLP/HMM models. The ability of recurrent MLPs, as a posteriori probability estimators, to model time variations is also considered, and its interaction with the dynamic modeling in the decoding phase is shown in the simulations...|$|R
40|$|The report {{presents}} the latest results in continuous <b>speech</b> <b>phonetic</b> analisys, concerning {{the problems of}} stable and effective speech recognition. These results are obtained {{on the base of}} earlier discussed [4] approach. At the heart of approach underlies the analysis of dynamics of short cross-correlation function (CCF) parameters, namely the time location and the value of CCF side peak. This approach in natural manner gives rise to the so-called detection / recognition paradigm in <b>phonetic</b> <b>speech</b> processing. From the general point of view the paradigm implies the detection of statistically homogeneous signal fragments and their structure clarification [5]. Both detection and recognition procedures are in frames of approach mutually dependent and represent two sides of the one uniform processing. The effectiveness o...|$|R
40|$|Abstract — The {{truncation}} error for a two-pass decoder is analyzed in {{a problem of}} <b>phonetic</b> <b>speech</b> recognition for very demanding latency constraints (look-ahead length < 100 ms) and for applications where successive refinements of the hypotheses are not allowed. This is done empirically {{in the framework of}} hybrid MLP/HMM models. The ability of recurrent MLPs, as a posteriori probability estimators, to model time variations is also considered, and its interaction with the dynamic modeling in the decoding phase is shown in the simulations. I...|$|R
40|$|This paper {{focuses on}} the use of both text and phonetic infor-mation in a speech {{translation}} system in order to make trans-lation results more robust to speech recognition errors. Con-ventional statistical speech translation formulas are extended to exploit both text-form and <b>phonetic</b> <b>speech</b> recognition re-sults. A novel data-driven word/text tying algorithm is then proposed to group words based on both pronunciation similar-ity and meaning equivalency. In our speech-to-text translation experiments, significant improvement was achieved by using phonetic information and the proposed word tying algorithm. 1...|$|R
40|$|<b>Phonetic</b> <b>speech</b> {{retrieval}} is used {{to augment}} word based retrieval in spoken document retrieval systems, for {{in and out of}} vocabulary words. In this paper, we present a new indexing and ranking scheme using metaphones and a Bayesian phonetic edit distance. We conduct an extensive set of experiments using a hundred hours of HUB 4 data with ground truth transcript and twenty-four thousands query words. We show improvement of up to 15 % in precision compare to results obtained speech recognition alone, at a processing time of 0. 5 Sec per query. ...|$|R
40|$|A noise {{estimation}} {{algorithm is}} proposed for highly nonstationary noise environments. The noise estimate is updated by averaging the noisy <b>speech</b> <b>power</b> spectrum using a time and frequency dependent smoothing factor, which is adjusted based on signal presence probability in subbands. Signal presence is determined by computing {{the ratio of the}} noisy <b>speech</b> <b>power</b> spectrum to its local minimum, which is computed by averaging past values of the noisy <b>speech</b> <b>power</b> spectra with a look-ahead factor. The local minimum estimation algorithm adapts very quickly to highly non-stationary noise environments. This was confirmed with formal listening tests that indicated that our noise estimation algorithm when integrated in speech enhancement was preferred over other noise estimation algorithms. 1...|$|R
40|$|Recently, it {{has been}} shown that MMSE-based noise power estima-tion [1] results in an {{improved}} noise tracking performance with re-spect to minimum statistics-based approaches. The MMSE-based approach employs two estimates of the <b>speech</b> <b>power</b> to estimate the unbiased noise power. In this work, we improve the MMSE-based noise power estimator by employing a more advanced estimator of the <b>speech</b> <b>power</b> based on temporal cepstrum smoothing (TCS). TCS can exploit knowledge about the speech spectral structure. As a result, only one <b>speech</b> <b>power</b> estimate is needed for MMSE-based noise power estimation. Moreover, the presented estimator results in an improved noise tracking performance, especially in babble noise, where SNR improvements of 1 dB over the original MMSE-based approach can be observed. Index Terms — Noise <b>power</b> estimation, <b>speech</b> enhancement. 1...|$|R
50|$|According to {{proponents of}} reverse <b>speech,</b> <b>phonetic</b> {{reversal}} occurs unknowingly during normal speech.|$|R
50|$|In <b>speech,</b> <b>phonetic</b> pitch reset {{occurs at}} the {{boundaries}} (pausa) between prosodic units.|$|R
40|$|The rapid {{development}} of concatenative speech synthesis systems in resource scarce languages requires an efficient and accurate solution {{with regard to}} automated phonetic alignment. However, in this context corpora are often minimally designed {{due to a lack}} of resources and expertise necessary for large scale development. Under these circumstances many techniques toward accurate segmentation are not feasible and it is unclear which approaches should be followed. In this paper we investigate this problem by evaluating alignment approaches and demonstrating how these approaches can be applied to limit manual interaction while achieving acceptable alignment accuracy with minimal ideal resources. Index Terms: <b>speech</b> synthesis, <b>phonetic</b> <b>speech</b> segmentation, resource scarce languag...|$|R
50|$|In {{the late}} 1960s and early 1970s Donald Shankweiler and Michael Studdert-Kennedy of Haskins Laboratories used a {{dichotic}} listening technique (presenting different nonsense syllables) {{to demonstrate the}} dissociation of <b>phonetic</b> (<b>speech)</b> and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning {{is an integral part}} of language and is typically processed in the left cerebral hemisphere. A dichotic listening performance advantage for one ear is interpreted as indicating a processing advantage in the contralateral hemisphere. In another example, Sidtis (1981) found that healthy adults have a left-ear advantage on a dichotic pitch recognition experiment. He interpreted this result as indicating right-hemisphere dominance for pitch discrimination.|$|R
5000|$|The Cato Institute is {{concerned}} that most proposed responses to Citizens United will give [...] "Congress unchecked new power over spending on political <b>speech,</b> <b>power</b> that will be certainly abused." ...|$|R
50|$|His {{expertise}} - {{which included}} formal phonetics, the aerodynamic and physiological production of <b>speech,</b> <b>phonetic</b> peculiarities in <b>speech,</b> and an astounding ability to reproduce words, and even speeches, backwards - {{led him to}} be invited to the University of Michigan. There, he headed the English Language Institute and the Laboratory of Communicative Sciences (current the Laboratory of Phonetics). He taught most of the Linguistics subjects in the same university.|$|R
50|$|He was {{interested}} in the field of Linguistics - as well the <b>phonetic</b> transcriptions of <b>speech</b> in the English language.|$|R
50|$|The Player is set of APIs (e.g. position2d, bumper, ir, <b>speech,</b> <b>power)</b> {{that can}} be {{implemented}} by a robot chassis (Roomba, Khephera etc.), possibly over serial line or network, or by Stage (2D simulator) or Gazebo (3D simulator).|$|R
40|$|A {{neural network}} based feature {{dimensionality}} reduction for speech recognition is described for accurate <b>phonetic</b> <b>speech</b> recognition. In our previous work, a neural network based nonlinear {{principal component analysis}} (NLPCA) was proposed as a dimensionality reduction approach for speech features. It was shown that the reduced dimensionality features are very effective for representing data for vowel classification. In this paper, we extend this neural network based NLPCA approach for phonetic recognition using continuous speech. The reduced dimensionality features obtained with NLPCA are used as the features for HMM phone models. Experimental evaluation using the TIMIT database shows that recognition accuracies with NLPCA reduced dimensionality features are higher than recognition rates obtained with original features, especially when {{a small number of}} states and mixtures are used for HMM phonetic models. Index Terms: feature dimensionality reduction, neural networks, HMMs, speech recognitio...|$|R
40|$|This paper {{describes}} {{the use of}} connectionist techniques in <b>phonetic</b> <b>speech</b> recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular {{attention has been paid}} to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency. Key words: speech recognition, neural network, low latency, non-linear dynamics...|$|R
50|$|Gorfs {{most notable}} feature is its robotic {{synthesised}} <b>speech,</b> <b>powered</b> by the Votrax speech chip. One {{of the first}} games to allow the player to buy additional lives before starting the game, Gorf allows the player to insert extra coins to buy up to seven starting lives.|$|R
5000|$|The [...] "Political Recoreda" [...] in 1987 was {{conducted}} in Iloilo City to drum up support for the Draft Constitution {{and at the same}} time the members distributed copies of the constitution and primers. <b>Speech</b> <b>power</b> of SABAKA members were heard when they spoke in rallies and symposia.|$|R
40|$|Enhancement {{algorithms}} {{are widely}} used to overcome the degra-dation of noisy speech signals. Most enhancement algorithms re-quire {{an estimate of the}} noise and noisy <b>speech</b> <b>power</b> spectra in order to compute the gain function used for the noise suppression. The variance of these power spectral estimates degrades the qual-ity of the enhanced signal and smoothing techniques are therefore often used to decrease the variance. In this paper we present a method to determine the noisy <b>speech</b> <b>power</b> spectrum based on an adaptive time segmentation. More specifically, the proposed al-gorithm determines for each noisy frame which of the surrounding frames should contribute to the corresponding noisy power spec-tral estimate. Objective and subjective experiments show that an adaptive time segmentation leads to significant performance im-provements, particularly in transitional speech regions. 1...|$|R
5|$|Extensions to the International <b>Phonetic</b> Alphabet for <b>speech</b> {{pathology}} {{were created}} in 1990 and officially adopted by the International Clinical Phonetics and Linguistics Association in 1994.|$|R
5000|$|Studdert-Kennedy, M., Shankweiler, D., & Pisoni, D. (1972). Auditory and <b>phonetic</b> {{processes}} in <b>speech</b> perception: Evidence from a dichotic study. Journal of Cognitive Psychology, 2, 455-466.|$|R
40|$|With limited {{training}} data, infrequent triphone {{models for}} speech recognition {{will not be}} observed in sufficient number. In this report, a speech production approach is used to predict the characteristics of unseen triphones by concatenating diphones and/or monophones in the parametric representation of a formant speech synthesiser. The parameter trajectories are estimated by interpolation between the endpoints of the original units. The spectral states of the created triphone are generated by the speech synthesiser. Evaluation of the proposed technique has been performed using spectral error measurements and recognition candidate rescoring of N-best lists. In both cases, the created triphones are shown to perform better than the shorter units from which they were constructed. 1. INTRODUCTION The triphone unit is the basic phone model in many current <b>phonetic</b> <b>speech</b> recognition systems. The {{reason for this is}} that triphones capture the coarticulation effect caused by the immediate pr [...] ...|$|R
40|$|Acoustic and articulatory studies {{demonstrate}} covert {{contrast in}} perceptually neutralised phonemic contrasts in both typical children {{and children with}} speech disorders. These covert contrasts {{are thought to be}} relatively common and symptomatic of <b>phonetic</b> <b>speech</b> disorders. However, clinicians in the speech therapy clinic have had no easy way of identifying this covertness. This study uses ultrasound tongue imaging to compare tongue contours for /t/ and /k/ in seven children with persistent velar fronting. We present a method of overlaying tongue contours to identify covert contrast at the articulatory level. Results show that all seven children, contrary to expectations, produced both /t/ and /k/ with near-identical tongue shapes showing no evidence of covert contrast. However, further analysis of one of the participants showed highly variable tongue shapes for /t/ and /k/, including retroflex productions of both. Although not phonologically conditioned, this covert error is evidence of speech disorder at the phonetic level...|$|R
5000|$|Michael Studdert-Kennedy's {{early work}} at Haskins Laboratories {{included}} {{work on their}} reading machine project. In the 1960s, Studdert-Kennedy and Donald Shankweilerhttp://www.haskins.yale.edu/staff/shankweiler.html used a dichotic listening technique (presenting different nonsense syllables simultaneously to opposite ears) to demonstrate the dissociation of <b>phonetic</b> (<b>speech)</b> and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning {{is an integral part}} of language, typically processed in the left cerebral hemisphere. Alvin Liberman, Franklin S. Cooper, Shankweiler, and Studdert-Kennedy summarized and interpreted fifteen years of research in [...] "Perception of the Speech Code," [...] still among the most cited papers in the speech literature. It set the agenda for many years of research at Haskins and elsewhere by describing speech as a code in which speakers overlap (or coarticulate) segments to form syllables. In recent years he has written a number of papers on the evolution of language, including work with Louis Goldstein.|$|R
40|$|Manual Cued Speech (MCS) is a {{supplement}} to speechreading that reduces lipreading errors by reducing the ambiguity of lip shapes. The effects of cues on the reception of Cued Speech were studied to determine the feasibility of building a real-time au-tomatic cueing system that would employ a <b>phonetic</b> <b>speech</b> recognizer. Synthetic cues based on the groups of MCS were superimposed on prerecorded sentences and simulated phone errors consisting of substitutions, deletions and insertions were intro-duced at rates {{similar to those of}} current ASRs. The effect of delayed cue presentation was also explored. The synthetic cues were discrete and presented at a slightly faster rate than MCS. Five deaf, experienced cue receivers were tested on their ability to speechread key words in sentences under 12 synthetic cue conditions. Results were compared with their scores on SA and MCS. Although scores on the perfect synthetic cues (PSC) condition did not quite attain the high scores obtained with MCS, differ-ences between the two conditions were not statistically significant. Subjects seeme...|$|R
40|$|The {{problems}} of increasing noise immunity {{of the digital}} algorithms for processing of the speech signals in the real acoustic situation have been investigated. The algorithms permitting to increase the articulation in the digital codecs have been developed. The statistical characteristics of the speech signal withous pauses have been made accurate, {{the model of the}} multicomponent acoustic interferences has been developed, the codecs of the speech signals at action of the acoustic interferences and widened bandpass of the speech signal have been used, the system for suppression of the acoustic interference complex has been developed, the noise immunity problems in the algorithms for <b>phonetic</b> <b>speech</b> representation have been investigated, the possibility for practical realization of the communication systems at action of the acoustic interferences has been analysed. The results have been introduced into the educational process of the Ryazan State Radiotechnical Academy and researchs of the Joint-Stock Company of Open Type "Ryazan Radio Works"Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Abstract: This paper {{deals with}} the problem of {{checking}} the consistency of the speech corpus during recording in terms of the level of <b>speech</b> <b>power</b> of individual recordings. The question was whether or not setting of the limits of RMS value is useful for checking the volume consistency of recordings destinated for unit selection speech synthesis...|$|R
50|$|On {{the day of}} his 18th birthday, Billy {{attended}} a <b>Speech</b> <b>Power</b> Toastmasters course, and registered his first finance business called the Australian Credit Network. He pursued a career in finance for approximately 15 years, working as a broker, financial planner and corporate banker. In 2011, he was awarded Corporate Solutions Executive of the Year by The Commonwealth Bank.|$|R
40|$|Most {{approaches}} {{to the problem of}} source separation use the assumption of statistical independence. To capture statistical independence higher order statistics are required. In this chapter we will demonstrate how higher order criteria, such as maximum kurtosis, arise naturally from the property of non-stationarity. We will also show that source sepa-ration of non-stationary signals can be based entirely on second order statistics of the signals. Natural signals, be it images or time sequences, are for the most part non-stationary. For natural signals therefore we argue that non-stationarity is the fundamental property, from which speci c second or higher order separation criteria can be derived. We contrast the linear bases obtained using second order non-stationarity and ICA for the cases of natural images and <b>speech</b> <b>powers.</b> Based on these results we argue that <b>speech</b> <b>powers</b> can in fact be understood as a linear superposition of non-stationary spectro-temporal independen...|$|R
2500|$|Should he {{not have}} chosen texts so rich in {{meaning in the}} first place, but rather sounds? At least for the {{sections}} where only the <b>phonetic</b> properties of <b>speech</b> are dealt with.|$|R
