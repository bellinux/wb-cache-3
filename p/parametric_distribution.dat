296|570|Public
25|$|Different {{from the}} {{analysis}} on differentially expressed individual genes, another type of analysis focuses on differential expression or perturbation of pre-defined gene sets and is called gene set analysis. Gene set analysis demonstrated several major advantages over individual gene differential expression analysis. Gene sets are groups of genes that are functionally related according to current knowledge. Therefore, gene set analysis is considered a knowledge based analysis approach. Commonly used gene sets include those derived from KEGG pathways, Gene Ontology terms, gene groups that share some other functional annotations, such as common transcriptional regulators etc. Representative gene set analysis methods include GSEA, which estimates significance of gene sets based on permutation of sample labels, and GAGE, which tests the significance of gene sets based on permutation of gene labels or a <b>parametric</b> <b>distribution.</b>|$|E
5000|$|Hence, the {{marginal}} PDF of the spherical (<b>parametric)</b> <b>distribution</b> is ...|$|E
5000|$|Example. A {{different}} {{measure of}} calculating the marginal distribution {{function of a}} spherical (<b>parametric)</b> <b>distribution</b> is provided below, ...|$|E
5000|$|Consider {{a mixture}} of <b>parametric</b> <b>distributions</b> of the same class. Let ...|$|R
50|$|EMpht is a C {{script for}} fitting phase-type {{distributions}} to data or <b>parametric</b> <b>distributions</b> using an expectation-maximization algorithm.|$|R
40|$|A general {{insurance}} risk model consists of in initial reserve, the premiums collected, {{the return on}} investment of these premiums, the claims frequency and the claims sizes. Except for the initial reserve, these components are all stochastic. The assumption of the distributions of the claims sizes {{is an integral part}} of the model and can greatly in uence decisions on reinsurance agreements and ruin probabilities. An array of <b>parametric</b> <b>distributions</b> are available for use in describing the distribution of claims. The study is focussed on <b>parametric</b> <b>distributions</b> that have positive skewness and are de ned for positive real values. The main properties and parameterizations are studied for a number of distributions. Maximum likelihood estimation and method-of-moments estimation are considered as techniques for tting these distributions. Multivariate numerical maximum likelihood estimation algorithms are proposed together with discussions on the e ciency of each of the estimation algorithms based on simulation exercises. These discussions are accompanied with programs developed in SAS PROC IML that can be used to simulate from the various <b>parametric</b> <b>distributions</b> and to t these <b>parametric</b> <b>distributions</b> to observed data. The presence of heavy upper tails in the context of {{general insurance}} claims size distributions indicates that there exists a high risk of observing very large and even extreme claims. This needs to be allowed for in the modeling of claims. Methods used to describe tail weight together with techniques that can be used to detect the presence of heavy upper tails are studied. These methods are then applied to the <b>parametric</b> <b>distributions</b> to classify their tails' heaviness. The study is concluded with an application of the techniques developed to t the <b>parametric</b> <b>distributions</b> and to evaluate the tail heaviness of reallife claims data. The goodness-of- t of the various tted distributions are discussed. Based on the nal results further research topics are identi ed. Dissertation (MSc) [...] University of Pretoria, 2014. lk 2014 StatisticsMScUnrestricte...|$|R
5000|$|An {{alternative}} is to derive an analytical [...] function by assuming a particular <b>parametric</b> <b>distribution</b> for the underlying decision values. For example, a binormal precision-recall curve {{can be obtained by}} assuming decision values in both classes to follow a Gaussian distribution.|$|E
5000|$|This is {{the most}} general {{definition}} of VaR and the two identities are equivalent (indeed for any random variable its probability distribution is well defined). However this formula cannot be used directly for calculations unless we assume that [...] has some <b>parametric</b> <b>distribution.</b>|$|E
5000|$|This {{specification}} {{does not}} encompass all the existing errors-in-variables models. For example {{in some of}} them function [...] may be non-parametric or semi-parametric. Other approaches model the relationship between [...] and [...] as distributional instead of functional, that is they assume that [...] conditionally on [...] follows a certain (usually <b>parametric)</b> <b>distribution.</b>|$|E
50|$|EMP SP (Extended Mathematical Programming for Stochastic Programming) - {{a module}} of GAMS created to {{facilitate}} stochastic programming (includes keywords for <b>parametric</b> <b>distributions,</b> chance constraints and risk {{measures such as}} Value at risk and Expected shortfall).|$|R
40|$|This paper {{extends the}} results of Andrews (1984) which {{considers}} the problem of robust estimation of location in a model with stationary strong mixing Gaussian <b>parametric</b> <b>distributions.</b> Three neighbourhood systems are considered, each of which contains the Hellinger neighbourhoods used in Andrews (1984). Optimal robust estimators for this dependent random variable model {{are found to be}} bounded influence estimators with optimal psi functions which are very nearly of Huber shape. These estimators are quite robust against different "amounts" of dependence, and against lack of dependence. To generate the optimal estimators a minimax asymptotic risk criterion is used, where minimaxing is done over neighbourhoods of the <b>parametric</b> Gaussian <b>distributions.</b> The neighbourhood systems include distributions of strong mixing processes. They allow for deviations from stationarity and from the Gaussian structure of dependence. In addition, deviations from the normal univariate <b>parametric</b> <b>distributions</b> are allowed within the neighbourhoods defined by (i) epsilon_{n}-contamination, (ii) variational metric distance, and (iii) Kolmogorov metric distance. ...|$|R
40|$|The non <b>parametric</b> <b>distributions</b> of {{survival}} nd, more and more, applications in varied areas like: theory of reliability and survival analysis, queues, maintenance, stock management, {{theory of the}} economy, [...] . A systematic and update classi cation of these distributions was given by E. Faguioli and F. Pellery (1993) ...|$|R
50|$|There {{are several}} other {{parametric}} survival functions that {{may provide a}} better fit to a particular data set, including normal, lognormal, log-logistic, and gamma. The choice of <b>parametric</b> <b>distribution</b> for a particular application can be made using graphical methods or using formal tests of fit.These distributions and tests are described in textbooks on survival analysis. Lawless has extensive coverage of parametric models.|$|E
50|$|The Generalized Additive Model for Location, Scale and Shape (GAMLSS) {{is about}} {{statistical}} modelling and learning. GAMLSS {{is a modern}} distribution based approach to (semiparametric) regression analysis. A <b>parametric</b> <b>distribution</b> is assumed for the response (target) variable but the parameters of this distribution can vary according to explanatory variables using linear, nonlinear or smooth functions. In data science language GAMLSS is about supervised machine learning.|$|E
50|$|A guiding {{principle}} of GAMLSS {{is how to}} learn from data generated in many fields. In particular, the GAMLSS statistical framework enables flexible regression and smoothing models to be fitted to the data. The GAMLSS model assumes that the distribution of response variable has any <b>parametric</b> <b>distribution</b> which might be heavy or light-tailed, and positively or negatively skewed. In addition, all {{the parameters of the}} distribution (e.g., mean), scale (e.g., variance) and shape (skewness and kurtosis) can be modelled as linear, nonlinear or smooth functions of explanatory variables.|$|E
40|$|Abstract—The use of phase-type {{distributions}} {{has been}} sug-gested {{as a way}} to extend the representational power of the con-tinuous time Bayesian network framework beyond exponentially-distributed state transitions. However, much of the discussion has focused on approximating a distribution that is learned from available data. This method is inadequate for applications where there is not sufficient data to represent a distribution. In this paper, we suggest a method for learning phase-type <b>distributions</b> from known <b>parametric</b> <b>distributions.</b> We find that by minimizing a modified KL-divergence value, we are able to obtain good phase-type approximations for a variety of <b>parametric</b> <b>distributions.</b> In addition, we investigate the effects of using varying numbers of phases. Finally, we propose and evaluate an extension that uses informed starting locations for the optimization process rather than random initialization. I...|$|R
40|$|Abstract—The use of phase-type {{distributions}} is {{an established}} method for extending the representational power of continuous time Bayesian networks beyond exponentially-distributed state transitions. In this paper, we propose {{a method for}} learning phase-type <b>distributions</b> from known <b>parametric</b> <b>distributions.</b> We find that by using particle swarm optimization to minimize a modified KL-divergence value, {{we are able to}} efficiently obtain good phase-type approximations for a variety of <b>parametric</b> <b>distributions.</b> Our experiments show that particle swarm opti-mization outperforms genetic algorithms and hill climbing with simulated annealing. In addition, we investigate the trade-off between accuracy and complexity with respect to the number of phases in the phase-type distribution. Finally, we propose and evaluate an extension that uses informed starting locations during optimization, which we found to improve convergence rates when compared to random initialization. I...|$|R
30|$|In current research, many <b>parametric</b> <b>{{distribution}}s</b> {{have been}} used to model wind speeds, such as Rayleigh, χ^ 2 and ECDF distributions. In this paper Weibull distribution is supposed to estimate wind speed series parameters. Weibull distribution is perfect to model curve shape of wind speed series and its accuracy has been verified [22, 23, 24].|$|R
50|$|Different {{from the}} {{analysis}} on differentially expressed individual genes, another type of analysis focuses on differential expression or perturbation of pre-defined gene sets and is called gene set analysis. Gene set analysis demonstrated several major advantages over individual gene differential expression analysis. Gene sets are groups of genes that are functionally related according to current knowledge. Therefore, gene set analysis is considered a knowledge based analysis approach. Commonly used gene sets include those derived from KEGG pathways, Gene Ontology terms, gene groups that share some other functional annotations, such as common transcriptional regulators etc. Representative gene set analysis methods include GSEA, which estimates significance of gene sets based on permutation of sample labels, and GAGE, which tests the significance of gene sets based on permutation of gene labels or a <b>parametric</b> <b>distribution.</b>|$|E
50|$|The term proof {{descended from}} its Latin roots (provable, probable, probare L.) meaning to test. Hence, proof {{is a form}} of {{inference}} by means of a statistical test. Statistical tests are formulated on models that generate probability distributions. Examples of probability distributions might include the binary, normal, or poisson distribution that give exact descriptions of variables that behave according to natural laws of random chance. When a statistical test is applied to samples of a population, the test determines if the sample statistics are significantly different from the assumed null-model. True values of a population, which are unknowable in practice, are called parameters of the population. Researchers sample from populations, which provide estimates of the parameters, to calculate the mean or standard deviation. If the entire population is sampled, then the sample statistic mean and distribution will converge with the <b>parametric</b> <b>distribution.</b>|$|E
5000|$|That is, {{we assume}} that the data belongs to [...] {{distinct}} clusters with means [...] and that [...] is the (unknown) prior probability of a data point belonging to the th cluster. We assume that we have no initial information distinguishing the clusters, which is captured by the symmetric prior [...] Here [...] denotes the Dirichlet distribution and [...] denotes a vector of length [...] where each element is 1. We further assign independent and identical prior distributions [...] to each of the cluster means, where [...] may be any <b>parametric</b> <b>distribution</b> with parameters denoted as [...] The hyper-parameters [...] and [...] are taken to be known fixed constants, chosen to reflect our prior beliefs about the system. To understand the connection to Dirichlet process priors we rewrite this model in an equivalent but more suggestive form: ...|$|E
40|$|Stein {{operators}} are (differential/difference) operators which arise within the so-called Stein’s method for stochastic approximation. We propose a new mechanism for constructing such operators for arbitrary (continuous or discrete) <b>parametric</b> <b>distributions</b> with continuous {{dependence on the}} parameter. We provide explicit general expressions for location, scale and skewness families. We also provide a general expression for discrete distributions. We use properties of our operators to provide upper and lower variance bounds (only lower bounds in the discrete case) on functionals h(X) of random variables X following <b>parametric</b> <b>distributions.</b> These bounds are {{expressed in terms of}} the first two moments of the derivatives (or differences) of h. We provide general variance bounds for location, scale and skewness families and apply our bounds to specific examples (namely the Gaussian, exponential, gamma and Poisson distributions). The results obtained via our techniques are systematically competitive with, and sometimes improve on, the best bounds available in the literature. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|We {{propose a}} simple method that, given a symbol distribution, yields {{upper and lower}} bounds on the average code length of a D-ary optimal code over that distribution. Thanks to its simplicity, the method permits {{deriving}} analytical bounds for families of <b>parametric</b> <b>distributions.</b> We demonstrate this by obtaining new bounds, much better than the existing ones, for Zipf and exponential distributions when D > 2...|$|R
40|$|Bitcoin, {{the first}} {{electronic}} payment system, {{is becoming a}} popular currency. We provide a statistical analysis of the log-returns of the exchange rate of Bitcoin versus the United States Dollar. Fifteen {{of the most popular}} <b>parametric</b> <b>distributions</b> in finance are fitted to the log-returns. The generalized hyperbolic distribution is shown to give the best fit. Predictions are given for future values of the exchange rate...|$|R
50|$|Utilization {{distributions}} {{are constructed}} from data providing {{the location of}} an individual or several individuals in space {{at different points in}} time by associating a local distribution function with each point and then summing and normalizing these local distribution functions to obtain a distribution function that pertains to the data as a whole. If the local distribution function is a <b>parametric</b> <b>distribution,</b> such as a symmetric bivariate normal distribution then the method is referred to as a kernel method, but more correctly should be designated as a parametric kernel method. On the other hand, if the local kernel element associated with each point is a local convex polygon constructed from the point and its k-1 nearest neighbors, then the method is nonparametric and referred to as a k-LoCoH or fixed point LoCoH method. This is in contrast to r-LoCoH (fixed radius) and a-LoCoH (adaptive radius) methods.|$|E
50|$|Irvine 1947 {{recorded}} a possible humpback whale at Prampram in September 1938. Van Waerebeek and Ofori-Danson (1999) first confirmed the species from Ghana {{based on a}} fresh neonate stranded at Ada in September 1997. An adult-sized humpback whale stranded at Ada Foah in October 2006. Humpback whales are regularly sighted inshore from the Dixcove Castle, from September till December. A neonate stranded in Lomé, Togo, in August 2005. Rasmussen et al. (2007) encountered several pods including a mother and possible newborn calf in Ghanaian waters in October 2006. From the presence of humpback whales exclusively from early August till late November, and the frequent observations of neonates and ‘competitive groups’, {{it is evident that}} the continental shelf of Benin, Togo, Ghana, and western Nigeria hosts a breeding/calving population with a Southern Hemisphere seasonality, referred to as the ‘Gulf of Guinea stock’. Its <b>parametric</b> <b>distribution</b> suggests it {{may be related to the}} IWC-defined breeding stock ‘B’ from central-west Africa. Mother and calf pairs have been sighted exclusively near shore in Benin, sometimes just beyond the surf-zone. The westernmost authenticated record is a stranding at Assini Mafia, eastern Côte d’Ivoire in August 2007. For many years, small-scale, seasonal humpback whale-watching sorties have been conducted from the ports of Sekondi-Takoradi, Lomé and Cotonou. The breeding stock off Gabon and Angola is the subject of longterm dedicated studies. Possibly up to 10% of world population of humpbacks migrate into Gulf of Guinea.|$|E
40|$|The cross-entropy, {{which is}} {{proportional}} to the Kullback-Leibler divergence, is widely used to gauge the deviation of a distribution of interest from a reference distribution in statistical inference. For example, the Akaike information criterion (AIC) is an asymptotically unbiased estimator of the cross-entropy from a <b>parametric</b> <b>distribution</b> to the true distribution of data. Minimizing the AIC allows us to find a <b>parametric</b> <b>distribution</b> close to the true distribution. In this paper, we generalize the AIC by letting the reference distribution be a target distribution to approximate when its density can be evaluated up to a multiplicative constant only at observed data points. We prove, under some conditions, that the generalized criterion, which we call the cross-entropy information criterion (CIC), is an asymptotically unbiased estimator of the cross-entropy (up to a multiplicative constant) from a <b>parametric</b> <b>distribution</b> to the target distribution. We demonstrate the usefulness of CIC for approximating the optimal importance sampling distribution by a mixture of parametric distributions. Comment: 32 pages, 4 figure...|$|E
40|$|International audienceWe {{consider}} a Wright-Fisher diffusion (x(t)) whose current state cannot be observed directly. Instead, at times t 1 1) is a computable filter {{in the sense}} that all distributions involved in filtering, prediction and smoothing are exactly computable. These distributions are expressed as finite mixtures of <b>parametric</b> <b>distributions.</b> Thus, the number of statistics to compute at each iteration is finite, but this number may vary along iterations...|$|R
30|$|A {{method is}} clearly needed to {{efficiently}} divide the sampling area into clusters, {{in order to}} run a parallel AS algorithm with multiple robots. Here, we propose an approach to efficiently divide the sampling area for <b>parametric</b> <b>distributions</b> using Fuzzy c-means clustering (FCM) and Centroidal Voronoi Tessellation (CVT) diagrams.FCM has frequently {{been used in the}} past for the classification of numerical data. CVT diagrams[24] have also been used for forming non-uniform size grids to better explore high-variance areas for non-parametric distributions[7]. Here, we employ a scheme to efficiently divide the sampling areas for <b>parametric</b> <b>distributions</b> using both FCM and CVT. In this approach, FCM clusters samples based on the estimated centers of the approximating Gaussians used to map the field. Note here that we have assumed that the partitioning is performed once only {{at the beginning of the}} Fusion filter. For a time-varying field, further accuracy can be obtained by re-partitioning the field (and hence repositioning the Gaussians) after some samples to account for the field evolution in time.|$|R
40|$|We suggest several goodness-of-fit (GOF) methods {{which are}} {{appropriate}} with Type-II right censored data. Our {{strategy is to}} transform the original observations from a censored sample into an approximately i. i. d. sample of normal variates and then perform a standard GOF test for normality on the transformed observations. A simulation study with several well known <b>parametric</b> <b>distributions</b> under testing reveals the sampling properties of the methods. We also provide theoretical analysis of the proposed method...|$|R
40|$|This paper {{emphasizes}} on analysing and predicting {{the reliability of}} an automobile crankshaft by analysing the time to failure (TTF) through the <b>parametric</b> <b>distribution</b> function. In this paper, the TTF was modelled to predict the likelihood of failure for crankshaft during its operational condition over a given time interval {{through the development of}} the stochastic algorithm. The developed stochastic algorithm has the capability to measure the <b>parametric</b> <b>distribution</b> function and validate the predict the reliability rate, mean time to failure and hazard rate. T, the algorithm has the capability to statistically validate the algorithm to obtain the optimal parametric model to represent the failure of the component against the actual time to failure data from the local automobile industry. Hence, the validated results showed that the three parameter Weibull distribution provided an accurate and efficient foundation in modelling the reliability rate when compared with the actual sampling data. The suggested <b>parametric</b> <b>distribution</b> function can be used to improve the design and the life cycle due to its capability in accelerating and decelerating the mechanism of failure based on time without adjusting the level of stress. Therefore, an understanding of the <b>parametric</b> <b>distribution</b> posed by the reliability and hazard rate onto the component can be used to improve the design and increase the life cycle based on the dependability of the component over a given period of time. The proposed reliability assessment through the developed stochastic algorithm provides an accurate, efficient, fast and cost effective reliability analysis in contrast to costly and lengthy experimental techniques...|$|E
40|$|In {{survival}} analysis, time-dependent covariates {{are usually}} available as longitudinal data collected periodically and with error. The longitudinal {{data can be}} assumed to follow a linear mixed effect model and Cox regression models are usually used for survival events. The hazard rate of survival times depends on the underlying time-dependent covariate which is described by random effects. Most methods proposed for such models assume a <b>parametric</b> <b>distribution</b> assumption on the random effects and specify a normally distributed error term for the linear mixed effect model. These assumptions may not be valid in practice. This paper proposes a new likelihood method for Cox regression models with error-contaminated time-dependent covariates. The new method does not require any <b>parametric</b> <b>distribution</b> assumption on random effects and rando...|$|E
40|$|Texas dryland {{upland cotton}} yields have {{historically}} exhibited greater variation and more distributional irregularities than the yields of other crops, raising concerns that conventional <b>parametric</b> <b>distribution</b> models may generate biased or otherwise inaccurate crop insurance premium rate estimates. Here, we formulate and estimate regime-switching models for Texas dryland cotton yields {{in which the}} distribution of yield is conditioned on local drought conditions. Our results indicate that drought-conditioned regime-switching models provide a better fit to Texas county-level dryland cotton yields than conventional <b>parametric</b> <b>distribution</b> models. They do not, however, generate significantly different Group Risk Plan crop insurance premium rate estimates. actuarial rating, adverse selection, cotton, crop insurance, group risk plan, regime-switching, yield distribution, Agribusiness, Crop Production/Industries, Farm Management, Q 10, Q 14, Q 18,...|$|E
40|$|This paper {{considers}} dynamic discrete choice {{models with}} conditionally independent and additively separable unobserved state variables as in Rust (1987) and Hotz and Miller (1993). Previous literature commonly assumed specific <b>parametric</b> <b>distributions</b> for the unobserved states, for example, extreme value distribution. Norets and Tang (2010) proposed {{an approach to}} identification and inference in dynamic binary choice models that does not impose distributional assumptions on the unobserved state variables. This paper generalizes this approach to dynamic multinomial choice models...|$|R
50|$|In 1965, Kulldorff was {{appointed}} {{as the first}} professor of statistics at the newly established Umeå University, where he also became the first dean of the Faculty of Arts and Sciences. In 1966, he {{was appointed}} professor of mathematical statistics, a position he held until retirement. During this time, he wrote papers on survey sampling and the optimum combinations of selected order statistics from various well known <b>parametric</b> <b>distributions,</b> in order to draw inference on population parameters.|$|R
40|$|A {{method is}} {{introduced}} {{to estimate the}} number of significant coefficients in non ordered model selection problems. The method {{is based on a}} convenient random centering of the partial sums of the ordered observations. Based on $L-$statistics methods we show consistency of the proposed estimator. An extension to unknown <b>parametric</b> <b>distributions</b> is considered. The method is then applied to a regression model and interpreted as a random threshold procedure. Simulated examples are included to show the accuracy of the estimator...|$|R
