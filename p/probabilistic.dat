10000|0|Public
5|$|Einstein {{was troubled}} {{by the fact that}} his theory seemed incomplete, since it did not {{determine}} the direction of a spontaneously emitted photon. A <b>probabilistic</b> nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which of the two paths a single photon would take. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's <b>probabilistic</b> interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.|$|E
25|$|A <b>probabilistic</b> {{proof is}} one in which an example is shown to exist, with certainty, by using methods of {{probability}} theory. <b>Probabilistic</b> proof, like proof by construction, is one of many ways to show existence theorems.|$|E
25|$|In {{the late}} 1980s Judea Pearl's text <b>Probabilistic</b> Reasoning in Intelligent Systems and Richard E. Neapolitan's text <b>Probabilistic</b> Reasoning in Expert Systems {{summarized}} {{the properties of}} Bayesian networks and established Bayesian networks as a field of study.|$|E
25|$|A Bayesian network, Bayes network, belief network, Bayes(ian) {{model or}} <b>probabilistic</b> {{directed}} acyclic graphical {{model is a}} <b>probabilistic</b> graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the <b>probabilistic</b> relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases.|$|E
25|$|In 1990 {{while working}} at Stanford University on large bioinformatic applications, Greg Cooper proved that exact {{inference}} in Bayesian networks is NP-hard. This result prompted a surge in research on approximation algorithms {{with the aim of}} developing a tractable approximation to <b>probabilistic</b> inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of <b>probabilistic</b> inference in Bayesian networks. First, they proved that there is no tractable deterministic algorithm that can approximate <b>probabilistic</b> inference to within an absolute error ɛ< 1/2. Second, they proved that there is no tractable randomized algorithm that can approximate <b>probabilistic</b> inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.|$|E
25|$|Vol. 1: Probability and <b>Probabilistic</b> Causality.|$|E
25|$|Grammar {{theory to}} model symbol strings {{originated}} from work {{in computational linguistics}} aiming to understand the structure of natural languages. <b>Probabilistic</b> context free grammars (PCFGs) have been applied in <b>probabilistic</b> modeling of RNA structures almost 40 years after they were introduced in computational linguistics.|$|E
25|$|Mecca Chiesa {{notes that}} the <b>probabilistic</b> or selectionistic {{determinism}} of B.F. Skinner comprised a wholly separate conception of determinism that was not mechanistic at all. Mechanistic determinism assumes that every event has an unbroken chain of prior occurrences, but a selectionistic or <b>probabilistic</b> model does not.|$|E
25|$|In <b>probabilistic</b> combinatorics, the {{questions}} are of the following type: what is {{the probability of a}} certain property for a random discrete object, such as a random graph? For instance, what is the average number of triangles in a random graph? <b>Probabilistic</b> methods are also used to determine the existence of combinatorial objects with certain prescribed properties (for which explicit examples might be difficult to find), simply by observing that the probability of randomly selecting an object with those properties is greater than 0. This approach (often referred to as the <b>probabilistic</b> method) proved highly effective in applications to extremal combinatorics and graph theory. A closely related area is the study of finite Markov chains, especially on combinatorial objects. Here again <b>probabilistic</b> tools are used to estimate the mixing time.|$|E
25|$|In <b>probabilistic</b> language, f is a {{probability}} density function.|$|E
25|$|As an {{alternative}} to <b>probabilistic</b> methods, heuristic methods exist for performing variant calling on NGS data. Instead of modelling {{the distribution of the}} observed data and using Bayesian statistics to calculate genotype probabilities, variant calls are made based on a variety of heuristic factors, such as minimum allele counts, read quality cut-offs, bounds on read depth, etc. Although they have been relatively unpopular in practice in comparison to <b>probabilistic</b> methods, in practice due to their use of bounds and cut-offs they can be robust to outlying data that violate the assumptions of <b>probabilistic</b> models.|$|E
25|$|See {{interacting}} particle {{system and}} stochastic cellular automata (<b>probabilistic</b> cellular automata).|$|E
25|$|Kallenberg, O. (2005) <b>Probabilistic</b> Symmetries and Invariance Principles. Springer -Verlag, New York. 510 pp.|$|E
25|$|Often {{associated}} with Paul Erdős, {{who did the}} pioneering work on the subject, <b>probabilistic</b> combinatorics was traditionally viewed {{as a set of}} tools to study problems in other parts of combinatorics. However, with the growth of applications to analysis of algorithms in computer science, as well as classical probability, additive and <b>probabilistic</b> number theory, the area recently grew to become an independent field of combinatorics.|$|E
25|$|Mosegaard, K., & Tarantola, A. (2002). 16 <b>Probabilistic</b> {{approach}} to inverse problems. International Geophysics, 81, 237-265.|$|E
25|$|The {{description}} {{given by}} the wave function is <b>probabilistic.</b> This principle is called the Born rule, after Max Born.|$|E
25|$|Constraint-based {{theories}} of language comprehension emphasize how people {{make use of}} the vast amount of <b>probabilistic</b> information available in the linguistic signal. Through statistical learning, the frequencies and distribution of events in linguistic environments can be picked upon, which inform language comprehension. As such, language users are said to arrive at a particular interpretation over another during the comprehension of an ambiguous sentence by rapidly integrating these <b>probabilistic</b> constraints.|$|E
25|$|Discrete choice models {{specify the}} {{probability}} that an individual chooses an option among a set of alternatives. The <b>probabilistic</b> description of discrete choice behavior is used not to reflect individual behavior that is viewed as intrinsically <b>probabilistic.</b> Rather, it {{is the lack of}} information that leads us to describe choice in a <b>probabilistic</b> fashion. In practice, we cannot know all factors affecting individual choice decisions as their determinants are partially observed or imperfectly measured. Therefore, discrete choice models rely on stochastic assumptions and specifications to account for unobserved factors related to a) choice alternatives, b) taste variation over people (interpersonal heterogeneity) and over time (intra-individual choice dynamics), and c) heterogeneous choice sets. The different formulations have been summarized and classified into groups of models.|$|E
25|$|In mathematics, a t-norm (also T-norm or, unabbreviated, {{triangular}} norm) {{is a kind}} of {{binary operation}} used in the framework of <b>probabilistic</b> metric spaces and in multi-valued logic, specifically in fuzzy logic. A t-norm generalizes intersection in a lattice and conjunction in logic. The name triangular norm refers to {{the fact that in the}} framework of <b>probabilistic</b> metric spaces t-norms are used to generalize triangle inequality of ordinary metric spaces.|$|E
25|$|There {{are various}} simple {{exercises}} that demonstrate <b>probabilistic</b> decay, for example involving flipping coins or running a statistical computer program.|$|E
25|$|At times, a non-rigorous, <b>probabilistic</b> {{approach}} {{leads to}} a number of heuristic algorithms and open problems, notably Cramér's conjecture.|$|E
25|$|Fulmer devised {{techniques}} for <b>probabilistic</b> mathematical modelling and in 1986 hosted {{the first international}} conference on Modelling under Uncertainty.|$|E
25|$|Information entropy {{is defined}} as the average amount of {{information}} produced by a <b>probabilistic</b> stochastic source of data.|$|E
25|$|S. Srivastava, and S. Bhanja, “Hierarchical <b>Probabilistic</b> Macromodeling for QCA Circuits”, IEEE Transactions on Computers,Vol. 56(2), p.174-190, Feb. 2007.|$|E
25|$|ZPP: The {{complexity}} {{class of}} decision {{problems that can}} be solved with zero error on a <b>probabilistic</b> Turing machine in polynomial time.|$|E
25|$|NFAs {{have been}} {{generalized}} in multiple ways, e.g., nondeterministic finite automaton with ε-moves, finite state transducers, pushdown automata, alternating automata, ω-automata, and <b>probabilistic</b> automata.|$|E
25|$|Mark Colyvan, Jay Garfield and Graham Priest (2005) {{have argued}} that a theistic {{explanation}} for fine tuning is faulted due to fallacious <b>probabilistic</b> reasoning.|$|E
25|$|A {{computationally}} hard problem, {{which is}} key for some relevant machine learning tasks, is {{the estimation of}} averages over <b>probabilistic</b> models {{defined in terms of}} a Boltzmann distribution. Sampling from generic <b>probabilistic</b> models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.|$|E
25|$|In {{computer}} science and operations research, the ant colony optimization algorithm (ACO) is a <b>probabilistic</b> technique for solving computational problems {{which can be}} reduced to finding good paths through graphs.|$|E
25|$|In machine learning, tree decompositions {{are also}} called {{junction}} trees, clique trees, or join trees; they {{play an important}} role in problems like <b>probabilistic</b> inference, constraint satisfaction, query optimization, and matrix decomposition.|$|E
25|$|This logic has applications, {{for example}} a {{cryptographic}} attack called the birthday attack, which uses this <b>probabilistic</b> model {{to reduce the}} complexity of finding a collision for a hash function.|$|E
25|$|Tu, K. and Honavar, V. (2012). Unambiguity Regularization for Unsupervised Learning of <b>Probabilistic</b> Grammars. In: Proceedings of EMNLP-CoNLL 2012 : Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp.1324–1334.|$|E
25|$|A deep belief network (DBN) is a <b>probabilistic</b> and {{generative}} model made up {{of multiple}} layers of hidden units. It {{can be considered a}} composition of simple learning modules that make up each layer.|$|E
25|$|The <b>probabilistic</b> {{version of}} the Schuette–Nesbitt formula has {{practical}} applications in actuarial science, where {{it is used to}} calculate the net single premium for life annuities and life insurances based on the general symmetric status.|$|E
25|$|And {{even if the}} <b>probabilistic</b> {{reasoning}} were rigorous, {{this would}} still imply only that the conjecture is almost surely true for any given integer, which does not necessarily imply that it is true for all integers.|$|E
25|$|<b>Probabilistic</b> {{formulation}} of inverse problems {{leads to the}} definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data).|$|E
