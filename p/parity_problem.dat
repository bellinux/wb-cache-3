124|106|Public
25|$|In number theory, the <b>parity</b> <b>problem</b> {{refers to}} a {{limitation}} in sieve theory that prevents sieves from giving good estimates in many kinds of prime-counting problems. The problem was identified and named by Atle Selberg in 1949. Beginning around 1996, John Friedlander and Henryk Iwaniec developed some parity-sensitive sieves that make the <b>parity</b> <b>problem</b> less of an obstacle.|$|E
25|$|Glyn Harman {{relates the}} <b>parity</b> <b>problem</b> to the {{distinction}} between Type I and Type II information in a sieve.|$|E
2500|$|Beginning around 1996 John Friedlander and Henryk Iwaniec {{developed}} some new sieve techniques to [...] "break" [...] the <b>parity</b> <b>problem.</b>|$|E
40|$|We {{describe}} a recipe to solve very large <b>parity</b> <b>problems</b> using GP. The recipe includes: smooth uniform crossover (a crossover operator inspired by our theoretical research), sub-machine-code GP (a technique {{to speed up}} fitness evaluation in Boolean classification problems), and interacting demes (sub-populations) running on separate workstations. We tested this recipe on <b>parity</b> <b>problems</b> with up to 22 input variables, solving them with a very high success probability. ...|$|R
40|$|We propose {{and study}} new search {{operators}} and a novel node representation {{that can make}} GP fitness landscapes smoother. Together with a tree evaluation method known as sub-machine code GP {{and the use of}} demes, these make up a recipe for solving very large <b>parity</b> <b>problems</b> using GP. We tested this recipe on <b>parity</b> <b>problems</b> with up to 22 input variables, solving them with a very high success probability...|$|R
40|$|Abstract—Self Modifying CGP (SMCGP) is a {{developmental}} form of Cartesian Genetic Programming(CGP). It differs from CGP by including primitive functions which modify the pro-gram. Beginning with the evolved genotype the self-modifying functions {{produce a new}} program (phenotype) at each iteration. In this paper we have applied it to a well known digital circuit building problem: even-parity. We show that {{it is easier to}} solve difficult <b>parity</b> <b>problems</b> with SMCGP than either with CGP or Modular CGP, and that the increase in efficiency grows with problem size. More importantly, we prove that SMCGP can evolve general solutions to arbitrary-sized even <b>parity</b> <b>problems.</b> I...|$|R
2500|$|This {{problem is}} {{significant}} because it may explain why {{it is difficult for}} sieves to [...] "detect primes," [...] in other words to give a non-trivial lower bound for the number of primes with some property. For example, in a sense Chen's theorem is very close to a solution of the twin prime conjecture, since it says that there are infinitely many primes p such that prime p + 2 is either prime or the product of two primes. The <b>parity</b> <b>problem</b> suggests that, because the case of interest has an odd number of prime factors (namely 1), it won't be possible to separate out the two cases using sieves.|$|E
50|$|In number theory, the <b>parity</b> <b>problem</b> {{refers to}} a {{limitation}} in sieve theory that prevents sieves from giving good estimates in many kinds of prime-counting problems. The problem was identified and named by Atle Selberg in 1949. Beginning around 1996, John Friedlander and Henryk Iwaniec developed some parity-sensitive sieves that make the <b>parity</b> <b>problem</b> less of an obstacle.|$|E
50|$|The {{techniques}} of sieve theory {{can be quite}} powerful, {{but they seem to}} be limited by an obstacle known as the <b>parity</b> <b>problem,</b> which roughly speaking asserts that sieve theory methods have extreme difficulty distinguishing between numbers with an odd number of prime factors and numbers with an even number of prime factors. This <b>parity</b> <b>problem</b> is still not very well understood.|$|E
5000|$|<b>Parity</b> <b>problems</b> {{are widely}} used as {{benchmark}} problems in genetic programming but inherited from the artificial neural network community. Parity is calculated by summing all the binary inputs and reporting if the sum is odd or even. This is considered difficult because: ...|$|R
40|$|In {{this paper}} we propose a {{cyclical}} coordinate descent (CCD) algorithm for solving high dimensional risk <b>parity</b> <b>problems.</b> We show that this algorithm converges and is very fast even with large covariance matrices (n > 500). Comparison with existing algorithms also shows that {{it is one of the}} most efficient algorithms. ...|$|R
40|$|In {{this paper}} we {{describe}} a recipe to solve very large <b>parity</b> <b>problems,</b> using GP without automatically defined functions. The recipe includes three main ingredients: smooth uniform crossover (a crossover operator inspired by theoretical research), sub-machine-code GP (a technique which allows speeding up fitness evaluation in Boolean classification problems by nearly 2 orders of magnitude), and distributed demes (weakly interacting sub-populations running on separate workstations). We tested this recipe on <b>parity</b> <b>problems</b> {{with up to}} 22 input variables (i. e. where the fitness function includes 2 22 = 4; 194; 304 fitness cases), solving them with a very high success probability. 1 Introduction The even-n-parity functions have long been recognised as difficult problems for induction algorithms such as genetic programming (GP) to solve, {{and for this reason}} have been widely used as benchmark tests ([6], [7], [4], [21], [5], [1], [18], [19]). For an even-parity function of order n (tha [...] ...|$|R
50|$|Glyn Harman {{relates the}} <b>parity</b> <b>problem</b> to the {{distinction}} between Type I and Type II information in a sieve.|$|E
50|$|Helfgott became active {{early on}} in the {{mathematical}} community in Lima, his home city. He graduated from Brandeis University in 1998 (BA, summa cum laude). He received his Ph.D. from Princeton University in 2003 under the direction of Henryk Iwaniec (and Peter Sarnak), with the thesis Root numbers and the <b>parity</b> <b>problem.</b>|$|E
5000|$|Razborov and Rudich give {{a number}} of {{examples}} of lower-bound proofs against classes C smaller than P/poly that can be [...] "naturalized", i.e. converted into natural proofs. An important example treats proofs that the <b>parity</b> <b>problem</b> {{is not in the}} class AC0. They give strong evidence that the techniques used in these proofs cannot be extended to show stronger lower bounds. In particular, AC0-natural proofs cannot be useful against AC0.|$|E
40|$|Perceptively written text {{examines}} optimization {{problems that}} can be formulated in terms of networks and algebraic structures called matroids. Chapters cover shortest paths, network flows, bipartite matching, nonbipartite matching, matroids and the greedy algorithm, matroid intersections, and the matroid <b>parity</b> <b>problems.</b> A suitable text or reference for courses in combinatorial computing and concrete computational complexity in departments of computer science and mathematics...|$|R
40|$|We {{describe}} a recipe to solve very large <b>parity</b> <b>problems</b> using GP. The recipe includes: smooth uniform crossover (a crossover operator inspired by our theoretical research), sub-machine-code GP (a technique {{to speed up}} fitness evaluation in Boolean classification problems), and interacting demes (sub-populations) running on separate workstations. We tested this recipe on <b>parity</b> <b>problems</b> with up to 22 input variables, solving them with a very high success probability. 1 INTRODUCTION The even-n-parity functions have long been recognised as difficult for Genetic Programming (GP) to induce if no bias favourable to their induction is introduced in the function set, the input representation, {{or in any other}} part of the algorithm. For this reason they have been widely used as benchmark tests [1, 3, 4, 5, 6, 16, 17, 19]. For an even-parity function of n Boolean inputs, the task is to evolve a function that returns 1 if an even number of the inputs evaluate to 1, 0 otherwise. The tas [...] ...|$|R
40|$|AbstractThe <b>parity</b> path <b>problem</b> is {{the problem}} of finding two paths of {{different}} parity between two given vertices. This problem is known to be NP-complete on general graphs. Polynomial algorithms were known for the <b>parity</b> path <b>problem</b> on chordal, planar perfect, interval and circular-arc graphs. In this paper, we give polynomial algorithms for this problem on comparability, cocomparability graphs, and linear algorithms on permutation graphs...|$|R
5000|$|Terence Tao {{gave this}} [...] "rough" [...] {{statement}} of the problem: <b>Parity</b> <b>problem.</b> If A is a set whose elements are all products of an odd number of primes (or are all products of an even number of primes), then (without injecting additional ingredients), sieve theory is unable to provide non-trivial lower bounds {{on the size of}} A. Also, any upper bounds must be off from the truth by a factor of 2 or more.|$|E
5000|$|In {{computational}} complexity theory, {{the complexity}} class of all regular languages is {{sometimes referred to}} as REGULAR or REG and equals DSPACE(O(1)), the decision problems that can be solved in constant space (the space used is independent of the input size). REGULAR ≠ AC0, since it (trivially) contains the <b>parity</b> <b>problem</b> of determining whether the number of 1 bits in the input is even or odd and this problem is not in AC0. On the other hand, REGULAR does not contain AC0, because the nonregular language of palindromes, or the nonregular language [...] can both be recognized in AC0.|$|E
5000|$|This {{problem is}} {{significant}} because it may explain why {{it is difficult for}} sieves to [...] "detect primes," [...] in other words to give a non-trivial lower bound for the number of primes with some property. For example, in a sense Chen's theorem is very close to a solution of the twin prime conjecture, since it says that there are infinitely many primes p such that prime p + 2 is either prime or the product of two primes. The <b>parity</b> <b>problem</b> suggests that, because the case of interest has an odd number of prime factors (namely 1), it won't be possible to separate out the two cases using sieves.|$|E
40|$|AbstractThe <b>parity</b> path <b>problem</b> is to test if a graph {{contains}} an even (odd) chordless path between two prescribed vertices. A graph is called perfectly orientable if there exists an orientation of its edges {{such that the}} resulting digraph has no obstructions: an obstruction is a chordless path (a,b,c,d) having →ab and ←cd. We present a polynomial algorithm for the <b>parity</b> path <b>problem</b> on perfectly orientable graphs...|$|R
40|$|We propose {{and study}} new search {{operators}} and a novel node representation {{that can make}} GP fitness landscapes smoother. Together with a tree evaluation method known as sub-machine code GP {{and the use of}} demes, these make up a recipe for solving very large <b>parity</b> <b>problems</b> using GP. We tested this recipe on <b>parity</b> <b>problems</b> with up to 22 input variables, solving them with a very high success probability. 1. INTRODUCTION The even-n-parity functions have long been recognised as difficult for Genetic Programming (GP) to induce if no bias favourable to their induction is introduced in the function set, the input representation, or in any other part of the algorithm. For this reason they are very interesting and have been widely used as benchmark tests [1, 4, 5, 6, 7, 23, 24, 26]. For an even-parity function of n Boolean inputs, the task is to evolve a function that returns 1 if an even number of the inputs evaluate to 1, 0 otherwise. The task seems to be difficult for at least two reasons [...] ...|$|R
40|$|Abstract- This {{research}} {{presents an}} evaluation of user de-fined domain specific functions of genetic programming using relational learning problems, generalisation for this class of learning problems and learning bias. After pro-viding a brief theoretical background, two sets of exper-iments are detailed: experiments and results concern-ing the Monk- 2 problem and experiments attempting to evolve generalising solutions to <b>parity</b> <b>problems</b> with in-complete data sets. The results suggest that using non-problem specific functions may result in greater generali-sation for relational problems. ...|$|R
40|$|In {{this work}} we shall {{investigate}} quantum query complexity of weak <b>PARITY</b> <b>problem.</b> Weak Parity {{is the following}} natural query problem: What is the minimum number of queries necessary to compute PARITY(x 1, x 2, x 3, [...] ., xn) on at least 1 2 + ɛ fraction of the inputs. For randomized classical or exact quantum machines this relaxed <b>PARITY</b> <b>problem</b> remains as hard as original <b>PARITY</b> <b>problem.</b> Although Bounded error quantum machines needed almost as many queries as classical machines for original <b>PARITY</b> <b>problem,</b> they outperform classical algorithms in this relaxed problem. n We will show upper O (...|$|E
40|$|In this paper, {{we present}} an O(r 4 n) {{algorithm}} for the linear matroid <b>parity</b> <b>problem.</b> Our solution technique is {{to introduce a}} modest generalization, the non-simple <b>parity</b> <b>problem,</b> and identify an important subclass of non-simple parity problems called 'easy ' parity problems which can be solved as matroid intersection problems. We then show how to solve any linear matroid <b>parity</b> <b>problem</b> parametrically as a sequence of 'easy ' parity problems. In contrast to other algorithmic work on this problem, we focus on general structural properties of dual solutions rather than on local primal structures. In a companion paper, we develop these ideas into a duality theory for the <b>parity</b> <b>problem...</b>|$|E
40|$|Abstract. The N-dimensional <b>parity</b> <b>problem</b> is {{frequently}} a difficult classification task for Neural Networks. We found an expression for the {{minimum number of}} errors nf as function of N for this problem, performed by a perceptron. We verified this quantity experimentally for N 1; [...] .; 15 using an optimal train perceptron. With a constructive approach we solved the full N-dimensional <b>parity</b> <b>problem</b> using a minimal feedforward neural network with a single hidden layer of h N units. Key words. classification tasks, minimerror, monoplan, <b>parity</b> <b>problem,</b> perceptrons, supervised learnin...|$|E
40|$|Scatterograms of {{the images}} of {{training}} set vectors in the hidden space help to evaluate the quality of neural network mappings and understand internal representations created by the hidden layers. Visualization of these representations leads to interesting conclusions about optimal architectures and training of such networks. Depending on network parameters only {{some parts of the}} unit hypercube [...] called here admissible spaces [...] may be reached. The usefulness of visualization techniques is illustrated on <b>parity</b> <b>problems</b> solved with RBF networks...|$|R
40|$|A Hierarchical Mixtures of Experts (HME) {{model has}} been applied to several classes of problems, and its {{usefulness}} has been shown. However, defining an adequate structure in advance is required and the resulting performance depends on the structure. To overcome this problem, a constructive learning algorithm for an HME is proposed; it includes an initialization method, a training method and an extension method. In our experiments, which used <b>parity</b> <b>problems</b> and a function approximation problem, the proposed algorithm worked much better than the conventional method. 1...|$|R
40|$|Abstract – In {{this paper}} we {{experiment}} TAG 3 P on the even <b>parity</b> <b>problems</b> {{in order to}} investigate the robustness of tree-adjunct grammar guided genetic programming [3] (TAG 3 P) on the problems classified as “finding {{a needle in a}} haystack ” [9]. We compare the result with grammar guided genetic programming [15] (GGGP) and genetic programming [7] (GP). The results show that TAG 3 P does not work well on the problems {{due to the nature of}} the search space and the structure of the solution...|$|R
40|$|In this letter, a {{constructive}} {{solution to the}} N-bit <b>parity</b> <b>problem</b> is provided with a neural network that allows direct connections between the input layer and the output layer. The present approach requires no training and adaptation, and thus it warrants {{the use of the}} simple threshold activation function for the output and hidden layer neurons. It is previously shown that this choice of activation function and network structure leads to several solutions for the 3 -bit <b>parity</b> <b>problem</b> obtained using linear programming. One of the solutions for the 3 -bit <b>parity</b> <b>problem</b> is then generalized to obtain a solution for the N-bit <b>parity</b> <b>problem</b> using b N 2 c hidden layer neurons. It is shown that through the choice of a " type activation function, the b N 2 c hidden layer neurons can be further combined into a single hidden layer neuron. Keywords|N-bit <b>parity</b> <b>problem,</b> exclusive-OR problem, neural networks List of special symbols: b c | round towards 1. E. g., b N+ 0 : 5 2 [...] ...|$|E
40|$|AbstractThis paper {{addresses}} a generalization of the matroid <b>parity</b> <b>problem</b> to delta-matroids. We give a minimax relation, {{as well as}} an efficient algorithm, for linearly represented delta-matroids. These are natural extensions of the minimax theorem of Lovász and the augmenting path algorithm of Gabow and Stallmann for the linear matroid <b>parity</b> <b>problem...</b>|$|E
40|$|This paper {{addresses}} saturation phenomena at hidden nodes {{during the}} learning phase of neural networks. The hidden-node saturation tends {{to cause a}} "plateau," a region of very little or no change in a graphic representation of the error learning curve. We investigate the saturation phenomena in multilayer perceptrons (MLP) with the well-known neuralnetwork benchmark problem: the <b>parity</b> <b>problem,</b> describing how to augment their learning capacity to solve it perfectly by avoiding the hidden-node saturation {{in conjunction with a}} dynamic programminglike recursive gradient formula. To make the <b>parity</b> <b>problem</b> especially challenging, we first show that the seven-bit <b>parity</b> <b>problem</b> can in principle be solved using only four hidden nodes and we then compare various learning algorithms using this MLP architecture with emphasis on whether or not each algorithm can solve the problem "perfectly" regardless of its learning speed. In particular, we highlight an onlinemode steepest descent-type learnin [...] ...|$|E
40|$|Abstract. We {{investigate}} a developmental tree-adjoining grammar guided genetic programming system (DTAG 3 P +), in which genetic operator application rates are adapted during evolution. We previously showed developmental evaluation could promote structured solutions and improve performance in symbolic regression problems. However testing on <b>parity</b> <b>problems</b> revealed an unanticipated problem, that good {{building blocks for}} early developmental stages might be lost in later stages of evolution. The adaptive variation rate in DTAG 3 P + preserves good building blocks found in early search for later stages. It gives both good performance on small k-parity problems, and good scaling to large problems...|$|R
40|$|Abstract. The paper {{presents}} {{for the first}} time automatic module acquisition and evolution within the graph based Cartesian Genetic Programming method. The method has been tested on a set of even <b>parity</b> <b>problems</b> and compared with Cartesian Genetic Programming without modules. Results are given that show that the new modular method evolves solutions up to 20 times quicker than the original non-modular method and that the speedup is more pronounced on larger problems. Analysis of some of the evolved modules shows that often they are lower order parity functions. Prospects for further improvement of the method are discussed. ...|$|R
5000|$|In this version, {{the samples}} may contain some error. Instead of samples (x, ƒ(x)), the {{algorithm}} {{is provided with}} (x, y), where y = 1 &minus; ƒ(x) with some small probability. The noisy version of the <b>parity</b> learning <b>problem</b> is conjectured to be hard.|$|R
