10|5|Public
40|$|This paper formulates a simple, regenerative, optimal-stopping {{model of}} bus-eng ine {{replacement}} {{to describe the}} behavior of Harold Zurcher, superinte ndent of maintenance at the Madison (Wisconsin) Metropolitan Bus Comp any. Admittedly, few {{people are likely to}} take particular interest in Harold Zurcher and bus engine replacement per se. The author focuses on a specific individual and capital good because it provides a simp le, concrete framework to illustrate two ideas: (1) a "bottom-up" a <b>pproach</b> <b>for</b> modeling replacement investment and (2) a "nested fixed point" algorithm for estimating dynamic programming models of discre te choice. Copyright 1987 by The Econometric Society. ...|$|E
40|$|There is a {{high demand}} for {{mosaicking}} of digital images. The digital images are of different formats, different resolutions, band combinations, sizes, etc. This increases {{the complexity of the}} mosaicking of images. We propose a dynamic, real-time <b>pproach</b> <b>for</b> mosaicking digital images of different temporal and spatial characteristics into tiles. Also, the source images at the time of mosaicking could be accessed from LAN locations and remote Internet locations. Further any specific source image can be reconstructed from the mosaicked tiles. This dynamic approach reuses digital images upon demand and generates mosaicked tiles only for the required region accord ng to user’s requirements such as resolution, temporal range, target bands, etc...|$|E
40|$|A new ollaborative <b>pproach</b> <b>for</b> e hanced enoising under ow-light xcitation (CANDLE) is {{introduced}} for the processing ofC A N D L E 3 D laser scanning multiphoton microscopy images. CANDLE {{is designed to}} be robust for low signal-to-noise ratio (SNR) conditions typically encountered when imaging deep in scattering biological specimens. Based on an optimized non-local means filter involving the comparison of filtered patches, CANDLE locally adapts the amount of smoothing {{in order to deal with}} the noise inhomogeneity inherent to laser scanning fluorescence microscopy images. An extensive validation on synthetic data, images acquired on microspheres and images is presented. These experiments show that the CANDLE filter obtained competitive resultsin vivo compared to a state-of-the-art method and a locally adaptive optimized nonlocal means filter, especially under low SNR conditions (PSNR< 8 dB). Finally, the deeper imaging capabilities enabled by the proposed filter are demonstrated on deep tissue images ofin vivo neurons and fine axonal processes in the Xenopus tadpole brain. Author Keywords nonlocal means; multiphoton image; denoising; filterin...|$|E
40|$|We {{present a}} unified {{treatment}} of solvatochromic shifts in liquid-ph alle ica g c in, pro me on consistent reaction field, which appears {{in several of}} the histor- can occur, the solvent and excited-state solute typically have time nic distributions but also their ose of the present article is to <b>pproaches</b> <b>for</b> calculating the t response and for calculating s on the fast time scale of his analysis to develop a new accurate prediction of sol-in polar and nonpolar media. s contributing on various time totype transitions, namely, th...|$|R
3000|$|... <b>pproach</b> (SPA) <b>for</b> planar point sets. The {{proposed}} {{convex hull}} algorithm termed as CudaChain {{consists of two}} stages: (1) two rounds of preprocessing performed on the GPU and (2) the finalization of calculating the expected convex hull on the CPU. Those interior points locating inside a quadrilateral formed by four extreme points are first discarded, and then the remaining points are distributed into several (typically four) sub regions. For each subset of points, they are first sorted in parallel; then {{the second round of}} discarding is performed using SPA; and finally a simple chain is formed for the current remaining points. A simple polygon can be easily generated by directly connecting all the chains in sub regions. The expected convex hull of the input points can be finally obtained by calculating the convex hull of the simple polygon. The library Thrust is utilized to realize the parallel sorting, reduction, and partitioning for better efficiency and simplicity. Experimental results show that: (1) SPA can very effectively detect and discard the interior points; and (2) CudaChain achieves 5 ×– 6 × speedups over the famous Qhull implementation for 20 M points.|$|R
40|$|Using the new variational <b>pproach</b> {{proposed}} recently <b>for</b> a systematxc {{improvement of}} the locally harmonic Feynman-Kleinert approximation t path integrals we calculate he partition function of the anharmonic osctllator for all temperatures and coupling strengths with high accuracy. 1. Some time ago, Feynman and Kleinert [1] extended the variational approach to Euclidean path integrals developed earlier by Feynman in his textbook on statistical mechanics [2], improving reatly the accuracy at low temperatures. A similar extension was found independently b Giacetti and Tognetti [3] who applied this method to several statistical systems [4]. The approach is particularly successful in systems in which the quan-tum effects do not produce ssentially new phenomena, their main result being a modification of the quasi-harmonic properties of a system. This happens in quantum crystals [5] where experimental data can now be explained very well. The approach yields a good approximation to the effective classzcal potential V~ff,¢i at all temperatures and serves to calculate the free energy of the system as well as particle distributions at all coupling strengths, including the strong coupling limit. The purpose of this note is to present results of a recently proposed systematic improvement [6] of this method for the anharmonic oscillator carried out to third order in the coupling strength. The results {{turn out to be}} in excellent agreement with precise numerical values of the free energy [7] at all temperatures and cou...|$|R
40|$|In {{wireless}} networks, due to {{the lack}} of fixed infrastructure or centralized administration, a Connected Dominating Set (CDS) of the graph representing the network is an ideal candidate which can serve as the virtual backbone of a wireless network. Connected ed Dominating Set based routing is a promising approach for enhancing the routing efficiency and communication range in case of wireless ad hoc networks. However, finding the Minimal Connected dominating set in an arbitrary graph is a NP NP-hard problem. Through ugh MCDS, I am proposing an approach through which we will be able to enhance the “Routing Efficiency & Communication Range ” of networks by choosing some specific nodes. These very specific nodes will be assumed to be “Dominator ” nodes. In case of wireless networks, the node represents the Workstations and a temporary connection between them is the links which connect any two nodes. This concept can be clearer by the theory of Graph. Connected Dominating Set (CDS) has been a well known approach <b>pproach</b> <b>for</b> constructing a virtual backbone to alleviate the broadcasting storm in wireless networks...|$|E
40|$|This paper {{introduces}} {{an analytical}} <b>pproach</b> <b>for</b> studying lexicography ingeneral-ized network problems. The equations obtained {{can help us}} to understand and to extend the existing theory. First, it is verified that all nonzero elements have the same sign in each row vector of a basis inverse for a generalized network (GN) prob-lem with positive multipliers. However, this property does not necessarily hold when there xist negative multipliers. Second, we developed a strategy to select he dropping arc in the GN simplex algorithm when addressing GN problems with positive and negative multipliers. This strategy is also based on lexicography and requires per-forming some comparisons. However, the values to be compared are already known since they can be obtained as a by-product of the calculations ecessary to compute the basis representation f the entering arc. Consequently, the computational effort per pivot step is O(n) in the worst case. This worst case effort {{is the same as}} that required by the strongly convergent rules for selecting the dropping arc in the method of strong convergence...|$|E
40|$|An exact formulation, {{based on}} the Trefftz-plane <b>pproach,</b> <b>for</b> the {{evaluation}} of aerodynamic loads acting on bodies having arbitrary motion, is presented for incompressible flows that are quasi-potential (i. e., potential everywhere except for the points of a zero- thickness wake surface). The resulting expressions, obtained without using simplifying assumptions, give an exact representation for forces and moments in terms of (i) the velocity potential over the body surface, (ii) the velocity potential and its normal derivative over an arbitrary plane that intersects the wake (Trefftz plane), and (iii) the potential discontinuity over {{the portion of the}} wake limited downstream by such a plane. It is shown that the present formulation includes, as particular cases, some classical results by Prandtl, Trefftz and Lamb, for the evaluation of forces and moments in incompressible quasi-potential flows. A convergence analysis is presented in order to assess the accuracy of the present formulation in comparison with the evaluation of forces and moments by direct integration of the pressure distribution over the body surface. Numerical results concerning lift, drag and moments acting on isolated wings in steady and unsteady flows are included...|$|E
40|$|Increases or {{decreases}} {{in the size}} of populations over space and time are, arguably, the motivation for much of pure and applied ecological research. The fundamental model for the dynamics of any population is straightforward: the net change over time in the abundance of some population is the simple difference between the number of additions (individuals entering the population) minus the number of subtractions (individuals leaving the population). Of course, the precise nature of the pattern and process of these additions and subtractions is often complex, and population biology is often replete with fairly dense mathematical representations of both processes. While {{there is no doubt that}} analysis of such abstract descriptions of populations has been of considerable value in advancing our, there has often existed a palpable discomfort when the "beautiful math" is faced with the often "ugly realities" of empirical data. In some cases, this attempted merger is abandoned altogether, because of the paucity of "good empirical data" with which the theoretician can modify and evaluate more conceptually–based models. In some cases, the lack of "data" is more accurately represented as a lack of robust estimates of one or more parameters. It is in this arena that methods developed to analyze multiple encounter data from individually marked organisms has seen perhaps the greatest advances. These methods have rapidly evolved to facilitate not only estimation of one or more vital rates, critical to population modeling and analysis, but also to allow for direct estimation of both the dynamics of populations (e. g., Pradel, 1996), and factors influencing those dynamics (e. g., Nichols et al., 2000). The interconnections between the various vital rates, their estimation, and incorporation into models, was the general subject of our plenary presentation by Hal Caswell (Caswell & Fujiwara, 2004). Caswell notes that although interest has traditionally focused on estimation of survival rate (arguably, use of data from marked individuals has been used for estimation of survival more than any other parameter, save perhaps abundance), it is only one of many transitions in the life cycle. Others discussed include transitions between age or size classes, breeding states, and physical locations. The demographic consequences of these transitions can be captured by matrix population models, and such models provide a natural link connecting multi–stage mark–recapture methods and population dynamics. The utility of the matrix approach for both prospective, and retrospective, analysis of variation in the dynamics of populations is well–known; such comparisons of results of prospective and retrospective analysis is fundamental to considerations of conservation management (sensu Caswell, 2000). What is intriguing is the degree to which these methods can be combined, or contrasted, with more direct estimation of one or more measures of the trajectory of a population (e. g., Sandercock & Beissinger, 2002). The five additional papers presented in the population dynamics session clearly reflected these considerations. In particular, the three papers submitted for this volume indicate the various ways in which complex empirical data can be analyzed, and often combined with more classical modeling approaches, to provide more robust insights to the dynamics of the study population. The paper by Francis & Saurola (2004) is an example of rigorous analysis and modeling applied to a large, carefully collected dataset from a long–term study of the biology of the Tawny Owl. Using a combination of live encounters and dead recoveries, the authors were able to separate the relative contributions of various processes (emigration, mortality) on variation in survival rates. These analyses were combined with periodic matrix models to explore comparisons of direct estimation of changes in population size (based on both census and mark– recapture analysis) with model estimates. The utility of combining sources of information into analysis of populations was the explicit subject of the other two papers Gauthier & Lebreton (2004) draw on a long–term study of an Arctic–breeding Goose population, where both extensive mark–recapture, ring recovery, and census data are available. The primary goal is to use these various sources of information to to evaluate the effect of increased harvests on dynamics of the population. A number of methods are compared; most notably they describe an approach based on the Kalman filter which allows for different sources of information to be used in the same model, that is demographic data (i. e. transition matrix) and census data (i. e. annual survey). They note that one advantage of this approach is that it attempts to minimize both uncertainties associated with the survey and demographic parameters based on the variance of each estimate. The final paper, by Brooks, King and Morgan (Brooks et al., 2004) extends the notion of the combining information in a common model further. They present a Bayesian analysis of joint ring–recovery and census data using a state–space model allowing for the fact that not all members of the population are directly observable. They then impose a Leslie–matrix–based model on the true population counts describing the natural birth–death and age transition processes. Using a Markov Chain Monte Carlo (MCMC) approach (which eliminates the need for some of the standard assumption often invoked in use of a Kalman filter), Brooks and colleagues describe methods to combine information, including potentially relevant covariates that might explain some of the variation, within a larger framework that allows for discrimination (selection) amongst alternative models. We submit that all of the papers presented in this session indicate clearly significant interest in <b>pproaches</b> <b>for</b> combining data and modeling approaches. The Bayesian framework appears a natural framework for this effort, since it is able to not only provide a rigorous way to evaluate and integrate multiple sources of information, but provides an explicit mechanism to accommodate various sources of uncertainty about the system. With the advent of numerical approaches to addressing some of the traditionally "tricky" parts of Bayesian inference (e. g., MCMC), and relatively user–friendly software, we suspect that there will be a marked increase in the application of Bayesian inference to the analysis of population dynamics. We believe that the papers presented in this, and other sessions, are harbingers of this trend...|$|R
40|$|The Hohokam Water Management Simulation (HWM) is a {{computer}} simulation for exploring {{the operation of the}} Hohokam irrigation systems in southern Arizona. The simulation takes a middle road between two common kinds of archaeological simulation: large-scale, detailed landscape and environmental reconstructions and highly abstract hypothesis-testing simulations. Given the apparent absence in the Hohokam context of a central authority, the specific aim of the HWM is approaching the Hohokam as a complex system, using principles such as resilience, robustness, and self-organization. The Hohokam case is reviewed, and general questions concerning how the irrigation systems operated are shown to subsume multiple crosscutting and unresolved issues. Existing proposals about the relevant aspects of Hohokam society and of its larger long-term trajectory are based on widely varying short- and long-term processes that invoke different elements, draw different boundaries, and operate at different spatial and temporal scales, and many rely on information that is only incompletely available. A framework <b>for</b> <b>pproaching</b> problems of this kind is put forward. A definition of modeling is offered that specifies its epistemological foundations, permissible patterns of inference, and its role in our larger scientific process. Invoking Logical Positivism, a syntactic rather than semantic view of modeling is proposed: modeling is the construction of sets of assertions about the world and deductions that can be drawn from them. This permits a general model structure to be offered that admits hypothetical or provisional assertions and the flexible interchange of model components of varying scope and resolution. Novel goals for archaeological inquiry fall from this flexible approach; these move from specific reconstruction to a search for more universal and general dynamics. A software toolkit that embodies these principles is introduced: the Assertion-Based Computer Modeling toolkit (ABCM), which integrates simulation with the logical architecture of a relational database, and further provides an easy means for linking models of natural and social processes (including agent-based modeling). The application of this to the Hohokam context is described, and an extended example is presented that demonstrates the flexibility, utility and challenges of the approach. An attached file provides sample output...|$|R
40|$|The aim of {{this work}} was to {{formulate}} minoxidil loaded liposome and niosome formulations to improve skin drug delivery. Multilamellar liposomes were prepared using soy phosphatidylcholine at different purity degrees (Phospholiponw 90, 90 % purity, soy lecithin (SL), 75 % purity) and cholesterol (Chol), whereas niosomes were made with two different commercial mixtures of alkylpolyglucoside (APG) surfactants (Oramixw NS 10, Oramixw CG 110), Chol and dicetylphosphate. Minoxidil skin penetration and permeation experiments were performed in vitro using vertical diffusion Franz cells and human skin treated with either drug vesicular systems or propylene glycol–water–ethanol solution (control). Penetration of minoxidil in epidermal and dermal layers was greater with liposomes than with niosomal formulations and the control solution. These differences might {{be attributed to the}} smaller size and the greater potential targeting to skin and skin appendages of liposomal carriers, which enhanced globally the skin drug delivery. The greatest skin accumulation was always obtained with nondialysed vesicular formulations. No permeation of minoxidil through the whole skin thickness was detected in the present study irrespective of the existence of hair follicles. Alcohol-free liposomal formulations would constitute a promising <b>pproach</b> <b>for</b> the topical delivery of minoxidil in hair loss treatment...|$|E
40|$|Security {{criteria}} is {{not keeping}} pace with the Informa. tion Revolution. This paper describes an evolutionary, operational experience ba. sed a. <b>pproach</b> <b>for</b> advancing criteria {{to be consistent with}} modern information systems. Interoperable a. nd flexible systems/components a. re (and will be increa. singly) demanded by users. This is especially true for distributed systems. These demands a. re not, ent,irely consistent with today’s foundationa. models of securit,y, leading to the conclusion by many individuals tl 1 a. t earthquake proportion changes in the foundat,ions of information security are necessary. Funda. mental revisions are necessary. There is, however, subst,antial risk in abandoning models tha. t ha. ve been proven t,o work in many environments. The road to success is based on managing the risk associa. ted with moving toward a new vision of information syst,em securit. y. A spiral approach to resolving informa. tion system securit,y issues has been proposed and is now being pra. cticetl. It consists of incremental expansion of security t. lieories and practices (based on esisting theories) wit. 11 directions of advancement det. ermined by operational experience. The experience drives t,heory in a evolutionary, ra. pid prototype verification ma. nner. This pa. per presents criteria rela. ted ba,ckground, describes the spiral concept, and presents examples. ...|$|E
40|$|The {{assessment}} of rock material properties i a critical element in any site investigation for {{the planning and}} construction of a tunnel through rock strata. This is irrespective {{of the number of}} rock strata involved and crucial to the determination f ground reference conditions as detailed in CIRIA Report No. 79 (1978). An extensive suite of laboratory tests is combined with a comprehensive literature search in order to investigate the inter-relationships between some commonly cited rock parameters. The suite includes Shore Scleroscope Rebound Values and results from a modified form of the Cerchar Abrasivity Test. In addition, the benefits from mineralogical analyses of rock samples are considered. Rock material strength and hardness, with regard to common tunnel excavation techniques i found to be a combination of shear and tensile strength and can respectively be designated as cutting strength and abrasivity. An overall classification system is proposed which incorporates these parameters into an unambiguous notation in order to clarify test data and facilitate interpretation. By an appraisal of each testing technique in respect of relative costs and applicability to rock failure under machine xcavation forces an incremental <b>pproach</b> <b>for</b> rock testing programme is censtructed {{in the form of a}} flow chart. The model proposes how the efficiency of current test programmes can be improved thus enabling more confident predictions of materia ",zroperties in single or multi-strata ground along a proposed tunnel route...|$|E
40|$|Abstract: The {{record of}} fossil plants in China can {{date back to}} the year 1086 during the Chinese Song Dynasty. The subject of palaeobotany was transplanted into China in the early 20 th century. The rise of Chinese palaeobotany had direct {{connections}} with the world. V. K. Ting {{played a major role in}} the establishment of academic organizations and English journals for Chinese geological sciences, which also received support from foreign experts. A geological <b>pproach</b> <b>for</b> palaeotanical studies was once popular in China because of practical use. H. C. Sze is usually called 'the founder of Chinese palaeobotany'. Sze was a disciple of W. Gothan and made a great contribution to the development of Chinese palaeobotany using a geological approach. Hu Hsen Hsu followed Asa Gray and thought that palaeobotany might be considered as a plant science subject. Hu's study on Metasequoia enhanced his reputation: the discovery of the living plants of Metasequoia is believed to be one of the most important discoveries inthe 20 th century. Hsii Jen majored in plant morphol-ogy and anatomy, and obtained palaeobotanical training in Birbal Sahni's laboratory in the 1940 s. Hsti preferred to employ abiological approach to work on fossil plants. It was as early as in 1086 that Shen Kuo (1029 - 1093), who was one of the great Chinese ancient scientists from the Song Dynasty, recorded the occurrence of fossil plants of so-called bamboo shoots in his volu...|$|E

