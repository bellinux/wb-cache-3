6925|1450|Public
25|$|Rosenblatt (1958) {{created the}} <b>perceptron,</b> an {{algorithm}} for pattern recognition. With mathematical notation, Rosenblatt described circuitry {{not in the}} basic <b>perceptron,</b> such as the exclusive-or circuit {{that could not be}} processed by neural networks at the time.|$|E
25|$|Yang, J., Parekh, R. & Honavar, V. (2000). Comparison of Performance of Variants of Single-Layer <b>Perceptron</b> Algorithms on Non-Separable Data. Neural, Parallel, and Scientific Computation. Vol. 8. pp.415–438.|$|E
25|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2001). Learn++: An Incremental Learning Algorithm for Multi-Layer <b>Perceptron</b> Networks. IEEE Transactions on Systems, Man, and Cybernetics. Vol. 31, No. 4. pp.497–508.|$|E
40|$|We {{study the}} {{capabilities}} of two-layered <b>perceptrons</b> for classifying exactly a given subset. Both necessary and sufficient conditions are derived for subsets to be exactly classifiable with two-layered <b>perceptrons</b> that use the hard-limiting response function. The necessary conditions {{can be viewed as}} generalizations of the linear-separability condition of one-layered <b>perceptrons</b> and confirm the conjecture that {{the capabilities of}} two-layered <b>perceptrons</b> are more limited than those of three-layered <b>perceptrons.</b> The sufficient conditions show that the capabilities of two-layered <b>perceptrons</b> extend beyond the exact classification of convex subsets. Furthermore, we present an algorithmic approach to the problem of verifying the sufficiency condition for a given subset. ]{ey words: classification, multi-layered <b>perceptrons,</b> neural networks...|$|R
40|$|<b>Perceptrons</b> {{have been}} {{known for a long}} time as a {{promising}} tool within the neural networks theory. The analytical treatment for a special class of <b>perceptrons</b> started in seminal work of Gardner Gar 88. Techniques initially employed to characterize <b>perceptrons</b> relied on a statistical mechanics approach. Many of such predictions obtained in Gar 88 (and in a follow-up GarDer 88) were later on established rigorously as mathematical facts (see, e. g. SchTir 02,SchTir 03,TalBook,StojnicGardGen 13,StojnicGardSphNeg 13,StojnicGardSphErr 13). These typically related to spherical <b>perceptrons.</b> A lot of work has been done related to various other types of <b>perceptrons.</b> Among the most challenging ones are what we will refer to as the discrete <b>perceptrons.</b> An introductory statistical mechanics treatment of such <b>perceptrons</b> was given in GutSte 90. Relying on results of Gar 88, GutSte 90 characterized many of the features of several types of discrete <b>perceptrons.</b> We in this paper, consider a similar subclass of discrete <b>perceptrons</b> and provide a mathematically rigorous set of results related to their performance. As it will turn out, many of the statistical mechanics predictions obtained for discrete predictions will in fact appear as mathematically provable bounds. This will in a way emulate a similar type of behavior we observed in StojnicGardGen 13,StojnicGardSphNeg 13,StojnicGardSphErr 13 when studying spherical <b>perceptrons.</b> Comment: arXiv admin note: substantial text overlap with arXiv: 1306. 3809, arXiv: 1306. 3980, arXiv: 1306. 397...|$|R
40|$|We {{investigate}} the network complexity of multi-layered <b>perceptrons</b> for solving exactly a given problem. We limit our study {{to the class}} of combinatorial optimization problems. It is shown how these problems can be reformulated as binary classification problems {{and how they can}} be solved by multi-layered <b>perceptrons.</b> Keywords: Combinatorial Optimization, Classification, Complexity, Exact Network Configurations, Multi-Layered <b>Perceptrons,</b> Neural Networks...|$|R
25|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2000). Learn++: An Incremental Learning Algorithm for Multilayer <b>Perceptron</b> Networks. In: Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2000. Istanbul, Turkey.|$|E
25|$|The {{multilayer}} <b>perceptron</b> is {{a universal}} function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.|$|E
25|$|SVMs {{belong to}} a family of {{generalized}} linear classifiers and {{can be interpreted as}} an extension of the <b>perceptron.</b> They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.|$|E
40|$|In this paper, a {{new type}} of branch {{predictor}} is being introduced. It bases on the <b>perceptrons,</b> one of the simplest possible neural networks. The <b>perceptrons</b> uses the weights vector instead of the saturated counter at the traditional branch predictors to get the proper prediction and effectively increases the accuracy of the predictor. Also it can apply with longer branch histories without increasing the resources exponentially. I will explain the basic principle of the <b>perceptrons</b> and simulate a single <b>perceptrons</b> predictor on the framework of the 2 nd Championship Branch Predictor. Also it will be evaluated on the related traces. We will see that it improves the mis-predict rate by nearly 9. 25 % comparing with a single Gshare predictors at the same 64 K hardware budget. And I introduce some current approaches about the <b>perceptrons.</b> 1...|$|R
40|$|Neural {{networks}} are organized in committees {{to improve the}} correctness of the decisions created by artificial neural networks (ANN’s). In the classification of human chromosomes, it is accustomed to use multilayer <b>perceptrons</b> with multiple (22 - 24) outputs. Because of the huge number of synaptic weights to be tuned, these classifiers cannot go beyond a level of 92 % overall correctness. In this study we represent a special organized committee of 462 simple <b>perceptrons</b> to improve the rate of correct classification of 22 types of human chromosomes. Each of these simple <b>perceptrons</b> is trained to distinguish between two types of chromosomes. When a new data is entered, the votes of these 462 simple <b>perceptrons</b> and additional 22 dummy <b>perceptrons</b> create a decision matrix of the size 22 × 22. By a special assembling of these votes we get {{a higher rate of}} correct classification of 22 types of human chromosomes...|$|R
40|$|We {{describe}} {{a system of}} thousands of binary <b>perceptrons</b> with coarse oriented edges as input which is able to successfully recognize shapes, even in a context with hundreds of classes. The <b>perceptrons</b> have randomized feedforward connections from the input layer and form a recurrent network among themselves. Each class is represented by a pre-learned attractor (serving as an associative `hook') in the recurrent net, corresponding to a randomly selected subpopulation of the <b>perceptrons.</b> In training rst the attractor of the correct class is activated among the <b>perceptrons,</b> and then the visual stimulus is presented at the input layer, The feedforward connections are then modied using eld dependent Hebbian learning with positive synapses, which we show to be stable with respect to large variations in feature statistics and coding levels, and allows {{the use of the}} same threshold on all <b>perceptrons.</b> Recognition is based only on the visual stimuli. These activate the recurrent n [...] ...|$|R
25|$|The group {{method of}} data {{handling}} (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer <b>perceptron</b> with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size {{and depth of}} the resulting network depends on the task.|$|E
25|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial neural network model, for example multi-layer <b>perceptron</b> network model, is not considered as sufficient. A dynamic spiking neural network model is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
25|$|Another {{approach}} to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, {{which has been}} shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, {{in the case of the}} k-medians and the k-nearest neighbors algorithms. Another application is a quadratic speedup in the training of <b>perceptron.</b>|$|E
30|$|RBFs can {{be trained}} much faster than <b>perceptrons.</b>|$|R
40|$|It is {{proved that}} there is a {{monotone}} function in AC (0) 4 which requires exponential size monotone <b>perceptrons</b> of depth 3. This solves the monotone version of a problem which, in the general case, would imply an oracle separation of PP PH. 1 Introduction Recently <b>perceptrons</b> (see definition below) have received some renewed attention because of their relevance to some important issues in structural complexity theory. In [6] it was shown that <b>perceptrons</b> cannot compute parity unless they are of exponential size 1. This has the consequence that (9 A) (6 ` PP PH A). A result for <b>perceptrons</b> of depth 2 was used by Beigel [5] to provide an oracle A such that P NP A 6 ` PP A. Some interesting results for <b>perceptrons</b> were also obtained by Beigel, Reingold and Spielman [3] {{as a consequence of the}} closure under intersection of PP. Further related results and extensions have been obtained by Beigel, Reingold and Spielman [4], Aspnes, Beigel, Furst and Rudich [2], and Taru [...] ...|$|R
5000|$|... scikit-neuralnetwork - Multi-layer <b>perceptrons</b> as a wrapper for Pylearn2 ...|$|R
25|$|Neural {{networks}} are {{modeled after the}} neurons in the human brain, where a trained algorithm determines an output response for input signals. The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, {{in the work of}} Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the <b>perceptron,</b> a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.|$|E
25|$|Photonic {{implementations}} are attracting more attention, not {{the least}} {{because they do not}} require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a <b>perceptron</b> model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first to experimentally demonstrated up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.|$|E
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward multilayer <b>perceptron</b> with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.|$|E
5000|$|<b>Perceptrons,</b> (with Marvin Minsky), MIT Press, 1969 (Enlarged edition, 1988), ...|$|R
5000|$|... #Subtitle level 3: <b>Perceptrons</b> and {{the dark}} age of {{connectionism}} ...|$|R
40|$|The {{theory of}} {{computational}} geometry in <b>Perceptrons</b> (Rosenblatt, 1962), developed by M. Minsky and S. Papert (1969), is extended to “Analog Perceptrons” with real-valued input and out-put. Mathematically, {{our problem is}} to determine the order of a function, i. e., the smallest number of variables necessary to make an additive representation of the function by employing partial functions of the smaller number of variables. Mathematical tools, called the group-invariance theorem, the classification theorem and the collapsing theorem, are given which are useful for evaluating the order of analog <b>Perceptrons.</b> These are also applied for several analog <b>Perceptrons...</b>|$|R
500|$|Theories in the [...] "learning" [...] {{category}} {{almost all}} derive from publications by Marr and Albus. Marr's 1969 paper {{proposed that the}} cerebellum is a device for learning to associate elemental movements encoded by climbing fibers with mossy fiber inputs that encode the sensory context. Albus proposed in 1971 that a cerebellar Purkinje cell functions as a <b>perceptron,</b> a neurally inspired abstract learning device. The most basic difference between the Marr and Albus theories is that Marr assumed that climbing fiber activity would cause parallel fiber synapses to be strengthened, whereas Albus proposed {{that they would be}} weakened. Albus also formulated his version as a software algorithm he called a CMAC (Cerebellar Model Articulation Controller), which has been tested in a number of applications.|$|E
500|$|A <b>perceptron</b> {{was a form}} {{of neural}} network {{introduced}} in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that [...] "perceptron may eventually be able to learn, make decisions, and translate languages." [...] An active research program into the paradigm was carried out throughout the 60s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.|$|E
2500|$|Each block {{consists}} of a simplified multi-layer <b>perceptron</b> (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is [...] Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: ...|$|E
40|$|Artificial neural networks, {{especially}} multilayer <b>perceptrons,</b> {{have been}} recognised {{as being a}} powerful technique for forecasting nonlinear time series; however, cascade-correlation architecture is a strong competitor in this task due to it incorporating several advantages related to the statistical identification of multilayer <b>perceptrons.</b> This paper compares the accuracy of a cascade-co- rrelation neural network to the linear approach, multilayer <b>perceptrons</b> and dynamic architecture for artificial neural networks (DAN 2) {{to determine whether the}} cascade-correlation network was able to forecast the time series being studied with more accu- racy. It was concluded that cascade-correlation was able to forecast time series with more accuracy than other approaches...|$|R
40|$|In {{this paper}} we will review major circuit {{complexity}} results for networks of <b>perceptrons.</b> In {{the first part}} we will present many theoretical results, while the second part is much more practical, as comparing nine different constructive solutions for a particular but important case: the addition of two binary numbers. KEY WORDS Neural / circuit complexity, <b>perceptrons,</b> threshold logic, addition. 1...|$|R
40|$|A theorem {{called the}} tuning theorem, which {{brings out the}} tuning {{capability}} of <b>perceptrons</b> where both the inputs and outputs are binary, has been stated and proved. It has been shown how {{with the help of}} this property, an associative memory has been developed with a single layer of <b>perceptrons</b> working as a massively parallel machine, rather than as a fully connected iterative Hopfield network...|$|R
2500|$|Suppose some given {{data points}} each belong {{to one of}} two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a -dimensional vector (a list of [...] numbers), and we want to know whether we can {{separate}} such points with a -dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the <b>perceptron</b> of optimal stability.|$|E
50|$|In {{the context}} of neural networks, a <b>perceptron</b> is an {{artificial}} neuron using the Heaviside step function as the activation function. The <b>perceptron</b> algorithm is also termed the single-layer <b>perceptron,</b> to distinguish it from a multilayer <b>perceptron,</b> which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer <b>perceptron</b> is the simplest feedforward neural network.|$|E
50|$|Another {{problem with}} the kernel <b>perceptron</b> {{is that it does}} not regularize, making it {{vulnerable}} to overfitting. The NORMA online kernel learning algorithm can be regarded as a generalization of the kernel <b>perceptron</b> algorithm with regularization. The sequential minimal optimization (SMO) algorithm used to learn support vector machines can also be regarded as a generalization of the kernel <b>perceptron.</b>|$|E
40|$|This work {{presents}} an architecture based on <b>perceptrons</b> to recognize phrase structures, and an online learning algorithm {{to train the}} <b>perceptrons</b> together and dependently. The recognition strategy applies learning in two layers: a filtering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reflects to each local function its committed errors from a global point of view, and allows to train them together online as <b>perceptrons.</b> Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. ...|$|R
40|$|When {{reinforcement}} learning {{is applied to}} large state spaces, such as those occurring in playing board games, {{the use of a}} good function approximator to learn to approximate the value function is very important. In previous research, multi-layer <b>perceptrons</b> have often been quite successfully used as function approximator for learning to play particular games with temporal difference learning. With the recent developments in deep learning, it is important to study if using multiple hidden layers or particular network structures can help to improve learning the value function. In this paper, we compare five different structures of multilayer <b>perceptrons</b> for learning to play the game Tic-Tac-Toe 3 D, both when training through self-play and when training against the same fixed opponent they are tested against. We compare three fully connected multilayer <b>perceptrons</b> with a different number of hidden layers and/or hidden units, as well as two structured ones. These structured multilayer <b>perceptrons</b> have a first hidden layer that is only sparsely connected to the input layer, and has units that correspond to the rows in Tic-Tac-Toe 3 D. This allows them to more easily learn the contribution of specific patterns on the corresponding rows. One of the two structured multilayer <b>perceptrons</b> has a second hidden layer that is fully connected to the first one, which allows the neural network to learn to non-linearly integrate the information in these detected patterns. The results on Tic-Tac-Toe 3 D show that the deep structured neural network with integrated pattern detectors has the strongest performance out of the compared multilayer <b>perceptrons</b> against a fixed opponent, both through self-training and through training against this fixed opponent...|$|R
40|$|The {{ice cover}} on lakes {{is one of}} the most {{influential}} factors in the lakes’ winter aquatic ecosystem. The paper presents a method for predicting ice coverage of lakes by means of multilayer <b>perceptrons.</b> This approach is based on historical data on the ice coverage of lakes taking Lake Onega as an example. The daily time series of ice coverage of Lake Onega for 2004 – 2017 was collected by means of satellite data analysis of snow and ice cover of the Northern Hemisphere. Input signals parameters for the multilayer <b>perceptrons</b> aimed at predicting ice coverage of lakes are based on the correlation analysis of this time series. The results of training of multilayer <b>perceptrons</b> showed that <b>perceptrons</b> with architectures of 3 - 2 - 1 within the Freeze-up phase (arithmetic mean of the mean square errors for training epoch MSE¯= 0. 0155 MSE = 0. 0155) and 3 - 6 - 1 within the Break-up phase (MSE¯= 0. 0105 MSE = 0. 0105) have the least mean-squared error for the last training epoch. Tests within the holdout samples prove that multilayer <b>perceptrons</b> give more adequate and reliable prediction of the ice coverage of Lake Onega (mean-squared prediction error MSPE = 0. 0076) comparing with statistical methods such as linear regression, moving average and autoregressive analyses of the first and second order...|$|R
