18|1093|Public
50|$|Grants {{are awarded}} by the Administration for Children and Families (ACF) Regional Offices and the American Indian - Alaska Native and Migrant and Seasonal <b>Program</b> <b>Branches</b> {{directly}} to local public agencies, private organizations, Indian tribes and school systems.|$|E
50|$|In 1989, {{the society}} was {{reactivated}} in the Ukrainian homeland (in Lviv) {{and once again}} undertook a large-scale research and publication <b>program.</b> <b>Branches</b> were soon founded in other Ukrainian cities and membership exceeded a thousand, including 125 full voting members.|$|E
50|$|Based on {{the success}} of the <b>program,</b> <b>branches</b> have been {{established}} at TPC Sawgrass, TPC Scottsdale, TPC Las Vegas, TPC San Antonio and six other resort locations in the United States. The academy at World Golf Village trains and certifies all instructors at TOUR academies.|$|E
40|$|The {{purpose of}} this policy is to define and embrace a {{commitment}} to providing safe housing as part of Housing <b>Programs</b> <b>Branch</b> mandate. This policy will prevent Housing <b>Programs</b> <b>Branch</b> from providing rent geared-to-income housing to a person or persons that have engaged in activities that {{have a negative effect}} on the health, safety or security of one or more persons on Housing <b>Programs</b> <b>Branch</b> property...|$|R
50|$|Natural Disaster Recovery <b>Program</b> <b>Branch.</b>|$|R
5000|$|Assistant Deputy Minister, Emergency Management and <b>Programs</b> <b>Branch</b> ...|$|R
5000|$|In {{computer}} security, a NOP slide, NOP sled or NOP ramp is {{a sequence}} of NOP (no-operation) instructions meant to [...] "slide" [...] the CPU's instruction execution flow to its final, desired destination whenever the <b>program</b> <b>branches</b> to a memory address anywhere on the slide.|$|E
50|$|The {{conditional}} IF statement {{can receive}} {{up to three}} line numbers as parameters (Arithmetic IF). The <b>program</b> <b>branches</b> to the first linenumber if {{the result of the}} expression in parentheses is less than zero, to the second if the result is zero and to the third if the result is above zero.|$|E
40|$|Commodity {{microprocessors}} uniformly apply {{branch prediction}} and single path speculative execution {{to all kinds}} of <b>program</b> <b>branches</b> and suffer from the high misprediction penalty which is caused by branches with low prediction accuracy and, in particular, by branches that are unpredictable. The Simultaneous Speculation Scheduling (S 3) technique removes such penalties by a combination of compiler and architectural techniques that enable speculative dual path execution after <b>program</b> <b>branches.</b> Two separate threads that represent alternative program paths after a branch instruction are generated by the compiler. Both threads are simultaneously executed although only one of them follows the eventually correct program path. The architectural requirements are the ability to run two or more threads in parallel, and an enhancement of the instruction set by instructions that start respectively terminate threads. We use program kernels from the SPECint 95 benchmark suite to demonstrate the perf [...] ...|$|E
5000|$|On April 13, 2009, it was {{announced}} that Gershengorn would be rejoining the United States Department of Justice as a deputy assistant attorney general in the Civil Division with oversight of the Federal <b>Programs</b> <b>Branch.</b> [...] The Federal <b>Programs</b> <b>Branch</b> is handling Guant√°namo Bay detainee cases and state secret matters.|$|R
50|$|Wrestling was {{produced}} by Jacques Bobet for the French <b>program</b> <b>branch</b> of the National Film Board of Canada.|$|R
5000|$|The Department of Social Services {{is divided}} into 10 divisions, with <b>programs,</b> <b>branches,</b> and {{agencies}} under those divisions: ...|$|R
40|$|This paper {{presents}} {{an approach to}} automatic unit test data generation for branch coverage using mixed-integer linear programming, execution trees, and symbolic execution. This approach can be useful to both general testing and regression testing after software maintenance and reengineering activities. Several strategies, including original algorithms, to move towards practical test data generation have been investigated in this paper. Methods include: ffl the analysis of minimum path-length partial execution trees for unconstrained arcs, thus increasing the generation performance and reducing the difficulties originated by infeasible paths ffl {{the reduction of the}} difficulties originated by nonlinear path conditions by considering alternative linear paths ffl the reduction of the number of test cases, which are needed to achieve the desired coverage, based on the concept of unconstrained arcs in a control flow graph ffl the extension of symbolic execution to deal with dynamic memory allocation and deallocation, pointers and pointers to functions Execution trees are symbolically executed to produce Extended Path Constraints (EPC), which are then partially mapped by an original algorithm into linear problems whose solutions correspond to the test data to be used as input to cover <b>program</b> <b>branches.</b> Partially mapping this problem into a linear optimization problem avoids infeasible and non-linear path problems, if a feasible linear alternate path exists in the same execution tree. The presented approach has been implemented in C++ and tested on C-language programs on a Pentium/Linux system. Preliminary results are encouraging and show that a high percentage of the <b>program</b> <b>branches</b> can be covered by the test data automatically produced. The approach is flexible to br [...] ...|$|E
40|$|Abstract: While {{decision}} tree compilation is a promisingway {{to carry out}} guard tests eciently, the methods given in the literature do {{not take into account}} either the execution characteristics of the program or the machine-level tradeos between dierent ways to implement branches. These methods therefore oer little or no guidance for the implementor with regard to how {{decision tree}}s are to be realized on a particular machine. In this paper, we describe an approach that takes execution frequencies of dierent <b>program</b> <b>branches,</b> as well as the costs of alternative branch realizations, to generate decision trees. Experiments indicate that the performance of our approach is uniformly better than that of plausible alternative schemes. ...|$|E
40|$|An inter-procedural {{data flow}} {{analysis}} operating on control flow graphs and collecting information about program expressions {{is described in}} this paper. The following relational and other expressions are analyzed: equivalences between program expressions and constants; linear-ordering inequalities between program expressions and constants; equalities originating from some program assignments; atomic constituents of controlling expressions of <b>program</b> <b>branches.</b> Analysis is executed by a worklist-based fixpoint algorithm which interprets conditional branches and incorporates a rule-based inference procedure. Two variants of the polyvariant program point specialization using results of the analysis are presented. The both specializations are done {{at the level of}} control flow graphs. The variants differ in terms of the size of residual programs. ...|$|E
50|$|Conditional: Evaluates a <b>program</b> <b>branch,</b> {{depending}} on a condition computed at runtime. This skeleton leaves the parallelism potential unchanged.|$|R
50|$|Comparison of polynomials has {{applications}} for <b>branching</b> <b>programs</b> (also called binary decision diagrams). A read-once <b>branching</b> <b>program</b> {{can be represented}} by a multilinear polynomial which computes (over any field) on {0,1}-inputs the same Boolean function as the <b>branching</b> <b>program,</b> and two <b>branching</b> <b>programs</b> compute the same function {{if and only if}} the corresponding polynomials are equal. Thus, identity of Boolean functions computed by read-once <b>branching</b> <b>programs</b> can be reduced to polynomial identity testing.|$|R
50|$|EMA {{is made up}} of the Security Coordination Branch, Crisis Coordination Branch, Crisis Support Branch and the Natural Disaster Recovery <b>Program</b> <b>Branch.</b>|$|R
40|$|While {{decision}} tree compilation is a promising way {{to carry out}} guard tests efficiently, the methods given in the literature do {{not take into account}} either the execution characteristics of the program or the machine-level tradeoffs between different ways to implement branches. These methods therefore offer little or no guidance for the implementor with regard to how {{decision tree}}s are to be realized on a particular machine. In this paper, we describe an approach that takes execution frequencies of different <b>program</b> <b>branches,</b> as well as the costs of alternative branch realizations, to generate decision trees. Experiments indicate that the performance of our approach is uniformly better than that of plausible alternative schemes. 1 Introduction There has {{been a great deal of}} research, in recent years, on the design and implementation of concurrent logic and constraint programming languages (see, for example, [12, 13, 14, 15, 17]). Much of the implementation effort in this context ha [...] ...|$|E
40|$|An {{analysis}} method for specialization of imperative programs {{is described in}} this paper. This anal-ysis is an inter-procedural data flow method operating on control flow graphs and collecting infor-mation about program expressions. It applies to various procedural languages. The set of analyzed formulas includes equivalences between program expressions and constants, linear-ordering ine-qualities between program expressions and constants, equalities originating from some program assignments, and atomic constituents of controlling expressions of <b>program</b> <b>branches.</b> Analysis is executed by a worklist-based fixpoint algorithm which interprets conditional branches and ignores some impossible paths. This analysis algorithm incorporates a simple inference procedure that uti-lizes {{both positive and negative}} information. The analysis algorithm is shown to be conservative; its asymptotic time complexity is cubic. A polyvariant specialization of imperative programs, that is based on the information collected by the analysis, is also defined at the level of nodes and edges of control flow graphs. The specialization incorporates a further refinement of analysis information through local propagation. Multiple variants are produced by replicating disjoint subgraphs whose in-links are limited to one node...|$|E
40|$|An {{operational}} data flow analysis framework for tracking controlling expressions of conditional branches {{is defined in}} this work. The framework is based on control flow graphs and applies to various procedural languages. In this framework, environments assigned to edges and nodes of control flow graphs are basically conjunctions of certain predicate formulas. These formulas include: atomic constituents of controlling expressions of <b>program</b> <b>branches</b> and their negations; equalities originating from program assignments and their negations; equivalences between program expressions and constants and their negations; linear-ordering inequalities between program expressions and constants. Analysis is executed by a worklist-based fixpoint algorithm which interprets conditional branches and ignores some impossible paths. This analysis algorithm incorporates a simple inference procedure that derives validity or inconsistency of environment constituents by exploiting {{both positive and negative}} information. The analysis algorithm is shown to be conservative; its asymptotic time complexity is cubic. Tracking branch conditions enables such optimizations as elimination of unreachable code, elimination of redundant conditional branches, and removal of conditional branches from some execution paths. Two novel optimizations enabled by the analysis are presented. These optimizations eliminate conditional branches from some paths after replicating code. They leverage upon additional local analysis...|$|E
5000|$|Cross-border {{education}} or Transnational education {{comes in a}} variety of forms ranging from twinning <b>programs,</b> <b>branch</b> campuses, joint and dual degrees and online education.|$|R
25|$|There {{are four}} {{principal}} elements within SAD's Special Operations Group: the Air Branch, the Maritime Branch, the Ground Branch, and the Armor and Special <b>Programs</b> <b>Branch.</b> The Armor and Special <b>Programs</b> <b>Branch</b> {{is charged with}} development, testing, and covert procurement of new personnel and vehicular armor and maintenance of stockpiles of ordnance and weapons systems used by SOG, almost all of which must be obtained from clandestine sources abroad, {{in order to provide}} SOG operatives and their foreign trainees with plausible deniability in accordance with U.S. Congressional directives.|$|R
50|$|Within the Operations Directorate, MEMA {{operates}} a Regional <b>Programs</b> <b>branch.</b> This branch employees Regional Liaison Officers {{that work in}} the field directly with local emergency managers and emergency responders.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimited. Efforts {{to bridge the}} cycle-time gap between high-end microprocessors and low-speed main memories {{have led to a}} hierarchical approach in memory subsystem design. The predictive read cache (PRC) has been developed as an alternative way to overcome the speed discrepancy without incurring the hardware cost of a second-level cache. Although the PRC can provide an improvement over a memory hierarchy using only a first-level cache, previous studies have shown that its performance is degraded due to the poor locality of reference caused by <b>program</b> <b>branches,</b> subroutine calls, and context switches. This thesis develops a new prediction algorithm that allows the PRC to track the miss patterns of the first-level cache, even with programs exhibiting poor locality. It presents PRC design alternatives and hardware cost estimates for the implementation of the new algorithm. The architectural support needed from the underlying microprocessor is also discussed. The second part of the thesis involves the development of a memory hierarchy simulator and an address-trace conversion program to perform trace-driven simulations of the PRC. Using address traces captured from a SPARC-based computer system, the simulations show that the new prediction algorithm provides a significant improvement in the PRC performance. This makes the PRC ideal for embedded systems in space-based, weapons-based and portable/mobile computing applications. [URL] Junior Grade, Turkish Nav...|$|E
40|$|Power leakage {{constitutes}} an increasing {{fraction of the}} total power consumption in modern semi-conductor technologies. Recent research efforts indicate that architectures, compilers, and software can be optimized so as to reduce the switching power (also known as dynamic power) in micropro-cessors. This has lead to interest in using architecture and compiler optimization to reduce leakage power (also known as static power) in microprocessors. In this article, we investigate compiler-analysis techniques that are related to reducing leakage power. The architecture model in our design is a system with an instruction set to support the control of power gating at the component level. Our compiler provides an analysis framework for utilizing instructions to reduce the leakage power. We present a framework for analyzing data flow for estimating the component activities at fixed points of programs whilst considering pipeline architectures. We also provide equations that can be used by the compiler to determine whether employing power-gating instructions in given program blocks will reduce the total energy requirements. As the duration of power gating on com-ponents when executing given program routines is related to the number and complexity of <b>program</b> <b>branches,</b> we propose a set of scheduling policies and evaluate their effectiveness. We performed experiments by incorporating our compiler analysis and scheduling policies into SUIF compiler tools and by simulating the energy consumptions on Wattch toolkits. The experimental results demonstrate that our mechanisms are effective in reducing leakage power in microprocessors...|$|E
40|$|Abstract. Power leakage {{constitutes}} an increasing {{fraction of the}} total power consumption in modern semiconductor technologies. Recent research efforts also indicate architecture, compiler, and software participations can help reduce the switching activities (also known as dynamic power) on microprocessors. This raises interests on the issues to employ architecture and compiler efforts to reduce leakage power (also known as static power) on microprocessors. In this paper, we investigate the compiler analysis techniques related to reducing leakage power. The architecture model in our design is a system with an instruction set to support the control of power gating in the component levels. Our compiler gives an analysis framework to utilize the instruction to reduce the leakage power. We present a data flow analysis framework to estimate the component activities at fixed points of programs with the consideration of pipelines of architectures. We also give the equation for the compiler to decide if the employment of the power gating instructions on given program blocks will benefit the total energy reductions. As the duration of power gating on components on given program routines is related to <b>program</b> <b>branches,</b> we propose a set of scheduling policy include Basic Blk Sched, MIN Path Sched, andAVG Path Sched mechanisms and evaluate the effectiveness of those schemes. Our experiment is done by incorporating our compiler analysis and scheduling policy into SUIF compiler tools [32] and by simulating the energy consumptions on Wattch toolkits [6]. Experimental results show our mechanisms are effective in reducing leakage powers on microprocessors. ...|$|E
40|$|We {{propose a}} new model of {{restricted}} <b>branching</b> <b>programs</b> specific to solving GEN problems, which we call incremental <b>branching</b> <b>programs.</b> We show that syntactic incremental <b>branching</b> <b>programs</b> capture previously studied models of computation for the problem GEN, namely marking machines [Co 74] and Poon‚Äôs extension [Po 93] of jumping automata on graphs [CoRa 80]. We then prove exponential size lower bounds for our syntactic incremental model, and for some other variants of <b>branching</b> <b>program</b> computation for GEN. We further show that nondeterministic syntactic incremental <b>branching</b> <b>programs</b> are provably stronger than their deterministic counterpart when solving a natural NL-complete GEN subproblem. It remains open if syntactic incremental <b>branching</b> <b>programs</b> are as powerful as unrestricted <b>branching</b> <b>programs</b> for GEN problems...|$|R
40|$|IN THE PUBLIC-USE FILEThis tape {{documentation}} {{was prepared}} in the Division of Vital Statistics. Charles Royer of the Systems and <b>Programming</b> <b>Branch</b> {{was responsible for}} developing the mortality documentation. Marian MacDorman of the Mortality Statistics Branch coordinated preparation of the Technical Appendix. The Registration Methods Branch and the Technical Services Branch provided consultation to State vi‚Äô certificate data. al statistics offices regarding collection of death Questions concerning the mortal ity file should be Division of Vital Statist MD 20782 (301 - 436 - 8900). documentation or general questions concerning the directed to the Systems and <b>Programming</b> <b>Branch...</b>|$|R
40|$|<b>Branching</b> <b>programs</b> are a {{well-established}} computation model for boolean functions, especially read-once <b>branching</b> <b>programs</b> {{have been studied}} intensively. Exponential lower bounds for deterministic and nondeterministic read-once <b>branching</b> <b>programs</b> are {{known for a long}} time. On the other hand, the problem of proving superpolynomial lower bounds for parity read-once <b>branching</b> <b>programs</b> is still open. In this paper restricted parity read-once <b>branching</b> <b>programs</b> are considered and an exponential lower bound on the size of well-structured parity graph-driven read-once <b>branching</b> <b>programs</b> for integer multiplication is proven. This is the first strongly exponential lower bound on the size of a nonoblivious parity read-once <b>branching</b> <b>program</b> model for an explicitly defined boolean function. In addition, more insight into the structure of integer multiplication is yielded...|$|R
40|$|Abstract‚ÄîModern {{embedded}} systems are {{built around the}} soft core processors implemented on FPGA. The FPGAs being capable of implementing custom hardware blocks giving the advantage of ASICs, and allowing the implementation of processor platform are resulting in powerful Configurablesystem on chip(C-SoC) platforms. The Microchip‚Äôs PIC microcontroller is very widely used microcontroller architecture across various {{embedded systems}}. The implementation of such core on FPGA is very much useful in CSOC based embedded systems. This type of designs can be widely used in those controlling fields demanding low power consumption and high ratio of performance to price. In this project a {{reduced instruction set computer}} (RISC) CPU IP core whose instructions are compatible with the Microchip PIC 16 C 6 Xseries of microcontrollers is implemented in VHDL. The core is based on 8 -bit RISC architecture and top-Down design methodology is used in developing the core. The RISC CPU core is based on Harvard architecture with 14 -bit instruction length and 8 -bit data length and two-stage instruction pipeline. The architecture will be designed aiming at single cycle execution of the instructions, except those related to <b>program</b> <b>branches.</b> Since this type of CPU based on RISC architecture, there are only 35 reduced instructions in its instruction set, which are easy to be learned and used. The performance of the 8 -bit RISC CPU is better than those of CPUs which are based on CISC architecture. Modelsim Xilinx Edition (MXE) will be used simulation and functional verification. The Xilinx Spartan- 3 E FPGAs will be used synthesis and timing analysis. The results will be verified on chip with chipscope tool...|$|E
40|$|Graduation date: 2015 Software {{testing is}} of {{critical}} importance {{for the success of}} software projects. Current inefficient testing methods often still take up half or more of a software project's budget. Automatic test data generation is the most promising way to lower the software testing cost. Manually creating testing data is expensive and often needs deep domain knowledge. Therefore, both industry and academia are always highly interested in automatic approaches to generating test data. Symbolic execution {{has been one of the}} most promising and exciting areas of automated testing research for many years now. In principle, symbolic execution "runs" a program, replacing concrete inputs with symbolic variables that represent all possible values. When a <b>program</b> <b>branches,</b> the execution takes both paths (if they are feasible under current constraints) and a set of path conditions on symbolic variables is modified for each path to record the new constraints on the symbolic values. However, scaling symbolic execution to large programs or programs with complex inputs remains difficult due to path explosion and complex constraints, as well as external method calls. Additionally, creating an effective test structure with symbolic inputs can be difficult. A popular symbolic execution strategy in practice is to perform symbolic execution not "from scratch" but based on existing test cases. This dissertation explores the idea that the effectiveness of this approach to symbolic execution can be enhanced by (1) reducing the size of seed test cases, (2) prioritizing seed test cases to maximize exploration efficiency, and (3) selecting a subset tests for symbolic execution without specifying the number of tests. The proposed test case reduction strategy is based on a recently introduced generalization of delta-debugging, and our prioritization and selection techniques include novel methods that, for this purpose, can outperform some traditional regression testing algorithms. Our results show that applying these methods can significantly improve the effectiveness of symbolic execution based on existing test cases...|$|E
40|$|The {{need for}} the study arose with the {{discovery}} that many older adults make significant contributions in volunteer work. However some do not feel as though these contributions are worthwhile (Morrow-Howell & Mui, 1989; Chambre, 1987). They may feel that the work they are doing is meaningless (Morrow-Howell & Mui, 1989; Chambre, 1987; Flynn & Webb, 1975). The {{study was conducted to}} determine the relationship of individuals who perceived they were making a significant contribution in volunteer work and their level of subjective morale. The number of hours, length of time, number of various jobs, and the use of a volunteer's skills, knowledge, and talents devoted to volunteer service are specifically identified as they relate to subjective morale. A pilot study was conducted on the telephone using 10 - 12 volunteers from a random sample of retired Ball State employees. These individuals were asked open-ended questions concerning several aspects of volunteering. Once the data was collected a Liekert questionnaire was developed using the common patterns noted in the volunteer's answers. The Liekert questionnaire was distributed to 50 volunteers, recruited from the Muncie, Indiana andthe New Castle, Indiana RSVP (The Retired Senior Volunteer <b>Program)</b> <b>branches.</b> The volunteers completed the Philadelphia Geriatric Center Morale Scale: A Revision (PGC) and the Volunteer Attitude Questionnaire (VAQ). The results indicated no statistically significant relationship between volunteers who perceived they were making a significant contribution and their level of subjective morale. Likewise, the data indicated no statistically significant relationship between subjective morale and number of hours, length time, number of various jobs, and use of skills, knowledge, and talents devoted to volunteer services. The volunteers in the study reported a wide variety of subjective morale levels regardless of the factors examined in the study. It is apparent that older volunteers who perceived they were significantly contributing do not all develop increased morale. Morale appears to be a very complex factor which is individually determined depending upon the lifestyle of the older adult. Institute of GerontologyThesis (M. S. ...|$|E
40|$|March 1993 This tape {{documentation}} {{was prepared}} in the Division of Vital Statistics. Charles Royer of the Systems and <b>Programming</b> <b>Branch</b> {{was responsible for}} developing the mortality documentation. He and Adrienne McDonald of the Technical Services Branch were responsible for providing all computer programming services necessary to keep it up-to-date. Bettie L. Hudson of the Mortality Statistics Branch prepared the Technical Appendix. The Registration Methods Branch and the Technical Services Branch provided consultation to State vital statistics offices regarding collection of death certificate data. Questions concerning the documentation or general questions concerning the mortal ity file should be directed to the Systems and <b>Programming</b> <b>Branch...</b>|$|R
40|$|Randomized <b>branching</b> <b>programs</b> are a {{probabilistic}} model of computation defined in analogy to the well-known probabilistic Turing machines. In this paper, we present complexity theoretic results for randomized read-once <b>branching</b> <b>programs.</b> Our main result shows that nondeterminism {{can be more}} powerful than randomness for read-once <b>branching</b> <b>programs.</b> We present a function which is computable by nondeterministic read-once <b>branching</b> <b>programs</b> of polynomial size, while {{on the other hand}} randomized read-once <b>branching</b> <b>programs</b> for this function with twosided error at most 21 ¬° 256 have exponential size. The same function exhibits an exponential gap between the randomized read-once <b>branching</b> <b>program</b> sizes for different constant worst-case errors, which shows that there is no ‚Äúprobability amplification ‚Äù technique for read-once <b>branching</b> <b>programs</b> which allows to decrease the error to an arbitrarily small constant by iterating probabilistic computations...|$|R
50|$|Floating-point {{operations}} were given {{pride of place}} in this architecture: the CDC 6600 (and kin) stand virtually alone in being able to execute a 60-bit floating point multiplication in time comparable to that for a <b>program</b> <b>branch.</b>|$|R
