10000|10000|Public
25|$|<b>Principal</b> <b>component</b> <b>analysis.</b> The {{method of}} fitting a linear {{subspace}} to multivariate data by minimising the chi distances.|$|E
25|$|<b>Principal</b> <b>component</b> <b>analysis</b> is used {{to study}} large data sets, such as those {{encountered}} in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, {{in the field of}} psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, <b>principal</b> <b>component</b> <b>analysis</b> {{can be used as a}} method of factor analysis in structural equation modeling.|$|E
25|$|Several {{important}} {{problems can}} be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm {{is based on the}} singular value decomposition. The corresponding tool in statistics is called <b>principal</b> <b>component</b> <b>analysis.</b>|$|E
40|$|This paper {{presents}} a method called quadtree <b>principal</b> <b>components</b> <b>analysis</b> for facial expression classification. The quadtree <b>principal</b> <b>components</b> <b>analysis</b> {{is an image}} transformation that {{takes its name from}} the quadtree partition scheme on which it is based. The quadtree <b>principal</b> <b>components</b> <b>analysis</b> method implements a global-local decomposition of the input face image. This solves the problems associated with the existing <b>principal</b> <b>components</b> <b>analysis</b> and local <b>principal</b> <b>components</b> <b>analysis</b> methods when applied to facial expression classification...|$|R
40|$|The {{essence of}} <b>principal</b> <b>components</b> <b>analysis</b> {{and the problem}} of {{dimension}} reduction are described. A method of <b>principal</b> <b>components</b> calculation is presented, which is based on the covariance matrix eigenvalues determination. Practical implementations of <b>principal</b> <b>components</b> <b>analysis</b> are described, which are based on QR-algorithm. Application of <b>principal</b> <b>components</b> <b>analysis</b> in space images classification for the reduction of training samples dimension is discussed...|$|R
30|$|A {{combination}} of methods for image subtraction and <b>principal</b> <b>components</b> <b>analysis</b> and {{the division of}} images and <b>principal</b> <b>components</b> <b>analysis</b> were used to achieve exact results since the application of only one method limits the precision of the results. <b>Principal</b> <b>components</b> <b>analysis</b> cannot be considered {{for the purpose of}} research because information on LULC change will not appear for only one component (Trincsi et al., 2014; Gong et al. 2015).|$|R
25|$|Multilayer kernel {{machines}} (MKM) {{are a way}} {{of learning}} highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel <b>principal</b> <b>component</b> <b>analysis</b> (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of the deep learning architecture.|$|E
25|$|The {{approach}} above {{belongs to}} the model-based approach. Another appearance-based approach recognizes individuals through binary gait silhouette sequences. For example, silhouette sequences of full gait cycles can be treated as 3D tensor samples, and multilinear subspace learning, such as the multilinear <b>principal</b> <b>component</b> <b>analysis,</b> can be employed to learning features for classification.|$|E
25|$|A 2007 {{study by}} Bauchet, which {{utilised}} about 10,000 autosomal DNA SNPs arrived at similar results. <b>Principal</b> <b>component</b> <b>analysis</b> clearly identified four widely dispersed groupings, corresponding to Africa, Europe, Central Asia and South Asia. PC1 separated Africans {{from the other}} populations, PC2 divided Asians from Europeans and Africans, whilst PC3 split Central Asians apart from South Asians.|$|E
40|$|<b>Principal</b> <b>components</b> <b>analysis</b> {{relates to}} the {{eigenvalue}} distribution of Wishart matrices. Given few observations and very many variables this distribution maps to eigenvalue statistics in the Gaussian orthogonal ensemble. <b>Principal</b> <b>components</b> selection can then be based on existing analytical results. <b>Principal</b> <b>components</b> <b>analysis</b> Random matrix theory...|$|R
40|$|This paper applies {{a hidden}} Markov {{model to the}} problem of Attention Deficit Hyperactivity Disorder (ADHD) {{diagnosis}} from resting-state functional Magnetic Resonance Image (fMRI) scans of subjects. The proposed model considers the temporal evolution of fMRI voxel activations in the cortex, cingulate gyrus, and thalamus regions of the brain {{in order to make a}} diagnosis. Four feature dimen- sionality reduction methods are applied to the fMRI scan: voxel means, voxel weighted means, <b>principal</b> <b>components</b> <b>analysis,</b> and kernel <b>principal</b> <b>components</b> <b>analysis.</b> Using <b>principal</b> <b>components</b> <b>analysis</b> and kernel <b>principal</b> <b>components</b> <b>analysis</b> for dimensionality reduction, the proposed algorithm yielded an accu- racy of 63. 01 % and 62. 06 %, respectively, on the ADHD- 200 competition dataset when differentiating between healthy control, ADHD innattentive, and ADHD combined types...|$|R
5000|$|Exploratory factor <b>analysis</b> versus <b>principal</b> <b>components</b> <b>analysis</b> ...|$|R
25|$|The SVD is also applied {{extensively}} to {{the study}} of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to <b>principal</b> <b>component</b> <b>analysis</b> and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.|$|E
25|$|A 2014 {{study by}} Carmi et al. {{published}} by Nature Communications found that Ashkenazi Jewish population originates from mixing between Middle Eastern and European peoples. According to the authors, that mixing likely occurred some 600–800 years ago, followed by rapid growth and genetic isolation (rate per generation 16–53%;). The {{study found that}} all Ashkenazi Jews descent from around 350 individuals, and that the <b>principal</b> <b>component</b> <b>analysis</b> of common variants in the sequenced AJ samples, confirmed previous observations, namely, the proximity of Ashkenazi Jewish cluster to other Jewish, European and Middle Eastern populations".|$|E
25|$|Two {{types of}} tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them {{decomposes}} a tensor into a sum of rank-1 tensors, {{which is called}} a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is {{referred to in the}} literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear <b>principal</b> <b>component</b> <b>analysis</b> in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.|$|E
40|$|PCA in high dimensions. • Sparsity of <b>principal</b> <b>components.</b> • Consistent {{estimation}} and minimax theory. • Feasible algorithms using convex relaxation. <b>Principal</b> <b>Components</b> <b>Analysis</b> • I have iid {{data points}} X 1, [...] .,Xn on p variables. • p may be large, so I {{want to use}} <b>principal</b> <b>components</b> <b>analysis</b> (PCA) for dimension reduction...|$|R
40|$|Abstract—The {{medical data}} {{statistical}} analysis often requires the using of some special techniques, {{because of the}} particularities of these data. The <b>principal</b> <b>components</b> <b>analysis</b> and the data clustering are two statistical methods for data mining very useful in the medical field, the first one as a method to decrease the number of studied parameters, and the second one as a method to analyze the connections between diagnosis and the data about the patient’s condition. In this paper we investigate the implications obtained from a specific data analysis technique: the data clustering preceded by a selection of the most relevant parameters, made using the <b>principal</b> <b>components</b> <b>analysis.</b> Our assumption was that, using the <b>principal</b> <b>components</b> <b>analysis</b> before data clustering- in order to select and to classify only the most relevant parameters – the accuracy of clustering is improved, but the practical results showed the opposite fact: the clustering accuracy decreases, with a percentage approximately equal with the percentage of information loss reported by the <b>principal</b> <b>components</b> <b>analysis.</b> Keywords—Data clustering, medical data, <b>principal</b> <b>components</b> <b>analysis.</b> I...|$|R
5000|$|... #Subtitle level 2: Exploratory factor <b>analysis</b> versus <b>principal</b> <b>components</b> <b>analysis</b> ...|$|R
25|$|Principal {{component}} regression (PCR) is {{used when}} the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using <b>principal</b> <b>component</b> <b>analysis</b> then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|E
25|$|In image processing, {{processed}} {{images of}} faces {{can be seen}} as vectors whose components are the brightnesses of each pixel. The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; {{this is an example of}} <b>principal</b> <b>component</b> <b>analysis.</b> They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.|$|E
25|$|A 2014 {{study by}} Carmi et al. {{published}} by Nature Communications {{found that the}} Ashkenazi Jewish population originates from an even mixture between Middle Eastern and European peoples. According to the authors, that mixing likely occurred some 600–800 years ago, followed by rapid growth and genetic isolation (rate per generation 16–53%;). The study found that all Ashkenazi Jews descent from around 350 individuals, about half of whom were Middle Eastern and half were European, and that all Ashkenazi Jews {{are related to the}} point of being no more than 30th cousins. The <b>principal</b> <b>component</b> <b>analysis</b> of common variants in the sequenced AJ samples, confirmed previous observations, namely, the proximity of Ashkenazi Jewish cluster to other Jewish, European and Middle Eastern populations.|$|E
50|$|This decorrelation {{is related}} to <b>principal</b> <b>components</b> <b>analysis</b> for multivariate data.|$|R
5000|$|... in <b>principal</b> <b>components</b> <b>analysis</b> (PCA) {{when there}} are many {{supplementary}} variables; ...|$|R
40|$|The aim of {{this article}} is the {{approximation}} of the <b>principal</b> <b>components</b> <b>analysis</b> (PCA) of a stationary function, defined on, by the PCA of a stationary series. For this, we use a discretization of the real line. We study the behavior of the approximation when the step of discretization decreases. <b>Principal</b> <b>components</b> <b>analysis</b> Stationary random function Frequency domain...|$|R
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. <b>Principal</b> <b>component</b> <b>analysis</b> of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
500|$|Such a two-good {{world is}} a {{theoretical}} simplification {{because of the difficulty}} of graphical analysis of multiple goods. If we are interested in one good, a composite score of the other goods can be generated using different techniques. [...] Furthermore, the production model can be generalised using higher-dimensional techniques such as <b>Principal</b> <b>Component</b> <b>Analysis</b> (PCA) and others.|$|E
500|$|Specimens of Tatuidris are small, about [...] {{in total}} length, but {{specimens}} can vary greatly in size, with larger specimens being {{twice as large}} as the smaller ones. Size variability within trap catches (possibly same colonies) may be considerable. For example, workers from one collection catch in Nicaragua varied 30% in size. It is still unclear whether intra-colony size variation is due to the presence of morphological worker castes (e.g. minor and major castes) or continuous size variability. <b>Principal</b> <b>component</b> <b>analysis</b> (PCA) revealed that most variability among specimens is related to size, with shape explaining little of total variation.|$|E
5000|$|Distinctiveness of {{descriptors}} {{is measured}} by summing the eigenvalues of the descriptors, obtained by the <b>Principal</b> <b>components</b> <b>analysis</b> of the descriptors normalized by their variance. This corresponds {{to the amount of}} variance captured by different descriptors, therefore, to their distinctiveness. PCA-SIFT (<b>Principal</b> <b>Components</b> <b>Analysis</b> applied to SIFT descriptors), GLOH and SIFT features give the highest values.|$|R
40|$|We {{propose a}} method for spatial <b>principal</b> <b>components</b> <b>analysis</b> that has two {{important}} advantages over the method that Wartenberg (1985) proposed. The first advantage is that, contrary to Wartenberg's method, our method has a clear and exact interpretation: it produces a summary measure (component) that itself has maximum spatial correlation. Second, an easy and intuitive link {{can be made to}} canonical correlation analysis. Our spatial canonical correlation analysis produces summary measures of two datasets (e. g., each measuring a different phenomenon), and these summary measures maximize the spatial correlation between themselves. This provides an alternative weighting scheme as compared to spatial <b>principal</b> <b>components</b> <b>analysis.</b> We provide example applications of the methods and show that our variant of spatial canonical correlation analysis may produce rather different results than spatial <b>principal</b> <b>components</b> <b>analysis</b> using Wartenberg’s method. We also illustrate how spatial canonical correlation analysis may produce different results than spatial <b>principal</b> <b>components</b> <b>analysis.</b> ...|$|R
40|$|In {{this report}} we {{address the problem}} of skin {{fluorescence}} in feature extraction from Raman spectra of skin lesions. We apply a highly automated neural network method for suppressing skin fluorescence from Raman spectrum of skin lesions before dimension reduction with <b>principal</b> <b>components</b> <b>analysis.</b> By applying the background suppression, the effect of outlier spectrum in the <b>principal</b> <b>components</b> <b>analysis</b> was greatly reduced...|$|R
2500|$|Covariation among traits (scatterplots and correlations, <b>principal</b> <b>component</b> <b>analysis)</b> ...|$|E
2500|$|RPCA {{background}} subtraction [...] (See Robust <b>principal</b> <b>component</b> <b>analysis</b> {{for more}} details) ...|$|E
2500|$|Undercomplete {{dictionaries}} {{represent the}} setup {{in which the}} actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like <b>principal</b> <b>component</b> <b>analysis</b> which require atoms [...] to be orthogonal. [...] The choice of these subspaces is crucial for efficient dimensionality reduction, {{but it is not}} trivial. [...] And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.|$|E
40|$|We thank Denis de Crombrugghe for useful {{comments}} and discussions. We propose {{a method for}} spatial <b>principal</b> <b>components</b> <b>analysis</b> that has two important advantages over the method that Wartenberg (1985) proposed. The first advantage is that, contrary to Wartenberg’s method, our method has a clear and exact interpretation: it produces a summary measure (component) that itself has maximum spatial correlation. Second, an easy and intuitive link {{can be made to}} canonical correlation analysis. Our spatial canonical correlation analysis produces summary measures of two datasets (e. g., each measuring a different phenomenon), and these summary measures maximize the spatial correlation between themselves. This provides an alternative weighting scheme as compared to spatial <b>principal</b> <b>components</b> <b>analysis.</b> We provide example applications of the methods and show that our variant of spatial canonical correlation analysis may produce rather different results than spatial <b>principal</b> <b>components</b> <b>analysis</b> using Wartenberg’s method. We also illustrate how spatial canonical correlation analysis may produce different results than spatial <b>principal</b> <b>components</b> <b>analysis...</b>|$|R
5000|$|The {{differences}} between <b>principal</b> <b>components</b> <b>analysis</b> and factor analysis are further illustrated by Suhr (2009): ...|$|R
40|$|Abstract-Based on {{the theory}} of wavelet <b>analysis</b> and <b>principal</b> <b>component</b> analysis,Multi-scale PCA is {{introduced}} which combines the ability of PCA to decorrelate the variables by extracting a linear relationship, with that of wavelet analysis to extract deterministic features and approximately decorrelate autocorrelated measurements to improve the performance of PCA whose modeling {{is limited to a}} single scale. It is applied to the fault monitor and diagnose of Floating Production Storage and Off-loading System. The result show: the fault diagnose method based on multi-scale <b>principal</b> <b>components</b> <b>analysis</b> can realized FPSO earlier period fault monitor and diagnose accurately, and the capability of multi-scale <b>principal</b> <b>components</b> <b>analysis</b> fault diagnosis is better than the <b>principal</b> <b>components</b> <b>analysis</b> for the small disturbance. Key words-MSPCA; PCA; Fault diagnose;FPSO I...|$|R
