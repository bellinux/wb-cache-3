20|11|Public
50|$|The {{on-screen}} score graphic {{during this}} period covered the entire top of the screen, unlike the Braves TBS Baseball graphic, which only took up the left half of the top. The look is almost identical to that used during Fox's baseball coverage, except that the illustration of the basepaths is near {{the left side of}} the screen instead of flushed on the right. The batting order starting lineup used beginning in 2008 resembles that of a cellphone. There is also a <b>pitch</b> <b>tracker</b> that can only be seen on the network's high-definition feed.|$|E
40|$|This paper {{describes}} an algorithm, which performs monophonic music transcription. A <b>pitch</b> <b>tracker</b> calculates {{the fundamental frequency}} of the signal from the autocorrelation function. A continuity-restoration block takes the extracted pitch and determines the score corresponding to the original performance. The signal envelope analysis completes the transcription system, calculating attack-sustain-decay-release times, which improves the synthesis process. Attention is also paid to the extraction of timbre and wavetable synthesis. 1...|$|E
40|$|Pitch {{tracking}} is {{an important}} component of Automatic Mu-sic Transcription (AMT) systems and Query by Humming (QBH) based melody retrieval systems. While the detec-tion of pitch (or fundamental frequency) is accomplished relatively easily for an individual voice, the presence of per-cussive accompaniment such as Tabla can greatly perturb the <b>pitch</b> <b>tracker.</b> With a view to build pitch detectors that are robust to percussive background, we present a study of the acoustic characteristics of common tabla strokes. An experimental investigation of the errors in autocorrelation function-based pitch detection is presented for a singing voice accompanied by tabla. 1...|$|E
40|$|This paper {{focuses on}} the problem of pitch {{tracking}} in noisy conditions. A method using harmonic information in the residual signal is presented. The proposed criterion is used both for pitch estimation, as well as for determining the voicing segments of speech. In the experiments, the method is compared to six state-of-the-art <b>pitch</b> <b>trackers</b> on the Keele and CSTR databases. The proposed technique is shown to be particularly robust to additive noise, leading to a significant improvement in adverse conditions. Index Terms: fundamental frequency, pitch tracking, pitch estimation, voicing decision...|$|R
40|$|We {{present results}} of {{applying}} <b>pitch</b> <b>trackers</b> to samples of South Indian classical (Carnatic) music. In particular, we investigate the various musical notes used and their intonation. We try different pitch tracking methods and observe their performance in Carnatic music analysis. Examining our data, we find only 12 distinct intervals per octave among the notes that are played with constant pitch. However, there are pitch inflexions used sometimes {{that are not}} mere ornamentations - they {{are essential to the}} correct rendition of certain notes. Though these inflexions can be viewed as different versions of a particular note, they are certainly not equivalent to constant-pitch intervals like Just Intonation intervals, semitones or quartertones...|$|R
30|$|The {{other method}} was another {{recently}} proposed VAD [27] (called Drugman’s method from here on). Four voicing measures of [26], MFCCs, and two <b>pitch</b> <b>trackers</b> {{were used as}} the features, and a neural network with a single hidden layer of 32 neurons was used to obtain posterior speech probabilities of frames. The reason for choosing this method for comparison was that it had achieved superior results against the four state-of-the-art VAD methods, as reported in [27]. Also, its memory and computational demands are low, and it includes smoothing processes {{before and after the}} neural network phase to remove spurious values and isolated misclassifications, respectively. The method is implemented by using the code obtained from the author’s website [43]. For the spectral subtraction, frames decided as non-speech were used to estimate average noise energy.|$|R
40|$|Many pitch {{trackers}} {{based on}} dynamic programming require meticulous design of local cost and transition cost functions. The forms of these functions are often empirically determined and their parameters are tuned accordingly. Parameter tuning usually requires great effort without {{a guarantee of}} optimal performance. This work presents a graphical model framework to automatically optimize pitch tracking parameters in the maximum likelihood sense. Therein, probabilistic dependencies between pitch, pitch transition and acoustical observations are expressed using the language of graphical models, and probabilistic inference is accomplished using the Graphical Model Toolkit (GMTK). Experiments show that this framework not only expedites {{the design of a}} <b>pitch</b> <b>tracker,</b> but also yields remarkably good performance for both pitch estimation and voicing decision. 1...|$|E
40|$|In {{this study}} we {{examined}} prosodic characteristics of a word used in several distinct senses in a task-oriented corpus of spontaneous speech. We compared the pitch characteristics of the word “right ” used in three different senses: as an acknowledgment, as a direction, and as an affirmative answer to a question. Significant differences in intonation for different classes of usage were found, although the differences are not reliable enough to allow systems to use prosody alone to distinguish between usages. These results suggest that pitch change as reported by a <b>pitch</b> <b>tracker</b> {{could serve as a}} confirming cue when analyzing ambiguous speech recognizer output, or could serve as input to a probabilistic parser to aid in disambiguating senses of homonyms. 1...|$|E
40|$|We {{present a}} noise-robust {{algorithm}} for estimating the active level of speech, {{which is the}} average speech power during intervals of speech activity. The proposed algorithm uses the clean speech phase to remove the quadrature noise component from the short-time power spectrum of the noisy speech, as well as SNR-dependent techniques to improve the estimation. The pitch of voiced speech frames is determined using a noise-robust <b>pitch</b> <b>tracker</b> and the speech level is estimated from {{the energy of the}} pitch harmonics using the harmonic summation principle. At low noise levels, the resultant active speech level estimate is combined with that from the standardized ITU-T P. 56 algorithm to give a final composite estimate. The algorithm has been evaluated using a range of noise signals and gives consistently lower errors than previous methods and than the ITU-T P. 56 algorithm, which is accurate for SNR levels of above 15 dB...|$|E
40|$|In {{this paper}} a model-based {{approach}} for restoring a continuous fundamental frequency (F 0) contour from the noisy output of an F 0 extractor is investigated. In {{contrast to the}} conventional <b>pitch</b> <b>trackers</b> based on numerical curve-fitting, the proposed method employs a quantitative pitch generation model, which is often used for synthesizing F 0 contour from prosodic event commands for estimating continuous F 0 pattern. An inverse filtering technique is introduced for obtaining the initial candidates of the prosodic commands. In order to find the optimal command sequence from the commands efficiently, a beam-search algorithm and an N-best technique are employed. Preliminary experiments for a male speaker of the ATR B-set database showed promising results both in quality of the restored pattern and estimation of the prosodic events. 1...|$|R
40|$|Speech pitch {{tracking}} {{is one of}} {{the elementary}} tasks of the Computational Auditory Scene Analysis (CASA). While a human can easily listen to the voiced pitch in highly noisy recordings, the performance of automatic speech pitch tracking degrades in unknown noisy audio conditions. Traditional <b>pitch</b> <b>trackers</b> use either autocorrelation or the Fourier transform to calculate periodicity, which works well for clean recordings. For noisy recordings, however, the accuracy of these <b>pitch</b> <b>trackers</b> degrades in general. For example, the information in parts of the frequency spectrum may be lost due to analog radio band transmission and/or contain additive noise of various kinds. Instead of explicitly using the most obvious features of autocorrelation, we propose a trained classier-based approach, which we call Subband Autocorrelation Classification (SAcC). A multi-layer perceptron (MLP) classier is trained on the principal components of the autocorrelations of subbands from an auditory filterbank. The output of the MLP classifier is temporally smoothed to produce the pitch track by finding the Viterbi path of a Hidden Markov Model (HMM). Training on various types of noisy speech recordings leads to a great increase in performance over state-of-the-art algorithms, according to both the traditional Gross Pitch Error (GPE) measure, and a proposed novel Pitch Tracking Error (PTE) which more fully reflects the accuracy of both pitch estimation/extraction and voicing detection in a single measure. To verify the generalization and specificity of SAcC, we test SAcC on a real world problem that has a large-scale noisy speech corpus. The data is from the DARPA Robust Automatic Transcription of Speech (RATS) program. The experiments on the performance evaluation of SAcC pitch tracking confirm the generalization power of SAcC across various unknown noise conditions and distinct speech corpora. We also report the use of SAcC output adds a significant improvement to a Speaker Identification (SID) system for RATS as well, suggesting the potential contribution of SAcC pitch tracking in the higher-level tasks...|$|R
40|$|The problem {{addressed}} {{here is that}} {{of detecting}} irregular phonation during conversational speech. While most published work tackles this problem only {{by focusing on the}} voiced regions of speech, we focus on detecting irregular phonation without assuming prior knowledge of voiced regions. In addition, we improve the pitch estimation accuracy of a current pitch tracking algorithm in regions of irregular phonation, where most <b>pitch</b> <b>trackers</b> fail to perform well. The algorithm has been tested on the TIMIT and NIST 98 databases. The detection rate for the TIMIT database is 91. 8 % (17. 42 % false detections). The detection rate for the NIST 98 database is 91. 5 % (12. 8 % false detections). The pitch detection accuracy increased from 95. 4 % to 98. 3 % for the TIMIT database, and from 94. 8 % to 97. 4 % for the NIST 98 database...|$|R
40|$|Introduction Digital {{libraries}} {{until now}} {{could hardly be}} described as popular: {{they tend to be}} based on esoteric, scholarly sources close to the interests of digital library researchers themselves. We are developing a digital library containing the quintessence of popular culture: music. The principal mode of searching this library will be by sung query: the system should be able to answer the kinds of queries that shop assistants in music stores deal with every day, where a customer can sing a tune, but can't remember the title or artist. The operation of our musical digital library is sketched in Figure 1. At the time of collection creation, MIDI files are gathered from the internet, and indexed based on their note sequences. At retrieval time, a user's sung query is transformed from a waveform to a sequence of pitch-duration events by a <b>pitch</b> <b>tracker.</b> We are using an off-the-shelf pitch tracker: pitch tracking {{is beyond the scope of}} the current project. Po...|$|E
40|$|Pitch {{tracking}} of voice in tabla background by the two-way mismatch method Obtaining the detailed pitch contour of the melody from audio recordings of Indian classical music is important both from a pedagogical {{as well as}} musicological perspective. In this work, the problem of pitch {{tracking of}} the singing voice in percussive accompaniment is considered. While the detection of pitch (or fundamental frequency) is accomplished relatively easily for an individual voice, the presence of percussive accompaniment such as tabla can greatly perturb the <b>pitch</b> <b>tracker.</b> The acoustic signal characteristics of the percussive accompaniment that pose specific challenges to conventional pitch detection algorithms (PDAs) are discussed. An experimental investigation {{of the performance of}} a frequency-domain PDA, the two-way mismatch method, is carried out for a variety of simulated and real music signals of singing voice in tabla accompaniment. A postprocessing method based on dynamic-programming based smoothing is proposed and shown to significantly improve the accuracy of the estimated pitch contour...|$|E
40|$|Audio sources {{frequently}} concentrate much {{of their}} energy into a relatively small proportion of the available time-frequency cells in a short-time Fourier transform (STFT). This sparsity {{makes it possible to}} separate sources, to some degree, simply by selecting STFT cells dominated by the desired source, setting all others to zero (or to an estimate of the obscured target value), and inverting the STFT to a waveform. The problem of source separation then becomes identifying the cells containing good target information. We treat this as a classification problem, and train a Relevance Vector Machine (a probabilistic relative of the Support Vector Machine) to perform this task. We compare the performance of this classifier both against SVMs (it has similar accuracy but is much more efficient), and against a traditional Computational Auditory Scene Analysis (CASA) technique based on a noise-robust <b>pitch</b> <b>tracker,</b> which the RVM outperforms significantly. Differences between the RVM- and pitch-tracker-based mask estimation suggest benefits to be obtained by combining both. 1...|$|E
40|$|A {{small set}} of {{extensions}} to the PureData architecture {{and to its}} external API is proposed, which will facilitate the creation of audio feature detectors (<b>pitch</b> followers, beat <b>trackers,</b> onset detectors, etc.) as Pd patches. An experimental implementation is discussed, and two application examples are demonstrated. 1...|$|R
50|$|TUBS {{notation}} {{has been}} adapted by several people; {{the most common}} adaptations use different symbols in the boxes to represent different sounds, for example different ways of hitting a drum or even different musical <b>pitches.</b> In fact, <b>tracker</b> notation is essentially TUBS rotated by 90 degrees.|$|R
40|$|This work {{addresses}} the detection & characterization of irregular phonation in spontaneous speech. While published work tackles this problem as a two-hypothesis problem only in regions of speech with phonation, this work focuses on distinguishing aperiodicity due to frication from {{that due to}} irregular voicing. This work also deals with correction of a current pitch tracking algorithm in regions of irregular phonation, where most <b>pitch</b> <b>trackers</b> fail to perform well. Relying on the detection of regions of irregular phonation, an acoustic parameter is developed in order to characterize these regions for speaker identification applications. The detection performance of the algorithm on a clean speech corpus (TIMIT) is seen to be 91. 8 %, with the percentage of false detections being 17. 42 %. On telephone speech corpus (NIST 98) database, the detection performance is 89. 2 %, with the percentage of false detections being 12. 8 %. The pitch detection accuracy increased from 95. 4 % to 98. 3 % for TIMIT, and from 94. 8 % to 97. 4 % for NIST 98 databases. The creakiness parameter was added {{to a set of}} seven acoustic parameters for speaker identification on the NIST 98 database, and the performance was found to be enhanced by 1. 5 % for female speakers and 0. 4 % for male speakers for a population of 250 speakers...|$|R
40|$|Baldr is the UAV Lab’s newest UltraStick 120 {{airframe}} {{that will}} be used for aircraft reliability research. For this first flight, only the baseline inner-loop control law was tested. The innerloop control law consists of a <b>pitch</b> <b>tracker</b> (tracking theta), a roll tracker (tracking phi), and a yaw damper. The gains used were {{the same as that of}} Faser. The controller testing was done by commanding straight and level segments, and pitch & roll doublets. First, Baldr was commanded to fly straight & level at 23 m/s, at a pitch attitude of 5 deg, and a throttle of 65 %. This was repeated thrice. Next, a roll doublet of +/- 20 deg was commanded (for 10 s) at a pitch attitude of 5 deg and a throttle of 65 %. This was repeated thrice. Finally, a pitch doublet of +/- 5 deg was commanded (for 6 s) at a throttle of 65 %. This was repeated thrice. All flight ops were smooth...|$|E
40|$|Back-channel feedback, {{responses}} such as uh-uh from a listener, is {{a pervasive}} feature of conversation. It {{has long been}} thought that the production of back-channel feedback depends {{to a large extent}} on the actions of the other conversation partner, not just on the volition of the one who produces them. In particular, prosodic cues from the speaker have long been thought to play a role, but have so far eluded identification. We have earlier suggested that an important prosodic cue involved, in both English and Japanese, is a region of low pitch late in an utterance (Ward, 1996). This paper presents evidence for this claim, surveys other factors which elicit or inhibit back-channel responses, discusses issues in the definition of back-channel feedback, and mentions a few related phenomena and theoretical issues. *We thank Keikichi Hirose for the <b>pitch</b> <b>tracker,</b> Yuichiro Fukuchi for efforts to disprove our hypothesis, Daniel Jurafsky, Kikuo Maekawa, Elizabeth Shriberg, Maki Sugi [...] ...|$|E
40|$|In this paper, {{we propose}} an F 0 Frame Error (FFE) metric which {{combines}} Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate {{the performance of}} fundamental fre-quency (F 0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we intro-duce a model-based Unvoiced/Voiced (U/V) classification frontend {{which can be used}} by any F 0 tracking algorithm. In the U/V classi-fication, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F 0 tracking. Exper-iments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the <b>pitch</b> <b>tracker</b> TEMPO [1] in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in er-ror rates for a number of F 0 tracking algorithms, especially in babble noise...|$|E
30|$|The methods {{referred}} in {{the previous}} paragraph are aiming to track the noise spectrum even under the speech activity. They are fast adapting to {{the changes in the}} noise level but may degrade the information during the speech regions. Since both the noise and the speech are present at the same time, exactly separating them is not possible. On the other hand, separating the speech-dominant and the noise-dominant frames can also be beneficial. It is also possible to estimate an average of the noise from the noise-dominant frames, or the noise parameters can be updated between the speech regions. Various voice activity detection algorithms have been developed for this purpose. In [22], long-term signal probabilities are used as the discrimination criteria and the decision is assigned to the frame {{in the middle of a}} long window. Similarly, in [23], long-term signal variability is used but the decision is assigned to the long window. Energy-based detection and Gaussian mixture model (GMM)-based statistical model are combined for the VAD in [24]. Modeling the feature distribution with a bi-Gaussian model can be also used as a VAD, where the Gaussian with the lower mean corresponds to noisy frames [25]. Four different voicing measures and a spectral feature are concatenated and mapped to a one-dimensional space via principal component analysis in [26], and the resulting feature distribution is bi-Gaussian. In addition to the four voicing measures used in [26], MFCCs and two <b>pitch</b> <b>trackers</b> are used as features for the VAD in [27]. A comparison of some standard VAD methods for speaker recognition can be found in [28], where the bi-Gaussian modeling-based VAD is reported to be the best performing one. Similar to the VAD, vowel-like regions are used in [29] and improved in [30] by including the non-vowel-like regions. Missing data approach is also investigated in several studies [31 – 34], where a binary time-frequency mask is constructed for the noisy spectrum to indicate reliable and unreliable features. The unreliable features are then reconstructed, or marginalized (ignored in score computation).|$|R
40|$|Human head {{gestures}} {{convey a}} rich message, containing information deliver for peoples as a communication tool. Nodding, shacking {{are commonly used}} gestures as non-verbal signals to communicate their intent and emotions. However, the majority of head gestures classification systems focused on head nodding and shaking detection. while they ignored other head gestures which have more expressive emotional signals like rest(up and down), turn, tilt, and tilting. In this paper we developed a new model to classify all head gestures (rest, turn, tilt, node, shake, and tilting) from complex head motions. The model methodology based on distinguishing basic head movements (rest, turn, and tilt) and combined movements (nodding, shaking, and tilting). The purpose of this system is to detect and label combined and basic head movements in dynamic video. In addition, this phase of this study looking at developing an affective machine uses head movements to extract complex affective states (this work is underway). The system used 3 D head rotation angles to classify relevant head gestures in-plan and out-plan of view during user interaction with computer. This system used an open source tracker to detect and track head movements. The Three angels that obtained from the <b>tracker</b> (<b>pitch,</b> yaw, and roll), were analyzed and packed into sequences of observation symbols or cues. Observations formed inputs to an all-vs-all discrete Hidden Markov Model (HMM) classifier. Three classifiers were used for each angle. The classifiers are trained on Boston University dataset, and tested on available mind reading data. The system evaluate on video streams in real time by webcam. The system is fully automatic without incurring any cost of technical methods and doesnגt require any sensitive tools...|$|R
40|$|Two {{systems are}} {{reviewed}} than perform automatic music transcription. The first perform monophonic transcription using an autocorrelation <b>pitch</b> <b>tracker.</b> The algorithm {{takes advantage of}} some heuristic parameters related to the similarity between image and sound in the collector. The detection is correct between notes B 1 to E 6 and further timbre analysis will provide the necessary parameters to reproduce a similar copy of the original sound. The second system is able to analyse simple polyphonic tracks. It is composed of a blackboard system, receiving its input from a segmentation routine {{in the form of}} an averaged STFT matrix. The blackboard contents an hypotheses database, an scheduler and knowledge sources, one of which is a neural network chord recogniser with the ability to reconfigure the operation of the system, allowing it to output more than one note hypothesis at the time. Some examples are provided to illustrate the performance and the weaknesses of the current implementation. Next steps for further development are defined...|$|E
40|$|Statistics of pitch have {{recently}} been used in speaker recognition systems with good results. The success of such systems depends on robust and accurate computation of pitch statistics {{in the presence of}} pitch tracking errors. In this work, we develop a statistical model of pitch that allows unbiased estimation of pitch statistics from pitch tracks which are subject to doubling and/or halving. We first argue by a simple correlation model and empirically demonstrate by QQ plots that "clean" pitch is distributed with a lognormal distribution rather than the often assumed normal distribution. Second, we present a probabilistic model for estimated pitch via a <b>pitch</b> <b>tracker</b> in the presence of doubling/halving, which leads to a mixture of three lognormal distributions with tied means and variances for a total of four free parameters. We use the obtained pitch statistics as features in speaker verification on the March 1996 NIST Speaker Recognition Evaluation data (subset of Switchboard) and [...] ...|$|E
40|$|An {{accurate}} <b>pitch</b> <b>tracker</b> {{has many}} useful applications, whether for creating interactive electroacoustic compositions, music transcription, ethnomusicological research and numerous others. Designed for the Csound sound synthesis and signal processing language, PVSPITCH is an opcode {{that can be}} utilised for such purposes. The opcode performs a mathemathical analysis upon Csound 2 ̆ 7 s phase vocoder data streams and from this examination, ascertains what it determines to be the signal 2 ̆ 7 s pitch. The algorithm handles well many types of signals, including those missing various harmonics, even a fundamental, and also signals with inharmonic partials. The only signal restriction are that it be single voiced, strongly pitched and slowly changing. This paper introduces the opcode and illustrates PVSPITCH 2 ̆ 7 s pitch determination algorithm. Schouten 2 ̆ 7 s pitch determination hypothesis {{and the concept of}} tonal fusion are briefly discussed as the background to the opcode 2 ̆ 7 s development. Three case studies are also explored to demonstrate the accuracy of the algorithm...|$|E
40|$|Acoustic {{instruments}} provide performers {{with much}} greater degrees of control {{than the typical}} electronic controller. If the qualities of an acoustic instrument's sound-pitch, intensity, and timbre-could be measured in real-time, the subtle nuances in a virtuoso's musical gestures {{could be used to}} create an electronically enhanced musical environment. In 1991, the Hyperinstrument Research Group completed an ambitious project to use a cello as a continuous controller for cellist Yo-Yo Ma's performance of Tod Machover's Begin Again Again [...] The original hyperinstrument that was created for the project lacked the means of adequately measuring the pitch and timbre of the acoustic cello. Subsequent research in analysis techniques that builds {{on the development of the}} cello hyperinstrument is detailed in this thesis. With the goal of developing pitch and timbre trackers for the cello, the dynamics of string instruments were studied as well as methods to characterize these dynamics. As a result, a new way of estimating pitch in pseudo-phase space was developed. The PPS <b>Pitch</b> <b>Tracker</b> analyzes the attractor that results from mapping the dynamics of the cello to pseudo-phase space, and determines the fundamental frequency of the system's oscillation. Additionally, a sligh...|$|E
40|$|We have {{implemented}} a real time front end for detecting voiced speech and estimating its fundamental frequency. The front end performs the signal processing for voice-driven agents that {{attend to the}} pitch contours of human speech and provide continuous audiovisual feedback. The algorithm we use for pitch tracking has several distinguishing features: it makes no use of FFTs or autocorrelation at the pitch period; it updates the pitch incrementally on a sample-by-sample basis; it avoids peak picking and does not require interpolation in time or frequency to obtain high resolution estimates; and it works reliably over a four octave range, in real time, {{without the need for}} postprocessing to produce smooth contours. The algorithm is based on two simple ideas in neural computation: the introduction of a purposeful nonlinearity, and the error signal of a least squares fit. The <b>pitch</b> <b>tracker</b> is used in two real time multimedia applications: a voice-to-MIDI player that synthesizes electronic music from vocalized melodies, and an audiovisual Karaoke machine with multimodal feedback. Both applications run on a laptop and display the user’s pitch scrolling across the screen as he or she sings into the computer. ...|$|E
40|$|Although the {{complexity}} of prosody is widely recognised, {{there is a lack}} of widely-accepted descriptive standards for prosodic phenomena. This situation has become particularly noticeable with the development of increasingly capable text-to-speech (TTS) systems. Such systems require detailed prosodic models to sound natural. For the languages of Southern Africa, the deficiencies in our modelling capabilities are acute. Little work of a quantitative nature has been published for the languages of the Nguni family (such as isiZulu and isiXhosa), and there are significant contradictions and imprecisions in the literature on this topic. We have therefore embarked on a programme aimed at understanding the relationship between linguistic and physical variables of a prosodic nature in this family of languages. We then use the information/knowledge gathered to build intonation models for isiZulu and isiXhosa as representatives of the Nguni languages. Firstly, we need to extract physical measurements from the voice recordings of the Nguni family of languages. A number of pitch tracking algorithms have been developed; however, to our knowledge, these algorithms have not been evaluated formally on a Nguni language. In order to decide on an appropriate algorithm for further analysis, evaluations have been performed on two stateof- the-art algorithms namely the Praat <b>pitch</b> <b>tracker</b> and Yin (developed by Alain de Cheveingn´e). Praat’s <b>pitch</b> <b>tracker</b> algorithm performs somewhat better than Yin in terms of gross and fine errors and we use this algorithm for the rest of our analysis. For South African languages the task of building an intonation model is complicated by the lack of intonation resources available. We describe the methodology used for developing a generalpurpose intonation corpus and the various methods implemented to extract relevant features such as fundamental frequency, intensity and duration from the spoken utterances of these languages. In order to understand how the ‘expected’ intonation relates to the actual measured characteristics extracted, we developed two different statistical approaches to build intonation models for isiZulu and isiXhosa. The first is based on straightforward statistical techniques and the second uses a classifier. Both intonation models built produce fairly good accuracy for our isiZulu and isiXhosa sets of data. The neural network classifier used produces slightly better results for both sets of data than the statistical method. The classification model is also more robust and can easily learn from the training data. We show that it is possible to build fairly good intonation models for these languages using different approaches, and that intensity and fundamental frequency are comparable in predictive value for the ascribed tone. Dissertation (MSc (Computer Science)) [...] University of Pretoria, 2007. Computer ScienceMScunrestricte...|$|E
40|$|Multitrack {{recordings}} {{of a mixed}} adult choir with 23 singers were collected in order to investigate the influence of varied virtual room acoustical conditions on a choir’s performance with regard to intonation, tempo, and timing precision. Headset microphones were used to record each chorister separately while the collected sound of all singers was presented via headphones exerting acoustic simulations of rooms with different acoustical parameters, for example, different reverberation times of 1. 87 and 5. 91 s {{as well as a}} dry condition (without reverberation added) according to 3 singing conditions. The choir was asked to sing “Locus Iste” by Anton Bruckner (1824 – 1896). Objective measures were obtained from single audio track analyses using the monophonic <b>pitch</b> <b>tracker</b> pYIN plugin for Sonic Visualiser. These revealed that intonation was barely affected by simulated room acoustics, whereas tempo was notably slower and timing precision declined in the condition where participants sang in a comparatively reverberant virtual room. Subjective judgments gathered by a questionnaire inquiring on the singers’ experiences showed a clear preference for singing in a virtual room with a reverberation time of 1. 87 s, while the dry acoustical condition was felt to be the best to sing in time. The significance of these results and their relationships to other musical and acoustical parameters are discussed...|$|E
40|$|The {{ability to}} {{automatically}} analyze bird vocalizations would provide major assistance to zoologists in their behavioral and ecological studies. Revisions of taxonomy {{need to be}} made in cases where a population of species has evolved or diversified enough to be treated as a new unique species. Bird vocalizations are an important facet of the review process as song has a major influence in mate choice, hence why zoologists tend to study distinguishable songs when analyzing populations. This thesis presents a method to quantify differences in bird vocalizations, inspired by dialect difference assessment from speech processing. The use of different codebooks is introduced. Reference to how easily pairs can be separated using Gaussian Mixture Model (GMM) classifiers with Mel-frequency Cepstral Coefficients (MFCCs) is also presented. As endangered populations will inevitably have small datasets, machine learning approaches may be hard to train. A forensics inspired pitch contour difference measure works with approximately 200 samples per population. An improved <b>pitch</b> <b>tracker</b> for bird vocalizations, called YIN-bird, is also introduced. Carefully tuned parameters improve pitch tracking with YIN (a commonly used pitch tracking tool in the speech community), but optimal parameters can change quickly even within one song. YIN-bird, a modified version of YIN which exploits spectrogram properties to automatically set a minimum Fundamental Frequency (F 0) parameter for YIN, is presented. Gross pitch errors on whistles and trills are reduced by up to 4 % on a ground truth dataset of synthetic bird song with known pitch which is an improvement. This dataset was evaluated by expert listeners and described as ?sounding like original & can hardly tell it is synthetic?. A qualitative analysis of complex bird vocalizations is also presented. Work then focuses on techniques to automatically extract acoustic features commonly used by zoologists. Signal processing techniques are employed to automate the extraction of the acoustic features: maximum, minimum and peak frequency, and bandwidth. YIN-bird and sine-tracking, a feature extraction method successfully applied to bird classification previously, are the automatic methods employed. The performance of automatic methods is compared to the manual method currently used by zoologists. Results are compared to a major study on species delimitation in the zoology domain. Both methods are well suited to this task, and demonstrate the strong potential to begin to automate the task of acoustic comparison of bird species. The thesis then addresses the automatic segmentation of bird recordings into target and non-target regions using a speaker diarization approach. Standard approaches to automatic bird species identification are inspired by speaker verification methods, and tend to use all available bird vocalizations in a sample. Zoologists however will first tend to isolate important parts of a vocalization, discarding less discerning elements. Thus the automation of such segmentation work is highly desirable. The first stage is to segment target vocalizations from field recordings which is performed automatically in this thesis. Birds are typically recorded in their natural environment with many competing signals present. A speaker diarization approach to segmenting such recordings into target/non-target events is presented. This diarization approach uses Bayesian Information Criterion (BIC) Hierarchical Agglomerative Clustering (HAC) from the LIUM toolkit. MFCCs are used as features. The classification error is reduced from 19. 20 % to 15. 01 % when comparing the proposed system to a HMM-based segmentation as a baseline. A discussion of system performance and conditions which influence false alarms is given...|$|E

