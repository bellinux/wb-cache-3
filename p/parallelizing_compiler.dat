249|347|Public
25|$|Another {{application}} of these transformations is in compiler optimizations of nested-loop code, and in <b>parallelizing</b> <b>compiler</b> techniques.|$|E
50|$|A cyclic multi-threading <b>parallelizing</b> <b>compiler</b> {{tries to}} split up a loop so that each {{iteration}} can be executed on a separate processor concurrently.|$|E
50|$|A pipelined multi-threading <b>parallelizing</b> <b>compiler</b> {{tries to}} break up the {{sequence}} of operations inside a loop into a series of code blocks,such that each code block can be executed on separate processors concurrently.|$|E
5000|$|IEEE Fellow, for {{contributions}} to optimizing and <b>parallelizing</b> <b>compilers.</b> 2005 ...|$|R
40|$|The {{recognition}} and exploitation of parallelism {{is a difficult}} problem for restructuring compilers. We present a method for evaluating the effectiveness of <b>parallelizing</b> <b>compilers</b> {{in general and of}} specific compiler techniques. We also report two groups of measurements that are the results of using this technique. One evaluates a commercially available parallelizer, KAP/Concurrent, and the other compares three different dependence analysis strategies. 1 INTRODUCTION In this paper we present a methodology to evaluate the overall effectiveness of <b>parallelizing</b> <b>compilers</b> [1] and of individual parallelization techniques. Evaluating the effectiveness of <b>parallelizing</b> <b>compilers</b> is necessary not only to help in the search for more effective translation algorithms, but also to help determine the influence of the translation techniques on the performance of a particular parallel or vector computer. Two evaluation strategies have been used in the past to evaluate <b>parallelizing</b> <b>compilers.</b> The fi [...] ...|$|R
40|$|Although {{much work}} has been done on <b>parallelizing</b> <b>compilers</b> for cache {{coherent}} shared memory multiprocessors and message-passing multiprocessors, there is relatively little research on <b>parallelizing</b> <b>compilers</b> for noncache coherent multiprocessors with global address space. In this paper, we present a preliminary study on automatic parallelization for the Cray T 3 D, a commercial scalable machine with a global memory space and noncoherent caches. ...|$|R
50|$|A pipelined multi-threading <b>parallelizing</b> <b>compiler</b> could assign each {{of these}} 6 {{operations}} to a different processor, perhaps arranged in a systolic array,inserting the appropriate code to forward the output of one processor to the next processor.|$|E
50|$|However, {{dependencies}} among statements or instructions may hinder parallelism â€” parallel {{execution of}} multiple instructions, either by a <b>parallelizing</b> <b>compiler</b> or by a processor exploiting instruction-level parallelism. Recklessly executing multiple instructions without considering related dependences may cause danger of getting wrong results, namely hazards.|$|E
50|$|Parallel {{programs}} {{can be divided into}} two general categories: explicitly and implicitly parallel. Using parallel language constructs defined for process creation, communication and synchronization make an application explicitly parallel. Using a tool or <b>parallelizing</b> <b>compiler</b> to convert a serial program into a parallel one, makes it implicitly parallel. Both categories are equally bug-prone.|$|E
40|$|Traditional <b>parallelizing</b> <b>compilers</b> are {{designed}} to generate paral-lel programs that produce identical outputs as the original sequen-tial program. The difficulty of performing the program analysis re-quired to satisfy this goal and the restricted space of possible target parallel programs have both posed significant obstacles to the de-velopment of effective <b>parallelizing</b> <b>compilers.</b> The QuickStep compiler is instead designed to generate paral-lel programs that satisfy statistical accuracy guarantees. The free-dom to generate parallel programs whose output may differ (within statistical accuracy bounds) from {{the output of the}} sequential pro-gram enables a dramatic simplification of the compiler and a signif-icant expansion in the range of parallel programs that it can legally generate. QuickStep exploits this flexibility to take a fundamen-tally different approach from traditional <b>parallelizing</b> <b>compilers.</b> I...|$|R
50|$|The OPS5 forward {{chaining}} process makes it extremely parallelizeable during the matching phase, and several automatic <b>parallelizing</b> <b>compilers</b> were created.|$|R
5000|$|In 2012, Dr. Rauchwerger {{was named}} an IEEE Fellow [...] "For {{contributions}} to thread-level speculation, <b>parallelizing</b> <b>compilers,</b> and parallel libraries" ...|$|R
5000|$|A source-to-source {{compiler}} {{is a type}} of compiler {{that takes}} a high-level language as its input and outputs a high-level language. For example, an automatic <b>parallelizing</b> <b>compiler</b> will frequently take in a high-level language program as an input and then transform the code and annotate it with parallel code annotations (e.g. OpenMP) or language constructs (e.g. Fortran's [...] statements).|$|E
5000|$|Traditional compilers used in parallelization {{were able}} to perform {{privatization}} on scalar elements only. In order to exploit parallelism that occurs across loops within a parallel program (loop-level parallelism), the need rose for compilers {{that are able to}} perform array variable privatization as well. Hence, most of today's compilers are capable of performing array privatization with more features and functions to enhance the performance of the parallel program in general. An example of such compiler is the Polaris <b>parallelizing</b> <b>compiler</b> ...|$|E
5000|$|A source-to-source {{compiler}}, transcompiler or transpiler {{is a type}} of compiler {{that takes}} the source code of a program written in one programming language as its input and produces the equivalent source code in another programming language. A source-to-source compiler translates between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language. For example, a source-to-source compiler may perform a translation of a program from Pascal to C. An automatic <b>parallelizing</b> <b>compiler</b> will frequently take in a high level language program as an input and then transform the code and annotate it with parallel code annotations (e.g., OpenMP) or language constructs (e.g. Fortran's [...] statements).|$|E
40|$|Divide {{and conquer}} {{algorithms}} {{are a good}} match for modern parallel machines: {{they tend to have}} large amounts of inherent parallelism and they work well with caches and deep memory hierarchies. But these algorithms pose challenging problems for <b>parallelizing</b> <b>compilers.</b> They are usually coded as recursive procedures and often use pointers into dynamically allocated memory blocks and pointer arithmetic. All of these features are incompatible with the analysis algorithms in traditional <b>parallelizing</b> <b>compilers.</b> This pape...|$|R
40|$|In {{this paper}} {{we present a}} {{generalization}} of the framework of unimodular loop transformations for <b>parallelizing</b> <b>compilers,</b> called multitransformations. Multi-transformations consist of applying a different unimodular transformation to the iteration space of each statement in the loop body, and include also alignments. Two key aspects are considered in this paper: the generation of efficient code that traverses the different transformed iteration spaces and the test to decide {{the legality of the}} multi-transformation. Some examples are used in the paper that show the usefulness of multi-transformations. In <b>parallelizing</b> <b>compilers</b> for shared-memory they allow an easy exploitation of parallelism; for distributed-memory multiprocessors they allow the generation of code that follows the owner-computes rule and exploits locality of references. Keywords: <b>Parallelizing</b> <b>compilers,</b> Unimodular loop transformations, Efficient code generation, Data dependences, Validity of transformations. This [...] ...|$|R
40|$|A {{method is}} {{presented}} {{for measuring the}} degree of success of a compiler at extracting implicit parallelism. The outcome of applying this method to evaluate a state-of-the-art parallelizer, KAP/Concurrent, using the Perfect Benchmarks and a few linear-algebra routines indicates {{that there is much}} room for improvement in the current generation of <b>parallelizing</b> <b>compilers.</b> 1 Introduction Compilers that translate sequential programs into semantically equivalent parallel or vector forms, known as <b>parallelizing</b> <b>compilers</b> [KKP + 81, PW 86], {{are an integral part of}} most supercomputers and have a significant influence on their performance. Despite their importance, there have been few attempts to evaluate systematically the effectiveness of <b>parallelizing</b> <b>compilers,</b> even though such evaluation is necessary to compare the effectiveness of different translation strategies and would be of great help in determining which techniques need further development. Two major approaches have been followed i [...] ...|$|R
40|$|This paper proposes an {{execution}} order control method of coarse grain distributed processes for <b>parallelizing</b> <b>compiler</b> on distributed computer environment. Currently proposed execution order control algorithms {{are designed for}} the basic-block-oriented distributed processes. However, we adopted an process generation algorithm based on function-oriented distributed processes and therefore propose new execution order control algorithm. Our algorithm gives more effective and simple executable conditions than the currently investigated ones since it is designed for function-oriented distributed processes especially. 1 Introduction Recently, parallelizing compilers are drawing attentions for software support of parallel processing. The <b>parallelizing</b> <b>compiler</b> decomposes a given sequential programs into program fragments and organizes them as parallel executable distributed processes [1 [...] 8]. We already proposed <b>parallelizing</b> <b>compiler</b> with coarse grain distributed processes on distributed com [...] ...|$|E
40|$|Precise and {{efficient}} dependence tests {{are essential to}} the effectiveness of a <b>parallelizing</b> <b>compiler.</b> This paper proposes a dependence testing scheme based on classifying pairs of subscripted variable references. Exact yet fast dependence tests are presented for certain classes of array references, as well as empirical results showing that these references dominate scientific Fortran codes. These dependence tests are being implemented at Rice University in both PFC, a <b>parallelizing</b> <b>compiler,</b> and ParaScope, a parallel programming environment...|$|E
40|$|Many modern machine {{architectures}} feature {{parallel processing}} {{at both the}} fine-grain and coarse-grain level. In order to efficiently utilize these multiple levels, a <b>parallelizing</b> <b>compiler</b> must orchestrate the interactions of fine-grain and coarse-grain transformations. The goal of the PROMIS compiler project {{is to develop a}} multi-source, multitarget <b>parallelizing</b> <b>compiler</b> in which the front-end and back-end are integrated via a single unified intermediate representation. In this paper, we examine the appropriateness of the Hierarchical Task Graph as that representation...|$|E
5000|$|However, current <b>parallelizing</b> <b>compilers</b> are {{not usually}} capable of {{bringing}} out these parallelisms automatically, and it is questionable whether this code would benefit from parallelization in the first place.|$|R
40|$|Programmer doesnâ€™t define how {{computation}} is <b>parallelized</b> <b>Compiler</b> <b>parallelizes</b> {{the execution}} automatically Languageâ€™s constructs are inherently parallel Often purely functional languages (single-assignment) Parallelization not programmed â‡’ no parallelization bugs in code, easier to program Explicit Parallelism [8] Is explicitly {{defined by the}} programmer Can be difficult to program, debugging is har...|$|R
40|$|Barriers, locks, and flags are {{synchronizing}} operations {{widely used}} by programmers and <b>parallelizing</b> <b>compilers</b> to produce race-free parallel programs. Often times, these operations are placed suboptimally, {{either because of}} conservative assumptions about the program, or merely for code simplicity. We propos...|$|R
40|$|Heterogeneous {{multi-core}} architectures, which integrates multiple {{general purpose}} CPU cores and special purpose accelerator cores on a chip, have become widely spread. However, heterogeneous multi-cores require very difficult coding for load distribution to CPU cores and accelerator cores, synchronizations and data transfer using DMA controllers. To release application programmers from such painful work, powerful <b>parallelizing</b> <b>compiler</b> for heterogeneous multi-core architectures is expected. Furthermore, cooperative work between <b>parallelizing</b> <b>compiler</b> and heterogeneous multi-core architectures {{is important to}} fully exploit the potential from these systems. Considering above situations, this poster proposes OSCAR <b>parallelizing</b> <b>compiler</b> cooperative heterogeneous multi-core architecture. 2. The Proposed Architecture The architecture comprises the following compiler-aware features(Figure. 1) : (1) Local data memory and distributed shared memory. (2) Advanced DMA controller called DTU (Data Transfer Unit) which enables overlapping task execution and data transfer. (3) Directly connecting an accelerator and a CPU on the same core...|$|E
40|$|In {{this paper}} we apply {{temporal}} logic and model checking {{to analyze the}} structure of a source program represented as a process graph. The nodes of this graph are sequential processes whose computations are specified as transition systems; the edges are dependence (flow and control) relations between the computations at the nodes. This process graph is used as an intermediate source program representation by a <b>parallelizing</b> <b>compiler.</b> By labeling the nodes and the edges of the process graph with descriptive atomic propositions and by specifying the conditions necessary for optimizations and parallelizations as temporal logic formulas, we can use a model checker to locate nodes and sub-graphs of the process graph where particular optimizations can be made. We illustrate this technique by showing how a <b>parallelizing</b> <b>compiler</b> can determine if the iterations of an enumerated loop can be executed concurrently. To add or modify optimizations in this <b>parallelizing</b> <b>compiler,</b> we need only specify [...] ...|$|E
40|$|Many {{analyses}} and transformations in a <b>parallelizing</b> <b>compiler</b> can bene t from {{the ability to}} compare arbitrary symbolic expressions. In this paper, we describe how one can compare expressions by using symbolic ranges of variables. A range is a lower and upper bound on a variable. We will also describe how these ranges can be e ciently computed from the program text. Symbolic range propagation has been implemented in Polaris, a <b>parallelizing</b> <b>compiler</b> being developed at the University of Illinois, and is used for symbolic dependence testing, detection of zero-trip loops, determining array sections possibly referenced by an access, and loop iteration-count estimation. ...|$|E
40|$|The {{development}} of single chip VLSI processors {{is the key}} technology of ever growing pervasive computing to answer overall demands for usability, mobility, speed, security, etc. We have so far developed a hardware cryptography-embedded multimedia mobile processor architecture, HCgorilla. Since HCgorilla integrates {{a wide range of}} techniques from architectures to applications and languages, one-sided design approach is not always useful. HCgorilla needs more complicated strategy, that is, hardware/software (H/S) codesign. Thus, we exploit the software support of HCgorilla composed of a Java interface and <b>parallelizing</b> <b>compilers.</b> They are assumed to be installed in servers {{in order to reduce the}} load and increase the performance of HCgorilla-embedded clients. Since compilers are the essence of software's responsibility, we focus in this article on our recent results about the design, specifications, and prototyping of <b>parallelizing</b> <b>compilers</b> for HCgorilla. The <b>parallelizing</b> <b>compilers</b> are composed of a multicore compiler and a LIW compiler. They are specified to abstract parallelism from executable serial codes or the Java interface output and output the codes executable in parallel by HCgorilla. The prototyping compilers are written in Java. The evaluation by using an arithmetic test program shows the reasonability of the prototyping compilers compared with hand compilers...|$|R
40|$|Dependence {{analysis}} and dependence information are critical components of many optimizing and <b>parallelizing</b> <b>compilers.</b> And there exist many fast and precise dependence tests that work on scalarsubscripted array references. We have extended this analysis by adding tests that directly handle Fortran 90 array-section references...|$|R
40|$|A {{wide variety}} of {{application}} programs have inherent loop-level parallelism that can be exploited on shared-memory multiprocessor systems for performance improvement. <b>Parallelizing</b> <b>compilers</b> are available that can automatically identify and extract some of the loop-level parallelism in sequential programs. Traditional <b>parallelizing</b> <b>compilers,</b> however, often cannot extract much of the available parallelism due to the inherent limitations of compile-time information. Loops with potential data dependences that are caused by run-time values, or loops with sequential constructs (e. g. do-while loops), in particular are impossible to <b>parallelize</b> using these <b>compilers.</b> The parallelism in such loops can be exploited only by using run-time parallelization techniques since the information required to parallelize these loops is available only at run-time. Once a program has been parallelized, {{it is necessary to}} efficiently execute it in parallel to improve its performance. <b>Compiler</b> <b>parallelized</b> code cannot predetermine the number of processors a parallel code region can most efficiently use, however, especially, if the program behavior changes dynamically or the system load in a multiprogrammed environment changes as the program executes. With a static mapping assigned by the compiler, the parallel execution performance may suffer due to an insufficient workload in each parallel task, or due to resource contention when too many processes are running in the system. This thesis proposes a dynamically adaptive parallelization model based on speculative multithreading for multiprogrammed shared-memory multiprocessor systems to address the problems associated with traditional <b>parallelizing</b> <b>compilers.</b> This work first addresses the problem of extracting loop-level parallelism from progra [...] ...|$|R
40|$|Automatic parallelization {{is usually}} {{believed}} to be less effective at exploiting implicit parallelism in sparse/irregular programs than in their dense/regular counterparts. However, not much is really known {{because there have been}} few research reports on this topic. In this work, we have studied the possibility of using an automatic <b>parallelizing</b> <b>compiler</b> to detect the parallelism in sparse/irregular programs. The study with a collection of sparse/irregular programs led us to some common loop patterns. Based on these patterns new techniques were derived that produced good speedups when manually applied to our benchmark codes. More importantly, these parallelization methods can be implemented in a <b>parallelizing</b> <b>compiler</b> and can be applied automatically...|$|E
40|$|Environment) that {{integrates}} {{the functions}} of an interactive <b>parallelizing</b> <b>compiler</b> and some interactive performance analyzers. By applying these tools in ParaPIE, we have successfully parallelized three difficult sequential programs in SPECfp 95 benchmarks {{that can not be}} parallelized by automatic parallelizing compilers...|$|E
40|$|We {{present a}} <b>parallelizing</b> <b>compiler</b> for lazy {{functional}} programs that uses strictness analysis {{to detect the}} implicit parallelism within programs. It generates an intermediate functional program, where a special syntactic construct `letpar', which is semantically equivalent to the well-known let-construct, is used to indicate subexpressions for which a parallel execution is allowed. Only for sufficiently complex expressions a parallelization will be worthwhile. For small expressions the communication overhead may outweigh {{the benefits of the}} parallel execution. Therefore, the <b>parallelizing</b> <b>compiler</b> uses some heuristics to estimate the complexity of expressions. The distributed implementation of parallelized functional programs described in [Loogen et al. 89] enabled us to investigate the impact of various parallelization strategies on the runtimes and speedups. The strategy, which only allows the parallel execution of non-predefined function calls in strict positions, shows t [...] ...|$|E
40|$|Loop transformations are {{becoming}} critical to exploiting parallelism and data locality in <b>parallelizing</b> and optimizing <b>compilers.</b> This document describes the Lambda loop transformation toolkit, an {{implementation of the}} non-singular matrix transformation theory, which can represent any linear one-to-one transformation. Lambda has a simple interface, and is independent of any compiler intermediate representation. It {{has been used in}} <b>parallelizing</b> <b>compilers</b> for multiprocessor machines as well as optimizing compilers for uniprocessor machines. Keywords: Parallel programming, <b>parallelizing</b> <b>compilers,</b> loop transformations, linear transformations, nonsingular transformations. 0 This work was partly supported by Cornell University, {{by a grant from the}} Hewlett-Packard Corporation, and by the University of Rochester. 1 Contents 1 Introduction 4 2 A Linear Loop Transformation Theory 4 3 Data Dependences 6 3. 1 Data Types : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 [...] ...|$|R
40|$|On a {{distributed}} memory machine, hand-coded message pass-ing {{leads to the}} most efficient execution, {{but it is difficult}} to use. <b>Parallelizing</b> <b>compilers</b> can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient exe-cution is limited to those programs for which precise analy-sis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of <b>parallelizing</b> <b>compilers,</b> but it lags in per-formance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports...|$|R
40|$|Current <b>parallelizing</b> <b>compilers</b> for message-passing {{machines}} only {{support a}} limited class of data-parallel applications. One method for eliminating this restriction is to combine powerful shared-memory <b>parallelizing</b> <b>compilers</b> with software distributed-shared-memory (DSM) systems. Preliminary results show simply combining the parallelizer and software DSM yields very poor performance. The compiler/software DSM interface {{can be improved}} based on relatively little compiler input by: 1) combining synchronization and parallelism information communication on parallel task invocation, 2) employing customized routines for evaluating reduction operations, and 3) selecting a hybrid update protocol to presend data by flushing updates at barriers. These optimizations yield decent speedups for program kernels, but are not sufficient for entire programs. Based on our experimental results, we point out areas where additional compiler analysis and software DSM improvements are necessary to achieve goo [...] ...|$|R
