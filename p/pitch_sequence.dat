42|108|Public
500|$|On September15,2000, US songwriters Seth Swirsky and Warren Campbell filed {{a lawsuit}} against Carey at the [...] 9th Circuit for {{copyright}} infringement, [...] "reverse passing off" [...] and false designation, claiming that [...] "Thank God I Found You" [...] borrowed heavily from a song they composed called [...] "One of Those Love Songs". It was recorded by the R group Xscape in 1998 for their album Traces of My Lipstick. The lawsuit claimed that Carey wrongfully gave the songwriting credits to Jam and Lewis. Swirsky and Campbell had sold the rights of the song to So So Def Recordings in 1998. [...] "I'm a fan of Mariah Carey; this is nothing personal against her. But I really do believe there's accountability, and it's very clear what happened here. I've never sued anybody before", Swirsky said. According to the district court, an expert witness (chair of the Musicology Department at the University of California at Los Angeles) determined that the songs shared a [...] "substantially similar chorus". The expert stated that although the lyrics and verse melodies of the two songs were different, the songs' choruses [...] "shared a 'basic shape and pitch emphasis' in their melodies, which were played over 'highly similar basslines' and chord changes, at very nearly the same tempo and in the same generic style." [...] He noted both the songs had their choruses sung in the key of B. The expert further remarked that [...] "the emphasis on musical notes" [...] on the two songs was the same, which [...] "contribute to the impression of similarity one hears when comparing the two songs." [...] He presented a series of visual transcriptions of his observations. The transcriptions contained details about the <b>pitch</b> <b>sequence</b> of both the songs' chorus, melody, and bassline.|$|E
50|$|Ludwig van Beethoven {{knew the}} symphony well, copying out 29 bars from the score {{in one of}} his sketchbooks. As Gustav Nottebohm {{observed}} in 1887, the copied bars appear amid the sketches for Beethoven's Fifth Symphony, whose third movement begins with a <b>pitch</b> <b>sequence</b> similar to that of Mozart's finale (see example above).|$|E
50|$|The double {{meaning of}} each modal {{signature}} {{as a kind}} of pitch class (phthongos) and an own echos or mode makes it necessary to distinct two levels: melos, the melody itself, and metrophonia, the memorization of a <b>pitch</b> <b>sequence</b> notated in phonic neumes which is followed step by step by the intonation formula {{as a kind of}} pitch class (phthongos) by the intonation formula of a certain echos.|$|E
50|$|Advance {{scouting}} on Darvish is made difficult by his {{tendency to}} change his most frequent <b>pitch</b> <b>sequences</b> over time.|$|R
40|$|To {{enrich the}} viewing {{experience}} of baseball games and provide some clues for enhancing pitcher’s performance, we propose a Kalman filter-based approach to track ball trajectory from singleview <b>pitching</b> <b>sequences.</b> Without setting extraordinary equipments in stadiums or other sensing instruments, this approach robustly extracts ball trajectory for <b>pitching</b> <b>sequences</b> captured from TV channels or downloaded from the Internet. To validate the detected ball trajectories, we investigate {{the characteristics of}} ball trajectories {{on the basis of}} a baseball physical model. The effectiveness of ball trajectory extraction and ball position detection are presented. 1...|$|R
30|$|Although implant {{recipients}} perceive basic rhythm patterns {{similarly to}} NH subjects [15], perception for <b>pitch,</b> <b>pitch</b> <b>sequences,</b> and melody recognition is significantly poorer {{than that of}} NH [15 – 21].|$|R
5000|$|The chordal opening soon dissolves {{into the}} {{turbulent}} [...] "centre" [...] of the piece, featuring tremolos and bursts of notes within a narrow registral span. There are also single notes and small clusters covering the piano's entire range. Concealed beneath this active surface {{of this section}} are repetitions and extensions of the chords {{from the beginning of}} the piece constituting a long repeated <b>pitch</b> <b>sequence</b> (containing internal recurring sequences). These relationships become more evident as the turbulence subsides toward the end of the section, finally coalescing into a conclusion of staccato chords similar to those at the outset of the Sequenza [...]|$|E
5000|$|In October 2012, Alabama {{singer and}} {{songwriter}} Allyson Nichole Burnett filed a copyright infringement suit authored by attorney Neville Johnson in California federal court against Jepsen and Young {{as well as}} several publishing companies and performing rights groups. She claimed that Young, Matt Thiessen and Brian Lee copied a prominent motif of her 2010 song, [...] "Ah, It's a Love Song". The lawsuit noted several similarities between the two songs, including an identical <b>pitch</b> <b>sequence</b> (5-3-5-3-2), melodic contour (down, up, down, down), rhythmic construction (8th rest, 8th note, 8th note, 8th note, 8th note, 8th rest, quarter note), and timbre (textless vocals). Burnett also stated that she suffered [...] "emotional and psychological damage" [...] from fans asking why she copied the song.|$|E
5000|$|He also {{throws a}} curveball and changeup that, {{according}} to Bleacher Report's Doug Mead, have been deemed as [...] "effective." [...] However, according to Thomas Belmont of Baseball Instinct, the curveball {{is actually a}} slider. The pitch has sharp movement and {{has the potential to}} be a major league quality out pitch. [...] Observed one American League scout during the 2013 World Series, [...] "He’s got one of those loose-arm deliveries that creates great life on his fastball. He has such tight spin on his slider that (Shane) Victorino and (Dustin) Pedroia looked helpless against it." [...] Rosenthal has developed his changeup to be an average pitch since 2011 when he played his first full season of professional baseball with Quad Cities, expanding the quality of the <b>pitch</b> <b>sequence</b> which he is able to throw to major league hitters. [...] One unusual trend about the results is that Rosenthal has consistently had a higher than league average batting average on balls in play (BABIP) - a statistic in which league average is typically about [...]300. In 2009, Rosenthal's BABIP was [...]362 and [...]334 in 2011. In 2010, it actually dipped to the other extreme, to [...]262. [...] In his first major league season, 2013, it was [...]347.|$|E
5000|$|Strike Zone, {{which shows}} <b>pitch</b> <b>sequences</b> with strikes {{displayed}} in yellow and balls in white. It {{can put a}} simulated pane of glass that shatters when a ball goes through the zone.|$|R
30|$|In {{order to}} better assess the {{performance}} of our proposed method, all the tracks from the dataset were manually processed by musical experts to extract ground truth pitch information. The Songs 2 See Editor interface [19] was used for this matter. The <b>pitch</b> <b>sequences</b> of the solo instrument were manually corrected up to the time and frequency precision offered by the software; however, a frame-wise precision cannot be guaranteed. The extracted ground truth <b>pitch</b> <b>sequences</b> were used as prior information for our proposed method (bypassing the pitch detection stage), and separation was conducted for the complete dataset.|$|R
50|$|Strike Zone, {{which shows}} <b>pitch</b> <b>sequences</b> with strikes in yellow and balls in white. It {{can put a}} {{simulated}} pane of glass that shatters when a ball goes through the zone (à la the computerized scoring graphics used for bowling).|$|R
5000|$|The early {{sketches}} for Trans {{show that}} Stockhausen initially considered using the formula technique {{he had recently}} (and very successfully) developed for Mantra. In the end, however, he decided against it, and settled instead on a simple <b>pitch</b> <b>sequence</b> of thirty-six notes without formal or rhythmic implications, which is [...] "treated with inspired flexibility", first a twelve-tone row in a sharply falling contour, and then a gradually rising, winding chromatic line [...] The music consists of three main layers, differentiated by audible characteristics, but {{in part by the}} visual aspects as well. The first layer is played back invisibly on tape in the theatre, and consists of the loudly amplified sound of a loom shuttle crossing the room, left-to-right and right-to-left, at first occurring at nearly periodic intervals of about 20 seconds [...] Over the course of the piece the intervals between these sounds deviate increasingly from this average, following a systematic process [...] These sounds mark off segments of the music similar to the colotomic percussion signals used in some Asian musics, such as Japanese gagaku. Stockhausen had used this device previously in Mantra and Telemusik [...] There are 33 of these smaller subsections, grouped into six large sections, with a clear evolutionary process [...] The shuttle-loom sounds also indicate the structural levels, according to whether they occur singly, in pairs, or—at the dividing points between the six main sections of the work—in threes [...]|$|E
50|$|So far, the Atlas F {{test program}} had gone well {{and nobody was}} {{prepared}} for the upcoming disaster on April 9, 1962 when Missile 11F exploded only one second after liftoff from LC-11. Subsequent investigation found that the sustainer LOX turbopump exploded due to the impeller blades rubbing against the metal casing and causing a spark that ignited the LOX. This failure mode also occurred in two static firing tests of the MA-3 engines, and it led to a plastic liner being added to the turbopump casing. Testing now began at Vandenberg AFB and Missile 15F flew successfully from 576-E on August 1. Nine days later, Missile 57F launched from OSTF-1 and failed to perform its roll maneuver. The <b>pitch</b> <b>sequence</b> was executed properly, {{but the lack of}} the roll program prevented the missile from attaining the correct trajectory and it was blown up by Range Safety 67 seconds into the launch. An electrical short in the programmer was found to be the cause, although the specific failure point was not determined. The roll signal had been extremely weak and barely enough to have any effect, as a consequence the missile's flight path was about 53° {{to the right of the}} planned trajectory. The next test launch took place from the now-repaired LC-11 at the Cape two days later and all subsequent R&D flights went without a hitch except for 13F on November 14 which experienced a thrust section fire at liftoff that led to sustainer shutdown. The booster engines retained attitude control until BECO, at which point the absence of sustainer thrust resulted in immediate missile tumbling and loss of the mission. The cause of the thrust section fire was unknown, however a fuel leak was suspected. The next launch, Missile 21F, successfully concluded the R&D phase of the Atlas F program.|$|E
5000|$|On September 15, 2000, US songwriters Seth Swirsky and Warren Campbell filed {{a lawsuit}} against Carey at the 9th Circuit for {{copyright}} infringement, [...] "reverse passing off" [...] and false designation, claiming that [...] "Thank God I Found You" [...] borrowed heavily from a song they composed called [...] "One of Those Love Songs". It was recorded by the R&B group Xscape in 1998 for their album Traces of My Lipstick. The lawsuit claimed that Carey wrongfully gave the songwriting credits to Jam and Lewis. Swirsky and Campbell had sold the rights of the song to So So Def Recordings in 1998. [...] "I'm a fan of Mariah Carey; this is nothing personal against her. But I really do believe there's accountability, and it's very clear what happened here. I've never sued anybody before", Swirsky said. According to the district court, an expert witness (chair of the Musicology Department at the University of California at Los Angeles) determined that the songs shared a [...] "substantially similar chorus". The expert stated that although the lyrics and verse melodies of the two songs were different, the songs' choruses [...] "shared a 'basic shape and pitch emphasis' in their melodies, which were played over 'highly similar basslines' and chord changes, at very nearly the same tempo and in the same generic style." [...] He noted both the songs had their choruses sung in the key of B. The expert further remarked that [...] "the emphasis on musical notes" [...] on the two songs was the same, which [...] "contributed to the impression of similarity one hears when comparing the two songs." [...] He presented a series of visual transcriptions of his observations. The transcriptions contained details about the <b>pitch</b> <b>sequence</b> of both the songs' chorus, melody, and bassline.|$|E
40|$|Mentor: Andrew J. OxenhamOn {{hearing a}} <b>sequence</b> of <b>pitches,</b> {{listeners}} develop expectations for how that sequence will continue. Research on melodic continuation generally supposes {{two kinds of}} factors: the top-down influence of perceived tonality, and the bottom-up influence of melodic contour (relative size {{and direction of the}} intervals). For bottom-up, contour-based factors, there is converging evidence that melodies with good continuation tend to have small intervals between notes and narrow overall ranges. Since melodic contour can also be perceived in sequences of notes varying in brightness (an aspect of timbre or sound quality) or loudness instead of pitch, it is reasonable to suppose that the same contour-based expectations that apply to <b>pitch</b> <b>sequences</b> also apply to brightness and loudness sequences. The present study found that perceptive continuation ratings for brightness and loudness sequences generally conform to the same contour-based expectations as <b>pitch</b> <b>sequences,</b> though some differences between dimensions were found. This is compatible with the hypothesis that perception of melodic contour is a general auditory phenomenon that is not unique to pitch. The ratings for brightness and loudness sequences were more similar to each other than to ratings for <b>pitch</b> <b>sequences,</b> and {{it is likely that the}} factors that set pitch apart from other auditory dimensions are closely related to perceived tonality. This research was supported by the Undergraduate Research Opportunities Program (UROP) ...|$|R
40|$|A {{model for}} the {{internal}} representation of <b>pitch</b> <b>sequences</b> in tonal music is advanced. This model assumes that <b>pitch</b> <b>sequences</b> are retained as hierarchical networks. At each level of the hierarchy, elements are organized as structural units in accordance with laws of figural goodness, such as proximity and good continuation. Further, elements that are present at each hierarchical level are elaborated by further elements so as to form structural units at the next-lower level, until the lowest level is reached. Processing advantages of the system are discussed. It may generally be stated that we tend to encode and retain information {{in the form of}} hierarchies when given the opportunity to do so. For example, programs of behavior tend to be retained as hierarchies (Miller, Galanter, & Pribram, 1960) and goals in problem solving as hierarchies of subgoal...|$|R
40|$|The {{purpose of}} this paper is to present an {{algorithmic}} pipeline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal <b>pitch</b> <b>sequences</b> are extracted from the audio recordings by means of a predominant fundamental frequency estimation technique; second, instances of the patterns are detected directly in the <b>pitch</b> <b>sequences</b> by means of a dynamic programming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the flamenco fandango style was performed. To this end, a number of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandangos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strategy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper. 1...|$|R
40|$|This paper {{considers}} the proposed algorithm {{submitted to the}} Music Information Retrieval Evaluation eXchange (MIREX) 2010 “Audio Melody Extraction ” task. The proposed melody extraction algorithm {{can be divided into}} three steps: (1) a spectral analysis using a variable length window, (2) a pitch candidate estimation, and (3) a <b>pitch</b> <b>sequence</b> identification. In the first step, the short-time Fourier transform (STFT) with variable length window is performed to be robust against dynamic variation of melody line. In the second step, melody pitch candidates of each frame are obtained from weights of a harmonic structure in the spectrum. Furthermore, a melody pitch range estimation is also considered to reduce false-positive and computation. In the third step, a single <b>pitch</b> <b>sequence</b> (melody line) is selected based on the general properties of melody line. 1...|$|E
40|$|This {{extended}} abstract descries KETI’s {{submission to}} the query-by-singing/humming (QbSH) task of MIREX 2011. Our QbSH system {{is based on}} dynamic time warping (DTW) and frame-based <b>pitch</b> <b>sequence.</b> Our system reduces false alarm to combine the distances of multiple DTW processes. To improve the performance, DTW with asymmetric sense, compensation, and distances insensitive to the error are investigated. 1...|$|E
40|$|We {{present an}} {{effective}} approach for automatic extraction {{of the main}} melody from polyphonic music, especially vocal melody songs. The approach {{is based on a}} Bayesian framework by calculating the probability of each pitch candidate using a set of characteristics, and searching for the best <b>pitch</b> <b>sequence</b> by viterbi algorithm. We submitted our algorithm for the audio melody extraction task of the Music Information Retrieval Evaluation eXchange (MIREX) 2013...|$|E
40|$|Perception of the {{acoustic}} world requires the simultaneous processing of {{the acoustic}} patterns associated with sound objects and their location in space. In this functional magnetic resonance experiment, we investigated {{the human brain}} areas engaged {{in the analysis of}} <b>pitch</b> <b>sequences</b> and sequences of acoustic spatial locations in a paradigm in which both could be varied independently. Subjects were presented with sequences of sounds in which the individual sounds were regular interval noises with variable pitch. Positions of individ-ual sounds were varied using a virtual acoustic space paradigm during scanning. Sound <b>sequences</b> with changing <b>pitch</b> specifically activated lateral Heschl’s gyrus (HG), anterior planum temporale (PT), planum polare, and superior temporal gyrus anterior to HG. Sound sequences with changing spatial locations specifically activated posteromedial PT. These results demonstrate directly that distinct mechanisms for the analysis of <b>pitch</b> <b>sequences</b> and acoustic spatial sequences exist in the human brain. This functional differentiation is evident as early as PT: within PT, pitch pattern is processed anterolaterally and spatial location is processed posteromedially. These areas may represent human homologs of macaque lateral and medial belt, respectively. Key words: pitch; auditory space; functional imaging; human auditory cortex; sound; brai...|$|R
40|$|A {{group of}} {{interactive}} autonomous singing robots were programmed {{to develop a}} shared repertoire of vocal singing-like intonations from scratch, {{after a period of}} spontaneous creations, adjustments and memory reinforcements. The robots are furnished with a physical model of the vocal tract, which synthesises vocal singing-like intonations, and a listening mechanism, which extracts <b>pitch</b> <b>sequences</b> from audio signals. The robots learn to imitate each other by babbling heard intonation patterns in order to evolve vectors of motor control parameters to synthesise the imitations. 1...|$|R
40|$|Quite {{reasonable}} retrieval {{effectiveness is}} achieved for retrieving polyphonic (multiple notes at once) {{music that is}} symbolically encoded via melody queries, using relatively simple pattern matching techniques based on <b>pitch</b> <b>sequences.</b> Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files...|$|R
40|$|This paper {{describes}} the proposed algorithm {{submitted to the}} MIREX 2009 “Audio Melody Extraction ” task. The algorithm addresses the task of extracting the predominant melody pitch from a polyphonic audio signal. The algorithm extracts the melody pitch in three steps. In the first step, transient analysis is performed on the polyphonic audio signal to determine the analysis frame length, and then a fixed number of pitch candidates are obtained by ranking the weights of the harmonic structure of the windowed signal. In the second step, a single dominant <b>pitch</b> <b>sequence</b> (melody line) is selected from the many possible pitch sequences based on the following properties of melody line: (1) while the nominal dynamic range of a singing vibrato is ± 60 ∼ 200 cents, it is only ± 20 ∼ 30 cents for instruments; (2) melody transitions are typically limited to one octave; (3) a rest during singing is often longer than 50 ms. In the third step, a smoothing process is performed to refine the estimated <b>pitch</b> <b>sequence...</b>|$|E
40|$|STUDIES HAVE SHOWN THAT PITCH SET, {{which refers}} {{to a set of}} pitches of {{constituent}} tones of a melody, is a primary cue for perceiving the key of a melody. The present study investigates whether characteristics other than pitch set function as additional cues for key perception. In Experiment 1, we asked 13 musicians with absolute p itch t o select k eys for 60 stim ulus t one sequences consisting of the same pitch set differing in <b>pitch</b> <b>sequence.</b> In Experiment 2, we asked 31 nonmusicians to select tonal centers for the 60 stimulus tone sequences. Responses made by the musicians and the nonmusicians yielded essentially equivalent results, suggesting that key perception is never unique to musicians. The listeners' responses were limited to a few keys/tones, and some tone sequences elicited agreement among the majority of the listeners for each of the keys/tones. These findings confirm that key perception is not only defined by pitch set but also influenced by characteristics other than pitch set such as <b>pitch</b> <b>sequence...</b>|$|E
40|$|We {{propose a}} general {{approach}} to discriminant feature extraction and fusion, built on an optimal feature transformation for discriminant analysis [6]. Our experiments indicate that our approach can dramatically reduce the dimensionality of original feature space whilst improving its discriminant power. Our feature fusion method {{can be carried}} out in the reduced lower-dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the <b>pitch</b> <b>sequence</b> derived from monophonic melodie...|$|E
40|$|In {{this paper}} {{we attempt to}} {{demonstrate}} the strengths of Hierarchical Hidden Markov Models (HHMMs) in the representation and modelling of musical structures. We show how relatively simple HHMMs, containing a minimum of expert knowledge, use their advantage of having multiple layers to perform well on tasks where flat Hidden Markov Models (HMMs) struggle. The examples in this paper show a HHMM’s performance at extracting higherlevel musical properties {{through the construction of}} simple <b>pitch</b> <b>sequences,</b> correctly representing the data set on which it was trained. 1...|$|R
40|$|Previous {{studies showed}} that the perceptual {{processing}} of sound sequences is more efficient when the sounds vary in pitch than when they vary in loudness. We show here that sequences of sounds varying in brightness of timbre are processed with the same efficiency as <b>pitch</b> <b>sequences.</b> The sounds used consisted of two simultaneous pure tones one octave apart, and the listeners' task was to make same/different judgments on pairs of sequences varying in length (one, two, or four sounds). In one condition, brightness of timbre was varied within the sequences by changing the relative level of the two pure tones. In other conditions, pitch was varied by changing fundamental frequency, or loudness was varied by changing the overall level. In all conditions, only two possible sounds {{could be used in}} a given sequence, and these two sounds were equally discriminable. When sequence length increased from one to four, discrimination performance decreased substantially for loudness sequences, but to a smaller extent for brightness <b>sequences</b> and <b>pitch</b> <b>sequences.</b> In the latter two conditions, sequence length had a similar effect on performance. These results suggest that the processes dedicated to pitch and brightness analysis, when probed with a sequence-discrimination task, share unexpected similarities...|$|R
40|$|A {{multi-agent system}} is {{presented}} which generates melody <b>pitch</b> <b>sequences</b> with a hierarchical structure. The system has no explicit melodic intelligence and generates the pitches {{as a result}} of emotional influence and communication between agents, and the hierarchical structure {{is a result of the}} emerging agent social structure. Another key element is that the system is not a mapping from multi-agent interaction onto musical features, but actually utilizes music for the agents to communicate emotions. Each agent in the society learns its own growing tune during the interaction process. 1...|$|R
40|$|This {{document}} describes our {{submission to}} QBSH task of MIREX 2013. Our algorithm adopts a two-stage cascaded solution based on Locality Sensitive Hashing (LSH) and accurate matching of frame-level <b>pitch</b> <b>sequence.</b> Firstly, LSH is employed to quickly filter out songs with low matching possibilities, {{resulting in a}} list of candidate songs for further processing. In the second stage, Dynamic Time Warping (DTW) is applied to find the N (set to 10) most matching songs from the candidate list. This approach allows for fast pruning of irrelevant songs whilst preserving the most matching ones. 1...|$|E
40|$|We define an {{automated}} music transcription {{system as a}} sequence of processing modules, beginning with acoustic acquisition and ending with a <b>pitch</b> <b>sequence.</b> This modular approach allows the various system components to be studied and optimized separately. Our own single-voice transcription system {{is presented as a}} working example of such modular designs. To assess the performance of such transcription systems under real-world constraints, we propose a single numerical score based on matching known events in the source material to corresponding events in the transcription. We use this score to evaluate our transcription system under several conditions...|$|E
40|$|To {{determine}} whether the accuracy of students ' aural perception of rhythmic com-pleteness and incompleteness is influenced by melodic context, 2, 207 public school students were administered a 42 -item data-collection instrument based on 14 rhythmic units, 7 complete and 7 incomplete. Students heard each unit (1) on one pitch, (2) {{as part of a}} melody in which the completeness or incompleteness of the <b>pitch</b> <b>sequence</b> matched that of the rhythm, and (3) as part of a nonmatching melody; indicating, in each case, whether the unit was complete or incomplete. According to the results, accuracy is influenced by melodic context...|$|E
5000|$|Since {{stopping}} as a flute {{performer in}} the mid 00s he has vehemently explored the novel areas of electronic music. He is equally at home in abstract and challenging sonic experiments and effortless, street-smart rhythm designs and this has allowed him develop a very personal take on realtime, electronic music. As an example, implementing aspects of his compositional procedures in computer code has allowed him to use empirical models of <b>pitch</b> <b>sequences</b> [...] that provide fresh, comprehensible melodic material, progressions, patterns and parametrical sonic effects in electronic performance.|$|R
2500|$|Faut was {{a pitcher}} with an {{analytical}} mind that {{allowed her to}} remember <b>pitching</b> <b>sequences</b> from batter to batter and game to game. In pre-game team meetings, she would memorize {{the weakness of the}} opposite hitters and during the game vary the rotation of pitches she used. [...] "Part of my success", she once reflected, [...] "was that in my mind I could record the order of pitches I threw to each girl, so they never saw the same thing twice". She added [...] "I was a mathematical whiz in school. They’d never know what was coming, so they’d start guessing".|$|R
40|$|A generic {{spinning}} missile with dithering canards is used {{to demonstrate}} the utility of an overset structured grid approach for simulating the aerodynamics of rolling airframe missile systems. The approach {{is used to}} generate a modest aerodynamic database for the generic missile. The database is populated with solutions to the Euler and Navier-Stokes equations. It is used to evaluate grid resolution requirements for accurate prediction of instantaneous missile loads and the relative aerodynamic significance of angle-of-attack, canard <b>pitching</b> <b>sequence,</b> viscous effects, and roll-rate effects. A novel analytical method for inter- and extrapolation of database results is also given...|$|R
