21|26|Public
50|$|UNIX SYS V {{implements}} {{message passing}} by keeping {{an array of}} linked lists as message queues. Each message queue is identified by its index in the array, and has a unique descriptor. A given index can have multiple <b>possible</b> <b>descriptors.</b> UNIX gives standard functions to access the message passing feature.|$|E
30|$|Finally, {{concerning}} {{the opportunity to}} use a balanced dataset for training, experiments evidenced that balanced dataset led to an average correct estimation (taking into account all the <b>possible</b> <b>descriptors</b> with no data scaling) of 93.7 % {{in the case of}} balanced dataset and 91.0 % in the case of unbalanced dataset.|$|E
30|$|Concerning the {{opportunity}} to scale in the range [0, 1], the variables of the feature vectors and experiments evidenced that, taking into account all the <b>possible</b> <b>descriptors</b> and combinations of dataset balancing, scaled data gave the best average correct estimations (67.0 %). However, an in-depth analysis of the confusion tables reveals as this result is mainly biased by the HOG descriptor, operating on the unbalanced dataset by means of non-scaled data, bad outcomes (48.3 %). With regard {{to all the other}} descriptors, the non-scaled data approach is not worse than the scaled one. Taking into account all the <b>possible</b> <b>descriptors,</b> the unbalanced dataset for training led to a better average correct estimation both in the case of scaled data (75.1 % for unbalanced case and 58.8 % for balanced case) and non-scaled data (69.0 % for unbalanced case and 59.7 % for balanced case).|$|E
30|$|The {{analyzer}} executes {{the chain}} of analyzer components consisting of a ResourceAnalyzer, an EnvironmentAnalyzer, and an ApplicationAnalyzer. Each component analyzes the parameters stored in one descriptor and generates a set of <b>possible</b> job <b>descriptors.</b>|$|R
40|$|By {{considering}} the Reynolds stress equations as a <b>possible</b> <b>descriptor</b> of complex turbulent fields, pressure-velocity interaction and turbulence dissipation are studied {{as two of}} the main physical contributions to Reynolds stress balancing in turbulent flow fields. It is proven that the pressure interaction term contains turbulence generation elements. However, the usual 'return to isotropy' element appears more weakly than in the standard models. In addition, convection-like elements are discovered mathematically, {{but there is no}} mathematical evidence that the pressure fluctuations contribute to the turbulent transport mechanism. Calculations of some simple one-dimensional fields indicate that this extra convection, rather than the turbulent transport, is needed mathematically. Similarly, an expression for the turbulence dissipation is developed. The end result is a dynamic equation for the dissipation tensor which is based on the tensorial length scales...|$|R
30|$|Inspired by {{previous}} literature, we investigated {{the interplay between}} momentary gratitude and mood, {{in the context of}} our intervention. We exemplify in Fig.  12 mood samples from a single participant, obtained by our novel tool. As expected, these exhibit frequent oscillations daily and even within a single day. To simplify our analysis, we first project the eight <b>possible</b> mood <b>descriptors</b> to a single affective dimension—either valence or arousal. Then we collapse the samples to binary values (positive or negative valence, high or low arousal). As a result, “Irritated”, for instance, will be coded as negative valence and positive arousal.|$|R
30|$|With {{regard to}} the {{opportunity}} to scale in the range [0, 1], the features in input to the SVM and the experiments evidenced that non-scaled data better preserved the embedded information: the average correct estimations (taking into account all the <b>possible</b> <b>descriptors</b> and combinations of dataset balancing) were 92.3 % {{in the case of}} non-scaled input and 85.4 % in the case of scaled input.|$|E
40|$|International audienceImages {{and music}} share deep and mysterious links that drive {{audio-visual}} playlist composition. In this paper we first explore the <b>possible</b> <b>descriptors</b> for each medium, and their difference {{of nature that}} makes the mixing of both media a difficult operation. In order {{to fill the gap}} between both worlds, we then propose a double method for indexing music titles and pictures on a common mood basis. The evocation process that triggers a picture when a music title is played or conversely that calls for a music when looking at a picture is modeled through a visual map metaphor that blends picture semantics and music moods. Then music moods are propagated to the pictures. The results of this indexing method are assessed and different improvements are proposed...|$|E
40|$|The d-band {{model has}} been useful in {{understanding}} how adsorbate molecules form bonds and interact with a transition metal catalyst surface. Recent {{research has shown that}} it is possible to apply the model to nanocluster catalysts on a support structure, but the d-band model has not yet been tested for sub-nanometer clusters. For this research, four atom transition metal clusters are used as catalysts to study bond formation in the dehydrogenation reaction of propane (C­ 3 H 8) to propene (C­ 3 H 6). Testing whether the d-band model applies for these clusters is a step toward developing <b>possible</b> <b>descriptors</b> of their catalytic activity. Our ultimate goal is to computationally screen bimetallic alloy clusters of varying compositions to determine which are favorable for propane dehydrogenation...|$|E
25|$|Six {{sandstone}} {{names are}} <b>possible</b> using the <b>descriptors</b> for grain composition (quartz-, feldspathic-, and lithic-) {{and the amount}} of matrix (wacke or arenite). For example, a quartz arenite would be composed of mostly (>90%) quartz grains and have little or no clayey matrix between the grains, a lithic wacke would have abundant lithic grains and abundant muddy matrix, etc.|$|R
40|$|Descriptors for hazelnut, or filbert, (Corylus avellana L.) were {{developed}} by Professor Dr A. Ilhami Koksal and Dr Nurdan Tuna Gunes. An advanced draft was subsequently prepared {{by a group of}} experts within the FAO-CIHEAM Interregional Cooperative Research Network on Nut trees, coordinated by Dr Ignasi Batlle. The document was harmonized as far as <b>possible</b> with <b>descriptors</b> developed by the International Union for the Protection of New Varieties of Plants (UPOV, 1979). This revised document was subsequently sent to a number of experts for their comments. A full list of the names and addresses of those involved is given in the 'Contributors' Section. Peer Revie...|$|R
40|$|This paper proposes {{the use of}} a {{similarity}} measure {{based on}} information theory called correntropy for the automatic classification of pathological voices. By using correntropy, it is <b>possible</b> to obtain <b>descriptors</b> that aggregate distinct spectral characteristics for healthy and pathological voices. Experiments using computational simulation demonstrate that such descriptors are very efficient in the characterization of vocal dysfunctions, leading to a success rate of 97 % in the classification. With this new architecture, the classification process of vocal pathologies becomes much more simple and efficient...|$|R
40|$|International audienceWhen {{evaluating}} an odor, non-specialists generally provide descriptions as bags of terms. Nevertheless, these evaluations {{cannot be}} processed by classical odor analysis methods {{that have been}} designed for trained evaluators having an excellent mastery of professional controlled vocabulary. Indeed, currently, mainly oriented approaches based on learning vocabularies are used. These approaches too restrictively limit the <b>possible</b> <b>descriptors</b> available for an uninitiated public and therefore require a costly learning phase of the vocabulary. The objective of this work is to merge the information expressed by these free descriptions (terms) into a set of non-ambiguous descriptors best characterizing the odor; this {{will make it possible}} to evaluate the odors based on non-specialist descriptions. This paper discusses a non-oriented approach based on Natural Language Processing and Knowledge Representation techniques - it does not require learning a lexical field and can therefore be used to evaluate odors with non-specialist evaluators...|$|E
40|$|Aims: Oak {{decline is}} a complex phenomenon, {{characterized}} by symptoms of canopy transparency, bark cracks and root biomass reduction. Root health status {{is one of the}} first stress indicators, and root turnover is a key process in plant adaptation to unfavourable conditions. In this study, the combined effects of decline and thinning were evaluated on fine root dynamics in an oak forest adjoining the Italian Pre-Alps by comparison of acute declining trees with non-declining trees, both with and without thinning treatment of surrounding trees. Methods: Dynamics of volumetric root length density (RLDV) and tip density (RTDV), root tip density per unit length of root (RTDL), diameter, branching index (BI) and mycorrhizal colonization were monitored by soil coring over 2 years as <b>possible</b> <b>descriptors</b> of decline. Key Results: At the beginning of the experiment, the relationship between canopy transparency and root status was weak, declining trees having slightly lower RLDV (– 20...|$|E
40|$|Today, {{there is}} a greater {{interest}} in the marketing world in using neuroimaging tools to evaluate the efficacy of TV commercials. This field of research is known as neuromarketing. In this article, we illustrate some applications of electrical neuroimaging, a discipline that uses electroencephalography (EEG) and intensive signal processing techniques for the evaluation of marketing stimuli. We also show how the proper usage of these methodologies can provide information related to memorization and attention while people are watching marketing-relevant stimuli. We note that temporal and frequency patterns of EEG signals are able to provide <b>possible</b> <b>descriptors</b> that convey information about the cognitive process in subjects observing commercial advertisements (ads). Such information could be unobtainable through common tools used in standard marketing research. Evidence of this research shows how EEG methodologies could be employed to better design new products that marketers are going to promote and to analyze the global impact of video commercials already broadcast on TV. © 2012 IEEE...|$|E
40|$|Besides {{the usual}} {{parameters}} for softness (Fukui indexes) and hardness (atomic charges), novel parameters, {{obtained from the}} molecular electrostatic potential on the van der Waals surface (ESP-indexes), were considered as stereochemical and <b>possible</b> hardness <b>descriptors.</b> The effect of oxirane ring activation by electrophiles was modeled by scanning the aforementioned parameters over the epoxide ring opening in the neutral substrates and in two corresponding protonated derivatives. C(1) is a softer center than C(3), but oxirane activation rapidly levels softness and shifts hardness from C(3) to C(1); with carbocycles, C(1) and C(3) hardness are leveled, and with heterocycles, C(1) becomes harder than C(3). ESP-indexes are suitable parameters to account for stereoselectivity and more reliable hardness descriptors than the atomic charges. An explanation for the prevailing syn-C(1) -selectivity observed with glycals and iminoglycals epoxides versus the anti-C(3) -selectivity observed with carba analogs is given. A rationalization of Ferrier rearrangement is also provided...|$|R
40|$|A {{large portion}} of image {{contours}} is characterized by local properties such as sharp variations of the image intensity across the contour. However, the integration of local image descriptors estimated by using these local properties into curvilinear descriptors is a difficult problem from a theoretical perspective because of the combinatorially large number of <b>possible</b> curvilinear <b>descriptors.</b> An approach, based {{on the notion of}} compressibility of a graph, is proposed to deal with this issue. A linear-time multiscale algorithm is proposed which provably recovers contours with an upper bound on the approximation error. In the noise-free limit case, all contours are recovered with probability one. Keywords: multiscale edge detection, curve estimation, edge linking, grouping, perceptual organization, image analysis. Research supported by US Army grant DAAH 04 - 95 - 1 - 0494, Center for Imaging Science, and MURI grant DAAH 04 - 96 - 1 - 0445, Foundations of Performance Metrics for Object Recognitio [...] ...|$|R
40|$|Modern {{high speed}} and power {{machinery}} components like gears, bearings, pumps, hydraulics and motors normally suffer from wear phenomena during operation. The study of wear debris can help estimate {{the condition of the}} surface of a component, so its basic features may be used to diagnose component health prior to failure. In this paper, a review is presented of the current literature related to wear debris and its analysis. The basic features of wear debris are highlighted, and their possible potential to diagnose the health of machine components is discussed. The basic features of wear debris have been classified with respect to the approach of measurement for component health diagnostics. In addition, each feature has been detailed with its <b>possible</b> measurement <b>descriptors,</b> its trend during machine component operation, and its distinct health diagnostics capability. Finally the paper proposes advances in machine component health diagnostics solution, by optimising the diagnostic capabilities of basic wear debris features...|$|R
40|$|As {{olfactory}} perceptions {{vary from}} person to person, {{it is difficult to}} describe smells objectively. In contrast, electronic noses also detect smells with their sensors, but in addition describe those using electronic signals. Here we showed a virtual connection method between a human nose perceptions and electronic nose responses with the smell of standard gases. In this method, Amorphophallus titanum flowers, which emit a strong carrion smell, could objectively be described using an electronic nose, in a way resembling the skill of sommeliers. We could describe the flower smell to be close to that of a mixture of methyl mercaptan and propionic acid, by calculation of the dilution index from electronic resistances. In other words, the smell resembled that of “decayed cabbage, garlic and pungent sour” with <b>possible</b> <b>descriptors.</b> Additionally, we compared the smells of flowers which bloomed on different dates and at different locations and showed the similarity of odor intensities visually, in standard gas categories. We anticipate our assay to be a starting point for a perceptive connection between our noses and electronic noses...|$|E
40|$|AbstractSolar {{activity}} is a space-time complex {{of events that}} are produced by Sun magnetic fields. One {{of the results of}} this activity are solar flares. The solar flares occurs mainly in the areas with especially strong magnetic fields called Active Regions (AR). Observation phenomenology indicates that significant change in the magnetic field topology precedes strong flares. Now high frequency temporal sequences of AR magnetograms containing flares are available from the space observatory Solar Dynamics Observatory (SDO). We analyzed them to investigate changes in complexity by using methods of computational topology. As <b>possible</b> <b>descriptors</b> of flares we used topological invariants: the Euler characteristics and Betti numbers. These characteristics of course do not pretend to be the comprehensive description of topological complexity but they are simple in construction and intuitively clear. We found that the large variation of the Betti numbers and Euler characteristics are preceded or accompanied by a large flares. These results give us hope that approach based on computational topology could be useful in the task of monitoring magnetic field evolution and should be developed in futur...|$|E
40|$|The Broensted and Lowry acid base {{theory is}} based on the {{capacity}} of proton donation or acceptance (in {{the presence or absence of}} a solvent) whereas the Lewis acid base theory {{is based on}} the propensity of electron pair acceptance or donation. We explore through DFT calculation the obvious question whether these two theories are in conformity with each other. We use pKa as the descriptor for the Broensted and Lowry acidity. The DFT descriptors like ionization potential, electron affinity, electronegativity, hardness and global electrophilicity are computed for 58 organic and inorganic acids. The fractional electron transfer, del(N) and the associated energy change, del(E) for the reaction of these acids with trimethyl amine (a strong base) are used as the <b>possible</b> <b>descriptors</b> for the Lewis acidity. A near exponential decrease in del(N) and (-del(E)) values is observed in general with an increase in pKa values. The findings reveal that a stronger Broensted acid in most cases behaves as a stronger Lewis acid as well. However it is not necessarily true for all acids. Comment: 17 pages, 6 figures, 2 table...|$|E
40|$|An algal {{assemblage}} {{growing on}} artificial substrata of fish-farm cages was investigated. Specifically, algal {{response to the}} effects of fish-farm facilities was studied, in order to identify a <b>possible</b> future <b>descriptor</b> of biodeposition impact. Some sites were positioned upstream of the farms (at least 750 m; ‘controls’) and other sites were positioned downstream of the farms (‘impacts’). All sites were situated in the Tyrrhenian Sea. Control and impact sites differed significantly with regard to the dissolved nutrient profile. The fouling community (samples were scraped from buoys) displayed a reduction gradient in diversity which increased with the effect of fish farms. A total of 51 taxa were identified (three Cyanophyceae, three Phaeophyceae, five Bacillariophyceae, three Chlorophyceae, six Ulvophyceae and 31 Rhodophyceae), with a dominance of opportunistic species (with r strategy). A general increase in values of the Rhodophyceae by Phaeophyceae ratio (R/P) were recorded, indicating a remarkable impact of nutrient enrichment from fish culture facilities on an algal community structure...|$|R
40|$|Abstract. We {{investigate}} image retrieval using interest point descrip-tors. New {{geographic information}} {{systems such as}} Google Earth and Microsoft Virtual Earth are providing increased access to remote sensed imagery. Content-based access to this data would support a much richer interaction than is currently <b>possible.</b> Interest point <b>descriptors</b> have proven surprisingly effective {{for a range of}} computer vision problems. We investigate their application to performing similarity retrieval in a ground-truth dataset manually constructed from 1 -m IKONOS satellite imagery. We compare results of using quantized versus full descriptors, Euclidean versus Mahalanobis distance measures, and methods for com-paring the sets of descriptors associated with query and target images. ...|$|R
40|$|The {{shape of}} objects seen in images {{depends on the}} viewpoint. This effect confounds recognition. We {{demonstrate}} a theoretical framework within which it is <b>possible</b> to construct <b>descriptors</b> for both curves and surfaces, which do not vary with viewpoint. These descriptors are known as invariants. We use this framework to construct invariant shape descriptors for plane curves. These invariant shape <b>descriptors</b> make it <b>possible</b> to recognise plane curves, without explicitly determining {{the relationship between the}} curve reference frame and the camera coordinate system, and can be used to index quickly and efficiently into a large model base of curves. Many of these ideas are demonstrated by experiments on real image data...|$|R
40|$|The {{main focus}} of this thesis is on the analysis, via simplification, {{estimation}} and clas-sification, of discrete geometric objects using methods from discrete and combinatorial geometry. Geometric objects are ubiquitous in computing today, with uses in areas from GIS to structural molecular biology to graphics and visualization. Usually computation on geometric objects involves several different aspects of their shapes. A first issue is compact and effective shape representation: given a geometric object, how to store it so as to enable efficient manipulation and querying. For example, while the inherently discrete nature of computers allow for only an approximate representation, one would further like to simplify objects (such as polygonal curves) for reducing storage and fast processing. Once we have an efficient representation of objects, it becomes possible to define and compute various geometric attributes, shape descriptors, over them. For example, <b>possible</b> <b>descriptors</b> of point sets are their diameter, depth distribution, spread and so on. Using these shape de-scriptors, it is natural to compare the various qualities of two different geometric objects, via shape matching and shape clustering, for classifying these objects. In this thesis, I wil...|$|E
3000|$|... 1 and robustness. We use {{specificity}} {{to refer}} to distinctiveness, i.e. {{the ability of a}} feature descriptor to distinguish between different regions with almost similar appearance. In the presence of perturbations caused by e.g. noise and varying density, local regions change appearance. If we consider the set of <b>possible</b> <b>descriptors</b> of a feature as a high-dimensional manifold, 2 such perturbations cause a displacement of the feature vector on the manifold. However, the discrete binning that is performed while building a histogram-based descriptor causes a discretization of this manifold, which prevents movement along the manifold under small perturbations. Thus, if a feature is robust, the discretization of this manifold is coarser, allowing for more perturbations. In other words, the feature is able to maintain invariance to a higher degree of perturbations. All LRF-based descriptors (USC, SHOT and RoPS) are expected to show a higher degree of specificity, but at the expense of a decreased robustness in the presence of noise. Additionally, as we will show in [...] "Matching accuracy" [...] section, these descriptors significantly degrade under occlusions, which will occur when a full model is matched with a scene view. For a comprehensive overview of different feature characteristics, we also refer to the analysis in Salti et al. (2014).|$|E
40|$|Copyright © 2011 Giovanni Vecchiato et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Here we present an overview of some published papers of interest for the marketing research employing electroencephalogram (EEG) and magnetoencephalogram (MEG) methods. The interest for these methodologies relies in their high-temporal resolution {{as opposed to the}} investigation of such problem with the functional Magnetic Resonance Imaging (fMRI) methodology, also largely used in the marketing research. In addition, EEG and MEG technologies have greatly improved their spatial resolution in the last decades with the introduction of advanced signal processing methodologies. By presenting data gathered through MEG and high resolution EEG we will show which kind of information it is possible to gather with these methodologies while the persons are watching marketing relevant stimuli. Such information will be related to the memorization and pleasantness related to such stimuli. We noted that temporal and frequency patterns of brain signals are able to provide <b>possible</b> <b>descriptors</b> conveying information about the cognitive and emotional processes in subjects observing commercial advertisements. These information could be unobtainable through common tools used in standard marketing research. We also show an example of how an EE...|$|E
40|$|Work on web query {{analysis}} {{has attempted to}} infer the information goals of search users behind the queries. In this thesis, we propose a faceted classification scheme for web queries. Unlike previous work, our functional scheme ties its classification to actionable strategies for search engines to take. The scheme consists of four facets of ambiguity, authority sensitivity, temporal sensitivity and spatial sensitivity. We hypothesize that the classification of queries into such facets yields insight on user intent and information needs. We demonstrate how to bridge our taxonomy to existing work on web query classification. To validate the classification scheme, we asked users to annotate queries with respect to our facets and obtained high agreement. We also assess the coverage of our faceted classification on {{a random sample of}} queries from logs. Finally, we discuss the algorithmic approaches we take in our current work to automate such faceted classification. In particular, we present our approach to characterize the authority sensitivity facet of web queries, which {{to the best of our}} knowledge, has not been studied before. We implemented and evaluated a prototype system to automatically classify this facet of queries and result shows that automatic classification is <b>possible.</b> Subject <b>Descriptors...</b>|$|R
40|$|Pixel-based scale {{saliency}} (PSS) work bases {{on information}} estimation of data content and structure in multiscale analysis; its theoretical aspects {{as well as}} practical implementation are discussed by Kadir et al [11]. Scale Saliency framework [10] does not work only for pixels but other basis-projected descriptors as well. While wavelet atoms, localization in both time and frequency domain, are <b>possible</b> alternative <b>descriptors,</b> no theoretical analysis and practical solutions have been proposed yet. Our contribution is introducing a mathematical model of utilizing wavelet-based descriptors in a correspondent Wavelet-based Scale Saliency (WSS). It treats wavelet sub-band energy density of two popular discrete wavelet transform (DWT) and dual-tree complex wavelet transform (DTCWT) as basis descriptors instead of pixel-value descriptors for saliency map estimation. Then, ROC, AUC, and NSS quantitative analysis are comparing WSS against PSS {{as well as other}} state-of-the-art saliency methods ITT [9], SUN [18], SRS [8] on N. Bruce 2 ̆ 7 s database [4] with human eye-tracking data as ground-truth. Furthermore, qualitative results, different saliency maps, are analyzed case by case for their pros and cons; especially their short-comings in specific situation or insensible results for human perception...|$|R
40|$|Abstract: An expert’s panel {{according}} to the official method of International Olive Oil Council analyzed the organoleptic profile of the oils produced during one single harvesting season from 57 cultivars of the Tuscan germoplasm. The oils were grouped in three different clusters depending {{on the presence of}} different organoleptic features. The identified clus-ters of the extra virgin olive oils were compared for maturity index of the fruits and for the phenols content and acidic pro-file. Since phenols content and acidic profile depend on the genetic matrix hence the cultivar played a fundamental role on the overall quality. The majority of the varieties produced oils with the attributes requested by the local Protected Origin while others induced sensations, at the moment, not considered in the disciplinary of production like tomato or herbs. The presence of peculiar organoleptic attributes like tomato, sweet pepper, camomile or others is not common within the olive oils and they are present only to few varieties. The list of <b>possible</b> positive <b>descriptors</b> for the extra virgin olive oils could be enlarged or better specified including the terms of melon and chestnut flour. We proposed also the olfactory sensation of mushroom as reminiscent of the typical and pleasant odour of fresh cut mushroom as a possible new negative attribute of the extra virgin olive oil induced by the presence of antrachnose (Colletotrichum gloesporioides; Glomerella cingulata) on the olive fruits...|$|R
40|$|Here {{we present}} an {{overview}} of some published papers of interest for the marketing research employing electroencephalogram (EEG) and magnetoencephalogram (MEG) methods. The interest for these methodologies relies in their high-temporal resolution {{as opposed to the}} investigation of such problem with the functional Magnetic Resonance Imaging (fMRI) methodology, also largely used in the marketing research. In addition, EEG and MEG technologies have greatly improved their spatial resolution in the last decades with the introduction of advanced signal processing methodologies. By presenting data gathered through MEG and high resolution EEG we will show which kind of information it is possible to gather with these methodologies while the persons are watching marketing relevant stimuli. Such information will be related to the memorization and pleasantness related to such stimuli. We noted that temporal and frequency patterns of brain signals are able to provide <b>possible</b> <b>descriptors</b> conveying information about the cognitive and emotional processes in subjects observing commercial advertisements. These information could be unobtainable through common tools used in standard marketing research. We also show an example of how an EEG methodology could be used to analyze cultural differences between fruition of video commercials of carbonated beverages in Western and Eastern countries. © 2011 Giovanni Vecchiato et al...|$|E
40|$|To {{keep pace}} with its rapid {{development}} an efficient approach for the risk assessment of nanomaterials is needed. Grouping concepts as developed for chemicals are now being explored for its applicability to nanomaterials. One of the recently proposed grouping systems is DF 4 nanoGrouping scheme. In this study, we have developed three structure-activity relationship classification tree models {{to be used for}} supporting this system by identifying structural features of nanomaterials mainly responsible for the surface activity. We used data from 19 nanomaterials that were synthesized and characterized extensively in previous studies. Subsets of these materials have been used in other studies (short-term inhalation, protein carbonylation, and intrinsic oxidative potential), resulting in a unique data set for modeling. Out of a large set of 285 <b>possible</b> <b>descriptors,</b> we have demonstrated that only three descriptors (size, specific surface area, and the quantum-mechanical calculated property ‘lowest unoccupied molecular orbital’) need to be used to predict the endpoints investigated. The maximum number of descriptors that were finally selected by the classification trees (CT) was very low– one for intrinsic oxidative potential, two for protein carbonylation, and three for NOAEC. This suggests that the models were well-constructed and not over-fitted. The outcome of various statistical measures and the applicability domains of our models further indicate their robustness. Therefore, we conclude that CT can be a useful tool within the DF 4 nanoGrouping scheme that has been proposed before. </p...|$|E
30|$|Machine {{learning}} {{involves the}} process of training a classifier to distinguish classes, using a large collection of high-quality accurately-labelled data samples. A variety of benchmark datasets exist online that include manually-crafted annotations and class labels, however for many real-world applications {{the process of}} collecting and annotating such large datasets by hand is neither practical nor feasible. As a motivating example, the popular MNIST digit classification dataset [1] consists of 60, 000 training images and 10, 000 test images of digits in the range 0 to 9. A human working at {{a response rate of}} 1 second/image would require 1000 min (over 16 h) to label this training data. Even a ‘skilled’ labeller with a response rate of 0.5 second/image would require over 8 h to perform this task, working continuously with no breaks. Clearly this is impractical for any human to perform, and the reliability of labels would most likely reduce over time. This example clearly highlights the issues of time and effort, and also human fatigue and reliability. Another factor to consider is understanding how the training samples contribute towards the accuracy of the classifier. Many batch learning systems take the approach of ‘more data is better’, without acknowledging that some samples that may be included could reduce performance (e.g., poorly-written digits in the MNIST example), or cases where the algorithm could be fooled by the input. Equally, since the learning features are typically just a subset of <b>possible</b> <b>descriptors</b> drawn from the real-world artefact, then it is vital to understand rapidly if the chosen features adequately represent those artefacts, to avoid extensive effort creating datasets that may never have sufficient discriminative power to construct a suitable classifier [2].|$|E
40|$|Developing more {{reactive}} {{and selective}} catalysts for petrochemical refining and synthesis, specifically the dehydrogenation of propane to form propylene, {{is extremely important}} for the US and global economy. Studies have suggested that sub-nanometer transition metal (TM) clusters can be synthesized with superior properties {{to those of the}} bulk metal. The use of alloy clusters can potentially tune their catalytic properties for specific reactions. Using computational methods, the dehydrogenation reaction pathways of possible catalysts with propane can be tested; however, this can be very time consuming. By looking at <b>possible</b> simple <b>descriptors</b> of promising catalytic behavior, the time spent testing specific alloys can be greatly reduced. We have found a possible correlation between the hydrogen binding energy of a four-atom TM cluster and its activation energy for the rate-limiting step of the propane dehydrogenation reaction. If such a correlation exists, the far less time consuming calculation of hydrogen binding energy could provide a clear indicator of catalytic activity. We have calculated the hydrogen binding energies for M 4 (M = TM atom) as well as Pt 4 -x Mx alloy clusters, (x = 0 - 3) and have tried to understand how differences in these energies are related to properties of the clusters such as ionization energies, electron affinities, and cohesive energies. We report our results and their implications for the design of new dehydrogenation catalysts...|$|R
40|$|Multiple linear {{regression}} analysis {{was performed on the}} quantitative structure-activity relationships (QSAR) of the triazoloquinazoline adenosine antagonists for human A 3 receptors. The data set used for the QSAR analysis encompassed the activities of 33 triazoloquinazoline derivatives and 72 physicochemical descriptors. A template moleculewas derived using the known molecular structure for one of the compounds when bound tothe human A 2 B receptor, in which the amide bond was in a cis-conformation. All the testcompounds were aligned to the template molecule. In order to identify a reasonable QSARequation to describe the data set, we developed a multiple {{linear regression}} program thatexamined every <b>possible</b> combination of <b>descriptors.</b> The QSAR equation derived from thisanalysis indicates that the spatial and electronic effects is greater than that of hydrophobiceffects in binding of the antagonists to the human A 3 receptor. It also predicts that a largesterimol length parameter is advantageous to activity, whereas large sterimol widthparameters and fractional positive partial surface areas are nonadvatageous...|$|R
40|$|AbstractUrban buses {{propose a}} {{challenge}} for traditional four-steps models of ridership estimation, as they require a different, closer scale approach, including the consideration of multiple possible stop-choices by travelers within walking distance. Thus, any model based on zoning and the bias of associating population to the nearest stop does not seem coherent {{in the case of}} urban bus. This study empirically examines the potential of <b>possible</b> ‘attraction’ <b>descriptors,</b> such as spatial integration (as described by the Space Syntax methodology) and other urban environment factors in order to estimate urban buses ridership by a direct forecast model based on multiple linear regression. Common explanatory factors found in the literature include population and employment in the vicinity area, as well as transport system service and performance. Some authors have claimed the predictive power of built environment variables (summarized by Cervero and Kockelman's three Ds: Density, Diversity and Design), which are supposed to describe pedestrian accessibility and attractiveness. This paper proposes that spatial-configurational measures (e. g. Space Syntax) could play an important role, given that these factors have proved themselves synthetic proxies for many urban processes and in order to describe spatial-configurational hierarchy and consequent attraction power. A demand forecast model at a stop level is explored by means of multiple linear regressions. Bus transport ridership at 84 stops in Madrid is forecasted using urban environment and spatial integration variables, as well as transport network accessibility indicators. Results seem encouraging and support that Space Syntax and other network integration variables could be an important asset for urban bus demand forecast models at a station level...|$|R
