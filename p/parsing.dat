10000|7254|Public
5|$|In {{his book}} A New Kind of Science, Stephen Wolfram {{points out that}} rule 184, when run on {{patterns}} with density 50%, {{can be interpreted as}} <b>parsing</b> the context free language describing strings formed from nested parentheses. This interpretation is closely related to the ballistic annihilation view of rule 184: in Wolfram's interpretation, an open parenthesis corresponds to a left-moving particle while a close parenthesis corresponds to a right-moving particle.|$|E
5|$|Perl has a Turing-complete grammar because <b>parsing</b> can be {{affected}} by run-time code executed during the compile phase. Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.|$|E
5|$|Perl takes lists from Lisp, hashes ("associative arrays") from AWK, {{and regular}} {{expressions}} from sed. These simplify and facilitate many <b>parsing,</b> text-handling, and data-management tasks. Also shared with Lisp are the implicit {{return of the}} last value in a block, {{and the fact that}} all statements have a value, and thus are also expressions and can be used in larger expressions themselves.|$|E
5000|$|A <b>Parse</b> Thicket is a graph that {{represents}} the syntactic {{structure of a paragraph}} of text in natural language processing. A <b>Parse</b> Thicket includes <b>Parse</b> tree for each sentence for this paragraph plus some arcs for other relations between words other than syntactic. <b>Parse</b> thickets can be constructed for both constituency <b>parse</b> trees and dependency <b>parse</b> trees. The relations which link <b>parse</b> trees within a <b>Parse</b> Thicket are: ...|$|R
40|$|Data-driven parsers rely on {{recommendations}} from <b>parse</b> models, which are generated from {{a set of}} training data using a machine learning classifier, to perform <b>parse</b> operations. However, in some cases a <b>parse</b> model cannot recommend a <b>parse</b> action to a parser unless it learns from the training data what <b>parse</b> action(s) to take in every possible situation. Therefore, {{it will be hard}} for a parser to make an informed decision as to what <b>parse</b> operation to perform when a <b>parse</b> model recommends no/several <b>parse</b> actions to a parser. Here we examine the effect of various deterministic choices on a datadriven parser when it is presented with no/several recommendation from a <b>parse</b> model...|$|R
5000|$|Reductions reorganize {{the most}} {{recently}} <b>parsed</b> things, {{immediately to the}} left of the lookahead symbol. So the list of already-parsed things acts like a stack. This <b>parse</b> stack grows rightwards. The base or bottom of the stack is on the left and holds the leftmost, oldest <b>parse</b> fragment. Every reduction step acts only on the rightmost, newest <b>parse</b> fragments. (This accumulative <b>parse</b> stack is very unlike the predictive, leftward-growing <b>parse</b> stack used by top-down parsers.) ...|$|R
5|$|The Perl {{languages}} borrow features {{from other}} programming languages including C, shell script (sh), AWK, and sed. They provide powerful text processing facilities without the arbitrary data-length limits of many contemporary Unix commandline tools, facilitating easy manipulation of text files. Perl 5 gained widespread {{popularity in the}} late 1990s as a CGI scripting language, {{in part due to}} its then unsurpassed regular expression and string <b>parsing</b> abilities.|$|E
5|$|Jefferson, on {{the other}} hand, took a stricter view of the Constitution: <b>parsing</b> the text carefully, he found no {{specific}} authorization for a national bank. This controversy was eventually settled by the Supreme Court of the United States in McCulloch v. Maryland, which in essence adopted Hamilton's view, granting the federal government broad freedom to select the best means to execute its constitutionally enumerated powers, specifically the doctrine of implied powers. Nevertheless, the American Civil War and the Progressive Era demonstrated the sorts of crises and politics Hamilton's administrative republic sought to avoid.|$|E
25|$|Like {{with some}} {{programming}} languages Lojban grammar can be parsed using <b>parsing</b> expression grammars.|$|E
5000|$|A <b>parsed</b> entity {{contains}} text, {{which will}} {{be incorporated into the}} document and <b>parsed</b> if the entity is referenced. A parameter entity can only be a <b>parsed</b> entity.|$|R
40|$|Abstract In {{this paper}} we propose and {{evaluate}} a method for locating causes of ambiguity in context-free grammars by automatic analysis of <b>parse</b> forests. A <b>parse</b> forest is the set of <b>parse</b> trees of an ambiguous sentence. Deducing causes of ambiguity from observing <b>parse</b> forests is hard for grammar engineers because of (a) {{the size of the}} <b>parse</b> forests, (b) the complex shape of <b>parse</b> forests, and (c) the diversity of causes of ambiguity. We first analyze the diversity of ambiguities in grammars for programming languages and the diversity of solutions to these ambiguities. Then we introduce Dr. Ambiguity: a <b>parse</b> forest diagnostics tools that explains the causes of ambiguity by analyzing differences between <b>parse</b> trees and proposes solutions. We demonstrate its effectiveness using a small experiment with a grammar for Java 5. ...|$|R
5000|$|At step 6 in {{the example}} <b>parse,</b> only [...] "A*2" [...] has been <b>parsed,</b> incompletely. Only the shaded lower-left corner of the <b>parse</b> tree exists. None of the <b>parse</b> tree nodes {{numbered}} 7 and above exist yet. Nodes 3, 4, and 6 are the roots of isolated subtrees for variable A, operator *, and number 2, respectively. These three root nodes are temporarily held in a <b>parse</b> stack. The remaining unparsed portion of the input stream is [...] "+ 1".|$|R
25|$|It finds {{applications}} to diverse problems, including {{the problem of}} <b>parsing</b> using stochastic grammars in NLP.|$|E
25|$|Note: Spaces are not ordinarily used in Japanese, {{but they}} are {{supplemented}} here to facilitate <b>parsing</b> by non-speakers of the language.|$|E
25|$|LL(k) and LL(*) grammars allow <b>parsing</b> {{by direct}} {{construction}} of a leftmost derivation as described above, and describe even fewer languages.|$|E
5000|$|State 2 in {{the example}} <b>parse</b> table is for the {{partially}} <b>parsed</b> rule ...|$|R
40|$|<b>Parser</b> {{disambiguation}} {{with precision}} grammars generally takes place via statistical ranking of the <b>parse</b> yield of the grammar using a supervised <b>parse</b> selection model. In the standard process, the <b>parse</b> selection model is trained over a hand-disambiguated treebank, meaning {{that without a}} significant investment of effort to produce the treebank, <b>parse</b> selection is not possible. Furthermore, as treebanking is generally streamlined with <b>parse</b> selection models, creating the initial treebank without a model requires more resources than subsequent treebanks. In this work, we show that, {{by taking advantage of}} the constrained nature of these HPSG grammars, we can learn a discriminative <b>parse</b> selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. ...|$|R
30|$|Step 1 : <b>parse</b> input {{text and}} analyze {{grammatical}} structure using the Stanford <b>Parser.</b>|$|R
25|$|LR(k) grammars (also {{known as}} {{deterministic}} context-free grammars) allow <b>parsing</b> (string recognition) with deterministic pushdown automata (PDA), {{but they can}} only describe deterministic context-free languages.|$|E
25|$|He lists {{four major}} Japanese problems: word order, <b>parsing</b> which Chinese {{characters}} {{should be read}} together, deciding how to pronounce the characters, and finding suitable equivalents for Chinese function words.|$|E
25|$|Simple LR, Look-Ahead LR grammars are subclasses {{that allow}} further {{simplification}} of <b>parsing.</b> SLR and LALR are recognized {{using the same}} PDA as LR, but with simpler tables, in most cases.|$|E
50|$|Join the L <b>parse</b> trees {{together}} as one <b>parse</b> tree with new root symbol Lhs.|$|R
50|$|<b>Parse</b> tree: A <b>parse</b> tree {{displays}} a derivation, showing the syntactic {{structure of a}} sentence.|$|R
40|$|A {{practical}} guide, featuring step-by-step instructions {{showing you}} how to use <b>Parse</b> iOS, and handle your data on cloud. If you are a developer who wants to build your applications instantly using <b>Parse</b> iOS as a back end application development, this book is ideal for you. This book will help you to understand <b>Parse,</b> featuring examples {{to help you get}} familiar with the concepts of <b>Parse</b> iOS...|$|R
25|$|Googlebot is {{described}} in some detail, but the reference is only about {{an early version of}} its architecture, which was based in C++ and Python. The crawler was integrated with the indexing process, because text <b>parsing</b> was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During <b>parsing,</b> the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server.|$|E
25|$|Mutated XSS happens, {{when the}} {{attacker}} injects {{something that is}} seemingly safe, but rewritten and modified by the browser, while <b>parsing</b> the markup. This makes it extremely hard to detect or sanitize within the websites application logic.|$|E
25|$|Many of the {{existing}} software measures count structural elements of the application that result from <b>parsing</b> the source code for such individual instructions (Park, 1992), tokens (Halstead, 1977), control structures (McCabe, 1976), and objects (Chidamber & Kemerer, 1994).|$|E
50|$|In gene {{expression}} programming the linear chromosomes {{work as the}} genotype and the <b>parse</b> trees as the phenotype, creating a genotype/phenotype system. This genotype/phenotype system is multigenic, thus encoding multiple <b>parse</b> trees in each chromosome. This means that the computer programs created by GEP are composed of multiple <b>parse</b> trees. Because these <b>parse</b> trees {{are the result of}} {{gene expression}}, in GEP they are called expression trees.|$|R
5000|$|Connectors {{can have}} an {{optional}} floating-point cost markup, so that some are [...] "cheaper" [...] to use than others, thus giving preference to certain <b>parses</b> over others. [...] That is, {{the total cost of}} <b>parse</b> is the sum of the individual costs of the connectors that were used; the cheapest <b>parse</b> indicates the most likely <b>parse.</b> This is used for parse-ranking multiple ambiguous <b>parses.</b> The fact that the costs are local to the connectors, and are not a global property of the algorithm makes them essentially Markovian in nature.|$|R
25|$|Two {{types of}} ambiguities can be distinguished. <b>Parse</b> tree {{ambiguity}} and structural ambiguity. Structural ambiguity {{does not affect}} thermodynamic approaches as the optimal structure selection {{is always on the}} basis of lowest free energy scores. <b>Parse</b> tree ambiguity concerns the existence of multiple <b>parse</b> trees per sequence. Such an ambiguity can reveal all possible base-paired structures for the sequence by generating all possible <b>parse</b> trees then finding the optimal one. In the case of structural ambiguity multiple <b>parse</b> trees describe the same secondary structure. This obscures the CYK algorithm decision on finding an optimal structure as the correspondence between the <b>parse</b> tree and the structure is not unique. Grammar ambiguity can be checked for by the conditional-inside algorithm.|$|R
25|$|The {{number of}} words in the British National Corpus (ca 100 million) is {{sufficient}} for many empirical strategies for learning about language for linguists and lexicographers, and is satisfactory for technologies that utilize quantitative information about the behavior of words as input (<b>parsing).</b>|$|E
25|$|For {{computer}} programming languages, the reference grammar is often ambiguous, due to {{issues such as}} the dangling else problem. If present, these ambiguities are generally resolved by adding precedence rules or other context-sensitive <b>parsing</b> rules, so the overall phrase grammar is unambiguous.|$|E
25|$|The {{construct}} {{that represents}} side effects {{is an example}} of a monad. Monads are a general framework that can model different kinds of computation, including error handling, nondeterminism, <b>parsing</b> and software transactional memory. Monads are defined as ordinary datatypes, but Haskell provides some syntactic sugar for their use.|$|E
50|$|Remove the matched topmost L symbols (and <b>parse</b> {{trees and}} {{associated}} state numbers) from the <b>parse</b> stack.|$|R
5000|$|... {{opening tag}} and closing tag that {{surrounds}} {{what is to}} be <b>parsed.</b> ie. [...] (<b>parses</b> to '007') ...|$|R
5000|$|The {{following}} commands {{will generate}} <b>parse</b> trees for the given phrases {{and open the}} produced PNG image using the system's eog command.> <b>parse</b> -lang=Eng [...] "John loves Mary" [...] | visualize_parse -view="eog"> <b>parse</b> -lang=Dut [...] "Jan heeft Marie lief" [...] | visualize_parse -view="eog" ...|$|R
