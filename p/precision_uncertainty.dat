16|141|Public
40|$|Development of an optical, {{laser-based}} flow-field {{measurement technique}} for large wind tunnels is described. The technique uses laser sheet illumination and charged coupled device detectors to rapidly measure flow-field velocity distributions over large planar {{regions of the}} flow. Sample measurements are presented that illustrate the capability of the technique. An analysis of measurement uncertainty, which focuses on the random component of uncertainty, shows that <b>precision</b> <b>uncertainty</b> is not dependent on the measured velocity magnitude. For a single-image measurement, the analysis predicts a <b>precision</b> <b>uncertainty</b> of +/- 5 m/s. When multiple images are averaged, this uncertainty is shown to decrease. For an average of 100 images, for example, the analysis shows that a <b>precision</b> <b>uncertainty</b> of +/- 0. 5 m/s can be expected. Sample applications show that vectors aligned with an orthogonal coordinate system are difficult to measure directly. An algebraic transformation is presented which converts measured vectors to the desired orthogonal components. Uncertainty propagation is then used {{to show how the}} uncertainty propagates from the direct measurements to the orthogonal components. For a typical forward-scatter viewing geometry, the propagation analysis predicts precision uncertainties of +/- 4, +/- 7, and +/- 6 m/s, respectively, for the U, V, and W components at 68 % confidence...|$|E
40|$|An {{uncertainty}} {{analysis performed}} {{in conjunction with}} the calibration of a subsonic venturi for use in a turbine test facility produced some unanticipated results that may have a significant impact in a variety of test situations. <b>Precision</b> <b>uncertainty</b> estimates using the preferred propagation techniques in the applicable American National Standards Institute/American Society of Mechanical Engineers standards were an order of magnitude larger than <b>precision</b> <b>uncertainty</b> estimates calculated directly from a sample of results (discharge coefficient) obtained at the same experimental set point. The differences were attributable to the effect of correlated precision errors, which previously have been considered negligible. An analysis explaining this phenomenon is presented. The article is not meant to document the venturi calibration, but rather to give a real example of results where correlated precision terms are important. The significance of the correlated precision terms could apply to many test situations...|$|E
40|$|The article {{presents}} a comparative {{analysis of the}} U. S. strategy “New Silk Road” and the project of China to establish economic belt “Silk Road”, {{as well as their}} implementation in Central Asia. However, the unstable situation in Afghanistan, the lack of sources of financing capital-intensive infrastructure projects, the existence of various contradictions between the countries of this region and other reasons prevent the full implementation of the U. S. strategy. At the same time, the Chinese project faces a lack of <b>precision</b> <b>uncertainty</b> of funding...|$|E
50|$|As {{the orbital}} {{elements}} are known {{with a limited}} <b>precision,</b> the <b>uncertainties</b> may lead to false positives (i.e. classification as resonant of an orbit which is not).|$|R
40|$|This paper {{describes}} {{a method to}} determine the uncertainties of measured forces and moments from multi-component force balances used in wind tunnel tests. A multivariate regression technique is first employed to estimate the uncertainties of the six balance sensitivities and 156 interaction coefficients derived from established balance calibration procedures. These uncertainties are then employed to calculate the uncertainties of force-moment values computed from observed balance output readings obtained during tests. Confidence and prediction intervals are obtained for each computed force and moment as functions of the actual measurands. Techniques are discussed for separate estimation of balance bias and <b>precision</b> <b>uncertainties...</b>|$|R
5000|$|... {{then the}} observables A and B can be {{measured}} simultaneously with infinite <b>precision</b> i.e. <b>uncertainties</b> , [...] simultaneously. ψ is then {{said to be the}} simultaneous eigenfunction of A and B. To illustrate this: ...|$|R
40|$|We analyze how markets {{adjust to}} new {{information}} when {{the reliability of}} news is uncertain {{and has to be}} estimated itself. We propose a Bayesian learning model where market participants receive fundamental information along with noisy estimates of news' precision. It is shown that the efficiency of a precision estimate drives the the slope and the shape of price response functions to news. Increasing estimation errors induce stronger nonlinearities in price responses. Analyzing high-frequency reactions of Treasury bond futures prices to employment releases, we find strong empirical support for the model's predictions and show that the consideration of <b>precision</b> <b>uncertainty</b> is statistically and economically important...|$|E
40|$|An {{experimental}} {{investigation of}} hypergolicity and ignition delay of fuel mixtures with hydrogen peroxide is presented. Example results of {{high speed photography}} and schleiren from drop tests are shown. Also, {{a discussion of the}} sensitivity to experimental parameters such as drop size and subsequent uncertainty considerations of ignition delay results is presented. It is shown that using the described setup on the mixtures presented, the <b>precision</b> <b>uncertainty</b> is on the order of 6 % of average ignition delay and 5 % of average decomposition delay. This represents sufficient repeatability for first order discrimination of ignition delay for propellant development and screening. Two mixtures, each using commonly available amines and transition metal compounds, are presented as examples that result in ignition delays on the order of 10 milliseconds...|$|E
40|$|Abstract. This paper {{introduces}} a new inversion algorithm for retrievals of stratospheric BrO from the Aura Microwave Limb Sounder. This version {{is based on}} the algorithm de-scribed by Livesey et al. (2006 a) but uses a more realis-tic atmospheric state to constrain the retrieval. A descrip-tion of the methodology and an error analysis are presented. Single daily profile <b>precision</b> <b>uncertainty,</b> when taking the ascending-descending (day-night) difference, was found to be up to 40 pptv while systematic error biases were esti-mated to be less than about 3 pptv. Monthly mean compar-isons show broad agreement with other measurements as well as with state-of-the-art numerical models. We infer a 2005 yearly total inorganic Bry using the measured MLS BrO to be 20. 3 ± 4. 5 pptv, which implies a contribution from very short lived substances to the stratospheric bromine budget of 5 ± 4. 5 pptv. ...|$|E
40|$|Experimental {{aerodynamic}} researchers require estimated <b>precision</b> and bias <b>uncertainties</b> of measured physical quantities, typically at 95 percent confidence levels. Uncertainties {{of final}} computed aerodynamic parameters are obtained by propagation of individual measurement uncertainties through the defining functional expressions. In this paper, rigorous mathematical techniques are extended to determine <b>precision</b> and bias <b>uncertainties</b> of any instrument-sensor system. Through this analysis, instrument uncertainties determined through calibration are now expressed as {{functions of the}} corresponding measurement for linear and nonlinear univariate and multivariate processes. Treatment of correlated measurement precision error is developed. During laboratory calibration, calibration standard uncertainties {{are assumed to be}} an order of magnitude less than those of the instrument being calibrated. Often calibration standards do not satisfy this assumption. This paper applies rigorous statistical methods for inclusion of calibration standard uncertainty and covariance due to the order of their application. The effects of mathematical modeling error on calibration bias uncertainty are quantified. The effects of experimental design on uncertainty are analyzed. The importance of replication is emphasized, techniques for estimation of both bias and <b>precision</b> <b>uncertainties</b> using replication are developed. Statistical tests for stationarity of calibration parameters over time are obtained...|$|R
40|$|Uncertainty Analysis (UA) was {{performed}} on the results of the ship experiments. The objective is to quantify the level of uncertainty in the measured data. Both systematic (fixed) and random (<b>precision)</b> <b>uncertainties</b> associated with the measured ship motions, wave impact forces, and pressures were calculated. Potential uncertainty "error" sources were identified, and all significant elemental uncertainties were calculated. Both systematic and random uncertainties were added to compute the overall uncertainty. A discussion is presented regarding the effects and the contributions of various sources to the overall uncertainty as well as the propagation of the elemental uncertainties throughout all stages of the experimental program. Conclusions are reported and recommendations for future work are indicated. Peer reviewed: NoNRC publication: Ye...|$|R
50|$|The Standoff Detection Technology Evaluation {{facility}} allows {{researchers to}} release a known amount of material while maintaining a calibrated material scatter so that a standoff detector’s ability to “see” can be accurately measured from up to several kilometers away. This increased <b>precision</b> reduces <b>uncertainty</b> about the potential field performance of standoff detectors.|$|R
40|$|Information on ice is {{important}} for shipping, weather forecasting, and climate monitoring. Historically, ice cover has been detected and ice concentration has been measured using relatively low-resolution space-based passive microwave data. This study presents an algorithm to detect ice and estimate ice concentration in clear-sky areas over the ocean and inland lakes and rivers using high-resolution data from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar Orbiting Partnership (S-NPP) and on future Joint Polar Satellite System (JPSS) satellites, providing spatial detail that cannot be obtained with passive microwave data. A threshold method is employed with visible and infrared observations to identify ice, then a tie-point algorithm is {{used to determine the}} representative reflectance/temperature of pure ice, estimate the ice concentration, and refine the ice cover mask. The VIIRS ice concentration is validated using observations from Landsat 8. Results show that VIIRS has an overall bias of − 0. 3 % compared to Landsat 8 ice concentration, with a <b>precision</b> (<b>uncertainty)</b> of 9. 5 %. Biases and precision values for different ice concentration subranges from 0 % to 100 % can be larger...|$|E
40|$|Estimating {{the change}} in {{groundwater}} recharge from an introduced artificial recharge system is important in order to evaluate future water availability. This paper presents an inverse modeling approach to quantify the recharge contribution from both an ephemeral river channel and an introduced artificial recharge system based on floodwater spreading in arid Iran. The study used the MODFLOW- 2000 to estimate recharge for both steady and unsteady-state conditions. The model was calibrated and verified based on the observed hydraulic head in observation wells and model <b>precision,</b> <b>uncertainty,</b> and model sensitivity were analyzed in all modeling steps. The results showed that in a normal year without extreme events the floodwater spreading system is the main contributor to recharge with 80 % and the ephemeral river channel with 20 % of total recharge in the studied area. Uncertainty analysis revealed that the river channel recharge estimation represents relatively more uncertainty {{in comparison to the}} artificial recharge zones. The model is also less sensitive to the river channel. The results show that by expanding the artificial recharge system the recharge volume can be increased even for small flood events while the recharge through the river channel increases only for major flood events...|$|E
40|$|MkIV {{measurements}} of the volume mixing ratio (VMR) of HO 2 NO 2 at 35 deg N, sunset on Sept. 25, 1993 are given. Measurements of HO 2 NO 2 made between approx. 65 and 70 deg N, sunrise on May 8, 1997 are listed. The uncertainties given are 1 sigma estimates of the measurement <b>precision.</b> <b>Uncertainty</b> in the HO 2 NO 2 line strengths {{is estimated to be}} 20 %; this is the dominant contribution to the systematic error of the HO 2 NO 2 measurement. Model inputs for the simulations are given. The albedos were obtained from Total Ozone Mapping Spectrometer reflectively data (raw data at ftp://jwocky. gsfc. nasa. gov) for the time and place of observation. Profiles of sulfate aerosol surface area ("Surf. Area") were obtained from monthly, zonal mean profiles measured by SAGE II [Thomason et al., 1997 updated via private communication]. The profile of Be(y) is based on the Wamsley et al. relation with N 2 O, using MkIV {{measurements of}} N 20 O. All other model inputs given are based on direct MkIV measurements. Finally, we note the latitude of the MkIV tangent point varied considerably during sunrise on May 8, 1997. The simulations shown here were obtained using different latitudes for each altitude...|$|E
40|$|This paper {{presents}} the calibration results and uncertainty {{analysis of a}} high-precision reference pressure measurement system currently used in wind tunnels at the NASA Langley Research Center (LaRC). Sensors, calibration standards, and measurement instruments are subject to errors due to aging, drift with time, environment effects, transportation, the mathematical model, the calibration experimental design, and other factors. Errors occur at every link {{in the chain of}} measurements and data reduction from the sensor to the final computed results. At each link of the chain, bias and <b>precision</b> <b>uncertainties</b> must be separately estimated for facility use, and are combined to produce overall calibration and prediction confidence intervals for the instrument, typically at a 95 % confidence level. The uncertainty analysis and calibration experimental designs used herein, based on techniques developed at LaRC, employ replicated experimental designs for efficiency, separate estimation of bias a [...] ...|$|R
5000|$|Hepburn, Cameron J. and Farmer, Doyne. (2014). [...] "Less <b>Precision,</b> more truth: <b>Uncertainty</b> {{in climate}} {{economics}} and macroprudential policy", Bank of England 2 April 2014 - Programme.|$|R
25|$|Assuming {{that these}} reconstructions {{accurately}} reflect what Hipparchus wrote in On Sizes and Distances, then {{this work was}} a remarkable accomplishment. This approach of setting limits on an unknown physical quantity was not new to Hipparchus (see Aristarchus of Samos. Archimedes also {{did the same with}} pi), but in those cases, the bounds reflected the inability to determine a mathematical constant to an arbitrary <b>precision,</b> not <b>uncertainty</b> in physical observations.|$|R
40|$|A turbine {{efficiency}} measurement {{system has been}} developed and installed on the turbine test facility (TTF) at QinetiQ Farnborough. The TTF is an engine-scale short-duration (0. 5 s run time) rotating transonic facility, which can operate as either single stage (HP vane and rotor) or 1 1 / 2 stage (HP stage with IP or LP vane). The current MT 1 HP stage is highly loaded and unshrouded and is therefore relevant to current design trends. Implementation of the efficiency measurement system forms part of the EU Turbine Aero-Thermal External Flows (TATEF II) program. The following aspects of the efficiency measurement system are discussed in this paper: mass-flow rate measurement, power measurement by direct torque measurement, turbine inlet and exit area traverse measurement systems, computation of efficiency by mass weighting, and uncertainty analysis of the experimentally determined {{turbine efficiency}}. The calibration of the mass-flow rate and torque measurement systems are also discussed. Emphasis {{was placed on the}} need for a low efficiency <b>precision</b> <b>uncertainty,</b> so that changes in efficiency associated with turbine inlet temperature distortion and swirl can be resolved with good accuracy. Measurements with inlet flow distortion form part of the TATEF II program and will be the subject of forthcoming publications. We are not permitted to provide full text of this article at this time. You may be able to access the article via the publisher copy link above...|$|E
40|$|The United States Air Force (USAF) is {{continually}} {{looking for ways}} to improve test and evaluation techniques to ensure systems meet military requirements. Since 1997, Air Combat Command (ACC) has been successfully using Design of Experiments (DOE) to construct and analyze operational test efforts. This paper highlights recent Air Force Materiel Command (AFMC) efforts to pursue statistically defensible test techniques to aid developmental test efforts. Defensible testing is a statistical approach which encompasses DOE but also emphasizes the need for better test planning by insisting on understanding the system under test, by requiring clear and achievable test objectives, by ensuring system performance is measurable, and by requiring that instrumentation accuracy and uncertainty are well understood and by estimating risk in test planning and evaluations which sometimes require confidence and power calculations. This paper highlights the Air Force Flight Test Center’s (AFFTC) first steps to improve aircraft propulsion system developmental test and evaluation (T&E) through the implementation of statistically defensible test techniques. Specifically, this paper provides insight into test accuracy. Engineers typically misuse terminology to specify test accuracy (error, <b>precision,</b> <b>uncertainty,</b> confidence intervals, etc…). This paper will present a case study in which the engineer specifies test accuracy requirements in terms of margin of error using 95 -percent confidence intervals (CI’s). However, statisticians recommend using “Statistical Power ” to ensure desired accuracy is achieved. Which is best to achiev...|$|E
40|$|Nondestructive testing (NDT) is the {{integral}} part of the performance evaluation of flexible pavements. In all NDT methods, Falling Weight Deflectometer (FWD) is probably the most popular technique. Basically, it measures time-domain deflections from numerous road sections emerging by the applied impulse load. In order to characterize the structural integrity of considered pavement system, it is required to make an inversion for the calculation of mechanical pavement properties using a backcalculation tool covering both a forward pavement response model and an optimization algorithm. On the other hand, backcalculation problem can also be solved by an adaptive system using a supervised learning algorithm. In this manner, multilayer perceptron (MLP) and adaptive neuro-fuzzy system (ANFIS) methodologies, popular universal functional approximating techniques of Artificial Intelligence (AI), are appropriate for pavement backcalculation problem. Therefore, two-phased (forward and backward) structure of traditional backcalculation approaches is reduced into one step {{with the help of the}} supervised learning mechanisms of MLP and ANFIS. In this study, these methodologies are both employed to backcalculate mechanical properties of flexible pavements and compared in terms of modeling <b>precision,</b> <b>uncertainty</b> handling, computational expense, and data requirements. Results indicated that, both techniques are valid and have certain advantages over each other and should be preferred with respect to quantity and quality of the data at hand. In addition, AI-based supervised nonlinear mapping techniques not only exhibit precise backcalculation results, but also enable real-time pavement analyzing abilities...|$|E
5000|$|The {{physical}} {{interpretation is}} that such a set represents what can [...] - [...] in theory [...] - [...] be simultaneously be measured with arbitrary <b>precision.</b> The Heisenberg <b>uncertainty</b> relation prohibits simultaneous exact measurements of two non-commuting observables.|$|R
50|$|In {{addition}} to measuring UI, the laboratory must also measure v and g using experimental methods {{that do not}} depend on the definition of mass. The overall precision of m depends on the precisions of the measurements of U, I, v and g. Since there are already methods of measuring v and g to very high <b>precision,</b> the <b>uncertainty</b> of the mass measurement is dominated by the measurement of UI, which is the value measured by the watt balance.|$|R
40|$|A {{methodology}} {{is proposed}} {{for the evaluation of}} uncertainty in the in-flight determination of aircraft thrust, which provides error traceability to a national standards laboratory, and is independent of the procedure used to calculate or measure thrust in flight, thereby yielding a consistent means for the evaluation of measurement capabilities. Attention is given to the factors of measurement error, <b>precision,</b> bias, <b>uncertainty,</b> error estimation and classification, error propagation, ground testing, and the related problems of model bias error, model precision error, and the uncertainty limit...|$|R
40|$|China is {{planning}} to launch more and more optical remote-sensing satellites with high spatial resolution and multistep gains. Field calibration, the current operational method of satellite in-flight radiometric calibration, still {{does not have enough}} capacity to meet these demands. Gaofen- 1 (GF- 1), as the first satellite of the Chinese High-resolution Earth Observation System, has been specially arranged to obtain 22 images over clean ocean areas using the Wide Field Viewing camera. Following this, Rayleigh scattering calibration was carried out for the visible channels with these images after the appropriate data processing steps. To guarantee a high calibration <b>precision,</b> <b>uncertainty</b> was analyzed in advance taking into account ozone, aerosol optical depth (AOD), seawater salinity, chlorophyll concentration, wind speed and solar zenith angle. AOD and wind speed were found to be the biggest error sources, which were also closely coupled to the solar zenith angle. Therefore, the best sample data for Rayleigh scattering calibration were selected at the following solar zenith angle of 19 – 22 ° and wind speed of 5 – 13 m/s to reduce the reflection contributed by the water surface. The total Rayleigh scattering calibration uncertainties of visible bands are 2. 44 % (blue), 3. 86 % (green), and 4. 63 % (red) respectively. Compared with the recent field calibration results, the errors are − 1. 69 % (blue), 1. 83 % (green), and − 0. 79 % (red). Therefore, the Rayleigh scattering calibration can become an operational in-flight calibration method for the high spatial resolution satellites...|$|E
40|$|This paper {{describes}} {{the development and}} implementation of a system to perform accurate measurements of mass flow rate in the Isentropic Light Piston (Turbine) Facility (ILPF) at QinetiQ Farnborough. The facility has recently been upgraded so that turbine aerodynamic efficiency measurements can be performed. The implementation of a system for mass flow rate measurement formed part of that upgrade. The measurement system is novel in that accurate measurements can be performed with both combustor representative turbine inlet temperature distortion (hot-streaks) and swirl. The ILPF is a short-duration (approximately 0. 5 s run time) turbine test facility, which has been used for aerodynamic and heat transfer investigations of-primarily-high-pressure turbine stages, although it has also been configured to operate as a 1 frac(1, 2) stage (HP stage with IP or LP vane) turbine. The MT 1 turbine is a highly-loaded unshrouded design relevant to modern military engine design, or future civil engine design. The turbine is engine scale, and all relevant dimensional parameters for aerodynamics and heat transfer are matched: Re, M, N / sqrt(T 01), Tgas / Twall. Hot streaks are simulated in the ILPF by the controlled mixing of hot and cold gas streams. The cold stream is introduced though a conventional sonic metering nozzle, from a large reservoir acting in blow-down mode. The hot stream is generated using a light free piston contained within a piston-tube, which, under the action of a driver gas, isentropically compresses and heats the working gas to the desired hot-gas temperature. The exit contraction of the piston tube acts as a subsonic converging-diverging venturi: upstream p 0 and T 0, and throat p are measured at a number of circumferential locations around the exit contraction to determine the mass flow rate of the hot stream. The effective area of the venturi was measured using a novel blow-down calibration technique which is described. The bias error in the measurement of mass flow rate through the turbine with and without temperature distortion was 1. 13 % and 1. 37 % respectively-of the same order as the accuracy associated with conventional tertiary devices. The <b>precision</b> <b>uncertainty</b> was 0. 198 % in both cases. Accuracy with the introduction of inlet swirl is 1. 37 %. © 2008 Elsevier Ltd. All rights reserved...|$|E
40|$|Fighter pilots {{operate in}} environments where an {{erroneous}} decision may have fatal consequences. A tactical {{decision support system}} (TDSS) could aid the pilots to analyze the situation and make correct decisions. The TDSS can, for instance, highlight important information and suggest suitable actions. The aim of this thesis {{is to provide a}} situation analysis model of combat survival that can be utilized in a TDSS. The first part of this thesis describes an analysis of what the model needs to describe and how it can be used. It is concluded that the model should evaluate the outcome of different actions with respect to combat survival. This evaluation can guide the pilot’s decision making, so that actions leading to dangerous situations are avoided. The analysis also highlights the need of handling uncertainties, both measurement <b>precision</b> <b>uncertainty</b> regarding the locations and capabilities of the threats (enemies) and inference uncertainties regarding the prediction of how the threats will act. Finally, arguments for focusing the rest of the work on a single fighter aircraft and threats located on the ground are presented. The second part of the thesis suggests a model, which describes the survivability, i. e., the probability that the aircraft can fly a route without being hit by fire from ground-based threats. Thus, the model represents the inference uncertainty, since it describes the probability of survival. The model’s characteristics are discussed, e. g., that the model is implementable and can be adapted to describe different kinds of ground-based threats. Uncertainty in terms of measurement precision influences the estimate of the survivability. Two different ways of representing this is discussed: calculating the worst case scenario or describing the input as random variables and the resulting survivability as a random variable with a probability distribution. Monte Carlo simulations are used for estimating the distribution for survivability in a few illustrative scenarios, where the input is represented as random variables. The simulations show that when the uncertainty in input is large, the survivability distribution may be both multimodal and mixed. Two uncertainty measures are investigated that condense the information in the distributions into a single value: standard deviation and entropy. The simulations show that both of these measures reflect the uncertainty. Furthermore, the simulations indicate that the uncertainty measures can be used for sensor management, since they point out which information that is the most valuable to gather in order to decrease the uncertainty in the survivability. Finally, directions for future work are suggested. A number of TDSS functions that can be developed based on the model are discussed e. g., warnings, countermeasure management, route-planning and sensor management. The design of these functions could require extending the threat model to incorporate airborne threats and the effects of countermeasures. Further investigations regarding the uncertainty in the model are also suggested...|$|E
40|$|With HERA {{currently}} {{in its second}} stage of operation, {{it is possible to}} assess the potential precision limits of HERA data and to estimate the potential impact of the measurements which are expected at HERA-II, in particular with respect to the PDF <b>uncertainties.</b> <b>Precision</b> limits of the structure function analyses at HERA are examined in [1]. Since larg...|$|R
5000|$|There {{are many}} ways to help prevent or {{overcome}} the logjam of analysis paralysis. There may be many factors contributing to the cause. Lon Roberts breaks down the common definition of [...] "analysis paralysis" [...] into three possibly overlapping conditions of paralysis: analysis process, decision <b>precision,</b> and risk <b>uncertainty.</b> He uses this to give specific actions for each condition. Becky Kane and others give these following suggestions: ...|$|R
40|$|The 'characteristic {{function}}' is a two-parameter function relating <b>precision</b> or <b>uncertainty</b> in {{analytical results}} to the concentration of the analyte. In previous papers, in this series, {{it has been shown}} to provide a good model of precision measured: (a) under reproducibility conditions and (b) under 'instrumental' conditions. The present study shows that it is also a valuable model for precision estimated under repeatability conditions. The study data were large sets of duplicated results obtained for the purposes of quality control on typical test materials in routine analysis. As the analytes exhibited concentration ranges encompassing between one and three orders of magnitude, there was ample scope to demonstrate goodness of fit to the function under different circumstances...|$|R
40|$|Groundwater {{depletion}} in arid and semiarid {{areas is}} of increasing concern. Increasing water demand due to increasing population {{and climate change}} impacts have intensified the water shortage problems and put further stress especially on groundwater. Water harvesting techniques have been traditional solutions to water scarcity problems in the arid and semiarid Middle East for thousands of years. These techniques are increasingly being encouraged and at present {{there has been a}} renewed interest to find improved methods for water harvesting and artificial recharge in many arid countries. In order to understand the function and efficiency of an introduced floodwater harvesting system, the contribution of the system for total recharge and crop yield needs to be quantified using proper techniques and experiments. This dissertation presents a combination of modeling techniques and field experiments to quantitatively evaluate a system for flash flood harvesting in arid southern Iran. To investigate the performance and hydrological function of an improved floodwater spreading system, a groundwater model together with water balance approach were developed to estimate the magnitude of recharged water through both natural riverbed and artificial recharge system. Also, effects of pumping rates on the reservoir from 1993 through 2007 were assessed. The impacts of future climate scenarios on surface and groundwater resources were also simulated using a sequential modeling approach. Further, as the floodwater spreading system is a multi-functional and multi-purposed system, the contribution of the system in spate irrigation farming was tested through a three-year field experiment. In the groundwater modeling, the recharge rate and aquifer hydraulic parameters were estimated through inverse modeling approach. The model was calibrated and verified based on the observed hydraulic head in observation wells and model <b>precision,</b> <b>uncertainty,</b> and model sensitivity were analyzed in all modeling steps. The results showed that in the steady-state groundwater flow with no recharge from surface water, the studied aquifer is mainly recharged by a fault, which conducts water into the area from an upper sub-basin. Estimation showed that the recharge amount in the studied floodwater spreading system varied from a few hundred thousand cubic meters per month during drought periods to about 4. 5 million cubic meter per month during rainy periods. The results also showed that in a normal year without extreme events the floodwater spreading system is the main contributor to recharge with 80 % and the ephemeral river channel with 20 % of total recharge in the studied area. The climate change impact scenarios revealed that the abstraction has the most substantial effect on the groundwater level and the continuation of current pumping rate would lead to a groundwater decline by 18 m up to 2050. The field cultivation of barley crop inside and outside the floodwater spreading system displayed a significant increase in yield for the plots inside the system relative to the plot outside the system. As a summary, recognizing that groundwater depletion is occurring in many arid areas due to the over-exploitation as a consequence of population growth and climate change, multi-purpose floodwater harvesting for artificial recharge and spate irrigation could be a parsimonious and appropriate way to efficiently utilize the potential agricultural capacity of the arid environments. In the floodwater spreading system, a flash flood can be harvested and stored beneath the ground to be abstracted for irrigated agriculture during the dry season while the crop can be directly irrigated by floods in the wet season. Therefore, the groundwater abstraction can be minimized, particularly, during the wet season and water can be saved for the dry season...|$|E
40|$|In this thesis, {{we assume}} a minimal supersymmetric {{extension}} of the Standard Model (MSSM) with conserved R-parity such that the lightest neutralino is the cold dark matter candidate. A stringent constraint on the MSSM parameter space can be set by the comparison of the predicted neutralino relic density with the experimentally determined value. In order to match the high experimental <b>precision,</b> <b>uncertainties</b> within the theoretical calculation have to be reduced. One of the main uncertainties arises from the cross section of annihilation and coannihilation processes of the dark matter particle. In a phenomenological study we investigate the interplay of neutralino-neutralino annihilation, neutralino-stop coannihilation and stop-stop annihilation. We demonstrate that neutralino-stop coannihilation contributes significantly to the neutralino relic density and is furthermore very well motivated due to the recent discovery of a 125 GeV Higgs boson. Due to this ample motivation we have calculated the full O(alpha_s) supersymmetric QCD corrections to neutralino-squark coannihilation. We show in detail our DRbar/on-shell renormalization scheme {{for the treatment of}} ultraviolet divergences, and describe the phase space slicing method which is used to handle soft and collinear infrared divergences. Further, we comment on the treatment of occurring intermediate on-shell states. The whole calculation is provided within the numerical tool DM@NLO that serves as an extension to existing relic density calculators, which consider only an effective tree-level calculation. Based on three example scenarios we study the impact of the NLO corrections on the total (co) annihilation cross section, and observe corrections of up to 30 %. This leads to a correction of 5 - 9 % on the relic density, which is larger than the current experimental uncertainty and is, thus, important to be taken into account...|$|R
40|$|This paper {{describes}} {{the development and}} implementation of a shaft torque measurement system for the Oxford Turbine Research Facility (formerly the Turbine Test Facility (TTF) at QinetiQ, Farnborough), or OTRF. As part of the recent EU TATEF II programme, the facility was upgraded to allow turbine efficiency measurements to be performed. A shaft torque measurement system was developed as part of this upgrade. The system is unique in that, to the authors' knowledge, it provided the first direct measurement of shaft torque in a transient turbine facility although the system has wider applicability to rotating test facilities in which power measurement is a requirement. The adopted approach removes the requirement to quantify bearing friction, which can be difficult to accurately calibrate under representative operating conditions. The OTRF is a short duration (approximately 0. 4 s run time) isentropic light-piston facility capable of matching all of the non-dimensional parameters important for aerodynamic and heat studies, namely Mach number, Reynolds number, non-dimensional speed, stage pressure ratio and gas-to-wall temperature ratio. The single-stage MT 1 turbine used for this study is a highly loaded unshrouded design, and as such is relevant to modern military, or future civil aero-engine design. Shaft torque was measured directly using a custom-built strain gauge-based torque measurement system in the rotating frame of reference. This paper {{describes the}} development of this measurement system. The system was calibrated, including the effects of temperature, to a traceable primary standard using a purpose-built facility. The bias and <b>precision</b> <b>uncertainties</b> of the measured torque were ± 0. 117 % and ± 0. 183 %, respectively. To accurately determine the shaft torque developed by a turbine in the OTRF, small corrections due to inertial torque (associated with changes in the rotational speed) and aerodynamic drag (windage) are required. The methods for performing these corrections are described. © 2011 IOP Publishing Ltd...|$|R
40|$|Replace page 4 {{with the}} {{attached}} page, which contains a corrected version of Table 1. 1. The original Table 1. 1 mistakenly contained values of water absorption () wa l taken from Pope’s PhD thesis (Pope 1993), {{rather than the}} currently accepted values from Pope and Fry (1997), which were derived by additional analyses of Pope’s measurements to reduce the uncertainties of reported values. The attached (revised) version of Table 1. 1 (Err. 1) contains () wa l taken from Pope and Fry (1997) from 400 nm to 715 nm, with the numbers of significant figures adjusted to match the stated <b>precision</b> and <b>uncertainties</b> of the reported values at each wavelength. In the original version of Table 1. 1, the uniform listing at 4 decimal places truncated the actual values from 400 nm to 465 nm, and exceeded the <b>precisions</b> and <b>uncertainties</b> of the reported data (3 decimal places) at wavelengths> 640 nm. On further reviewing the remaining () wa l entries at other wavelengths in the original Table 1. 1, {{it was discovered that}} the listed values of () wa l at wavelengths> 720 nm, as derived from the complex refractive indices reported by Kou et al. (1993), were simply “nearest neighbor ” values, rather than interpolations to the stated wavelengths. These values were recomputed, interpolated to 5 nm intervals using a least squares cubic polynomial fit, and entered using the number of significant figures appropriate to the values reported by Kou et al. (1993). Kou et al. (1993) reported measured refractive index values, and 1 -standard deviation relative uncertainties () s l, at more than 60 wavelengths spanning 715 nm to 800 nm. The cubic polynomial fit used to interpolate the data i...|$|R
40|$|Optical atomic clocks {{represent}} the state-of-the-art in {{the frontier of}} modern measure-ment science. In this article we provide a detailed review {{on the development of}} optical atomic clocks that are based on trapped single ions and many neutral atoms. We discuss important technical ingredients for optical clocks, and we present measurement <b>precision</b> and systematic <b>uncertainty</b> associated with some of the best clocks to date. We conclude with an outlook on the exciting prospect for clock applications. ar X i...|$|R
40|$|We {{review the}} first {{measurements}} of the CKM angle γ from LHCb. These measurements have been performed with b-hadron decays dominated by b → u and b → c tree-level amplitudes, from which γ can be determined without theoretical <b>uncertainties.</b> <b>Precision</b> is achieved by averaging results from B− → Dh− (h = K,π) decays with D → h+h−, D → K+π−, and D → K+π−π+π−, and D → Ks 0 h+h−. Prospects for these and future measurements of γ using neutral b-hadron decays are briefly discussed...|$|R
