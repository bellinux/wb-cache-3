5443|3133|Public
25|$|It {{follows from}} {{theory that the}} Kalman filter is the optimal linear filter in cases where a) the model {{perfectly}} matches the real system, b) the entering noise is white (uncorrelated) and c) the covariances of the noise are exactly known. Several methods for the noise covariance estimation have been proposed during past decades, including ALS, mentioned in the section above. After the covariances are estimated, {{it is useful to}} evaluate the performance of the filter; i.e., whether it is possible to improve the state estimation quality. If the Kalman filter works optimally, the innovation sequence (the output <b>prediction</b> <b>error)</b> is a white noise, therefore the whiteness property of the innovations measures filter performance. Several different methods can be used for this purpose. If the noise terms are non-Gaussian distributed, methods for assessing performance of the filter estimate, which use probability inequalities or large-sample theory, are known in the literature.|$|E
500|$|Within the brain, {{dopamine}} functions {{partly as}} a [...] "global reward signal", where an initial phasic dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward. In the context of reward-related learning, dopamine also functions as a reward <b>prediction</b> <b>error</b> signal, that is, {{the degree to which}} the value of a reward is unexpected. According to this hypothesis of Wolfram Schultz, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level. The [...] "prediction error" [...] hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes <b>prediction</b> <b>error.</b> This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.|$|E
2500|$|... where [...] and [...] are the <b>prediction</b> <b>error</b> {{covariance}} and {{the gains}} {{of the standard}} Kalman filter (i.e., [...] ).|$|E
40|$|This article {{introduces}} two {{new types}} of <b>prediction</b> <b>errors</b> in time series: the filtered <b>prediction</b> <b>errors</b> and the deletion <b>prediction</b> <b>errors.</b> These two <b>prediction</b> <b>errors</b> are obtained in the same sample used for estimation, but {{in such a way}} that they share some common properties with out of sample <b>prediction</b> <b>errors.</b> It is proved that the filtered <b>prediction</b> <b>errors</b> are uncorrelated, up to terms of magnitude order O(T- 2), with the in sample innovations, a property that share with the out-of-sample <b>prediction</b> <b>errors.</b> On the other hand, deletion <b>prediction</b> <b>errors</b> assume that the values to be predicted are unobserved, a property that they also share with out-of-sample <b>prediction</b> <b>errors.</b> It is shown that these <b>prediction</b> <b>errors</b> can be computed with parameters estimated by assuming innovative or additive outliers, respectively, at the points to be predicted. Then the <b>prediction</b> <b>errors</b> are obtained by running the procedure for all the points in the sample of data. Two applications of these new <b>prediction</b> <b>errors</b> are presented. The first is the estimation and comparison of the <b>prediction</b> mean squared <b>errors</b> of competing predictors. The second is the determination of the order of an ARMA model. In the two applications the proposed filtered <b>prediction</b> <b>errors</b> have some advantages over alternative existing methods [...] ...|$|R
40|$|The {{empirical}} Bayes estimators {{in mixed}} models {{are useful for}} small area estimation {{in the sense of}} increasing precision of prediction for small area means, and one wants to know the <b>prediction</b> <b>errors</b> of the empirical Bayes estimators based on the data. This paper is concerned with conditional <b>prediction</b> <b>errors</b> in the mixed models instead of conventional unconditional <b>prediction</b> <b>errors.</b> In the mixed models based on natural exponential families with quadratic variance functions, it is shown that the difference between the conditional and unconditional <b>prediction</b> <b>errors</b> is significant under distributions far from normality. Especially for the binomial-beta mixed and the Poisson-gamma mixed models, the leading terms in the conditional <b>prediction</b> <b>errors</b> are, respectively, a quadratic concave function and an increasing function of the direct estimate in the small area, while the corresponding leading terms in the unconditional <b>prediction</b> <b>errors</b> are constants. Second-order unbiased estimators of the conditional <b>prediction</b> <b>errors</b> are also derived and their performances are examined through simulation and empirical studies...|$|R
40|$|In {{this note}} we analyze the {{relationship}} between one-step ahead <b>prediction</b> <b>errors</b> and interpolation errors in time series. We obtain {{an expression of the}} <b>prediction</b> <b>errors</b> in terms of the interpolation errors and then we show that minimizing the sum of squares of the one step-ahead standardized <b>prediction</b> <b>errors</b> is equivalent to minimizing the sum of squares of standardized interpolation errors. ...|$|R
2500|$|Note {{that in this}} {{expression}} for [...] the covariance of the observation noise [...] represents {{at the same time}} the covariance of the <b>prediction</b> <b>error</b> (or innovation) these covariances are equal only in the case of continuous time.|$|E
2500|$|Neurobiologically, neuromodulators like {{dopamine}} {{are considered}} to report the precision of prediction errors by modulating the gain of principal cells encoding <b>prediction</b> <b>error.</b> This {{is closely related to}} – but formally distinct from – the role of dopamine in reporting prediction errors per se [...] and related computational accounts.|$|E
5000|$|Accuracy of {{prediction}} at validation points: mean <b>prediction</b> <b>error</b> (MPE) {{and root}} mean square <b>prediction</b> <b>error</b> (RMSPE); ...|$|E
40|$|In {{this study}} {{expectations}} and <b>prediction</b> <b>errors</b> are {{introduced in the}} context of retail price setting. A new model and a new data set are used to examine whether <b>prediction</b> <b>errors</b> influence retail price setting, whether <b>prediction</b> <b>errors</b> cause only limited price changes to maintain price stability and whether there are differences in influences according to whether <b>prediction</b> <b>errors</b> are positive or negative. The model is an extended version of a full costs pricing model and the averaged data are for a large number of shop types in German retailing for a long series of successive years...|$|R
3000|$|... [...]. Then, the {{information}} of P and T is {{extracted from the}} LSB values of the first 32 <b>prediction</b> <b>errors.</b> Furthermore, the hidden data and the original <b>prediction</b> <b>errors</b> E are extracted from E [...]...|$|R
40|$|This paper investigates multistep <b>prediction</b> <b>errors</b> for non-stationary autoregressive {{processes}} {{with both}} model order and true parameters unknown. We give asymptotic expressions for the multistep mean squared <b>prediction</b> <b>errors</b> and accumulated <b>prediction</b> <b>errors</b> of two important methods, plug-in and direct prediction. These expressions not only characterize how the <b>prediction</b> <b>errors</b> {{are influenced by}} the model orders, prediction methods, values of parameters and unit roots, but also inspire us to construct some new predictor selection criteria that can ultimately choose the best combination of the model order and prediction method with probability 1. Finally, simulation analysis confirms the satisfactory finite sample performance of the newly proposed criteria. Comment: Published in at [URL] the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
5000|$|On {{the other}} hand, if the mean square <b>prediction</b> <b>error</b> is {{constrained}} to be unity and the <b>prediction</b> <b>error</b> equation is included {{on top of}} the normal equations, the augmented set of equations is obtained as ...|$|E
5000|$|If {{everything}} goes fine, the algorithm {{will be able}} to find a matching block with little <b>prediction</b> <b>error</b> so that, once transformed, the overall size of motion vector plus <b>prediction</b> <b>error</b> is lower than the size of a raw encoding.|$|E
50|$|In {{pursuit of}} testing these <b>prediction</b> <b>error</b> ideas in humans, Montague founded the Human Neuroimaging Lab at Baylor College of Medicine in Houston, Texas, and pursued {{functional}} neuroimaging experiments analogous {{to those used}} in other model species. This work tested the reward <b>prediction</b> <b>error</b> model in human subjects using simple conditioning experiments directly analogous to those used in rodents and non-human primates. His group then tested the reward <b>prediction</b> <b>error</b> idea during an abstract task of social exchange between two interacting humans and showed striatal BOLD signals that changed their timing consistent with a <b>prediction</b> <b>error</b> signal, but {{in the context of}} a social exchange. They also tested the idea of cultural brand identity and its impact on reward <b>prediction</b> <b>error</b> signals. With Brooks King-Casas and colleagues, Montague later applied the same social exchange approach as a probe of Borderline Personality Disorder, and these efforts have been used to provide a new probe of psychopathology.|$|E
40|$|Sensory <b>prediction</b> <b>errors</b> are {{fundamental}} brain responses that signal {{a violation of}} expectation in either the internal or external sensory environment, and are therefore crucial for survival and adaptive behaviour. Patients with schizophrenia show deficits in these internal and external sensory <b>prediction</b> <b>errors,</b> which can be measured using electroencephalography (EEG) components such as N 1 and mismatch negativity (MMN), respectively. New evidence suggests that these deficits in sensory <b>prediction</b> <b>errors</b> are more widely distributed on a continuum of psychosis, whereas psychotic experiences exist to varying degrees throughout the general population. In this paper, we review recent findings in sensory <b>prediction</b> <b>errors</b> in the auditory domain across the continuum of psychosis, and discuss these {{in light of the}} predictive coding hypothesis...|$|R
40|$|Midbrain {{dopamine}} neurons encode reward <b>prediction</b> <b>errors.</b> In {{this issue}} of Neuron, Takahashi et al. (2016) show that the ventral striatum provides dopamine neurons with prediction information specific to the timing, but not the quantity, of reward, suggesting a surprisingly nuanced neural implementation of reward <b>prediction</b> <b>errors...</b>|$|R
30|$|Generally, we can {{estimate}} whether large <b>prediction</b> <b>errors</b> {{will occur}} from known observed data {{based on some}} classifiers such as a support vector machine (SVM) [8]. This {{is related to the}} idea of outlier detection that has recently been studied [9, 10]. By using this method, it becomes feasible to assist meteorologists performing early detection of large <b>prediction</b> <b>errors</b> and deciding which areas should be monitored carefully. However, since this method can only detect the occurrence of large errors, it is still difficult to estimate their details such as signs and degrees. Specifically, the estimation of ‘whether large <b>prediction</b> <b>errors</b> will occur’ enables detection of places where absolute differences between predicted values and observed values are large, e.g., larger than a predefined threshold. Then, by regarding such places as positive examples to construct a classifier, it becomes feasible to estimate the locations of large <b>prediction</b> <b>errors.</b> However, since the obtained classifier performs binary classification, {{it is difficult to determine}} whether predicted values are larger or smaller than observed values and how large the <b>prediction</b> <b>errors</b> are.|$|R
5000|$|... {{represents}} the instantaneous a posteriori backward <b>prediction</b> <b>error</b> ...|$|E
5000|$|If {{the block}} {{matching}} algorithm fails {{to find a}} suitable match the <b>prediction</b> <b>error</b> will be considerable. Thus the overall size of motion vector plus <b>prediction</b> <b>error</b> will be greater than the raw encoding. In this case the encoder would make an exception and send a raw encoding for that specific block.|$|E
5000|$|Out-of-bag (OOB) error, {{also called}} out-of-bag estimate, {{is a method}} of {{measuring}} the <b>prediction</b> <b>error</b> of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating to sub-sample data samples used for training. OOB is the mean <b>prediction</b> <b>error</b> on each training sample , using only the trees {{that did not have}} [...] in their bootstrap sample.|$|E
40|$|AbstractTransmission system {{operators}} {{have to buy}} {{control power}} to cover deviations in energy production of wind power plants due to <b>prediction</b> <b>errors.</b> The risk of errors is immanent to any prediction. This leads to financial risks, especially for the unexpected large deviations. Therefore large-scale integration of wind power could oblige the system operator to allocate more spinning and supplemental energy reserve. This would cause more operation costs, in order to balance wind power <b>prediction</b> <b>errors</b> in a certain time period. Battery storage technology {{can be used to}} supply backup power for wind power plants. However, the high cost of battery storage systems (BESS) is the major drawback for their commercial applications. Gradually decreasing costs of batteries can bring BESS in a competitive positon for balancing wind <b>prediction</b> <b>errors.</b> By analyzing the application of BESS as means to balance <b>prediction</b> <b>errors,</b> the resulting cost associated with wind generation <b>prediction</b> <b>errors</b> in a liberalized electricity market in the year 2050 is assessed. The result shows, that BESS is an outstanding alternative for short-term balancing {{in order to reduce the}} cost of prediction uncertainties...|$|R
40|$|Theoretical and {{empirical}} models {{can be used}} to model the migration or separation characteristics in micellar electrokinetic chromatography in order to optimise the resolution, in this paper only empirical models were used, because it is easier and more straightforward to obtain these models. Several empirical approaches for the optimisation of the resolution were compared in order to determine which response should be modelled preferably. The use of models of the effective mobility in combination with average plate numbers proved to be the most suitable approach to optimisation of the resolution, because the relative <b>prediction</b> <b>errors</b> of the models of the effective mobility were a factor of 2 - 4 smaller than the relative <b>prediction</b> <b>errors</b> of the models of the apparent mobility. Moreover for the least separated peak pair the resolutions based on the models of the apparent and effective mobility showed relative <b>prediction</b> <b>errors</b> that were approximately a factor of 2 smaller than the relative <b>prediction</b> <b>errors</b> of the resolutions based on the models of the resolution and separation factor. The predictions of the separation factor based on the different models generally showed lower <b>prediction</b> <b>errors</b> than the <b>predictions</b> of the corresponding resolutions. Although the relative <b>prediction</b> <b>errors</b> were large, particularly for closely migrating compounds, the empirical approach will probably lead to the optimum separation buffer composition. (C) 2000 Elsevier Science B. V. AU rights reserved...|$|R
30|$|Scattering {{coefficient}} variations do not greatly {{influence the}} <b>prediction</b> <b>errors.</b>|$|R
5000|$|... {{should be}} chosen {{so as to}} {{minimise}} the variance of the <b>prediction</b> <b>error,</b> ...|$|E
5000|$|... is the {{one that}} {{minimizes}} the <b>prediction</b> <b>error</b> sequence [...] in a mean-square sense ...|$|E
50|$|Montague {{and colleagues}} have also pursued {{the nature of}} {{counterfactual}} signals in human subjects {{and their relationship to}} <b>prediction</b> <b>error</b> signaling. This work has most recently led to a first-of-its-kind measurement of sub-second dopamine fluctuations in the striatum of conscious human subjects where reward <b>prediction</b> <b>error</b> signals and counterfactual errors signals appear to be integrated into a composite dopamine signal.|$|E
30|$|Although the {{performance}} of numerical weather prediction has improved, large <b>prediction</b> <b>errors</b> might still occur in leading edge technologies. In this paper, we define values obtained by subtracting observed values from predicted values calculated by numerical weather <b>prediction</b> as <b>prediction</b> <b>errors,</b> {{and they can be}} written by the following equation: [Prediction error]=[Predicted value]−[Observed value].|$|R
40|$|A {{new method}} to encode the <b>prediction</b> <b>errors</b> in a hybrid coding scheme is presented. The {{technique}} tries to reduce camera noise transcription in the <b>prediction</b> <b>errors,</b> thereby decreasing bit-rate. Camera noise {{is responsible for}} vector field inhomogeneity in regions of the image where the true motion is uniform. Vector field smoothing techniques, which cancel the effect of camera noise on motion vectors determination, form the central steps of the motion estimation algorithms used here. Nevertheless, these techniques do not always yield bit-rate decrease in DCT-based schemes. Indeed, although they decrease the camera noise impact on the estimated motion field by removing it from the estimation, this noise is reintroduced in the <b>prediction</b> <b>errors,</b> thereby increasing the bit-rate. To avoid this effect, <b>prediction</b> <b>errors</b> encoding is done by using information from the vector field. <b>Prediction</b> <b>errors</b> of regions where motion vectors are correctly estimated are coarsely quantized, while in other regions, a lower quantization factor is adopted. This technique yields a decrease of the P-frames bit-rate up to 40 % {{as well as a}} PSNR decrease, but the image visual quality remains almost constant. (C) 1998 Elsevier Science B. V. All rights reserved...|$|R
40|$|This article analyzesthe simple Rescorla–Wagner {{learning}} rule {{from the vantage}} point of least squares learning theory. In particular, it suggests how measures of risk, such as prediction risk, can be used to adjust the learning constant in reinforcement learning. It argues that prediction risk is most effectively incorporated by scaling the <b>prediction</b> <b>errors.</b> This way, the learning rate needs adjusting only when the covariance between optimal predictions and past (scaled) <b>prediction</b> <b>errors</b> changes. Evidence is discussed that suggests that the dopaminergic system in the (human and nonhuman) primate brain encodes prediction risk, and that <b>prediction</b> <b>errors</b> are indeed scaled with prediction risk (adaptive encoding) ...|$|R
5000|$|... where [...] and [...] are the <b>prediction</b> <b>error</b> {{covariance}} and {{the gains}} {{of the standard}} Kalman filter (i.e., [...] ).|$|E
50|$|In summary, {{cross-validation}} combines (averages) {{measures of}} fit (<b>prediction</b> <b>error)</b> to derive {{a more accurate}} estimate of model prediction performance.|$|E
5000|$|A <b>Prediction</b> <b>Error</b> Criterion for Choosing the Lower Quantile in Pareto Index Estimation, by Debbie J. Dupuis and Maria-Pia Victoria-Feser ...|$|E
40|$|Summary. This paper investigates multistep <b>prediction</b> <b>errors</b> for nonstationary autore-gressive {{processes}} {{with both}} model order and true parameters unknown. We give asymptotic expressions for the multistep mean squared <b>prediction</b> <b>errors</b> and accumulated <b>prediction</b> er-rors of two important methods, plug-in and direct prediction. These expressions not only characterize how the <b>prediction</b> <b>errors</b> {{are influenced by}} the model orders, prediction meth-ods, values of parameters, and unit roots, but also inspire us to construct some new predictor selection criteria that can ultimately choose the best combination of the model order and pre-diction method with probability 1. Finally, simulation analysis confirms the satisfactory finite sample performance of the newly proposed criteria...|$|R
40|$|Reward <b>prediction</b> <b>errors</b> are {{quantitative}} signed {{terms that}} express {{the difference between}} the value of an obtained outcome and the expected value that was placed on it prior to its receipt. Positive reward <b>prediction</b> <b>errors</b> constitute reward, negative reward <b>prediction</b> <b>errors</b> constitute punishment. Reward <b>prediction</b> <b>errors</b> {{have been shown to be}} powerful drivers of reinforcement learning in formal models and there is thus a strong reason to believe they are used in the brain. Isolating such neural signals stands to help elucidate how reinforcement learning is implemented in the brain, and may ultimately shed light on individual differences, psychopathologies of reward such as addiction and depression, and the apparently non-normative behaviour under risk described by behavioural economics. In the present thesis, I used the event related potential technique to isolate and study electrophysiological components whose behaviour resembled reward <b>prediction</b> <b>errors.</b> I demonstrated that a candidate component, “feedback related negativity”, occurring 250 to 350 ms after receipt of reward or punishment, showed such behaviour. A meta-analysis of the existing literature on this component, using a novel technique of “great grand averaging”, supported this view. The component showed marked asymmetries however, being more responsive to reward than punishment and more responsive to appetitive rather than aversive outcomes. I also used novel data-driven techniques to examine activity outside the temporal interval associated with the feedback related negativity. This revealed a later component responding solely to punishments incurred in a Pavlovian learning task. It also revealed numerous salience-encoding components which were sensitive to a <b>prediction</b> <b>error’s</b> size but not its sign...|$|R
40|$|Attention-deficit/hyperactivity {{disorder}} (ADHD) {{has been}} associated with deficient decision making and learning. Models of ADHD have suggested that these deficits could be caused by impaired reward <b>prediction</b> <b>errors</b> (RPEs). Reward <b>prediction</b> <b>errors</b> are signals that indicate violations of expectations and are known to be encoded by the dopaminergic system. However, the precise learning and decision-making deficits and their neurobiological correlates in ADHD are not well known...|$|R
