18|265|Public
25|$|Create retail coupons {{based on}} a <b>proportional</b> <b>scale</b> to how much the {{customer}} has spent, to ensure a higher redemption rate.|$|E
5000|$|... #Caption: This chart {{shows the}} <b>proportional</b> <b>scale</b> {{differences}} from 1080p (1920×1080 pixels) to 8K×8K fulldome video.|$|E
50|$|Create retail coupons {{based on}} a <b>proportional</b> <b>scale</b> to how much the {{customer}} has spent, to ensure a higher redemption rate.|$|E
40|$|We propose {{iterative}} <b>proportional</b> <b>scaling</b> (IPS) via decomposable submodels for maximizing {{likelihood function}} of a hierarchical model for contingency tables. In ordinary IPS the <b>proportional</b> <b>scaling</b> is performed by cycling through {{the elements of the}} generating class of a hierarchical model. We propose to adjust more marginals at each step. This is accomplished by expressing the generating class as a union of decomposable submodels and cycling through the decomposable models. We prove convergence of our proposed procedure, if the amount of scaling is adjusted properly at each step. We also analyze the proposed algorithms around the maximum likelihood estimate (MLE) in detail. Faster convergence of our proposed procedure is illustrated by numerical examples...|$|R
5000|$|To {{compensate}} {{for the lack of}} differently-sized master character images (as implemented in Monotype’s Monophoto system, for instance), <b>proportional</b> <b>scaling</b> was implemented: the larger the type size specified, the tighter the spacing (an automatic feature). (For pi fonts and connecting scripts this was over-ridden by implementing a font number beginning with the figure 5, cancelling auto letterspacing.) ...|$|R
40|$|Previous {{studies of}} the {{development}} of phonological similarity and word length effects in children have shown that these effects are small or absent in young children, particularly when measured using visual presentation of the memoranda. This has often been taken as support for the view that young children do not rehearse. The current paper builds on recent evidence that instead suggests that absent phonological similarity and word length effects in young children reflects the same proportional cost of these effects in children of all ages. Our aims are to explore the conditions under which this a <b>proportional</b> <b>scaling</b> account can reproduce existing developmental data, and in turn suggest ways that future studies might measure and model phonological similarity and word length effects in children. To that end, we first fit a single mathematical function through previously reported data that simultaneously captures absent and negative proportional effects of phonological similarity in young children plus constant proportional similarity effects in older children. This developmental function therefore provides the benchmark that we seek to re-produce in a series of subsequent simulations that test the <b>proportional</b> <b>scaling</b> account. These simulations reproduce the developmental function well, provided that they take into account the influence of floor effects and of measurement error. Our simulations suggest that future empirical studies examining these effects in the context {{of the development of}} rehearsal need to take into account <b>proportional</b> <b>scaling.</b> They also provide a demonstration of how proportional costs can be explored, and of the possible developmental functions associated with such an analysis...|$|R
50|$|Although the Egyptians {{achieved}} extraordinary {{feats of}} engineering, {{they appear to}} have done so with relatively primitive technology. As far as is known they did not use wheels or pulleys. They transported massive stones over great distances using rollers, ropes and sledges hauled by large numbers of workers. The ancient Egyptians are credited with inventing the ramp, lever, lathe, oven, ship, paper, irrigation system, window awning, door, glass, a form of plaster of Paris, the bath, lock, shadoof, weaving, a standardized measurement system, geometry, silo, a method of drilling stone, saw, steam power, <b>proportional</b> <b>scale</b> drawings, enameling, veneer, plywood, rope truss, and more. There are no surviving Egyptian manuals so there has been considerable speculation on how stones were lifted to great heights and obelisks erected. Most theories centre on the use of ramps.|$|E
40|$|In {{this paper}} we propose a multi-scale edge {{detection}} algorithm based on <b>proportional</b> <b>scale</b> summing. Our analysis shows that <b>proportional</b> <b>scale</b> summing successfully improves edge detection rate by applying independent thresholds on multi-scale gradient images. The proposed method improves edge detection and localization by summing gradient images with a proportional parameter cn (c < 1); which ensures that the detected edges are {{as close as possible to}} the fine scale. We employ non-maxima suppression and thinning step similar to Canny edge detection framework on the summed gradient images. The proposed method can detect edges successfully and experimental results show that it leads to better edge detection performance than Canny edge detector and scale multiplication edge detector...|$|E
40|$|Many {{countries}} use {{income taxation}} {{as one of}} the essential tools of income redistribution. This paper covers positive and negative effect of the <b>proportional</b> <b>scale</b> of individual taxation introduced by the Russian Federation. The observation suggests that the flat rate increases the social inequality, which, in turn, reflects adversely on the economic growth of the country. The document describes the contribution of the income tax to the budget revenues of the developed countries and provides the data of foreign practice to apply income tax rates. The authors demonstrate the need for the progressive taxation and establishment of a tax-free allowance...|$|E
40|$|We discuss an {{efficient}} {{implementation of the}} iterative <b>proportional</b> <b>scaling</b> procedure in the multivariate Gaussian graphical models. We show that the computational cost can be reduced by localization of the update procedure in each iterative step by using {{the structure of a}} decomposable model obtained by triangulation of the graph associated with the model. Some numerical experiments demonstrate the competitive performance of the proposed algorithm. ...|$|R
40|$|Local signals {{obtained}} from BOLD fMRI are generally confounded by global effects. In this paper, we make an essential distinction between global effects {{and the global}} signal. Global effects have a similar influence on local signals from {{a large proportion of}} cerebral voxels. They may reflect diffuse physiological processes or variations in scanner sensitivity and are difficult to measure directly. Global effects are often estimated from the global signal, which is the spatial average of local signals from all cerebral voxels. If the global signal is strongly correlated with experimental manipulations, meaningfully different results may be obtained whether or not global effects are modeled (G. K. Aguirre et al., 1998, NeuroImage, 8, 302 – 306). In particular, if local BOLD signals make a significant contribution to the global signal, analyses using ANCOVA or <b>proportional</b> <b>scaling</b> models may yield artifactual deactivations. In this paper, we present a modification to the <b>proportional</b> <b>scaling</b> model that accounts for the contribution of local BOLD signals to the global signal. An event-related oddball stimulus paradigm and a block design working memory task were used to illustrate the efficacy of our model. © 2001 Academic Pres...|$|R
3000|$|... {{which is}} kept {{constant}} at ρrel= 0.1 by <b>proportional</b> <b>scaling</b> of W and H. As a default aspect ratio for the systems {{we use the}} aspect ratio R_ 0 = 2 /√(3) of a single regular honeycomb. Each Timoshenko beam {{is assumed to be}} linear elastic with Young’s modulus EB= 0.1 GPa and Poisson’s ratio νB= 0.3. The cross-section is quadratic: t=w= 0.05 Δp with a shear coefficient proposed by Cowper (1966) of κ= 10 + 10 νB/(12 + 11 νB)≈ 0.85.|$|R
40|$|AbstractA metric {{transform}} of a semimetric space X {{is obtained}} from X {{by measuring the}} distances by a different (not always <b>proportional)</b> <b>scale.</b> Two semimetric spaces {{are said to be}} isomorphic if one is isometric to a metric transform of the other. If X is a finite semimetric space, then it will be shown that X is isomorphic to a subset of a euclidean space. The dimension of X is defined to be the minimum dimension of a euclidean space containing an isomorph of X. In this paper we examine scales and dimensions for finite semimetric spaces, especially, for connected graphs and trees as metric spaces. We also count the number of non-isomorphic semimetric spaces...|$|E
40|$|AbstractThe {{main purpose}} of this paper is the study of the multivariate Behrens–Fisher distribution. It is defined as the {{convolution}} of two independent multivariate Student t distributions. Some representations of this distribution as the mixture of known distributions are shown. An important result presented in the paper is the elliptical condition of this distribution in the special case of <b>proportional</b> <b>scale</b> matrices of the Student t distributions in the defining convolution. For the bivariate Behrens–Fisher problem, the authors propose a non-informative prior distribution leading to highest posterior density (H. P. D.) regions for the difference of the mean vectors whose coverage probability matches the frequentist coverage probability more accurately than that obtained using the independence-Jeffreys prior distribution, even with small samples...|$|E
40|$|Abstract. We find mathematically optimal side-channel distinguishers {{by looking}} at the side-channel as a {{communication}} channel. Our method-ology can be adapted to any given scenario (device, signal-to-noise ratio, noise distribution, leakage model, etc.). When the model is known and the noise is Gaussian, the optimal distinguisher outperforms CPA and covariance. However, we show that CPA is optimal when the model is only known on a <b>proportional</b> <b>scale.</b> For non-Gaussian noise, we obtain different optimal distinguishers, one for each noise distribution. When the model is imperfectly known, we consider the scenario of a weighted sum of the sensitive variable bits where the weights are unknown and drawn from a normal law. In this case, our optimal distinguisher performs better than the classical linear regression analysis...|$|E
40|$|We give a {{bijection}} {{between a}} quotient {{space of the}} parameters and the space of moments for any $A$-hypergeometric distribution. An algorithmic method to compute the inverse image of the map is proposed utilizing the holonomic gradient method and an asymptotic equivalence of the map and the iterative <b>proportional</b> <b>scaling.</b> The algorithm gives a method to solve a conditional maximum likelihood estimation problem in statistics. Our interplay between the theory of hypergeometric functions and statistics gives some new formulas of $A$-hypergeometric polynomials...|$|R
40|$|OBJECTIVES: To {{assess the}} {{suitability}} of analyzing functional images of brain serotonin (5 -HT) synthesis with statistical parametric mapping (SPM), and to investigate further possible sex-related regional differences. DESIGN: Prospective study. PARTICIPANTS: Six healthy men and 5 healthy women. INTERVENTION: Participants' brains were scanned with positron-emission tomography (PET) after intravenous injection of alpha-[11 C]methyl-L-tryptophan (alpha-[11 C]MTrp). OUTCOME MEASURES: Tissue radioactivity images were converted into functional images using the Patlak plot approach, and analyzed with 2 methods for global normalization in the SPM program: <b>proportional</b> <b>scaling</b> and analysis of covariance (ANCOVA). RESULTS: The data structure suggests that PET alpha-[11 C]MTrp data meet the criteria for analysis with SPM, and that the <b>proportional</b> <b>scaling</b> method is more appropriate than the ANCOVA method for normalization. Regional differences in 5 -HT synthesis were identified between men and women, and {{the significance of these}} findings was supported by region of interest (ROI) analyses. CONCLUSION: SPM analyses of PET alpha-[11 C]MTrp data may be of value for identifying regional differences in brain 5 -HT synthesis between groups, and in investigating the effects of psychotropic drugs. Since we found regional differences between male and female subjects, men and women should not be grouped for data analysis in PET alpha-[11 C]MTrp studies...|$|R
40|$|We {{present a}} theory of pattern {{formation}} in growing domains inspired by biological examples of tissue development. Gradients of signaling molecules regulate growth, while growth changes these graded chemical patterns by dilution and advection. We identify a critical point of this feedback dynamics, which is characterized by spatially homogeneous growth and <b>proportional</b> <b>scaling</b> of patterns with tissue length. We apply this theory to the biological model system of the developing wing of the fruit fly Drosophila melanogaster and quantitatively identify signatures of the critical point. Comment: 5 pages, 3 figure...|$|R
40|$|The {{main purpose}} of this paper is the study of the multivariate Behrens-Fisher distribution. It is defined as the {{convolution}} of two independent multivariate Student t distributions. Some representations of this distribution as the mixture of known distributions are shown. An important result presented in the paper is the elliptical condition of this distribution in the special case of <b>proportional</b> <b>scale</b> matrices of the Student t distributions in the defining convolution. For the bivariate Behrens-Fisher problem, the authors propose a non-informative prior distribution leading to highest posterior density (H. P. D.) regions for the difference of the mean vectors whose coverage probability matches the frequentist coverage probability more accurately than that obtained using the independence-Jeffreys prior distribution, even with small samples. Multivariate Behrens-Fisher distribution Convolution Mixture H. P. D. regions Monte Carlo methods Frequentist coverage...|$|E
30|$|The main {{indicator}} of forest {{health in the}} ICP Forests program is the ‘crown defoliation’ (Eichhorn et al. 2016). It integrates intrinsic tree genetic variability, site effects, and external factors such as abiotic and biotic stresses. Crown defoliation expresses the percentage of lacking leaves with respect to an ideal healthy tree identified through photo guides (Müller and Stierlin 1990; Ferretti 1994) or local reference tree and it is measured by a <b>proportional</b> <b>scale</b> with 5 % steps ranging from 0 % (not defoliated) to 100 % (dead tree). Alongside crown defoliation, damage symptoms (and, when evident, damaging agents) are currently assessed on leaves, branches, and trunk. The most widely used method to report the results is the percentage of trees for a sample population with crown defoliation higher than 25 % (Michel and Seidling 2016), both at plot and national/European levels.|$|E
40|$|Microbial {{engineering}} {{often requires}} fine control over protein expression; for example, to connect genetic circuits 1 - 7 or control flux through a metabolic pathway 8 - 13. We {{have developed a}} predictive design method for synthetic ribosome binding sites that enables the rational control of a protein's production rate on a <b>proportional</b> <b>scale.</b> Experimental validation of over 100 predictions in Escherichia coli shows that the method is accurate to within a factor of 2. 3 over a range of 100, 000 -fold. The design method also correctly predicts that reusing a ribosome binding site sequence in different genetic contexts can result in different protein expression levels. We demonstrate the method's utility by rationally optimizing a protein's expression level to connect a genetic sensor to a synthetic circuit. The proposed forward engineering approach will accelerate the construction and systematic optimization of large genetic systems...|$|E
40|$|We {{revisit the}} classic {{iterative}} <b>proportional</b> <b>scaling</b> (IPS) for contingency table analysis, from a modern optimization perspective. In {{contrast to the}} criticisms made in the literature, we show that based on a coordinate descent characterization, IPS can be slightly modified to deliver coefficient estimates, and from a majorization-minimization standpoint, IPS can be extended to handle log-affine models with general designs. The optimization techniques help accelerate IPS to provide highly salable algorithms for big count data applications, and can adapt IPS to shrinkage estimation {{to deal with a}} large number of variables...|$|R
40|$|This paper revisits {{the classic}} {{iterative}} <b>proportional</b> <b>scaling</b> (IPS) from a modern optimization perspective. In {{contrast to the}} criticisms made in the literature, we show that based on a coordinate descent characterization, IPS can be slightly modified to deliver coefficient estimates, and from a majorization-minimization standpoint, IPS can be extended to handle log-affine models with features not necessarily binary-valued or nonnegative. Furthermore, some state-of-the-art optimization techniques such as block-wise computation, randomization and momentum-based acceleration can be employed to provide more scalable IPS algorithms, {{as well as some}} regularized variants of IPS for concurrent feature selection...|$|R
40|$|We {{introduce}} the beta model for random hypergraphs {{in order to}} represent the occurrence of multi-way interactions among agents in a social network. This model builds upon and generalizes the well-studied beta model for random graphs, which instead only considers pairwise interactions. We provide two algorithms for fitting the model parameters, IPS (iterative <b>proportional</b> <b>scaling)</b> and fixed point algorithm, prove that both algorithms converge if maximum likelihood estimator (MLE) exists, and provide algorithmic and geometric ways of dealing the issue of MLE existence. Comment: 9 pages, 2 figures, Proceedings of 21 st International Conference on Computational Statistics (2014), to appea...|$|R
30|$|Estimation of {{variance}} components and best linear unbiased predictions (BLUPs) of genotype random effects on continuous traits by fitting linear mixed models (LMMs) to familial data is well established. Statistical models with complex variance structures {{that account for}} pedigree as well as spatial trends within a field layout have been extensively applied to such data to assess if the trait of interest has a significant genetic component and is heritable (Piepho et al. 2008). However, for transformed proportional data the use of LMMs can be limiting and results can be unreliable, particularly when sample sizes are variable and small. Furthermore, in case of some transformations such as the angular, model predictions back-transformed to the original <b>proportional</b> <b>scale</b> are not necessarily bounded in the interval [0, 1]. The empirical logit and probit transformations do not suffer from this problem. When estimating genetic parameters, such as the heritability, of binary traits, parameterisation is better handled on an underlying unbounded continuous liability scale {{in which it is}} most interpretable (Lee et al. 2011).|$|E
40|$|Slovenian {{features}} {{at least}} two lexical items that are potential semantic counterparts of the English many, namely "veliko" and "precej", whose meaning appears close to identical. Yet speakers are certain that the two items are not equivalent, although they find intuitively felt differences hard to pinpoint. We argue that "precej" and "veliko" are lexically synonymous, but their meanings are pragmatically strengthened under relevant conditions, which leads to subtle interpretative differences. Specifically, we extend Krifka’s (2007) analysis of double negatives and propose that "veliko" is assigned the stereotypical interpretation of a quantity degree word, whereas "precej" is identified with the non-stereotypical one and consequently relates to moderately big amounts. To support this claim, we report {{the results of an}} experiment involving a sentence-picture verification task, which highlight the similarities and contextually determined differences in the use of both determiners. Our results suggest that the interpretation of "precej" is not consistent with relations in {{the upper part of the}} <b>proportional</b> <b>scale</b> and is dependent on whether or not it is in direct competition with "veliko" in the appropriate contexts...|$|E
40|$|Hofstee and Ten Berge (2004 /this issue) present {{procedures}} {{based on}} an absolute conception of scales in personality assessment as distinct from the dominant interval-scale interpretation. On application, these procedures resulted in a contraction of a 5 -dimensional structure into essentially I personality dimension. McGrath (2004 /this issue) and Ozer (2004 /this issue) comment on the various aspects of these procedures: the transformation of data onto a bipolar <b>proportional</b> <b>scale,</b> {{the adoption of the}} raw scores product average as an index of association, and raw scores principal component analysis. In reply to these comments, we emphasize that the central ingredient in our procedures is the interpretation of the midpoint on a bipolar scale as a threshold. We provide further arguments for that interpretation and demonstrate the robustness of the simplified structure under that central assumption. We acknowledge the comment that our conception does not fit in comparative contexts capitalizing on individual differences but argue that other contexts involving thresholds are relevant to the study of personality. We also acknowledge that item pools should be sufficiently homogeneous for scales to be meaningful...|$|E
40|$|Context {{specific}} interaction {{models is}} {{a class of}} interaction models for contingency tables in which interaction terms are allowed to vanish in specific contexts given by the levels of sets of variables. Such restrictions can entail conditional independencies which only hold for some values of the conditioning variables and allows also for irrelevance of some variables in specific contexts. A Markov property is established and so is an iterative <b>proportional</b> <b>scaling</b> algorithm for maximum likelihood estimation. Decomposition of the estimation problem is treated and model selection is discussed. Copyright Board of the Foundation of the Scandinavian Journal of Statistics 2004. ...|$|R
40|$|The {{assumption}} of full proportionality {{is incorporated in}} the constant returns-to-scale (CRS) technology and allows for <b>proportional</b> <b>scaling</b> of inputs and outputs of production units. The {{assumption of}} selective proportionality was recently incorporated in the hybrid returns-to-scale (HRS) technology in which only a subset of outputs is proportional to a subset of inputs. In this paper we develop a production technology that exhibits both the full and selective proportionality at the same time. Real examples of such technology are pointed out. Subject to certain conditions, the DEA models based on this technology provide better discrimination than the CRS and HRS models...|$|R
40|$|This paper {{develops}} log {{linear model}} representations for the Bradley-Terry paired comparisons and Luce multiple comparisons models. Various multivariate {{extensions of the}} Bradley-Terry model are then considered. A model is presented, similar to one of Davidson & Bradley (1969), but {{which is based on}} the log odds ratio as a measure of association rather than the coefficient of correlation. It is shown that, by the adaptive use of log linear model theory, complex data seta involving paired and multiple comparisons as well as rankings may be easily analyzed. In all cases estimates of cell expectations may be computed using an iterative <b>proportional</b> <b>scaling</b> algorithm that is computationally easy to carry out...|$|R
40|$|Understanding {{variability}} of population abundances is of central concern to theoretical and applied evolutionary ecology, yet quantifying the conceptually simple idea has been substantially problematic. Standard statistical measures of variability are particularly biassed by rare events, zero counts and other ‘non-Gaussian ’ behaviour, {{which are often}} inappropriately weighted or excluded from analysis. I conjecture that these problems are primarily a function of calculating variation as deviation from an average abundance, while the average may not be static, nor actually reflect abundance {{at any point in}} the time series. Here I describe a simple metric (population variability PV) that quantifies variability as the average percent difference between all combinations of observed abundances. Zero counts can be included if desired. Similar to standard metrics, variability is measured on a <b>proportional</b> <b>scale,</b> facilitating comparative applications. Standard metrics are based on Gaussian distributions, are over-sensitive to rare events and heavy tailed behaviour, and can inappropriately indicate ‘more time-more variation ’ effects (reddened spectrum). Here I demonstrate that, while PV behaves similarly for ‘normal ’ time series, it is independent of deviation from mean abundance for heavy tailed distributions, its robustness to non-Gaussian behaviour resolves artificial reddened spectrum issues, and variability calculated using PV from short time series is substantially more accurate at estimating known long term variability than standard metrics. PV therefore provides common ground for evaluating the {{variability of}} population...|$|E
40|$|Jonathan Evans, FCAS, MAAA For {{problems}} such as rating excess of toss remsurance and estimating deductible credits, actuaries frequently employ exposure rating factors. In the context of property insurance this {{takes the form of}} loss tables such as the Lloyds scale or Salzmann tables. These tables display the fraction of loss cost retained for layers expressed as fractions of insured value, or policy limit. In the liability insurance context, Increased Limits Factors (ILFs) or Excess Loss Factors (ELFs) tables are expressed in terms of actual dollar amounts for attachment points and limits. Implicit in the property tables is the assumption that an increase in policy limit or insured value corresponds to a <b>proportional</b> <b>scale</b> factor increase in the claim severity random variable, but other than the change in scale the distribution of claim sizes remains the same and any increase or decrease in loss cost per exposure is frequency based. Without a special adjustment to the loss cost or premium rate, the implied loss frequency is the same lor the larger policy. Implicit in the liability tables is the assumption that larger policies produce the same distribution of claim severity. In summary, the property perspective generally assumes that all the extra exposure shows up as larger claims, and the liability perspective generally assumes that all the extra exposure shows up as more claims. This paper shows how both perspectives for claim severity, and additional considerations of frequency changes may easily be incorporated into a unified model. Additionally, such a unilied approach allows tbr a compromise where increasing exposure for a given policy or risk may be partially reflected in the scale of claim size and partially in the frequency...|$|E
40|$|Background: Risk of {{the outcome}} is a {{mathematical}} determinant of the absolute treatment benefit of an intervention, yet this can vary substantially within a trial population, complicating interpretation of trial results. Methods: We derived risk models using Cox or logistic regression {{on a set of}} large publically available RCTs. Risk heterogeneity was evaluated using the extreme quartile risk ratio (EQRR, the ratio of outcome rates in the lowest risk quartile to that in the highest). Skewness was evaluated with median to mean risk ratio (MMRR, the ratio of risk in the median risk patient to the average). Heterogeneity of treatment effect (HTE) across risk strata was also examined. Results: We describe 39 analyses using data from 32 large trials, with event rates across studies ranging from 3 %- 63 % (median= 15 %, interquartile range [IQR]= 9 %- 29 %). C-statistics of risk models ranged from 0. 59 - 0. 89 (median= 0. 70, IQR= 0. 65 - 0. 71). The EQRR ranged from 1. 9 - 35. 2 (median= 4. 0, IQR= 3. 1 - 5. 4). The MMRR ranged from 0. 4 - 1. 0 (median= 0. 86, IQR= 0. 80 - 0. 92). EQRRs were predictably higher and MMRRs predictably lower as the C-statistic increased or the overall outcome incidence decreased. Among 18 comparisons with a significant overall treatment effect, there was a significant interaction between treatment and baseline risk on the <b>proportional</b> <b>scale</b> in only one. The difference in the absolute risk reduction between extreme risk quartiles ranged from - 3. 2 - 28. 3 % (median= 5. 1 %; IQR= 0. 3 - 10. 9). Conclusions: There is typically substantial variation in outcome risk in clinical trials, commonly leading to clinically significant differences in absolute treatment effects. Most patients have outcome risks lower than the trial average reflected in the summary result. Risk stratified trial 3 analyses are feasible and may be clinically informative, particularly when the outcome is predictable and uncommon...|$|E
5000|$|If {{velocities}} are constant, then {{time scales}} must be directly <b>proportional</b> to distance <b>scales.</b>|$|R
40|$|The {{computational}} cost of Gaussian process regression grows cubically {{with respect}} to the number of variables due to the inversion of the covariance matrix, which is impractical for data sets with more than a few thousand nodes. Furthermore, Gaussian processes lack the ability to represent conditional independence assertions between variables. We describe iterative <b>proportional</b> <b>scaling</b> for directly estimating the precision matrix without inverting the covariance matrix, given an undirected graph and a covariance function or data. We introduce a variant of the Shafer-Shenoy algorithm combined with IPS that runs in O(nC 3) -time, where C is the largest clique size in the induced junction tree. We present results on synthetic data and temperature prediction in a real sensor network. ...|$|R
40|$|R) -[11 C]PK 11195 {{has been}} used for {{quantifying}} cerebral microglial activation in vivo. In previous studies, both plasma input and reference tissue methods have been used, usually in combination with a region of interest (ROI) approach. Definition of ROIs, however, can be labourious and prone to interobserver variation. In addition, results are only obtained for predefined areas and (unexpected) signals in undefined areas may be missed. On the other hand, standard pharmacokinetic models are too sensitive to noise to calculate (R) -[11 C]PK 11195 binding on a voxel-by-voxel basis. Linearised versions of both plasma input and reference tissue models have been described, and these are more suitable for parametric imaging. The {{purpose of this study was}} to compare the performance of these plasma input and reference tissue parametric methods on the outcome of statistical parametric mapping (SPM) analysis of (R) -[11 C]PK 11195 binding. Dynamic (R) -[11 C]PK 11195 PET scans with arterial blood sampling were performed in 7 younger and 11 elderly healthy subjects. Parametric images of volume of distribution (Vd) and binding potential (BP) were generated using linearised versions of plasma input (Logan) and reference tissue (Reference Parametric Mapping) models. Images were compared at the group level using SPM with a two-sample t-test per voxel, both with and without <b>proportional</b> <b>scaling.</b> Parametric BP images without scaling provided the most sensitive framework for determining differences in (R) -[11 C]PK 11195 binding between younger and elderly subjects. Vd images could only demonstrate differences in (R) -[11 C]PK 11195 binding when analysed with <b>proportional</b> <b>scaling</b> due to intersubject variation in K 1 /k 2 (blood-brain barrier transport and non-specific binding...|$|R
