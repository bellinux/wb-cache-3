3|153|Public
30|$|Figure  1 gives {{a diagram}} of the axial {{cross-section}} through a specimen for tensile testing. In its fabrication, {{we had to take}} special care in mixing the raw materials, melting, casting, annealing, and <b>precision</b> <b>working</b> to fix the final form. As stress in the gripping regions is complicated, and should be eliminated, two specimens with identical grip sizes but different gauge lengths were prepared (Takahashi and Motegi 1987).|$|E
40|$|Inversion {{of almost}} {{arbitrary}} Laplace transforms is effected by trapezoidal integration along a special contour. The number n of points {{to be used}} {{is one of several}} parameters, in most cases yielding absolute errors of order 10 - 7 for n = 10, 10 - 11 for n = 20, 10 - 23 for n = 40 (with double <b>precision</b> <b>working),</b> and so on, for all values of the argument from 0 + up to some large maximum. The extreme accuracy of which the method is capable means that it has many possible applications of various kinds, and some of these are indicated...|$|E
40|$|The {{potential}} impact of nanomaterials {{on the environment and}} on human health has already triggered legislation requiring labelling of products containing nanoparticles. However, so far, no validated analytical methods for the implementation of this legislation exist. This paper outlines a generic approach for the validation of methods for detection and quantification of nanoparticles in food samples. It proposes validation of identity, selectivity, <b>precision,</b> <b>working</b> range, limit of detection and robustness, bearing in mind that each "result" must include information about the chemical identity, particle size and mass or particle number concentration. This has an impact on testing for selectivity and trueness, which also must take these aspects into consideration. Selectivity must not only be tested against matrix constituents and other nanoparticles, but it shall also be tested whether the methods apply equally well to particles of different suppliers. In trueness testing, information whether the particle size distribution has changed during analysis is required. Results are largely expected to follow normal distributions due to the expected high number of particles. An approach of estimating measurement uncertainties from the validation data is given. (Résumé d'auteur...|$|E
40|$|In the paper, {{we examine}} the {{behavior}} of the Newton's method in floating point arithmetic for the computation of a simple zero of a polynomial. We allow an extended <b>precision</b> (twice the <b>working</b> <b>precision)</b> in the computation of the residual. We prove that, for a sufficient number of iteration, the zero is as accurate as if computed in twice the <b>working</b> <b>precision.</b> We provides numerical experiments confirming this...|$|R
40|$|International audienceIn the paper, {{we examine}} the local {{behavior}} of Newton’s method in floating point arithmetic for the computation of a simple zero of a polynomial assuming that an good initial approximation is available. We allow an extended <b>precision</b> (twice the <b>working</b> <b>precision)</b> in the computation of the residual. We prove that, for {{a sufficient number of}} iterations, the zero is as accurate as if computed in twice the <b>working</b> <b>precision.</b> We provide numerical experiments confirming this...|$|R
40|$|We {{propose a}} general {{algorithm}} for solving a n× n nonsingular linear system Ax = b based on iterative refinement with three <b>precisions.</b> The <b>working</b> <b>precision</b> {{is combined with}} possibly different precisions for solving for the correction term and for computing the residuals. Via rounding error analysis of the algorithm we derive sufficient conditions for convergence and bounds for the attainable normwise forward error and normwise and componentwise backward errors. Our results generalize and unify many existing rounding error analyses for iterative refinement. With single <b>precision</b> as the <b>working</b> <b>precision,</b> we show that by using LU factorization in IEEE half precision as the solver and calculating the residuals in double precision {{it is possible to}} solve Ax = b to full single precision accuracy for condition numbers κ_ 2 (A) < 10 ^ 4, with the O(n^ 3) part of the computations carried out entirely in half precision. We show further that by solving the correction equations by GMRES preconditioned by the LU factors the restriction on the condition number can be weakened to κ_ 2 (A) < 10 ^ 8, although in general {{there is no guarantee that}} GMRES will converge quickly. Taking for comparison a standard Ax = b solver that uses LU factorization in single precision, these results suggest that on architectures for which half precision is efficiently implemented it will be possible to solve certain linear systems Ax = b up to twice as fast and to greater accuracy. Analogous results are given with double <b>precision</b> as the <b>working</b> <b>precision...</b>|$|R
40|$|AbstractThere {{is growing}} {{interest}} in performing ever more complex classification tasks on mobile and embedded devices in real-time, which results in the need for e_cient implementations of the respective algorithms. Support vector machines (SVMs) represent a powerful class of nonlinear classifiers, and reducing the <b>working</b> <b>precision</b> represents a promising approach to achieving e_cient implementations of the SVM classification phase. However, the relationship between SVM classification accuracy and the arithmetic precision used is not yet su_ciently understood. We investigate this relationship in floating-point arithmetic and illustrate that often a large reduction in the <b>working</b> <b>precision</b> of the classification process is possible without loss in classification accuracy. Moreover, we investigate the adaptation of bounds on allowable SVM parameter perturbations in order to estimate the lowest possible <b>working</b> <b>precision</b> in floating-point arithmetic. Among the three representative data sets considered in this paper, none requires a precision higher than 15 bit, which is a considerable reduction from the 53 bit used in double precision floating-point arithmetic. Furthermore, we demonstrate analytic bounds on the <b>working</b> <b>precision</b> for SVMs with Gaussian kernel providing good predictions of possible reductions in the <b>working</b> <b>precision</b> without sacrificing classification accuracy...|$|R
5000|$|As {{a rule of}} thumb, {{iterative}} refinement for Gaussian elimination {{produces a}} solution correct to <b>working</b> <b>precision</b> if double the <b>working</b> <b>precision</b> {{is used in the}} computation of , e.g. by using quad or double extended precision IEEE 754 floating point, and if [...] is not too ill-conditioned (and the iteration and the rate of convergence are determined by the condition number of [...] ).|$|R
40|$|Algorithms for {{summation}} and {{dot product}} of {{floating point numbers}} are presented which are fast in terms of measured computing time. We show that the computed results are as accurate as if computed in twice or K-fold <b>working</b> <b>precision,</b> K ≥ 3. For twice the <b>working</b> <b>precision</b> our algorithms for summation and dot product are some 40 % faster than the corresponding XBLAS routines while sharing similar error estimates. Our algorithms are widely applicable because they require only addition, subtraction and multiplication of floating point numbers in the same <b>working</b> <b>precision</b> as the given data. Higher precision is unnecessary, algorithms are straight loops without branch, and no access to mantissa or exponent is necessary...|$|R
50|$|Two {{definitions}} {{were allowed}} for {{the determination of the}} 'tiny' condition: before or after rounding the infinitely precise result to <b>working</b> <b>precision,</b> with unbounded exponent.|$|R
40|$|Sterling cast beads, glass beads, turquoise, {{sterling}} found beads 82 ̆ 2 x 102 ̆ 2 These pieces {{require a}} lot of careful counting of tiny beads to get the proper gradations in size. Making them satisfies my need for <b>precision,</b> for <b>working</b> at a high level of craftsmanship and for finding the order in a chaotic universe. [URL]...|$|R
60|$|Thereupon Pinkerton {{gave him}} the whole tale, {{beginning}} with a business-like <b>precision,</b> and <b>working</b> himself up, as he went on, to the boiling-point of narrative enthusiasm. Nares sat and smoked, hat still on head, and acknowledged each fresh feature of the story with a frowning nod. But his pale blue eyes betrayed him, and lighted visibly.|$|R
40|$|International audienceSeveral {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of resultscomputed in a fixed finite precision. Here we focus on methods to improve the accuracyof summation, dot product and polynomial evaluation. Such algorithms exist real floatingpoint numbers. In this paper, we provide new algorithms which deal with complex floatingpoint numbers. We show that the computed results are as accurate as if computed intwice the <b>working</b> <b>precision.</b> The algorithms are simple since they only require additionsubtraction and multiplication of floating point numbers in the same <b>working</b> <b>precision</b> asthe given data...|$|R
40|$|Several {{different}} techniques {{intend to}} improve the accuracy of results computed in floating-point precision. Here, we focus on a method {{to improve the}} accuracy of the evaluation of rational functions. We present a compensated algorithm to evaluate rational functions. This is algorithm is accurate and fast. The accuracy of the computed result is similar to the one given by the classical algorithm computed in twice the <b>working</b> <b>precision</b> and then rounded to the current <b>working</b> <b>precision.</b> This algorithm runs much more faster than existing implementation producing the same output accuracy...|$|R
40|$|AbstractSeveral {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of results computed in a fixed finite precision. Here we focus on methods to improve the accuracy of summation, dot product and polynomial evaluation. Such algorithms exist real floating point numbers. In this paper, we provide new algorithms which deal with complex floating point numbers. We show that the computed results are as accurate as if computed in twice the <b>working</b> <b>precision.</b> The algorithms are simple since they only require addition, subtraction and multiplication of floating point numbers in the same <b>working</b> <b>precision</b> as the given data...|$|R
40|$|Information Retrieval Systems (IRSs) {{based on}} an ordinal fuzzy {{linguistic}} approach present some problems of loss of information and lack of <b>precision</b> when <b>working</b> with discrete linguistic expression domains or when applying approximation operations in the symbolic aggregation methods. In this paper, we present a new IRS model based on the 2 -tuple fuzzy linguistic approach, which allows us to overcome the problems of ordinal fuzzy linguistic IRSs and improve their performance...|$|R
50|$|In 1978, Wu resumed {{her studies}} as a master's student Tsinghua University, researching <b>precision</b> {{instruments}} and <b>working</b> {{as an assistant}} engineer. From 1981 to 1986, Wu studied for a PhD at the Swiss Federal Institute of Technology in Zurich.|$|R
40|$|A Chebyshev {{tensor product}} surface {{is widely used}} in image {{analysis}} and numerical approximation. This article illustrates an accurate evaluation for the surface in form of Chebyshev tensor product. This algorithm {{is based on the}} application of error-free transformations to improve the traditional Clenshaw Chebyshev tensor product algorithm. Our error analysis shows that the error bound is u+Ou 2 ×condP,x,y in contrast to classic scheme u×cond(P,x,y), where u is <b>working</b> <b>precision</b> and condP,x,y is a condition number of bivariate polynomial P(x,y), which means that the accuracy of the computed result is similar to that produced by classical approach with twice <b>working</b> <b>precision.</b> Numerical experiments verify that the proposed algorithm is stable and efficient...|$|R
40|$|Abstract. We {{present a}} {{compensated}} Horner scheme, {{that is an}} accurate and fast algorithm to evaluate univariate polynomials in floating point arithmetic. The accuracy of the computed result {{is similar to the}} one given by the Horner scheme computed in twice the <b>working</b> <b>precision.</b> This compensated Horner scheme runs at least as fast as existing implementations producing the same output accuracy. We also propose to compute in pure floating point arithmetic a valid error estimate that bound the actual accuracy of the compensated evaluation. Numerical experiments involving ill-conditioned polynomials illustrate these results. All algorithms are performed at a given <b>working</b> <b>precision</b> and are portable assuming the floating point arithmetic satisfies the IEEE- 754 standard...|$|R
40|$|Several {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of results computed in a fixed finite precision. Here we focus on a method to improve {{the accuracy of the}} product of floating point numbers. We show that the computed result is as accurate as if computed in twice the <b>working</b> <b>precision.</b> The algorithm is simple since it only requires addition, subtraction and multiplication of floating point numbers in the same <b>working</b> <b>precision</b> as the given data. Such an algorithm can be useful for example to compute the determinant of a triangular matrix and to evaluate a polynomial when represented by the root product form. It {{can also be used to}} compute the power of a floating point number...|$|R
40|$|International audienceSeveral {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of results computed in a fixed finite precision. Here we focus on a method to improve the accuracy of polynomial evaluation via Horner’s scheme. Such an algorithm exists for polynomials with real floating point coefficients. In this paper, we provide a new algorithm which deals with polynomials with complex floating point coefficients. We show that the computed result is as accurate as if computed in twice the <b>working</b> <b>precision.</b> The algorithm is simple since it only requires addition, subtraction and multiplication of floating point numbers in the same <b>working</b> <b>precision</b> as the given data. Such an algorithm can be useful for example to compute zeros of polynomial by Newton-like methods...|$|R
40|$|Several {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of results computed in a fixed finite precision. Here we focus on a method to improve {{the accuracy of the}} polynomial evaluation. It is well known that the use of the Fused Multiply and Add operation available on some microprocessors like Intel Itanium improves slightly the accuracy of the Horner scheme. In this paper, we propose an accurate compensated Horner scheme specially designed {{to take advantage of the}} Fused Multiply and Add. We prove that the computed result is as accurate as if computed in twice the <b>working</b> <b>precision.</b> The algorithm we present is fast since it only requires well optimizable floating point operations, performed in the same <b>working</b> <b>precision</b> as the given data...|$|R
40|$|Micro air {{abrasion}} {{is a new}} modern method for removing decay and old fillings. It is used a fine stream of aluminium oxide powder and compressed air. Micro {{air abrasion}} was discovered in 1940 in USA. Sandman’s unique patented whirl system and special hand pieces secure a unique <b>precision</b> and <b>working</b> condition of the aluminium oxide powder. Dentists can obtain optimal results using the unit with a working pressure between 1 ½ - 3 bars. In comparison, other systems are used pressure working area of 4 - 8 bars...|$|R
40|$|We {{present a}} {{compensated}} Horner scheme, {{that is an}} accurate and fast algorithm to evaluate univariate polynomials in floating point arithmetic. The accuracy of the computed result {{is similar to the}} one given by the Horner scheme computed in twice the <b>working</b> <b>precision.</b> This compensated Horner scheme runs at least as fast as existing implementations producing the same output accuracy. We also propose to compute in pure floating point arithmetic a valid error estimate that bound the actual accuracy of the compensated evaluation. Numerical experiments involving ill-conditioned polynomials illustrate these results. All algorithms are performed at a given <b>working</b> <b>precision</b> and are portable assuming the floating point arithmetic satisfies the IEEE- 754 standard. Keywords: IEEE- 754 floating point arithmetic, error-free transformations, extended precision, polynomial evaluation, compensated Horner scheme, running error bound Résum...|$|R
40|$|Abstract—Several {{different}} {{techniques and}} softwares intend {{to improve the}} accuracy of results computed in a fixed finite precision. Here, we focus on a method to improve {{the accuracy of the}} product of floating-point numbers. We show that the computed result is as accurate as if computed in twice the <b>working</b> <b>precision.</b> The algorithm is simple since it only requires addition, subtraction, and multiplication of floating-point numbers in the same <b>working</b> <b>precision</b> as the given data. Such an algorithm can be useful for example to compute the determinant of a triangular matrix and to evaluate a polynomial when represented by the root product form. It {{can also be used to}} compute the integer power of a floating-point number. Index Terms—Accurate product, exponentiation, finite precision, floating-point arithmetic, faithful rounding, error-free transformations. ...|$|R
30|$|Further {{research}} {{is necessary and}} should focus on improving scan sequences and registration algorithms, {{in order to further}} improve the <b>precision</b> and thereby <b>working</b> towards clinical validation. Consequently, once this technique is validated within a patient cohort, low-field MRI is suggested to be a marker free and radiation free alternative for RSA.|$|R
40|$|By {{viewing the}} {{nonuniform}} discrete Fourier transform (NUDFT) as a perturbed {{version of a}} uniform discrete Fourier transform, we propose a fast, stable, and simple algorithm for computing the NUDFT that costs O(N N(1 /ϵ) /(1 /ϵ)) operations based on the fast Fourier transform, where N {{is the size of}} the transform and 0 <ϵ < 1 is a <b>working</b> <b>precision.</b> Our key observation is that a NUDFT and DFT matrix divided entry-by-entry is often well-approximated by a low rank matrix, allowing us to express a NUDFT matrix as a sum of diagonally-scaled DFT matrices. Our algorithm is simple to implement, automatically adapts to any <b>working</b> <b>precision,</b> and is competitive with state-of-the-art algorithms. In the fully uniform case, our algorithm is essentially the FFT. We also describe quasi-optimal algorithms for the inverse NUDFT and two-dimensional NUDFTs. Comment: 18 page...|$|R
30|$|So far, {{there have}} emerged many {{improved}} versions of ETAM method, {{most of which}} endeavor to improve the bearing estimation <b>precision</b> by <b>working</b> on the overlap correlator [7, 8]. But in this paper, we try to improve the angular resolution of ETAM method. As well known, in real sea environment, the effective synthetic aperture {{is limited by the}} coherence length of underwater signals. In some adverse sea environments [9], the effective synthetic aperture size is only 1.5 times of the physical array. Thus, how to further improve the bearing resolution within the effective synthetic aperture is a meaningful problem.|$|R
40|$|Abstract: The magneto-rheological (MR) {{vibration}} reduction control {{technology is}} studied and a {{design and implementation}} of an automobile MR vibration reduction control system based on ARM 9 is given in this paper. This paper introduces the system design process of embedded controller, current driver and other hardware, system software structure. Design of real time control system can realize the multi-channel vibration signal acquisition, control algorithm, CAN communications and other functions. The simulation and experiment show that, the control system software and hardware design can meet the functional requirements and have the advantages of fast response, high control <b>precision</b> and <b>working</b> stability...|$|R
5000|$|The spheres {{range in}} size from a few {{centimetres}} to over 2 m in diameter, and weigh up to 15 tons. [...] Most are sculpted from gabbro, the coarse-grained equivalent of basalt. There are a dozen or so made from shell-rich limestone, and another dozen made from a sandstone. They appear to have been made by hammering natural boulders with other rocks, then polishing with sand. The degree of finishing and <b>precision</b> of <b>working</b> varies considerably. The gabbro came from sites in the hills, several kilometres away from where the finished spheres are found, though some unfinished spheres remain in the hills.|$|R
40|$|We {{present a}} new {{algorithm}} that solves linear triangular systems accurately and efficiently. By accurately, we mean that this algorithm should yield a solution {{as accurate as}} the one computed in twice the <b>working</b> <b>precision.</b> By efficiently, we mean that its implementation should run faster than the corresponding XBLAS routine with the same output accuracy...|$|R
40|$|International audienceA {{commonly}} used argument reduction technique in elementary function computations begins with two positive {{floating point numbers}} α and γ that approximate (usually irrational but not necessarily) numbers 1 /C and C, e. g., C = 2 π for trigonometric functions and ln 2 for ex. Given an argument to the function of interest it extracts z as defined by xα = z + ς with z = k 2 −N and |ς| ≤ 2 −N− 1, where k,N are integers and N ≥ 0 is preselected, and then computes u = x − zγ. Usually zγ takes more bits than the <b>working</b> <b>precision</b> provides for storing its significand, and thus exact x−zγ may not be represented exactly by a floating point number of the same precision. This will cause performance penalty when the <b>working</b> <b>precision</b> is the highest available on the underlying hardware and thus considerable extra work is needed {{to get all the}} bits of x − zγ right. This paper presents theorems that show under mild conditions that can be easily met on today's computer hardware and still allow α ≈ 1 /C and γ ≈ C to almost the full <b>working</b> <b>precision,</b> x−zγ is a floating point number of the same precision. An algorithmic procedure based on the theorems is obtained. The results will enhance performance, in particular on machines that has hardware support for fused multiply-add (fma) instruction(s) ...|$|R
40|$|AbstractThe foundation's {{stability}} of theodolite {{is very important}} complication of <b>working</b> <b>precision.</b> Bring forwarda new model of vibration isolation. Through analysis and comparison of several control methods, choose the absolute velocity feedback control method. The results of MATLAB simulation and experiment {{results indicated that the}} method can advance the effect of vibration isolation and strengthen the foundation's {{stability of}} theodolite...|$|R
40|$|International audienceThis {{paper is}} {{concerned}} with the fast and accurate evaluation of elementary symmetric functions. We present a new compensated algorithm by applying error-free transformations to improve the accuracy of the so-called Summation Algorithm, which is used, by example, in the MATLAB's poly function. We derive a forward round off error bound and running error bound for our new algorithm. The round off error bound implies that the computed result is as accurate as if computed with twice the <b>working</b> <b>precision</b> and then rounded to the current <b>working</b> <b>precision.</b> The running error analysis provides a shaper bound along with the result, without increasing significantly the computational cost. Numerical experiments illustrate that our algorithm runs much faster than the algorithm using the classic double-double library while sharing similar error estimates. Such an algorithm can be widely applicable for example to compute characteristic polynomials from eigen values. It can also be used into the Rasch model in psychological measurement...|$|R
40|$|AbstractVisual {{working memory}} {{is a system}} used to hold {{information}} actively in mind for a limited time. The number of items and the precision {{with which we can}} store information has limits that define its capacity. How much control do we have over the precision with which we store information when faced with these severe capacity limitations? Here, we tested the hypothesis that rank-ordered attentional priority determines the <b>precision</b> of multiple <b>working</b> memory representations. We conducted two psychophysical experiments that manipulated the priority of multiple items in a two-alternative forced choice task (2 AFC) with distance discrimination. In Experiment 1, we varied the probabilities with which memorized items were likely to be tested. To generalize the effects of priority beyond simple cueing, in Experiment 2, we manipulated priority by varying monetary incentives contingent upon successful memory for items tested. Moreover, we illustrate our hypothesis using a simple model that distributed attentional resources across items with rank-ordered priorities. Indeed, we found evidence in both experiments that priority affects the <b>precision</b> of <b>working</b> memory in a monotonic fashion. Our results demonstrate that representations of priority may provide a mechanism by which resources can be allocated to increase the precision with which we encode and briefly store information...|$|R
40|$|Abstract. We {{investigate}} how extra-precise accumulation of dot products {{can be used}} to solve ill-conditioned linear systems accurately. For a given p-bit <b>working</b> <b>precision,</b> extra-precise evaluation of a dot product means that the products and summation are executed in 2 p-bit precision, and that the final result is rounded into the p-bit <b>working</b> <b>precision.</b> Denote by u = 2 p the relative rounding error unit in a given <b>working</b> <b>precision.</b> We treat two types of matrices: first up to condition number u 1, and second up to condition number u 2. For both types of matrices we present two types of methods: first for calculating an approximate solution, and second for calculating rigorous error bounds for the solution together with the proof of non-singularity of the matrix of the linear system. In {{the first part of this}} paper we present algorithms using only rounding to nearest, in Part II we use directed rounding to obtain better results. All algorithms are given in executable Matlab code and are available from my homepage. Key words. Linear systems, rounding to nearest, (extremely) ill-conditioned, error-free transformation, error analysis, rigorous error bounds. AMS subject classications. 65 F 05, 65 G 20 1. Introduction and notation. The solution of a linear system Ax = b is a ubiquitous task in numerical computations. In Part I and II of this paper we present different methods to compute guaranteed error bounds for the solution of a linear system, i. e. with a certied accuracy. The methods in this Part...|$|R
