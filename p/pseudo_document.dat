6|24|Public
40|$|Abstract [...] Search Engines are {{the major}} data source of the current world. In this {{situation}} performance have very much importance. When a user submits queries to a search engine it returns millions of results. From these results users will select what they want. Sometimes they may achieve their goals from the first attempt, sometimes may not. In this paper we propose an approach to find the user search goals by analyzing search engines query logs and search history of the current user. In the first step creating a feedback session from given query and {{the next step is}} conversion of feedback session to a useful form called <b>pseudo</b> <b>document.</b> Then create the clusters of <b>pseudo</b> <b>document</b> to obtain the user search goals. In this clusters organize the result based on page ranking. Finally optimization of the number of clusters based Classified Average Precision (CAP) Evaluation...|$|E
40|$|Latent Semantic Analysis (LSA) {{is widely}} used for finding the {{documents}} whose semantic {{is similar to the}} query of keywords. Although LSA yield promising similar results, the existing LSA algorithms involve lots of unnecessary operations in similarity computation and candidate check during on-line query processing, which is expensive in terms of time cost and cannot efficiently response the query request especially when the dataset becomes large. In this paper, we study the efficiency problem of on-line query processing for LSA towards efficiently searching the similar documents to a given query. We rewrite the similarity equation of LSA combined with an intermediate value called partial similarity that is stored in a designed index called partial index. For reducing the searching space, we give an approximate form of similarity equation, and then develop an efficient algorithm for building partial index, which skips the partial similarities lower than a given threshold θ. Based on partial index, we develop an efficient algorithm called ILSA for supporting fast on-line query processing. The given query is transformed into a <b>pseudo</b> <b>document</b> vector, and the similarities between query and candidate documents are computed by accumulating the partial similarities obtained from the index nodes corresponds to non-zero entries in the <b>pseudo</b> <b>document</b> vector. Compared to the LSA algorithm, ILSA reduces the time cost of on-line query processing by pruning the candidate documents that are not promising and skipping the operations that make little contribution to similarity scores. Extensive experiments through comparison with LSA have been done, which demonstrate the efficiency and effectiveness of our proposed algorithm...|$|E
40|$|Abstract. We {{propose a}} {{proximity}} probabilistic model (PPM) that advances a bag-of-words probabilistic retrieval model. In our proposed model, a document is transformed to a <b>pseudo</b> <b>document,</b> {{in which a}} term count is propagated to other nearby terms. Then we consider three heuristics, i. e., the distance of two query term occurrences, their order, and term weights, and try four kernel functions in measuring a positiondependent term count, which {{can be viewed as}} a pseudo term frequency. Finally, we integrate term proximity into the probabilistic model BM 25 by using the pseudo term frequency to replace term frequency. Experimental results on TREC data sets indicate that the proximity probabilistic model with the reverse kernel function consistently improves the BM 25 model by 5 %- 11 %, in terms of Mean Average Precision. ...|$|E
40|$|Web search {{applications}} represent {{user information}} needs by submission of query to search engine. But still the entire query submitted to search engine doesn’t satisfy the user information needs, because users {{may want to}} get information on diverse aspects when they submit the same query. From this discovering the numeral of dissimilar user search goals for query and depicting each goal with several keywords automatically become complicated. The suggestion and examination of user search goals can be very valuable in improving search engine importance and user knowledge. Discovering the numeral of dissimilar user search goals for query by k-means clustering with user feedback sessions. Efficiently reflect user information needs generate a pseudo-document to map the different user feedback sessions. clustering <b>Pseudo</b> <b>documents</b> with K means clustering result are computationally difficult and semantic similarity between the pseudo terms is also important while clustering. To conquer this problem proposed a FCM clustering algorithm to group the <b>pseudo</b> <b>documents</b> and it also measure the semantic similarity between the pseudo terms in the documents using wordnet. The FCM algorithm divides <b>pseudo</b> <b>documents</b> data for dissimilar size cluster by using fuzzy systems. FCM choosing cluster size and central point depend on fuzzy model. The FCM clustering algorithm it congregate quickly to a local optimum or grouping of the <b>pseudo</b> <b>documents</b> in well-organized way. Semantic similarity between the pseudo terms with Wordnet based similarity is used for comparing the similarity and diversity of pseudo terms. Finally experimental result measures the clustering results with parameters like classified average precision (CAP), Voted AP (VAP), risk to avoid classifying search results and average precision (AP). It shows FCM based system improve the feedback sessions outcome than the normal <b>pseudo</b> <b>documents...</b>|$|R
40|$|Abstract- In web search applications, {{queries are}} {{submitted}} to search engines {{to represent the}} information needs of users. Discovering the number of diverse user search goals for a query and depicting each goal with some keywords automatically. In the existing work propose a novel approach to infer user search goals by analyzing search engine query logs. First propose a novel approach to infer user search goals for a query by clustering our proposed feedback sessions. Second we propose a novel optimization method to map feedback sessions to pseudo-documents which can efficiently reflect user information needs. In the end, we cluster these <b>pseudo</b> <b>documents</b> to infer user search goals and depict them with some keywords. In proposed system k means clustering algorithm is computationally difficult, in order to overcome the k means clustering problem, enhancement a Fuzzy c-means clustering (FCM) algorithm to group the <b>pseudo</b> <b>documents</b> and it also measure the similarity between the pseudo terms in the documents, it improves the feedback sessions results than the normal <b>pseudo</b> <b>documents.</b> The FCM algorithm divides <b>pseudo</b> <b>documents</b> data for dissimilar size cluster by using fuzzy systems. FCM choosing cluster size and central point depend on fuzzy model. The FCM clustering algorithm it congregate quickly to a local optimum or grouping of the <b>pseudo</b> <b>documents</b> in wellorganized wayAnnotation of the search results of the database we proposed a annotation based results. We used Automatic annotation approach that first aligns the data units on a result page into different groups such that the data in the same group have the same semantic results. Finally measures the clustering results classified average precision (CAP) to evaluate {{the performance of the}} restructured web search results. Keywords-pseudo-documents, k means clustering, Fuzzy c-means clustering (FCM), annotation,classified average precision (CAP) IJSER 1...|$|R
40|$|Pseudo {{feedback}} is a commonly used technique to improve information retrieval performance. It assumes a few top-ranked documents to be relevant, and learns from them {{to improve the}} retrieval accuracy. A serious {{problem is that the}} performance is often very sensitive to the number of <b>pseudo</b> feedback <b>documents.</b> In this poster, we address this problem in a language modeling framework. We propose a novel twostage mixture model, which is less sensitive to the number of <b>pseudo</b> feedback <b>documents</b> than an effective existing feedback model. The new model can tolerate a more flexible setting of the number of <b>pseudo</b> feedback <b>documents</b> without the danger of losing much retrieval accuracy...|$|R
40|$|Abstract — Different users {{may have}} {{dissimilar}} search {{purpose when they}} submit large topic and ambiguous query, to a search engine. The inference and analysis of user search goals is an important point in terms of improving performance of search engine. A novel approach is used to infer user search goals by analyzing search engine query logs. First, we propose frameworks {{that are used to}} find out different user search goals for a user submitted query. After that we use K-means algorithm for clustering the proposed pseudo documents generated from feedback session. The Feedback sessions are implement by using user click-through data and efficiently reflect the information needs of users for that query. Second step is that generation of pseudo documents from feedback session. Finally clustering of pseudo documents is done. For clustering purpose we use a algorithm which is K-means algorithm. K-means algorithm is easiest algorithm for <b>pseudo</b> <b>document</b> clustering other than other clustering algorithm. In proposed novel approach new criterion “Classified Average Precision (CAP) ” is used for evaluate the performance of inferring user search goals...|$|E
40|$|Copyright © 2014 ISSR Journals. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: Search engine {{is one of the}} most important applications in today’s internet. Users collect required information through the search engine in the internet. Analyzing user search goals are essential to provide best result for which the user looks for in the internet. In existing system, various techniques such as Feedback session, goal text, Pseudo-documents restructuring search result based on term frequency are used to infer user search goals. Existing search results based on term frequency (keywords) which may display unwanted results. In proposed system “Classified Average Precision (CAP) ” algorithm is used to understand user search goals efficiently and evaluate the performance of inferring user search goals. Phrase search is performed in proposed system instead of keyword search. Initially Noun Phrase of user query is framed using natural language processing. Framed noun phrases are searched in webpages available in Internet. Term frequency of each noun phrase is found in <b>Pseudo</b> <b>document</b> i. e., finding number of webpages a particular noun phrase is occurred. Based on term frequency, place the webpage/document which contain only the above noun phrases at top link. Here user needs is highlighted and provides a user friendly search engine. Performance of inferring user search goal is evaluated using a new CAP algorithm...|$|E
40|$|The {{performance}} of statistical language modeling retrieval is directly {{determined by the}} estimation of document model ??D, query model ??Q, and the similarly between those two models. In this thesis, we propose to improve the estimation of ??D and ??Q through corpus local structures. We also further evaluate the optimality of the similarity measure and examine its variations to achieve better retrieval performance. The accuracy of model estimation relies {{on the size of}} sampling data. In information retrieval tasks, a typical document model is only obtained from a single document, which is clearly a sample too small to be sufficient for an accurate estimation. In this thesis, we develop a new document expansion technique, which expands the original small sampling space (a single document) into a much larger one (a much larger <b>pseudo</b> <b>document)</b> by constructing a probabilistic neighborhood around the original document. We then propose to estimate document models over the <b>pseudo</b> <b>document,</b> since a much larger sampling space can potential lead to much better model estimation. Our experiment results show that hypothesis that the new estimation outperforms the old estimation on all test collections. In typical information retrieval tasks, queries are very short and cannot result in solid query model estimation. We traditionally use feedback process to expand queries and this usually leads to much better query model estimation ??Q. Many previous pseudo feedback models have been proposed before. However, each of them has to introduce one or more extra parameters to control the feedback process, and therefore they suffer from the parameter sensitivity: The parameter values working well on one collection may perform very badly on others. We observe the reason of such sensitivity is that learning process on feedback documents cannot fully utilize all query information. In this thesis, we therefore propose a new feedback model and an automatic parameter tuning algorithm. The new model integrates feedback documents and queries into a unified framework so that original queries can guide query model learning by gradually collecting relevant information from feedback documents. We develop a new learning EM algorithm, which dynamically changes its model prior to adapt new added information from feedback documents. On this stopping point, we believe the learning process reaches a good balance between information from query side and feedback document side. The new model and its learning algorithm does not require any pre-defined parameter. It automatically learns the parameter by adapting itself to the amount of feedback information added into query models. Experiments show that the new model performs much more robust than other feedback models on our text data sets. We further study {{what is the best way}} to compute the similarity between a query mode and a document model. We propose and study several new similarity functions, and show that certain variations of KL-divergence can improve retrieval accuracy significantly in relevance feedback. Both query model and document estimation above follows the bag-of-words model, which assumes term independence and totally ignores term locations in a document. In this thesis, we break this assumption by adding query term proximity into ranking functions. In particular, we propose five different features, each of which models proximity from a different prospective. Experiments show that one of such features, Minimum Pairwise Distance (MinDist), is indeed highly correlated with document relevance. However, this feature alone is not sufficient; we need to integrate it into retrieval formulas to form fully functional ones. We therefore follow the heuristic constrain framework to design two heuristics to limit the choice of proximity modeling functions. We then choose one function which satisfy both constraints and develop a new retrieval formula on the top of this function. Experiment results show the effectiveness of this new retrieval formula. In summary, this thesis studies KL-divergence from different perspectives and proposes several new models to address the existing problems in KL-divergence language models. It results in more effective retrieval models, which should potentially benefit all retrieval applications...|$|E
40|$|When {{different}} users {{may have}} different search goals when they submit it to a search engine. The inference and analysis of user search goals can be very useful in improving search engine relevance and user experience. The Novel approach to infer user search goals by analyzing search engine query logs. Once the User entered the query, the Resultant URLs will be filtered and the Pseudo-Documents are generated. Once the <b>Pseudo</b> <b>documents</b> are generated the Server will apply the Clustering Mechanism to URL’s. So that the URLs are listed as different categories. Feedback sessions are constructed from user click-through logs and can efficiently reflect the information needs of user. Second, we propose a novel approach to generate <b>pseudo</b> <b>documents</b> to better represents the feedback sessions for clustering. Finally we proposed new criterion “Classified Average Precision (CAP) ” to evaluate the performance of inferring user search goals. Experimental results are presented using user click-through logs from a commercial search engine to validate the effectiveness of our proposed methods. Third, the distributions of user search goals can also be useful in applications such as re ranking web search results that contain different user search goals...|$|R
40|$|Internet {{is widely}} used by users to satisfy various {{information}} needs. This paper proposed a novel approach to infer user goals. <b>Pseudo</b> <b>documents</b> are generated through feedback sessions. We introduce new criterion”classified average precision(CAP) ”, for evaluate performance of inferring user search goals. Results are represented on search engine to validate the effectiveness of our work. Extraction of interesting information from web data has become more popular and result of that web mining attracted lot of attention in recent times. Keywords User search goals,feedback session, pseudo documents,restructuring search result,classified average precision. 1...|$|R
40|$|Abstract. Using keyword queries to find {{entities}} {{has emerged}} as one of the major search types on the Web. In this paper, we study the task of ad-hoc entity retrieval: keyword search in a collection of structured data. We start with a baseline retrieval system that constructs <b>pseudo</b> <b>documents</b> from RDF triples and introduce three extensions: preprocessing of URIs, using two-fielded retrieval models, and boosting popular domains. Using the query sets of the 2010 and 2011 Semantic Search Challenge, we show that our straightforward approach outperforms all previously reported results, some generated by far more complex systems. ...|$|R
40|$|Abstract. In {{conventional}} {{vector space}} model for information retrieval, query vector generation is imperfect for retrieval of precise documents which are desired by user. In this paper, {{we present a}} stochastic based approach for optimizing query vector without user involvement. We explore the document search space using particle swarm optimization and exploit the search space of possible relevant and non-relevant documents for adaption of query vector. Proposed method improves the retrieval accuracy by optimizing the query vector which is generated in conventional vector space model based on various term weighting strategies including TF-IDF and document length normalization. Our experimental result on two collections Medline and Cranfield shows that adapted query vector in <b>pseudo</b> relevant <b>document</b> performs better over the classical vector space model. We achieved improvement of 3 - 4 % in Mean Average Precision (MAP) and 5 - 10 % in Precision at lower recall. Further expansion of search space in <b>pseudo</b> non-relevant <b>documents</b> {{does not lead to}} significant improvement, but proper representation of <b>pseudo</b> non-relevant <b>document</b> leaves a scope in future to guide the better optimization of query vector...|$|R
40|$|This paper {{proposes a}} novel {{document}} re-ranking approach in information retrieval, which {{is done by}} a label propagation-based semi-supervised learning algorithm to utilize the intrinsic structure underlying in the large document data. Since no labeled relevant or irrelevant documents are generally available in IR, our approach tries to extract some <b>pseudo</b> labeled <b>documents</b> from the ranking list of the initial retrieval. For <b>pseudo</b> relevant <b>documents,</b> we determine a cluster of documents from the top ones via cluster validation-based k-means clustering; for pseudo irrelevant ones, we pick a set of documents from the bottom ones. Then the ranking of the documents can be conducted via label propagation. Evaluation on benchmark corpora shows that the approach can achieve significant improvement over standard baselines and performs better than other related approaches...|$|R
40|$|Different {{users have}} {{different}} search goals when they submit a query to a search engine. In this paper {{we aim at}} discovering the number of diverse user’s search goal for giving a query and for each goal a keyword is associated automatically. We initially derive user's search goal for a query by clustering our proposed feedback conclave. Then the feedback conclave is mapped to pseudo-documents so that the user's needs are retrieved efficiently. Finally, these <b>pseudo</b> <b>documents</b> are then clustered to deduce user search goals and depict them with some keywords. Though K means clustering {{is used in the}} existing system sometimes queries may not exactly represent user specific information needs. This method only finds whether a pair of query is belonging to the same set of goal and does not look into goal in detail. Hence we put forward a fuzzy similarity-based self-constructing algorithm for feature clustering. Our method works efficiently and will return provide better inferred properties than any other method...|$|R
40|$|The Relevance Model (RM) {{incorporates}} pseudo {{relevance feedback}} to derive query language model and {{has shown a}} good performance. Generally, {{it is based on}} uni-gram models of individual feedback documents from which query terms are sampled independently. In this paper, we present a new method to build the query model with latent state machine (LSM) which captures the inherent term dependencies within the query and the term dependencies between query and documents. Our method firstly splits the query into subsets of query terms (i. e., not only single terms, but different combinations of multiple query terms). Secondly, these query term combinations are then considered as weighted latent states of a hidden Markov Model to derive a new query model from the <b>pseudo</b> relevant <b>documents.</b> Thirdly, our method integrates the Aspect Model (AM) with the EM algorithm to estimate the parameters involved in the model. Specifically, the <b>pseudo</b> relevant <b>documents</b> are segmented into chunks, and different chunks are associated with different weights in relation to a latent state. Our approach is empirically evaluated on three TREC collections, and demonstrates statistically significant improvements over a baseline language model and the Relevance Model. ...|$|R
40|$|In this paper, we {{describe}} our ideas and related experiments in TREC- 11 Adaptive Filtering Track. In the track we focused much on a robust way for effective profile training. We developed an incremental learning method which selects <b>pseudo</b> positive <b>documents</b> in less bias {{from a few}} initial positive training documents. We also did some experiments with newly emerged information retrieval model, language model-based retrieval mechanism, to evaluate its performance when used in adaptive filtering task. Related experiment results show the incremental learning method can be helpful for profile training, while the new language model perform not well. 1...|$|R
30|$|First, for {{expression}} microarray data, {{the research}} subject of studies by Perina et al. (2010) {{is similar to}} that in Rogers et al. (2005) and Pratanwanich and Lio (2014): there is also a straightforward analogy between the pairs word-document and gene-sample. Nonetheless, Perina et al. introduced biologically aware latent Dirichlet allocation (BaLDA) to perform a classification task that extends the LDA model by integrating document dependences and starts from the LPD. BaLDA does not contain the assumption present in both PLSA and LDA that each gene is independently generated given its corresponding latent topic. A priori knowledge about relations among genes is expressed in terms of gene categorization. In the training phase, this categorization (topic) can be computed beforehand; in the testing phase, it can also be estimated. Finally, the authors demonstrated the usefulness of BaLDA in two classification experiments. Another study on classification of gene expression data is a pathway-based LDA model proposed by Pratanwanich and Lio (2014). That study was aimed at learning drug-pathway-gene relations by treating known gene-pathway associations as prior knowledge. In that study, they drew an analogy between drug-pathway-gene and document-topic-word. They regarded genes as words and viewed a pathway as a topic. First, <b>pseudo</b> drug <b>documents</b> were produced in the training phase, and the model was learned by parameter inference. Then, for a new <b>pseudo</b> drug <b>document,</b> this model can predict responsiveness of the pathway to a new drug treatment.|$|R
40|$|Abstract. The {{paper is}} {{concerned}} with the design and the evaluation of the combination of user interaction and informative content features for implicit and <b>pseudo</b> feedback-based <b>document</b> re-ranking. The features are observed during the visit of the top-ranked documents returned in response to a query. Experiments on a TREC Web test collection have been carried out and the experimental results are illustrated. We report that the effectiveness of the combination of user interaction for implicit feedback depends on whether document re-ranking is on a single-user or a user-group basis. Moreover, the adoption of document re-ranking on a user-group basis can improve pseudo-relevance feedback by providing more effective document for expanding queries. ...|$|R
40|$|Abstract. This paper {{presents}} a novel framework to further advance the recent trend of using query decomposition and high-order term relationships in query language modeling, which {{takes into account}} terms implicitly associated with different subsets of query terms. Existing approaches, most remarkably the language model based on the Information Flow method are however unable to capture multiple levels of associations and also suffer from a high computational overhead. In this paper, we propose to compute association rules from <b>pseudo</b> feedback <b>documents</b> that are segmented into variable length chunks via multiple sliding windows of different sizes. Extensive experiments have been conducted on various TREC collections and our approach significantly outperforms a baseline Query Likelihood language model, the Relevance Model and the Information Flow model...|$|R
40|$|Table 1 : Kendall’s Tau rank {{correlation}} coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0. 01). Column 2 contains {{the average number}} of sampled documents from the pool. For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the <b>pseudo</b> relevant <b>documents.</b> This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomnes...|$|R
40|$|Many {{successful}} query expansion techniques ignore {{information about}} the term dependencies that exist within natural language. However, researchers have recently demonstrated that consistent and significant improvements in retrieval effectiveness {{can be achieved by}} explicitly modelling term dependencies within the query expansion process. This has created an increased interest in dependency-based models. State-of-the-art dependency-based approaches primarily model term associations known within structural linguistics as syntagmatic associations, which are formed when terms co-occur together more often than by chance. However, structural linguistics proposes that the meaning of a word is also dependent on its paradigmatic associations, which are formed between words that can substitute for each other without effecting the acceptability of a sentence. Given the reliance on word meanings when a user formulates their query, our approach takes the novel step of modelling both syntagmatic and paradigmatic associations within the query expansion process based on the (<b>pseudo)</b> relevant <b>documents</b> returned in web search. The results demonstrate that this approach can provide significant improvements in web re- trieval effectiveness when compared to a strong benchmark retrieval system...|$|R
40|$|International audienceMatching users’ {{information}} needs and relevant documents {{is the basic}} goal of information retrieval systems. However, relevant documents do not necessarily contain the same terms as the ones in users’ queries. In this paper, we use semantics to better express users’ queries. Furthermore, we distinguish between two types of concepts: those extracted from a set of <b>pseudo</b> relevance <b>documents,</b> and those extracted from a semantic resource such as an ontology. With this distinction in mind we propose a Semantic Mixed query Expansion and Reformulation Approach (SMERA) that uses {{these two types of}} concepts to improve web queries. This approach considers several challenges such as the selective choice of expansion terms, the treatment of named entities, and the reformulation of the query in a user-friendly way. We evaluate SMERA on four standard web collections from INEX and TREC evaluation campaigns. Our experiments show that SMERA improves the performance of an information retrieval system compared to non-modified original queries. In addition, our approach provides a statistically significant improvement in precision over a competitive query expansion method while generating concept-based queries that are more comprehensive and easy to interpret...|$|R
40|$|Text {{segmentation}} {{in natural}} language processing typically refers {{to the process of}} decomposing a document into constituent subtopics. Our work centers on the application of text segmentation techniques within information retrieval (IR) tasks. For example, for scoring a document by combining the retrieval scores of its constituent segments, exploiting the proximity of query terms in documents for ad-hoc search, and for question answering (QA), where retrieved passages from multiple documents are aggregated and presented as a single document to a searcher. Feedback in ad hoc IR task is shown to benefit from the use of extracted sentences instead of terms from the <b>pseudo</b> relevant <b>documents</b> for query expansion. Retrieval effectiveness for patent prior art search task is enhanced by applying text segmentation to the patent queries. Another aspect of our work involves augmenting text segmentation techniques to produce segments which are more readable with less unresolved anaphora. This is particularly useful for QA and snippet generation tasks where the objective is to aggregate relevant and novel information from multiple documents satisfying user information need on one hand, and ensuring that the automatically generated content presented to the user is easily readable without reference to the original source document...|$|R
40|$|Pseudo {{relevance}} feedback (PRF), {{which has been}} widely applied in IR, aims to derive a distribution from the top n <b>pseudo</b> relevant <b>documents</b> D. However, these documents are often a mixture of relevant and irrelevant documents. As a result, the derived distribution is actually a mixture model, which has long been limiting the performance of PRF. This is particularly the case when we deal with difficult queries where the truly relevant documents in D are very sparse. In this situation, it is often easier to identify {{a small number of}} seed irrelevant documents, which can form a seed irrelevant distribution. Then, a fundamental and challenging problem arises: solely based on the mixed distribution and a seed irrelevance distribution, how to automatically generate an optimal approximation of the true relevance distribution? In this paper, we propose a novel distribution separation model (DSM) to tackle this problem. Theoretical justifications of the proposed algorithm are given. Evaluation results from our extensive simulated experiments on several large scale TREC data sets demonstrate the effectiveness of our method, which outperforms a well respected PRF Model, the Relevance Model (RM), as well as the use of RM on D with the seed negative documents directly removed...|$|R
40|$|This study {{proposes a}} new way of using WordNet for Query Expansion (QE). We choose {{candidate}} expansion terms, as usual, from a set of pseudo relevant documents; however, the usefulness of these terms is measured based on their definitions provided in a hand-crafted lexical resource like WordNet. Experiments with a number of standard TREC collections show that this method outperforms existing WordNet based methods. It also compares favorably with established QE methods such as KLD and RM 3. Leveraging earlier work in which a combination of QE methods was found to outperform each individual method (as well as other well-known QE methods), we next propose a combination-based QE method that takes into account three different aspects of a candidate expansion term's usefulness: (i) its distribution in the <b>pseudo</b> relevant <b>documents</b> and in the target corpus, (ii) its statistical association with query terms, and (iii) its semantic relation with the query, as determined by the overlap between the WordNet definitions of the term and query terms. This combination of diverse sources of information appears to work well on a number of test collections, viz., TREC 123, TREC 5, TREC 678, TREC robust new and TREC 910 collections, and yields significant improvements over competing methods on most of these collections. Comment: 18 pages, 7 table...|$|R
40|$|Good term {{selection}} {{is an important}} issue for an automatic query expansion (AQE) technique. AQE techniques that select expansion terms from the target corpus usually do so in one of two ways. Distribution based term selection compares the distribution of a term in the (<b>pseudo)</b> relevant <b>documents</b> with that in the whole corpus / random distribution. Two well-known distribution-based methods are based on Kullback-Leibler Divergence (KLD) and Bose-Einstein statistics (Bo 1). Association based term selection, on the other hand, uses information about how a candidate term co-occurs with the original query terms. Local Context Analysis (LCA) and Relevance-based Language Model (RM 3) are examples of association-based methods. Our goal in this study is to investigate how these two classes of methods may be combined to improve retrieval effectiveness. We propose the following combination-based approach. Candidate expansion terms are first obtained using a distribution based method. This set is then refined based on the strength of the association of terms with the original query terms. We test our methods on 11 TREC collections. The proposed combinations generally yield better results than each individual method, as well as other state-of-the-art AQE approaches. En route to our primary goal, we also propose some modifications to LCA and Bo 1 which lead to improved performance. Comment: 19 pages, 1 figure, 2 result table...|$|R
40|$|Abstract. In pseudo {{relevance}} feedback (PRF), the document weight which indicates how important a document {{is for the}} PRF model, plays a key role. In this paper, we investigate the smoothness issue of the document weights in PRF. The term smoothness means that the document weights decrease smoothly (i. e. gradually) along the document ranking list, and the weights are smooth (i. e. similar) within topically similar documents. We postulate that a reasonably smooth documentweighting function can benefit the PRF performance. This hypothesis is tested under a typical PRF model, namely the Relevance Model (RM). We propose a two-step document weight smoothing method, the different instantiations of which have different effects on weight smoothing. Experiments on three TREC collections show that the instantiated methods with better smoothing effects generally lead to better PRF performance. In addition, the proposed method can significantly improve the RM’s performance and outperform various alternative methods which {{can also be used}} to smooth the document weights. Key words: <b>Pseudo</b> {{relevance feedback}}, <b>Document</b> weight smoothness...|$|R
40|$|As {{noted by}} President Obama 2 ̆ 7 s recent Review Group on Intelligence and Communications Technologies, {{pervasive}} state surveillance {{has never been}} more feasible. There has been an inexorable rise in the size and reach of the national security bureaucracy since it was created after World War II, as we have gone through the Cold War and the War on Terror. No one doubts that our national security bureaucracies need to gain intelligence and keep some of it secret. But the consensus of decades of experts, both insiders and outsiders, is that there is rampant overclassfication by government agencies. From its inception in 1966, the Freedom of Information Act (FOIA) has presumed disclosure. And from its inception, Congress intended the federal courts to act as a brake on unfettered agency discretion regarding classification. But courts have not played a strong role in this regard. This Article examines the interplay of overclassfication, excessive judicial deference, and illusory agency expertise {{in the context of the}} national security exemption to the FOIA. The national security exemption allows documents to be withheld that are 2 ̆ 2 specifically authorized under criteria established by an Executive Order to be kept secret in the interest of national defense or foreign policy 2 ̆ 2 and that 2 ̆ 2 are in fact properly classified pursuant to such Executive Order. 2 ̆ 2 The history of national security classification and the passage of the FOIA illuminate the tension between legislative demands for transparency and the growth of the national security state with its agency culture of secrecy. That tension has generally been resolved by the courts in favor of secrecy, despite agreement that there is rampant overclassification and <b>pseudo</b> classification (labeling <b>documents</b> as sensitive but unclassified). This deference in turn leads agencies routinely to deny FOIA requests that should in fact be granted. Without adequate court oversight, there is no agency incentive to comply with the FOIA 2 ̆ 7 s presumption of disclosure. We argue that courts have been systematically ignoring their clear legislative mandate. Although the government is entitled to substantial deference, the role of the judiciary is not to rubber stamp claims of national security, but to undertake de novo and in camera review of government claims that the information requested was both required to be kept secret and properly classified. Congress amended the FOIA in 1974 to make this requirement explicit, overruling a judicial attempt to defer completely to government claims that national security classifications are proper. There are many reasons that courts are reluctant to get involved in determining the validity of exemption claims based on national security. Overestimation of risk may be one reason, as is fear of the consequences of error. We also discuss a 2 ̆ 2 secrecy heuristic 2 ̆ 2 whereby people attribute greater accuracy to 2 ̆ 2 secret 2 ̆ 2 documents. Notwithstanding these rationales, courts have, in other contexts, wrestled successfully with the conflict between national security and paramount rights, such as those found in the First and Fourth Amendments. Courts have the institutional expertise to review claims of national security, if they choose to exercise it. Our conclusion is that the systematic failures of the federal courts in the FOIA context are neither inevitable nor justified. We show that courts do occasionally order the release of some documents. This Article includes the first empirical investigation into the decisionmaking of the D. C. district courts and federal circuit courts in cases involving the national security exemption to determine what, if any, factors favor document release. We find that party characteristics are the biggest predictor of disclosure. We also show that, while politics do not seem to matter at most courts, they do at the D. C. Circuit Court of Appeals, at which Republican dominated panels have never ordered disclosure...|$|R

