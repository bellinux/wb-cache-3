5483|2404|Public
5|$|UM {{maintains}} one of {{the largest}} centralized academic cyber infrastructures in the country with numerous assets. The Center for Computational Science High <b>Performance</b> <b>Computing</b> group has been in continuous operation since 2007. Over that time the core has grown from a zero HPC cyberinfrastructure to a regional high-performance computing environment that currently supports more than 1,200 users, 220 TFlops of computational power, and more than 3 Petabytes of disk storage. The center's latest system acquisition, an IBM IDataPlex system, was ranked at number 389 on the November 2012 Top 500 Supercomputers.|$|E
5|$|A free {{version was}} {{introduced}} {{for students in}} 2010. SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year. SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year. JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel. The following year, a High <b>Performance</b> <b>Computing</b> appliance was made available in a partnership with Teradata and EMC Greenplum. In 2011, the company released Enterprise Miner 7.1. The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others. At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, {{as well as new}} training software.|$|E
5|$|The {{university}} operates six {{research centres}} and institutes, the Centre for Neuroscience Studies, GeoEngeering Centre, High <b>Performance</b> <b>Computing</b> Virtual Laboratory, Human Mobility Research Centre, Sudbury Neutrino Observatory Institute, and the Southern African Research Centre. The Sudbury Neutrino Observatory's director, Arthur B. McDonald, {{is a member}} of the university's physics department. The observatory managed the SNO experiment, which demonstrated that the solution to the solar neutrino problem was that neutrinos change flavour (type) as they propagate through the Sun. The SNO experiment proved that a non-zero mass neutrino exists. This was a major breakthrough in cosmology. In October 2015, Arthur B. McDonald and Takaaki Kajita (University of Tokyo) jointly received the Nobel Prize in Physics for illustration of neutrino change identities and identification of mass. This is the first Nobel Prize awarded to a Queen's University researcher. In 1976 urologist Alvaro Morales, along with his colleagues, developed the first clinically effective immunotherapy for cancer by adapting the Bacille Calmette-Guérin tuberculosis vaccine for treatment of early stage bladder cancer.|$|E
5000|$|Computer Centre with High <b>Performance</b> Scientific <b>Computing</b> cluster ...|$|R
5000|$|Center for Research in High <b>Performance</b> Reconfigurable <b>Computing</b> ...|$|R
40|$|The {{recent change}} of {{emphasis}} from high <b>performance</b> parallel <b>computing</b> to high <b>performance</b> parallel/distributed <b>computing</b> {{has called for}} new techniques in distributed problem solving. Mobile agent, as a new computing paradigm, breaks the barrier of traditional client/server model and provides a more e#cient solution to distributed computing. In mobile...|$|R
25|$|The IEEE Computer Society's Seymour Cray Computer Engineering Award, {{established}} in late 1997, recognizes innovative contributions to high <b>performance</b> <b>computing</b> systems exemplifying Cray's creative spirit.|$|E
25|$|Bader {{has served}} as a lead {{scientist}} in several DARPA programs including High Productivity Computing Systems (HPCS) with IBM PERCS, Ubiquitous High <b>Performance</b> <b>Computing</b> (UHPC) with NVIDIA ECHELON, Anomaly Detection at Multiple Scales (ADAMS), Power Efficiency Revolution For Embedded Computing Technologies (PERFECT), and Hierarchical Identify Verify Exploit (HIVE).|$|E
25|$|Starting with Mono 2.6, it is {{possible}} to configure Mono to use the LLVM as the code generation engine instead of Mono's own code generation engine. This is useful for high <b>performance</b> <b>computing</b> loads and other situations where the execution performance {{is more important than the}} startup performance.|$|E
5000|$|... #Caption: RapidIO - {{the unified}} fabric for <b>Performance</b> Critical <b>Computing</b> ...|$|R
5000|$|... #Caption: Marc Tremblay {{giving a}} talk on high <b>performance</b> {{throughput}} <b>computing.</b>|$|R
40|$|Abstract. NetSolve {{is a kind}} of grid {{middleware}} {{used for}} high <b>performance</b> <b>compute.</b> In this article, the architecture and operational principle of NetSolve are first analyzed. This paper mainly discusses the implementation of the server with several numerical packages and numerical experiment is given. At last, we point out the limitations of the Netsolve. ...|$|R
25|$|The Maui High <b>Performance</b> <b>Computing</b> Center (MHPCC) at the Air Force Maui Optical and Supercomputing {{observatory}} in Kīhei is a United States Air Force {{research laboratory}} center which is {{managed by the}} University of Hawaii. It provides more than 10 million hours of computing time per year to the research, science, and military communities.|$|E
25|$|In 1988, Kleinrock chaired a {{group which}} {{produced}} the report Toward a National Research Network. This report {{was presented to}} Congress and was so influential on then-Senator Al Gore that {{it proved to be}} the foundation for what would be passed as the High <b>Performance</b> <b>Computing</b> and Communication Act of 1991, written and developed by Gore. Indeed, funding for the development of Mosaic in 1993, the World Wide Web browser which is often credited as leading to the Internet boom during the mid-1990s, came from the High-Performance Computing and Communications Initiative, a program created by the High <b>Performance</b> <b>Computing</b> Act of 1991., On January 11, 1994, as Vice-President, Gore gave the opening speech for The Superhighway Summit held at UCLA's Royce Hall. In 2001, Gore joined the faculty of UCLA as a visiting professor in the School of Public Policy and Social Research, Department of Policy Studies, family-centered community building.|$|E
25|$|In August 2009 a {{joint venture}} was {{announced}} between Sandia National Laboratories/California campus and LLNL to create an open, unclassified research and development space called the Livermore Valley Open Campus (LVOC). The motivation for the LVOC stems from current and future national security challenges that require increased coupling {{to the private sector}} to understand threats and deploy solutions in areas such as high <b>performance</b> <b>computing,</b> energy and environmental security, cyber security, economic security, and non-proliferation.|$|E
25|$|Sony's high <b>performance</b> media <b>computing</b> server ZEGO uses a 3.2GHz Cell/B.E processor.|$|R
50|$|Sony's high <b>performance</b> media <b>computing</b> server ZEGO uses a 3.2 GHz Cell/B.E processor.|$|R
5000|$|Calculators: Normative {{measures}} {{provided for}} many criteria {{to evaluate the}} project's <b>performance</b> and <b>compute</b> a final criterion score.|$|R
25|$|Boucher {{has been}} active on Internet-related legislation, {{including}} cosponsoring the High <b>Performance</b> <b>Computing</b> and Communication Act of 1991. He chaired the Science Subcommittee of the House Committee on Science and Technology and through hearings oversaw the transition of the Internet from a National Science Foundation managed government research project (known as NSFnet) to the private sector. In that role, he authored the legislation which permitted the first commercial use of the Internet. His proposals to promote competition in the cable and local telephone industries contributed to {{the enactment of the}} Telecommunications Act of 1996.|$|E
25|$|In November 2006, Bader was {{selected}} by Sony, Toshiba, and IBM, to direct the first Center of Competence for the Cell Processor. Bader also serves on the Internet2 Research Advisory Council. Bader was elected as an IEEE Fellow in 2009. Since 2011, {{he has been working}} with the Georgia Tech Research Institute on the Proactive Discovery of Insider Threats Using Graph Analysis and Learning project. Dr. Bader also plays leadership roles in: Computing Research Association (CRA) Board, NSF Advisory Committee on Cyberinfrastructure, Council on Competitiveness High <b>Performance</b> <b>Computing</b> Advisory Committee, IEEE Computer Society Board of Governors and Editor-in-Chief, IEEE Transactions on Parallel and Distributed Systems.|$|E
25|$|As the price/performance {{of general}} purpose graphic {{processors}} (GPGPUs) has improved, {{a number of}} petaflop supercomputers such as Tianhe-I and Nebulae have started to rely on them. However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and the overall applicability of GPGPUs in general purpose high <b>performance</b> <b>computing</b> applications {{has been the subject}} of debate, in that while a GPGPU may be tuned to score well on specific benchmarks its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it. However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by replacing CPUs with GPUs.|$|E
5000|$|Image space photon mapping {{achieves}} real-time <b>performance</b> by <b>computing</b> {{the first}} and last scattering using a GPU rasterizer.|$|R
40|$|The {{recent change}} of {{emphasis}} from high <b>performance</b> parallel <b>computing</b> to high <b>performance</b> parallel/distrib-uted <b>computing</b> {{has called for}} new techniques in distrib-uted problem solving. Mobile agent, as a new computing paradigm, breaks the barrier of traditional client/server model and provides a more efficient solution to distributed computing. In the mobile-agent-based paradigm, data stay at the local site, while the execution code is moved to the data sites. In this paper, we describe the deployment of mobile agent in Distributed Sensor Networks (DSNs). Mo-bile agent is used to integrate pre-processed data located at local sensor nodes. We take target classification as an example to illustrate how mobile-agent-based DSN (MADSN) supports high performance distributed integra-tion. Key words: distributed sensor networks, mobile agent, high <b>performance</b> distributed <b>computing,</b> sensor inte-gration...|$|R
40|$|Abstract. The {{simulation}} of biochemical networks provides insight and {{understanding about the}} underlying biochemical processes and pathways used by cells and organisms. BioNessie is a biochemical network simulator which has been developed at the University of Glasgow. This paper describes the simulator and focuses in particular on how it has been extended to benefit {{from a wide variety}} of high <b>performance</b> <b>compute</b> resources across the UK through Grid technologies to support larger scale simulations. ...|$|R
25|$|Unlike {{analytics}} products {{offered by}} SAS Institute, R does not natively handle datasets larger than main memory. In 2010 Revolution Analytics introduced ScaleR, a package for Revolution R Enterprise {{designed to handle}} big data through a high-performance disk-based data store called XDF (not related to IBM's Extensible Data Format) and high <b>performance</b> <b>computing</b> across large clusters. The release of ScaleR marked a push away from consulting and services alone to custom code and a la carte package pricing. ScaleR also works with Apache Hadoop and other distributed file systems and Revolution Analytics has partnered with IBM to further integrate Hadoop into Revolution R. Packages to integrate Hadoop and MapReduce into open source R {{can also be found}} on the community package repository, CRAN.|$|E
25|$|Since 1992 the National Center for Computational Sciences (NCCS) has overseen high <b>performance</b> <b>computing</b> at ORNL. It {{manages the}} Oak Ridge Leadership Computing Facility that {{contains}} the machines. In 2012, Jaguar was upgraded to the XK7 platform, a fundamental change as GPUs are used {{for the majority of}} processing, and renamed Titan. Titan performs at 17.59petaFLOPS and holds the number 1 spot on the TOP500 list for November 2012. Other computers include a 77 node cluster to visualise data that the larger machines output in the Exploratory Visualization Environment for Research in Science and Technology (EVEREST), a visualisation room with a 10 by 3metre (30 by 10ft) wall that displays 35megapixel projections. Smoky is an 80 node linux cluster used for application development. Research projects are refined and tested on Smoky before running on larger machines such as Titan.|$|E
500|$|As a Senator, Gore {{began to}} craft the High <b>Performance</b> <b>Computing</b> Act of 1991 (commonly {{referred}} to as [...] "The Gore Bill") after hearing the 1988 report Toward a National Research Network submitted to Congress by a group chaired by UCLA professor of computer science, Leonard Kleinrock, {{one of the central}} creators of the ARPANET (the ARPANET, first deployed by Kleinrock and others in 1969, is the predecessor of the Internet). The bill was passed on December 9, 1991, and led to the National Information Infrastructure (NII) which Gore {{referred to as}} the [...] "information superhighway." ...|$|E
30|$|Planning {{requires}} <b>computing</b> <b>performance.</b>|$|R
2500|$|Gordon Moore's Law of {{affordable}} <b>computing</b> <b>performance</b> growth ...|$|R
5000|$|Qualcomm Adreno 530 GPU for {{next-generation}} console-quality VR gaming and apps, while delivering up to 40 percent improvement to graphics <b>performance</b> and <b>compute</b> capabilities {{compared to}} its predecessor ...|$|R
500|$|As a {{computer}} system grows in complexity, the {{mean time between failures}} usually decreases. Application checkpointing is a technique whereby the computer system takes a [...] "snapshot" [...] of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high <b>performance</b> <b>computing.</b>|$|E
500|$|In {{the modern}} age, a lunar crater {{has been named}} Hippocrates. The Hippocratic Museum, a museum on the Greek island of Kos is {{dedicated}} to him. The Hippocrates Project is a program of the New York University Medical Center to enhance education through use of technology. Project Hippocrates (an acronym of [...] "HIgh <b>PerfOrmance</b> <b>Computing</b> for Robot-AssisTEd Surgery") is an effort of the Carnegie Mellon School of Computer Science and Shadyside Medical Center, [...] "to develop advanced planning, simulation, and execution technologies {{for the next generation}} of computer-assisted surgical robots." [...] Both the [...] and American Hippocratic Registry are organizations of physicians who uphold the principles of the original Hippocratic Oath as inviolable through changing social times.|$|E
2500|$|High <b>performance</b> <b>computing,</b> {{allowing}} on-the-fly {{planning and}} revisioning ...|$|E
30|$|Simulations in the {{downlink}} {{section of}} the CDMA system are performed by comparing De Bruijn and OVSF sequences of length 32, {{in the case of}} 2, 3, and 4 active users. De Bruijn sequences belong to the set Φ 4 that includes 12 pairwise complementary sequences: 6 sequences are chosen, by excluding the corresponding complementary ones, so that they may result orthogonal with respect to the corresponding cross-correlation. At the same time, 32 OVSF sequences are generated, and the average <b>performance</b> <b>computed</b> over all the possible subsets of 4 sequences obtainable from the whole set.|$|R
40|$|This is a {{preprint}} of a {{paper from}} the HealthGrid 2008 Conference published by HealthGrid. [URL] simulation of biochemical networks provides insight and understanding about the underlying biochemical processes and pathways used by cells and organisms. BioNessie is a biochemical network simulator which has been developed at the University of Glasgow. This paper describes the simulator and focuses in particular on how it has been extended to benefit {{from a wide variety}} of high <b>performance</b> <b>compute</b> resources across the UK through Grid technologies to support larger scale simulations. 2 - 4 JuneOpen Acces...|$|R
3000|$|... channels. For each metric, {{objective}} {{scores were}} fitted to subjective scores using logistic fitting. <b>Performance</b> indexes were <b>computed</b> {{to assess the}} accuracy, monotonicity, and consistency of the metrics estimation of subjective scores. Finally, {{statistical analysis was performed}} on the <b>performance</b> indexes <b>computed</b> from 240 data points to discriminate small differences between two metrics. Hence, with this study, we expect to produce a valid contribution for future objective quality studies on HDR imaging.|$|R
