211|895|Public
5000|$|Inger-lise Frogner, met.no, Project {{leader on}} <b>probabilistic</b> <b>forecasting</b> ...|$|E
5000|$|... #Caption: Visualization of the Quantile Regression Averaging (QRA) <b>probabilistic</b> <b>forecasting</b> technique.|$|E
50|$|In {{the fields}} of {{predictive}} modelling and <b>probabilistic</b> <b>forecasting,</b> the Markov property is considered desirable; such a model {{is known as a}} Markov model.|$|E
40|$|Interval <b>probabilistic</b> <b>{{forecasts}}</b> for {{a binary}} event are forecasts issued as {{a range of}} probabilities for the occurrence of the event, for example, ‘chance of rain: 10 - 20 %’. To verify interval <b>probabilistic</b> <b>forecasts,</b> use can be made of a scoring rule that assigns a score to each forecast-outcome pair. An important requirement for scoring rules, {{if they are to}} provide a faithful assessment of a forecaster, is that they be proper, by which is meant that they direct forecasters to issue their true beliefs as their forecasts. Proper scoring rules for <b>probabilistic</b> <b>forecasts</b> issued as precise numbers have been studied extensively. But, applying such a proper scoring rule to, for example, the mid-point of an interval <b>probabilistic</b> <b>forecast,</b> does not, typically, produce a proper scoring rule for interval <b>probabilistic</b> <b>forecasts.</b> Complementing parallel work by other authors, we derive a general characterisation of scoring rules that are proper for interval <b>probabilistic</b> <b>forecasts</b> and from this characterisation we determine particular scoring rules for interval <b>probabilistic</b> <b>forecasts</b> that correspond to the familiar scoring rules used for <b>probabilistic</b> <b>forecasts</b> given as precise probabilities. All the scoring rules we derive apply immediately to rounded <b>probabilistic</b> <b>forecasts,</b> being a special case of interval <b>probabilistic</b> <b>forecasts...</b>|$|R
5000|$|... is {{positively}} oriented if for two different <b>probabilistic</b> <b>forecasts</b> (such as [...] and [...] ), [...] means that [...] {{is a better}} <b>probabilistic</b> <b>forecast</b> than [...]|$|R
40|$|This study aims {{to explore}} the {{differences}} in various dimensions of forecasting accuracy that may result from the task format used to elicit the <b>probabilistic</b> <b>forecasts.</b> In particular, we {{examine the effects of}} using multiple-interval and dichotomous formats on the performance of portfolio managers' <b>probabilistic</b> <b>forecasts</b> of stock prices. <b>Probabilistic</b> <b>forecasts</b> of these experts are compared with those provided by semi-experts comprised of other banking professionals trained in portfolio management, as well as with forecasts provided by a novice group. The results suggest that the task format used to elicit the <b>probabilistic</b> <b>forecasts</b> has a differential impact on the performance of experts, semi-experts, and novices. The implications of these findings for financial forecasting are discussed and directions for future research are given...|$|R
5000|$|<b>Probabilistic</b> <b>forecasting</b> {{summarizes}} what {{is known}} about, or opinions about, future events. In contrast to single-valued forecasts (such as forecasting that the maximum temperature at given site {{on a given day}} will be 23 degrees Celsius, or that the result in a given football match will be a no-score draw), probabilistic forecasts assign a probability to each of a number of different outcomes, and the complete set of probabilities represents a probability forecast. Thus, <b>probabilistic</b> <b>forecasting</b> is a type of probabilistic classification.|$|E
50|$|<b>Probabilistic</b> <b>{{forecasting}}</b> {{is used in}} {{a weather}} forecasting {{in a number of}} ways. One of the simplest is the publication of about rainfall {{in the form of a}} probability of precipitation.|$|E
5000|$|The {{continuous}} ranked probability {{score is}} a measure how good forecasts that are expressed as probability distributions are in matching observed outcomes. Both the location and spread of the forecast distribution {{are taken into account}} in judging how close the distribution is the observed value: see <b>probabilistic</b> <b>forecasting.</b>|$|E
40|$|Based {{on recent}} advances, skilled objectively-determined <b>probabilistic</b> <b>forecasts</b> of some weather {{phenomena}} may {{be provided to}} operational decision-makers. Objective <b>probabilistic</b> <b>forecasts</b> that are generated from ensemble prediction systems (EPS) are attractive as a forecast methodology for Department of Defense (DoD) applications for three reasons: first, atmospheric scientists understand that the atmosphere has a limit of predictability, which means that traditional deterministic forecasts lack important uncertainty information; second, it has been demonstrated that quantifying uncertainty may improve a weather forecast user's ability {{to make a better}} decision based on their own utility function, which translates to better operational risk management (ORM) for the DoD; and finally, progress points towards a future with machine-to-machine warfare. These assertions are examined by applying <b>probabilistic</b> <b>forecasts</b> from an ensemble-based aircraft-scale turbulence forecast system to several cases and scenarios. Results clearly demonstrate the advantage of using ensemble-based <b>probabilistic</b> <b>forecasts</b> versus deterministic forecasts. Additionally, application of ensemble-based <b>probabilistic</b> <b>forecast</b> information to DoD operations is shown to be possible through its ORM programs. Specifically, air refueling scenarios are identified that demonstrate the integration of <b>probabilistic</b> turbulence <b>forecast</b> guidance into the U. S. Air Force ORM process...|$|R
30|$|In addition, <b>probabilistic</b> <b>forecasts</b> {{according}} to post-processing are also {{proved to be}} effective. In Xie’s [6] and Mcsharry’s [16] studies, residual simulation was used to convert point <b>forecasts</b> to <b>probabilistic</b> <b>forecasts.</b> Liu [12] applied forecasting combination to optimize results, which tended to manifest a great boost in performance.|$|R
40|$|Cataloged from PDF {{version of}} article. This study aims {{to explore the}} {{differences}} in various dimensions of forecasting accuracy that may result from the task format used to elicit the <b>probabilistic</b> <b>forecasts.</b> In particular, we {{examine the effects of}} using multiple-interval and dichotomous formats on the performance of portfolio managers' <b>probabilistic</b> <b>forecasts</b> of stock prices. <b>Probabilistic</b> <b>forecasts</b> of these experts are compared with those provided by semi-experts comprised of other banking professionals trained in portfolio management, as well as with forecasts provided by a novice group. The results suggest that the task format used to elicit the <b>probabilistic</b> <b>forecasts</b> has a differential impact on the performance of experts, semi-experts, and novices. The implications of these findings for financial forecasting are discussed and directions for future research are given...|$|R
5000|$|Although scoring {{rules are}} {{introduced}} in <b>probabilistic</b> <b>forecasting</b> literature, the definition is general enough to consider non-probabilistic {{measures such as}} mean absolute error or mean square error as some specific scoring rules. The main characteristic of such scoring rules is [...] is just {{a function of the}} expected value of [...] (i.e., [...] ).|$|E
5000|$|In this approach, the {{presence}} of multiple solutions to the interpolation problem is acknowledged. Each realization is considered as a possible scenario of what the real variable could be. All associated workflows are then considering ensemble of realizations, and consequently ensemble of predictions that allow for <b>probabilistic</b> <b>forecasting.</b> Therefore, geostatistics {{is often used to}} generate or update spatial models when solving inverse problems.|$|E
50|$|Sports betting {{is another}} field of {{application}} where <b>probabilistic</b> <b>forecasting</b> can play a role. The pre-race odds published for a horse race can be considered to correspond to a summary of bettors' opinions about the likely outcome of a race, although {{this needs to be}} tempered with caution as bookmakers' profits needs to be taken into account. In sports betting, probability forecasts may not be published as such, but may underlie bookmakers' activities in setting pay-off rates, etc.|$|E
5000|$|... {{effective}} {{representation of}} aggregated <b>probabilistic</b> <b>forecasts</b> and their distributions.|$|R
50|$|Canada {{has been}} one of the first {{countries}} to broadcast their <b>probabilistic</b> <b>forecast</b> by giving chances of precipitation in percentages. As an example of fully <b>probabilistic</b> <b>forecasts,</b> recently, distribution forecasts of rainfall amounts by purely statistical methods have been developed whose performance is competitive with hybrid EPS/statistical rainfall forecasts of daily rainfall amounts.|$|R
50|$|Assessing <b>probabilistic</b> <b>forecasts</b> is {{more complex}} than {{assessing}} deterministic forecasts. If an ensemble-based approach is being used, the individual ensemble members need first to be combined and expressed in terms of a probability distribution. There exist probabilistic (proper) scoring rules such as the continuous ranked probability score for evaluating <b>probabilistic</b> <b>forecasts.</b> One example of such a rule is the Brier score.|$|R
50|$|In {{probability}} theory, a Markov {{model is}} a stochastic model used to model randomly changing systems where {{it is assumed that}} future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and <b>probabilistic</b> <b>forecasting,</b> it is desirable for a given model to exhibit the Markov property.|$|E
50|$|Quantile Regression Averaging (QRA) is a {{forecast}} combination {{approach to the}} computation of prediction intervals. It involves applying quantile regression to the point forecasts of {{a small number of}} individual forecasting models or experts. It has been introduced in 2014 by Jakub Nowotarski and Rafał Weron and originally used for <b>probabilistic</b> <b>forecasting</b> of electricity prices and loads. Despite its simplicity it has been found to perform extremely well in practice - the top two performing teams in the price track of the Global Energy Forecasting Competition (GEFCom2014) used variants of QRA.|$|E
50|$|The use of {{prediction}} intervals (PI) and densities, or <b>probabilistic</b> <b>forecasting,</b> {{has become}} much more common over the past three decades, as practitioners have come to understand the limitations of point forecasts. Despite the bold move by the organizers of the Global Energy Forecasting Competition 2014 to require the participants to submit forecasts of the 99 percentiles of the predictive distribution (day-ahead in the price track) and not the point forecasts as in the 2012 edition, this {{does not seem to be}} a common case in EPF as yet.|$|E
50|$|The CST group {{works on}} {{mathematical}} foundations and statistical methodology for forecasting. The {{aim is to}} develop methods for <b>probabilistic</b> <b>forecasts,</b> to generate predictive probability distributions for future events and quantities. For example, <b>probabilistic</b> <b>forecasts</b> are used in weather prediction and economics. The group’s second research focus is on spatial statistics, which {{is concerned with the}} analysis and interpretation of spatially distributed data.|$|R
30|$|Literature on <b>probabilistic</b> load <b>forecasting</b> are {{relatively}} limited compare to traditional point forecasting. According to Hong and Fan [3], {{the combination of}} two or three of the following component can be utilized to generate probabilistic load forecasts: creating input scenario simulation, designing probabilistic models, and transforming point <b>forecasts</b> to <b>probabilistic</b> <b>forecasts</b> through post-processing. References [4, 5, 6] mainly utilized input scenario simulation, therefore, creating <b>probabilistic</b> <b>forecasts.</b> In [7], three basic input scenario generation methods, fix-date, shifted-date, bootstrap, were discussed, and an empirical study on these methods was established, measured by pinball loss.|$|R
40|$|The Brier {{score and}} a {{covariance}} partition due to Yates {{are considered to}} study the <b>probabilistic</b> <b>forecasts</b> of a vector autoregression on stock market returns. <b>Probabilistic</b> <b>forecasts</b> from a model and data developed by Campbell (1991) are studied with ordinary least squares. Calibration measures and the Brier score and its partition are used for model assessment. The partitions indicate that the ordinary least squares version of Campbell's model does not forecast stock market returns particularly well. While the model offers honest <b>probabilistic</b> <b>forecasts</b> (they are well-calibrated), the model shows little ability to sort events that occur into different groups from events that do not occur. The Yates-partition demonstrates this shortcoming. Calibration metrics do not. ...|$|R
5000|$|While {{there have}} been a variety of {{empirical}} studies on point forecasts (i.e., the [...] "best guess" [...] or expected value of the spot price), probabilistic - i.e., interval and density - forecasts have not been investigated extensively to date. However, this is changing and nowadays both researchers and practitioners are focusing on the latter. While the Global Energy Forecasting Competition in 2012 was on point forecasting of electric load and wind power, the 2014 edition aimed at <b>probabilistic</b> <b>forecasting</b> of electric load, wind power, solar power and electricity prices.|$|E
50|$|Probabilistic {{forecasts}} {{have not}} been investigated extensively to date {{in the context of}} energy forecasting. However, the situation is changing. While the Global Energy Forecasting Competition (GEFCom) in 2012 was on point forecasting of electric load and wind power, the 2014 edition aimed at <b>probabilistic</b> <b>forecasting</b> of electric load, wind power, solar power and electricity prices. The top two performing teams in the price track of GEFCom2014 used variants of Quantile Regression Averaging (QRA), a new technique which involves applying quantile regression to the point forecasts of a small number of individual forecasting models or experts, hence allows to leverage existing development of point forecasting.|$|E
5000|$|An {{example of}} <b>probabilistic</b> <b>forecasting</b> is in {{meteorology}} where a weather forecaster may give {{the probability of}} rain on the next day. One could note {{the number of times}} that a 25% probability was quoted, over a long period, and compare this with the actual proportion of times that rain fell. If the actual percentage was substantially different from the stated probability we say that the forecaster is poorly calibrated. A poorly calibrated forecaster might be encouraged to do better by a [...] system. A bonus system designed around a proper scoring rule will incentivize the forecaster to report probabilities equal to his personal beliefs.|$|E
40|$|This paper {{considers}} the correction of deterministic forecasts {{given by a}} flood forecasting model. A stochastic correction based {{on the evolution of}} an adaptive, multiplicative, gain is presented. A number of models for the evolution of the gain are considered {{and the quality of the}} resulting <b>probabilistic</b> <b>forecasts</b> assessed. The techniques presented offer a computationally efficient method for providing <b>probabilistic</b> <b>forecasts</b> based on existing flood forecasting system output...|$|R
40|$|International audienceThe {{last decade}} has seen growing {{research}} in producing <b>probabilistic</b> hydro-meteorological <b>forecasts</b> and increasing their reliability. This followed the promise that, supplied {{with information about}} uncertainty, people would take better risk-based decisions. In recent years, therefore, research and operational developments have also started focusing attention on ways of communicating the <b>probabilistic</b> <b>forecasts</b> to decision-makers. Communicating <b>probabilistic</b> <b>forecasts</b> includes preparing tools and products for visualisation, but also requires understanding how decision-makers perceive and use uncertainty information in real time. At the EGU General Assembly 2012, we conducted a laboratory-style experiment in which several cases of flood forecasts and a choice of actions to take were presented {{as part of a}} game to participants, who acted as decision-makers. Answers were collected and analysed. In this paper, we present the results of this exercise and discuss if we indeed make better decisions on the basis of <b>probabilistic</b> <b>forecasts...</b>|$|R
40|$|AIM: provide <b>probabilistic</b> <b>forecasts</b> {{in terms}} of {{conditional}} quantiles Why: <b>probabilistic</b> <b>forecasts</b> for decision maker {{in terms of}} quantiles more intuitive for end user forecast of „extremal “ quantiles What: quantile regression, introduced by Koenker and Bassett (1978) example in meteorology is Bremnes (Mon. Wea. Rev., 2004) classical regression derives conditional exspectation value conditional quantile of a response variable in linear model context Wherefor: multiple regression used for Model Output Statistics (MOS...|$|R
5000|$|...In {{the nuclear}} safety area, Rasmussen formalized EJ by documenting all {{steps in the}} expert {{elicitation}} process for scientific review. This made visible wide spreads in expert assessments and teed up questions regarding the validation and synthesis of expert judgments. The nuclear safety community later took onboard expert judgment techniques underpinned by external validation. Empirical validation {{is the hallmark of}} science, and forms the centerpiece of the classical model of <b>probabilistic</b> <b>forecasting</b> [...] A European Network coordinates workshops. Application areas include nuclear safety, investment banking, volcanology, public health, ecology, engineering, climate change and aeronautics/aerospace. For a survey of applications through 2006 see [...] and [...] give exhortatory overviews. A recent large scale implementation by the World Health Organization is described in [...] A long running application at the Montserrat Volcano Observatory is described in [...]The classical model scores expert performance in terms of statistical accuracy (sometimes called calibration) and informativeness [...] These terms {{should not be confused with}} “accuracy and precision”. Accuracy “is a description of systematic errors” while precision “is a description of random errors”. In the classical model statistical accuracy is measured as the p-value or probability with which one would falsely reject the hypotheses that an expert’s probability assessments were statistically accurate. A low value (near zero) means it is very unlikely that the discrepancy between an expert’s probability statements and observed outcomes should arise by chance. Informativeness is measured as Shannon relative information (or Kullback Leibler divergence) with respect to an analyst-supplied background measure. Shannon relative information is used because it is scale invariant, tail insensitive, slow, and familiar. Parenthetically, measures with physical dimensions, such as the standard deviation, or the width of prediction intervals, raise serious problems, as a change of units (meters to kilometers) would affect some variables but not others. The product of statistical accuracy and informativeness for each expert is their combined score. With an optimal choice of a statistical accuracy threshold beneath which experts are unweighted, the combined score is a long run “strictly proper scoring rule”: an expert achieves his long run maximal expected score by and only by stating his true beliefs. The classical model derives Performance Weighted (PW) combinations. These are compared with Equally Weighted (EW) combinations, and recently with Harmonically Weighted (HW) combinations, as well as with individual expert assessments.|$|E
40|$|This paper {{discusses}} a new <b>probabilistic</b> <b>forecasting</b> {{method that}} was designed for the 2015 British general election. It proceeds {{in a series of}} steps from opinion poll averaging, forecasting national-level vote shares and uncertainty estimates, and subsequent simulation of hypothetical election results, through modelling of constituency polls and survey data to identify and adjust for patterns in the constituency-level variation in party performance, and finally to <b>probabilistic</b> <b>forecasting</b> of seat outcomes and of different combinations of parties commanding relevant governing majorities in parliament...|$|E
30|$|In this section, several {{evaluation}} criteria {{in the field}} of <b>probabilistic</b> <b>forecasting</b> are reviewed, and benchmark models for further comparison in case study will be proposed.|$|E
30|$|A {{participatory}} water allocation process {{based on}} <b>probabilistic</b> <b>forecasts</b> {{was introduced by}} Sankarasubramanian et al., (2009) (Sankarasubramanian et al. 2009 a).|$|R
3000|$|... aThe <b>probabilistic</b> <b>forecast</b> product {{format that}} IRI first {{introduced}} became the standard format {{for most of}} the consensus outlooks of the RCOFs.|$|R
40|$|Convection-allowing models offer forecasters unique {{insight into}} {{convective}} hazards relative to numerical models using parameterized convection. However, methods to best characterize {{the uncertainty of}} guidance derived from convection-allowing models are still unrefined. This paper proposes a method of deriving calibrated <b>probabilistic</b> <b>forecasts</b> of rare events from deterministic forecasts by fitting a parametric kernel density function to the model’s historical spatial error characteristics. This kernel density function is then applied to individual forecast fields to produce <b>probabilistic</b> <b>forecasts.</b> 1...|$|R
