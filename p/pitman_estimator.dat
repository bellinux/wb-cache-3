14|11|Public
25|$|In {{mathematical}} statistics, Haar {{measures are}} used for prior measures, which are prior probabilities for compact groups of transformations. These prior measures are used to construct admissible procedures, by appeal to the characterization of admissible procedures as Bayesian procedures (or limits of Bayesian procedures) by Wald. For example, a right Haar measure {{for a family of}} distributions with a location parameter results in the <b>Pitman</b> <b>estimator,</b> which is best equivariant. When left and right Haar measures differ, the right measure is usually preferred as a prior distribution. For the group of affine transformations on the parameter space of the normal distribution, the right Haar measure is the Jeffreys prior measure. Unfortunately, even right Haar measures sometimes result in useless priors, which cannot be recommended for practical use, like other methods of constructing prior measures that avoid subjective information.|$|E
50|$|This {{family has}} an {{interesting}} property, the <b>Pitman</b> <b>estimator</b> {{of the location}} parameter {{does not depend on}} the choice of the loss function. Only two statistical models satisfy this property: One is the normal family of distributions and the other one is a three-parameter statistical model which contains the log-harmonic law.|$|E
50|$|In {{mathematical}} statistics, Haar {{measures are}} used for prior measures, which are prior probabilities for compact groups of transformations. These prior measures are used to construct admissible procedures, by appeal to the characterization of admissible procedures as Bayesian procedures (or limits of Bayesian procedures) by Wald. For example, a right Haar measure {{for a family of}} distributions with a location parameter results in the <b>Pitman</b> <b>estimator,</b> which is best equivariant. When left and right Haar measures differ, the right measure is usually preferred as a prior distribution. For the group of affine transformations on the parameter space of the normal distribution, the right Haar measure is the Jeffreys prior measure. Unfortunately, even right Haar measures sometimes result in useless priors, which cannot be recommended for practical use, like other methods of constructing prior measures that avoid subjective information.|$|E
40|$|Generalizing {{a result}} of [4], for {{location}} parameter families, it was shown by [11], that <b>Pitman</b> <b>estimators</b> are minimax for translation invariant experiments with a Euclidean parameter space. In the present paper this theorem is extended to experiments whose parameter space is homogeneous under the action of a topological group. <b>Pitman</b> <b>estimators</b> invariant experiments minimaxity...|$|R
40|$|Introduction and Synopsis Introduction Synopsis Preliminaries Introduction Inference in Linear Models Robustness Concepts Robust and Minimax Estimation of Location Clippings from Probability and Asymptotic Theory Problems Robust Estimation of Location and Regression Introduction M-Estimators L-Estimators R-Estimators Minimum Distance and <b>Pitman</b> <b>Estimators</b> Differentiable Statistical Functions Problems Asymptotic Representations for L-Estimators <...|$|R
40|$|Laplace approximations for the <b>Pitman</b> <b>estimators</b> of {{location}} or scale parameters, including terms O(n-l), are obtained. The resulting expressions involve the maximum-likelihood estimate and the derivatives of the log-likelihood function up to order 3. The {{results can be}} used to refine the approximations for the optimal compromise estimators for location parameters considered by Easton (199 I). Some applications and Monte Carlo simulations are discussed...|$|R
40|$|For {{a family}} of {{truncated}} distributions with a location parameter, the asymptotic expansion of the <b>Pitman</b> <b>estimator</b> and its asymptotic variance are given by Akahira, Ohyauchi and Takeuchi (2007). In this article, the asymptotic distributions of the <b>Pitman</b> <b>estimator</b> and weighted esti-mators are derived, and their asymptotic concentration probabilities are also computed. Further the asymptotic comparison of their estimators for some truncated distributions is discussed...|$|E
40|$|Abstract:- Two dierent {{types of}} Maximum Likelihood based approximations of the <b>Pitman</b> <b>Estimator</b> in a {{location}} family are discussed. Both approximations rely on asymptotic Laplace expansions for integrals. At the rst approximation {{we try to}} nd an M- estimator with a "- function that is better than minus the loglikelihood derivative of the density". At the second approximation we directly improve the Maximum Likelihood Estimator towards the <b>Pitman</b> <b>Estimator</b> by adding an Op(1 =n) correction term. This approximation can be applied also for multivariate parameters. The eect of the improvement can be felt by small sample sizes when the Laplace approximation is usually better than the normal- based asymptotics...|$|E
40|$|This paper {{addresses}} {{the issue of}} deriving estimators improving on the best loca-tion equivariant (or <b>Pitman)</b> <b>estimator</b> under the squared error loss when a location parameter is restricted to a bounded interval. A class of improved estimators is constructed, and it is verified that the Bayes estimator for the uniform prior over the bounded interval and the truncated estimator belong to the class. This paper also obtains the sufficient conditions for the density under which the class includes the Bayes estimators {{with respect to the}} two-point boundary symmetric prior and general continuous prior distributions when a symmetric density is considered for the location family. It is demonstrated that the conditions on the symmetric density can be applied to logistic, double exponential and t-distributions as well as to a nor-mal distribution. These conditions can be also applied to scale mixtures of normal distributions. Finally, some similar results are developed in the scale family. Key words and phrases: Bayes estimator, bounded mean, decision theory, improved estimator, location family, minimaxity, monotone likelihood ratio, <b>Pitman</b> <b>estimator...</b>|$|E
40|$|In some nonregular {{statistical}} estimation problems, {{the limiting}} likelihood processes are functionals of fractional Brownian motion (fBm) with Hurst's parameter H, 0 < H ≤ 1. In this paper we present several analytical and numerical {{results on the}} moments of <b>Pitman</b> <b>estimators</b> represented {{in the form of}} integral functionals of fBm. We also provide Monte Carlo simulation results for variances of Pitman and asymptotic maximum likelihood estimators. 14 page(s...|$|R
40|$|AbstractGeneralizing {{a result}} of Girshick and Savage (In Proc. 2 nd Berkeley Sympos. Math. Statist. Probab., Vol. 1, 1951, pp. 53 – 74), for {{location}} parameter families, it was shown by Strasser, 1982 (Z. Wahrsch. Verw. Gebiete 60 223 – 247), that <b>Pitman</b> <b>estimators</b> are minimax for translation invariant experiments with a Euclidean parameter space. In the present paper this theorem is extended to experiments whose parameter space is homogeneous under the action of a topological group...|$|R
40|$|This paper {{presents}} <b>Pitman</b> closest equivariant <b>estimators</b> of {{the location}} of elliptically symmetric distributions. In the scalar parameter case the best estimator {{is the same for}} all loss functions that satisfy certain mild monotonicity conditions, which makes the estimator very useful when the exact loss function is difficult to ascertain. In the multiparameter case the <b>Pitman</b> closest <b>estimator</b> coincides with the minimum risk estimator. Loss function Median unbiasedness Transformation group Transitive...|$|R
40|$|We {{provide an}} {{analytic}} expression for {{the variance of}} ratio of integral functionals of fractional Brownian motion which arises as an asymptotic variance of Pitman estimators for a location parameter of independent identically distributed observations. The expression is obtained in terms of derivatives of a logarithmic moment of the integral functional of limit likelihood ratio process (LLRP). In the particular case when the LLRP is a geometric Brownian motion, we show that the established expression leads to the known representation of the asymptotic variance of <b>Pitman</b> <b>estimator</b> in terms of Riemann zeta-function. 9 page(s...|$|E
40|$|The {{topic of}} this thesis is {{estimation}} of a location parameter in small samples. Chapter 1 is {{an overview of}} the general theory of statistical estimates of parameters, with a special attention on the Fisher information, <b>Pitman</b> <b>estimator</b> and their polynomial versions. The new results are in Chapters 2 and 3 where the following inequality is proved for the variance of the <b>Pitman</b> <b>estimator</b> t_n from a sample of size n from a population F(x−θ) : nVar(t_n) >= (n+ 1) Var(t_n+ 1) for any n >= 1, only under the condition of finite second moments(even the absolute continuity of F is not assumed). The result is much stronger than the known Var(t_n) >= Var(t_n+ 1). Among other new results are (i) superadditivity of 1 /Var(t_n) with respect to the sample size: 1 /Var(t_m+n) >= 1 /Var(t_m) + 1 /Var(t_n), proved as a corollary of a more general result; (ii) superadditivity of Var(t_n) for a fixed n with respect to additive perturbations; (iii) monotonicity of Var(t_n) with respect to the scale parameter of an additive perturbation when the latter belongs to the class of self-decomposable random variables. The technically most difficult result is an inequality for Var(t_n), which is a stronger version of the classical Stam inequality for the Fisher information. As a corollary, an interesting property of the conditional expectation of the sample mean given the residuals is discovered. Some analytical problems arising in connection with the Pitman estimators are studied. Among them, {{a new version of the}} Cauchy type functional equation is solved. All results are extended to the case of polynomial Pitman estimators and to the case of multivariate parameters. In Chapter 4 we collect some open problems related to the theory of location parameters...|$|E
40|$|Let π_i be {{a normal}} {{population}} N (μ_i, σ^ 2) with an unknown mean μ_i and a common known variance σ^ 2 for i= 1, 2, 3. We consider the problem of estimating μ^^*=max(μ_ 1, μ_ 2, μ_ 3), based on the sample means X, Y, Z drawn from π_i for i= 1, 2, 3. This paper presents two estimators for μ^^*, namely, the <b>Pitman</b> <b>estimator</b> μ^^^_p, which has never been derived and suspected by Blumenthal (1984) to be very complex so that Monte Carlo simulation is only a possible way to investigate its performances, and the estimator μ^^^, which is derived by considering the class of linear combination of X, Y, and Z. Numerical comparisons of μ^^^_p, μ^^^ and an ordinal estimator μ^^^_m=max(X, Y, Z) are made on the criteria of the bias (BIAS) and mean square error (MSE) ...|$|E
40|$|In a multiparameter {{estimation}} problem, for first-order efficient <b>estimators,</b> second-order <b>Pitman</b> admissibility, and Pitman closeness {{properties are}} studied. Bearing {{in mind the}} dominant role of Stein-rule estimators in multiparameter estimation theory, such second-order properties are also studied for shrinkage maximum likelihood estimators. BAN estimators Edgeworth expansion first-order efficiency maximum likelihood <b>estimator</b> <b>Pitman</b> neighborhood shrinkage <b>estimators</b> second-order efficiency shrinkage maximum likelihood estimator (null) ...|$|R
40|$|Consider {{the model}} Y = µ + σ · E, µ ∈ R, σ ∈ R+, where E follows a known {{distribution}} F. The parameter µ {{is called the}} location parameter. Pitman (1939) introduced a method to determine an optimal equivariant estimator of µ, when the distribution F is continuous, but otherwise of any form. This method uses a conditioning on an ancillary statistic to minimize the mean square error. In this talk we present {{a way to use}} Pitman’s method when the error distribution is unknown. Let F be a given set of distributions. We propose to choose one or several members of this set, F 1, [...] ., Fm, and construct the final estimator by computing a weighted mean of the <b>Pitman</b> <b>estimators</b> associated to each chosen distribution. The weights will depend on the observed likelihood of the models. The question of the choice of the distribution within the set F, and the value of m, the number of distributions to be taken, will be studied trough the asymptotic behavior of the final compromis...|$|R
40|$|A {{method for}} {{deriving}} point estimates of percentiles in the extreme-value distribution conditioned on the ancillary information is given. The resulting conditional median unbiased estimate {{is the same}} regardless of the arbitrary choice of equivariant estimators. The conditional median unbiased estimator is also shown to be optimal with respect to Pitman Nearness. Weibull failure model median unbiased <b>estimators</b> <b>Pitman</b> Nearness...|$|R
40|$|This {{paper is}} {{concerned}} with the estimation under squared-error loss of a normal mean θ based on X ∼ N (θ, 1) when |θ | ≤ m for a known m> 0. Nine estimators are compared, namely the maximum likelihood estimator (mle), three dominators of the mle obtained from Moors, from Charras and from Charras and van Eeden, two minimax estimators from Casella and Strawderman, a Bayes estimator of Marchand and Perron, the <b>Pitman</b> <b>estimator</b> and Bickel’s asymptotically-minimax estimator. The comparisons are based on analytical as well as on graphical results concerning their risk functions. In particular, we comment on their gain in accuracy from using the restriction, as well as on their robustness with respect to misspecification of m. Key-Words: • admissibility; Bayes estimator; bounded Normal mean; restricted estimators; robust-ness; squared-error loss. AMS Subject Classification...|$|E
40|$|Abstract—An {{algorithm}} {{for learning}} an overcomplete dictionary using a Cauchy mixture model for sparse decomposition of an underdetermined mixing system is introduced. The mixture density function {{is derived from}} a ratio sample of the observed mixture signals where 1) {{there are at least}} two but not necessarily more mixture signals observed, 2) the source signals are statistically independent and 3) the sources are sparse. The basis vectors of the dictionary are learned via the optimization of the location parameters of the Cauchy mixture components, which is shown to be more accurate and robust than the conventional data mining methods usually employed for this task. Using a well known sparse decomposition algorithm, we extract three speech signals from two mixtures based on the estimated dictionary. Further tests with additive Gaussian noise are used to demonstrate the proposed algorithm’s robustness to outliers. Keywords—expectation-maximization, <b>Pitman</b> <b>estimator,</b> sparse decompositio...|$|E
40|$|A {{smoothing}} {{principle for}} M-estimators is proposed. The smoothing {{depends on the}} sample size so that the resulting smoothed M-estimator coincides with the initial M-estimator when n [...] >[infinity]. The smoothing principle is motivated by {{an analysis of the}} requirements in the proof of the Cramér-Rao bound. The principle can be applied to every M-estimator. A simulation study is carried out where smoothed Huber, ML-, and Bisquare M-estimators are compared with their non-smoothed counterparts and with Pitman estimators on data generated from several distributions with and without estimated scale. This leads to encouraging results for the smoothed estimators, and particularly the smoothed Huber estimator, as they improve upon the initial M-estimators particularly in the tail areas of the distributions of the estimators. The results are backed up by small sample asymptotics. <b>Pitman</b> <b>estimator</b> ML-estimator Median MAD Breakdown point Small sample asymptotics Cauchy distribution Huber's least favourable distribution Double exponential distribution Robust estimation...|$|E
40|$|In this paper, {{the largest}} and the {{smallest}} observations are considered, {{at the time when}} a new record of either kind (upper or lower) occurs based on a sequence of independent random variables with identical continuous distributions. These statistics are referred to as current upper and lower records, respectively, in the statistical literature. We derive expressions for the Pitman closeness of current records to a common population parameter and then apply these results to location-scale families of distributions with a special emphasis on the estimation of quantiles. In the case of symmetric distributions, we show that this criterion possesses some symmetry properties. Exact expressions are derived for the Pitman closeness probabilities in the case of Uniform (- 1, 1) and exponential distributions. Moreover, for the population median, we show that the Pitman closeness probability is distribution-free. Current records Location-scale family Pitman closeness <b>Pitman</b> closer <b>estimator</b> Quantiles...|$|R
40|$|According to <b>Pitman</b> (1937), an <b>{{estimator}}</b> X {{is closer}} than an estimator Y to a scalar parameter [theta] (or, in the terminology used below, X Pitman-dominates Y) if Pr[subscript][theta](ǁ X - [theta] ǁ 1 / 2, ∀[theta]. This criterion {{is now called}} the Pitman Closeness Criterion (PCC). Pitman suggested that median-unbiased estimators derived from sufficient statistics are well suited to PCC, and noted that FCC is intransitive;After Pitman gave the 2 ̆ 2 comparison theorem 2 ̆ 2 for identifying classes of estimators Pitman-dominated by median-unbiased estimators derived from sufficient statistics, Ghosh and Sen (1989) and Nayak (1990) showed that a median-unbiased estimator is best equivariant in the Pitman sense. These investigations are in a sense supportive of Pitman 2 ̆ 7 s idea;Following a different line of research based on certain shrinkage constructions, Salem and David (1973) constructed a class of continuous increasing functions of a median-unbiased estimator Pitman-dominating the sample mean for estimating the mean [theta] of a normal density with known variance (see also Efron (1975) for an example in a similar vein). David and Salem (1991) extended the result of Salem and David (1973) {{to the case of}} a single observation from any symmetric density, and also constructed intransitive triples of estimators of a Laplace location parameter, each member of the triple Pitman-dominating the single observation. This direction of research is less supportive of Pitman 2 ̆ 7 s idea;We generalize the approach of David and Salem (1991). A number of parametric situations are considered, including some considered by Pitman. In each case, a class of continuous not necessarily increasing functions of a median-unbiased or otherwise natural estimator derived from sufficient statistics is considered, each member of the class Pitman-dominating the estimator itself. Special attention is given to Pitman domination for location-scale families. Finally, we construct Pitman-intransitive triples of estimators based on the earlier results on shrinkage and equivariant estimators...|$|R
40|$|New inequalities are proved for the {{variance}} of the Pitman estimators (minimum variance equivariant estimators) of θ constructed from samples of fixed size from populations F(x-θ). The inequalities {{are closely related to}} the classical Stam inequality for the Fisher information, its analog in small samples, and a powerful variance drop inequality. The only condition required is finite variance of F; even the absolute continuity of F is not assumed. As corollaries of the main inequalities for small samples, one obtains alternate proofs of known properties of the Fisher information, as well as interesting new observations like the fact that {{the variance}} of the <b>Pitman</b> <b>estimator</b> based on a sample of size n scaled by n monotonically decreases in n. Extensions of the results to the polynomial versions of the Pitman estimators and a multivariate location parameter are given. Also, the search for characterization of equality conditions for one of the inequalities leads to a Cauchy-type functional equation for independent random variables, and an interesting new behavior of its solutions is described...|$|E
40|$|Comparisons of the {{performances}} of estimators of a bounded normal mean under squared-error loss Yiping DOU 1 and Constance van EEDEN 2 The problem of estimating a normal mean θ based on X ∼ N (θ, 1) when |θ | ≤ m for a known m> 0 under squared-error loss is considered in this paper. Eight estimators are compared, namely, the maximum likelihood estimators (mle), three dominators of the mle obtained from Moors (1981, 1985), Charras (1979) and Charras and van Eeden (1991), two minimax estimators from Casella and Strawderman (1981), the <b>Pitman</b> <b>estimator</b> and Bickel’s (1981) asymptotically-minimax estimator. Numerical as well analytical results are presented. In particular we show that the dominating estimators constructed by Charras and van Eeden are inadmissible and that, for m ≤ 1, Moors’ dominating estimator is Casella and Strawderman’s minimax estimator {{with respect to a}} two-point least-favourable prior. We also show that, for 0 < m ≤ m 1 ' 0. 5204372, the estimator δo(x) ≡ 0 dominates the mle. Explicit expressions are given for the dominators of Moors, Charras and Charras and van Eeden. Asymptotic results are proved on the behaviour of these estimators when m → ∞, results which can also be observed from our graphs...|$|E

