20|136|Public
2500|$|In April 2008, Prime {{switch to}} {{broadcasting}} {{in a lower}} quality anamorphic widescreen [...] format {{following the lead of}} other Freeview and Sky channels. Previously they had opted to use the more <b>proportionally</b> <b>scaled</b> letterbox format for 16:9 content. They like other Kordia PAL analogue broadcasters are using a [...] letterbox format on their PAL simulcast.|$|E
50|$|To {{adapt the}} system to {{different}} propagation conditions, the number of tones and the bandwidth can be changed and the time and frequency parameters are <b>proportionally</b> <b>scaled.</b> The number of tones can be 2, 4, 8, 16, 32, 64, 128 or 256. The bandwidth can be 125, 250, 500, 1000 or 2000 Hz.|$|E
50|$|In April 2008, Prime {{switch to}} {{broadcasting}} {{in a lower}} quality anamorphic widescreen 16:9 format {{following the lead of}} other Freeview and Sky channels. Previously they had opted to use the more <b>proportionally</b> <b>scaled</b> letterbox format for 16:9 content. They like other Kordia PAL analogue broadcasters are using a 14:9 letterbox format on their PAL simulcast.|$|E
25|$|When an {{air burst}} occurs, lethal blast and thermal effects <b>proportionally</b> <b>scale</b> much {{more rapidly than}} lethal {{radiation}} effects, as higher and higher yield nuclear weapons are used.|$|R
50|$|Suppose a 10000-by-10000 matrix takes 800 MB {{of memory}} and can be factorized in 1 hour on a uniprocessor. Now for the scaled {{workload}} suppose is possible to factorize a 320,000-by-320,000 matrix in 32 hours. The time increase is quite large, but the increase in problem size may be more valuable for someones whose premier goal is accuracy. For example, an astrophysicist may {{be more interested in}} simulating an N-body problem with as the number of particles as large as possible. This example shows for computation intensive applications, memory capacity does not need to <b>proportionally</b> <b>scale</b> up with computing power.|$|R
50|$|One bit per {{direction}} {{is less than}} many bits per direction when a client communicates a move to a server. With network prediction code a client sends their input data, as their future position can be calculated from this. This {{may have been the}} case with Quake III Arena when it went from singleplayer to multiplayer focused. It is possible that with network bandwidth having increased many times over the years, variable translation control could return to multiplayer games. What could be done is population based scaling. When more players come near each other, the number of bits per direction could <b>proportionally</b> <b>scale</b> down. As fewer players are near each other the number of bits per direction could scale back up. This would at least allow the player fluid motion or near fluid motion {{at least some of the}} time in multiplayer games.|$|R
5000|$|The {{use of a}} {{historical}} perspective in its design approach allows the team for continual analysis and interpretation of a given culture and its architectural heritage. In contrast to Socialist and Le Corbusier#urbanism urban planning models, Ricardo Bofill Taller de Arquitectura proposes a Mediterranean city model. Such model is defined by a network of public space that connects <b>proportionally</b> <b>scaled</b> streets and squares. “Mediterranean city” {{is at the same}} time cast as an interregional synthesis, a complex region of interrelating regions. This intense set of cultural, social and material interactions highlights the capitalist and cosmopolitan dimensions of the Mediterranean.” ...|$|E
40|$|A nonlocal damage {{continuum}} and a viscoplastic damage continuum {{are used}} to model size effects. Three-point bending specimens are analysed, whereby a distinction is made between unnotched specimens, specimens with a constant notch and specimens with a <b>proportionally</b> <b>scaled</b> notch. Numerical finite element simulations have been performed for specimen sizes {{in a range of}} 1 : 64. Size effects are established in terms of nominal strength and compared to existing size effect models from the literature. ...|$|E
40|$|The {{reflection}} and transmission coefficients of metafilms composed of U-shaped resonators were {{obtained in the}} frequency range of 0. 1 - 1 THz by numerical finite-element simulation method. The side length, the gap depth and the period of resonators were <b>proportionally</b> <b>scaled.</b> The experimental spectra of the transmission coefficients for metafilms with U-shaped resonators were received. The re-tuning of the resonant modes was shown from 0. 12 till 0. 51 THz at the geometric parameters scaling...|$|E
3000|$|R−L+ 1 [42], with L {{denoting}} {{the total}} number of layers being equal to the number of spatially multiplexed users. Hence, if L <b>scales</b> <b>proportionally</b> with N [...]...|$|R
5000|$|The mean {{absolute}} {{difference is}} invariant to translations and negation, and varies <b>proportionally</b> to positive <b>scaling.</b> That is to say, if X is a random variable and c is a constant: ...|$|R
30|$|CCR model {{assumes that}} {{return to scale}} is constant, for example, {{expanding}} the input scale of surveyed unit can increase its output <b>scale</b> <b>proportionally.</b> Relative to CCR model, BCC model introduces convexity assumption, i.e., increases constraint condition, to evaluate relative efficiency of each decision-making unit when scale return is different, thereby obtaining the input-oriented BCC model.|$|R
40|$|A {{modification}} of Kruithof's double-factor algorithm (sometimes called the Furness iterative method) {{that takes into}} account zones which lie outside the boundary of a traffic network is examined analytically. Basically the revised algorithm allows certain rows and columns of the base traffic matrix to be only <b>proportionally</b> <b>scaled</b> while the remaining rows and columns which have pre-specified row and column totals are scaled in the usual biproportional way. This paper shows that all the properties (such as uniqueness, reversibility, existence) which hold for the traditional biproportional transformation also hold for the modified transformation under certain positivity assumptions. ...|$|E
40|$|In {{the history}} of type design, two methods {{have been used to}} scale type [...] to produce {{enlarged}} or reduced letterforms from a reference size. With original handcut fonts, designers performed optical scaling (scaling by eye) that varied the proportions of letterform features over a range of sizes in a nonlinear manner. That is, letterform feature proportions were size dependent. This was an entirely manual and intuitive process. More recently, however, the use of the lens, as well as computational and other technologies, has allowed letterforms to be scaled automatically from a reference character, a simple proportional enlargement or reduction. To date, little work has been done to combine these two methods, that is to say, to automatically perform nonlinear scaling of a reference character in order to approximate the optical scaling performed by skilled type designers and punchcutters. This research developed a mathematical model of optical scale in type design, consisting of two parts: (1) a model of the scaling of individual letterform features; and (2) a model of the scaling of entire letterforms. The model was tested by applying it to the original handcut fonts that supplied the initial data for the research in order to generate synthetic letterforms. These nonlinear synthetic letterforms were then compared with the originals, as well with <b>proportionally</b> <b>scaled</b> letterforms generated by the model approximated the original optically scaled handcut letterforms. In addition, the performance of the <b>proportionally</b> <b>scaled</b> letterforms was compared with the originals, {{as well as with the}} nonlinear, synthetic forms...|$|E
40|$|This {{study is}} {{concerned}} with the question whether there is perceptual invariance of expressive timing under tempo-transformation in audio recordings. This was investigated by asking listeners to distinguish between an original recording and a time-stretched (i. e. tempotransformed) version. The original recordings were identified by a significant proportion of the participants. The results suggest that expressive timing can function as a clue in identifying a real performance. This is taken as evidence for the tempo-specific timing hypothesis, and counter evidence for the relational invariance hypothesis that predicts <b>proportionally</b> <b>scaled</b> expressive timing to be perceived natural as well. The results are discussed in the context of whether there is perceptual invariance of expressive timing under tempo transformation and possible improvements to state-of-the-art time-stretching algorithms. 1...|$|E
40|$|Although the {{importance}} of the diurnal cycle in modulating clouds and precipitation has long been recognized, its impact on the climate system at longer timescales has remained elusive. Mounting evidence indicates that the diurnal cycle may substantially affect leading climate modes through nonlinear rectification. In this study, an idealized cloud-resolving model experiment is executed to isolate a diurnal timescale feedback in the shallow cumulus regime over the tropical warm pool. This feedback is isolated by modifying the period of the diurnal cycle (or removing it), which <b>proportionally</b> <b>scales</b> (or removes) the diurnal thermodynamic forcing that clouds respond to. This diurnal forcing is identified as covarying cycles of static stability and humidity in the lower troposphere, wherein the most unstable conditions coincide with greatest humidity each afternoon. This diurnal forcing yields deeper clouds and greater daily-mean cumulus heating than would otherwise occur, in turn reducing large-scale subsidence from day to day according to the “weak temperature gradient” approximation. This diurnal forcing therefore manifests as a timescale feedback by accelerating the onset of deep convection. The longwave cloud-radiation effect is found to amplify this timescale feedback, since the resulting invigoration of clouds (increased upper-cloud radiative cooling, with suppressed cooling below) scales with cloud depth (i. e., optical thickness), and hence with the magnitude of diurnal forcing. These findings highlight the pressing need to remedy longstanding problems related to the diurnal cycle in many climate models. Given the evident sensitivity of climate variability to diurnal processes, doing so may yield advances in climate prediction at longer timescales...|$|R
5000|$|Viscous drag - This type of drag is {{also known}} as [...] "skin friction" [...] as it is thought to derive entirely from the {{frictional}} resistance of water molecules imposing a force as they slide past the wetted surface of the hull and its appendages. This type of drag <b>scales</b> <b>proportionally</b> to wetted surface {{and is one of the}} two constitutive components of hull resistance.|$|R
40|$|The {{wake of a}} {{helicopter}} rotor can {{have a significant effect}} on a fuselage. Results from a recent wind-tunnel investigation show that certain fuselage characteristics, normalized by rotor thrust, <b>scale</b> <b>proportionally</b> to a rotor-wake-induced velocity parameter. Effects on the body of changes in velocity, thrust, tip-path-plane angle of attack, and rotor/body position are discussed. These results show that the rotor can have a favorable or unfavorable influence on the body, depending upon the operating condition...|$|R
40|$|Injecting {{drug use}} (IDU) {{is a major}} risk factor for {{contracting}} HIV- 1 infection. Both HIV and IDU are neurotoxic, and their coexis-tence may lead to increased dysfunction of brain metabolic processes. The objective {{of this research was}} to investigate the effects of HIV- 1 infection and IDU on 18 F-FDG PET brain metab-olism. Methods: 18 F-FDG PET brain imaging, with a standard clin-ical protocol, was performed on 59 subjects who belonged to 3 groups: HIV-positive/IDU-positive (n 5 17), HIV-negative/ IDU-positive (n 5 13), and HIV-negative/IDU-negative controls (n 5 29). A voxel-based analysis of the 18 F-FDG PET brain images was performed using statistical parametric mapping. The images were spatially normalized to a standard 18 F-FDG template, <b>proportionally</b> <b>scaled</b> to compensate for count differences, and then appropriately smoothed. Statistical 2 -sample t tests were performed to determine regional metabolic distribution dif...|$|E
30|$|Note {{that the}} focal length of our lens scales {{up with the}} lateral {{dimension}} of a lens, that is, the size of nanoslit array, and the maximum glancing angle of aperture transmission. In the present work we obtained 4 λ focal length from a 5.6 -µm size array lens. By employing a larger size lens the focal length can be further increased, for example, 40 λ focal length from a 56 -µm lens while maintaining {{the same level of}} sub-Abbe-limit focusing. This scalability can be compared with the super-oscillatory lens that demonstrates 25 λ focal length from a 40 -µm diameter lens [37]. It {{should also be noted that}} the focal spot size can be <b>proportionally</b> <b>scaled</b> down to much smaller width by employing a high-index immersion optics configuration. Ref [38] reports an experimental work that demonstrates sub- 50  nm FWHM focal spot of a super-oscillatory lens placed in high-index solid-state immersion ambient (GaP with a refractive index of 3.72).|$|E
40|$|An {{improved}} visualization device made of polymethyl methacrylate(PMMA), {{packed with}} C,,, was utilized to study,the 3 D flow profile of conical columns with an angle of 10 degrees. The outside {{wall of the}} conical columns was a rectangular shape {{in order to improve}} the transparency property of the column wall and reduce the deformation of image for better observation of the flow profiles of colored solutes inside the column. The influence of mobile phase flow rate, particle size and shape on the flow profile of a colored band was studied both for a 5 cm long column and a scaled-up column of 4 fold in volume. The experimental results show that the flow rate of mobile phase has a little influence on the flat flow profiles of the iodine band while the properties of stationary phase have a certain influence on them. The flow profiles of the scaled-up column are flat during the whole chromatographic process, and the efficiency and resolution of the column are also increased in accordance with theoretical prediction. The results indicate that the 10 degrees conical columns can be <b>proportionally</b> <b>scaled</b> up while still keeping the flat flow profile and superior column efficiency than conventional column...|$|E
40|$|Here We Are is a {{suite of}} ten digital {{paintings}} made {{in response to a}} commission by South Liverpool Homes for a public artwork involving residents and community groups to mark the building of new housing developments in Speke and Allerton. The paintings, which are also designed to be fully-functioning postcards, celebrate the rich mix of 'landscapes' of South Liverpool: industrial, residential, natural, agricultural, political and historical. The imagery is drawn directly from these sources and, where applicable, <b>proportionally</b> accurate <b>scale</b> dimensions and authentic period colours have been used. The title of the work, Here We Are, conveys a sense of place, of arrival and of homecoming. The 'postcard paintings' were presented as a loose-bound pack to be distributed to current and future tenants of South Liverpool Homes...|$|R
30|$|Moreover, the maximizing system {{performance}} is also {{accompanied by the}} overhead cost for the CSI acquisition via channel training and feedback in frequency division duplex (FDD) systems. It needs to <b>scale</b> <b>proportionally</b> {{to the number of}} transmit and receive antennas as well as the number of users in the system in order to maintain a constant gap of the sum rate with respect to the full CSI case. The cooperative BSs via a wired backbone network brings about huge data traffic and information.|$|R
40|$|We {{study the}} {{spectral}} gap of the Wilson [...] Dirac operator in two-flavour lattice QCD {{as a function}} of the lattice spacing a, the space-time volume V and the current-quark mass m. It turns out that the median of the probability distribution of the gap <b>scales</b> <b>proportionally</b> to m and that its width is practically equal to a/√(V). In particular, numerical simulations are safe from accidental zero modes in the large-volume regime of QCD. Comment: Plain TeX source, 23 pages, 8 figure...|$|R
40|$|Monitoring of {{filamentous}} {{fungal growth}} by spectrophotometry {{is generally considered}} not feasible. This report describes the monitoring of growth of the filamentous fungi Trichophyton mentagrophytes, Rhizopus oryzae, and Sporothrix schenckii in broth by two new spectrophotometric methods and by 14 C incorporation from [U- 14 C]glucose. Microcultures (200 microliter) were prepared in 96 -well, flat-bottom microtiter trays, and macrocultures (4 ml) were prepared in glass vials <b>proportionally</b> <b>scaled</b> up from microcultures. Mycelium accumulation in microcultures was measured without terminating the cultures by in situ microspectrophotometry. Accumulation in macrocultures was monitored by uniformly fragmenting the mycelium with a Broeck tissue grinder and by measuring absorbance density in plastic cuvettes with a dual-beam spectrophotometer. Absorbance measurements were found to increase linearly with mycelial weight. In situ absorbance correlated with absorbance density of fragmented mycelium, indicating that both methods monitored growth equivalently. Both defined lag-, exponential-, and stationary-growth phases. Increases in 14 C incorporation, absorbance, and mycelial dry weight were kinetically identical for macrocultures and microcultures of T. mentagrophytes. For R. oryzae and S. schenckii, {{with the exception of}} R. oryzae growing in microcultures, incorporation of 14 C also defined lag, exponential, and stationary growth after selection of the appropriate isotope-specific activity. This incorporation correlated directly with absorbance. We conclude that in situ microspectrophotometry, fragmented mycelium absorbance density, and, to a lesser extent, 14 C incorporation can be used to effectively monitor filamentous fungal growth...|$|E
40|$|Recreation {{benefits}} {{constitute a}} substantial part of the total economic value of forests, and are important for the choice of multi-functional forest policies. The application of methods valuing such benefits is in its infancy in transition economies in Central and Eastern Europe (CEE), so value estimates for policy use are sometimes transferred from Western Europe <b>proportionally</b> <b>scaled</b> down by GDP. However, little is known about how recreation values vary with income, and one risks underestimating benefits in CEE. This paper reports the findings of the first comprehensive, national-level study in any CEE country estimating annual and per trip forest recreation values in Poland using the Travel Cost (TC) and Contingent Valuation (CV) methods. Two in-person interview surveys of forest recreation behaviour were carried out. The first was administered on-site in ten representative forest areas, and the other in the homes of a national sample of adult Poles. Results show that forest recreation is highly valued in Poland, at Euros 0. 64 - 6. 93 per trip per person, depending on the valuation method. Both trip frequency and per trip values are higher than the average in Western Europe, despite a lower income level. Thus, a simple GDP-adjusted transfer from Western Europe would substantially undervalue forest recreation in Poland. Further, a comparison of TC consumer surplus estimates and GDP/capita in Europe shows no clear relationship, indicating that a range of cultural, institutional and other factors may be important. ...|$|E
40|$|This paper {{presents}} the results from three lab-based studies that investigated different ways of delivering Mobile TV News by measuring user responses to different encoding bitrates, image resolutions and text quality. All {{studies were carried out}} with participants watching News content on mobile devices, with a total of 216 participants rating the acceptability of the viewing experience. Study 1 compared the acceptability of a 15 -second video clip at different video and audio encoding bit rates on a 3 G phone at a resolution of 176 x 144 and an iPAQ PDA (240 x 180). Study 2 measured the acceptability of video quality of full feature news clips of 2. 5 minutes which were recorded from broadcast TV, encoded at resolutions ranging from 120 x 90 to 240 x 180, and combined with different encoding bit rates and audio qualities presented on an iPAQ. Study 3 improved the legibility of the text included in the video simulating a separate text delivery. The acceptability of News ’ video quality was greatly reduced at a resolution of 120 x 90. The legibility of text was a decisive factor in the participants ’ assessment of the video quality. Resolutions of 168 x 126 and higher were substantially more acceptable when they were accompanied by optimized high quality text compared to <b>proportionally</b> <b>scaled</b> inline text. When accompanied by high quality text TV news clips were acceptable to the vast majority of participants at resolutions as small as 168 x 126 for video encoding bitrates of 160 kbps and higher. Service designers and operators can apply this knowledge to design a cost-effective mobile TV experience...|$|E
40|$|In this paper, {{we study}} the ergodic {{capacity}} of free space optical communication systems over Gamma-Gamma atmospheric turbulence fading channels with perfect channel state information {{at both the}} transmitter and the receiver. In our framework, we mainly focus on the low signal-to-noise ratio range and show that the ergodic capacity <b>scales</b> <b>proportionally</b> to SNR log^ 4 (1 /SNR). We show also that one-bit CSI feedback at the transmitter is enough to achieve this capacity using an on-o ff power control scheme...|$|R
40|$|ABSTRACT: In {{estimation}} of comminution effects, apart from process efficiency, forces presented during comminution and comminution level, the grain composition {{and content of}} irregular grains in product are very important. The jaw crushers give disadvantageous results of grain symmetry of products, but the very high changeability of crushing forces and individual comminution energy was obtained, dependently on the compressing tool shape. In experimental jaw crusher of straight jaw movement, the jaws with longitudinal ruts of transverse triangular, trapezoidal or circular profile were used. Furthermore, the ruts of various teeth sizes, i. e. of various height (h) and scale (t), were applied. The best results were obtained for jaws with trapezoidal ruts, of <b>proportionally</b> low <b>scale</b> t(t= 1, 1 e) - where e is the output rift size – and high values of teeth height h(h= 0, 8 e). The industrial efforts confirmed the results given in laboratory scale...|$|R
5|$|This metric {{contains}} a scale factor, which {{describes how the}} size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their coordinate distance (comoving distance) remains constant, the physical distance between two such co-moving points expands <b>proportionally</b> with the <b>scale</b> factor of the universe.|$|R
40|$|While the {{selection}} of new “backbone ” device structure {{in the era of}} post-planar CMOS is open to a few candidates, FinFET and its variants show great potential in scalability and manufacturability for nanoscale CMOS. In this paper we report the design, fabrication, performance, and integration issues of double-gate FinFET with the physical gate length being aggressively shrunk down to 10 nm and the fin width down to 12 nm. These MOSFETs are believed to be the smallest double-gate transistors ever fabricated. Excellent short-channel performance is observed in devices {{with a wide range of}} gate lengths (10 ~ 105 nm). The subthreshold slopes of the 10 nm gate length FinFETs are 125 mV/dec for n-FET and 101 mV/dec for p-FET, respectively. The DIBL’s are 71 mV/V for n-FET and 120 mV/V for p-FET, respectively. At 55 nm gate length, the subthreshold slopes are 64 mV/dec for n-FET and 68 mV/dec for p-FET, which is very close to the ideal MOSFET behavior (at room temperature). The DIBL’s are 11 mV/V for n-FET and 27 mV/V for p-FET, respectively. All measurements were performed at a supply voltage of 1. 2 V. The observed short-channel behavior outperforms any reported single-gate silicon MOSFETs. Due to the (110) channel crystal orientation, hole mobility in the fabricated p-channel FinFET remarkably exceeds that in a traditional planar MOSFET. At 105 nm gate length, pchannel FinFET shows a record-high transconductance of 633 µS/µm at a Vdd of 1. 2 V. At extremely small gate lengths, parasitic Rsd in the narrow fin (<b>proportionally</b> <b>scaled</b> with Lg) influences the device performance. Working CMOS FinFET inverters are also demonstrated...|$|E
40|$|Aggressive CMOS {{technology}} {{feature size}} scaling {{has been going}} on for the past decades, while the supply voltage is not <b>proportionally</b> <b>scaled.</b> Due to the increasing power density and electric field in the gate dielectric, the accelerating factors of failure mechanisms in nanoscale Integrated Circuits (ICs) have become more severe than ever. As a result, maintaining IC reliability at the desired level becomes a critical challenge at both design-time and runtime. Addressing the pessimistic reliability landscape outlook over current and future technology nodes, this dissertation investigates reliability-aware design and management techniques to ensure the reliability and quality of IC products. With our special interests on the time-dependent device parameter degradations due to intrinsic failure mechanisms, we focus our discussion on: (i) runtime reliability assessment, (ii) aging degradations, and (iii) mitigation techniques that enable reliability-aware computation. To this end we propose a Dynamic Reliability Management (DRM) framework to combat the aging-induced degradation. In order to achieve a quantitative management, dedicated online aging sensors are employed in the proposed framework to extract dynamic degradation information from circuits. We first propose a unified aging model for the emerging FinFET devices as the physical basis for understanding the underlying aging degradation. Then, we introduce two types of aging sensors, based on threshold voltage and power supply current measurement, respectively, to assist online reliability assessment in DRM systems. Next, we introduce a compensation technique to manage 6 T SRAM cell stability under spatial and temporal variations, by threshold voltage modulation using back-gate biasing of independent-gate FinFET devices. We conclude the dissertation by presenting a lifetime reliability modeling and enhancement framework, which demonstrates how to utilize the aging information from dedicated low-level aging sensors to maintain the overall IC health status within prescribed bounds. Software and Computer TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|This paper {{describes}} a relatively inexpensive, fast, {{and easy to}} execute approach to maping the volumetric errors of a machine tool, coordinate measuring machine, or robot. An error map is used to characterize a machine or to improve its accuracy by compensating for the systematic errors. The method consists of three steps: (1) models the relationship between volumetric error and {{the current state of}} the machine, (2) acquiring error data based on distance measurements throughout the work volume; and (3) fitting the error model using the nonlinear equation for the distance. The error model is formulated from the kinematic relationship among the six degrees of freedom of error an each moving axis. Expressing each parametric error as function of position each is combined to predict the error between the functional point and workpiece, also as a function of position. A series of distances between several fixed base locations and various functional points in the work volume is measured using a Laser Ball Bar (LBB). Each measured distance is a non-linear function dependent on the commanded location of the machine, the machine error, and the location of the base locations. Using the error model, the non-linear equation is solved producing a fit for the error model Also note that, given approximate distances between each pair of base locations, the exact base locations in the machine coordinate system determined during the non-linear filling procedure. Furthermore, with the use of 2048 more than three base locations, bias error in the measuring instrument can be removed The volumetric errors of three-axis commercial machining center have been mapped using this procedure. In this study, only errors associated with the nominal position of the machine were considered Other errors such as thermally induced and load induced errors were not considered although the mathematical model has the ability to account for these errors. Due to the proprietary nature of the projects we are not able to report actual error measurements. instead, we have scaled the work volume to 5 OOx 5 OOx 5 OOmm and <b>proportionally</b> <b>scaled</b> the errors. The fitted model was able to predict independently measured laser body diagonals to within 3. {mu}m peak-to-valley throughout @ scaled 500 x 500 x 500 mm` volume. or approximately 88 % of the total error. Furthermore, this approach performed as well as, if not better than the parametric methods, but required only four hours to collect data for calibration...|$|E
3000|$|... {{should be}} reduced, {{such that the}} data {{utilized}} to estimate the description length has a higher probability {{of being in the}} stationary regime. Finally, even though the procedure of discarding existing centers <b>scales</b> <b>proportionally</b> to the window size only, {{there is no need to}} check it at every sample in local stationary environments because they do not occur that fast nor all the time. We suggest that the update be done associated with the local stationary of the input data, i.e., at a rate at least twice the inverse of L [...]...|$|R
40|$|Many topics {{within the}} school {{mathematics}} and science curriculum require {{knowledge and understanding of}} ratio and proportion and the ability to reason <b>proportionally</b> (e. g., <b>scale</b> drawing, surface area, density, probability, molarity, fractions). Proportional reasoning, according to Lamon (2005) is fundamental to both mathematics and science. However, research has consistently highlighted students 2 ̆ 7 difficulties with proportion and proportion related tasks and applications (e. g, Behr, Harel, Post & Lesh, 1992), which means that, as a result, many students will struggle with topics in the middle years (fourth-ninth grades) in both mathematics and science...|$|R
5000|$|Buffering is used {{throughout}} high performance network systems to handle {{delays in the}} system. In general, buffer size {{will need to be}} <b>scaled</b> <b>proportionally</b> to the amount of data [...] "in flight" [...] at any time. For very high performance applications that are not sensitive to network delays, it is possible to interpose large end to end buffering delays by putting in intermediate data storage points in an end to end system, and then to use automated and scheduled non-real-time data transfers to get the data to their final endpoints.|$|R
