2781|1659|Public
25|$|Experimental {{research}} {{has spawned a}} large number of hypotheses about the architecture and mechanisms of sentence comprehension. Issues like modularity versus interactive processing and serial versus <b>parallel</b> <b>computation</b> of analyses have been theoretical divides in the field.|$|E
25|$|The {{situation}} is {{further complicated by}} the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, {{as a rule of}} thumb, high-performance <b>parallel</b> <b>computation</b> in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.|$|E
25|$|The {{programming}} of the {{stored program}} for ENIAC {{was done by}} Betty Jennings, Clippinger and Adele Goldstine. It was first demonstrated as a stored-program computer on September 16, 1948, running a program by Adele Goldstine for John von Neumann. This modification reduced the speed of ENIAC {{by a factor of}} six and eliminated the ability of <b>parallel</b> <b>computation,</b> but as it also reduced the reprogramming time to hours instead of days, it was considered well worth the loss of performance. Also analysis had shown that due to differences between the electronic speed of computation and the electromechanical speed of input/output, almost any real-world problem was completely I/O bound, even without making use of the original machine's parallelism. Most computations would still be I/O bound, even after the speed reduction imposed by this modification.|$|E
40|$|With {{the advent}} of many-core {{architectures}} and strong need for Petascale (and Exascale) performance in scientific domains and industry analytics, efficient scheduling of <b>parallel</b> <b>computations</b> for higher productivity and performance has become very important. Further, movement of massive amounts (Terabytes to Petabytes) of data is very expensive, which necessitates affinity driven computations. Therefore, distributed scheduling of <b>parallel</b> <b>computations</b> on multiple places 1 needs to optimize multiple performance objectives: follow affinity maximally and ensure efficient space, time and message complexity. Simultaneous consideration of these objectives makes distributed scheduling a particularly challenging problem. In addition, <b>parallel</b> <b>computations</b> have data dependent execution patterns which requires onlin...|$|R
50|$|A {{prospective}} way {{to accelerate}} logic simulation is using distributed and <b>parallel</b> <b>computations.</b>|$|R
40|$|Abstract — This {{paper is}} {{concerned}} with the analytical modeling of computer architectures to aid in the design of high-level language-directed computer architectures. High-level language-directed computers are computers that execute programs in a high-level language directly. The design procedure of these computers are at best described as being ad hoc. In order to systematize the design procedure, we introduce analytical models of computers that predict the performance of <b>parallel</b> <b>computations</b> on concurrent computers. We model computers as queueing networks and <b>parallel</b> <b>computations</b> as precedence graphs. The models that we propose are simple and lead to computationally efficient procedures of predicting the performance of <b>parallel</b> <b>computations</b> on concurrent computers. We demonstrate the use of these models in the design of high-level languagedirected computer architectures. I...|$|R
2500|$|... whereupon the {{induction}} principle [...] "automates" [...] log log n applications of this inference in getting from P(0) to P(n). [...] This form of induction has been used, analogously, to study log-time <b>parallel</b> <b>computation.</b>|$|E
2500|$|One key to {{the speed}} of analog {{computers}} was their fully <b>parallel</b> <b>computation,</b> but this was also a limitation. The more equations required for a problem, the more analog components were needed, even when the problem wasn't time critical. [...] "Programming" [...] a problem meant interconnecting the analog operators; even with a removable wiring panel this was not very versatile. Today {{there are no more}} big hybrid computers, but only hybrid components.|$|E
2500|$|The primary feature {{highlight}} for the {{new release}} of DirectX was the introduction of advanced low-level programming APIs for Direct3D 12 which can reduce driver overhead. Developers {{are now able to}} implement their own command lists and buffers to the GPU, allowing for more efficient resource utilization through <b>parallel</b> <b>computation.</b> Lead developer Max McMullen stated that the main goal of Direct3D 12 is to achieve [...] "console-level efficiency on phone, tablet and PC". The release of Direct3D 12 comes alongside other initiatives for low-overhead graphics APIs including AMD's Mantle for AMD graphics cards, Apple's Metal for iOS and macOS and Khronos Group's cross-platform Vulkan.|$|E
40|$|Researchers {{with deep}} {{knowledge}} of scientific domains {{are becoming more}} interested in developing highly-adaptive and irregular (asymmetrical) <b>parallel</b> <b>computations,</b> leading to development challenges for both delivery of data for compu-tation and mapping of processes to physical resources. Us-ing software engineering principles, we have developed a new communications protocol and architectural style for asym-metrical <b>parallel</b> <b>computations</b> called ADaPT. Utilizing the support of architecturally-aware middleware, we show that ADaPT provides a more efficient solution in terms of message passing and load balancing than asym-metrical <b>parallel</b> <b>computations</b> using collective calls in the Message-Passing Interface (MPI) or more advanced frame-works implementing explicit load-balancing policies. Ad-ditionally, developers using ADaPT gain significant wind-fall from good practices in software engineering, including implementation-level support of architectural artifacts and separation of computational loci from communication pro-tocols...|$|R
50|$|Fork-join queues {{have been}} used to model zoned RAID systems, <b>parallel</b> <b>computations</b> and for {{modelling}} order fulfilment in warehouses.|$|R
40|$|The present paper {{discusses}} real <b>parallel</b> <b>computations.</b> On {{the basis}} of a selected group of dynamic programming algorithms, a number of factors affecting the efficiency of <b>parallel</b> <b>computations</b> such as, e. g., the way of distributing tasks, the interconnection structure between particular elements of the parallel system or the way of organizing of interprocessor communication are analyzed. Computations were implemented in the parallel multitransputer SUPER NODE 1000 system using from 5 to 50 transputers...|$|R
50|$|In {{computational}} complexity theory, the <b>parallel</b> <b>computation</b> {{thesis is}} a hypothesis {{which states that}} the time used by a (reasonable) parallel machine is polynomially related to the space used by a sequential machine. The <b>parallel</b> <b>computation</b> thesis was set forth by Chandra and Stockmeyer in 1976.|$|E
5000|$|Chrono::Parallel, {{a library}} for {{enabling}} <b>parallel</b> <b>computation</b> in Chrono ...|$|E
50|$|Trilinos {{supports}} distributed-memory <b>parallel</b> <b>computation</b> {{through the}} Message Passing Interface (MPI). In addition, some Trilinos packages have growing support for shared-memory <b>parallel</b> <b>computation.</b> They {{do so by}} means of the Kokkos package in Trilinos, which provides a common C++ interface over various parallel programming models, including OpenMP, POSIX Threads, and CUDA.|$|E
30|$|The aim of {{the present}} work {{is to develop a}} {{steady-state}} formulation compatible with a velocity-based formulation, <b>parallel</b> <b>computations</b> and unstructured meshes. It is also expected to be sufficiently robust to handle all kinds of complex geometries met in metal forming applications. Therefore, the iterative fixed point approach is used to ensure compatibility with velocity formulation, and the free surface correction is computed through a global resolution to ensure compatibility with unstructured meshes and <b>parallel</b> <b>computations.</b>|$|R
5000|$|... (2009) The island {{model for}} <b>parallel</b> <b>computations</b> is made {{available}} and extended to non evolutionary algorithm {{in the open}} source software suite PaGMO.|$|R
40|$|We {{investigate}} the average-case speed and scalability of parallel algorithms executing on multiprocessors. Our performance metrics are average-speed and isospeed scalability. For {{the purpose of}} average-case performance prediction, we formally define the concepts of average-case average-speed and average-case isospeed scalability. By modeling parallel algorithms on multiprocessors using task precedence graphs, we are mainly interested in the effects of synchronization overhead and load imbalance {{on the performance of}} <b>parallel</b> <b>computations.</b> Thus, we focus on the structures of <b>parallel</b> <b>computations,</b> whose inherent sequential parts are limitations to high performance. Task execution times are treated as random variables, so that we can analyze the average-case performance of <b>parallel</b> <b>computations.</b> For several typical classes of task graphs, including iterative computations, search trees, partitioning algorithms, and diamond dags, we derive the growth rate of the number of tasks as well as [...] ...|$|R
50|$|He found {{theoretical}} bounds on <b>parallel</b> <b>computation</b> of multivariate polynomial.|$|E
5000|$|PETSc {{provides}} many {{features for}} <b>parallel</b> <b>computation,</b> broken into several modules: ...|$|E
50|$|A {{more general}} form of these {{relationships}} is expressed by the <b>parallel</b> <b>computation</b> thesis.|$|E
40|$|This paper {{describes}} Re-Vision, {{a software}} engineering methodology and a support ing computer enivornment for <b>parallel</b> <b>computations</b> {{based on the}} behavior-oriented formalism. A key feature provided by the environment is support for multiple views of the problem. Two levels of data transformation are supported: metamodeling and modeling. At the metamodeling level, methods used to construct different views of a given problem are specified. At the modeling level, tools to build different models for a given problem are introduced. In the Re-Vision environment, a set of computerized tools support data transformations from methods to metamodels and models. The Re-Vision environment thus provides a complexity reduction of the data transformation and user-interfaces between different tools for the design and analysis of <b>parallel</b> <b>computations.</b> Keywords: <b>parallel</b> <b>computations,</b> multiple views, data transformation, methodology specification. 1 Background Software engineering methodology and suppo [...] ...|$|R
40|$|AbstractExascale {{computing}} is {{fast becoming}} a mainstream research area. In order to realize exascale performance, {{it is necessary to}} have efficient scheduling of large <b>parallel</b> <b>computations</b> with scalable performance on a large number of cores/processors. The scheduler needs to execute in a pure distributed and online fashion, should follow affinity inherent in the computation and must have low time and message complexity. Further, it should also avoid physical deadlocks due to bounded resources including space/memory per core. Simultaneous consideration of these factors makes affinity driven distributed scheduling particularly challenging. We attempt to address this challenge for hybrid <b>parallel</b> <b>computations</b> which contain tasks that have pre-specified affinity to a place and also tasks that can be mapped to any place in the system. Specifically, we address two scheduling problems of the type Pm|Mj,prec|Cmax. This paper presents online distributed scheduling algorithms for hybrid <b>parallel</b> <b>computations</b> assuming both unconstrained and bounded space per place. We also present the time and message complexity for distributed scheduling of hybrid computations. To the best of our knowledge, {{this is the first time}} that distributed scheduling algorithms for hybrid <b>parallel</b> <b>computations</b> have been presented and analyzed for time and message bounds under both unconstrained space and bounded space...|$|R
50|$|From 1995 to 2002, he {{conducted}} research on computing with DNA. In May 1997 he and Mitsunori Ogihara discovered DNA based computers can perform massively <b>parallel</b> <b>computations.</b>|$|R
5000|$|... 1989-2000:Director of Education and Outreach Programs, Center for Research on <b>Parallel</b> <b>Computation,</b> Rice University ...|$|E
50|$|A cluster {{supercomputer}} {{has been}} installed facilitating <b>parallel</b> <b>computation,</b> and services researchers from several faculties.|$|E
5000|$|R. Motwani and P. Raghavan (1995); Randomized Algorithms, Cambridge International Series in <b>Parallel</b> <b>Computation,</b> Cambridge University Press.|$|E
5000|$|... {{which is}} a screw. The dot and cross {{products}} of screws satisfy the identities of vector algebra, and allow <b>computations</b> that directly <b>parallel</b> <b>computations</b> in the algebra of vectors.|$|R
3000|$|... 4 Conventional High Performance Computing (HPC) {{approaches}} use <b>parallel</b> <b>computations</b> {{to optimise}} processing time. We refer {{the reader to}} [37] for a good coverage of HPC-bound approaches for parallelising applications.|$|R
5000|$|In R (programming language) - The Simple Network of Workstations (SNOW) package {{implements}} {{a simple}} mechanism {{for using a}} set of workstations or a Beowulf cluster for embarrassingly <b>parallel</b> <b>computations.</b>|$|R
5000|$|Natural and Artificial <b>Parallel</b> <b>Computation</b> by Michael A. Arbib (Editor), J. Alan Robinson (Editor) (Hardcover - December 21, 1990) ...|$|E
5000|$|In Computer Science Helman-Bader-JaJa model [...] is a concise {{message passing}} model of <b>parallel</b> <b>computation</b> defined with the {{following}} parameters: ...|$|E
5000|$|A recent (2001) {{computational}} {{evaluation of}} Dantzig-Wolfe {{in general and}} Dantzig-Wolfe and <b>parallel</b> <b>computation</b> is the PhD thesis by J. R. Tebboth ...|$|E
40|$|Abstract — This paper {{proposes a}} novel {{strategy}} using <b>parallel</b> optimization <b>computations</b> for nonlinear moving horizon state estimation, and parameter identification problems of dynamic systems. The parallelization {{is based on}} the multipoint derivative-based Gauss-Newton search, {{as one of the most}} efficient algorithms for the nonlinear least-square problems. A numerical experiment is performed to demonstrate the <b>parallel</b> <b>computations</b> with the comparison to sequential computations. I...|$|R
40|$|The {{authors would}} like to express their {{appreciation}} to the following people who contributed to this program: Mr. Joseph P. Veres, NASA Lewis Research Center, for his suggestions and critical review of the program; Dale Hubler and Scott Townsend, NASA Lewis Research Center, for their assistance in implementing the <b>parallel</b> <b>computations</b> on the NASA Lewis LACE workstation cluster; James Jones, NASA Ames Research Center and the&quot; NAS facility staff, for their assistance in implementing the <b>parallel</b> <b>computations</b> on the NASA Ames davinci and babbage workstation clusters. Access to the computational resources of the NASA Ames NAS systems i...|$|R
40|$|A new formal {{model of}} <b>parallel</b> <b>computations</b> - the Kirdin kinetic machine - is {{suggested}} in [1]. It {{is expected that}} this model will play the role for <b>parallel</b> <b>computations</b> similar to Markov normal algorithms, Kolmogorov and Turing machine or Post schemes for sequential computations. The basic ways in which computations are realized are described; correctness of the elementary programs for the Kirdin kinetic machine is investigated. It is proved that the deterministic Kirdin kinetic machine is an effective calculator. A simple application of the Kirdin kinetic machine - heap encoding - is suggested. Subprograms similar to usual programming enlarge the Kirdin kinetic machine...|$|R
