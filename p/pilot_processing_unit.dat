0|10000|Public
40|$|In {{the present}} work, {{on the basis}} of a {{detailed}} experimental analysis, we propose a mathematical model for the interpretation of the hydrogenation of aromatic compounds during gasoil hydroprocessing. The model considers a lumped scheme for gasoil composition and assumes that hydrogenation/dehydrogenation reactions occur according to the Langmuir-Hinshel wood mechanisms, where hydrocarbons and hydrogen adsorption occur on different active sites. Kinetic rate constants were evaluated through the fitting of a set of experimental data obtained on a <b>pilot</b> <b>unit</b> <b>processing</b> with a commercial NiMo/Al 2 O 3 catalyst a gasoil mixture in a wide range of operating conditions, spanning the typical industrial ranges of pressure, temperature and LHSV. Further, independent sets of data obtained with the same catalyst but different feed qualities are correctly predicted by the model, confirming its reliability. Also, the model is applicable for the interpretation of hydrogenation behavior with different types of commercial hydroprocessing catalysts. (C) 2004 Elsevier Ltd. All rights reserved...|$|R
40|$|In this paper, we have modeled a {{production}} line consisting of an arbitrary number of <b>processing</b> <b>units</b> {{arranged in a}} series. Each of the <b>processing</b> <b>units</b> has multi-server facility. Arrivals at the first <b>processing</b> <b>unit</b> are according to Poisson distribution and service times {{at each of the}} <b>processing</b> <b>units</b> are exponentially distributed. At each of the <b>processing</b> <b>units,</b> the authors have taken into account immediate feedback and the rejection possibility. Taking into account the stationary behavior of queues in series, the solution for infinite queuing space have been found in the product form. Considering the processing cost at each of the <b>processing</b> <b>units,</b> the average loss to the system due to rejection, caused by ill processing at various <b>processing</b> <b>units,</b> is obtained...|$|R
40|$|Current {{generations of}} {{graphics}} <b>processing</b> <b>units</b> {{have turned into}} highly parallel devices with general computing capabilities. Thus, graphics <b>processing</b> <b>units</b> may be utilized, for example, to solve time dependent partial differential equations by the Fourier split operator method. In this contribution, we demonstrate that graphics <b>processing</b> <b>units</b> are capable to calculate fast Fourier transforms much more efficiently than traditional central <b>processing</b> <b>units.</b> Thus, graphics <b>processing</b> <b>units</b> render efficient implementations of the Fourier split operator method possible. Performance gains of more than {{an order of magnitude}} as compared to implementations for traditional central <b>processing</b> <b>units</b> are reached in the solution of the time dependent Schrödinger equation and the time dependent Dirac equation...|$|R
5000|$|... pGPU (physical {{graphics}} <b>processing</b> <b>unit)</b> is a graphics <b>processing</b> <b>unit,</b> {{as opposed}} to virtual graphics <b>processing</b> <b>unit</b> (vGPU). pGPU and vGPU terms are often used to distinguish modes of operating of server graphics virtualization solutions, such as Nvidia GRID.|$|R
5000|$|The POWERx'plorer {{was based}} on 8 <b>processing</b> <b>units</b> {{arranged}} in a 2D mesh. Each <b>processing</b> <b>unit</b> had ...|$|R
50|$|Each <b>processing</b> <b>unit</b> was an {{independent}} one-board microcomputer. Motorola's microprocessor MC68020 (25MHz) {{was used as}} the CPU.The local memory was 4MBytes with 100ns 1Mbit DRAM.The QCDPAX utilized LSI Logic's floating-point <b>processing</b> <b>unit</b> L64133 onthe market. L64133 had peak performance of the 33MFLOPS.The floating-point <b>processing</b> <b>unit</b> controller, newly developed by the gate array, was alsoutilized to derive the performance from the FPU by controlling the direct memory accessbetween the data memory and floating-point <b>processing</b> <b>unit.</b>|$|R
40|$|We are {{considering}} the following assignment problem. A finite set T of tasks th, h= 1, [...] .,n, {{has to be}} assigned to a finite set P of <b>processing</b> <b>units</b> pj, j= 1, [...] .,m. Each task th has to be assigned to exactly one <b>processing</b> <b>unit,</b> however, more than one <b>processing</b> <b>unit</b> is able to process th. Each task th has, moreover, a different “preference ” for being assigned to a particular <b>processing</b> <b>unit...</b>|$|R
5000|$|... #Caption: A {{part of an}} IBM T42 laptop motherboard. CPU: Central <b>processing</b> <b>unit.</b> NB: Northbridge. GPU: Graphics <b>processing</b> <b>unit.</b> SB: Southbridge.|$|R
50|$|Apple {{designed}} 32-bit ARMv7 based {{application processor}} APL0778 {{as the central}} <b>processing</b> <b>unit</b> (CPU), with an integrated PowerVR SGX543 graphics <b>processing</b> <b>unit</b> (GPU).|$|R
40|$|This paper {{presents}} the parallel computing {{implementation of the}} MitISEM algorithm, labeled Parallel MitISEM. The basic MitISEM algorithm provides an automatic and flexible method to approximate a non-elliptical target density using adaptive mixtures of Student-t densities, where only a kernel of the target density is required. The approximation {{can be used as}} a candidate density in Importance Sampling or Metropolis Hastings methods for Bayesian inference on model parameters and probabilities. We present and discuss four canonical econometric models using a Graphics <b>Processing</b> <b>Unit</b> and a multi-core Central <b>Processing</b> <b>Unit</b> version of the MitISEM algorithm. The results show that the parallelization of the MitISEM algorithm on Graphics <b>Processing</b> <b>Units</b> and multi-core Central <b>Processing</b> <b>Units</b> is straightforward and fast to program using MATLAB. Moreover the speed performance of the Graphics <b>Processing</b> <b>Unit</b> version is much higher than the Central <b>Processing</b> <b>Unit</b> one...|$|R
25|$|As of 2016, vision <b>processing</b> <b>units</b> are {{emerging}} as {{a new class of}} processor, to complement CPUs and Graphics <b>processing</b> <b>units</b> (GPUs) in this role.|$|R
50|$|Desmond is also {{available}} in a graphics <b>processing</b> <b>unit</b> (GPU) accelerated version that is about 60-80 {{times faster than the}} central <b>processing</b> <b>unit</b> (CPU) version.|$|R
50|$|As of 2016, vision <b>processing</b> <b>units</b> are {{emerging}} as {{a new class of}} processor, to complement CPUs and Graphics <b>processing</b> <b>units</b> (GPUs) in this role.|$|R
40|$|Architecture for fully {{autonomous}} digital {{electronic control}} system developed for use in identification and adaptive control of dynamic system. Architecture modular and hierarchical. Combines relatively simple, standardized <b>processing</b> <b>units</b> into complex parallel-processing subsystems. Although architecture based on neural-network concept, <b>processing</b> <b>units</b> themselves not neural networks; <b>processing</b> <b>units</b> implemented by programming of currently available microprocessors...|$|R
5000|$|... #Caption: A {{part of an}} IBM T42 laptop motherboard, {{with the}} {{following}} labels: CPU (central <b>processing</b> <b>unit),</b> NB (northbridge), GPU (graphics <b>processing</b> <b>unit),</b> and SB (southbridge).|$|R
40|$|An {{electronic}} {{computing device}} {{including at least}} one <b>processing</b> <b>unit</b> that implements a specific fault signal upon experiencing an associated fault, a control unit that generates a specific recovery signal upon receiving the fault signal from {{the at least one}} <b>processing</b> <b>unit,</b> and at least one input memory unit. The recovery signal initiates specific recovery processes in the at least one <b>processing</b> <b>unit.</b> The input memory buffers input data signals input to the at least one <b>processing</b> <b>unit</b> that experienced the fault during the recovery period...|$|R
40|$|International audienceOil and gas {{companies}} rely on {{high performance}} computing to process seismic imaging algorithms such as reverse time migration. Graphics <b>processing</b> <b>units</b> {{are used to}} accelerate reverse time migration, but these deployments suffer from limitations such as the lack of high graphics <b>processing</b> <b>unit</b> memory capacity, frequent CPU-GPU communications that may be bottlenecked by the PCI bus transfer rate, and high power consumptions. Recently, AMD has launched the Accelerated <b>Processing</b> <b>Unit</b> (APU) : a processor that merges a CPU and a graphics <b>processing</b> <b>unit</b> on the same die featuring a unified CPU-GPU memory. In this paper, we explore how efficiently may the APU be applicable to reverse time migration. Using OpenCL (along with MPI and OpenMP), a CPU/APU/GPU comparative study is conducted on a single node for the 3 D acoustic reverse time migration, and then extended on up to 16 nodes. We show the relevance of overlapping the I/O and MPI communications with the computations for the APU and graphics <b>processing</b> <b>unit</b> clusters, that performance results of APUs range between those of CPUs and those of graphics <b>processing</b> <b>units,</b> and that the APU power efficiency is {{greater than or equal}} to the graphics <b>processing</b> <b>unit</b> one...|$|R
3000|$|For decoders with {{parallel}} <b>processing</b> <b>units</b> (see [7, 25]) {{the architectural}} efficiency becomes {{a measure of}} the parallelization used in the <b>processing</b> <b>units</b> and it can be expressed as [...]...|$|R
40|$|There is {{a wealth}} of {{literature}} on distributed algorithms and data structures. Standard models used in the research community are synchronous or asynchronous shared memory or network models. The shared memory model is basically a generalization of the von Neumann model from one <b>processing</b> <b>unit</b> to multiple <b>processing</b> <b>units</b> or processes acting on a single, linear addressable memory. In the network model, there is no shared memory. Every <b>processing</b> <b>unit</b> has its own, private memory, and the <b>processing</b> <b>units</b> are connected by a network of (usually) bidirectional communication links that allow the <b>processing</b> <b>units</b> to exchange messages. The set of <b>processing</b> <b>units</b> is usually considered to be fixed though <b>processing</b> <b>units</b> may fail and recover according to some stochastic or adversarial model. With the rise of very large distributed systems such as peer-to-peer systems, these models are not appropriate any more. For example, the set of <b>processing</b> <b>units</b> can be highly dynamic and {{there may not be}} any mutual trust relationships between the units. This creates fundamental problems, such as keeping the (honest) units in a single connected component, that the previous models cannot address in their basic form. We show how to extend the network model so that we have a model that is powerful enough to design algorithms and data structures that are provably robust even against massive adversaria...|$|R
25|$|More recent {{implementations}} {{based upon}} this work {{run on the}} game systems graphics <b>processing</b> <b>unit</b> (GPU) {{as opposed to the}} central <b>processing</b> <b>unit</b> (CPU) and achieve a much higher degree of performance.|$|R
30|$|Central <b>processing</b> <b>unit</b> (CPU) {{refers to}} the main {{processor}} of a computer and graphics <b>processing</b> <b>unit</b> (GPU) refers to a discrete graphics card connected to the computer via a high speed bus.|$|R
5000|$|Ryzen ( [...] ) is a {{brand of}} central <b>processing</b> <b>units</b> (CPUs) and {{accelerated}} <b>processing</b> <b>units</b> (APUs) marketed and designed by AMD. The brand was introduced in 2017 with products implementing the Zen microarchitecture.|$|R
5000|$|Vision <b>processing</b> <b>units</b> are {{distinct}} from video <b>processing</b> <b>units</b> (which are specialised for video encoding and decoding) in their suitability for running machine vision algorithms such as convolutional neural networks, SIFT etc.|$|R
50|$|Established {{in an area}} of Greece rich in bauxite deposits, the company's plants are: a bauxite <b>processing</b> <b>unit</b> for the {{production}} of alumina and an alumina <b>processing</b> <b>unit</b> for {{the production}} of aluminum.|$|R
50|$|The LogP machine {{consists}} of arbitrarily many <b>processing</b> <b>units</b> with distributed memory.The <b>processing</b> <b>units</b> are connected through an abstract communication medium which allows point-to-point communication. This model is pair-wise synchronous and overall asynchronous.|$|R
50|$|The API enables and {{provides}} access to hardware-accelerated video processing, using hardware such as graphics <b>processing</b> <b>units</b> (GPU) to accelerate video encoding and decoding by offloading processing from the central <b>processing</b> <b>unit</b> (CPU).|$|R
5000|$|The 168 was {{described}} as having [...] "two types of multiprocessing support" [...] since it also offered attaching a second <b>processing</b> <b>unit,</b> an IBM 3062 Attached <b>Processing</b> <b>Unit,</b> which lacked access to Input/Output channels.|$|R
40|$|Graphics <b>processing</b> <b>units</b> {{function}} well as {{high performance}} computing devices for scientific computing. The non-standard processor architecture and high memory bandwidth allow graphics <b>processing</b> <b>units</b> (GPUs) {{to provide some}} of the best performance in terms of FLOPS per dollar. Recently these capabilities became accessible for general purpose computations with the CUDA programming environment on NVIDIA GPUs and ATI Stream Computing environment on ATI GPUs. Many applications in computational science are constrained by memory access speeds and can be accelerated significantly by using GPUs as the compute engine. Using graphics <b>processing</b> <b>units</b> as a compute engine gives the personal desktop computer a processing capacity that competes with supercomputers. Graphics <b>Processing</b> <b>Units</b> represent an energy efficient architecture for high performance computing in flow simulations and many other fields. This document reviews the graphic <b>processing</b> <b>unit</b> and its features and limitations. ...|$|R
40|$|Engineering {{problems}} {{involve the}} solution of large sparse linear systems, and require therefore fast and high performance algorithms for algebra operations such as dot product, and matrix-vector multiplication. During the last decade, graphics <b>processing</b> <b>units</b> have been widely used. In this paper, linear algebra operations on graphics <b>processing</b> <b>unit</b> for single and double precision (with real and complex arithmetic) are analyzed {{in order to make}} iterative Krylov algorithms efficient compared to central <b>processing</b> <b>units</b> implementation. The performance of the proposed method is evaluated for the Laplace and the Helmholtz equations. Numerical experiments clearly show the robustness and effectiveness of the graphics <b>processing</b> <b>unit</b> tuned algorithms for compressed-sparse row data storage...|$|R
40|$|In this {{research}} a universal, flexible, and compact <b>processing</b> <b>unit</b> {{with a time}} precision of about 30 ps is being designed and satisfactorily tested using the FPGA technology {{as a first step}} to implement the <b>processing</b> <b>unit</b> in a more compact fashion utilizing an ASIC technology. The work is being designed to replace the traditional expensive sampling method, which is used in most of the pulses radar application as core of the <b>processing</b> <b>unit.</b> The unit has been designed, implemented and tested on a LIDAR, which was accommodating a sampling oscilloscope to perform the signal <b>processing.</b> This <b>processing</b> <b>unit</b> could be considered as a low cost time-to-digital converter module...|$|R
40|$|This {{document}} {{presents a}} proposal {{of a new}} architecture for implementation of Digital Signal Processing (DSP) algorithms in Field-Programmable Gate Array (FPGA). The proposed approach uses the dual port memory for fast exchange of information between the <b>processing</b> <b>units</b> implemented in the FPGA. The special, parametrized scheme of interconnections between <b>processing</b> <b>units</b> has been also proposed, which allows to synthesize DSP system with customized number of <b>processing</b> <b>units.</b> The proposed interconnections scheme provides possibility to quickly transfer the data between <b>processing</b> <b>units,</b> at reasonable consumption of routing resources. The proposed architecture has been tested in simulations, and synthesized for real FPGA chips to verify its correctness. ...|$|R
40|$|The {{influence}} of multi-core central <b>processing</b> <b>units</b> and graphics <b>processing</b> <b>units</b> on several algebraic multigrid methods is investigated in this work. Different performance metrics traditionally employed for algebraic multigrid are re-considered and reevaluated on these novel computing archi-tectures. Our benchmark {{results show that}} {{with the use of}} graphics <b>processing</b> <b>units</b> for the solver phase, it is crucial to keep algebraic multigrid setup low, even if this leads to a higher number of solver iterations. 1...|$|R
40|$|Abstract—In this paper, {{we present}} a throughput-scalable {{parallel}} and pipeline architecture for high-throughput compu-tation of multilevel three-dimensional discrete wavelet transform (3 -D DWT). The computation of 3 -D DWT for each level of decomposition is split into three distinct stages, and all the three stages are implemented in parallel by a <b>processing</b> <b>unit</b> consisting of an array of processing modules. The <b>processing</b> <b>unit</b> for the first level decomposition of a video stream of frame-size (M×N) consists of Q/ 2 processing modules, where Q {{is the number of}} input samples available to the structure in each clock cycle. The <b>processing</b> <b>unit</b> for a higher level of decomposition requires 1 / 8 times the number of processing modules required by the <b>processing</b> <b>unit</b> for its preceding level. For J level 3 -D DWT of a video stream, each of the proposed structures involves J <b>processing</b> <b>units</b> in a cascaded pipeline. The proposed structure...|$|R
50|$|ND4J is for {{performing}} linear algebra and matrix manipulation in a production environment, integrating with Apache Hadoop and Spark {{to work with}} distributed central <b>processing</b> <b>units</b> (CPUs) or graphics <b>processing</b> <b>units</b> (GPUs). It supports n-dimensional arrays for JVM-based languages.|$|R
40|$|A DRAM {{communicates}} with a <b>processing</b> <b>unit</b> via two interfaces: a {{data interface}} and a command interface. In today 2 ̆ 7 s DRAMs, {{also known as}} synchronous DRAMs (SDRAMs), both interfaces use a clock {{to communicate with the}} <b>processing</b> <b>unit.</b> The clock times the communication between the <b>processing</b> <b>unit</b> and the SDRAM on both the data interface and the command interface. We propose a self-timed DRAM. The self-timed DRAM introduces more flexibility into the DRAM interface by eliminating the clock. The command interface and the data interface each communicate with the <b>processing</b> <b>unit</b> using a handshake protocol rather than a clock. This thesis presents the data interface between the self-timed DRAM and the <b>processing</b> <b>unit.</b> The proposed data interface is self-timed. The self-timed data interface allows the DRAM to deliver data to or accept data from the <b>processing</b> <b>unit</b> as the <b>processing</b> <b>unit</b> demands rather than on a schedule set from the command interface. The self-timed data interface is designed using GasP circuits and micropipeline circuits. The design is simulated in 180 nm CMOs process technology using hspice. This thesis presents the effects of width mismatch on the self-timed data interface. The micropipeline is slightly faster than the GasP. Also, the thesis compares the self-timed DRAM data interface with synchronous DRAM for the data burst rate...|$|R
40|$|A finite-difference Micromagnetic {{simulation}} code {{written in}} MATLAB is presented with Graphics <b>Processing</b> <b>Unit</b> (GPU) acceleration. The high performance of Graphics <b>Processing</b> <b>Unit</b> (GPU) is demonstrated {{compared to a}} typical Central <b>Processing</b> <b>Unit</b> (CPU) based code. The speed-up of GPU to CPU is shown to be greater than 30 for problems with larger sizes on a mid-end GPU in single precision. The code is less than 200 lines and suitable for new algorithm developing. Comment: 17 pages, 5 figure...|$|R
