37|10000|Public
50|$|To be {{eligible}} for entering the importance calculation, an account must have at least 10,000 vested XEM. All accounts owning more than 10,000 vested XEM have a non-zero importance score. With a supply of 8,999,999,999 XEM, the theoretical maximum number of accounts with non-zero importance is 899,999. In practice, the number of actual accounts with non-zero importance {{is not expected to}} approach the theoretical max due to inequalities in held XEM and also the temporal costs associated with vesting. If NEM becomes very popular, a threshold of 10,000 vested XEM could be undesirable. If necessary, this number could be updated in the future via a hard fork, which is the same <b>procedure</b> <b>for</b> <b>adjusting</b> transaction fees and other parameters related to harvesting.|$|E
50|$|Alterations {{to local}} {{government}} boundaries had been suspended with {{the outbreak of}} the Second World War in 1939. Previously they had been carried out by a number of processes: county boroughs could be constituted or extended by private act of parliament, while county councils were to carry out reviews of county districts (non-county boroughs, urban and rural districts) on a ten yearly cycle. There was no general <b>procedure</b> <b>for</b> <b>adjusting</b> boundaries between administrative counties, or for amalgamating them. The different procedures were not coordinated. The wartime coalition government published a white paper in January, 1945 entitled Local government in England and Wales during the period of reconstruction. The document proposed the establishment of a Local Government Boundary Commission with executive powers to alter council areas, taking over the powers of the county councils and Minister for Health to change areas. In future all proposed changes by local authorities were to be submitted to the Commission. The Commission was to consider administration in each geographical county (the administrative county plus associated county boroughs) and see if there was a prima facie case for a review. If it felt a review was warranted the commission was to notify the Minister and the relevant county council, who could then require the holding of an inquiry into local government in the county.|$|E
40|$|This report {{discusses}} the <b>procedure</b> <b>for</b> <b>adjusting</b> the public debt limit. Nearly {{all of the}} outstanding debt {{of the federal government}} is subject to a statutory limit, which is set forth as a dollar limitation in 31 U. S. C. 3101 (b). From time to time, Congress considers and passes legislation to adjust or suspend this limit...|$|E
40|$|Current <b>procedures</b> <b>for</b> <b>adjusting</b> {{estimates}} of the mechanical properties of lumber for changes in moisture content are based on trends in the observed means. The present study was initiated to develop analytical <b>procedures</b> <b>for</b> <b>adjusting</b> {{estimates of}} the flexural properties of 2 -inch-thick Douglas-fir dimension lumber that would be applicable to {{all levels of the}} flexural properties. Equations are derived <b>for</b> <b>adjusting</b> modulus of rupture (MOR), modulus of elasticity (MOE), moment capacity (RS = MOR × section modulus), and flexural stiffness (EI = MOEX moment of inertia) for changes in moisture content. The best of these equations are found to be significantly more accurate than current <b>procedures</b> <b>for</b> <b>adjusting</b> estimates of strength properties such as MOR and RS. Because MOE and EI are less affected by changes in moisture content, most of the equations work well for these properties...|$|R
40|$|No {{research}} {{to date has}} been conducted to investigate the efficacy of and proper <b>procedures</b> <b>for</b> <b>adjusting</b> multiple correlations <b>for</b> the combined effects of regression overfitting and indirect range restriction. The present study uses Monte Carlo analyses to investigate the implementation of both of these adjustments...|$|R
40|$|The {{principal}} {{concern of}} this paper is the development of <b>procedures</b> <b>for</b> <b>adjusting</b> the criteria currently being used for federally-legislated health planning activities. These procedures would enable the planner to account for the demographic, geographic and health-system conditions which cause variations in the need for health-care services in local communities. A case-mix method, hospital chart-abstract data and demographic, geographic and health-system data from New Jersey were used to: create a list of diagnoses eligible for treatment in a Cardiac-Care Unit (CCU); select a sample of hospitals for study, and conduct a step-wise regression analysis of CCU utilization in these hospitals. It was concluded that CCU utilization was affected by factors such as the in-hospital availability of CCU beds, the type of hospital, CCU-patients' clinical severity, and the availability of ambulances and mobile intensive care units. <b>Procedures</b> <b>for</b> <b>adjusting</b> planning criteria to account for local conditions have yet to be developed. However, a method for using the types of results presented in this paper to develop such adjustment procedures was presented and illustrated. It is recommended that this method be used to create such adjustment <b>procedures</b> <b>for</b> the planning criteria for all hospital services and hence to assist Health Systems Agencies in rationalizing the distribution of our hospital care. ...|$|R
40|$|A {{method of}} {{compensation}} for a diamagnetic loop that is magnetically coupled to a concentric stainless-steel vacuum vessel is presented. This compensation method accounts for imperfect magnetic coupling between the vessel eddy currents, the diamagnetic loop, and the plasma diamagnetic currents, {{and it also}} corrects for a finite loading resistance on the diamagnetic loop. A <b>procedure</b> <b>for</b> <b>adjusting</b> and calibrating the active-filter compensation circuit is presented. It {{can be applied to}} internal or external diamagnetic loops...|$|E
40|$|This paper {{develops}} a <b>procedure</b> <b>for</b> <b>adjusting</b> the Current Population Survey gross changes {{data for the}} effects of reporting errors. The corrected data suggest that the labor market is much less dynamic than has frequently been suggested. Conventional measures sy understate the duration of unemployment by as much as eighty percent and overstate the extent of movement into and out ofthe labor force by several hundred percent. The adjusted data also throw demographic differences in patterns of labor market dynamics into sharp relief. ...|$|E
40|$|Abstract:- This work {{presents}} an automatic <b>procedure</b> <b>for</b> <b>adjusting</b> the gains of a Proportional-Integral-Derivative (PID) controller. Genetic Algorithms {{are used for}} tuning this controller so that closed-loop s ep response specifications are satisfied. By using this procedure, designers need only specify the desired closed-loop response. Experiments with different processes indicate that the gains obtained through genetic algorithms may provide better responses than those obtained by the classical Ziegler-Nichols method. Moreover, the genetic algorithm is capable of generating adequate gains for systems where classical rules are not applicable...|$|E
30|$|The {{concretes}} {{used in the}} APT test slabs {{were produced}} by Argos concrete plant in Gainesville, Florida. The LWA was prepared in accordance with procedure documented in ASTM C 1761. The LWA pile was sprinkled with water for one week before batching to ensure the saturated condition. Standard <b>procedures</b> <b>for</b> <b>adjusting</b> excess water were conducted similarly to typical coarse aggregate. They were mixed in a central mix plant and transported to the test site by a concrete delivery truck. The transit time from the mixing plant to the job site was less than 30  min.|$|R
40|$|This paper simulates {{the effects}} of {{proposals}} to modify <b>procedures</b> <b>for</b> <b>adjusting</b> the Social Security benefits of those who work after normal retirement age. A basic set of policies, currently under consideration, is projected to raise long run costs by $ 30 billion dollars net of taxes, while inducing an increase of 5 percent {{in the number of}} full-time male workers between the ages of 65 and 69. Alternative policies may create very different flows of funds. Outcomes, especially in the short run, will vary widely with the timing of the application decision for benefits. ...|$|R
50|$|Steve Wozniak {{found out}} {{that some of the}} SA-390 drives didn't work without {{adjustments}} while others simply wouldn't work at all. Fellow engineer Cliff Huston came up with several <b>procedures</b> <b>for</b> <b>adjusting</b> the drives on the assembly line. When Apple sent an order into Shugart for more SA-390s, a Shugart engineer admitted that the disk drive manufacturer had been scamming Apple and that the SA-390s were actually reject SA-400s that failed to pass factory inspection. The idea was that Apple couldn't get the drives to work and would be forced to go back and purchase more expensive SA-400s.|$|R
40|$|Abstract — This paper {{presents}} algorithms {{and techniques}} for designing adaptive finite impulse response (FIR) filters that maintain nearly lossless constraints on their input-output characteristics. Both single-channel allpass and multichannel paraunitary adaptive filters are considered. Our methods {{make use of}} a simple, iterative <b>procedure</b> <b>for</b> <b>adjusting</b> the coefficients of a single- or multichannel FIR filter to impose allpass or paraunitary filter constraints periodically during coefficient adaptation. The technique is easily extended to multidimensional convolution form, and we illustrate such an extension via a simple image sequence encoding application. I...|$|E
40|$|Increasingly, {{researchers}} in marketing are recognizing the “lability” of attribute importance weights derived from measurement techniques, such as conjoint analysis. As {{has been suggested}} by Simonson and Tversky, attribute importance weights can be sensitive to competitive product context and to purchase situation. This paper describes and applies a <b>procedure</b> <b>for</b> <b>adjusting</b> conjoint importance weights to predict consumers' actual or potential product choices. We discuss the approach from both a descriptive and prescriptive viewpoint. In particular, the latter perspective provides strategic insights into how attribute importance modifications can increase brand share. An industry case, based on real data, is used to illustrate the approach. competitive strategy, scaling methods...|$|E
40|$|Article {{appears in}} Applied Physics Letters ([URL] and is copyrighted by American Institute of Physics ([URL] is {{observed}} in the charge–voltage (Q–V) or internal charge–phosphor field (Q–Fp) characteristics of certain alternating-current thin-film electroluminescent (ACTFEL) devices. This offset arises from a displacement along the voltage axis of a transient curve measured across a sense capacitor in the electrical characterization setup. A <b>procedure</b> <b>for</b> <b>adjusting</b> this offset is proposed that allows ACTFEL devices manifesting offset to be meaningfully analyzed. Two possible sources of offset are deduced from simulation and are associated with an asymmetry in the interface state energy depths at the two phosphor–insulator interfaces or with an asymmetry in the location of space charge generation in the phosphor...|$|E
40|$|Numerical and {{graphical}} {{summaries of}} RNA-Seq read data. Within-lane normalization <b>procedures</b> to <b>adjust</b> <b>for</b> GC-content effect (or other gene-level effects) on read counts: loess robust local regression, global-scaling, and full-quantile normalization (Risso et al., 2011). Betweenlane normalization <b>procedures</b> to <b>adjust</b> <b>for</b> distributional differences between lanes (e. g., sequencing depth) : global-scaling and full-quantile normalization (Bullard et al., 2010) ...|$|R
40|$|This report {{presents}} specific <b>procedures</b> used <b>for</b> <b>adjusting</b> radiation doses {{to radiation}} personnel at the ORNL and Y- 12 plants {{during the early}} years. Topics discussed include: background information; selection of employment years to be considered; hardcopy monitoring methods and records; pocket meter data; and replacement of 1943 unmonitored employment years. These topics were discussed for both years...|$|R
40|$|Recurrent connectionist {{networks}} {{are important because}} they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks which have an important temporal component not easily handled {{through the use of}} simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning <b>procedures</b> <b>for</b> <b>adjusting</b> the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are describe [...] ...|$|R
40|$|An {{interpolation}} procedure {{using the}} rational spline approximation is presented. The rational spline contains an adjustable tension parameter for {{each pair of}} uccessive knots such that {{in the limit of}} zero tension, the spline reduces to a conventional cubic spline and in the limit of infinite tension the spline reduces to a straight line. The <b>procedure</b> <b>for</b> <b>adjusting</b> the tension parameters is completely automated such that by adjusting the tension it is possible to achieve the quot;bestquot; fit (in a least-squares sense) of the data. A detailed derivation of the algorithm is presented along with its procedural implementation. the method is successfully implemented on a computer and tested for two cases. Recommendations are made for further extension and application of this method...|$|E
40|$|A little-studied <b>procedure</b> <b>for</b> <b>adjusting</b> the {{properties}} of low-fat products {{is to use the}} influence that both composition and certain processing factors exert on these properties. The object of the present work was to assess the effects of protein level (P, ranging from 10 % to 16 %), fat level (F, ranging from 10. 1 % to 22 %) and cooking temperature (HT, ranging from 77 °C to 105 °C) on the binding properties and colour of meat emulsions. Protein content was the variable that most influenced total expressible fluid (TEF) and purge loss. Heating rate had scarcely any effect on the binding properties of Bologna sausages. Analysis of variance indicated that the regression models for parameters L, a and b were not significant. © 1995. Peer Reviewe...|$|E
40|$|In this paper, an {{inventory}} model of deteriorating products with life {{time has been}} discussed. Demand rate has been taken of quadratic form which starts from zero in the beginning and ends to zero {{at the completion of}} the cycle. Deterioration rate has been taken constant. Shortages are considered and backlogged. Cost minimization technique has been used {{in the development of the}} model. Key-words: Deterioration, life time, shortages and backlogged. Inventory models involving time dependent demand pattern have received the attention of several researchers. Silver and Meal (1973), Donaldson (1977), Ritchie (1984) are worth mentioning for doing work in this direction. Mitra et al. (1984) presented a simple <b>procedure</b> <b>for</b> <b>adjusting</b> the economic order quantity (EOQ) model for the cases of increasing or decreasing linear trend in demand. Dave & Pate...|$|E
40|$|The present article {{emphasizes}} that measurement issues must be explicitly considered even in studies {{that focus on}} substantive questions. First, dynamics associated with insuf-ficient attention being paid to score reliabilities in substantive studies are discussed. Next, reasons to adjust effect size indices for score unreliability are presented. Finally, some <b>procedures</b> <b>for</b> <b>adjusting</b> effect sizes <b>for</b> score reliability are briefly reviewed. Most social science researchers {{are aware of the}} roughly 80 -year-old controversy (cf. Boring, 1919) regarding statistical significance testing (Pedahazur & Schmelkin, 1991). Although there are thoughtful supporters of null hypothesis testing (e. g., Abelson, 1997; Cortina & Dunlap, 1997; Frick, 1996; Robinson & Wainer, in press), researchers increasingly recognize the limitations of the statistical method Rozeboom (1997) described as “the most bone-headed misguided procedure ever institutionalized in the rote training of science students ” (p. 335). This awakening is attributable to the efforts o...|$|R
40|$|Abstract Background We {{propose a}} {{statistical}} method {{that includes the}} use of longitudinal regression models and estimation <b>procedures</b> <b>for</b> <b>adjusting</b> <b>for</b> covariate effects in applying the Haseman-Elston (HE) method for linkage analysis. Our methodology, which uses the covariate adjusted trait, contains three steps: a) modelling the covariate-adjusted population means of quantitative traits through regression; b) estimating the value of covariate-adjusted quantitative traits; and c) evaluating the linkage between the adjusted trait values and the markers based on alleles shared identically by descent. Results We applied our adjusted HE method and the standard HE method in S. A. G. E. to the sib-pair subset of the Framingham Heart Study distributed by Genetic Analysis Workshop 13 with systolic blood pressure as the quantitative trait. Both methods gave similar patterns for the LOD scores, and exhibited highest multipoint LOD scores near location 70 cM of chromosome 12. Conclusion The adjusted HE method has two major advantages over the standard HE method used in S. A. G. E. : a) it has the capability to handle longitudinal data; b) it provides a more natural approach <b>for</b> <b>adjusting</b> the repeatedly measured covariates from each subject. </p...|$|R
40|$|The {{efficiency}} of final classification score as a selective criterion for improving genetically the twelve components of type descriptively classified by Holstein-Friesian Association of America was ex-amined. Comparisons were between selec-tion on final score and direct selection on each descriptive trait, and between selec-tion on final score and selection on several least squares indexes including descriptive traits. <b>Procedures</b> <b>for</b> <b>adjusting</b> phenotypic and genetic correlations be-tween categorically scored descriptive traits {{for the effects}} of discontinuity and skewness were determined from theory and were tested by computer simulated data. Final classification score was an efficient selective criterion for improving individual descriptive components of type and several linear combinations of com-ponents. Expected correlated responses in descriptive traits to selection on final score ranged from 53 to 124 % of the expected direct response. Additionally, selection on final score was expected to be 80 to 119 % as efficient as index selection for improving genetically seven linear combinations of type traits...|$|R
40|$|The current paper {{provides}} a <b>procedure</b> <b>for</b> <b>adjusting</b> the failure strain of shell elements based on both mesh size and stress triaxiality. This procedure {{is a general}} framework that gives a homogenisation of a neck over {{the entire length of}} an element and it uses an arbitrary forming limit diagram (FLD) and stress triaxiality-based failure criterion as input. In the example given in this paper, a Swift FLD is used together with the plane stress version of the modified Mohr–Coulomb failure locus. The results of this study show that simply scaling a failure locus is not correct. The proposed method produces the same mesh size correction for uniaxial tension as prior studies, but it produces different levels for other states of stress. In the limit of very large element sizes, it degenerates to the FLD...|$|E
40|$|We {{provide an}} {{effective}} and efficient implementation of a sequential quadratic programming (SQP) algorithm for the general large scale nonlinear programming problem. In this algorithm the quadratic programming subproblems are solved by an interior point method that can be prematurely halted by a trust region constraint. Numerous computational enhancements to improve the numerical performance are presented. These include a dynamic <b>procedure</b> <b>for</b> <b>adjusting</b> the merit function parameter and procedures for adjusting the trust region radius. Numerical results and comparisons are presented. Key words: nonlinear programming, interior point, SQP, merit function, trust region, large scale 1. Introduction. In a series of recent papers, [3], [6], and [8], the authors have developed a new algorithmic approach for solving large, nonlinear, constrained optimization problems. This proposed procedure is, in essence, a sequential quadratic programming (SQP) method that uses an interior point algorithm [...] ...|$|E
40|$|MacKay's Bayesian {{framework}} for backpropagation {{is a practical}} and powerful means of improving the generalisation ability of neural networks. The framework is reviewed and extended in a pedagogical way. The notation is simplified using the ordinary weight decay parameter, and the noise parameter fi is shown {{to be nothing more}} than an overall scale. A detailed and explicit <b>procedure</b> <b>for</b> <b>adjusting</b> several weight decay parameters is given. Pruning is incorporated into the Bayesian framework. Appropriate symmetry factors on sparse architectures are deduced. Bayesian weight decay is demonstrated using artificial data generated by a sparsely connected network. Pruning yields computational advantages: by removing unimportant weights the posterior weight distribution becomes Gaussian, and pruning removes zero-modes of the Hessian and redundant hidden units. In addition, pruning improves generalisation. The Bayesian evidence is used as a stop criterion for pruning. Bayesian backprop is applied [...] ...|$|E
40|$|In {{countries}} {{lacking a}} comprehensive system of death registration, {{data on the}} survival of close relatives are routinely collected for estimating risks of dying in the adult population. One {{of the most widely}} used approach, the orphanhood method, consists in converting proportions of parents alive into probabilities of surviving through standard demographic models. Since the 1980 s, HIV epidemics have introduced severe breaches in the assumptions underpinning this method. In this paper, a set of populations with substantial mortality from AIDS are generated with microsimulations. The extent of the biases introduced by HIV/AIDS in the mortality estimates conventionally obtained from reports on parental survival is evaluated. <b>Procedures</b> <b>for</b> <b>adjusting</b> theses biases are discussed. New coefficients to estimate adult mortality from data on orphanhood for the period prior to the scale-up of antiretroviral therapy are developed. The orphanhood technique can be used in settings heavily affected by HIV/AIDS. Countries lacking a comprehensive vital registration system should continue to ask survey and census respondents about the survival of their parents...|$|R
40|$|Studies using open–ended {{response}} modes {{to elicit}} probabilistic beliefs have sometimes found an elevated frequency (or blip) at 50 in their response distributions. Our previous research(1 - 3) {{suggests that this}} is caused by intrusion of the phrase “fifty–fifty,” which represents epistemic uncertainty, rather than a true numeric probability of 50 %. Such inappropriate responses pose a problem for decision analysts and others relying on probabilistic judgments. Using an explicit numeric probability scale (ranging from 0 – 100 %) reduces thinking about uncertain events in verbal terms like “fifty–fifty,” and, with it, exaggerated use of the 50 response. (1, 2) Here, we present two <b>procedures</b> <b>for</b> <b>adjusting</b> response distributions <b>for</b> data already collected with open–ended response modes and hence vulnerable to an exaggerated presence of 50 %. Each procedure infers the prevalence of 50 s had a numeric probability scale been used, then redistributes the excess. The two procedures are validated on some of our own existing data and then applied to judgments elicited from experts in groundwater pollution and bioremediation...|$|R
40|$|We {{consider}} {{the problem of}} adjusting a machine that manufactures parts in batches or lots and experiences random offsets or shifts whenever a set-up operation takes place between lots. The existing <b>procedures</b> <b>for</b> <b>adjusting</b> set-up errors in a production process over a set of lots {{are based on the}} assumption of known process parameters. In practice, these parameters are usually unknown, especially in short-run production. Due to this lack of knowledge, adjustment procedures such as Grubbs' (1954, 1983) rules and discrete integral controllers (also called EWMA controllers) aimed at <b>adjusting</b> <b>for</b> the initial offset in each single lot, are typically used. This paper presents an approach <b>for</b> <b>adjusting</b> the initial machine offset over a set of lots when the process parameters are unknown and are iteratively estimated using Markov Chain Monte Carlo (MCMC). As each observation becomes available, a Gibbs Sampler is run to estimate the parameters of a hierarchical normal means model given the observations up to that point in time. The current lot mean estimate is then used for adjustment. If used over a series of lots, the proposed method allows one eventually to start adjusting the offset before producing the first part in each lot. The method is illustrated with application to two examples reported in the literature. It is shown how the proposed MCMC adjusting procedure can outperform existing rules based on a quadratic off-target criterion. Process Adjustment, Gibbs Sampling, Bayesian Hierarchical Models, Random Effects Model, Normal Means Model, Process Control,...|$|R
40|$|In many {{biomedical}} research contexts, treatment effects are estimated from studies based on subjects {{who have been}} recruited because of high (low) measurements of a response variable, e. g., high blood pressure or low scores on a stress test. In this situation, simple change scores will overestimate the treatment effect; {{and the use of}} the paired t-test may find significant change due not to the treatment per se but, rather, due to regression towards the mean. A PC program implementing a <b>procedure</b> <b>for</b> <b>adjusting</b> the observed change for the regression effect in simple pre-test-post-test experiments is described, illustrated, and made available to interested readers. The method is due to Mee and Chua (Am Stat, 45 (1991) 39 - 42), and may be considered as an alternative to the paired t-test which separates the effect of the treatment from the so-called regression effect...|$|E
40|$|International audienceThe Shape Boltzmann Machine (SBM) [6] and its multilabel version MSBM [5] {{have been}} {{recently}} introduced as deep generative models that capture the variations {{of an object}} shape. While being more flexible MSBM requires datasets with labeled parts of the objects for training. In the paper we present an algorithm for training MSBM using binary masks of objects and the seeds which approximately correspond to the locations of objects parts. The latter {{can be obtained from}} part-based detectors in an unsupervised manner. We derive a latent variable model and an EM-like training <b>procedure</b> <b>for</b> <b>adjusting</b> the weights of MSBM using a deep learning framework. We show that the model trained by our method outperforms SBM in the tasks related to binary shapes and is very close to the original MSBM in terms of quality of multilabel shapes...|$|E
40|$|We {{provide an}} {{effective}} and efficient implementation of a sequential quadratic programming (SQP) algorithm for the general large scale nonlinear programming problem. In this algorithm the quadratic programming subproblems are solved by an interior point method that can be prematurely halted by a trust region constraint. The resulting solution is {{shown to be a}} descent direction for a merit function that we have developed. A dynamic <b>procedure</b> <b>for</b> <b>adjusting</b> the merit function parameter is described, as well as procedures for adjusting the trust region radius. Numerical results are presented. Key words: nonlinear programming, interior point, SQP, merit function, trust region, large scale Contribution of the National Institute of Standards and Technology and not subject to copyright in the United States. y Applied and Computational Mathematics Division, National Institute of Standards and Technology, Gaithersburg, MD 20899 z Mathematics Department, University of North Carolina, Chapel [...] ...|$|E
40|$|The global {{convergence}} {{properties of}} a class of penalty methods for nonlinear pro-gramming are analyzed. These methods include successive linear programming ap-proaches, and more specically, the successive linear-quadratic programming approach presented by Byrd, Gould, Nocedal and Waltz (Math. Programming 100 (1) : 27 { 48, 2004). Every iteration requires the solution of two trust-region subproblems involving piecewise linear and quadratic models, respectively. It is shown that, for a xed penalty parameter, the sequence of iterates approaches stationarity of the penalty function. A <b>procedure</b> <b>for</b> dynamically <b>adjusting</b> the penalty parameter is described, and global convergence results for it are established...|$|R
40|$|In {{cellular}} mobile communications, to optimize spectrum efficiency {{of a cell}} net, carrier-to-interference-ratio (C/I) balancing by centralized power control (CPC) would be the optimum power control technique. CPC might be difficult to implement due to its heavy required signaling load. However, the study of CPC gives enlightening insight into the mechanism relevant to power control in mobile radio systems. CPC requires a large dynamic range of the transmission powers at the base stations (BSs), say {{of the order of}} 100 dB and sometimes more. This is practically not feasible, and hence there is a requirement for adequate power control techniques which work satisfactorily under the limited power constraint. The dynamic range of power typical for state-of-the-art transmitter amplifiers is very limited, for e. g., to a maximum of 20 dB as quoted by manufacturers and operators. In this paper, novel <b>procedures</b> <b>for</b> <b>adjusting</b> the transmitter powers under the consideration of limited dynamic range are envisaged to provide maximum possible C/I ratio at the receivers. Obviously, the C/I performance would degrade by a certain amount due to the limitation of the transmit power. The target system envisaged is the time divided-code division multiple access (TD-CDMA) system utilizing joint detection (JD). Monte Carlo simulations have been performed to evaluate the performance...|$|R
40|$|The Very Large Array (VLA) radio {{telescope}} is being instrumented at 8. 4 GHz to receive telemetry from Voyager 2 during its encounter with Neptune in 1989. The procedure {{in which the}} 27 antennas have their phases adjusted in near real time so that the signals from the individual elements of the array can be added coherently is examined. Calculations of the expected {{signal to noise ratio}}, tests of the autophasing process at the VLA, and off-line simulations of that process are all presented. Various possible <b>procedures</b> <b>for</b> <b>adjusting</b> the phases are considered. It is shown that the signal to noise ratio at the VLA is adequate for summing the signals from the individual antennas with less than 0. 1 dB of loss caused by imperfect coherence among the antennas. Tropospheric variations during the summer of 1989 could cause enough loss of coherence to make the losses higher than 0. 1 dB. Experiments show that the losses caused by the troposphere can probably be kept below 0. 2 dB if the time delay inherent in the phase adjustment process is no longer than approx. 5 secs. This relatively small combining loss meets the goal estabished to minimize the bit error rate in the Voyager telemetry and implies adequate autophasing of the VLA...|$|R
