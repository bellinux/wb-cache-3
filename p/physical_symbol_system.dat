50|964|Public
5|$|These {{philosophers}} {{had begun}} to articulate the <b>physical</b> <b>symbol</b> <b>system</b> hypothesis that would become the guiding faith of AI research.|$|E
25|$|The Chinese room (and {{all modern}} computers) {{manipulate}} physical objects {{in order to}} carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a <b>physical</b> <b>symbol</b> <b>system.</b> It is also equivalent to the formal systems used {{in the field of}} mathematical logic.|$|E
500|$|In a 1990 paper, [...] "Elephants Don't Play Chess," [...] {{robotics}} researcher Rodney Brooks took direct {{aim at the}} <b>physical</b> <b>symbol</b> <b>system</b> hypothesis, {{arguing that}} symbols are not always necessary since [...] "the world is its own best model. It is always exactly up to date. It always has every detail {{there is to be}} known. The trick is to sense it appropriately and often enough." [...] In the 80s and 90s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.|$|E
40|$|<b>Physical</b> <b>symbol</b> <b>systems</b> {{are needed}} for {{open-ended}} cognition. A good way to understand <b>physical</b> <b>symbol</b> <b>systems</b> is by comparison of thought to chemistry. Both have systematicity, productivity and compositionality. The {{state of the art}} in cognitive architectures for open-ended cognition is critically assessed. I conclude that a cognitive architecture that evolves symbol structures in the brain is a promising candidate to explain open-ended cognition. Part 2 of the paper presents such a cognitive architecture. Comment: Darwinian Neurodynamics. Submitted as a two part paper to Living Machines 2013 Natural History Museum, Londo...|$|R
40|$|Cognitive {{scientists}} have historically used {{a diverse group}} of approaches to modeling of cognitive processes, including <b>physical</b> <b>symbol</b> <b>systems,</b> behaviorism, connectionism, embodiment, dynamic systems, etc. Comparing such different models seems difficult, in part, due to the different levels of description used. For instance, models describing generalized behaviors without describing internal states of an organism wor...|$|R
50|$|The {{field of}} {{symbolic}} AI {{is based on}} the <b>physical</b> <b>symbol</b> <b>systems</b> hypothesis by Simon and Newell, which states that expressing aspects of cognitive intelligence can be achieved through the manipulation of symbols. However, John McCarthy focused more on the initial purpose of artificial intelligence, which is to breakdown the essence of logical and abstract reasoning {{regardless of whether or not}} human employs the same mechanism.|$|R
2500|$|Newell and Simon had conjectured that a <b>physical</b> <b>symbol</b> <b>system</b> (such as {{a digital}} computer) {{had all the}} {{necessary}} machinery for [...] "general intelligent action", or, {{as it is known}} today, artificial general intelligence. They framed this as a philosophical position, the <b>physical</b> <b>symbol</b> <b>system</b> hypothesis: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the necessary and sufficient means for general intelligent action." [...] The Chinese room argument does not refute this, because it is framed in terms of [...] "intelligent action", i.e. the external behavior of the machine, rather than {{the presence or absence of}} understanding, consciousness and mind.|$|E
2500|$|Newell and Simon's <b>physical</b> <b>symbol</b> <b>system</b> hypothesis: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the {{necessary}} and sufficient means of general intelligent action." [...] Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and {{on having a}} [...] "feel" [...] for the situation rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.) ...|$|E
2500|$|When digital {{computers}} became {{widely used}} in the early 50s, this argument was extended {{to suggest that the}} brain was a vast <b>physical</b> <b>symbol</b> <b>system,</b> manipulating the binary symbols of zero and one. Dreyfus was able to refute the biological assumption by citing research in neurology that suggested that the action and timing of neuron firing had analog components. To be fair, however, Daniel Crevier observes that [...] "few still held that belief in the early 1970s, and nobody argued against Dreyfus" [...] about the biological assumption.|$|E
50|$|Newell and Simon {{formed a}} lasting partnership. They founded an {{artificial}} intelligence laboratory at Carnegie Mellon University {{and produced a}} series of important programs and theoretical insights throughout the late fifties and sixties. This work included the General Problem Solver, a highly influential implementation of means-ends analysis, and the <b>physical</b> <b>symbol</b> <b>systems</b> hypothesis, the controversial philosophical assertion that all intelligent behavior could be reduced {{to the kind of}} symbol manipulation that Newell's programs demonstrated.|$|R
5000|$|Pamela McCorduck {{also sees}} in the Logic Theorist the debut of a new theory of the mind, the {{information}} processing model (sometimes called computationalism). She writes that [...] "this view would come to be central to their later work, and in their opinion, as central to understanding mind {{in the twentieth century}} as Darwin's principle of natural selection had been to understanding biology in the nineteenth century." [...] Newell and Simon would later formalize this proposal as the <b>physical</b> <b>symbol</b> <b>systems</b> hypothesis.|$|R
40|$|One of the {{interesting}} questions in cognitive science seems to be whether human beings are already programmed from birth or can be programmed after birth to do certain things. For example, can there be algorithms residing in the human brain that can carry out tasks, e. g. planning? The traditional approaches of artificial intelligence suggest the use of <b>physical</b> <b>symbol</b> <b>systems</b> for representing domain knowledge (Newell & Simon, 1976; Anderson, 2005). The reasoning is carried out by rules and algorithms that manipulate symbols. To summarize, traditional AI has two important aspects which are 1) <b>physical</b> <b>symbols</b> and 2) rules and algorithms that operate on these <b>symbols.</b> However, the <b>physical</b> <b>symbols</b> aspect of traditional AI has received criticisms because of the “symbol grounding” problem (Harnard, 1990). Barsalou proposes perceptua...|$|R
5000|$|Newell and Simon's <b>physical</b> <b>symbol</b> <b>system</b> hypothesis: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the {{necessary}} and sufficient means of general intelligent action." ...|$|E
5000|$|Newell and Simon had conjectured that a <b>physical</b> <b>symbol</b> <b>system</b> (such as {{a digital}} computer) {{had all the}} {{necessary}} machinery for [...] "general intelligent action", or, {{as it is known}} today, artificial general intelligence. They framed this as a philosophical position, the <b>physical</b> <b>symbol</b> <b>system</b> hypothesis: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the necessary and sufficient means for general intelligent action." [...] The Chinese room argument does not refute this, because it is framed in terms of [...] "intelligent action", i.e. the external behavior of the machine, rather than {{the presence or absence of}} understanding, consciousness and mind.|$|E
5000|$|... #Subtitle level 2: Arguments {{in favor}} of the <b>physical</b> <b>symbol</b> <b>system</b> {{hypothesis}} ...|$|E
2500|$|These {{predictions}} {{were based}} on the success of an [...] "information processing" [...] model of the mind, articulated by Newell and Simon in their <b>physical</b> <b>symbol</b> <b>systems</b> hypothesis, and later expanded into a philosophical position known as computationalism by philosophers such as Jerry Fodor and Hilary Putnam. Believing that they had successfully simulated the essential process of human thought with simple programs, it seemed a short step to producing fully intelligent machines. However, Dreyfus argued that philosophy, especially 20th-century philosophy, had discovered serious problems with this information processing viewpoint. The mind, according to modern philosophy, is nothing like a computer.|$|R
40|$|This survey {{presents}} {{an overview of}} the autonomous development of mental capabilities in computational agents. It does so based on a characterization of cognitive systems as systems which exhibit adaptive, anticipatory, and purposive goaldirected behaviour. We present a broad survey of the various paradigms of cognition, addressing cognitivist (<b>physical</b> <b>symbol</b> <b>systems)</b> approaches, emergent systems approaches, encompassing connectionist, dynamical, and enactive systems, and also efforts to combine the two in hybrid systems. We then review several cognitive architectures drawn from these paradigms. In each of these areas, we highlight the implications and attendant problems of adopting a developmental approach, both from phylogenetic and ontogenetic points of view. We conclude with a summary of the key architectural features that systems capable of autonomous development of mental capabilities should exhibit...|$|R
40|$|Abstract. Artificial Intelligence {{was born}} in 1956 as the off-spring of the newly-created cognitivist {{paradigm}} of cognition. As such, it inherited a strong philosophical legacy of functionalism, dualism, and positivism. This legacy found its strongest statement some 20 years later in the <b>physical</b> <b>symbol</b> <b>systems</b> hypothesis, a conjecture that deeply influenced the evolution of AI in subsequent years. Recent history has seen a swing away from the functionalism of classical AI toward an alternative position that re-asserts the primacy of embodiment, development, interaction, and, more recently, emotion in cognitive systems, focussing {{now more than ever}} on enactive models of cognition. Arguably, this swing represents a true paradigm shift in our thinking. However, the philosophical foundations of these approaches — phenomenology — entail some far-reaching ontological and epistemological commitments regarding the nature of a cognitive system, its reality, and the role of its interaction with its environment. The goal {{of this paper is to}} draw out the full philosophica...|$|R
5000|$|The <b>physical</b> <b>symbol</b> <b>system</b> {{hypothesis}} {{claims that}} both of these are also examples of physical symbol systems: ...|$|E
5000|$|Newell and Simon's <b>physical</b> <b>symbol</b> <b>system</b> hypothesis: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the {{necessary}} and sufficient means of general intelligent action." [...] Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and {{on having a}} [...] "feel" [...] for the situation rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.) ...|$|E
5000|$|Nils Nilsson has {{identified}} four main [...] "themes" [...] or grounds {{in which the}} <b>physical</b> <b>symbol</b> <b>system</b> hypothesis has been attacked.|$|E
40|$|This paper {{explores the}} {{hypothesis}} that schematic abstraction—rule following—is partially implemented through processes and knowledge used to understand motion. Two experiments explore the mechanisms used by reasoners solving simple linear equations with one variable. Participants solved problems displayed against a background that moved rightward or leftward. Solving was facilitated when the background motion moved {{in the direction of}} the numeric transposition required to solve for the unknown variable. Previous theorizing has usually assumed that such formal problems are solved through the repeated application of abstract transformation patterns (rules) to equations, replicating the steps produced in typical worked solutions. However, the current results suggest that in addition to such strategies, advanced reasoners often employ a mental motion strategy when manipulating algebraic forms: elements of the problem are “picked up ” and “moved ” across the equation line. This demonstration supports the suggestion that genuinely schematic reasoning could be implemented in perceptual-motor systems through the simulated transformation of referential (but <b>physical)</b> <b>symbol</b> <b>systems...</b>|$|R
40|$|This paper {{deals with}} the question: what are the key {{requirements}} for a physical system to perform digital computation? Oftentimes, cognitive scientists are quick to employ the notion of computation simpliciter when asserting basically that cognitive activities are computational. They employ this notion {{as if there is}} a consensus on just what it takes for a physical system to compute. Some cognitive scientists in referring to digital computation simply adhere to Turing computability. But if cognition is indeed computational, then it is concrete computation that is required for explaining cognition as an embodied phenomenon. Three accounts of computation are examined here: 1. Formal <b>Symbol</b> Manipulation. 2. <b>Physical</b> <b>Symbol</b> <b>Systems</b> and 3. The Mechanistic account. I argue that the differing requirements implied by these accounts justify the demand that one commits to a particular account when employing the notion of digital computation in regard to physical systems, rather than use these accounts interchangeably...|$|R
40|$|The basic entity {{model was}} {{constructed}} {{to provide information}} processing with a better theoretical foundation. The human information processing systems are perceived as <b>physical</b> <b>symbol</b> <b>systems.</b> The artifacts or basic entities that these systems handle and that are unique to information (processing) theory have been observed. The four basic entities are data, information, knowledge and wisdom. The three postulates that are fundamental to the model are the law of boundary, the law of interaction, and the law of constructed (information) systems. The transformations of the basic entities {{taking place in the}} model create an information space that contains a set of information states in a particular knowledge domain. The information space then serves as the platform for decision making to be executed. The model is also used to analyze the structure of constructed information systems mathematically. The ontological, deep structure approach is adopted. A theoretical foundation of this nature is beneficial to unify all information-related disciplines...|$|R
5000|$|Hubert Dreyfus {{attacked}} the necessary {{condition of the}} <b>physical</b> <b>symbol</b> <b>system</b> hypothesis, calling it [...] "the psychological assumption" [...] and defining it thus: ...|$|E
5000|$|The <b>physical</b> <b>symbol</b> <b>system</b> {{hypothesis}} (PSSH) is {{a position}} in the philosophy of artificial intelligence formulated by Allen Newell and Herbert A. Simon. They wrote: [...] "A <b>physical</b> <b>symbol</b> <b>system</b> has the necessary and sufficient means for general intelligent action." [...] Allen Newell and Herbert A. Simon This claim implies both that human thinking is a kind of symbol manipulation (because a symbol system is necessary for intelligence) and that machines can be intelligent (because a symbol system is sufficient for intelligence).|$|E
50|$|A <b>{{physical}}</b> <b>symbol</b> <b>system</b> (also {{called a}} formal system) takes physical patterns (symbols), combining them into structures (expressions) and manipulating them (using processes) to produce new expressions.|$|E
40|$|The {{original}} publication {{is available}} at: www. springerlink. com???. Copyright Springer [Full text {{of this article}} is not available in the UHRA]While computers can be used to model human competencies, formalization has its limits. Sensori-motor dynamics are probably necessary to intelligence. Applied to language, verbal patterns become constraints or, in Elman???s (2004) terms, cues to meaning. Unlike symbol processors, humans act, mean and use the feeling of thinking (Harnad 2005). While language has an artificial (or formal) aspect, human intelligence is embodied. In spite of widespread belief to the contrary, brains do not need to generate sets of sentences. In challenging code views of language, we find parallels with the complex systems we call cells. Given DNA code-makers, formal features constrain protein synthesis. Life, Barbieri (2007) argues, can be traced to natural artifacts. 1 This parallels how culture enables us to bring biodynamics under the control of physical and non-physical (or cultural) patterns. In turning from <b>physical</b> <b>symbol</b> <b>systems,</b> weight falls on DEEDS: human thinking is Dynamical, Embodied, Embedded, Distributed and Situated (Walmsley 2008) ...|$|R
40|$|Cognitive science {{uses the}} notion of {{computational}} information processing to explain cognitive information processing. Some philosophers have argued that anything {{can be described as}} doing computational information processing, and if so, it is a vacuous notion for explanatory purposes. An attempt is made to explicate the notions of cognitive information processing and computational information processing and to specify the relationship between them. It is demonstrated that the resulting notion of computational information processing can only be realized in a restrictive class of dynamical systems called physical notational systems (after Goodman's theory of notationality), and that the systems generally appealed to by cognitive science [...] <b>physical</b> <b>symbol</b> <b>systems</b> [...] are indeed such systems. Furthermore, it turns out that other alternative conceptions of computational information processing, Fodor's (1975) Language of Thought and Cummins' (1989) Interpretational Semantics appeal to substantially the same restrictive class of systems. The necessary connection of computational information processing with notationality saves the enterprise from charges of vacuousness and has some interesting implications for connectionism. But unfortunately, it distorts the subject matter and entails some troubling consequences for a cognitive science which tries to make notationality do the work of genuine mental representations...|$|R
40|$|When certain formal <b>symbol</b> <b>systems</b> (e. g., {{computer}} programs) {{are implemented}} as dynamic <b>physical</b> <b>symbol</b> <b>systems</b> (e. g., {{when they are}} run on a computer) their activity can be interpreted at higher levels (e. g., binary code {{can be interpreted as}} LISP, LISP code can be interpreted as English, and English can be interpreted as a meaningful conversation). These higher levels of interpretability are called "virtual" systems. If such a virtual system is interpretable as if it had a mind, is such a "virtual mind" real? This is the question addressed in this "virtual" symposium, originally conducted electronically among four cognitive scientists: Donald Perlis, a computer scientist, argues that according to the computationalist thesis, virtual minds are real and hence Searle's Chinese Room Argument fails, because if Searle memorized and executed a program that could pass the Turing Test in Chinese he would have a second, virtual, Chinese-understanding mind of which he was unaware (as in multiple personality). Stevan Harnad, a psychologist, argues that Searle's Argument is valid, virtual minds are just hermeneutic overinterpretations, and symbols must be grounded in the real world of objects, not just the virtual world of interpretations. Computer scientist Patrick Hayes argues that Searle's Argument fails, but because Searle does not really implement the program: A real implementation must not be homuncular but mindless and mechanical, like a computer. Only then can it give rise to a mind at the virtual level. Philosopher Ned Block suggests that there is no reason a mindful implementation would not be a real one...|$|R
50|$|The Chinese room (and {{all modern}} computers) {{manipulate}} physical objects {{in order to}} carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a <b>physical</b> <b>symbol</b> <b>system.</b> It is also equivalent to the formal systems used {{in the field of}} mathematical logic.|$|E
5000|$|John Searle's Chinese room argument, {{presented}} in 1980, attempted {{to show that}} a program (or any <b>physical</b> <b>symbol</b> <b>system)</b> could not be said to [...] "understand" [...] the symbols that it uses; that the symbols themselves have no meaning or semantic content, and so the machine can never be truly intelligent from symbol manipulation alone.|$|E
5000|$|In other words, our {{intelligence}} derives from {{a form of}} calculation, similar to arithmetic. This is the <b>physical</b> <b>symbol</b> <b>system</b> hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI ("Can a machine have mind, mental states and consciousness?"), most versions of computationalism claim that (as Stevan Harnad characterizes it): ...|$|E
40|$|The {{constantly}} changing and tenuous nature of organizations is complicating {{the relationships between}} individuals and organizations. We argue that <b>physical</b> <b>symbols</b> offer individuals and organizations access to a rich, non-verbal “language ” that can help clarify this relational complexity. We advance <b>physical</b> <b>symbols</b> to be a mechanism for managing two key aspects of relationships – identity and status, and offer suggestions for better understanding, and even enhancing {{the use of these}} symbols in modern management. Treating <b>physical</b> <b>symbols</b> as a language helps us comprehend their versatility and their ability to represent and maintain relationships. It also advances the argument that some changes in relationships are often realized via <b>physical</b> <b>symbols.</b> These two less discussed aspects of symbols are the premise of our chapter. We examine the language offered by <b>physical</b> <b>symbols,</b> in light of what is currently known about verbal language, and highlight the relational messages that can be communicated by this language. We suggest that the language of <b>physical</b> <b>symbols</b> can and should be analyzed because it is prevalent, powerful, and can easily be misinterpreted. We also offer suggestions for avoiding or correcting misinterpretations. PHYSICAL SYMBOLS IN ORGANIZATIONS: AN OVERVIE...|$|R
5000|$|... "The {{family living}} in an ancestral home is {{surrounded}} by visible, <b>physical</b> <b>symbols</b> of family continuity and solidarity".|$|R
5000|$|Tangible <b>symbol</b> <b>system</b> {{offers a}} manual and DVD {{as well as}} an online course. For more {{information}} on tangible <b>symbol</b> <b>system</b> instructional strategies, please reference: http://designtolearn.com/products/tangible_symbol_systems ...|$|R
