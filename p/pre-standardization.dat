17|0|Public
50|$|Seatalk2 was a <b>pre-standardization</b> {{version of}} NMEA 2000 using square connectors.|$|E
5000|$|... {{books from}} the <b>pre-standardization</b> period of Croatian that are adapted to {{nowadays}} standard Croatian ...|$|E
5000|$|... #Caption: <b>Pre-standardization</b> British School Zone with metal-cutout generic Warning symbol {{embellished with}} red glass reflector-spheres.|$|E
40|$|Standards {{have become}} one of the most {{important}} elements in technological development in ICT. However, standard-setting process is a complex coordination between different players. One of the strategies is the inter-firm alliance during the <b>pre-standardization</b> stage. Yet, it remains unclear how the inter-firm alliances occur during the process. Using the case study of the current developing technology in ICT industry and interviews with people who are familiar with standardization work, this paper points out two types of partnerships in the <b>pre-standardization</b> with the analysis of the firms ’ motivation of having the partnerships. Besides to promote the technology as the standard, two other motives of having partnerships are also discovered...|$|E
40|$|This paper uncovers the“mysterious veil”above the {{formulations}} {{and concerned}} properties of existing weighted additive {{data envelopment analysis}} (WADD) models associated with dataset standardization techniques. Based on the truth that the formulation of objective functions in WADD models seems random and confused for users, the study investigates the correspondence relationship between the formulation of objective functions by statistical data-based weights aggregating slacks in WADD models and the <b>pre-standardization</b> of original datasets before using the traditional ADD model in terms of satisfying unit and translation invariance. Our work presents a statistical background for WADD models’ formulations, and makes them become more interpretive and more convenient to be computed and practically applied. Based on the <b>pre-standardization</b> techniques, two new WADD models satisfying unit invariance are formulated to enrich the family of WADD models. We compare all WADD models in some concerned properties, and give {{special attention to the}} (in) efficiency discrimination power of them. Moreover, some suggestions guiding theoretical and practical applications of WADD models are discussed. ...|$|E
40|$|NISO White Papers are contributed or {{solicited}} {{papers that}} address {{an issue that}} has implications for standards development. White Papers {{can be viewed as a}} <b>pre-standardization</b> activity. A NISO White Paper might define and explore some of the questions that come into play before formal standardization work is started. Or, a NISO White Paper might identify areas that are opportunities for standards development and suggest possible approaches. All White Papers are posted on the NISO website (www. niso. org) ...|$|E
40|$|Thoracoscopic esophagectomy in the prone {{position}} (TEPP) might enable solo-surgery in cases requiring resection of the esophagus {{and the surrounding}} lymph nodes due to the associated advantages of good exposure of the surgical field and ergonomic considerations for the surgeon. However, no one approach can be for all patients requiring extensive lymphadenectomy. We recently developed an assistant-based procedure to standardize exposure of the surgical field. Patients were divided into 1 of 2 groups:a <b>pre-standardization</b> group (n＝ 37) and a post-standardization group (n＝ 28). The thoracoscopic operative time was significantly shorter (p＝ 0. 0037) in the post-standardization group (n＝ 28; 267 ± 31 min) than in the <b>pre-standardization</b> group (n＝ 37; 301 ± 53 min). Further, learning curve analysis using the moving average method showed stabilization of the thoracoscopic operative time after the standardization. No {{significant differences were found}} in the number of mediastinal lymph nodes dissected or intraoperative blood loss between the 2 groups. There were also no significant differences in the complication rate. Assistant-based surgery and standardization of the procedure resulted in a well-exposed and safe surgical field. TEPP decreased the operative time, even in patients requiring extensive lymphadenectomy...|$|E
40|$|Standards {{process is}} seen as an {{important}} determinant of innovation within the ICT sector. However, not many studies have focused on the mechanism at work within the standards- making process. Therefore, to find out how the standards selection process work, this paper tries to describe the negotiations occur between different players during the standards setting process, which influence the outcome of the process itself. The analysis primarily focuses on the <b>pre-standardization</b> stage. The negotiations are classified into three main phases with different activities at each phase...|$|E
40|$|The {{demonstration}} of immunological differences between poliovirus strains {{of any one}} type is a valuable procedure in epidemiological research as it may allow a virus strain {{to be identified as}} derived from or unrelated to a given possible source of infection. It is obviously of particular importance in connexion with live poliovirus vaccination campaigns. Both kinetic tests and conventional neutralization and complement-fixation techniques have been used to this end, the former involving a more complicated test procedure and the latter demanding greater nicety in the <b>pre-standardization</b> of reagents. The present paper reports on attempts to establish a simplified technique...|$|E
40|$|Abstract. Standardization {{activities}} {{are recognized as}} one of the tools to incubate research results and accelerate their transfer to innovative marketable products and services. However, the European Commission (EC) research community and its associated stakeholders acknowledge the lack of research transfer via the standardization channel, generally referred to as the research-to-standardization gap. This chapter analyzes the root causes for this gap and proposes way forward. In particular research-focused standardization is considered as the instrument to address this issue. This chapter shows that <b>pre-standardization</b> should be supplemented by a methodology and its associated process aiming to systematically analyze the standardization aspects of research projects and by helping them out to draw their standardization strategy. ...|$|E
40|$|This {{paper is}} part of the series of <b>pre-standardization</b> {{research}} aimed to analyze the existing methods of calculating the Buildings Energy Performance (PEC) in view of their correction of completing. The entire research activity aims to experi-mentally validate the PEC Calculation Algorithm as well as the comparative application, on the support of several case studies focused on representative buildings of the stock of buildings in Romania, of the PEC calculation methodology for buildings equipped with occupied spaces heating systems. The targets of the report are the experimental testing of the calculation models so far known (NP 048 - 2000, Mc 001 - 2006, SR EN 13790 : 2009), on the support provided by the CE INCERC Bucharest experimental building, together with the complex calculation algorithms specific to the dynamic modeling, for th...|$|E
40|$|Abstract—The METIS-II {{project will}} be an EU-funded project within the 5 G Infrastructure Public Private Partnership (5 G-PPP) that is {{dedicated}} towards an overall 5 G radio access network (RAN) design. It will build strongly upon the METIS project, which has {{laid the foundation for}} 5 G by identifying key scenarios and requirements and determining the most promising technology components and key system design characteristics. However, METIS-II will go significantly further than previous projects by developing a comprehensive and detailed 5 G RAN design according to “technology readiness level 2 ”. This is seen as an especially important step to ensure a timely and efficient standardization of 5 G, likely to start in 2016. To achieve this, in addition to the technical work on designing the comprehensive control and user plane of an overall 5 G RAN, the METIS-II project has the ambition to actively foster the collaboration within 5 G-PPP, for instance by conducting workshops in particular with other projects within the wireless strand, and obtain an early <b>pre-standardization</b> consensus on fundamental design aspects related to the 5 G RAN. In this paper, a brief overview on the project is provided, covering the objectives, the consortium composition, project structure and key innovations, as well as the timing and expected impact of the project...|$|E
40|$|Received on; revised on; {{accepted}} on Motivation: DNA {{copy number}} variants (CNV) are {{gains and losses}} of segments of chromosomes, and comprise an important class of genetic variation. Recently, various microarray hybridization-based techniques {{have been developed for}} high throughput measurement of DNA copy number. In many studies, multiple technical platforms or different versions of the same platform were used to interrogate the same samples; and it became necessary to pool information across these multiple sources to derive a consensus molecular profile for each sample. An integrated analysis is expected to maximize resolution and accuracy, yet currently there is no well formulated statistical method to address the between-platform differences in probe coverage, assay methods, sensitivity, and analytical complexity. Results: The conventional approach is to apply one of the CNV detection (a. k. a. “segmentation”) algorithms to search for DNA segments of altered signal intensity. The results from multiple platforms are combined after segmentation. Here we propose a new method, Multi-Platform Circular Binary Segmentation (MPCBS), which pools statistical evidence across platforms during segmentation, and does not require <b>pre-standardization</b> of different data sources. It involves a weighted sum of t-statistics, which arises naturally from the generalized log-likelihood ratio of a multi-platform model. We show by comparing the integrated analysis of Affymetrix and Illumina SNP array data with Agilent and fosmid clone end-sequencing results on 8 HapMap samples that MPCBS achieves improved spatial resolution, detection power, and provides a natural consensus across platforms. We also apply the new method to analyze multi-platform data for tumor samples. Availability: The R package for MPCBS is registered on R-Forg...|$|E
40|$|DNA {{copy number}} {{variants}} (CNV) are {{gains and losses}} of segments of chromosomes, and comprise an important class of genetic variation. Recently, various microarray hybridization based techniques {{have been developed for}} high throughput measurement of DNA copy number. In many studies, multiple technical platforms or different versions of the same platform were used to interrogate the same samples; and it became necessary to pool information across these multiple sources to derive a consensus molecular profile for each sample. An integrated analysis is ex-pected to maximize resolution and accuracy, yet currently there is no well formulated statistical method to address the between-platform differences in probe design, assay methods, sensitivity, and analytical complexity. The conventional approach is to apply one of the CNV detection (a. k. a. “segmentation”) algorithms to search for DNA segments of altered signal intensity. The results from three platforms are combined after segmen-tation. Here we propose a new method, Multi-Platform Circular Binary Segmentation (MPCBS), which pools statistical evidence across platforms during segmentation, and does not require <b>pre-standardization</b> of differ-ent data sources. It involves a weighted sum of t-statistics, which arises naturally from the generalized log-likelihood ratio of a multi-platform model. We show by comparing the integrated analysis of Affymetrix and Illumina SNP array data with fosmid clone end-sequencing results on 8 HapMap samples that MPCBS achieves improved spatial resolution, de-tection power, and provide a natural consensus across platforms. We also apply the new method to analyze the multi-platform data from TCGA. The R package for MPCBS is registered on R-Forge under project nam...|$|E
40|$|This paper {{presents}} a design exploration, at both system and circuit levels, of integrated transceivers {{for the upcoming}} fifth generation (5 G) of wireless communications. First, a system level model for 5 G communications is carried out to derive transceiver design specifications. Being 5 G still in <b>pre-standardization</b> phase, a few currently used standards (ECMA- 387, IEEE 802. 15. 3 c, and LTE-A) {{are taken into account}} as the reference for the signal format. Following a top-down flow, this work presents the design in 65 nm CMOS SOI and bulk technologies of the key blocks of a fully integrated transceiver: low noise amplifier (LNA), power amplifier (PA) and on-chip antenna. Different circuit topologies are presented and compared allowing for different trade-offs between gain, power consumption, noise figure, output power, linearity, integration cost and link performance. The best configuration of antenna and LNA co-design results in a peak gain higher than 27 dB, a noise figure below 5 dB and a power consumption of 35 mW. A linear PA design is presented to face the high Peak to Average Power Ratio (PAPR) of multi-carrier transmissions envisaged for 5 G, featuring a 1 dB compression point output power (OP 1 dB) of 8. 2 dBm. The delivered output power in the linear region can be increased up to 13. 2 dBm by combining four basic PA blocks through a Wilkinson power combiner/divider circuit. The proposed circuits are shown to enable future 5 G connections, operating in a mm-wave spectrum range (spanning 9 GHz, from 57 GHz to 66 GHz), with a data-rate of several Gb/s in a short-range scenario, spanning from few centimeters to tens of meters...|$|E
40|$|This {{paper is}} part of the series of <b>pre-standardization</b> {{research}} aimed to analyze the existing methods of calculating the Buildings Energy Performance (PEC) in view of their correction of completing. The entire research activity aims to experimentally validate the PEC Calculation Algorithm as well as the comparative application, on the support of several case studies focused on representative buildings of the stock of buildings in Romania, of the PEC calculation methodology for buildings equipped with occupied spaces heating systems. The targets of the report are the experimental testing of the calculation models so far known (NP 048 - 2000, Mc 001 - 2006, SR EN 13790 : 2009), on the support provided by the CE INCERC Bucharest experimental building, together with the complex calculation algorithms specific to the dynamic modeling, for the evaluation of the occupied spaces heat demand in the cold season, specific to the traditional buildings and to modern buildings equipped with solar radiation passive systems, of the ventilated solar space type. The schedule of the measurements performed in the 2008 - 2009 cold season is presented as well as the primary processing of the measured data and the experimental validation of the heat demand monthly calculation methods, on the support of CE INCERC Bucharest. The calculation error per heating season (153 days of measurements) between the measured heat demand and the calculated one was of 0. 61 %, an exceptional value confirming the phenomenological nature of the INCERC method, NP 048 - 2006. The mathematical model specific to the hourly thermal balance is recurrent – decisional with alternating paces. The experimental validation of the theoretical model is based on the measurements performed on the CE INCERC Bucharest building, within a time lag of 57 days (06. 01 - 04. 03. 2009). The measurements performed on the CE INCERC Bucharest building confirm the accuracy of the hourly calculation model by comparison to the values provided by measurements and to those provided by the monthly calculation (NP 048 - 2006). The deviations of 1. 45 % and 2. 2 % respectively validate the hourly calculation model, as they actually have no physical significance. The report presents a phenomenological analysis of the building transfer functions synthesized as tev (τ), functions which attest their phenomenological objectivity in macro and hourly terms. The case studies completing the calculation models experimental validation emphasize unacceptably large differences between the results provided by the use of standardized calculation methods (Mc 001 / 2 - 2006 and SR EN 13790 : 2009) and those specific to the methods referred to in this report, experimentally validated...|$|E
40|$|Since it&# 226;&# 128;&# 153;s {{foundation}} in June 1967 the Technical Institute for Materials and Construction (INTEMAC) established the incentivation of technical {{training and the}} researching vocation of it&# 226;&# 128;&# 153;s staff as it&# 226;&# 128;&# 153;s goals. For the past eighteen years INTEMAC&# 226;&# 128;&# 153;s quarterlies have represented the shaping of this spirit, for what the Institute has become {{well known in the}} building business. Covering more and more topics every year, the sixty-six titles which have been published until now have all been dedicated to a big variety of issues such as the <b>pre-standardization,</b> the creation of guides of practical application on specific subjects, pathology or the building works of a certain importance inside the national panorama. Trying to maintain, altogether, a certain balance between edification issues, civil construction and rehabilitation of already existing buildings. The publication of INTEMAC&# 226;&# 128;&# 153;s quarterly has never stopped, with a three-month periodicity, programmed with a lot of anticipation, and a bilingual edition in Spanish and English, which is around 2500 copies nowadays. The following text tries to be a mirror of the evolution of this publication throughout time, with a few comments which will help the reader guess what we expect from the future days. Desde su fundaci&# 243;n, en Junio de 1967, el Instituto T&# 233;cnico de Materiales y Construcciones (INTEMAC) estableci&# 243; como uno de sus objetivos la incentivaci&# 243;n de la formaci&# 243;n t&# 233;cnica y de la vocaci&# 243;n investigadora de su plantilla. Durante los &# 250;ltimos dieciocho a&# 241;os los Cuadernos INTEMAC han representado la plasmaci&# 243;n de este esp&# 237;ritu, por el que el Instituto ha llegado a ser bien conocido en el mundo de la Construcci&# 243;n. Cubriendo campos cada vez m&# 225;s amplios, los 66 n&# 250;meros publicados hasta ahora se han dedicado a asuntos tan variados como la pre-normativa, la creaci&# 243;n de gu&# 237;as de aplicaci&# 243;n pr&# 225;ctica sobre temas concretos, la patolog&# 237;a o las obras de una especial importancia y actualidad dentro del panorama nacional, todo ello intentando mantener un cierto equilibrio entre los temas de edificaci&# 243;n, obra civil o de rehabilitaci&# 243;n de obras ya existentes. A partir de su aparici&# 243;n, la publicaci&# 243;n de los Cuadernos INTEMAC ha sido ininterrumpida, con una periodicidad trimestral programada con mucha anticipaci&# 243;n y una tirada biling&# 252;e en espa&# 241;ol e ingl&# 233;s que actualmente es de 2500 ejemplares. El texto que sigue intenta ser un reflejo del discurrir de esta publicaci&# 243;n a lo largo del tiempo, con unos breves comentarios que permitan adivinar al lector c&# 243;mo esperamos que sea el tiempo por venir...|$|E

