257|8917|Public
5|$|The {{new field}} was unified and {{inspired}} {{by the appearance of}} <b>Parallel</b> <b>Distributed</b> <b>Processing</b> in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.|$|E
25|$|In the mid-1980s, <b>parallel</b> <b>distributed</b> <b>processing</b> {{became popular}} {{under the name}} connectionism. Rumelhart and McClelland (1986) {{described}} the use of connectionism to simulate neural processes.|$|E
2500|$|These {{developments}} {{have led to}} the modern study of logic and computability, and indeed the field of theoretical computer science as a whole. Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and <b>parallel</b> <b>distributed</b> <b>processing</b> were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete [...] a landmark result in computational complexity theory.|$|E
5000|$|... #Article: International <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> Symposium ...|$|R
40|$|Abstract—The {{available}} literatures on {{image processing}} in agriculture application under high performance computing (HPC) are limited and sometimes are not discussed in details. This paper reviewed the steps of image analysis done in some image processing focusing on agriculture application and also the details analysis of <b>parallel</b> and <b>distributed</b> image <b>processing.</b> The memory architecture in <b>parallel</b> and <b>distributed</b> image <b>processing</b> and some suitable application programming interface (API) in <b>parallel</b> and <b>distributed</b> image <b>processing</b> are examined. In general, this study provides basic understanding of <b>parallel</b> and <b>distributed</b> image <b>processing</b> for agriculture application...|$|R
40|$|In recent years, <b>parallel</b> and <b>distributed</b> <b>processing</b> {{technologies}} {{become more}} and more required. Since <b>parallel</b> and <b>distributed</b> <b>processing</b> systems are, however, very complicated, the users require many knowledge and experience of the target systems {{in order to get the}} performance benefit. Therefore we have been developing AgentSphere as a <b>parallel</b> and <b>distributed</b> <b>processing</b> system to reduce the difficulties of the use and to get the performance benefit easily. In this paper, we propose its information sharing mechanism using distributed hash table, which is one of the P 2 P technologies, and implement the mechanism to share and retrieve the information of each mobile agents which move autonomically in AgentSphere network...|$|R
2500|$|Howard Bloom has {{discussed}} mass behavior – collective behavior from {{the level of}} quarks {{to the level of}} bacterial, plant, animal, and human societies. He stresses the biological adaptations that have turned most of this earth's living beings into components of what he calls [...] "a learning machine". In 1986 Bloom combined the concepts of apoptosis, <b>parallel</b> <b>distributed</b> <b>processing,</b> group selection, and the superorganism to produce a theory of how collective intelligence works. Later he showed how the collective intelligences of competing bacterial colonies and human societies can be explained in terms of computer-generated [...] "complex adaptive systems" [...] and the [...] "genetic algorithms", concepts pioneered by John Holland.|$|E
50|$|In the mid-1980s, <b>parallel</b> <b>distributed</b> <b>processing</b> {{became popular}} {{under the name}} connectionism. Rumelhart and McClelland (1986) {{described}} the use of connectionism to simulate neural processes.|$|E
50|$|J.J. McCarthy, A. Prince. 1993 Generalized alignment. Springer. S. Pinker, A. Prince. 1988. On {{language}} and connectionism: Analysis of a <b>parallel</b> <b>distributed</b> <b>processing</b> model of language acquisitionCognition.|$|E
50|$|Euromicro focuses upon multimedia, telecommunication, {{software}} engineeringreal-time systems, <b>parallel</b> and <b>distributed</b> <b>processing,</b> computer architecture, robotics {{and hardware}} design.|$|R
5000|$|Lovas and Sunderam. 2002: Debugging of metacomputing applications. In: Proc. of 16th {{international}} <b>parallel</b> and <b>distributed</b> <b>processing</b> symposium. IPDPS 2002.http://doi.ieeecomputersociety.org/10.1109/IPDPS.2002.1016506 ...|$|R
40|$|CARUSO - an {{approach}} towards {{a network of}} low power autonomic systems on chips for embedded real-time application / Uwe Brinkschulte, Jürgen Becker, Theo Ungerer. - In: International <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> Symposium : Proceedings / 18 th International <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> Symposium : Santa Fe, New Mexico, April 26 - 30, 2004; [abstracts and CD-ROM]. - Los Alamitos, Calif. [u. a. ] : IEEE Computer Society, 2004. - 1 CD-RO...|$|R
50|$|Parallel {{constraint}} satisfaction processes (PCSP) {{is a model}} that integrates the fastest growing research areas {{in the study of}} the mind; Connectionism, neural networks, and <b>parallel</b> <b>distributed</b> <b>processing</b> models.|$|E
50|$|Schema theory, <b>parallel</b> <b>distributed</b> <b>processing,</b> and connectionist models: Rumelhart (1980), {{working in}} {{conjunction}} with others, developed the schema theory of information processing and memory. He suggested that a schema is a data structure for representing generic concepts stored in memory.|$|E
50|$|While the {{algorithm}} of BCM is too complicated for large-scale <b>parallel</b> <b>distributed</b> <b>processing,</b> {{it has been}} put to use in lateral networks with some success. Furthermore, some existing computational network learning algorithms {{have been made to}} correspond to BCM learning.|$|E
50|$|By construction, fusion frames easily lend {{themselves}} to <b>parallel</b> or <b>distributed</b> <b>processing</b> of sensor networks consisting of arbitrary overlapping sensor fields.|$|R
5000|$|<b>Parallel</b> and <b>distributed</b> query <b>processing</b> is {{available}} (including JOINs).|$|R
40|$|Cluster based {{architectures}} {{are standing}} out {{in the last years}} as an alternative for the construction of versatile, low cost parallel machines. This versatility permits their use as much as a teaching tool or as a research environment in the field of <b>parallel</b> and <b>distributed</b> <b>processing.</b> This paper describes some of the possibilities found today on the market for the construction of cluster based parallel machines and proposes different configurations based on cost and application areas. Key Words — Computer architectures, <b>parallel</b> and <b>distributed</b> <b>processing,</b> cluster based <b>parallel</b> machines...|$|R
5000|$|The {{prevailing}} connectionist approach {{today was}} originally known as <b>parallel</b> <b>distributed</b> <b>processing</b> (PDP). It was an {{artificial neural network}} approach that stressed the parallel nature of neural processing, and the distributed nature of neural representations. It provided a general mathematical framework for researchers to operate in. The framework involved eight major aspects: ...|$|E
50|$|The {{new field}} was unified and {{inspired}} {{by the appearance of}} <b>Parallel</b> <b>Distributed</b> <b>Processing</b> in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.|$|E
50|$|Mitsunori Miki (born 1950) is {{a famous}} Japanese engineer. He is a {{professor}} of engineering department, knowledge engineering course of Doshisha University. Parallel computing, System engineering and Intelligent System Designing are in his line. His laboratory is called Intelligent System Design Laboratory (ISDL). Now he studies System optimization by <b>Parallel</b> <b>Distributed</b> <b>Processing,</b> Genetic Algorithm and computer clustering.|$|E
5000|$|Chivilikhin D., Shalyto A., Vyatkin V. Inferring Automata Logic From Manual Control Scenarios: Implementation in Function Blocks / Proceedings of the 13th IEEE International Symposium on <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> with Applications (ISPA'15). 2015, pp. 307-312.|$|R
40|$|This paper {{presents}} {{a method for}} solving power system transient stability problem by multiprocessing computing. A new algorithm based on system splitting, partitioning and Wmatrix techniques has been developed and implemented in the High Speed Transient Stability (HSTS) program. This program has embedded parallelism and can be executed on various platforms of <b>parallel</b> and <b>distributed</b> <b>processing</b> hardware. Keywords - Transient stability, network solution, system splitting, current compensation, <b>parallel</b> <b>processing,</b> <b>distributed</b> <b>processing,</b> realtime simulation. I. Introduction The emergence of parallel processing architectures has opened new opportunities and challenges for solving power system problems. For obtaining the transient stability solution using parallel processing hardware, suitable algorithms must be developed if high speed is to be achieved for large system size. Using these new algorithms implies significant modifications to existing programs [1, 2]. In order to harness th [...] ...|$|R
40|$|We {{describe}} {{a course on}} <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> taught at undergraduate level in our Informatics Engineering degree. It presents an integrated approach concerning concurrency, parallelism, and distribution issues. It's a breadth-first course addressing {{a wide spectrum of}} abstractions...|$|R
5000|$|David Everett Rumelhart (June 12, 1942 [...] - [...] March 13, 2011) was an American {{psychologist}} who made many {{contributions to the}} formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and <b>parallel</b> <b>distributed</b> <b>processing.</b> He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.|$|E
50|$|The IAC {{model is}} used by the <b>parallel</b> <b>distributed</b> <b>processing</b> (PDP) Group and is {{associated}} with James L. McClelland and David E. Rumelhart; it is described in detail in their book Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. This model does not contradict any currently known biological data or theories, and its performance is close enough to human performance as to warrant further investigation.|$|E
50|$|This account {{extends beyond}} the {{discrimination}} between two alternatives (mines and rocks) to that between different faces. Therefore, the distributive brain may have a single-cell output layer fire whenever a specific person, e.g. your grandmother, is presented to the eye; although many famous people, such as Jennifer Aniston or Halle Berry, may be of greater interest. Neural networks, then, explain very plausibly the otherwise paradoxical results of the recent grandmother cell experiments. Neural nets are a form of <b>parallel</b> <b>distributed</b> <b>processing.</b>|$|E
50|$|Walfredo Cirne and Keith Marzullo. The {{computational}} Co-op: Gathering clusters into a metacomputer. Proceedings 13th International Parallel Processing Symposium and 10th Symposium on <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> (IPPS/SPDP 1999). IEEE Computer Society 1999, pp. 160-6. Los Alamitos, CA, USA.|$|R
40|$|Java’s {{support for}} <b>parallel</b> and <b>distributed</b> <b>processing</b> makes the {{language}} attractive for metacomputing applications, such as parallel applications {{that run on}} geographically distributed (wide-area) systems. To obtain actual experience with a Java-centric approach to metacomputing, we have built and used a high-performance wide...|$|R
5000|$|Many <b>parallel</b> {{databases}} use <b>distributed</b> <b>parallel</b> <b>processing</b> {{to execute}} the queries. While executing an aggregate function such as sum, the following strategy is used: ...|$|R
50|$|When the <b>Parallel</b> <b>Distributed</b> <b>Processing</b> volumes were {{released}} in 1986-87, they provided some relatively simple software. The original PDP software {{did not require}} any programming skills, which led to its adoption by {{a wide variety of}} researchers in diverse fields. The original PDP software was developed into a more powerful package called PDP++, which in turn has become an even more powerful platform called Emergent. With each development, the software has become more powerful, but also more daunting for use by beginners.|$|E
5000|$|James Lloyd [...] "Jay" [...] McClelland, FBA (born December 1, 1948) is the Lucie Stern Professor at Stanford University, {{where he}} was {{formerly}} {{the chair of the}} Psychology Department. [...] He {{is best known for his}} work on statistical learning and <b>Parallel</b> <b>Distributed</b> <b>Processing,</b> applying connectionist models (or neural networks) to explain cognitive phenomena such as spoken word recognition and visual word recognition. McClelland is to a large extent responsible for the large increase in scientific interest for connectionism in the 1980s.|$|E
50|$|Farah {{was also}} among the first information-processing psychologists to use the {{behavior}} of neurological patients to test cognitive theories, starting in the early 1980s. At this time, cognition was understood by analogy with computers - mind is to brain as software is to hardware - {{and the difficulty of}} understanding a computer’s programs by exploring the effects of hardware “lesions” discouraged the use of neuropsychological methods in cognitive science. This criticism is only valid for certain types of computational architectures, and one of Farah’s contributions was to develop <b>parallel</b> <b>distributed</b> <b>processing</b> models of neuropsychological impairments.|$|E
50|$|The International <b>Parallel</b> and <b>Distributed</b> <b>Processing</b> Symposium (or IPDPS) is {{an annual}} conference for {{engineers}} and scientists to present recent {{findings in the}} fields of <b>parallel</b> <b>processing</b> and <b>distributed</b> computing. In addition to technical sessions of submitted paper presentations, the meeting offers workshops, tutorials, and commercial presentations & exhibits. IPDPS is sponsored by the IEEE Computer Society's Technical Committee on Parallel Processing.|$|R
40|$|Abstract: We discuss {{analytic}} {{procedures for}} evaluating the availability of parallel computer systems comprised of P processors with N tasks subject to failures and repairs. In addition, we argue, via analytic and numeric examples, that not incorporating the task-stream into the model is an inadequate approach for evaluating system performance. Keywords:- <b>Parallel</b> and <b>Distributed</b> <b>Processing...</b>|$|R
40|$|Classifier {{committee}} learning {{approaches have}} demonstrated great success in increasing the prediction accuracy of classifier learning, {{which is a}} key technique for datamining. It {{has been shown that}} Boosting and Bagging, as two representative methods of this type, can significantly decrease the error rate of decision tree learning. Boosting is generally more accurate than Bagging, but the former is more variable than the latter. In addition, Bagging is amenable to <b>parallel</b> or <b>distributed</b> <b>processing,</b> while Boosting is not. In this paper, we study a new committee learning algorithm, namely MB (Multiple Boosting). It creates multiple subcommittees by combining Boosting and Bagging. Experimental results in a representative collection of natural domains show that MB is, on average, more accurate than either Bagging or Boosting alone. It is more stable than Boosting, and is amenable to <b>parallel</b> or <b>distributed</b> <b>processing.</b> These characteristics make MB a good choice for parallel datamining. K [...] ...|$|R
