136|291|Public
5000|$|The data {{processed}} in the cube is made analysis ready [...] before being ingested and indexed into the AGDC. Analysis ready data is <b>pre-processed</b> <b>data</b> that has applied corrections for instrument calibration (gains and offsets), geolocation (spatial alignment) and radiometry (solar illumination, incidence angle, topography, atmospheric interference). The ingestion process manages {{the translation of}} datasets into the storage units while maintaining a database index. The data within the storage and index can be accessed via API calls often compiled within code such as Python (programming language).|$|E
30|$|Transporting <b>pre-processed</b> <b>data</b> to the cloud.|$|E
30|$|In {{the second}} step, the <b>pre-processed</b> <b>data</b> with chosen anomaly {{detection}} technique {{obtained from the}} first step is used to train the data-driven model based on predictive data mining techniques. The outcome of investigation from these steps will explore the interplay between anomaly detection technique and forecasting model accuracy.|$|E
50|$|Statistics {{guided by}} rules: Rules {{are used to}} <b>pre-process</b> <b>data</b> {{in an attempt to}} better guide the {{statistical}} engine. Rules are also used to post-process the statistical output to perform functions such as normalization. This approach has a lot more power, flexibility and control when translating.|$|R
30|$|Two {{distinct}} setups {{are built}} {{to implement the}} TPM-AES cryptosystem. First, a virtual model is created for analysis of possible security-compromising scenarios. Then, an experimental setup is constructed based on low-cost equipment and available measurement instrumentation to evaluate how SCAs can affect its security. The methodology to acquire and <b>pre-process</b> <b>data</b> is explained and usage of the AVR ATMega 328 P microcontroller for SCAs is discussed.|$|R
40|$|We {{present a}} novel {{technique}} for combining statistical machine learning for proof-pattern recognition with symbolic methods for lemma discovery. The resulting tool, ACL 2 (ml), gathers proof statistics and uses statistical pattern-recognition to <b>pre-processes</b> <b>data</b> from libraries, and then suggests auxiliary lemmas in new proofs by analogy with already seen examples. This paper presents {{the implementation of}} ACL 2 (ml) alongside theoretical descriptions of the proof-pattern recognition and lemma discovery methods involved in it...|$|R
40|$|In this paper, {{we address}} the problem of {{interference}} mitigation with data pre-processing in the 4 G uplink systems, and propose to use the Grubbs/Wright algorithm to detect and remove the interference contaminated data. The Markov algorithm is also applied to correct the system errors. The <b>pre-processed</b> <b>data</b> are used for channel estimation and data detection in base station...|$|E
40|$|Managing {{data from}} {{large-scale}} projects (such as The Cancer Genome Atlas (TCGA)) for further analysis {{is an important}} and time consuming step for research projects. Several efforts, such as the Firehose project, make TCGA <b>pre-processed</b> <b>data</b> publicly available via web services and data portals, but this information must be managed, downloaded and prepared for subsequent steps. We have developed an open source and extensible R based data client for <b>pre-processed</b> <b>data</b> from the Firehouse, and demonstrate its use with sample case studies. Results show that our RTCGAToolbox can facilitate data management for researchers interested in working with TCGA data. The RTCGAToolbox can also be integrated with other analysis pipelines for further data processing. The RTCGAToolbox is open-source and licensed under the GNU General Public License Version 2. 0. All documentation and source code for RTCGAToolbox is freely available at [URL] for Linux and Mac OS X operating systems...|$|E
30|$|The Landsat 5 TM scenes 5 were {{acquired}} in summer 2007 (Julian day 248, postfire) 6 and in summer 2003 (Julian day 237, prefire) 7. These are already <b>pre-processed</b> <b>data</b> of Level- 18 and delivered as scaled digital numbers. Since {{we do not}} cross-compare data from different sensors, and burned areas feature distinct spectral profiles, no further pre-processing was performed.|$|E
40|$|The {{next-generation}} sequencing era requires reliable, fast and efficient approaches for the accurate annotation of the ever-increasing number of biological sequences and their variations. Transfer of annotation upon similarity search {{is a standard}} approach. The procedure of all-against-all protein comparison is a preliminary step of different available methods that annotate sequences based on information already present in databases. Given the actual volume of sequences, methods are necessary to <b>pre-process</b> <b>data</b> to reduce the time of sequence comparison...|$|R
5000|$|Statistics {{guided by}} rules: Rules {{are used to}} <b>pre-process</b> <b>data</b> {{in an attempt to}} better guide the {{statistical}} engine. Rules are also used to post-process the statistical output to perform functions such as normalization. This approach has a lot more power, flexibility and control when translating. It also provides extensive control over {{the way in which the}} content is processed during both pre-translation (e.g. markup of content and non-translatable terms) and post-translation (e.g. post translation corrections and adjustments).|$|R
40|$|SUMMARY: MAnGO (Microarray Analysis at the Gif/Orsay platform) is an {{interactive}} R-based {{tool for the}} analysis of two-colour microarray experiments. It is a compilation of various methods, which allows the user (1) to control data quality by detecting biases with a large number of visual representations, (2) to <b>pre-process</b> <b>data</b> (filtering and normalisation) and (3) to carry out differential analyses. MAnGO is not only a "turn-key" tool, oriented towards biologists but also a flexible and adaptable R script oriented towards bioinformaticians. AVAILABILITY: [URL]...|$|R
40|$|Increased {{implementation}} of new databases related to multidimensional data involving techniques to support efficient query process, create opportunities for more extensive research. Pre-processing is required {{because of lack}} of data attribute values, noisy data, errors, inconsistencies or outliers and differences in coding. Several types of pre-processing based on component analysis will be carried out for cleaning, data integration and transformation, as well as to reduce the dimensions. Component analysis can be done by statistical methods, with the aim to separate the various sources of data into a statistical pattern independent. This paper aims {{to improve the quality of}} <b>pre-processed</b> <b>data</b> based on component analysis. RapidMiner is used for data pre-processing using FastICA algorithm.   Kernel K-mean is used to cluster the <b>pre-processed</b> <b>data</b> and Expectation Maximization (EM) is used to model. The model was tested using wisconsin breast cancer datasets, lung cancer datasets and prostate cancer datasets. The result shows that the performance of the cluster vector value is higher and the processing time is shorter. </p...|$|E
40|$|Data mining {{in medical}} data has {{successfully}} converted raw data into useful information. This information helps the medical experts {{in improving the}} {{diagnosis and treatment of}} diseases. In this paper, we review studied data mining applications applied exclusively on an open source diabetes dataset. Type II Diabetes Mellitus is one of the silent killer diseases worldwide. According to the World Health Organization, 346 million people are suffering from diabetes worldwide. Diagnosis or prediction of diabetes is done through various data mining techniques such as association, classification, clustering and pattern recognition. The study led to the related open issues of identifying the need of a relation between the major factors that lead to the development of diabetes. This is possible by mining patterns found between the independent and dependant variables in the dataset. This paper compares the classification accuracies of non-processed and <b>pre-processed</b> <b>data.</b> The results clearly show that the <b>pre-processed</b> <b>data</b> gives better classification accuracy...|$|E
40|$|We have {{collected}} the access logs for our university's web domain over a time span of 4. 5 years. We now release the <b>pre-processed</b> <b>data</b> of a 3 -month period for research into user navigation behavior. We preprocessed the data {{so that only}} successful GET requests of web pages by non-bot users are kept. The resulting 3 -month collection comprises 9. 6 M page visits (190 K unique URLs) by 744 K unique visitors. © 2016 ACM...|$|E
40|$|Hierarchical {{system of}} network nodes is {{suitable}} solution how {{to collect and}} how to <b>pre-process</b> <b>data</b> from large amount of end-nodes. By contrast to flat (one layer) architecture there are special intermediary nodes used and they are called summarization nodes. These special nodes have to be suitably placed in the network to enable efficient data collection and their number in the hierarchy {{is one of the}} key parameters of the architecture. The article deals with the tree architecture design, with its optimisation and with the problem of limited number of summarization nodes. </em...|$|R
25|$|Even though PNG {{has been}} {{designed}} as a lossless format, PNG encoders can <b>pre-process</b> image <b>data</b> in a lossy fashion (so as to reduce colors used) to improve PNG compression.|$|R
40|$|This {{document}} is a user's {{guide for the}} Energy Charting and Metrics Tool to facilitate the examination of energy information from buildings, reducing the time spent analyzing trend and utility meter data. This user guide was generated to help <b>pre-process</b> <b>data</b> {{with the intention of}} utilizing the Energy Charting and Metrics (ECAM) tool to improve building operational efficiency. There are numerous occasions when the metered data that is received from the building automation system (BAS) isn't in the right format acceptable for ECAM. This includes, but isn't limited to, cases such as inconsistent time-stamps for the trends (e. g., each trend has its own time-stamp), data with holes (e. g., some time-stamps have data and others are missing data), each point in the BAS is trended and exported into an individual. csv or. txt file, the time-stamp is unrecognizable by ECAM, etc. After reading through this user guide, the user should be able to <b>pre-process</b> all <b>data</b> files and be ready to use this data in ECAM to improve their building operational efficiency...|$|R
30|$|To {{evaluate}} {{the performance of}} our recommendation method, {{we set up a}} data set by splitting the <b>pre-processed</b> <b>data</b> into a training set (90  %) and a test set (10  %). The training set contains approximately 8.1 million tweets; roughly 900, 000 of these contain at least one hashtag. The test set contains about 100, 000 tweets, all of which include at least one hashtag. Finally, the THFM and HFM were generated by running our Map-Reduce applications using the training set as input.|$|E
30|$|The fMRI {{data was}} {{collected}} at 500 ms sampling rate in a 3 D space of 64 × 64 × 8 voxels, and the <b>pre-processed</b> <b>data</b> of 6 subjects is available to public. The scanned area contains 25 – 30 anatomical regions of interest (ROIs), which have approximately 4000 voxels. Particularly, 7 ROIs are highlighted by the proposer as they are most relevant to this task. Thus, the number of voxels to be analyzed in our study is reduced to around 2000, varying from subject to subject.|$|E
40|$|Problems and {{experience}} related to data mining in medical data ADAMEK are described. Data ADAMEK concerns 1122 cardiologic patients. The information collected for each patient {{is stored in}} 951 attributes. Pre-processing of data resulted into set of 180 attributes. After getting first experience in mining <b>pre-processed</b> <b>data,</b> a system of more than 70 analytical questions was defined. The analytical questions can be (at least partly) solved by students teams within education process at the Faculty of Informatics and Statistics of University of Economics, Prague. Paper summarizes first experience concerning this approach...|$|E
40|$|In {{the age of}} E-Business many {{companies}} faced with massive data sets that must be analysed for gaining a competitive edge. these data sets are in many instances incomplete and quite often not of very high quality. Although statistical analysis {{can be used to}} <b>pre-process</b> these <b>data</b> sets, this technique has its own limitations. In this paper we are presenting a system - and its underlying model - {{that can be used to}} test the integrity of existing <b>data</b> and <b>pre-process</b> the <b>data</b> into clearer data sets to be mined. LH 5 is a rule-based system, capable of self-learning and is illustrated using a medical data set...|$|R
40|$|The use {{of machine}} {{learning}} techniques to automatically analyse {{data for information}} is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to <b>pre-process</b> <b>data</b> before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract {{more information about the}} domain. Categories and Subject Descriptors I. 2. 2 [Artificial Intelligence]: Automatic Programming...|$|R
40|$|Dealing with {{structured}} data has always represented {{a huge problem}} for classical neural methods. Although many efforts have been performed, they usually <b>pre-process</b> <b>data</b> and then use classic machine learning algorithm. Another problem that machine learning algorithm have to face is the intrinsic uncertainty of data, where in such situations classic algorithm {{do not have the}} means to handle them. In this work a novel neuro-fuzzy model for {{structured data}} is presented that exploits both neural and fuzzy methods. The proposed model called Fuzzy Graph Neural Network (F-GNN) is based on GNN, a model able to handle structure data. A proof of F-GNN approximation properties is provided together with a training algorithm...|$|R
40|$|Abstract:- Eddy-current {{displacement}} sensors {{are very}} sensitive to many environmental factors and especially to temperature variations. We deal here with a real world problem that is, distance measurement between an eddy-current sensor and a metallic target (an aluminun sheet) which temperature varies during displacement. We show, that source separation techniques applied to the experimental <b>pre-processed</b> <b>data</b> set issued from the sensor response permit to obtain the sensor-target distance as well as the target temperature profile, although disturbed by an additive noise due to the instrumentation. Key-Words:- eddy-current sensor, source separation, independent component analysis, linear filtering...|$|E
40|$|In {{this paper}} {{identification}} of laryngeal disorders using cepstral parameters of human voice is investigated. Mel-frequency cepstral coefficients (MFCC), extracted from audio recordings, are further approximated, using 3 strategies: sampling, averaging, and estimation. SVM and LS-SVM categorize <b>pre-processed</b> <b>data</b> into normal, nodular, and diffuse classes. Since it is a three-class problem, various combination schemes are explored.  Constructed custom kernels outperformed a popular non-linear RBF kernel. Features, estimated with GMM, and SVM kernels, designed to exploit this information, {{is an interesting}} fusion of probabilistic and discriminative models for human voice-based classification of larynx pathology...|$|E
40|$|The thesis {{deals with}} the {{valuation}} of real estates in the Czech Republic using statistical methods. The work focuses on a complex task {{based on data from}} an advertising web portal. The aim of the thesis is to create a prototype of the statistical predication model of the residential properties valuation in Prague and to further evaluate the dissemination of its possibilities. The structure of the work is conceived according to the CRISP-DM methodology. On the <b>pre-processed</b> <b>data</b> are tested the methods regression trees and random forests, which are used to predict the price of real estate...|$|E
40|$|Cloud {{computing}} {{has become}} an integral part of IT services, storing the application softwares and databases in large centralized shared data servers. Since it’s a shared platform, the data and services may not be fully trust worthy. In this work, we have implemented an efficient security model that ensures the data integrity of stored data in cloud servers. The computational load of data verification linearly grows with the complexity of the security model and this poses a serious problem at the resource constrained user’s end. Therefore to tackle this problem we have implemented a new cloud storage scheme which ensures proof of retrivebility (OPoR) at a third party cloud audit server to <b>pre-process</b> <b>data</b> before uploading into cloud storage server...|$|R
40|$|Feature Extraction (FE) {{techniques}} {{are widely used}} in many applications to <b>pre-process</b> <b>data</b> {{in order to reduce}} the complexity of subsequent processes. A group of Kernel-based nonlinear FE (H E) algorithms has attracted much attention due to their high performance. However, a serious limitation that is inherent in these algorithms [...] the maximal number of features extracted by them is limited by the number of classes involved [...] dramatically degrades their flexibility. Here we propose a modified version of those KFE algorithms (MKFE), This algorithm is developed from a special form of scatter-matrix, whose rank is not determined by the number of classes involved, and thus breaks the inherent limitation in those KFE algorithms. Experimental results suggest that MKFE algorithm is. especially useful when the training set is small...|$|R
30|$|The {{ubiquitous}} actuating {{characteristic of}} IoT enables energy resources to fast respond to external control signals and be controlled in a real-time/near real-time manner. The edge computing capability of embedded processors of IoT devices enable different energy resources {{be able to}} perform computing tasks and achieve objectives that are hard to be achieved by centralized management. For example, distributed energy resources can perform <b>data</b> <b>pre-process</b> logics (<b>data</b> consistency checking, noise filtering, etc.) before it is transmitted to a wider environment (e.g., Blockchain and cloud); the distributed energy resources in a certain area can communicate with each other to perform decentralized control decision-making, and so forth.|$|R
40|$|There is a {{dependence}} of TM output (proportional to scene radiance {{in a manner}} which will be discussed) upon season, upon cover type and upon view angle. The existence of a significant systematic variation across uniform scenes in p-type (radiometrically and geometrically <b>pre-processed)</b> <b>data</b> is demonstrated. Present pre-processing does remove the effects and the problem must be addressed because the effects are large. While this {{is in no way}} attributable to any shortcomings in the thematic mapper, it is an effect which is sufficiently important to warrant more study, with a view to developing suitable pre-processing correction algorithms...|$|E
40|$|A {{number of}} {{techniques}} {{have been used}} to provide functional connectivity estimates for a given fMRI data set. In this study we compared two methods: a 'rest-like' method where the functional connectivity was estimated for the whitened residuals after regressing out the task-induced effects, and a within-condition method where the functional connectivity was estimated separately for each experimental condition. In both cases four pre-processing strategies were used: 1) time courses extracted from standard <b>pre-processed</b> <b>data</b> (standard); 2) adjusted time courses extracted using the volume of interest routines in SPM 2 from standard <b>pre-processed</b> <b>data</b> (spm); 3) time courses extracted from ICA denoised data (standard denoised); and 4) adjusted time courses extracted from ICA denoised data (spm denoised). The temporal correlation between time series extracted from two cortical regions were statistically compared with the temporal correlation between a time series extracted from a cortical region and a time series extracted form a region placed in CSF. Since the later correlation is due to physiological noise and other artifacts, we used this comparison to investigate whether rest-like and task modulated connectivity could be estimated from the same data set. The pre-processing strategy had a significant effect on the connectivity estimates with the standard time courses providing larger connectivity values than the spm time courses for both estimation methods. The CSF comparison indicated that for our data set only rest-like connectivity could be estimated. The rest-like connectivity values were similar with connectivity estimated from resting state data...|$|E
40|$|With {{the rapid}} advancements in {{information}} and communication technology in the world, crimes committed are also becoming technically intensive. When crimes committed use digital devices, forensic examiners have to adopt practical frameworks and methods for recovering data for analysis as evidence. Data Generation, Data Warehousing and Data Mining, are the three essential features involved in this process. This paper proposes a unique way of generating, storing and analyzing data, retrieved from digital devices which pose as evidence in forensic analysis. A {{statistical approach is used}} in validating the reliability of the <b>pre-processed</b> <b>data.</b> This work proposes a practical framework for digital forensics on flash drives...|$|E
5000|$|... the {{algorithm}} can <b>pre-process</b> static Associated <b>Data</b> (AD), useful for encryption/decryption of communication session parameters (where session parameters {{may represent the}} Associated Data).|$|R
40|$|This is a macro which {{facilitates}} remote {{execution of}} WinBUGS from within SAS. The macro <b>pre-processes</b> <b>data</b> for WinBUGS, writes the WinBUGS batch-script, executes this script and reads in output {{statistics from the}} WinBUGS log-file back into SAS native format. The user specifies the input and output file names and directory path {{as well as the}} statistics to be monitored in WinBUGS. The code works best for a model that has already been set up and checked for convergence diagnostics within WinBUGS. An obvious extension of the use of this macro is for running simulations where the input and output files all have the same name but all that differs between simulation iterations is the input dataset. The functionality and syntax of the macro call are described in this paper and illustrated using a simple linear regression model. ...|$|R
40|$|The {{extraction}} of features for classification is often performed heuristically, despite the effect this step {{has on the}} performance of the classifier. The Evolutionary Pre-Processor is presented, an automatic nonparametric method for the {{extraction of}} non-linear features. Using genetic programming, the Evolutionary Pre-Processor evolves networks of different non-linear functions which <b>pre-process</b> the <b>data</b> to improve the discriminatory performance of a classifier. In experiments performed on 9 real-world data sets, the Evolutionary Pre-Processor was able to <b>pre-process</b> the <b>data</b> to reduce the test set misclassification rate. The dimensionality of the data was decreased and those measurements not required for classification were excised. The Evolutionary PreProcessor behaved intelligently by deciding whether to perform feature extraction or feature selection. 1 Introduction A common step in Pattern Classification is the extraction of features from the original data, motivated by the red [...] ...|$|R
