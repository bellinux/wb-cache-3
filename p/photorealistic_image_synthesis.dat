22|1176|Public
40|$|Radiative flux {{transfer}} between Lambertian surfaces can {{be described}} in terms of linear resistive networks with voltage sources. In this paper we examine how these “radiative transfer networks ” provide a physical interpretation for the eigenvectors of form factor matrices. This leads to a novel approach to <b>photorealistic</b> <b>image</b> <b>synthesis</b> and radiative transfer analysis called eigenvector radiosity. 1. ...|$|E
40|$|This thesis {{presents}} an in-depth {{study of the}} problem of <b>photorealistic</b> <b>image</b> <b>synthesis</b> in the field of computer graphics. To better understand the proposed solutions to this problem, we first briefly overview several relevant fundamental topics, such as probability, Monte Carlo integration, sampling methods, reflection methods, etc. We then review some of the most influential Monte-Carlo-Based image synthesis methods introduced over the past four decades. Finally, to better understand the advantages and weaknesses of each method, we evaluate the reviewed methods using the Mitsuba Renderer, an open-source, research-oriented rendering system...|$|E
40|$|As {{opposed to}} Monte Carlo {{integration}} the quasi-Monte Carlo method {{does not allow}} for an (consistent) error estimate from the samples used for the integral approximation. In addition the deterministic error bound of quasi-Monte Carlo integration is not accessible in the setting of computer graphics, since usually the integrands are of unbounded variation. The structure of the high dimensional functionals to be computed for <b>photorealistic</b> <b>image</b> <b>synthesis</b> implies the application of the randomized quasi-Monte Carlo method. Thus we can exploit low discrepancy sampling {{and at the same time}} we can estimate the variance. The resulting technique is much more efficient than previous bidirectional path tracing algorithms...|$|E
40|$|Painterly {{rendering}} styles have received some attention lately as an interesting alternative to <b>photorealistic</b> <b>image</b> processing and <b>synthesis.</b> State {{of the art}} painterly renderings, while producing results that superficially resemble certain artistic styles, such as impressionistic or expressionistic, fail to convey the implicit detail provided by the artist’s choices of brushes and canvas. In this paper we present techniques to simulate the effects of paint thickness and canvas texture as they interact under arbitrary lighting conditions. We also extend previous works by allowing the user to specify brush nibs of any size, shape and texture. 1...|$|R
40|$|Synthesizing high {{resolution}} <b>photorealistic</b> <b>images</b> {{has been a}} long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for <b>image</b> <b>synthesis.</b> We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional <b>image</b> <b>synthesis</b> models. These analyses demonstrate that {{high resolution}} samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are {{more than twice as}} discriminable as artificially resized 32 x 32 samples. In addition, 84. 7 % of the classes have samples exhibiting diversity comparable to real ImageNet data...|$|R
40|$|This work {{presents}} {{method of}} the quality analyses of the <b>photorealistic</b> <b>images.</b> Developed method utilizes proposed criteria of image definition quality for fine details. Presented method has the following significant property: the quality estimation is provided without test images or patterns. Algorithm for search and recognition of fine structures in the <b>photorealistic</b> <b>images</b> using the predefined criterion is considered...|$|R
40|$|Superquadrics {{are well}} known and often used 3 D surface objects in {{computer}} graphics. They are used for modelling parts of scenes that are then rendered using <b>photorealistic</b> <b>image</b> <b>synthesis</b> algorithms (e. g., ray tracing). For some techniques like texturing, which are part of these rendering methods, the type of the parametrization of such a surface has to be chosen carefully and is not intuitively obvious at first sight. There are cases, where the straight forward extension of quadric parameterizations to superquadrics do not produce satisfying results. We therefore investigate {{a number of different}} parameterizations in combination with the corresponding formulas, and point out some significant differences between them...|$|E
40|$|In the <b>Photorealistic</b> <b>Image</b> <b>Synthesis</b> {{process the}} {{spectral}} {{content of the}} synthetic scene is carefully reproduced, and the final output contains the exact spectral intensity light field of the perceived scene. This is the first important step toward the goal of producing a synthetic image that is indistinguishable fiom the actual one, but the real scene and its synthetic reproduction should be studied under the same conditions, {{in order to make}} a correct comparison and evaluate the degree of photorealism. To simplifL this goal, a synthetic observer could be employed to compensate differences in the viewing conditions, since a real observer can not enter into a synthetic world. Various solutions have been proposed to this end. Most of them are based more on perceptive measures of the Human Visual System (HVS) under controlled conditions rather than on the HVS behavior under real conditions, e. g. observing a common image and not a controlled black and white striped pattern. Another problem in synthetic image generation is the visualization phase, or tone reproduction, whose purpose is to display the final result of the simulation model on a monitor screen or on a printed paper. The tone reproduction problem consists of finding the best solution to compress the extended dynamic range of the computed light field into the limited range of the displayable colors. We would like to propose a working hypothesis to solve the appearance and the tone reproduction problems in the synthetic image generation, integrating the Retinex model into the <b>photorealistic</b> <b>image</b> <b>synthesis</b> context, including in this way a model of the human visual system in the synthesis process...|$|E
40|$|In the <b>photorealistic</b> <b>image</b> <b>synthesis</b> process, an {{accurate}} approximation of the spectral light radiance field of a synthetic scene is carefully reproduced, {{with the goal}} of generating a synthetic image that is indistinguishable from the actual one. The paradigm of photorealism requires a comparison of the real scene and its synthetic reproduction, and the two visual representations should be studied under the same conditions to make a correct comparison and evaluate the degree of accuracy. To reproduce the same observing conditions, we need to define a sort of synthetic observer (given the impossibility to enter into a synthetic world) to compensate the deep differences in the viewing conditions, between the real and synthetic images. Various solutions have been proposed to this end; most of them are based more on perceptive measures of the human visual system (HVS) under controlled conditions, rather than on the HVS behavior under real conditions, e. g., observing a natural image and not a controlled black and white or colored pattern. Besides the comparison problem, difficulties can arise from the visualization phase, whose purpose is to display the final results of the simulation model on a monitor screen or printed paper. This is known as the tone reproduction problem, and in most cases, one has to find the best solution to compress an extended dynamic range of the computed light field into the limited range of displayable colors. Several solutions have been proposed to solve this problem. On the contrary, no mapping is usually made in case of low luminance and extremely limited dynamic range images, and consequently photorealism and visual appearance are lost. We propose a working hypothesis to solve the appearance and the tone reproduction problems in the synthetic image generation, integrating the Retinex model into the <b>photorealistic</b> <b>image</b> <b>synthesis</b> context, including in this way a model of the HVS in the image synthesis proces...|$|E
5000|$|Create a <b>photorealistic</b> <b>image</b> of your {{arrangement}} {{depending on}} the time of the day and the light sources placed in the plan.|$|R
5000|$|Autodesk Renderer (formerly Presenter) - With the Renderer {{users can}} apply {{materials}} and lighting {{to the model}} and produce <b>photorealistic</b> <b>images</b> and animations ...|$|R
50|$|Also {{available}} is a sketch rendering mode that produces non <b>photorealistic</b> <b>images,</b> which {{appear as if}} they were drawn by manual rendering techniques, such as oil painting, water color, or pencil hatches.|$|R
40|$|In this paper, we presenta rigoroustheoretical {{formulation}} of the fundamental problem [...] -indirectillumination from area sources via curved ideal specular surfaces. Intensity and area factors are introduced to clarify this problem and to rectify the radiance from these specular surfaces. They take surface geometry, such as Gaussian curvature, into account. Based on this formulation, an algorithm for integrating ideal specular transfers into global illumination is also presented. This algorithm can deal with curved specular reflectorsand transmitters. An implementation is described based on wavefront tracing and progressive radiosity. Sample images generated by this method are presented. Keywords:Rendering equation; Global illumination; Specular transfer; Wavefront tracing; Radiosity; Ray tracing; Meshing 1. Introduction Computing solutions to the global illumination problem {{is an essential part}} of <b>photorealistic</b> <b>image</b> <b>synthesis.</b> Global illumination effects produced by multiple surface [...] ...|$|E
40|$|<b>Photorealistic</b> <b>image</b> <b>synthesis</b> is a {{computationally}} demanding {{task that}} relies on ray tracing {{for the evaluation of}} integrals. Rendering time is dominated by tracing long paths that are very incoherent by construction. We therefore investigate the use of SIMD instructions to accelerate incoherent rays. SIMD is used in the hierarchy construction, the tree traversal and the leaf intersection. This is achieved by increasing the arity of ac-celeration structures, which also reduces memory requirements. We show that the resulting hierarchies can be built quickly and are smaller than acceleration structures known so far {{while at the same time}} outperforming them for incoherent rays. Our new acceleration structure speeds up ray tracing by a factor of 1. 6 to 2. 0 compared to a highly optimized bounding inter-val hierarchy implementation, and 1. 3 to 1. 6 compared to a...|$|E
40|$|The {{rendering}} of realistic images requires a precise treatment of lighting effects by simulating the underlying physical phenomena of light emission, propagation, and reflection. The radiosity method {{is widely used}} for computing such global illumination effects in image synthesis applications. Dueto the extensive computational demands and the enormous memory requirements of this method, an efficient data-parallel radiosity algorithm is needed. Dynamic load balancing techniques are the most critical part of an efficient implementation of parallel algorithms on distributed computing systems. In this paper a new dynamic load balancing strategy is proposed which increase the efficiency of a previously introduced data-parallel progressive refinement radiosity method. Keywords: computer graphics, global illumination, parallel radiosity, dynamic load balancing 1 Introduction The goal of <b>photorealistic</b> <b>image</b> <b>synthesis</b> is to generate pictures with a maximum degree of realism. It should be diff [...] ...|$|E
50|$|The RenderMan Interface Specification, or RISpec in short, {{is an open}} API {{developed}} by Pixar Animation Studios to describe three-dimensional scenes {{and turn them into}} digital <b>photorealistic</b> <b>images.</b> It includes the RenderMan Shading Language.|$|R
40|$|Computer {{graphics}} rendering {{software is}} capable of generating highly <b>photorealistic</b> <b>images</b> that can be impossible to differentiate from photographic images. As a result, the unique stature of photographs as a definitive recording of events is being diminished (the ease with which digital images can be manipulated is, of course, also contributing to this demise). To this end, we describe a method for differentiating between <b>photorealistic</b> and photographic <b>images.</b> Specifically, we show that a statistical model based on first- and higher-order wavelet statistics reveals subtle but significant differences between <b>photorealistic</b> and photographic <b>images...</b>|$|R
40|$|Generating <b>photorealistic</b> <b>images</b> of astrophysical {{simulations}} {{can enhance}} the experience of watching galactic visualizations for both the specialists who study the data and the average person who is simply interested in outer space. Unfortunately, the astrophysicist who is creating the simulations typically lacks the expertise required to generate <b>photorealistic</b> <b>images.</b> Likewise, a 3 D artist may be unaware of the physics behind certain astrophysical events. We aim to use Spiegel, a user interface that controls the rendering of astrophysical data and Maya, a high end 3 D animation program, to allow a non-artist to easily create renders of <b>photorealistic</b> <b>images.</b> Spiegel provides a user-friendly interface for controlling the creation of potentially complex rendering applications by individuals with little experience in computer programming. Since Spiegel’s basic visualization capabilities are limited to simple primitives like points and lines, {{it was necessary to}} develop an additional program for Spiegel to interface with Maya’s 3 D rendering capabilities. This software interface is called Miegel. Using Miegel, the astrophysicist now has access to Maya’s 3 D rendering capabilities allowing them to create stunning visualizations of astrophysical phenomena. In addition, new artistic effects can be created with Maya in the form of presets, which can be integrated into the user’s visualization with minimal knowledge of computer programming. ...|$|R
40|$|The {{two worlds}} of {{interactive}} graphics and realistic graphics have remained separate. Fast graphics hardware runs simple algorithms and generates simple looking images. <b>Photorealistic</b> <b>image</b> <b>synthesis</b> software runs slowly on large expensive computers. The {{time has come}} for these two branches of computer graphics to merge. The speed and expense of graphics hardware is no longer the barrier to the wide acceptance of photorealism. There is {{every reason to believe that}} high quality image synthesis will become a standard capability of every graphics machine, from superworkstation to personal computer. The significant barrier has been the lack of a common language, an agreed-upon set of terms and conditions, for 3 -D modeling systems to talk to 3 -D rendering systems for computing an accurate rendition of that scene. Pixar has introduced RenderMan to serve as that common language. RenderMan, specifically the extensibility it offers in shading calculations, is discussed...|$|E
40|$|In the <b>photorealistic</b> <b>image</b> <b>synthesis</b> process, an {{accurate}} approximation of the spectral light radiance field of a synthetic scene is carefully reproduced, {{with the goal}} of generating a synthetic image that is indistinguishable from the actual one. The paradigm of photorealism requires a comparison of the real scene and its synthetic reproduction, and the two visual representations should be studied under the same conditions to make a correct comparison and evaluate the degree of accuracy. To reproduce the same observing conditions, we need to define a sort of synthetic observer (given the impossibility to enter into a synthetic world) to compensate the deep differences in the viewing conditions, between the real and synthetic images. Various solutions have been proposed to this end; most of them are based more on perceptive measures of the human visual system (HVS) under controlled conditions, rather than on the HVS behavior under real conditions, e. g., observing a natura...|$|E
40|$|<b>Photorealistic</b> <b>image</b> <b>synthesis</b> {{requires}} the simulation of many naturally occurring lighting effects such as caustics and color bleeding, which are indirect lighting effects that {{are supported by}} global illumination algorithms. This thesis presents a GPU based parallel adaptation of the photon mapping global illumination algorithm using Nvidia’s ‘Compute Unified Device Architecture ’ (CUDA). Part of this implementation entails the parallel adaptation of a nearest neighbor search algorithm, for which we use the spatially aware uniform grid and KD-tree data structures {{from the field of}} geometric algorithms. Finally, in order to address the memory related bottleneck of photon mapping on the GPU, we also present a novel parallel adaptation of the suitably fitting progressive photon mapping technique on the same architecture. Our work results in a real-time parallel implementation of the photon mapping algorithm and a novel adaptation of the Progressive Photon Mapping on the same architecture, showing that high quality progressive global illumination on the GPU architecture is attainable when adapting the algorithm of choice to address architecture-specific issues...|$|E
40|$|We {{present a}} simple nearest-neighbor (NN) {{approach}} that synthesizes high-frequency <b>photorealistic</b> <b>images</b> from an "incomplete" signal {{such as a}} low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional <b>image</b> <b>synthesis</b> lack two important things: (1) {{they are unable to}} generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds: the first stage uses a convolutional neural network (CNN) to maps the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. We demonstrate our approach for various input modalities, and for various domains ranging from human faces to cats-and-dogs to shoes and handbags. Comment: Project Page: [URL]...|$|R
40|$|International audienceA popular {{approach}} for computing <b>photorealistic</b> <b>images</b> of virtual objects requires applying reflectance profiles measured from real surfaces, introducing several challenges: the memory needed to faithfully capture realistic material reflectance is large, {{the choice of}} materials {{is limited to the}} set of measurements, and <b>image</b> <b>synthesis</b> using the measured data is costly. Typically, this data is either compressed by projecting it onto a subset of its linear principal components or by applying non-linear methods. The former requires many components to faithfully represent the input reflectance, whereas the latter necessitates costly extrapolation algorithms. We learn an underlying, low-dimensional non-linear reflectance manifold amenable to rapid exploration and rendering of real-world materials. We can express interpolated materials as linear combinations of the measured data, despite them lying on an inherently non-linear manifold. This allows us to efficiently interpolate and extrapolate measured BRDFs, and to render directly from the manifold representation. We exploit properties of Gaussian process latent variable models and use our representation for high-performance and offline rendering with interpolated real-world materials...|$|R
40|$|Light {{fields are}} an {{approach}} to entirely capture the visual information of a threedimensional object or scene. The intention is to reproduce <b>photorealistic</b> <b>images</b> of the scene for any desired viewpoint and for any viewing angle. In contrast to classical geometry-based approaches, where information such as the geometry and surfac...|$|R
40|$|Abstract — Ray tracing is a {{well known}} method for <b>photorealistic</b> <b>image</b> <b>synthesis,</b> volume {{visualization}} and rendering. Over the last decade the method is being adopted throughout the research community around the world. With {{the advent of the}} high speed processing units, the method has been emerging from offline rendering towards real time rendering. The success behind ray tracing algorithms lies in the use of acceleration data structures and modern processing power of CPUs and GPUs. kd-tree {{is one of the most}} widely used data structures based on surface area heuristics (SAH). The major bottleneck in kd-tree construction is the time consumed to find optimum split locations. In this paper, we propose a prediction algorithm for animated ray tracing based on Kalman filtering. The algorithm successfully predicts the split locations for the next consecutive frame in the animation sequence. Thus, giving good initial starting points for one dimensional search algorithms to find optimum split locations – in our case parabolic interpolation combined with golden section search. With our technique implemented, we have reduced the “running kd-tree construction ” time by between 78 % and 87 % for dynamic scenes with 16. 8 K and 252 K polygons respectively...|$|E
40|$|Image-based {{rendering}} (IBR) {{is one of}} {{the most}} promising techniques for <b>photorealistic</b> <b>image</b> <b>synthesis.</b> However, the naive IBR framework has some drawbacks, including restrictions on viewing direction, and difficulties in managing illumination change or object deformation. In this paper, we present a new IBR method capable of composing images of deformable objects from arbitrary view points, and under arbitrary illumination. To do this, we measured and utilized geometric object model and bidirectional texure funcion (BTF). BTF is genaralization of BRDF with spatial variation along object’s surface, represented as a set of texture databases which are captured from every viewing angle and light direction. To evaluate the efficiency of this method, we performed several experiments on objects with non-rigid characteristics (e. g., cloth with varying shininess, a 3 D object with complicated surface attributes) which are difficult to render correctly by general modelbased CG techniques. Compared to previous BRDF based rendering approaches, our work more fully utilizes the 4 -dimensional lighting/viewing parameters of BTF, since it is essential for image composition of deformed objects. Further, our implementation sorts the data based on BRDF parameters, resulting in compact data representation. 1...|$|E
40|$|<b>Photorealistic</b> <b>image</b> <b>synthesis</b> {{deals with}} {{simulation}} {{of the light}} energy transport in an artificial scene in order to generate realistic looking snapshot of the scene from a given position. From a physical point of view we need to compute the value of radiance L reflected to the camera at every visible point of the scene. This task is very difficult and time consuming, because of recursion in the computation process. In our work, we chose a combination of two algorithms for realistic image synthesis: Photon Maps and Irradiance Cache. We further extended the algorithms in some aspects not addressed before. 3. Photon Maps 1. Describing the Light Transport: The Rendering Equation Rendering equation describes light transport in the space. Its simplified form is: It tells that the reflected (or outgoing) radiance L at a given point and in a given direction is o equal to the self emitted radiance L plus the radiance reflected from the surface from all the e incoming directions. It is a Fredholm's integral equation of second kind. It can be solved by stochastic Monte Carlo methods (ray tracing) or finite elements methods (radiosity). The algorithms we used are the modification of stochastic Monte Carlo methods...|$|E
50|$|In 2013, the Dutch {{branch of}} Terre des hommes, noting {{that efforts to}} combat child sex tourism in impoverished {{countries}} had resulted in pedophiles instead seeking victims online, joined forces with local animation company Lemz that at first preferred to remain anonymous to create the animated, <b>photorealistic</b> <b>image</b> of a 10-year-old Filipina girl.|$|R
50|$|Neuroplasticity is a {{key element}} of observing many pixel images. While two {{individuals}} will observe the same photons reflecting off a <b>photorealistic</b> <b>image</b> and hitting their retinas, someone whose mind has been primed with the theory of pointillism may see a very different image as the image is interpreted in the visual cortex.|$|R
40|$|ISBN 2 - 7261 - 1297 8 International audienceIn {{order to}} create a {{photorealistic}} Virtual Reality model, we have to record the appearance of the object from different directions under different illuminations. In this paper, we propose a method that renders <b>photorealistic</b> <b>images</b> from a small amount of data. First, we separate the images of the object into a diffuse reflection component and a specular reflection component by using linear polarizers. Then, we estimate the parameters of the reflection model for each component. Finally, we compress the difference between the input images and the rendered images by using wavelet transform. At the rendering stage, we first calculate the diffuse and specular reflection images from the reflection parameters, then add the difference decompressed by inverse wavelet transform into the calculated reflection images, and finally obtain the <b>photorealistic</b> <b>image</b> of the object...|$|R
40|$|For some time, {{rendering}} photorealistic synthetic {{images from}} observations of real objects {{has been a}} major research topic in the computer vision and computer graphics communities. An extensive amount of work in this research area has lead to a few representative schools: image-based rendering, inverse rendering and 3 D photography. However, each of these methods still suffers from several drawbacks, such as massive data storage, constraints on applicable objects, restrictive scenarios on application, etc. To overcome these problems, we propose twomethods. The first method, Eigen-Texture Rendering, handles the appearance variation of the target object on its own 2 D surface, enabling effective compression and interpolation with PCA thereby resulting in a compact representation of objects with arbitrary reflectance properties. The second method, Rendering from a Sparse Set of Images, recovers both the illumination distribution and reflection parameters simultaneously from input images, providing even more compact representation for <b>photorealistic</b> <b>image</b> <b>synthesis.</b> With the leverage of assuming a specific reflection model and restricting the treatable objects, the latter method provides more flexibility in application, i. e., fewer input images. In this dissertation, we present the theory of these methods, and report on the results obtained by applying the methods to real world objects...|$|E
40|$|<b>Photorealistic</b> <b>image</b> <b>synthesis</b> can be {{described}} by a path integral. This integral is numerically approximated by summing up contributions of transport paths that connect light sources and sensors like e. g. a camera or the eye. The paths are trajectories of Markov processes, whose edges are straight lines along rays of light and whose vertices are light scattering events. The goal of this thesis was to accelerate the simulation of light transport, nd new algorithms and data structures to e ciently trace rays, and to better approximate the distribution of light by simultaneously simulating an ensemble of paths instead of single trajectories, using quasi-Monte-Carlo methods. The main results of this thesis are contributions to both computer science and mathematics. We rst present new data structures and heuristics that feature a smaller memory footprint at improved numerical precision. In addition {{it is possible to}} ray trace even massive scenes in a strictly limited, a priori xed, memory block using rapid construction techniques that allow to rebuild the complete data structure at interactive frame rates. All e orts were combined in a uni ed framework that further allows one to build the acceleration hierarchy using an on demand policy and optionally balance the construction time versus the ray intersection time. Besides nding faster ray tracing algorithms, the total number of rays to be shot was reduced by mathematica...|$|E
40|$|<b>Photorealistic</b> <b>Image</b> <b>Synthesis</b> is a {{relevant}} research and application field in computer graphics, whose {{aim is to}} produce synthetic images that are undistinguishable from real ones. Photorealism is based upon accurate computational models of light material interaction, {{that allow us to}} compute the spectral intensity light field of a geometrically described scene. The fundamental methods are ray tracing and radiosity. While radiosity allows us to compute the diffuse component of the emitted and reflected light, applying ray tracing in a two pass solution we can also cope with non diffuse properties of the model surfaces. Both methods can be implemented to generate an accurate photometric distribution of light of the simulated environment. A still open problem is the visualization phase, whose purpose is to display the final result of the simulation model on a monitor screen or on a printed paper. The tone reproduction problem consists of finding the best solution to compress the extended dynamic range of the computed light field into the limited range of the displayable colors. Recently some scholars have addressed this problem considering the perception stage of image formation, so including a model of the human visual system in the visualization process. In this paper we present a working hypothesis to solve the tone reproduction problem of synthetic image generation, integrating Retinex perception model into the photo realistic image synthesis context...|$|E
50|$|Rendering {{converts}} a model into {{an image}} either by simulating light transport to get physically based <b>photorealistic</b> <b>images,</b> or by applying {{some kind of}} style as in non-photorealistic rendering. The two basic operations in realistic rendering are transport (how much light gets {{from one place to}} another) and scattering (how surfaces interact with light).|$|R
40|$|This paper {{discusses}} different techniques {{being used}} today to mimic all the subtle {{elements of a}} <b>photorealistic</b> <b>image</b> that are traditionally missing in a real-time generated one. We focus on issues of surface modeling and especially bump and horizon mapping, recent reflection modeling approaches, and illumination with environment mapping. We also {{take a look at}} popular shadow generation techniques. ...|$|R
5000|$|Lev Manovich {{likewise}} {{questions the}} indexical identity of motion pictures, rather labelling cinema a subgenre of painting, {{since it is}} possible to digitally modify frames, generate <b>photorealistic</b> <b>images</b> entirely using 3-D computer animation, and [...] "...to cut, bend, stretch and stitch digitised film images into something which has perfect photographic credibility, although it was never actually filmed." ...|$|R
