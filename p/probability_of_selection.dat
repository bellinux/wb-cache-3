167|10000|Public
25|$|As {{described}} above, {{systematic sampling}} is an EPS method, because all elements {{have the same}} <b>probability</b> <b>of</b> <b>selection</b> (in the example given, one in ten). It is not 'simple random sampling' because different subsets {{of the same size}} have different selection probabilities – e.g. the set {4,14,24,...,994} has a one-in-ten <b>probability</b> <b>of</b> <b>selection,</b> but the set {4,13,24,34,...} has zero <b>probability</b> <b>of</b> <b>selection.</b>|$|E
25|$|In {{the above}} example, not {{everybody}} {{has the same}} probability of selection; what makes it a probability sample {{is the fact that}} each person's probability is known. When every element in the population does have the same <b>probability</b> <b>of</b> <b>selection,</b> this is known as an 'equal probability of selection' (EPS) design. Such designs are also referred to as 'self-weighting' because all sampled units are given the same weight.|$|E
25|$|Nonprobability {{sampling}} is any {{sampling method}} where {{some elements of}} the population have no chance of selection (these are {{sometimes referred to as}} 'out of coverage'/'undercovered'), or where the <b>probability</b> <b>of</b> <b>selection</b> can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population.|$|E
3000|$|... that is, the <b>probabilities</b> <b>of</b> <b>selection</b> are {{proportional}} to the fitness, F_i. To generate each child, we start by randomly selecting a pair of parents, one at a time, from the current population following the corresponding PMF. For example, the <b>probability</b> <b>of</b> I_ 1 being selected is 210 times smaller than the probability that I_ 2 is selected.|$|R
40|$|For the {{statistical}} selection problem we formulate a general framework comprising both sequential sampling allocation and optimal design selection, {{for which the}} traditional <b>probability</b> <b>of</b> correct <b>selection</b> measure is inadequate. Therefore, we introduce the integrated <b>probability</b> <b>of</b> correct <b>selection</b> to better characterize the objective. In this framework, the usual <b>selection</b> policy <b>of</b> choosing the design with the largest sample mean as the estimate of the best is no longer optimal. Rather, the optimal selection policy is to choose the design that maximizes the posterior integrated <b>probability</b> <b>of</b> correct <b>selection,</b> which {{is a function of}} both the posterior mean and the correlation structure induced by the posterior variance. ...|$|R
5000|$|<b>Probabilities</b> <b>of</b> <b>selection</b> <b>of</b> the {{reservoir}} methods {{are discussed in}} Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to [...] (or, in case of Chao's procedure, to an arbitrary set <b>of</b> unequal <b>probabilities),</b> the second order selection probabilities depend on {{the order in which}} the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).|$|R
2500|$|A {{probability}} sample is a sample [...] {{in which every}} unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits {{makes it possible to}} produce unbiased estimates of population totals, by weighting sampled units according to their <b>probability</b> <b>of</b> <b>selection.</b>|$|E
50|$|As {{described}} above, {{systematic sampling}} is an EPS method, because all elements {{have the same}} <b>probability</b> <b>of</b> <b>selection</b> (in the example given, one in ten). It is not simple random sampling because different subsets {{of the same size}} have different selection probabilities - e.g. the set {4,14,24,...,994} has a one-in-ten <b>probability</b> <b>of</b> <b>selection,</b> but the set {4,13,24,34,...} has zero <b>probability</b> <b>of</b> <b>selection.</b>|$|E
50|$|This {{means that}} every {{student in the}} school has in any case {{approximately}} a 1 in 10 chance of being selected using this method. Further, all combinations of 100 students have the same <b>probability</b> <b>of</b> <b>selection.</b>|$|E
40|$|The present work aims to {{illustrate}} the criteria used for the determination <b>of</b> <b>probabilities</b> <b>of</b> <b>selection</b> <b>of</b> first and second stage units, given the available information, within the TLS design implemented for the survey of the Relevant National Interest Project (PRIN) on mobility of regional incoming tourism in Sicily and Sardinia. First and second stage units selection probabilities are necessary {{for the implementation of}} Hansen-Hurwitz class estimators for the estimation of the average number of stops made by tourists in Sicily, for the estimation of the share of un-observed tourism, and for the sampling errors associated...|$|R
40|$|In {{this paper}} we {{consider}} the model selection / discrimination among the three important lifetime distributions. All these three distributions have been used quite e®ectively to analyze lifetime data in the reliability analysis. We study the <b>probability</b> <b>of</b> correct <b>selection</b> using the maximized likelihood method, {{as it has been}} used in the literature. We further compute the asymptotic <b>probability</b> <b>of</b> correct <b>selection</b> and compare the theoretical and simulation results for di®erent sample sizes and for di®erent model parameters. The results have been extended for Type-I censored data also. The theoretical and simulation results match quite well. Two real data sets have been analyzed for illustrative purposes. We also suggest a method to determine the minimum sample size required to discriminate among the three distributions for a given <b>probability</b> <b>of</b> correct <b>selection</b> and a user speci¯ed protection level...|$|R
50|$|The main {{objective}} of OCBA is {{to maximize the}} <b>probability</b> <b>of</b> correct <b>selection</b> (PCS). PCS {{is subject to the}} sampling budget of a given stage of sampling τ.|$|R
50|$|Without {{modifying the}} {{estimated}} parameter, cluster sampling is unbiased when the clusters are {{approximately the same}} size. In this case, the parameter is computed by combining all the selected clusters. When the clusters are of different sizes, probability proportionate to size sampling is used. In this sampling plan, the probability of selecting a cluster is proportional to its size, so that a large clusters has a greater <b>probability</b> <b>of</b> <b>selection</b> than a small cluster. However, when clusters are selected with probability proportionate to size, {{the same number of}} interviews should be carried out in each sampled cluster so that each unit sampled has the same <b>probability</b> <b>of</b> <b>selection.</b>|$|E
5000|$|The first {{sample is}} chosen with {{probability}} {{proportional to the}} size of the x variate. The remaining n - 1 samples are chosen at random without replacement from the remaining N - 1 members in the population. The <b>probability</b> <b>of</b> <b>selection</b> under this scheme is ...|$|E
5000|$|Stochastic hill {{climbing}} is {{a variant of}} the basic {{hill climbing}} method. While basic hill climbing always chooses the steepest uphill move, [...] "stochastic hill climbing chooses at random from among the uphill moves; the <b>probability</b> <b>of</b> <b>selection</b> can vary with the steepness of the uphill move." ...|$|E
50|$|In {{probability}} and statistics, {{a mixture}} distribution is the <b>probability</b> distribution <b>of</b> a random variable that {{is derived from}} a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given <b>probabilities</b> <b>of</b> <b>selection,</b> and then {{the value of the}} selected random variable is realized. The underlying random variables may be random real numbers, or they may be random vectors (each having the same dimension), in which case the mixture distribution is a multivariate distribution.|$|R
40|$|Revised {{edition of}} Memorandum COSOR 96 - 22) This paper {{discusses}} a selection criterion that generalizes the well-known concept <b>of</b> indifference zone <b>selection</b> through a preference threshold. A population is preferred to another population if {{the difference in}} the sums of observed values exceeds a given nonnegative threshold value. We present an argument for this selection rule by modelling preference by imprecise previsions. We aim at guidelines to design a selection experiment, which is characterized by two numbers: the number of necessary observations per population, and the preference threshold. Next to the <b>probability</b> <b>of</b> correct <b>selection</b> we also need a second specification. In this paper we consider a <b>probability</b> <b>of</b> false <b>selection</b> that is strongly related to the minimum <b>probability</b> <b>of</b> correct <b>selection.</b> Based on this model the outcome of an experiment may be 'no selection', at least not based on strong preference of a single population. The ideas are presented through a simple selection problem for normal populations with common known variance. Although the theory has a frequentist nature, the derivation and justification <b>of</b> the <b>selection</b> rule through imprecise previsions relies on Bayesian foundations, and via this route we gain more insight into the selection criterion...|$|R
40|$|This paper {{discusses}} {{a selection}} criterion that generalizes the well-known concept <b>of</b> indifference zone <b>selection</b> through a preference threshold. A population is preferred to another population if {{the difference in}} the sums of observed values exceeds a given nonnegative threshold value. We present an argument for this selection rule by modelling preference by imprecise previsions. We aim at guidelines to design a selection experiment, which is characterized by two numbers: the number of necessary observations per population, and the preference threshold. Next to the <b>probability</b> <b>of</b> correct <b>selection</b> we also need a second specification. In this paper we consider a <b>probability</b> <b>of</b> false <b>selection</b> that is strongly related to the minimum <b>probability</b> <b>of</b> correct <b>selection.</b> Based on this model the outcome of an experiment may be no selection, at least not based on strong preference of a single population. The ideas are presented through a simple selection problem for normal populations with common known variance. Although the theory has a frequentist nature, the derivation and justification <b>of</b> the <b>selection</b> rule through imprecise previsions relies on Bayesian foundations, and via this route we gain more insight into the selection criterion...|$|R
50|$|A {{probability}} sample is a sample {{in which every}} unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits {{makes it possible to}} produce unbiased estimates of population totals, by weighting sampled units according to their <b>probability</b> <b>of</b> <b>selection.</b>|$|E
5000|$|Simple random sample: The {{sample data}} is a random {{sampling}} from a fixed distribution or population where every collection {{of members of}} the population of the given sample size has an equal <b>probability</b> <b>of</b> <b>selection.</b> Variants of the test have been developed for complex samples, such as where the data is weighted. Other forms can be used such as purposive sampling.|$|E
5000|$|Selection Bias: Selection bias {{occurs when}} some units have a differing <b>probability</b> <b>of</b> <b>selection</b> that is {{unaccounted}} {{for by the}} researcher. For example, some households have multiple phone numbers making them {{more likely to be}} selected in a telephone survey than households with only one phone number. This selection bias would be corrected by applying a survey weight equal to of phone numbers) to each household.|$|E
40|$|The paper {{considers}} carefully {{the problem}} of selecting a subset containing exactly the best K candidates from the pool prospective candidates wishing {{to go into the}} medical schools in Nigerian Universities. the method <b>of</b> ranking and <b>selection</b> is adopted. The <b>probability</b> <b>of</b> correct <b>selection</b> is considered...|$|R
40|$|Nowadays, beta and Kumaraswamy {{distributions}} are {{the most}} popular models to fit continuous bounded data. These models present some characteristics in common and to select one of them in a practical situation can be of great interest. With this in mind, in this paper we propose a method <b>of</b> <b>selection</b> between the beta and Kumaraswamy distributions. We use the logarithm of the likelihood ratio statistic (denoted by T_n, where n is the sample size) and obtain its asymptotic distribution under the hypotheses H_ B and H_ K, where H_ B (H_ K) denotes that the data come from the beta (Kumaraswamy) distribution. Since both models has the same number of parameters, based on the Akaike criterion, we choose the model that has the greater log-likelihood value. We here propose to use the <b>probability</b> <b>of</b> correct <b>selection</b> (given by P(T_n> 0) or P(T_n< 0) depending on the null hypothesis) instead of only to observe the maximized log-likelihood values. We obtain an approximation for the <b>probability</b> <b>of</b> correct <b>selection</b> under the hypotheses H_ B and H_ K and select the model that maximizes it. A simulation study is presented in order to evaluate the accuracy <b>of</b> the approximated <b>probabilities</b> <b>of</b> correct <b>selection.</b> We illustrate our method <b>of</b> <b>selection</b> in two applications to real data sets involving proportions. Comment: 17 pages. Submitted for publicatio...|$|R
3000|$|... on {{the basis}} of its head count. The head count is {{basically}} the frequency of a node of becoming cluster head so far. As the head count increases, the <b>probability</b> <b>of</b> its <b>selection</b> as cluster head decreases by certain magnitude.|$|R
50|$|In {{the above}} example, not {{everybody}} {{has the same}} probability of selection; what makes it a probability sample {{is the fact that}} each person's probability is known. When every element in the population does have the same <b>probability</b> <b>of</b> <b>selection,</b> this is known as an 'equal probability of selection' (EPS) design. Such designs are also referred to as 'self-weighting' because all sampled units are given the same weight.|$|E
50|$|Using this {{procedure}} each {{element in the}} population has a known and equal <b>probability</b> <b>of</b> <b>selection.</b> This makes systematic sampling functionally similar to simple random sampling (SRS). However {{it is not the}} same as SRS because not every possible sample of a certain size has an equal chance of being chosen (e.g. samples with at least two elements adjacent to each other will never be chosen by systematic sampling). It is however, much more efficient (if variance within systematic sample is more than variance of population).|$|E
5000|$|Example: We have a {{population}} of 5 units (A to E). We want to give unit A a 20% <b>probability</b> <b>of</b> <b>selection,</b> unit B a 40% probability, and so on up to unit E (100%). Assuming we maintain alphabetical order, we allocate each unit to the following interval: A: 0 to 0.2 B: 0.2 to 0.6 (= 0.2 + 0.4) C: 0.6 to 1.2 (= 0.6 + 0.6) D: 1.2 to 2.0 (= 1.2 + 0.8) E: 2.0 to 3.0 (= 2.0 + 1.0) ...|$|E
40|$|X 7 H E N {{samples taken}} from several » ^ stocks (or strains, varieties, etc.) are {{compared}} {{with a view to}} selecting the best stock, it is of interest to know the <b>probability</b> <b>of</b> correct <b>selection.</b> The prob-lem <b>of</b> computing this <b>probability</b> is com-plicated {{by the fact that the}} true stock means are usually either fixed but un-known, or else are themselves a more or less random sample from a larger popula-tion. Some mathematical expressions for the <b>probabilities</b> <b>of</b> correct <b>selection</b> in the latter case, assuming a normal distribu-tion of the true stock means, were given by Dunnett (1960), but they involve mul-tiple integrals which have not been numer-ically evaluated so far. On the other hand, the conditional <b>probability</b> <b>of</b> correct <b>selection</b> given any particular fixed configuration of the true stock means is much simpler to compute. Assuming (as usual) that the data for individual layers are normally distributed around the true stock means, Xi, • • •, XK (XI>X 2 > • • •>XK), with a pheno-typic variance denoted by <r 2, the proba-bility that the stock whose true mean is X will come out best in a single rando...|$|R
40|$|A {{modified}} regression-type estimator {{has been}} constructed for two-phase sampling using two auxiliary variables w, x and {{a measure of}} size z. The estimator has been developed by using probability proportional to a measure of size with replacement at first phase and equal probability sampling without replacement at the second phase. The mean square error of the modified estimator has been derived under arbitrary <b>probabilities</b> <b>of</b> <b>selection.</b> An empirical study {{has been made of}} the performance of new estimator with Roy’s estimator (2003). KEY WORDS Two-phase sampling; auxiliary variable; measure of size; arbitrary probabilities; regression-type estimator. 1...|$|R
40|$|Key words Subset <b>selection</b> <b>selection</b> <b>probability</b> <b>of</b> correct <b>selection</b> loss {{function}} best population generalized selection goal In selection problems the usual {{loss function}} is the 	 one ie the selection {{goal is to}} bound from below <b>probabilities</b> <b>of</b> making correct <b>selections</b> In the present paper a selection goal based on a general loss function is presented The populations have unknown location parameters and good populations are the ones with large values of this parameter The case of two populations is considered in detail For this case the selection rule is given and its performance is investigated for normal population...|$|R
50|$|Nonprobability {{sampling}} is any {{sampling method}} where {{some elements of}} the population have no chance of selection (these are {{sometimes referred to as}} 'out of coverage'/'undercovered'), or where the <b>probability</b> <b>of</b> <b>selection</b> can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population.|$|E
5000|$|In 2005, Feldstein {{was widely}} {{considered}} a leading candidate to succeed chairman Alan Greenspan as Chairman of the Federal Reserve Board. This {{was in part}} due to his prominence in the Reagan administration and his position as an economic advisor for the Bush presidential campaign. The New York Times wrote an editorial advocating that Bush choose either Feldstein or Ben Bernanke due to their credentials, and the week of the nomination The Economist predicted that the two men had the greatest <b>probability</b> <b>of</b> <b>selection</b> out of the field of candidates. Ultimately, the position went to Bernanke, possibly because Feldstein was a board member of AIG, which announced the same year that it would restate five years of past financial reports by $2.7 billion. Subsequently, AIG suffered a massive financial collapse that {{played a central role in}} the worldwide economic crisis of 2007-08 and the ensuing global recession. The firm was rescued only by multiple capital infusions by the U.S. Federal Reserve Bank, which extended a $182.5 billion line of credit. Although Feldstein was not explicitly linked to the accounting practices in question, he had served as a Director of AIG since 1988. In March 2007, the Lynde and Harry Bradley Foundation announced that one of four 2007 Bradley Prizes to honor outstanding achievement would be awarded to Feldstein. [...] On September 10, 2007, Feldstein announced that he would be stepping down as president of NBER effective June 2008.|$|E
30|$|Select {{two parents}} randomly, {{assigning}} higher <b>probability</b> <b>of</b> <b>selection</b> {{to the parents}} with a better fitness value.|$|E
40|$|Degradation {{analysis}} {{is used to}} analyze the useful lifetimes of systems, their failure rates, and various other system parameters like mean time to failure (MTTF), mean time between failures (MTBF), and the system failure rate (SFR). In many systems, certain possible parallel paths of execution that have greater chances of success are preferred over others. Thus we introduce here the concept of probabilistic parallel choice. We use binary and $n$-ary probabilistic choice operators in describing the <b>selections</b> <b>of</b> parallel paths. These binary and $n$-ary probabilistic choice operators are considered so as to represent the complete system (described as a series-parallel system) in terms <b>of</b> the <b>probabilities</b> <b>of</b> <b>selection</b> <b>of</b> parallel paths and their relevant parameters. Our approach allows us to derive new and generalized formulae for system parameters like MTTF, MTBF, and SFR. We use a generalized exponential distribution, allowing distinct installation times for individual components, and use this model to derive expressions for such system parameters...|$|R
40|$|This paper {{presents}} a selection criterion that generalizes the well-known concept <b>of</b> indifference zone <b>selection,</b> by introducing a preference threshold. A population is preferred to another population if {{the difference in}} the sums of observed values exceeds a given nonnegative threshold value. This requires explicit specification <b>of</b> both the <b>probability</b> <b>of</b> correct <b>selection</b> and the <b>probability</b> <b>of</b> false <b>selection,</b> the sum <b>of</b> which may be less than one. Hence, the model includes the possibility <b>of</b> no <b>selection</b> after an experiment, at least not based on real preference of a single population. The ideas are presented through a simple selection problem for normal populations with common known variance. Although the theory has a frequentist nature, it is shown how the selection rule relates to a formalism of preference within a theory of imprecise previsions that is built on Bayesian foundations...|$|R
40|$|In this paper, we {{consider}} the problem of selecting the best design from a discrete number of alternatives {{in the presence of}} a stochastic constraint via simulation experiments. The best design is the design with smallest mean of main objective among the feasible designs. The feasible designs are the designs of which constraint measure is below the constraint limit. The Optimal Computing Budget Allocation (OCBA) framework is used to tackle the problem. In this framework, we aim at maximizing the <b>probability</b> <b>of</b> correct <b>selection</b> given a computing budget by controlling the number of simulation replications. An asymptotically optimal allocation rule is derived. A comparison with Equal Allocation (EA) in the numerical experiments shows that the proposed allocation rule gains higher <b>probability</b> <b>of</b> correct <b>selection.</b> ...|$|R
