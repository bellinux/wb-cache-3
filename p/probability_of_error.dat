1824|10000|Public
2500|$|The white {{fields in}} this table show {{the number of}} hashes needed to achieve the given {{probability}} of collision (column) given a hash space of a certain size in bits (row). Using the birthday analogy: the [...] "hash space size" [...] resembles the [...] "available days", the [...] "probability of collision" [...] resembles the [...] "probability of shared birthday", and the [...] "required number of hashed elements" [...] resembles the [...] "required {{number of people in}} a group". One could of course also use this chart to determine the minimum hash size required (given upper bounds on the hashes and <b>probability</b> <b>of</b> <b>error),</b> or the probability of collision (for fixed number of hashes and <b>probability</b> <b>of</b> <b>error).</b>|$|E
2500|$|The {{class of}} {{problems}} that can be efficiently solved by quantum computers is called BQP, for [...] "bounded error, quantum, polynomial time". Quantum computers only run probabilistic algorithms, so BQP on quantum computers is the counterpart of BPP ("bounded error, probabilistic, polynomial time") on classical computers. It {{is defined as the}} set of problems solvable with a polynomial-time algorithm, whose <b>probability</b> <b>of</b> <b>error</b> is bounded away from one half. A quantum computer is said to [...] "solve" [...] a problem if, for every instance, its answer will be right with high probability. If that solution runs in polynomial time, then that problem is in BQP.|$|E
2500|$|The {{property}} of being prime is called primality. A simple but slow method of verifying the primality {{of a given}} number n is known as trial division. It consists of testing whether n is a multiple of any integer between 2 and [...] Algorithms much more efficient than trial division have been devised to test the primality of large numbers. These include the Miller–Rabin primality test, which is fast but has a small <b>probability</b> <b>of</b> <b>error,</b> and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. , the largest known prime number has 22,338,618 decimal digits.|$|E
40|$|The {{shortcomings}} of the classical approach are set forth, and the newer methods resulting from these shortcomings are explained. The problem was approached {{with the assumption that}} the <b>probabilities</b> <b>of</b> <b>error</b> were known, as well as without knowledge of the distribution <b>of</b> the <b>probabilities</b> <b>of</b> <b>error.</b> The advantages <b>of</b> the newer approach are discussed...|$|R
40|$|There is a known best {{possible}} {{upper bound on}} the <b>probability</b> <b>of</b> undetected <b>error</b> for linear codes. The $[n,k;q]$ codes with <b>probability</b> <b>of</b> undetected <b>error</b> meeting the bound have support of size $k$ only. In this note, linear codes of full support ($=n$) are studied. A {{best possible}} upper bound on the <b>probability</b> <b>of</b> undetected <b>error</b> for such codes is given, and the codes with <b>probability</b> <b>of</b> undetected <b>error</b> meeting this bound are characterized...|$|R
5000|$|... {{for some}} finite {{constants}} [...] More refined estimates including exponentially small <b>probability</b> <b>of</b> <b>errors</b> are developed in.|$|R
2500|$|A player might {{lie about}} his own share {{to gain access to}} other shares. A verifiable secret sharing (VSS) scheme allows players to be certain that no other players are lying about the {{contents}} of their shares, up to a reasonable <b>probability</b> <b>of</b> <b>error.</b> Such schemes cannot be computed conventionally; the players must collectively add and multiply numbers without any individual's knowing what exactly is being added and multiplied. Tal Rabin and Michael Ben-Or devised a multiparty computing (MPC) system that allows players to detect dishonesty {{on the part of the}} dealer or on part of up to one third of the threshold number of players, even if those players are coordinated by an [...] "adaptive" [...] attacker who can change strategies in realtime depending on what information has been revealed.|$|E
2500|$|Information theory {{studies the}} transmission, processing, extraction, and {{utilization}} of information. Abstractly, {{information can be}} thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was made concrete in 1948 by Claude Shannon in his paper [...] "A Mathematical Theory of Communication", in which [...] "information" [...] is thought of as a set of possible messages, where the goal is to send these messages over a noisy channel, and then to have the receiver reconstruct the message with low <b>probability</b> <b>of</b> <b>error,</b> in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.|$|E
60|$|She then {{spoke of}} the letter, repeating the whole of its {{contents}} {{as far as they}} concerned George Wickham. What a stroke was this for poor Jane! who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind, as was here collected in one individual. Nor was Darcy's vindication, though grateful to her feelings, capable of consoling her for such discovery. Most earnestly did she labour to prove the <b>probability</b> <b>of</b> <b>error,</b> and seek to clear the one without involving the other.|$|E
30|$|Increasing the {{constraint}} {{length of}} the convolutional code reduces the <b>probability</b> <b>of</b> <b>errors</b> at the receiver, without increasing the coding rate.|$|R
3000|$|... maximizing (9) {{consists}} of the L most probable error patterns. However, {{in order to achieve}} the small <b>probability</b> <b>of</b> decoding <b>error</b> Pe and approach the <b>probability</b> <b>of</b> decoding <b>error,</b> [...]...|$|R
3000|$|..., it is mathematically more {{convenient}} {{to design and}} analyze the list decoding algorithms assuming the <b>probability</b> <b>of</b> codeword <b>error.</b> Thus, {{we can assume that}} the decoding algorithm designed using the <b>probability</b> <b>of</b> codeword <b>error</b> also achieves a good BER performance [19].|$|R
6000|$|... [83] Professor Forbes {{gives the}} bearing of the Cervin {{from the top}} [...] of the Riffelhorn as 351°, or N. 9° W., supposing local {{attraction}} [...] to have caused an error of 65° to the northward, which would make [...] the true bearing N. 74° W. From the point just under the Riffelhorn [...] summit, e, in Fig. 78, at which my drawing was made, I found the [...] Cervin bear N. 79° W. without any allowance for attraction; the [...] disturbing influence would seem therefore confined, or nearly so, to [...] the summit a. I did not {{know at the time}} that there was any such [...] influence traceable, and took no bearing from the summit. For the [...] rest, I cannot vouch for bearings as I can for angles, as their [...] accuracy was of no importance to my work, and I merely noted them [...] with a common pocket compass and in the sailor's way (S. by W. and ½ [...] W. & C.), which involves the <b>probability</b> <b>of</b> <b>error</b> of from two to [...] three degrees {{on either side of the}} true bearing. The other drawing [...] in Plate +38+ was made from a point only a degree or two to the [...] westward of the village of Zermatt. I have no note of the bearing; [...] but it must be about S. 60° or 65° W.|$|E
5000|$|The <b>probability</b> <b>of</b> <b>error</b> of {{this scheme}} {{is divided into}} two parts: ...|$|E
5000|$|The minimum <b>probability</b> <b>of</b> <b>error</b> to {{distinguish}} between binary coherent states becomes, ...|$|E
50|$|The {{error is}} taken to be a random {{variable}} and as such has a probability distribution. Thus distribution can be used to calculate the <b>probabilities</b> <b>of</b> <b>errors</b> with values within any given range.|$|R
50|$|Pierre-Simon Laplace (1774) {{made the}} first attempt to deduce a rule for the {{combination}} of observations from {{the principles of the}} theory <b>of</b> <b>probabilities.</b> He represented the law <b>of</b> <b>probability</b> <b>of</b> <b>errors</b> by a curve and deduced a formula for the mean of three observations.|$|R
30|$|We {{introduce}} {{the concept of}} reliability in watermarking as the ability of assessing that a <b>probability</b> <b>of</b> false alarm is very low and below a given significance level. We propose an iterative and self-adapting algorithm which estimates very low <b>probabilities</b> <b>of</b> <b>error.</b> It performs much quicker and more accurately than a classical Monte Carlo estimator. The article finishes with applications to zero-bit watermarking (<b>probability</b> <b>of</b> false alarm, <b>error</b> exponent), and to probabilistic fingerprinting codes (<b>probability</b> <b>of</b> wrongly accusing a given user, code length estimation).|$|R
5000|$|The {{number of}} tests needed for a zero <b>probability</b> <b>of</b> <b>error</b> scales as [...]|$|E
5000|$|The {{number of}} tests needed for an {{asymptotically}} small average <b>probability</b> <b>of</b> <b>error</b> scales as [...]|$|E
50|$|Therefore, in practice, {{there is}} no penalty {{associated}} with accepting a small <b>probability</b> <b>of</b> <b>error,</b> since with a little care the <b>probability</b> <b>of</b> <b>error</b> can be made astronomically small. Indeed, even though a deterministic polynomial-time primality test has since been found (see AKS primality test), it has not replaced the older probabilistic tests in cryptographic software nor is it expected {{to do so for}} the foreseeable future.|$|E
40|$|Abstract — In this work, we {{introduce}} {{the notion of}} the diversity gain region for a multi-user channel. This region specifies the set of diversity-gain vectors that are simultaneously achievable by all users in the multi-user channel. This is done by associating different <b>probabilities</b> <b>of</b> <b>error</b> for different users, contrary to the traditional approach where a single <b>probability</b> <b>of</b> system <b>error</b> is considered. We derive an inner bound (achievable region) and an outer bound for the diversity gain region of a MIMO fading broadcast channel. I...|$|R
40|$|Abstract- The authors {{present a}} method for {{computing}} the <b>probability</b> <b>of</b> undetected <b>error</b> <b>of</b> hybrid ARQ over the Gilbert-Elliott channel. The method described by El-liott is extended to the case where the code is used for error detection and error correction simultaneously (hybrid ARQ. type-I). This analytical approach to the evaluation <b>of</b> the <b>probability</b> <b>of</b> undetected <b>error</b> allows to calculate the throughput {{as a function of}} the code rate and length, and to design systems that maximize the throughput, given the channel parameters and the desired undetected error probability. Keywords- Hybrid ARQ, Gilbert-Elliott channel, Through-put, <b>Probability</b> <b>of</b> undetected <b>error...</b>|$|R
40|$|It is easy {{to create}} {{parallel}} genetic algorithm software with master-slave type paralelization on a cluster of workstations. In a real situation the <b>probability</b> <b>of</b> <b>errors</b> in communication or {{in some of the}} slave processes during a long calculation is significant. In this paper we deal with different error handling strategies in master-slave type paralelization of standard GA algorithms and show results of test calculations. Our simulations are close to real applications {{in the sense that we}} examine the best achieved objective function value at a fixed wall clock time with different error handling strategies depending on the <b>probability</b> <b>of</b> <b>errors</b> and number <b>of</b> processors. Using these results we make suggestions on the selection <b>of</b> a good <b>error</b> handling method in different optimization problems...|$|R
50|$|The {{converse}} is also important. If , an arbitrarily small <b>probability</b> <b>of</b> <b>error</b> is not achievable. All codes {{will have}} a <b>probability</b> <b>of</b> <b>error</b> greater than a certain positive minimal level, and this level increases as the rate increases. So, information cannot be guaranteed to be transmitted reliably across a channel at rates beyond the channel capacity. The theorem {{does not address the}} rare situation in which rate and capacity are equal.|$|E
5000|$|The <b>probability</b> <b>of</b> <b>error</b> for DPSK is {{difficult}} to calculate in general, but, {{in the case of}} DBPSK it is: ...|$|E
5000|$|We {{can observe}} that as [...] goes to infinity, if [...] for the channel, the <b>probability</b> <b>of</b> <b>error</b> {{will go to}} 0.|$|E
30|$|As seen in Figure 2, {{both the}} IPC with {{confirmation}} and the IPC with stability check {{result in the}} same <b>probability</b> <b>of</b> code-word <b>error</b> as the standard schedule for the rate- 1 / 2 WiMAX code. (For all three algorithms, almost all code-word errors are known decoder failures.) The <b>probability</b> <b>of</b> code-word <b>error</b> with each schedule is 10 - 3 if the signal-to-noise ratio is 1.97 dB, and it is 10 - 4 if the signal-to-noise ratio is 2.18 dB. Agreement <b>of</b> the <b>error</b> <b>probabilities</b> for the three schedules is also observed when considering the <b>probability</b> <b>of</b> bit <b>error</b> and when comparing the performance for the other three WiMAX codes.|$|R
40|$|Low-density parity-check {{codes are}} {{commonly}} decoded using iterative message-passing decoders, {{such as the}} min-sum and sum-product decoders. Computer sim-ulations demonstrate that these suboptimal decoders are capable <b>of</b> achieving low <b>probability</b> <b>of</b> bit <b>error</b> at signal-to-noise ratios close to capacity. However, current methods for analyzing {{the behavior of the}} min-sum and sum-product decoders fails to produce usable bounds on the <b>probability</b> <b>of</b> bit <b>error.</b> Thus, the resulting <b>probability</b> <b>of</b> bit <b>error</b> when using these decoders remains largely unknown for signal-to-noise ra-tios beyond the reach of simulation. For this reason, it is worth considering alternative methods for decoding low-density parity-check codes. New methods for decoding low-density parity-check codes, known as finite tree-based decoders, are presented as alternative decoders for low-density parity-check codes. The goal of the finite tree-based decoders is to achieve <b>probability</b> <b>of</b> bit <b>error</b> comparable to that of the min-sum and sum-product decoders, while allowing fo...|$|R
40|$|This Thesis {{deals with}} the power of a {{statistical}} test and the associated problem of determining the appropriate sample size. It should be large enough to meet the requirements <b>of</b> the <b>probabilities</b> <b>of</b> <b>errors</b> <b>of</b> both the first and second kind. The aim of this Thesis is to demonstrate theoretical methods that result in derivation of formulas for minimum sample size determination. For this Thesis, three important probability distributions have been chosen: Normal, Bernoulli, and Exponential...|$|R
50|$|He {{presented}} {{performance analysis}} based on LDT (Large Deviations Theory) and presented the trade-off between compression rate, distortion level and <b>probability</b> <b>of</b> <b>error.</b>|$|E
5000|$|He {{presented}} {{performance analysis}} based on large deviations theory (LDT) and presented the trade-off between compression rate, distortion level, and <b>probability</b> <b>of</b> <b>error.</b>|$|E
5000|$|The {{term in the}} {{exponent}} {{should be}} maximized over [...] {{in order to achieve}} the tightest upper bound on the <b>probability</b> <b>of</b> <b>error.</b>|$|E
50|$|Turning now to non-adaptive group testing, {{significant}} gains {{can be made}} by not requiring that the group-testing procedure be certain to succeed, but rather to have some non-zero <b>probability</b> <b>of</b> mis-labelling an item. In particular, it is known that zero-error algorithms require significantly more tests asymptotically (in the number of defective items) than algorithms that allow asymptotically small <b>probabilities</b> <b>of</b> <b>error.</b>|$|R
40|$|In today's {{world to}} {{determine}} the reliability of software are widely used Jelinski-Moranda and Schick-Wolverton reliability models. But these models do {{not take into account}} the possibility that the developer can allow the appearance <b>of</b> new <b>errors</b> in correction <b>of</b> previously detected <b>errors.</b> The article discusses the option of modification of these models, taking into account the <b>probability</b> <b>of</b> emergence <b>of</b> <b>error</b> in correcting previously detected, that did not be before finalize the software. The article also contains studies <b>of</b> the <b>probability</b> <b>of</b> new <b>errors</b> when removing previously detected...|$|R
40|$|This {{paper is}} devoted to the problem <b>of</b> <b>error</b> {{detection}} with quantum codes. In the first part we examine possible problem settings for quantum error detection. Our goal is to derive a functional that describes the <b>probability</b> <b>of</b> undetected <b>error</b> under natural physical assumptions concerning transmission with error detection with quantum codes. We discuss possible transmission protocols with stabilizer and unrestricted quantum codes. The set of results proved in part I shows that in all the cases considered the average <b>probability</b> <b>of</b> undetected <b>error</b> for a given code is essentially given by one and the same function of its weight enumerators. This enables us to give a consistent definition <b>of</b> the undetected <b>error</b> event. In part II we derive bounds on the <b>probability</b> <b>of</b> undetected <b>error</b> for quantum codes. In the final section of the paper we examine polynomial invariants of quantum codes and show that Rains’s “unitary weight enumerators” [16] are known for classical codes under the name of binomial moments of the distance distribution. As in the classical situation, they provide an alternative expression for the <b>probability</b> <b>of</b> undetected <b>error...</b>|$|R
