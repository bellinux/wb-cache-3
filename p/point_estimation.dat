710|669|Public
5|$|Emphasis {{is placed}} not on actual {{arithmetic}} computation, {{but rather on}} conceptual understanding and interpretation. The course curriculum is organized around four basic themes; the first involves exploring data and covers 20–30% of the exam. Students are expected to use graphical and numerical techniques to analyze distributions of data, including univariate, bivariate, and categorical data. The second theme involves planning and conducting a study and covers 10–15% of the exam. Students {{must be aware of}} the various methods of data collection through sampling or experimentation and the sorts of conclusions that can be drawn from the results. The third theme involves probability and its role in anticipating patterns in distributions of data. This theme covers 20–30% of the exam. The fourth theme, which covers 30–40% of the exam, involves statistical inference using <b>point</b> <b>estimation,</b> confidence intervals, and significance tests.|$|E
2500|$|Since the program's inception, {{extensive}} {{testing has}} been conducted on the JLENS system. [...] For example, in September 2005 the program successfully completed a two-day functional review, which examined fire-control radar, surveillance radar, processing station, communication system and platform. [...] In 2012, tests were conducted December 6–7 at White Sands Missile Range in New Mexico. [...] The system tracked four threats similar to tactical ballistic missiles, and it met its primary and secondary goals, including launch <b>point</b> <b>estimation,</b> ballistic tracking and discrimination performance.|$|E
5000|$|Efficient {{formulas}} {{and references}} for diagonal kernel <b>point</b> <b>estimation</b> {{can be found}} in [...] and [...]|$|E
40|$|This {{presentation}} {{shows the}} preliminary work carried out till the date about the Nephrops FU 30 -specific Harvest Ratio reference <b>points</b> <b>estimation.</b> This presentation shows the preliminary work carried out till the date about the Nephrops FU 30 -specific Harvest Ratio reference <b>points</b> <b>estimation.</b> EU by the European Maritime and Fisheries Fund (EMFF) and ISUNEPCA projec...|$|R
30|$|Transform model estimation: Depending on {{corresponding}} control <b>points,</b> <b>estimation</b> {{of transformation}} is done. This estimation {{has to find}} the possible transformation functions and their respective parameters, so that once applied to transformed image; it becomes closest to the original one.|$|R
40|$|Linear {{polarization}} measurements {{provide access}} to two quantities, the degree (DOP) and the angle of polarization (AOP). The aim of this work is to give a complete and concise overview of how to analyze polarimetric measurements. We review interval estimations for the DOP with a frequentist and a Bayesian approach. <b>Point</b> <b>estimations</b> for the DOP and interval estimations for the AOP are further investigated with a Bayesian approach to match observational needs. <b>Point</b> and interval <b>estimations</b> are calculated numerically for frequentist and Bayesian statistics. Monte Carlo simulations are performed to clarify {{the meaning of the}} calculations. Under observational conditions, the true DOP and AOP are unknown, so that classical statistical considerations - based on true values - are not directly usable. In contrast, Bayesian statistics handles unknown true values very well and produces <b>point</b> and interval <b>estimations</b> for DOP and AOP, directly. Using a Bayesian approach, we show how to choose DOP <b>point</b> <b>estimations</b> based on the measured signal-to-noise ratio. Interval estimations for the DOP show great differences in the limit of low signal-to-noise ratios between the classical and Bayesian approach. AOP interval estimations that are based on observational data are presented for the first time. All results are directly usable via plots and parametric fits. Comment: 11 pages, 14 figures, 3 table...|$|R
5000|$|For <b>point</b> <b>estimation</b> (estimating {{a single}} {{value for the}} total, [...] ), the minimum-variance {{unbiased}} estimator (MVUE, or UMVU estimator) is given by: ...|$|E
5000|$|Monotone likelihoods {{are used}} in several areas of {{statistical}} theory, including <b>point</b> <b>estimation</b> and hypothesis testing, {{as well as in}} probability models.|$|E
50|$|In statistics, {{interval}} estimation {{is the use}} of sample data to calculate an interval of possible (or probable) values of an unknown population parameter, in contrast to <b>point</b> <b>estimation,</b> which is a single number. Jerzy Neyman (1937) identified {{interval estimation}} ("estimation by interval") as distinct from <b>point</b> <b>estimation</b> ("estimation by unique estimate"). In doing so, he recognised that then-recent work quoting results {{in the form of an}} estimate plus-or-minus a standard deviation indicated that interval estimation was actually the problem statisticians really had in mind.|$|E
30|$|As {{described}} in Section 3, subblock parallelism {{takes place at}} frame level and requires initializations. Proper initialization is mandatory to achieve correct decoding since information on recursion metrics is available at frame ending points, but not at subblock ending <b>points.</b> <b>Estimation</b> of undetermined information can be obtained either by acquisition or by message passing between neighboring subblocks.|$|R
40|$|AbstractExact {{expressions}} {{are derived}} for the probability density function (pdf), {{cumulative distribution function}} (cdf), shape of the pdf, asymptotics of the pdf and the cdf, Laplace transform, moment properties and the order statistics properties of the product of m independent gamma and n independent Pareto random variables. Computer programs are provided for computing the probability density function and the associated percentage <b>points.</b> <b>Estimation</b> issues by the methods of moments and maximum likelihood are discussed...|$|R
40|$|We {{consider}} a stochastic gradient process, {{which is used}} for the esti-mation of a conditional expectation: Xn+ 1 = Xn−an∇xφ(Vn,Xn) (φ(Vn,Xn) − Un). We give one theorem of almost sure convergence and one theorem of mean quadratic convergence. Several applications are given: lin-ear estimation of a conditional expectation, sequential estimation of law mixture parameters in classification, estimation of an observable function in random <b>points,</b> <b>estimation</b> of a function h(x) = E[Z(x) ], estimation of a linear regression parameters, estimation of baysian dis-criminant function...|$|R
5000|$|Andrew D. Martin and Kevin M. Quinn. 2002. [...] "Dynamic Ideal <b>Point</b> <b>Estimation</b> via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999." [...] Political Analysis. 10:134-153.|$|E
50|$|In general, <b>point</b> <b>estimation</b> {{should be}} contrasted with {{interval}} estimation: such interval estimates are typically either confidence intervals {{in the case}} of frequentist inference, or credible intervals {{in the case of}} Bayesian inference.|$|E
5000|$|... (with John D. Storey and Jonathan E. Taylor) Strong control, {{conservative}} <b>point</b> <b>estimation</b> and simultaneous conservative {{consistency of}} false discovery rates: a unified approach, Journal of the Royal Statistical Society, Series B 66, #1 (February 2004), pp. 187-205, [...]|$|E
40|$|Proposes a fuzzy {{multistage}} ISODATA algorithm for {{the classification}} of remotely sensed data. The concept of fuzzy set theory is used in resolving impreciseness present in the data. The proposed algorithm involves four stages; data reduction, computation of seed <b>points,</b> <b>estimation</b> of number of classes, and classification. In data reduction, the remotely sensed data are converted into symbolic form using a fuzzy α-cut technique, which minimizes computation time and memory. Computation of seed <b>points</b> and <b>estimation</b> of number of classes uses farthest membership function and fuzzy measure respectively. In the final stage, the remotely sensed data are classified using fuzzy equivalence relation. The classification results of remotely sensed data of an IRS 1 B LISS-II image of Nagarhole forest in India are encouraging. The results signify that the algorithm is efficient with less computational time and less memor...|$|R
40|$|International audienceIn {{this paper}} we present the Toulouse Vanishing Points Dataset, a public {{photographs}} database of Manhattan scenes taken with an iPad Air 1. The {{purpose of this}} dataset is the evaluation of vanishing <b>points</b> <b>estimation</b> algorithms. Its originality is the addition of Inertial Measurement Unit (IMU) data synchronized with the camera under the form of rotation matrices. Moreover, contrary to existing works which provide vanishing points of reference {{in the form of}} single points, we computed uncertainty regions. The Toulouse Vanishing Points Dataset is publicly available at [URL]...|$|R
40|$|Least {{absolute}} deviation regression is applied using a fixed {{number of points}} for all values of the index to estimate the index and scale parameter of the stable distribution using regression methods based on the empirical characteristic function. The recognized fixed number of <b>points</b> <b>estimation</b> procedure uses ten points in the interval zero to one, and least squares estimation. It is shown that using the more robust least absolute regression based on iteratively re-weighted least squares outperforms the least squares procedure with respect to bias and also mean square error in smaller samples...|$|R
5000|$|In statistics, <b>point</b> <b>estimation</b> {{involves}} the use of sample data to calculate a single value (known as a statistic) which is to serve as a [...] "best guess" [...] or [...] "best estimate" [...] of an unknown (fixed or random) population parameter.|$|E
5000|$|When {{the word}} [...] "estimator" [...] is used without a qualifier, it usually refers to <b>point</b> <b>estimation.</b> The {{estimate}} {{in this case}} is a single point in the parameter space. There also exists an other type of estimator: interval estimators, where the estimates are subsets of the parameter space.|$|E
5000|$|The {{minimum message length}} {{principle}} (Wallace and Boulton, 1968, WB1968) [...] - [...] an information-theoretic {{principle in}} statistics, econometrics, machine learning, inductive inference and knowledge discovery which can be seen both as a mathematical formalisation of Occams Razor and as an invariant Bayesian method of model selection and <b>point</b> <b>estimation,</b> ...|$|E
3000|$|... {{by pulling}} the slider with a thread as {{described}} in Section “Transient on an isotropic surface” and measuring the pulling force at the fixed <b>point</b> (The <b>estimation</b> of F [...]...|$|R
40|$|In reality many {{time series}} are {{non-linear}} and non-gaussian. They show the characters like flat stretches, bursts of activity and outliers. (Bivariate) Mixture Transition Distribution model {{are introduced to}} study these time series data. EM algorithm is used for <b>point</b> <b>estimations</b> of parameters. However as is known, for many mixture models, the likelihoods couldn't be maximized since {{they will go to}} infinity. Number of mixtures should be prefixed in this way but in many realities it is unknown. In our research, Bayesian methods are used to solve these problems. When the posterior is got, EM algorithm is used to maximize the posterior. Under some conditions these estimates are proved to be consistent. The second method is using MCMC to sample from the posterior and now number of mixtures itself can be treated as a random variable. Two methods for MCMC sampling are used. The first is Birth-Death process: if a birth happens, a new mixture component is added; if a death happens, an existed mixture will disappear. The second is Dirichlet process mixtures where we choose Dirichlet process priors for the parameters. When using MCMC, not only <b>point</b> <b>estimations</b> but also interval estimations can be constructed. For all these methods we do simulations to compare Bayesian methods with non-Bayesian methods and to show the excellence of Bayesian methods...|$|R
40|$|One of {{the major}} problem facing the data {{modelling}} at social area is multicollinearity. Multicollinearity can have {{significant impact on the}} quality and stability of the fitted regression model. Common classical regression technique by using Least Squares estimate is highly sensitive to multicollinearity problem. In such a problem area, Partial Least Squares Regression (PLSR) is a useful and flexible tool for statistical model building; however, PLSR can only yields <b>point</b> <b>estimations.</b> This paper will construct the interval estimations for PLSR regression parameters by implementing Jackknife technique to poverty data. A SAS macro programme is developed to obtain the Jackknife interval estimator for PLSR...|$|R
50|$|Petković is {{a leading}} expert {{in the theory of}} {{iterative}} processes for solving nonlinear equations and Interval mathematics. He wrote over 255 academic papers (140 in Thomson Reuters' SCI journals) and 24 books, including four monographs Iterative Methods for Simultaneous Inclusion of Polynomial Zeros, Springer-Verlag 1989, Complex Interval Arithmetic and Its Applications, Wiley-VCH 1998, <b>Point</b> <b>Estimation</b> of Root Finding Methods, Springer-Verlag 2008, and Multipoint Methods for Solving Nonlinear Equations, Elsevier 2012.|$|E
50|$|A {{stochastic}} investment model {{tries to}} forecast how returns and prices on different assets or asset classes, (e. g. equities or bonds) vary over time. Stochastic models are not applied for making <b>point</b> <b>estimation</b> rather interval estimation {{and they use}} different stochastic processes. Investment models can be classified into single-asset and multi-asset models. They are often used for actuarial work and financial planning to allow optimization in asset allocation or asset-liability-management (ALM).|$|E
50|$|Instead of, or in {{addition}} to, <b>point</b> <b>estimation,</b> interval estimation {{can be carried}} out, such as confidence intervals.These are easily computed, based on the observation that the probability that k samples will fall in an interval covering p of the range (0 ≤ p ≤ 1) is pk (assuming in this section that draws are with replacement, to simplify computations; if draws are without replacement, this overstates the likelihood, and intervals will be overly conservative).|$|E
40|$|This paper studied {{statistical}} analysis {{of a new kind}} of accelerated life testing which combined progressively and constantly life test. The <b>point</b> <b>estimations</b> of the parameters in Exponential distribution and acceleration equation are obtained based on type II progressively censored data. Furthermore, we give the interval estimations of the parameters by Bootstrap sample. Besides, the estimations of the failure rate, the reliability functions and mean life are given at normal stress level, as well as the approximated unbiased estimations of failure rate and mean life. In the end, an illustrative numerical example is examined by means of the Monte-Carlo simulation...|$|R
30|$|In Eq. (14), the {{left-hand}} side matrix shows the dissimilarities among data points {{and the right}} vector in the Eq. (14) describes the dissimilarities between the <b>estimation</b> <b>point</b> and the data points. As you noticed, output of kriging interpolation method is always the estimated value of an unvisited <b>point</b> and <b>estimation</b> error variance. The estimation error variance {{can be used as}} a measure of how accurate is the kriging interpolation method. Estimation error variance can be obtained from Eq. (8).|$|R
30|$|The {{extensive}} {{literature review}} to identify all dew <b>point</b> pressure <b>estimation</b> models, classifying them into two groups, comparing {{the performance of}} each, and then suggesting which one performs better based on our extensive database.|$|R
50|$|In {{distribution}} regression, {{the goal}} is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and <b>point</b> <b>estimation</b> problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images.|$|E
50|$|Since the program's inception, {{extensive}} {{testing has}} been conducted on the JLENS system. For example, in September 2005 the program successfully completed a two-day functional review, which examined fire-control radar, surveillance radar, processing station, communication system and platform. In 2012, tests were conducted December 6-7 at White Sands Missile Range in New Mexico. The system tracked four threats similar to tactical ballistic missiles, and it met its primary and secondary goals, including launch <b>point</b> <b>estimation,</b> ballistic tracking and discrimination performance.|$|E
50|$|In nontechnical terms, a {{confidence}} distribution {{is a function}} of both the parameter and the random sample, with two requirements. The first requirement (R1) simply requires that a CD should be a distribution on the parameter space. The second requirement (R2) sets a restriction on the function so that inferences (point estimators, confidence intervals and hypothesis testing, etc.) based on the confidence distribution have desired frequentist properties. This is similar to the restrictions in <b>point</b> <b>estimation</b> to ensure certain desired properties, such as unbiasedness, consistency, efficiency, etc.|$|E
40|$|AbstractThis study {{proposes a}} new {{efficient}} methodology to estimate an engagement between cutter of endmill and machined workpiece surface in continuous multi axis controlled machining process. In order to shorten calculation {{time for the}} engagement domain and realize real-time prediction of instantaneous cutting force, a new concept adapting ultra-parallel processing technology is proposed. In the proposed concept, the engagement domain is divided into {{a large number of}} <b>estimation</b> <b>point.</b> Then, inclusion <b>estimation</b> process between the <b>estimation</b> <b>point</b> and the machined workpiece volume is decomposed into simple estimation processes about tool swept volume. The developed prototype system shows good performance for complicate NC programs generated by commercial CAM system...|$|R
40|$|AbstractThe {{present study}} {{aimed to develop}} a new simulation-based {{algorithm}} for finding the local minimums of multi-level inventory control systems with random parameters. The optimization refers to minimization of cost function along with maximization of customer service level of the units. In developing the algorithm, the authors were determined to achieve a local optimum point through linear localization of limitations and Genetic Algorithm. Since <b>point</b> <b>estimations</b> of goal function and repletion rates have been done through Monte Carlo Simulation Technique, the statistical test have been employed for examining possibility and improvability of solutions. Finally, the proposed algorithm has been used in an example with three levels...|$|R
50|$|Ultrasound {{has also}} been used to measure {{subcutaneous}} fat thickness, and by using multiple <b>points</b> an <b>estimation</b> of body composition can be made. Ultrasound {{has the advantage of}} being able to also directly measure muscle thickness and quantify intramuscular fat.|$|R
