38|130|Public
50|$|Fujitsu {{had built}} a <b>prototype</b> <b>vector</b> co-processor known as the F230-75, which was {{installed}} attached to their own mainframe machines in the Japanese Atomic Energy Commission and National Aerospace Laboratory in 1977. The processor was similar in most ways to the famed Cray-1, but did not have vector chaining capabilities and was therefore somewhat slower. Nevertheless, the machines were rather inexpensive, and during the late 1970s supercomputers were seen {{as a source of}} national pride, and an effort started to commercialize the design by combining it with a scalar processor to create an all-in-one design.|$|E
3000|$|Generating the classes—using the {{determined}} model parameters, a SOM was generated, {{with similar}} distributions grouped. For each grouping, a <b>prototype</b> <b>vector,</b> characterizing the group, is generated, with each data point {{assigned to a}} group based on the similarity to the <b>prototype</b> <b>vector.</b> This created the new categorical VDC variable, that captures a class of vitrinite reflectance distribution {{and can be used}} as a parameter in prediction models [...]...|$|E
30|$|Centroid based (CB) {{classifier}} calculates centroid vector or <b>prototype</b> <b>vector</b> {{for each}} {{class in the}} training dataset. Centroid vector is the central point of the class and may not represent an actual training data. The distance of each test document is calculated with the <b>prototype</b> <b>vector</b> of the class and is classified based on similarity with it. Its performance depends on the chosen centroid vectors. It is efficient since time and space complexities are proportional {{to the number of}} classes rather than training documents. To double the training data reverse of reviews are generated in (Xia et al. 2015) by inverting the sentiment terms and their labels. Using both sets of training data with Mutual Information (MI) the results were improved when only selected reviews were inverted. External dictionary WordNet is used to generate inverse for sentiment terms, however, pseudo-antonyms can be generated internally using the corpus.|$|E
3000|$|... [...]. Each {{set from}} {C} {{contains}} the feature vectors {{assigned to the}} n th cluster, and has an associated set of <b>prototype</b> <b>vectors,</b> v [...]...|$|R
40|$|In {{this paper}} we show {{how to build}} global and local RBF models once the Self-Organizing Map has been trained using the Vector-Quantized Temporal Associative Memory (VQTAM) method. Through the VQTAM, <b>prototype</b> <b>vectors</b> (centroids) of input {{clusters}} are associated with <b>prototype</b> <b>vectors</b> of output clusters, so that the SOM can learn dynamic input-output mappings in a very simple and effective way. Global RBF models are built using all the input prototypes as centers of M gaussian basis functions, while the hidden-to-output layer weights are given by the output prototypes. Local RBF models are build in a similar fashion, but using only K M neurons. We evaluate the proposed RBF models and other global/local neural models in a complex nonlinear channel equalization task...|$|R
40|$|The Self-Organizing Map (SOM) is an {{efficient}} tool for visualization and clustering of multidimensional data. It transforms the input vectors on two-dimensional grid of <b>prototype</b> <b>vectors</b> and orders them. The ordered <b>prototype</b> <b>vectors</b> {{are easier to}} visualize and explore than the original data. Mobile networks produce {{a huge amount of}} spatiotemporal data. The data consists of parameters of base stations (BS) and quality information of calls. There are two alternatives in starting the data analysis. We can build either a general one-cell-model trained using state vectors from all cells, or a model of the network using state vectors with parameters from all mobile cells. In both methods, further analysis is needed to understand the reasons for various operational states of the entire network. ...|$|R
40|$|Abstract: To apply {{run-time}} reconfiguration {{there is}} {{need for an}} appropriate model to understand reconfiguration. System on chip technology allowed the use of complex low gate/price new type of FPGA. Modelling reconfiguration and using Matlab tools is possible to simulate and rapid <b>prototype</b> <b>vector</b> control systems. The paper presents the reconfigurable models as introduced by Luk, Cheung, Shirazi and Athanas and will present the reconfiguration model for vector control systems...|$|E
40|$|The Hypercube Segmentation {{problem was}} {{recently}} introduced by Kleinberg et al., {{along with several}} algorithms that select each segment's <b>prototype</b> <b>vector</b> from the segment. The algorithms were shown to have an approximation ratio of at least 2 (√(2) - 1) = 0. 828. We show that a lemma used in this proof is tight, and that the asymptotic approximation ratio of no algorithm of this type can exceed 5 / 6 = 0. 833...|$|E
40|$|In {{this paper}} a further {{generalization}} of differential evolution based data classification method is proposed, demonstrated and initially evaluated. The differential evolution classifier is a nearest <b>prototype</b> <b>vector</b> based classifier that applies a global optimization algorithm, differential evolution, {{for determining the}} optimal values for all free parameters of the classifier model during the training phase of the classifier. The earlier version of differential evolution classifier that applied individually optimized distance measure for each new data set to be classified is generalized here so, that instead of optimizing a single distance measure for the given data set, we take a further step by proposing an approach where distance measures are optimized individually for each feature of the data set to be classified. In particular, distance measures for each feature are selected optimally from a predefined pool of alternative distance measures. The optimal distance measures are determined by differential evolution algorithm, which is also determining the optimal values for all free parameters of the selected distance measures in parallel. After determining the optimal distance measures for each feature together with their optimal parameters, we combine all featurewisely determined distance measures to form a single total distance measure, {{that is to be}} applied for the final classification decisions. The actual classification process is still based on the nearest <b>prototype</b> <b>vector</b> principle; A sample belongs to the class represented by the nearest <b>prototype</b> <b>vector</b> when measured with the above referred optimized total distance measure. During the training process the differential evolution algorithm determines optimally the class vectors, selects optimal distance metrics for each data feature, and determines the optimal values for the free parameters of each selected distance measure. Based on experimental results with nine well known classification benchmark data sets, the proposed approach yield a statistically significant improvement to the classification accuracy of differential evolution classifier. Web of Science 40104082407...|$|E
40|$|This article {{deals with}} a {{recognition}} system using an algorithm based on the Principal Component Analysis (PCA) technique. The recognition system consists only of a PC and an integrated video camera. The algorithm is developed in MATLAB language and calculates the eigenfaces considered as features of the face. The PCA technique {{is based on the}} matching between the facial test image and the training <b>prototype</b> <b>vectors.</b> The mathcing score between the facial test image and the training <b>prototype</b> <b>vectors</b> is calculated between their coefficient vectors. If the matching is high, we have the best recognition. The results of the algorithm based on the PCA technique are very good, even if the person looks from one side at the video camera...|$|R
40|$|From {{the recent}} {{analysis}} of supervised learning by on [...] line gradient descent in multilayered neural networks {{it is known}} that the necessary process of student specialization can be delayed significantly. We demonstrate that this phenomenon also occurs in various models of unsupervised learning. A solvable model of competitive learning is presented, which identifies <b>prototype</b> <b>vectors</b> suitable for the representation of high [...] dimensional data. The specific case of two overlapping clusters of data and a matching number of <b>prototype</b> <b>vectors</b> exhibits non [...] trivial behavior like almost stationary plateau configurations. As a second example scenario we investigate the application of Sanger's algorithm for principal component analysis in the presence of two relevant directions in input space. Here, the fast learning of the first principal component may lead to an almost complete loss of initial knowledge about the second one...|$|R
40|$|In this paper, {{the problem}} of multi-objective {{supervised}} learning is discussed within the non-evolutionary optimiza-tion framework. The proposed MOBJ learning algorithm performs the search of Pareto-optimal models determining weights, width, <b>prototype</b> <b>vectors,</b> and the quantity of ba-sis functions of the RBF network. In combination with the Akaike information criterion, the algorithm provides high quality solutions. 1...|$|R
40|$|In {{this paper}} {{we present a}} {{visualization}} technique to visualize single clusters of high-dimensional data. Our method maps a single cluster to the plane trying to preserve the relative distances of feature vectors to the corresponding <b>prototype</b> <b>vector.</b> Thus, fuzzy clustering results representing relative distances {{in the form of}} a partition matrix as well as hard clustering partitions can be visualized with this technique. The resulting two-dimensional scatter plot illustrates the compactness of a certain cluster and the need of additional prototypes as well. In this work, we will demonstrate the visualization method on some benchmark data sets and on a practical application...|$|E
40|$|ABSTRACT. Knowledge-based systems (KBS) {{development}} and maintenance both require time-consuming analysis of domain knowledge. Where example cases exist, KBS can be built, and later updated, by incorporating learning capabilities into their architecture. This applies to both supervised and unsupervised learning scenarios. In this paper, the important issues for learning systems—memory, feedback, pattern formulation, and pattern recognition—are {{described in terms}} of an instance vector set, a <b>prototype</b> <b>vector</b> set, and a mapping between those sets. While learning systems can possess robustness, recency, adaptability, and extensibility, they also require: careful attention to example case security, correct interpretation of feedback, modification for uncertainty calculations, and treatment of ambiguous output. Despite the difficulties associated with adding learning to KBS, it is essential for ridding them of artificiality...|$|E
40|$|A fuzzy prototype-based {{method is}} {{introduced}} for learning from exam-ples. A {{special kind of}} <b>prototype</b> <b>vector</b> with fuzzy attributes is de-rived for each class from aggregat-ing fuzzified cases {{for the purpose of}} concept description. The fuzzi-fied cases are derived by defining a fuzzy membership function for each attribute of the sample cases. In a first method, for the classification of a new case, the membership degrees of its crisp attributes to fuzzy ag-gregated prototypes are measured. In a second method, after fuzzify-ing the new case, fuzzy set compar-ison methods are applied for mea-suring the similarity. The methods are compared to case-based ones like POSSIBL and kNN using UCI ma-chine learning repository. We also make comparisons by using various transformation methods from prob-abilities to possibilities instead of defining membership functions...|$|E
40|$|In {{this paper}} we test the Self-Organizing Map (SOM) {{on the problem}} of {{predicting}} chaotic time-series (speci cally Mackey-Glass series) with local linear models de ned separately for each of the <b>prototype</b> <b>vectors</b> of the SOM. We see that the method achieves good results. This together with the capabilities of the SOM make itavaluable tool in exploratory data mining. ...|$|R
40|$|Mobile {{networks}} {{produce a}} huge amount of spatio-temporal data. The data consists of parameters of base stations and quality information of calls. The Self-Organizing Map (SOM) is an efficient tool for visualization and clustering of multidimensional data. It transforms the input vectors on two-dimensional grid of <b>prototype</b> <b>vectors</b> and orders them. The ordered <b>prototype</b> <b>vectors</b> are easier to visualize and explore than the original data. There are two possible ways to start the analysis. We can build either a model of the network using state vectors with parameters from all mobile cells or a general one cell model trained using one cell state vectors from all cells. In both methods further analysis is needed. In the first method the distributions of parameters of one cell can be compared with the others and in the second it can be compared how well the general model represents each cell...|$|R
40|$|Given a set {{of samples}} of a {{probability}} distribution on {{a set of}} discrete random variables, we study the problem of constructing a good approximative neural network model of the underlying probability distribution. Our approach is based on an unsupervised learning scheme where the samples are first divided into separate clusters, and each cluster is then coded as a single <b>vector.</b> These Bayesian <b>prototype</b> <b>vectors</b> consist of conditional probabilities representing the attribute-value distribution inside the corresponding cluster. Using these <b>prototype</b> <b>vectors,</b> it is possible to model the underlying joint probability distribution as a simple Bayesian network (a tree), which can be realized as a feedforward neural network capable of probabilistic reasoning. In this framework, learning means choosing the size of the prototype set, partitioning the samples into the corresponding clusters, and constructing the cluster prototypes. We describe how the prototypes can be determined, given a partit [...] ...|$|R
40|$|AbstractAlthough picornaviruses provide {{attractive}} vectors for {{expression of}} foreign genes, poor genetic stability restricts their use for immunization purposes. A new <b>prototype</b> <b>vector</b> was generated to increase foreign insert retention, by shifting of the initiation codon to a cryptic AUG within the internal ribosomal entry site (IRES) and replacement of IRES domain VI with foreign ORFs. Using our strategy to replace regulatory noncoding sequences with unrelated foreign genetic material, we generated stable poliovirus-based expression vectors with robust long-term expression of foreign ORFs. Our studies revealed that size and predicted secondary structure {{formed by the}} heterologous sequences govern long-term retention and efficiency of expression of foreign inserts replacing IRES structures. These observations indicate that, with certain limitations imposed by structural preferences, foreign sequences can functionally replace IRES substructures in stable picornavirus immunization vectors...|$|E
40|$|Practicaldataminingrarelyfalls exactlyinto the supervisedlearning scenario. Rather, {{the growing}} amount of unlabeled data poses a big {{challenge}} to large-scale semi-supervised learning (SSL). We note that the computationalintensivenessofgraph-based SSLarises largely from the manifold or graph regularization, which in turn lead to large models that are dificult to handle. To alleviate this, we proposed the <b>prototype</b> <b>vector</b> machine (PVM), a highlyscalable,graph-based algorithm for large-scale SSL. Our key innovation {{is the use of}} "prototypes vectors" for effcient approximation on both the graph-based regularizer and model representation. The choice of prototypes are grounded upon two important criteria: they not only perform effective low-rank approximation of the kernel matrix, but also span a model suffering the minimum information loss compared with the complete model. We demonstrate encouraging performance and appealing scaling properties of the PVM on a number of machine learning benchmark data sets...|$|E
30|$|The LE-cloning {{system has}} been {{developed}} to ligate two or more genes into a vector system, thereby enabling the reliable and easy production and purification of multifunctional biomass degrading fusion enzymes (Marquardt et al. 2014; Neddersen and Elleuche 2015). The <b>prototype</b> <b>vector</b> pQE- 30 -LE {{is based on the}} medium-copy plasmid pQE- 30 (Qiagen, Hilden, Germany; utilization of ColE 1 origin of replication results in 15 – 20 copies of plasmids in a single cell) that contains a T 5 -promoter and a sequence encoding the N-terminal HIS 6 -tag. Moreover, this vector is adapted to be optimally used in combination with expression strain E. coli M 15 [pREP 4]. In addition, the MCS was replaced in pQE- 30 -LE by a merged recognition site for restriction endonucleases LguI and Eco 81 I to allow step-wise ligation of DNA-fragments into a continuously growing plasmid (Marquardt et al. 2014).|$|E
40|$|The Self-Organizing Map (SOM) is a vector {{quantization}} method which places <b>prototype</b> <b>vectors</b> {{on a regular}} low-dimensional grid in an ordered fashion. A new method to obtain piecewise linear models of dynamic processes is presented. The operating regimes of the local linear models are obtained by the Delaunay tessellation of the codebook of the SOM. The proposed technique is demonstrated {{by means of the}} identification of a pH process...|$|R
40|$|The {{clustering}} algorithms {{based on}} potential functions {{are capable of}} clustering a set of data, making no implicit assumptions on the cluster shapes and without knowing in advance the number of clusters. They are similarity-based type clustering algorithms and do not use any <b>prototype</b> <b>vectors</b> of the clusters. In this paper, some properties of these algorithms are studied: points arrangement tendency, constant potential surface, cluster shapes and robustness to noise...|$|R
50|$|Vector {{quantization}} (VQ) is {{a classical}} quantization technique from signal processing {{that allows the}} modeling of probability density functions by the distribution of <b>prototype</b> <b>vectors.</b> It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately {{the same number of}} points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.|$|R
40|$|Mesenchymal {{stem cells}} (MSCs) are {{adult stem cells}} with multilineage potential, which makes them {{attractive}} tools for regenerative medicine applications. Efficient gene transfer into MSCs is essential not only for basic research in developmental biology but also for therapeutic applications involving gene-modification in regenerative medicine. Adenovirus vectors (Advs) can efficiently and transiently introduce an exogenous gene into many cell types via their primary receptors, the coxsackievirus and adenovirus receptors, but not into MSCs, which are deficient in coxsackievirus and adenovirus receptors expression. To overcome this problem, we developed an Adv coated with a spermine-pullulan (SP) cationic polymer and investigated its physicochemical properties and internalization mechanisms. We demonstrated that the SP coating could enhance adenoviral transduction of MSCs without detectable cytotoxicity or effects on differentiation. Our results argue {{in favor of the}} potentiality of the SP-coated Adv as a <b>prototype</b> <b>vector</b> for efficient and safe transduction of MSCs...|$|E
40|$|Drift is a <b>prototype,</b> <b>vector</b> space based, {{information}} retrieval system in {{development at the}} University of Virginia. The system is designed to do experiments in distributed, dynamic {{information retrieval}}. We describe our first experiments using Drift on larger test collections, specifically the Category B subset of the TREC corpus. 1 Introduction The intelligent application of classic Information Retrieval (IR) methods in today's information environment is pivotal to enabling effective, efficient search of distributed document collections. Recent research results in the IR literature [1, 2, 6, 7, 8] illustrate that IR researchers are recognizing and attacking this problem {{on a number of}} fronts. It seems clear that the dynamic, distributed environment of the present and future Internet requires some changes in the way we look at searching document collections. As identified in [2], several interesting research questions arise: ffl Collection Selection / Resource Discovery; ffl Effici [...] ...|$|E
40|$|In this paper, {{we present}} a novel {{approach}} to group users according to their Web access patterns. Our technique for grouping users {{is based on the}} ART 1 neural network. We compare the quality of clustering of our ART 1 based clustering technique with that of the K-Means clustering algorithm in terms of inter-cluster and intra-cluster distances. Our results show that the average inter-cluster distance of the clusters formed by K-Means algorithm varies from 12. 66 to 24. 20, while the average inter-cluster distance of clusters formed by our ART 1 based clustering technique is almost constant (approximately 18. 01), which indicates the high quality of clusters formed by our approach. We present a prefetching scheme in which we apply our clustering technique to group users and then prefetch their requests according to the <b>prototype</b> <b>vector</b> of each group. Our prefetching scheme has prediction accuracy as high as 97. 78 %...|$|E
40|$|Many {{kinds of}} {{data can be}} viewed as {{consisting}} of a set of vectors, each of which is a noisy combination of a small number of noisy <b>prototype</b> <b>vectors.</b> Physically, these <b>prototype</b> <b>vectors</b> may correspond to different hidden variables that play a role in determining the measured data. For example, a gene’s expression is influenced by the presence of transcription factor proteins, and two genes may be activated by overlapping sets of transcription factors. Consequently, the activity of each gene {{can be explained by the}} activities of a small number of transcription factors. This task {{can be viewed as}} the problem of factorizing a data matrix, while taking into account hard constraints reflecting structural knowledge of the problem and probabilistic relationships between variables that are induced by known uncertainties in the problem. We present soft-decision probabilistic sparse matrix factorization (PSMF) to better account for uncertainties due to varying levels of noise in the data, varying levels of noise in the prototypes used to explain the data, and uncertainty as to which hidden prototypes are selected to explain each expression vector...|$|R
40|$|This paper {{presents}} a new classification methodology for hyperspectral data based on synergetics theory, which describes the sponta-neous formation of patterns and structures {{in a system}} through self-organization. We introduce a representation for hyperspectral data, in which a spectrum can be projected in a space spanned {{by a set of}} user-defined <b>prototype</b> <b>vectors,</b> which belong to some classes of interest. Each test vector is attracted by a final state associated to a prototype, and can be thus classified. As typical synergetics-based systems have the drawback of a rigid training step, we modify it to allow the selection of user-defined training areas, used to weight the <b>prototype</b> <b>vectors</b> through attention parameters and to produce a more accurate classification map through majority voting of indepen-dent classifications. Results are comparable to state of the art classification methodologies, both general and specific to hyperspectral data and, as each classification is based on a single training sample per class, the proposed technique would be particularly effective in tasks where only a small training dataset is available. ...|$|R
40|$|We review {{our recent}} {{investigation}} of on-line unsupervised learning from high-dimensional structured data. First, on-line competitive learning is studied {{as a method}} for the identification of <b>prototype</b> <b>vectors</b> from overlapping clusters of examples. Specifically, we analyse {{the dynamics of the}} well-known winner-takes-all or K-means algorithm. As a second standard learning technique, the application of Sanger's rule for principal component analysis is investigated. In both scenarios the necessary process of student specialization may be delayed significantly due to underlying symmetries. ...|$|R
40|$|The {{objective}} of this thesis is to develop and generalize further the differential evolution based data classification method. For many years, evolutionary algorithms have been successfully applied to many classification tasks. Evolution algorithms are population based, stochastic search algorithms that mimic natural selection and genetics. Differential evolution is an evolutionary algorithm that has gained popularity because of its simplicity and good observed performance. In this thesis a differential evolution classifier with pool of distances is proposed, demonstrated and initially evaluated. The differential evolution classifier is a nearest <b>prototype</b> <b>vector</b> based classifier that applies a global optimization algorithm, differential evolution, to determine the optimal values for all free parameters of the classifier model during the training phase of the classifier. The differential evolution classifier applies the individually optimized distance measure for each new data set to be classified is generalized to cover a pool of distances. Instead of optimizing a single distance measure for the given data set, {{the selection of the}} optimal distance measure from a predefined pool of alternative measures is attempted systematically and automatically. Furthermore, instead of only selecting the optimal distance measure from a set of alternatives, an attempt is made to optimize the values of the possible control parameters related with the selected distance measure. Specifically, a pool of alternative distance measures is first created and then the differential evolution algorithm is applied to select the optimal distance measure that yields the highest classification accuracy with the current data. After determining the optimal distance measures for the given data set together with their optimal parameters, all determined distance measures are aggregated to form a single total distance measure. The total distance measure is applied to the final classification decisions. The actual classification process is still based on the nearest <b>prototype</b> <b>vector</b> principle; a sample belongs to the class represented by the nearest <b>prototype</b> <b>vector</b> when measured with the optimized total distance measure. During the training process the differential evolution algorithm determines the optimal class vectors, selects optimal distance metrics, and determines the optimal values for the free parameters of each selected distance measure. The results obtained with the above method confirm that the choice of distance measure {{is one of the most}} crucial factors for obtaining higher classification accuracy. The results also demonstrate that it is possible to build a classifier that is able to select the optimal distance measure for the given data set automatically and systematically. After finding optimal distance measures together with optimal parameters from the particular distance measure results are then aggregated to form a total distance, which will be used to form the deviation between the class vectors and samples and thus classify the samples. This thesis also discusses two types of aggregation operators, namely, ordered weighted averaging (OWA) based multi-distances and generalized ordered weighted averaging (GOWA). These aggregation operators were applied in this work to the aggregation of the normalized distance values. The results demonstrate that a proper combination of aggregation operator and weight generation scheme play an important role in obtaining good classification accuracy. The main outcomes of the work are the six new generalized versions of previous method called differential evolution classifier. All these DE classifier demonstrated good results in the classification tasks...|$|E
40|$|Practical {{data mining}} rarely falls exactly into the {{supervised}} learning scenario. Rather, the growing amount of unlabeled data poses {{a big challenge}} to large-scale semi-supervised learning (SSL). We note that the computational intensiveness of graph-based SSL arises largely from the manifold or graph regularization, which in turn lead to large models {{that are difficult to}} handle. To alleviate this, we proposed the <b>prototype</b> <b>vector</b> machine (PVM), a highly scalable, graph-based algorithm for large-scale SSL. Our key innovation is the use of “prototypes vectors ” for efficient approximation on both the graphbased regularizer and model representation. The choice of prototypes are grounded upon two important criteria: they not only perform effective low-rank approximation of the kernel matrix, but also span a model suffering the minimum information loss compared with the complete model. We demonstrate encouraging performance and appealing scaling properties of the PVM on a number of machine learning benchmark data sets. 1...|$|E
40|$|Abstract. Multi-label {{learning}} {{deals with}} the problem where each instance is associated with multiple labels simultaneously. The task of this learning paradigm is to predict the label set for each unseen instance, through analyzing training instances with known label sets. In this paper, a neural network based multi-label learning algorithm named Ml-rbf is proposed, which {{is derived from the}} traditional radial basis function (RBF) methods. Briefly, the first layer of an Ml-rbf neural network is formed by conducting clustering analysis on instances of each possible class, where the centroid of each clustered groups is regarded as the <b>prototype</b> <b>vector</b> of a basis function. After that, second layer weights of the Ml-rbf neural network are learned by minimizing a sum-of-squares error function. Specifically, information encoded in the prototype vectors corresponding to all classes are fully exploited to optimize the weights corresponding to each specific class. Ex-periments on three real-world multi-label data sets show that Ml-rbf achieves highly competitive performance to other well-established multi-label learning algorithms...|$|E
40|$|The Self-Organizing Map (SOM) is a vector {{quantization}} method which places the <b>prototype</b> <b>vectors</b> {{on a regular}} low-dimensional grid in an ordered fashion. This makes the SOM a powerful visualization tool. The SOM Toolbox is an implementation of the SOM and its visualization in the Matlab 5 computing environment. In this article, the SOM Toolbox and its usage are shortly presented. Also its performance in terms of computational load is evaluated and compared to a corresponding C-program...|$|R
40|$|Abstract. We {{introduce}} a novel method to build multiple local regression models {{based on the}} <b>prototype</b> <b>vectors</b> of the SOM network and other well-known vector quantization (VQ) algorithms. The resulting models are evaluated in the task of identifying the inverse dynamics of a heat exchanger data set. Additionally, we evaluate through statistical hypothesis testing {{the influence of the}} VQ algorithm on the performance of the local model. Simulation results demonstrate that the proposed method consistently outperforms previous MLP- and SOM-based approaches for system identification. ...|$|R
40|$|An {{algorithm}} is proposed to prune the <b>prototype</b> <b>vectors</b> (pro-totype selection) used in a nearest neighbor classifier so that a compact classifier can be obtained with similar or even better performance. The pruning procedure is error based; a proto-type will be pruned if its deletion leads to the smallest clas-sification error increase. Also each pruning iteration is fol-lowed by one epoch of Learning Vector Quantization (LVQ) training. Simulation {{results show that the}} selected prototypes can approach optimal or near optimal locations based on the training data distribution...|$|R
