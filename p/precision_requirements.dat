334|281|Public
25|$|Shader Model 3.0 {{altered the}} specification, {{increasing}} full <b>precision</b> <b>requirements</b> {{to a minimum}} of FP32 support in the fragment pipeline. ATI's Shader Model 3.0 compliant R5xx generation (Radeon X1000 series) supports just FP32 throughout the pipeline while Nvidia's NV4x and G7x series continued to support both FP32 full precision and FP16 partial precisions. Although not stipulated by Shader Model 3.0, both ATI and Nvidia's Shader Model 3.0 GPUs introduced support for blendable FP16 render targets, more easily facilitating the support for High Dynamic Range Rendering.|$|E
25|$|These assertions {{are tested}} {{on a large}} set of {{randomized}} input images, to handle the worst cases. The former IEEE 1180–1990 standard contained some similar <b>precision</b> <b>requirements.</b> The precision has a consequence on the implementation of decoders, and it is critical because some encoding processes (notably used for encoding sequences of images like MPEG) {{need to be able}} to construct, on the encoder side, a reference decoded image. In order to support 8-bit precision per pixel component output, dequantization and inverse DCT transforms are typically implemented with at least 14-bit precision in optimized decoders.|$|E
2500|$|The {{encoding}} {{description in}} the JPEG standard does not fix the precision {{needed for the}} output compressed image. However, the JPEG standard (and the similar MPEG standards) includes some <b>precision</b> <b>requirements</b> for the decoding, including {{all parts of the}} decoding process (variable length decoding, inverse DCT, dequantization, renormalization of outputs); the output from the reference algorithm must not exceed: ...|$|E
30|$|In {{practical}} applications, {{the estimation}} of the first six harmonics can meet the <b>precision</b> <b>requirement.</b>|$|R
3000|$|... n: The maximal number (integer) of test organisms, {{with which}} the {{laboratory}} is willing to cope. Limiting the number is necessary to avoid non-essential calculations and thereby save computing time. The programme will invite the user {{to increase the number}} if the number is not high enough to estimate the SL with the given <b>precision</b> <b>requirement.</b>|$|R
40|$|We {{introduce}} ASAP 2, {{an improved}} {{variant of the}} batchmeans algorithm ASAP for steady-state simulation output analysis. ASAP 2 operates as follows: the batch size is progressively increased until the batch means pass the ShapiroWilk test for multivariate normality; and then ASAP 2 delivers a correlation-adjusted confidence interval. The latter adjustment {{is based on an}} inverted Cornish-Fisher expansion for the classical batch means t-ratio, where the terms of the expansion are estimated via a first-order autoregressive time series model of the batch means. ASAP 2 is a sequential procedure designed to deliver a confidence interval that satisfies a prespecified absolute or relative <b>precision</b> <b>requirement.</b> When used in this way, ASAP 2 compares favorably to ASAP and the well-known procedures ABATCH and LBATCH with respect to close conformance to the <b>precision</b> <b>requirement</b> as well as coverage probability and mean and variance of the half-length of the final confidence interval...|$|R
50|$|The ultra-high <b>precision</b> <b>requirements</b> for optical {{surfaces}} for X-ray {{astronomy and}} deep-ultraviolet lithography often require ion figuring.|$|E
50|$|Within the {{electronic}} production, <b>precision</b> <b>requirements</b> increase greatly. This {{is due to}} smaller components and a technological change, e.g. chip-on board, flip-chip technology or wafer-level-chip packaging. A repeatability accuracy of less than 10 µm is required. Roller bearings do not achieve this because of their Stick-slip phenomenon and “a tight drawer” effect.|$|E
5000|$|The {{encoding}} {{description in}} the JPEG standard does not fix the precision {{needed for the}} output compressed image. However, the JPEG standard (and the similar MPEG standards) includes some <b>precision</b> <b>requirements</b> for the decoding, including {{all parts of the}} decoding process (variable length decoding, inverse DCT, dequantization, renormalization of outputs); the output from the reference algorithm must not exceed: ...|$|E
40|$|The {{implications}} of the propagation of uncertainty for the efficiency of stochastic water pollution control were analyzed. The empirical analysis focused on nitrogen pollution of the Baltic Sea from Swedish crop farms. A two moment model (mean and variance) {{of the distribution of}} coastal load was developed and integrated with an economic model of arable land-use choice. It was shown that uncertainty propagates through the system if arable emissions and retention are negatively correlated, and riverine loads (emissions less retention) positively correlated. Under these conditions a <b>precision</b> <b>requirement</b> increases the efficiency of abatement in catchments with relatively high load variability ceteris paribus. Relative abatement increased significantly with the <b>precision</b> <b>requirement</b> in high retention regions because these regions had a relatively large impact on the variability of coastal load. Thus if the Baltic damage function is convex then increasing abatement in high retention regions should increase overall economic efficiency...|$|R
30|$|At n=k, we {{approximate}} δ_j^k by U_j^k- 1 {{and then}} system (21) becomes a linear equation. We can solve the linear equation {{to obtain a}} first approximation U_j^k to U_j^k. We iterate using (21) for some iterations with δ_j^k approximated by U_j^k to refine the approximation to U_j^k. The process is repeated until the result satisfies the error <b>precision</b> <b>requirement.</b>|$|R
40|$|Consider {{learning}} tasks {{where the}} <b>precision</b> <b>requirement</b> is very high, for example a 99 % <b>precision</b> <b>requirement</b> for a video classification application. We report that when very differ-ent sources of evidence such as text, audio, and video features are available, combining the outputs of base classifiers trained on each feature type separately, aka late fusion, can substan-tially increase {{the recall of}} the combination at high precisions, compared {{to the performance of}} a single classifier trained on all the feature types, i. e., early fusion, or compared to the individ-ual base classifiers. We show how the probability of a joint false-positive mistake can be less – in some cases significantly less – than the product of individual probabilities of conditional false-positive mistakes (a Noisy-OR combination). Our analysis highlights a simple key crite-rion for this boosted precision phenomenon and justifies referring to such feature families as (nearly) independent. We assess the relevant factors for achieving high precision empirically, and explore combination techniques informed by the analysis. ...|$|R
50|$|Partly {{due to the}} <b>precision</b> <b>requirements</b> of his locks, Bramah {{spent much}} time {{developing}} machine tools to assist manufacturing processes. He relied heavily on the expertise of Henry Maudslay whom he employed in his workshop {{from the age of}} 18. Between them they created a number of innovative machines that made the production of Bramah's locks more efficient, and were applicable to other fields of manufacture.|$|E
50|$|Shader Model 3.0 {{altered the}} specification, {{increasing}} full <b>precision</b> <b>requirements</b> {{to a minimum}} of FP32 support in the fragment pipeline. ATI's Shader Model 3.0 compliant R5xx generation (Radeon X1000 series) supports just FP32 throughout the pipeline while Nvidia's NV4x and G7x series continued to support both FP32 full precision and FP16 partial precisions. Although not stipulated by Shader Model 3.0, both ATI and Nvidia's Shader Model 3.0 GPUs introduced support for blendable FP16 render targets, more easily facilitating the support for High Dynamic Range Rendering.|$|E
50|$|These assertions {{are tested}} {{on a large}} set of {{randomized}} input images, to handle the worst cases. The former IEEE 1180-1990 standard contained some similar <b>precision</b> <b>requirements.</b> The precision has a consequence on the implementation of decoders, and it is critical because some encoding processes (notably used for encoding sequences of images like MPEG) {{need to be able}} to construct, on the encoder side, a reference decoded image. In order to support 8-bit precision per pixel component output, dequantization and inverse DCT transforms are typically implemented with at least 14-bit precision in optimized decoders.|$|E
30|$|At n= 1, we {{approximate}} δ_j^ 1 by U_j^ 0 {{and then}} system (20) becomes a linear equation. We can solve the linear equation {{to obtain a}} first approximation U_j^ 1 to U_j^ 1. We iterate using (20) for some iterations with δ_j^ 1 approximated by U_j^ 1 to refine the approximation to U_j^ 1. The process is repeated until the result satisfies the error <b>precision</b> <b>requirement.</b>|$|R
40|$|A global {{recursive}} bisection {{algorithm is}} described for computing the complex zeros of a polynomial. It has complexityO(n 3 p) wheren {{is the degree}} of the polynomial andp the bit <b>precision</b> <b>requirement.</b> Ifn processors are available, it can be realized in parallel with complexityO(n 2 p); also it can be implemented using exact arithmetic. A combined Wilf-Hansen algorithm is suggested for reduction in complexity...|$|R
40|$|This paper {{discusses}} {{implementation of}} a sequential quantileestimation algorithm for highly correlated steady-state simulation output. Our primary focus is on issues related to computational and storage requirements of order statistics. The algorithm can compute exact sample quantiles and process sample sizes up to several billion without storing and sorting the whole sequence. The algorithm dynamically increases the sample size so that the quantile estimated satisfies a pre-specified <b>precision</b> <b>requirement...</b>|$|R
50|$|Note however, {{that many}} of these values have been {{truncated}} because they contain repeating fractions. When we try to store these in our fixed point format, we're going to lose some of our precision (which didn't seem all that precise when they were just integers). This is an interesting problem because we said we could fit 256 different values into our 8 bit format, and we're only trying to store values from a range with 161 possible values (0 through 160). As it turns out, the problem was our scale factor, 11, which introduced unnecessary <b>precision</b> <b>requirements.</b> The resolution of the problem is to find a better scaling factor. For more information, read on.|$|E
50|$|Surface {{distortion}} due to grinding or polishing {{increases with}} the aspect {{ratio of the}} part (diameter to thickness ratio). To a first order, glass strength increases as the cube of the thickness. Thick lenses at 4:1 to 6:1 aspect ratios will flex much less than high aspect ratio parts, such as optical windows, which can have aspect ratios of 15:1 or higher. The combination of surface or wavefront error <b>precision</b> <b>requirements</b> and part aspect ratio drives the degree of back support uniformity required, especially during the higher down pressures and side forces during polishing. Optical working typically involves a degree of randomness that helps greatly in preserving figure-of-revolution surfaces, provided the part is not flexing during the grind/polish process.|$|E
50|$|The {{theodolite}} {{became a}} modern, accurate instrument in 1787, {{with the introduction}} of Jesse Ramsden's famous great theodolite, which he created using a very accurate dividing engine of his own design. The demand could not be met by foreign theodolites owing to their inadequate precision, hence all instruments meeting high <b>precision</b> <b>requirements</b> were made in England. Despite the many German instrument builders {{at the turn of the}} century, there were no usable German theodolites available. A transition was brought about by Breithaupt and the symbiosis of Utzschneider, Reichenbach and Fraunhofer.As technology progressed, in the 1840s, the vertical partial circle was replaced with a full circle, and both vertical and horizontal circles were finely graduated. This was the transit theodolite. Theodolites were later adapted to a wider variety of mountings and uses. In the 1870s, an interesting waterborne version of the theodolite (using a pendulum device to counteract wave movement) was invented by Edward Samuel Ritchie. It was used by the U.S. Navy to take the first precision surveys of American harbors on the Atlantic and Gulf coasts.|$|E
40|$|Abstract. For the {{improvement}} of the system real-time capability, the DSP Curve Surface interpolator has been designed to ensure high speed, high <b>precision</b> <b>requirement</b> for the machining of complexity curve surface. The hardware and software has been designed. The changeable feedrate interpolation algorithm of limit curve error has been studied. Furthermore, the algorithm has been applied to the interpolation and has well real-time capability to satisfy the requirement of high capability CNC by experimentation...|$|R
5000|$|There are no set {{limit for}} age {{uncertainty}} and the cut-off value varies with different <b>precision</b> <b>requirement.</b> Although excluding data with huge age uncertainty would enhance the overall zircon grain age accuracy, over elimination may lower overall research reliability (decrease in {{size of the}} database). The best practice would be to filter accordingly, i.e. setting the cut-off error to eliminate reasonable portion of the dataset (say <5% of the total ages available) ...|$|R
30|$|Finally, {{it would}} be {{interesting}} to explore how many simulations are needed to train a GAN model for a given <b>precision</b> <b>requirement.</b> Another future direction would be to further explore the agreement between the original and GAN-generated images in terms of advanced statistics, such as for example 3 -pt functions or Minkowski functionals. Going beyond the cross-correlations to further tests for independence of the GAN-generated samples could also be of interest. We leave this exploration to future work.|$|R
40|$|<b>Precision</b> <b>requirements</b> are {{determined}} for space-based column-averaged CO 2 dry air mole fraction data. These requirements result from {{an assessment of}} spatial and temporal gradients {{in the relationship between}} precision and surface CO 2 flux uncertainties inferred from inversions of the data, and the effects of biases on the fidelity of CO 2 flux inversions. Observational system simulation experiments and synthesis inversion modeling demonstrate that the Orbiting Carbon Observatory mission design and sampling strategy provide the means to achieve these data <b>precision</b> <b>requirements...</b>|$|E
40|$|The paper {{investigates the}} {{frequency}} error {{in a big}} frequency range. Nowadays we cannot ignore {{it because of the}} <b>precision</b> <b>requirements</b> satisfaction. The analysis is carried out for a case of the voltage comparison. ? ?????? ??????????????? ????????? ??????????? ? ???????????? ????????? ??????, ????????????? ??????? ??? ??????????? ??????????? ?? ???????? ???????? ????????????. ?????? ??????????? ??? ?????? ?????????????? ??????????...|$|E
40|$|The {{emphasis}} {{in this article}} is to exploit the fact that <b>precision</b> <b>requirements</b> for solutions of most economic models in practice are moderate only. A simple approach is introduced for solving linearly constrained partial equilibrium models based on an iterative scheme similar to the simplex method. It allows large-scale models to be solved, within a practical tolerance, efficiently even in a micro computer environment. Extensions to linearly constrained convex optimization problems are presented. Finally, a set of computational tests on 68 linear programs from the NETLIB library is reported. Comparison of our approach with the simplex method (using MINOS 5. 1) and with Karmarkar's algorithm is reported. For moderate <b>precision</b> <b>requirements</b> these preliminary results are highly encouraging. linear programming, large-scale optimization, partial equilibrium...|$|E
40|$|This paper {{discusses}} {{implementation of}} a sequential procedure to estimate the steady-state density of a stochastic process. The procedure computes sample densities at certain points and uses Lagrange interpolation to estimate the density f(x). Even though the proposed sequential procedure is a heuristic, it does have strong basis. Our empirical {{results show that the}} procedure gives density estimates that satisfy a pre-specified <b>precision</b> <b>requirement.</b> An experimental performance evaluation demonstrates the validity of using the procedure to estimate densities. ...|$|R
40|$|We {{formulate}} {{and evaluate}} the Automated Simulation Analysis Procedure (ASAP), an algorithm for steady-state simulation output analysis based on the method of nonover-lapping batch means (NOBM). ASAP delivers a confidence interval for an expected response that is centered on the sample mean of {{a portion of a}} simulation-generated time series and satisfies a user-specified absolute or relative <b>precision</b> <b>requirement.</b> ASAP operates as follows: The batch size is progressively increased until either (a) the batch means pass the von Neumann test for independence, and then ASAP delivers a classical NOBM confidence interval; or (b) the batch means pass the Shapiro-Wilk test for multivariate normality, and then ASAP delivers a correlation-adjusted confidence interval. The latter adjustment is based on an inverted Cornish-Fisher expansion for the classical NOBM t-ratio, where the terms of the expansion are estimated via an autoregressive-moving average time series model of the batch means. After determining the batch size and confidence-interval type, ASAP sequentially increases the number of batches until the <b>precision</b> <b>requirement</b> is satisfied. An extensive experimental study demonstrates the performance improvements achieved by ASAP versus well-known batch means procedures, especially in confidence-interval coverage probability. Simulation, Statistical Analysis, Method of Batch Means, Steady-State Output Analysis...|$|R
40|$|Simulated {{estimates}} for several proportions {{are needed in}} many simulation studies. We propose a method for controlling {{the length of a}} simulation run so that the proportions estimated satisfy a prespecified <b>precision</b> <b>requirement.</b> The method applies the arcsin transform in order to generate confidence intervals with the desired width. The Bonferroni inequality is applied in a new way to construct a rectangular confidence area for the proportions being estimated. Since we are using the Bonferroni inequality, we can use the existing methods developed for estimating the variance of a single mean. The properties of the method proposed depend on three factors: 1) the arcsin transform, 2) the proposed way of applying the Bonferroni inequality, and 3) the method for estimating the variance. We examine the effects that the transform and the way of applying the Bonferroni inequality have. Empirical results indicate that the method provides estimates that satisfy the prespecified <b>precision</b> <b>requirement,</b> when the spectral method is used to estimate the variances of the proportions. In addition, simulation runs, when estimating either several proportions or the corresponding mean, are about the same lengths. discrete-event simulation, simultaneous estimation, run length control...|$|R
40|$|The paper {{describes}} {{a new kind}} of sun tracker intended to satisfy the <b>precision</b> <b>requirements</b> of high concentration photovoltaic systems. The tracker is designed according to a Delta type parallel kinematic schematics. The paper describes the kinematic schematics, the sensor chain and the control algorithm. Finally the results obtained in functional tests are shown...|$|E
40|$|This paper {{presents}} {{a method to}} register a series of images captured by a high resolution camera in real-time. The proposed algorithm attempts to balance accuracy and efficiency properly {{in order to meet}} both the time constraints and <b>precision</b> <b>requirements</b> of the Pen Project. The results obtained are promising and possible extensions to this research are discussed...|$|E
40|$|The {{effect of}} grating errors on {{transverse}} beam stability is analyzed. We,characterize grating errors by random groove displacements {{and find that}} transverse displacements due to such errors approach limiting values of the same order as the grating displacements themselves. It therefore appears that transverse stability requirements will not impose unusually stringent <b>precision</b> <b>requirements</b> on the grating structure...|$|E
30|$|From the {{existing}} research results, it’s {{seen that the}} analytical method based on semi-infinite body model lack of theoretical support and the FEM cost considerable time [32]. For the geometry and stress situation of the work roll and back-up roll, the space problem can be simplified to plane problem approximately, which can effectively {{reduce the amount of}} calculation and meet the engineering <b>precision</b> <b>requirement.</b> In this paper, the roll flattening model is analyzed and modified based on the elastic half plane theory.|$|R
40|$|Abstract. A second order forward {{differences}} based subdivision depth computation {{technique for}} extra-ordinary Catmull-Clark subdivision surface (CCSS) patches is presented. The new technique improves a previous technique {{in that the}} computation of the subdivision depth {{is based on the}} patch’s curvature distribution, instead of its dimension. Hence, with the new technique, no excessive subdivision is needed for extra-ordinary CCSS patches to meet the <b>precision</b> <b>requirement</b> and, consequently, one can make trimming, finite element mesh generation, boolean operations, and tessellation of CCSSs more efficient. ...|$|R
40|$|A spaceflight {{qualified}} Radar Altimeter {{capable of}} achieving the TOPEX Mission measurement <b>precision</b> <b>requirement</b> of 2 -centimeters, is provided and its performance (Engineering Assessment) will be evaluated after launch and continuously during its 3 -year mission operational period. Information {{will be provided}} to JPL about the calibration of the TOPEX Radar Altimeter. The specifications for the required data processing algorithms which {{will be necessary to}} convert the Radar Altimeter mission telemetry data into the geophysical data will also be provided. The stringent 2 cm <b>precision</b> <b>requirement</b> for ocean topography determination from space necessitated examining existing Radar Altimeter designs for their applicability towards TOPEX. As a result, a system configuration evolved using some flight proven designs in conjunction with needed improvements which include: (1) a second frequency or channel to remove the range delay or apparent height bias caused by the electron content of the ionosphere; (2) higher transmit pulse repetition frequencies for correlation benefits at higher sea states to maintain precision; and (3) a faster microprocessor to accommodate two channels of altimetry data. Additionally, examination of past altimeter programs associated data processing algorithms was accomplished to establish the TOPEX-class Radar Altimeter data processing algorithms, and the necessary direction was outlined to begin to generate these for the TOPEX Mission...|$|R
