0|5208|Public
40|$|Experiment requirements, {{technical}} characteristics, and GEOS-C {{radar altimeter}} related analyses are discussed along with {{results of a}} study on engineering test data requirements. Statistical analyses related to determination of wave height resolution achievable {{as a function of}} system characteristics and averaging period are described, in addition to a discussion on the desirability of using computer procedures to compensate for altitude tracker time-jitter. Data processing considerations for the GEOS-C system are examined. An extensive analysis of the spatial filter effect is given and results of a computation of geoidal power spectral density, based on Skylab altimeter data, is displayed and interpreted in terms of <b>projected</b> GEOS-C <b>random</b> <b>errors.</b> This information is then used in deriving minimum-mean-square filter procedures for both geoid undulation and slope data. The characteristics of mean received waveforms as a function of off-nadir angle are used to obtain tracker bias as a function of sea state and pointing angle. The angle estimation process proposed by the GEOS-C hardware contractor is also investigated from a standpoint of achievable angular resolution...|$|R
25|$|<b>Random</b> <b>error</b> is {{the result}} of {{fluctuations}} around a true value because of sampling variability. <b>Random</b> <b>error</b> is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of <b>random</b> <b>error</b> include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical <b>error</b> during coding. <b>Random</b> <b>error</b> affects measurement in a transient, inconsistent manner and it is impossible to correct for <b>random</b> <b>error.</b>|$|R
30|$|The <b>random</b> <b>errors</b> {{ranged from}} 0.16 (over-bite) to 4.29 (Mx 7.Md 7). The <b>random</b> <b>errors</b> do not {{significantly}} compromise {{the validity of}} this work, since four landmarks are needed to obtain these angles {{and some of these}} landmarks are subjective [32 – 34]. Therefore, a few <b>random</b> <b>errors</b> of greater magnitude were expected. Similar studies do not mention the <b>random</b> <b>error</b> magnitude [16, 24]. Therefore, probably, they would be similar for the mentioned reasons.|$|R
5000|$|<b>Random</b> error: <b>Error</b> {{that occurs}} due to natural {{variation}} in the process. <b>Random</b> <b>error</b> is typically assumed to be normally distributed with zero mean and a constant variance. <b>Random</b> <b>error</b> is also called experimental error.|$|R
40|$|Monitoring and {{controlling}} <b>random</b> <b>errors</b> {{is an important}} function of a measurement control program. This report describes the principal sources of <b>random</b> <b>error</b> in the common nuclear material measurement processes {{and the most important}} elements of a program for monitoring, evaluating {{and controlling}} the <b>random</b> <b>error</b> standard deviations of these processes...|$|R
25|$|Precision in {{epidemiological}} variables is {{a measure}} of <b>random</b> <b>error.</b> Precision is also inversely related to <b>random</b> <b>error,</b> so that to reduce <b>random</b> <b>error</b> is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.|$|R
40|$|Errors in {{measurement}} can {{be categorized}} into two types: systematic errors that are predictable, and <b>random</b> <b>errors</b> that are inherently unpredictable and have null expected value. <b>Random</b> <b>error</b> is always present in a measurement. More often than not, readings in time series may contain inherent <b>random</b> <b>errors</b> due to causes like dynamic error, drift, noise, hysteresis, digitalization error and limited sampling frequency. <b>Random</b> <b>errors</b> may {{affect the quality of}} time series analysis substantially. Unfortunately, most of the existing time series analysis methods do not address <b>random</b> <b>errors,</b> possibly because <b>random</b> <b>error</b> in a time series, which can be modeled as a random variable of unknown distribution, is hard to handle. In this paper, we tackle this challenging problem. Taking similarity search as an example, which is an essential task in time series analysis, we develop MISQ, a statistical approach for <b>random</b> <b>error</b> reduction in time series analysis. The major intuition in our method is to use only the readings at different time instants in a time series to reduce <b>random</b> <b>errors.</b> We achieve a highly desirable property in MISQ: it can ensure that the recall is above a user-specified threshold. An extensive empirical study on 20 benchmark real data sets clearly shows that our method can lead to better performance than the baseline method without <b>random</b> <b>error</b> reduction in real applications such as classification. Moreover, MISQ achieves good quality in similarity search. I...|$|R
3000|$|In this paper, we are {{interested}} in linear models, (1), where the <b>random</b> <b>errors</b> may have skew-normal errors. In this case, the <b>random</b> <b>error</b> can be written as e [...]...|$|R
40|$|New attempt of {{ensemble}} rainfall-runoff {{prediction is}} presented with radar rainfall prediction and spatial <b>random</b> <b>error</b> field simulation. A radar image extrapolation model gives deterministic rainfall predictions, and its prediction error structure is analyzed by comparing with the observed rainfall fields. With the analyzed <b>error</b> characteristics, spatial <b>random</b> <b>error</b> fields are simulated using covariance matrix decomposition method. The simulated <b>random</b> <b>error</b> fields successfully keep the analyzed error structure {{and improve the}} accuracy of the deterministic rainfall predictions; then the <b>random</b> <b>error</b> fields with the deterministic fields are given to a distributed hydrologic model to achieve an ensemble runoff prediction...|$|R
40|$|Testing the {{influence}} of <b>random</b> <b>error</b> is important for most cross-cultural studies. This paper evaluates Witkowski’s test procedure and <b>random</b> <b>error</b> hypotheses. Using his own methods, we found little support for his hypotheses. The Lazarsfeld Method of Elaboration exposes defects in the <b>random</b> <b>error</b> hypothesis statements and test procedure. When these are modified, Witkowski’s system proves ten-able, We suggest that the Method of Elaboration improves investi-gation of the quality control test variable role for both <b>random</b> and systematic <b>error...</b>|$|R
40|$|Abstract. The {{straightness}} {{error was}} measured {{from a certain}} part surface by CMM. The paper expounded the principle of error separation technique, and separated the straightness error into deterministic <b>error</b> and <b>random</b> <b>error.</b> It proved strictly an opinion that <b>random</b> <b>error</b> is obedient to normal distribution. Extraction point of <b>random</b> <b>error</b> far outweighs the extraction point which is obtained from deterministic error. Based {{on the principle of}} sampling signal, a conclusion can be pointed out that utilizing the characteristic of <b>random</b> <b>error</b> to recover the signal of straightness error without distortion is feasible and normal distribution model can be used in the computer simulation to study the problem of extraction point...|$|R
50|$|All these {{deviations}} can {{be classified}} as systematic <b>errors</b> or <b>random</b> <b>errors.</b> Systematic errors can sometimes be compensated for by means {{of some kind of}} calibration strategy. Noise is a <b>random</b> <b>error</b> that can be reduced by signal processing, such as filtering, usually {{at the expense of the}} dynamic behavior of the sensor.|$|R
40|$|Multi-particle {{simulations}} {{are performed}} to study emittance {{growth in the}} Fermilab Booster. Analysis shows {{that the source of}} vertical emittance growth comes mostly from <b>random</b> <b>errors</b> in skew quadrupoles {{in the presence of a}} strong transverse space-charge force. [1] <b>Random</b> <b>errors</b> in dipole rolls and the Montague resonance do contribute but to lesser extent. The effect of <b>random</b> <b>errors</b> in the quadrupoles is small because the betatron envelope tunes are reasonably far away from the half-integer stopband...|$|R
40|$|In {{conventional}} stochastic computation, all {{the input}} streams are Bernoulli sequences (BSs), which {{may result in}} large <b>random</b> <b>error.</b> To reduce <b>random</b> <b>error</b> and improve computational accuracy, some other sequences have been reported as alternatives to BSs. However, these sequences only apply to the specific stochastic circuits, have difficulties in hardware generation, or have length constraints. To this end, new sequences without these disadvantages should be considered. This paper proposes the <b>random</b> <b>error</b> analysis method for stochastic computation based on autocorrelation sequence (AS), which is more general than the conventional one based on BS. The analysis results show {{that we can use}} the proper ASs as input streams of stochastic circuits to reduce <b>random</b> <b>error.</b> On the basis of that conclusion, we propose the <b>random</b> <b>error</b> reduction scheme based on maximal concentrated autocorrelation sequence (MCAS) and BS, both of which are ASs. MCAS and BS are applicable to any combinational stochastic circuit, are easily generated by hardware, and have no length constraints, which avoid the disadvantages of sequences in the previous work. Moreover, we apply the proposed <b>random</b> <b>error</b> reduction scheme into several typical stochastic circuits as case studies. The simulation results confirm the effectiveness of the proposed scheme...|$|R
40|$|Early {{approaches}} to characterizing errors in target displacement during a fractionated course of radiotherapy {{assumed that the}} underlying fraction-to-fraction variability in target displacement, known as the treatment <b>error</b> or <b>random</b> <b>error,</b> could be regarded as constant across patients. More recent approaches have modelled target displacement allowing for differences in <b>random</b> <b>error</b> between patients. However, until recently {{it has not been}} feasible to compare the goodness of fit of alternate models of <b>random</b> <b>error</b> rigorously. This is because the large volumes of real patient data necessary to distinguish between alternative models have only very recently become available. This work uses real-world displacement data collected from 365 patients undergoing radical radiotherapy for prostate cancer to compare five candidate models for target displacement. The simplest model assumes constant <b>random</b> <b>errors</b> across patients, while other models allow for <b>random</b> <b>errors</b> that vary according to one of several candidate distributions. Bayesian statistics and Markov Chain Monte Carlo simulation of the model parameters are used to compare model goodness of fit. We conclude that modelling the <b>random</b> <b>error</b> as inverse gamma distributed provides a clearly superior fit over all alternatives considered. This finding can facilitate more accurate margin recipes and correction strategies...|$|R
40|$|<b>Random</b> <b>errors</b> in the {{measurement}} of 10 commonly investigated cardiovascular risk factors (systolic and diastolic blood pressure, blood cholesterol, blood glucose, pulse rate, body mass index (BMI), cigarette consumption, passive smoking, alcohol intake and physical exercise) were assessed in a general population cohort (n = 2517) and a workforce cohort (n = 8008). <b>Random</b> <b>errors</b> were estimated from regression dilution ratios (lower ratios imply greater <b>random</b> <b>error,</b> and a ratio of one implies no <b>random</b> <b>error).</b> All of the risk factors, except for BMI (which had regression dilution ratios of 0. 93 and 0. 98 in the two cohorts), were measured with substantial levels of <b>random</b> <b>error.</b> Particularly low regression dilution ratios were observed for physical exercise (0. 28 and 0. 39) and pulse rate (0. 47 and 0. 56). For each of these risk factors, {{with the possible exception}} of BMI, associations with long-term average values could be importantly biased toward the null unless appropriate corrections are made...|$|R
40|$|Measured surface-atmosphere fluxes {{of energy}} (sensible heat, H, and latent heat, LE) and CO 2 (FCO 2) {{represent}} the ‘‘true’’ flux {{plus or minus}} potential random and systematic measurement errors. Here, we use data from seven sites in the AmeriFlux network, including five forested sites (two of which include “tall tower” instrumentation), one grassland site, and one agricultural site, to conduct a cross-site analysis of <b>random</b> flux <b>error.</b> Quantification of this uncertainty is a prerequisite to model-data synthesis (data assimilation) and for defining confidence intervals on annual sums of net ecosystem exchange or making statistically valid comparisons between measurements and model predictions. We differenced paired observations (separated by exactly 24 h, under similar environmental conditions) to infer {{the characteristics of the}} <b>random</b> <b>error</b> in measured fluxes. <b>Random</b> flux <b>error</b> more closely follows a double-exponential (Laplace), rather than a normal (Gaussian), distribution, and increase as a linear function of the magnitude of the flux for all three scalar fluxes. Across sites, variation in the <b>random</b> <b>error</b> follows consistent and robust patterns in relation to environmental variables. For example, seasonal differences in the <b>random</b> <b>error</b> for (H are small, in contrast to both LE and FCO 2, for which the <b>random</b> <b>errors</b> are roughly three-fold larger {{at the peak of the}} growing season compared to the dormant season. <b>Random</b> <b>errors</b> also generally scale with R(n (H and LE) and PPFD (FCO 2). For FCO 2 (but not (H or LE), the <b>random</b> <b>error</b> decreases with increasing wind speed. Data from two sites suggest that FCO 2 <b>random</b> <b>error</b> may be slightly smaller when a closed-path, rather than open-path, gas analyzer is used...|$|R
40|$|In {{order to}} judge the quality of survey data it is {{necessary}} above all to estimate {{the size of the}} individual error components, i. e. the systematic and the <b>random</b> <b>errors</b> in the responses (survey data) of the surveyed units and, if feasible, to determinate their true value. This paper develops an estimation procedure by which the individual <b>random</b> <b>errors</b> included in individual response values can be ascertained as well as the total <b>random</b> <b>error</b> of a survey. As costumary in statistical surveys an additive linear error model is assumed: Individual response value = true value + systematic <b>error</b> + <b>random</b> <b>error.</b> The true value of a characteristic together with its systematic error are referred to as the permanent (smooth) component of the survey value. To assess the value of the permanent component the variate difference method is suggested in analogy to time series analysis, and the difference between response and estimated permanent component is the estimated <b>random</b> <b>error.</b> The permanent component is being estimated for each unit in a first stochastic approximation by means of an average of responses in three (re-) enumerations. With more than three (re-) enumarations further individual <b>random</b> <b>errors</b> and their average could be computed, this meaning a horizontal aggregation of individual estimates. This procedure is demonstrated for a simulated case. Variate Difference Method for estimating individual <b>random</b> <b>errors,</b> main survey and re-enumerations, linear error model for the unit i, response variability...|$|R
40|$|Describes {{the design}} and {{realization}} of an error-correcting codec. For error protection of TV lines the realized codec utilizes a shortened (2196, 2136) BCH-code with 60 bits (2. 7 %) redundancy. The codec provides correction of 4 <b>random</b> <b>errors</b> or correction of one error burst up to 26 bits alternatively; 5 or 6 <b>random</b> <b>errors</b> can be detected. In the case of <b>random</b> <b>errors</b> the residual error rate after error correction is less than 10 - 8 even at 10 - 4 channel error rate...|$|R
30|$|In short, {{the direct}} method usually {{performs}} well in modelling, but will probably gain an uncontrollable <b>random</b> <b>error</b> when forecasting. On the contrary, the data-driven method can limit the forecasting <b>random</b> <b>error,</b> but {{makes it harder}} to construct a precise model for each subsequence. As a combination of the above two methods, the proposed DLC method can reduce the <b>random</b> forecasting <b>error</b> while guaranteeing modelling accuracy, providing improved forecasting results.|$|R
40|$|Abstract. Owing to {{the typical}} {{morphology}} of spectra of early-type stars, <b>random</b> <b>errors</b> play a much more impor-tant role in the derivation of their radial velocities than {{in the case of}} late-type spectra. We derive a generalised lower bound of the <b>random</b> <b>error</b> on a cross-correlation derived radial velocity shift in the presence of random noise on both object and template spectrum, and discuss its depen-dence on spectral parameters and noise. In order to limit in practice the <b>random</b> <b>error</b> to this lower bound for early-type spectra, we show that a number of specic cross-correlation peak centering techniques are required. The influence of rotational mismatch between object and tem-plate on the <b>random</b> <b>error</b> is examined. Finally, the widely used error estimate based on the so-called r-statistic is critically evaluated in the context of early-type spectra...|$|R
40|$|The {{problem of}} {{estimating}} the <b>random</b> <b>errors</b> in the LHC dipole is considered. The main contributions to <b>random</b> <b>errors</b> {{are due to}} random displacements of the coil position with respect to nominal design and to the variation of the magnetization of the superconducting cable. Coil displacements can be induced either by mechanical tolerances or by the manufacturing process. Analytical and numerical scaling laws that provide the dependence of the <b>random</b> <b>errors</b> due to <b>random</b> displacements on the multipolar order are worked out. Both simplified and more realistic models of the coil structure are analysed. The obtained scaling laws are used to extract from experimental field shape data the amplitude of the coil displacements in the magnet prototypes. Finally, <b>random</b> <b>errors</b> due to interstrand resistance variation during the ramp are estimate...|$|R
30|$|The {{estimation}} error includes two parts: systematic <b>error</b> and <b>random</b> <b>error.</b> Optimizing the distortion functions to derive the minimum error is a non‐trivial task. However, through some approximations, either the systematic or <b>random</b> <b>error</b> {{can be controlled}} to a small value. The approximations are illustrated as Lemmas 1 and 2.|$|R
40|$|The {{effects of}} {{imperfect}} gate operations in implementation of Shor's prime factorization algorithm are investigated. The gate imperfections may be classified into three categories: the systematic <b>error,</b> the <b>random</b> <b>error,</b> {{and the one}} with combined errors. It is found that Shor's algorithm is robust against the systematic errors but is vulnerable to the <b>random</b> <b>errors.</b> Error threshold {{is given to the}} algorithm for a given number $N$ to be factorized. Comment: 5 pages 4 figure...|$|R
30|$|This {{parameter}} represents {{influence of}} <b>random</b> <b>errors</b> on measured value.|$|R
40|$|The most {{frequently}} used methods for handling <b>random</b> <b>error</b> are largely misunderstood or misused by researchers. We propose a simple approach to quantify the amount of <b>random</b> <b>error</b> which does not require solid background in statistics for its proper interpretation. This method may help researchers refrain from oversimplistic interpretations relying on statistical significance...|$|R
5000|$|... The <b>errors</b> are <b>random,</b> {{thus the}} mean of the errors is zero: [...] * The <b>random</b> <b>errors</b> are {{uncorrelated}} with each other: [...] * The <b>random</b> <b>errors</b> are uncorrelated with the independent variables: , [...] and [...] * The method factors are assumed to be uncorrelated with one another and with the trait factors: ...|$|R
40|$|A {{class of}} linear {{convolutional}} codes for compound channels (i. e., channels on which burst and <b>random</b> <b>errors</b> occur) is given. It is shown that for any rate and burst length {{there exists a}} linear convolutional code of that rate that can correct almost all bursts of errors of that length and that requires a much smaller guard space than any code that can correct all possible bursts of errors {{less than or equal}} to that length. Protection against <b>random</b> <b>errors</b> can be obtained at the expense of the burst-error-correction capability. Explicit formulas are given for the number of bursts and <b>random</b> <b>errors</b> that can be corrected. A particular scheme to construct some codes is given. It is noted that these codes exhibit limited error propagation. A technique for constructing codes that can correct <b>random</b> <b>errors</b> within the guard space is also given...|$|R
30|$|In this study, we presume both {{normally}} distributed <b>random</b> <b>errors</b> {{and systematic}} errors and propose {{the use of}} the weighted likelihood method (WLL) (Hu and Zidek, 2002; Wang and Zidek, 2005) to address this problem. First, we demonstrate that WLSQ and WLL provide the same solution; however, the variance of <b>random</b> <b>errors</b> estimated by WLL exceeds that estimated by WLSQ. In order to clarify which method estimates more reasonable errors, both methods are applied to data contaminated with systematic errors in order to simulate hypocenter determination with an unsuitable velocity model. The solutions with both methods could be analytically obtained for the simulated data. This paper compares the variance of <b>random</b> <b>errors</b> estimated by the maximum likelihood method with the assumed ones. This comparison indicates that WLSQ underestimates the variance of the <b>random</b> <b>errors</b> and is unreliable as an approach to this problem.|$|R
50|$|<b>Random</b> <b>error</b> (or <b>random</b> variation) is due {{to factors}} which cannot or will not be controlled. Some {{possible}} reason to forgo controlling for these <b>random</b> <b>errors</b> is because {{it may be too}} expensive or it may be an oversight of these factors to control them each time the experiment is conducted or the measurements are made. Other reasons is that it may even be that whatever we are trying to measure is changing in time (see dynamic models), or is fundamentally probabilistic (as is the case in quantum mechanics — see Measurement in quantum mechanics). <b>Random</b> <b>error</b> often occurs when instruments are pushed to the extremes of their operating limits. For example, it is common for digital balances to exhibit <b>random</b> <b>error</b> in their least significant digit. Three measurements of a single object might read something like 0.9111g, 0.9110g, and 0.9112g.|$|R
30|$|The {{literature}} on measurement error suggests {{a wide variety}} of issues that might contribute to measurement errors in both current (contemporaneous panel) and recalled (retrospective) data. The implications of measurement error depend substantially {{on the nature of the}} problems. Truly <b>random</b> <b>errors</b> in continuous variables will not bias estimates of key statistics such as means or estimates of linear regression models when serving as the dependent variable (Bound et al. 2001). <b>Random</b> <b>errors</b> in an explanatory variable, x, will downward-bias or attenuate the estimated coefficient. <b>Random</b> <b>errors</b> in categorical or binary variables are more problematic as they bias model estimates and descriptive statistics.|$|R
40|$|Data input for the AVE-SESAME I {{experiment}} are {{utilized to}} describe the effects of <b>random</b> <b>errors</b> in rawinsonde data on the computation of ageostrophic winds. Computer-generated <b>random</b> <b>errors</b> for wind direction and speed and temperature are introduced into the station soundings at 25 mb intervals from which isentropic data sets are created. Except for the isallobaric and the local wind tendency, all winds are computed for Apr. 10, 1979 at 2000 GMT. Divergence fields reveal that the isallobaric and inertial-geostrophic-advective divergences are less affected by rawinsonde <b>random</b> <b>errors</b> than the divergence of the local wind tendency or inertial-advective winds...|$|R
30|$|<b>Random</b> <b>errors</b> of radar {{distance}} and the position: 200 and 0.3.|$|R
30|$|For the <b>random</b> <b>error</b> R_ 2, we {{have the}} {{following}} estimation.|$|R
50|$|Random: <b>Random</b> <b>errors</b> {{are small}} {{unavoidable}} fluctuations. They {{are caused by}} imperfections in measuring equipment, eyesight, and conditions. They can be minimized by redundancy of measurement and avoiding unstable conditions. <b>Random</b> <b>errors</b> tend to cancel each other out, but checks {{must be made to}} ensure they are not propagating from one measurement to the next.|$|R
50|$|Precision is a {{description}} of <b>random</b> <b>errors,</b> a measure of statistical variability.|$|R
