6|41|Public
30|$|Multiple parameters: For {{position}} verification, plausibility is {{checked for}} multiple parameters. The parameters are called maximum density threshold (MDT), acceptance range threshold (ART) and mobility grade threshold (MGT). Messages outside the range will be discarded. Multiple messages sent from single location indicates false position. For a Sybil attack detection, map-based verification and claim position are used. Map-based assign <b>plausibility</b> <b>value</b> to beacon messages {{and compare it}} with positions in road map [59].|$|E
40|$|We {{consider}} the iterated belief change that occurs following an alternating sequence of actions and observations. At each instant, an agent has {{beliefs about the}} actions that have occurred as well as beliefs about the resulting state of the world. We represent such problems by a sequence of ranking functions, so an agent assigns a quantitative <b>plausibility</b> <b>value</b> to every action and every state at each point in time. The resulting formalism is able to represent fallible belief, erroneous perception, exogenous actions, and failed actions. We illustrate that our framework is a generalization of several existing approaches to belief change, and it appropriately captures the non-elementary interaction between belief update and belief revision. 1...|$|E
40|$|In this {{contribution}} {{we present}} a method for checking plausibility of electromagnetically measured position values {{in the vicinity of}} metallic surgical instruments and OR equipment Furthermore, the method is used in a simple and applicable error compensation algorithm. We present the results of a first analysis of the method for navigation in ENT surgery. The concept is based on redundant pose information of a tool. This is achieved by a rigid electromagnetic localizer consisting of two sensor elements each measuring the pose of the tool tip separately. The length of the difference vector of the measured positions is a measure for the plausibility. We applied the method in a navigation system for FESS and determined the correlation of the measuring error caused by metallic instruments. The experiment showed that the computed <b>plausibility</b> <b>value</b> shows sufficient correlation to relevant measuring errors...|$|E
40|$|AbstractIt {{is shown}} that unrenormalized {{plausibility}} functions, interpreted as {{measures of the}} impact of contrary evidence, satisfy the axioms for local computation proposed by Shenoy and Shafer. It is argued that in cases where the underlying domain is generated as the union of singletons representing most specific descriptions, plausibility functions rather than belief functions are the natural measures of uncertainty. In those cases, decision should be {{made on the basis of}} the ratios of <b>plausibility</b> <b>values.</b> The latter are unaffected by renormalization, which is superfluous...|$|R
40|$|We {{discuss the}} problem of {{construction}} of inference procedures which can manipulate with uncertainties measured in ordinal scales and fulfill to the property of strict monotonicity of conclusion. The class of A-valuations of plausibility is considered where operations based only on information about linear ordering of <b>plausibility</b> <b>values</b> are used. In this class the modus ponens generating function fulfiling to the property of strict monotonicity of conclusions is introduced. Comment: Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI 1994...|$|R
40|$|AbstractIn this paper, a {{semantic}} basis for Possibility Theory based on likelihood functions is presented. In some cases, possibilities {{have been considered}} as approximations of Shafer plausibility measures. This approximation exchanges exactness of <b>plausibility</b> <b>values</b> for the simplicity of use of possibility values. In this paper, a different direction is followed. Possibility measures are considered as the supremum {{of a family of}} likelihood functions. This is an exact interpretation, not an approximation. The minimum rule to combine possibility distributions is justified in this framework under general conditions. Conditions under which other rules can be applied are also studied...|$|R
40|$|In this study, the {{profession}} of teaching physical education for secondary school students is aimed to develop a likert-type attitude scale (SASPPET). The scale is consisted 5 likert -type of 25 items {{at the beginning of}} the study. The scale was applied for 400 students who randomly selected among the students of the 11 th grade Balıkesir Secondary school of the central. At the end of the application the scale was reduced to 18 items. To determine the validity of the scale structure as a result of factor analysis scale factor loads items ranging between 0. 35 - 0. 89 was influenced by 3 and the Kaiser-Meyer-Olkin (KMO) value was 0. 890, Barlett test <b>plausibility</b> <b>value</b> was found 0. 00. Calculated for the scale,reliability, internal consistency coefficint (Cronbach alpha) values were α= 0. 93. The findings indicate that the scale has a valid and reliable structure. It was concluded that the scale, obtained from these findings, was a valid and reliable scale in measuring the attitude directed at {{the profession}} of physical education teaching...|$|E
40|$|Ethotic {{arguments}} and fallacies: The credibility function in multi-agent dialogue systems DOUGLAS N. WALTON In this paper, it is shown how formal dialectic {{can be extended}} to model multiagent argumentation in which each participant is an agent. An agent {{is viewed as a}} participant in a dialogue who not only has goals, and the capability for actions, but who also has stable characteristics of types that can be relevant to an assessment of some of her arguments used in that dialogue. When agents engage in argumentation in dialogues, each agent has a credibility function that can be adjusted upwards or downwards by certain types of arguments brought forward by the other agent in the dialogue. One type is the argument against the person or argumentum ad hominem, in which personal attack on one party's character is used to attack his argument. Another is the appeal to expert opinion, traditionally associated with the informal fallacy called the argumentum ad verecundiam. In any particular case, an agent will begin a dialogue with a given degree of credibility, and what is here called the credibility function will affect the plausibility of the arguments put forward by that agent. In this paper, an agent is shown to have specific character traits that are vital to properly judging how this credibility function should affect the plausibility of her arguments, including veracity, prudence, sincerity and openness to opposed arguments. When one of these traits is a relevant basis for an adjustment in a credibility function, there is a shift to a subdialogue in which the argumentation in the case is re-evaluated. In such a case, it is shown how the outcome can legitimately be a reduction in the credibility rating of the arguer who was attacked. Then it is shown how the credibility function should be brought into an argument evaluation in the case, yielding the outcome that the argument is assigned a lower <b>plausibility</b> <b>value...</b>|$|E
40|$|The {{research}} {{presented in}} this thesis develops new statistical techniques for estimating regional skewness coefficients to improve flood frequency analysis in the United States. Flood frequency guidelines for the United States, specified in Bulletin 17 B, recommend fitting the log-Pearson Type III (LP 3) distribution to the series of annual flood maxima, in which the third moment of the distribution, the skewness coefficient gamma, is combined with a regional skewness coefficient to improve its precision. The research presented here extends the quasi-analytic Bayesian analysis of the Generalized Least Squares (GLS) regional hydrologic regression framework introduced by Reis et al. [2005] to more accurately and precisely estimate regional skewness coefficients. Specifically, formulas derived within a Bayesian regression framework for the computation of estimators, standard errors, and diagnostic statistics are provided by Reis [2005] and Reis et al. [2005]. Diagnostic statistics further developed here include a Bayesian <b>plausibility</b> <b>value,</b> pseudo adjusted R-squared, pseudo-Analysis of Variance table, two diagnostic error variance ratios, as well as leverage and influence metrics. In addition, this research also develops a new delta-influence diagnostic statistic which, {{in conjunction with the}} Bayesian extension of GLS leverage and influence metrics, can be used to better identify rogue observations and to effectively address lack-of-fit when estimating skewness coefficients. Currently, Bulletin 17 B allows for regional skew values to be obtained from the skew map included with the Bulletin. As it is over 30 years old, the regional skew values from the Bulletin 17 B skew map do not reflect annual maximum data acquired since 1976. This increase in available data, along with advances in computing power to support the Bayesian GLS regional hydrologic regression framework, allow for a much more precise estimate of the regional skewness coefficient for use in flood frequency analysis. This research employs the Bayesian GLS regression framework to estimate regional log-space skewness coefficients for three data sets: the Illinois River basin, the state of South Carolina, and the Southeastern United States. Bulletin 17 B allows for the generation of skew prediction equations as an alternative method for determining regional skew coefficients when the mean squared error of the equations is smaller than reported from the Bulletin's skew map. These skew prediction equations can be generated using Ordinary Least Squares analysis, Weighted Least Squares analysis, Generalized Least Squares analysis employing the method of moment model-error-variance estimator introduced by Stedinger and Tasker [1985, 1986 ab], or the new Bayesian GLS estimator. The advantages of using the Bayesian GLS estimation technique to determine a skew prediction equation are demonstrated here in the Illinois River basin and the state of South Carolina studies. To correctly analyze the Southeastern United States data set, methods are developed for identifying and screening redundant sites corresponding to nested watersheds with similar drainage areas. Special attention is devoted to developing an improved cross-correlation model of annual peak flows. The Bayesian GLS analysis using 342 stations from the Southeastern U. S. results in a highly accurate, constant regional skew model (^gamma = - 0. 019), with an average variance of prediction equal to 0. 14. More complex models which include regional information and basin characteristics as additional regression parameters result in very little improvement. The application of the Bayesian estimator in the Southeastern study generates improved results over the mean square error of 0. 30 reported for the Bulletin 17 B regional map skew...|$|E
40|$|International audienceIn this paper, we {{consider}} parameter identification from measurement fields in an uncertain environment. An approach {{based on the}} theory of belief functions is developed {{to take into account}} all possible sources of information. Information from measurements is described through likelihood-based belief functions, while consonant random sets are used to handle prior information on the model parameters. Next, we construct the posterior random set by combining measurement and prior information using Dempster's rule. To summarize the posterior random sets, we propose to find the minimal-area region in the parameter space, whose belief and <b>plausibility</b> <b>values</b> exceed given thresholds. This approach was applied to identify the elastic properties of a 2 D plate from a measured kinematic field...|$|R
40|$|In his {{groundbreaking}} formal work on context-dependent expressions, Kaplan (1989 a) {{argued that}} the conventional meaning of natural language indexicals cannot be modified by sentential operators. Kaplan’s original theory, however, has been recently challenged by empirical studies. It turned out that in certain languages, for example in Amharic and Zazaki, attitude operators are able to modify the meaning of indexicals. Some of these empirical findings appear to be clearly inconsistent with the Kaplanian account. From a metatheoretical point of view, the current debate about the operator-sensitivity of indexicals can also {{be interpreted as a}} debate about the role of linguistic data. In following the p-model and considering linguistic data as statements with <b>plausibility</b> <b>values,</b> one can make some progress in this field of research...|$|R
40|$|Based on a {{black box}} model of a complex system, and on {{intervals}} and probabilities describing the known information about the inputs, we want to estimate the system’s reliability. This problem is motivated {{by a number of}} problem areas, most specifically in engineering reliability analysis under conditions of poor measurement and high complexity of system models. Using the results of tests performed on the system’s computer model, we can estimate the lower and upper bounds of the probability that the system is in a desirable state. This is equivalent to using Monte-Carlo sampling to estimate cumulative belief and <b>plausibility</b> <b>values</b> of functionally propagated finite random intervals. In this paper, we prove that these estimates are correct in the sense that under reasonable assumptions, these estimates converge to the actual probability bounds. Keywords: Interval probability, interval analysis, reliability analysis, Dempster-Shafer evidence theory, random sets, random intervals, epistemic uncertainty, Monte-Carlo sampling. ...|$|R
40|$|Modal logic {{interpretations of}} {{plausibility}} and belief measures are developed {{based on the}} observation that the inverse of the value assignment function in a model of modal logic induces the upper plausibility and lower belief measures of the plausibility and belief measures induced by the accessibility relation, regarded as a multivalued mapping. Keywords. Accessibility relation, basic probability assignment, belief measure, modal logic, multivalued mapping, <b>plausibility</b> measure, <b>value</b> assignment function. ...|$|R
40|$|The use of {{attribute}} sets to rank {{units of}} health provision (e. g., states, organizations) against policy goals {{is an essential}} task within decision-making and analysis. This paper elucidates and compares two techniques, SMARTER (Simple Multiattribute Rating Technique Exploiting Ranks) and CaRBS (Classification and Ranking Belief Simplex), within an expositional ranking of US states long-term care (LTC) systems against the policy goal of providing a balance between (traditionally dominant) institutional care, and alternative home and community-based services (HCBS). While the (more established) SMARTER technique is used primarily for comparative purposes, greater {{emphasis is placed on}} elucidating CaRBS which is based on the Dempster-Shafer theory of evidence. It is shown that CaRBS offers four appealing features for health policy analysis: (1) the capacity to rank using either of two confidence measures (DST-related belief and <b>plausibility</b> <b>values),</b> (2) a systematic approach to managing missing data, (3) the production of stable rankings, and (4) the simplex plot method of data representation. In addition to discussing the LTC policy implications of the study findings, the issues of rank order stability and the management of missing data are discussed with respect to the two techniques employed...|$|R
40|$|The {{likelihood}} {{approach to}} statistics {{can be interpreted}} as a theory of fuzzy probability. This paper presents a generalization of credal networks obtained by generalizing imprecise probabilities to fuzzy probabilities; that is, by additionally considering the relative <b>plausibility</b> of different <b>values</b> in the probability intervals...|$|R
30|$|Naturally, {{outcomes}} {{may be more}} or less {{specific to}} this set of baseline assumptions. In case of small changes in the parameters, the differences compared to the baseline are typically of a quantitative nature rather than a qualitative nature. If changes get larger, the results can also change qualitatively. Many parameters have been estimated in other papers so that for most parameters there is certainly an amount of empirical <b>plausibility</b> to these <b>values.</b>|$|R
40|$|International audienceThis {{paper is}} focused on the {{development}} of a damage detection indicator that combines a data driven baseline model (reference pattern obtained from the healthy structure) based on principal component analysis (PCA) and multivariate hypothesis testing. More precisely, a test for the <b>plausibility</b> of a <b>value</b> for a normal population mean vector is performed. The results indicate that the test is able to accurately clasify random samples as healthy or not...|$|R
40|$|International audienceIn this paper, {{we provide}} two axiomatizations of {{algebraic}} expected utility, {{which is a}} particular generalized expected utility, in a von Neumann-Morgenstern setting, i. e. uncertainty representation {{is supposed to be}} given and here to be described by a <b>plausibility</b> measure <b>valued</b> on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility, [...] .) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker’s attitude toward uncertainty...|$|R
40|$|This {{paper is}} focused on the {{development}} of a damage detection indicator that combines a data driven baseline model (reference pattern obtained from the healthy structure) based on principal component analysis (PCA) and multivariate hypothesis testing. More pre- cisely, a test for the <b>plausibility</b> of a <b>value</b> for a normal population mean vector is performed. The results indicate that the test is able to accurately clasify random samples as healthy or not. Peer ReviewedPostprint (author’s final draft...|$|R
40|$|International audienceIn this paper, {{we provide}} an {{algebraic}} approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards {{are expressed in}} a semiring structure, uncertainty is represented by a decomposable <b>plausibility</b> measure <b>valued</b> on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies {{the use of the}} Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty...|$|R
40|$|Expected Utility: Algebraic Expected Utility In this paper, {{we provide}} two axiomatizations of {{algebraic}} expected utility, {{which is a}} particular generalized expected utility, in a von Neumann-Morgenstern setting, i. e. uncertainty representation {{is supposed to be}} given and here to be described by a <b>plausibility</b> measure <b>valued</b> on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility, [...] .) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty measure, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty. Comment: Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI 2006...|$|R
30|$|The data on monthly {{household}} {{incomes are}} provided with the data. They also include equivalized household incomes. We use the variable oecdincn that applies the new OECD equivalence scale and deflate the equivalized household incomes with {{the consumer price index}} from the German Statistical Office. Unfortunately, the income data include some extremely large income values, also in the samples of UBII recipients. In the absence of any information on the <b>plausibility</b> of these <b>values</b> we decided to use the log-transformation in order to downsize the large values and make the distribution more symmetric.|$|R
40|$|Identification {{can be a}} {{major issue}} in causal {{modeling}} contexts, and in contexts where observational studies have various limitations. Partially identified models can arise, whereby the identification region for a target parameter [...] the set of values consistent with the law of the observable data [...] is strictly contained in the set of a priori plausible values, but strictly contains the single true value. The first part of this paper reviews the use of Bayesian inference in partially identified models, and describes the large-sample limit of the posterior distribution over the target parameter. This limiting distribution will have the identification region as its support. The second part of the paper focuses on the informativeness of the shape of the limiting distribution. This provides a point of departure with non-Bayesian approaches, since they focus on inferring the identification region without attempting to speak to relative <b>plausibilities</b> of <b>values</b> inside the identification region. The utility of the shape is investigated in several partially identified models. ...|$|R
40|$|The {{speed of}} the center of mass of the binary black hole is rediscussed using actual post-Newtonian orbit integrations. The {{velocity}} aqcuired by the center of mass is found to greater than the estimate by Fitchett [1]. The exact values is uncertain but can be as high as 2000 km/s. The result depends on the merger distance used. The <b>plausibility</b> of <b>values</b> of this distance are discussed. Application to double radio sources is reviewed and possible applications are suggested. keywords: black holes, binary, gravitational radiation 1 Introduction There are a number of ways the general relativity affects the motion of few body systems. These effects are most conveniently described with post-Newtonian approximations. The order to which this approximation has been developed is fi 5 (PNA 2. 5) where fi = v=c. The PNA 2. 5 causes circularization of the orbit, precession of the perihelion and loss of energy. The error term of order O(fi 6) hides many complicated terms but the dominant irreduci [...] ...|$|R
30|$|This step {{includes}} collecting {{sample data}} that are available and deciding which data, including format and size, will be needed. To better understand the strengths and limitations of the data, it also includes checking data completeness, redundancy, missing <b>values,</b> the <b>plausibility</b> of attribute <b>values.</b> Background knowledge {{can be used to}} guide these checks. Another critical part of this step is estimating {{the costs and benefits of}} each data source and deciding whether further investment in collection is worthwhile. Finally, this step includes verifying that the data matches one or more DS subtasks in the last step.|$|R
40|$|Abstract. Continuous {{constraint}} reasoning assumes {{the uncertainty of}} numerical variables within given bounds and propagates such knowledge through a network of constraints, reducing the uncertainty. In some problems there is also information about the <b>plausibility</b> distribution of <b>values</b> within such bounds. However, the classical constraint framework cannot accommodate that information. This paper describes how the continuous constraint programming paradigm may be extended, {{in order to accommodate}} some probabilistic considerations, bridging the gap between the pure interval-based approach, that does not consider likelihoods, and the pure stochastic approach, that does not guarantee the safety of the obtained results...|$|R
40|$|The {{lognormal}} RCVs possess bet-ter biological <b>plausibility.</b> Paradoxi-cal <b>values</b> of decreases {{greater than}} 100 % are eliminated. However, {{in view of}} monitoring applications, these RCVs are still rather high. The corrected lognormal RCVs refer to the commonly used 5 % bidirectional statistical error. This RCV setup im-plies that 5 % of clinically stable patients show changes greater than the RCV (false positives). However, {{for the treatment of}} heart failure, false negatives present the major risk, and it is imperative that deteri-oration in a patient’s clinical condi-tion is not missed so that appropriate clinical intervention can be initiated (9). The common probability “insur-ance ” of 95 % against false positives could be too high, and another value, 80 %, would be clinically more appro-priate to lower the false-negative rate. When 80 % is used in the con-struction of RCVs, then NT-proBNP week-to-week lognormal RCVs nar-row to 85 % for increases and to 46 % for decreases. The important message we take from this analysis is that the skew-ness of the distribution requires ade-quate methods to deal with it to achieve clinically and biologically valid RCVs...|$|R
2500|$|The prior {{represents}} {{beliefs about}} [...] before [...] is available, {{and it is}} often specified by choosing a particular distribution among a set of well-known and tractable families of distributions, such that both the evaluation of prior probabilities and random generation of values of [...] are relatively straightforward. For certain kinds of models, it is more pragmatic to specify the prior [...] using a factorization of the joint distribution of {{all the elements of}} [...] in terms of a sequence of their conditional distributions. If one is only interested in the relative posterior <b>plausibilities</b> of different <b>values</b> of , the evidence [...] can be ignored, as it constitutes a normalising constant, which cancels for any ratio of posterior probabilities. It remains, however, necessary to evaluate the likelihood [...] and the prior [...] For numerous applications, it is computationally expensive, or even completely infeasible, to evaluate the likelihood, which motivates the use of ABC to circumvent this issue.|$|R
40|$|We discuss precise {{assumptions}} entailing Bayesianism in {{the line}} of investigations started by Cox, and relate them to a recent critique by Halpern. We show that every finite model which cannot be rescaled to probability violates a natural and simple refinability principle. A new condition, separability, was found su#cient and necessary for rescalability of infinite models. We finally characterize the acceptable ways to handle uncertainty in infinite models based on Cox's assumptions. Certain closure properties must be assumed before all the axioms of ordered fields are satisfied. Once this is done, a proper plausibility model can be embedded in an ordered field containing the reals, namely either standard probability (field of reals) for a real <b>valued</b> <b>plausibility</b> model, or extended probability (field of reals and infinitesimals) for an ordered plausibility model. The end result is that if our assumptions are accepted, all reasonable uncertainty management schemes must be based on se [...] ...|$|R
5000|$|The prior {{represents}} {{beliefs about}} [...] before [...] is available, {{and it is}} often specified by choosing a particular distribution among a set of well-known and tractable families of distributions, such that both the evaluation of prior probabilities and random generation of values of [...] are relatively straightforward. For certain kinds of models, it is more pragmatic to specify the prior [...] using a factorization of the joint distribution of {{all the elements of}} [...] in terms of a sequence of their conditional distributions. If one is only interested in the relative posterior <b>plausibilities</b> of different <b>values</b> of , the evidence [...] can be ignored, as it constitutes a normalising constant, which cancels for any ratio of posterior probabilities. It remains, however, necessary to evaluate the likelihood [...] and the prior [...] For numerous applications, it is computationally expensive, or even completely infeasible, to evaluate the likelihood, which motivates the use of ABC to circumvent this issue.|$|R
40|$|We {{consider}} {{the complexity of}} combining bodies of evidence {{according to the rules}} of the Dempster [...] Shafer theory of evidence. We prove that, given as input a set of tables representing basic probability assignments m 1; : : :; mn over a frame of discernment Θ, and a set A ` Θ, the problem of computing the combined basic probability value (m 1 Φ: : : n) (A) is #P -complete. As a corollary, we obtain that while the simple belief, <b>plausibility,</b> and commonality <b>values</b> Bel(A), Pl(A), and Q(A) can be computed in polynomial time, the problems of computing the combinations (Bel 1 Φ : : : Φ Beln) (A), (Pl 1 Φ : : : Φ Pln) (A), and (Q 1 Φ : : : Φ Qn) (A) are #P -complete. 1 Introduction The Dempster [...] Shafer theory of evidence [8] has recently been attracting increasing attention as a theoretically well-founded way of dealing with the problem of uncertain information in artificial intelligence systems (cf. [2, 5, 6, 11]). The apparently prohibitive computationa [...] ...|$|R
40|$|Selected allotropes of {{phosphorus}} are {{investigated by}} {{different levels of}} density functional theory (DFT) calculations to evaluate the relative stability orders with a special focus {{on the role of}} van der Waals interactions. Phosphorus is an excellent reference system with a large number of allotropes. Starting from low-dimensional molecular (0 D, white P) and polymer structures (1 D, P nanorods) to layered (2 D, black P) and tubular structures (2 D and 3 D, crystalline forms of red P), covalent structure motifs are interconnected by van der Waals interactions. They are a key factor for the correct energetic description of all P allotropes. A comparative study is carried out within the local density approximation (LDA) and the generalized gradient approximation (GGA), with and without implementation of a dispersion correction by Grimme (GGA-D 2). Our intention is to achieve a reasonable agreement of our calculations with experimental data, the <b>plausibility</b> of energy <b>values,</b> and the treatment of long-range interactions. The effect of van der Waals interactions is exemplified for the interlayer distances of black phosphorous and its electronic structure...|$|R
40|$|It {{is argued}} that {{continued}} attempts to estimate MSYR from accumulating data, to refine the plausible range of values for this parameter and relative plausibilities within this range, cannot be other than a crucial component {{of the process of}} development (and, in due course, refinement) of the Revised Management Procedure (RMP) and of the interpretation {{of the results of the}} associated Implementation Simulation Trials (ISTs) for particular RMP applications. In 1987, when the range of MSYR values for RMP trials was first specified, four of the six independent sources of information available suggested definite "low" MSYR values (~ 1 %). None of these four sources appears to have survived to the present. Estimates of MSYR for twenty populations have become available since 1987 - eleven based on population model fits and the balance on the relationship MSYR > r(0) / 2. Two arguments advanced previously against the use of this last relationship are considered: the one is dismissed because it lacks support in empirical data, while the other appears negated by an analysis by Best (1993). In the fourteen cases where estimates of MSYR (in terms of uniform selectivity harvesting on the 1 + population) are determined with reasonable precision, most lie in the 2 %- 6 % range, and only one of these has a lower 90 or 95 % confidence/probability bound below 1 %. Cases of low point estimates of MSYR show wide confidence intervals not incompatible with this 2 - 6 % range. Thus, evidence forthcoming since 1987 (much of it subsequent to 1993 when the Scientific Committee last discussed this issue substantively) would seem to support a change in the Committee's perception at that time of the likely range of values for MSYR for baleen whale stocks, as well as informing judgments on the relative <b>plausibilities</b> of <b>values</b> within this range...|$|R
40|$|In {{this paper}} we study the {{plausibility}} of navigation relevant AIS parameter based on {{time series analysis}} of HELCOM AIS data, obtained during September 2011. Previous analysis[1] of AIS data sets were mainly based on studies of not known or not existent (default) values in the AIS messages. The results of these {{studies have shown that}} especially true heading (THDG) and rate of turn (ROT) parameters were strongly affected by default / unknown values. We discuss algorithms to evaluate the plausibility of data contained in the AIS message for dynamic data, like time, position, speed, and course. The contained information is checked against the <b>plausibility</b> of their <b>values</b> according to other parameters in the time series. For example the speed can be calculated from the two position information from the AIS message and the time difference between these positions. The dynamic data is consistent in this case when both the calculated and the measured value are similar within an appropriate limit. For static data the determination of plausibility is based on checks against the AIS specification. The obtained results will serve as error model {{for the development of a}} maritime traffic situation assessment facility, where AIS, radar and specific PNT data shall be fused together to create a reliable traffic situation image...|$|R
40|$|In disseminated {{intravascular}} coagulation (DIC) {{there is}} extensive crosstalk between activation of inflammation and coagulation. Endogenous anticoagulatory pathways are downregulated by inflammation, thus decreasing the natural anti-inflammatory mechanisms that these pathways possess. Supportive strategies aimed at inhibiting activation of coagulation and inflammation may theoretically be justified and {{have been found to}} be beneficial in experimental and initial clinical studies. This review assembles the available experimental and clinical data on biological mechanisms of antithrombin in inflammatory coagulation activation. Preclinical research has demonstrated partial interference of heparin – administered even at low doses – with the therapeutic effects of antithrombin, and has confirmed – at the level of cellular mechanisms – a regulatory role for antithrombin in DIC. Against this biological background, re-analyses of data from randomized controlled trials of antithrombin in sepsis suggest that antithrombin has the potential to be developed further as a therapeutic agent in the treatment of DIC. Even though there is a lack of studies employing satisfactory methodology, the results of investigations conducted thus far into the mechanisms of action of antithrombin allow one to infer that there is biological <b>plausibility</b> in the <b>value</b> of this agent. Final assessment of the drug’s effectiveness, however, must await the availability of positive, prospective, randomized and placebo-controlled studies...|$|R
40|$|There are 266 endos and {{equations}} in the model, many nonlinear, and 130 exos. Of the 266 equations 100 * are behavioural and 166 identities. Period of estimation is the 18 years 1960 - 1977, {{time unit}} being the year. The behavioural system is dynamic, i. e., some equations containing on r. h. s. endos lagged one year. Method of solution is, {{for the most}} part, single equation ordinary least squares (SEOLS). 2 SLS was resorted to for the wage-price sectors but it is stated {{that there was little}} difference between SEOLS and 2 SLS in coefficient estimates of these equations. There is a description and commentary on each of the equations, giving the usual statistical test functions, R 2, D. W., coefficient estimates with t-values, etc. for each of the behavioural equations. Much of the commentary in the report bears on the <b>plausibility</b> of the <b>values</b> and signs of the coefficients of the latter. There are many diagrams comparing graphs for actual and estimated (miscalled "predicted", as is usual). The numerical effect is shown of unit increases in certain policy instruments (mainly public authority taxes and expenditure), for six years starting with base year 0, for 36 most important endos...|$|R
