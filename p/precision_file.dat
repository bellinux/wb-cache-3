1|11|Public
5000|$|Version 5.50 (2017) adds {{support for}} a master {{password}} {{which can be used}} to encrypt passwords stored in WinRAR. The default RAR format is changed to version 5. Other additions include: support for decompressing Lzip archives; support for high <b>precision</b> <b>file</b> dates, longer file names and larger file sizes for TAR archives, and many minor changes and error fixes.|$|E
5000|$|Dec. 9, 1969. “System and Method for Alternating Current Machines, and Apparatus Therefor”. (Variable-speed, <b>Precision</b> frequency, {{originally}} <b>filed</b> Dec. 5, 1952, Pat. Application No. 324,318.).|$|R
40|$|About {{twenty years}} ago, Choptuik studied numerically the {{gravitational}} collapse (Einstein field equations) of a massless scalar field in spherical symmetry, and found strong {{evidence for a}} universal, self-similar solution {{at the threshold of}} black hole formation. We prove rigorously the existence of a real analytic solution, that we interpret as the solution observed by Choptuik. Our construction covers an open neighborhood of the past light cone of the singularity. The proof is computer assisted. Starting from an explicit approximate solution, we show that nearby there is a true solution. The source code and a high <b>precision</b> data <b>file</b> (about 80 significant decimal digits, with rigorous error bounds) are included. We do not study perturbations. Comment: 46 page...|$|R
40|$|Abstract. We {{present a}} system that {{automatically}} turns {{the pages of the}} music score for musicians during a performance. It is based on a new algorithm for following an incoming audio stream in real time and aligning it to a music score (in the form of a synthesised audio <b>file).</b> <b>Precision</b> and robustness of the algorithm are quantified in systematic experiments, and a demonstration using an actual page turning machine built by an Austrian company is described. ...|$|R
40|$|AbstractAmong {{numerous}} Chinese keyword extraction methods, Chinese {{characteristics were}} shortly considered. This phenomenon {{going against the}} precision enhancement of the Chinese keyword extraction. An extended term frequency based method(Extended TF) is proposed in this paper which combined Chinese linguistic characteristics with basic TF method. Unary, binary and ternary grammars for the candidate keyword extraction {{as well as other}} linguistic features were all taken into account. The method establishes classification model using support vector machine. Tests show that the proposed extraction method improved key words precision and recall rate significantly. We applied the key words extracted by the extended TF method into the text file classification. Results show that the key words extracted by the proposed method contributed greatly to raising the <b>precision</b> of text <b>file</b> classification...|$|R
40|$|Abstract: Selective Laser Sintering (SLS) {{has many}} {{advantages}} such as fast building speed, {{wide range of}} materials, complicate shape of prototype and simple post-treatment. However, the shrinkage and mechanical stresses result in lost of the accuracy. This paper investigates the dimension precision of polymer SLS prototypes and proposes methods to control dimensions in suitable degree. The influencing factors of dimensional <b>precision</b> in the <b>file</b> preparation of CAD model, manufacturing system of SLS prototype and sintering process are analyzed. It indicates that the sintering shrinkage in sintering process {{is the main reason}} of dimensional changes. To improve the dimensional precision, the measures are proposed to diminish the shrinkage: improving material of prototype, optimizing the sintering process and compensating the lost of dimension. After analysis the experimental results, the rule of error changes is summarized by fitting equations and feedback to controlling software of SLS system to improve the dimensional precision effectively. The SLS prototype could be controlled in 100 ± 0. 20 mm through compensation...|$|R
40|$|You {{are heavily}} {{encouraged}} to update as this incorporates many new features and bug-fixes. Fix a bug when reading non-Gamma TSHS files, now the supercell information is correct. tbtncSileSiesta now distinguishes between: electronic_temperature [K] and kT [eV] where the units {{are not the}} same. Fixed TBT_DN. nc TBT_UP. nc detection as a Sile Added information printout for the TBT. nc files sdata siesta. TBT. nc [...] info will print out what information is contained in the file. Atoms overhauled {{with a lot of}} the utility routines inherent to the Geometry object. It is now much faster to perform operations on this object. The FDF sile now allows setting and retrieving variables from the fdf file. Hence one may now set specific fdf flags via: sdata RUN. fdf [...] set SolutionMethod Transiesta Changed default output <b>precision</b> for TXT <b>files</b> to. 8 f. Additionally one may use flag [...] format in sgeom to define the precision. Shapes have been added. There are now several Shapes which may be used to easily find atoms within a given Shape. This should in principle allow construction of very complex Shapes and easier construction of complex Hamiltonian...|$|R
40|$|INPAFAMDB – add {{the missing}} piece to your patent prior-art search on STN ® INPAFAMDB – the International Patent Family Database – is a bibliographic patent family database, {{covering}} {{the full spectrum}} of patent technologies for more than 90 issuing authorities, dating from the early 1800 s. The database features a one-record-per-patent-family file design to provide maximum multi-file prior-art synergy with CAplus SM and the Derwent World Patents Index ® (DWPI SM) on STN®. INPAFAMDB for prior-art technology searching • Access {{the full spectrum of}} global technologies from patents including chemistry, life-science, and engineering • Comprehensive text searching of applicant titles and abstracts • The complete and up-to-date archive of all European ECLA and International IPC patent classifications, providing for optimum recall and <b>precision</b> • One-record-per-patent-family <b>file</b> design provides maximum multi-file prior-art search synergy with DWPI and CAplus INPAFAMDB for inventor and assignee searching • Standardized inventor and patent assignee searching • Seamless search options which incorporate vital corrections and reassignments from the latest INPADOC Legal Status data Easy access to accurate patent families • Quickly find a comprehensive patent family, from just a single patent number • More accurate patent families, via FIZ Karlsruhe’s quality contro...|$|R
40|$|Vertebrate fossil tracks {{have been}} studied through 3 D {{modelling}} techniques {{during the last two}} decades to improve tracking procedures. Different laser scanners and software have been used during field investigation and this differentiation caused incompatibility between many analysis programs. Post processing procedures are not standardized among scientists and <b>file</b> <b>precision</b> give different results in additionally constrain comparison. The starting point to find {{a solution to the problem}} is to point out the main technologies used to collect data in the field. The resulting protocol does not oblige to uniform hardware but it does suggest combining different laser scanners with a minimum post processing to optimize the result. Important inkling would be to use 2 D relief as a reference to organize 3 D tracking procedures. Saving formats during post processing are improved by setting fixed landmarks integrated in the 3 D model in a multitask file. The D. I. C. O. M. medical standard (*. cdm format) and the 3 D printing international standard (*. stl format) when combined, may be a good solution for uniform the format. This approach might be the first step for the standardization of source and derived files and the creation of a worldwide 3 D ichnological catalogue. This will allow creating a scientific improvement in terms of reproducibility and comparison of the experience.  </p...|$|R
40|$|Abstract—Energy-efficient {{computation}} {{is critical}} {{if we are}} going to continue to scale performance in power-limited systems. For floating-point applications that have large amounts of data parallelism, one should optimize the throughput=mm 2 given a power density constraint. We present a method for creating a trade-off curve that can be used to estimate the maximum floating-point performance given a set of area and power constraints. Looking at FP multiply-add units and ignoring register and memory overheads, we find that in a 90 nm CMOS technology at 1 W=mm 2, one can achieve a performance of 27 GFlops=mm 2 single precision, and 7 : 5 GFlops=mm 2 double <b>precision.</b> Adding register <b>file</b> overheads reduces the throughput by less than 50 percent if the compute intensity is high. Since the energy of the basic gates is no longer scaling rapidly, to maintain constant power density with scaling requires moving the overall FP architecture to a lower energy/performance point. A 1 W=mm 2 design at 90 nm is a “high-energy ” design, so scaling it to a lower energy design in 45 nm still yields a 7 performance gain, while a more balanced 0 : 1 W=mm 2 design only speeds up by 3 : 5 when scaled to 45 nm. Performance scaling below 45 nm rapidly decreases, with a projected improvement of only 3 for both power densities when scaling to a 22 nm technology. Index Terms—Arithmetic and logic structures, high-speed arithmetic, floating point, fused multiply-add, throughput/mm 2 optimization. Ç...|$|R
40|$|Introduction This {{synthetic}} ground-truth dataset accurately models long-term, continuous extracellular tetrode recordings {{from the}} rodent brain over a time-period of 256 hours. Each "recording" comprises spiking of 8 distinct single-units with firing rates ranging from 0. 1 - 6 Hz, superimposed on background multi-unit spiking activity at 20 Hz. The recording sampling rate is 30 kHz. Single-unit spike amplitudes drift over {{a range of}} 100 to 400 μ V based on the drift we observe in our own long-term recordings from the rodent motor cortex and striatum. For more details, please see our paper "Automated long-term recording and analysis of neural activity in behaving animals" ([URL] These recordings {{can be used to}} test the accuracy of spike-sorting algorithms when clustering non-stationary spike waveform data, such as our own Fast Automated Spike Tracker (FAST) outlined in our paper and available at [URL] Dataset Due to size restrictions, we provide here 1 sample tetrode of the full 6 tetrode dataset. Please contact us ([URL] if you require access to the other 5 synthetic tetrode recordings. Instructions The dataset comprises spike times and spike waveform snippets extracted a continuous synthetic tetrode recording. Provided are [...] . 	A SpikeTimes file with a list of sample numbers for detected events (spikes) at uint 64 <b>precision.</b> 	A Spikes <b>file</b> with the waveforms of the detected events in int 16 precision. Each event waveform comprises 4 channels X 64 samples 16 -bit words arranged in the order [Ch 0 -Sample 0, Ch 1 -Sample 0, Ch 2 -Sample 0, Ch 3 -Sample 0, Ch 0 -Sample 1, etc. ]. To convert to units of voltage, change type to double precision and multiply by 1. 95 e- 7. 	A SnippeterSettings. xml file with snippeting parameters (this is auto-generated by the FAST snippeting algorithm). 	A dataset_params. mat MATLAB data file containing the simulation parameters. The most important variables in the mat file are sp which contains a list of true spike-times (in samples @ 30 kHz) for all single-units in the dataset, and sp_u which specifies which unit (1 - 8) each spike originates from. Spike-times are generated by a homogenous Poisson process with firing rate specified for each unit by the variable uFRs and an absolute refractory period of 2 ms. The variable d_Amps specifies the amplitude of each single-unit spikes. The basic spike-waveform shape of each unit is provided in the variable uWVs. The spike-times and identity of background (multi-unit) spikes are specified in b_sp and b_sp_u...|$|R
40|$|Povezivanjem postupka izlučivanja gradiva u pismohrani s postojećom aplikacijom uredskog poslovanja postupak se uvelike olakšava. Gradski zavod za automatsku obradu podataka izrađuje sintetičke i analitičke popise gradiva koje se predlaže za izlučivanje. Na temelju analitičkog popisa moguće je provesti izlučivanje gradiva s točnošću do nivoa predmeta i svih njegovih akata, a ne, kao do sada, do nivoa klaseThe {{project is}} {{developed}} for users who keep their records (by legal obligation or by choice) {{according to the}} Records Management Ordinance (Narodne novine/Official Gazette of the Republic of Croatia nos 38 / 87, 42 / 88), same as to the Regulation on Unique Classification Codes and Number Codes of Records Creators and Receivers (Narodne novine/Official Gazette of the Republic of Croatia nos 49 / 87, 38 / 88). Due {{to the fact that}} jobs of records reception and records processing are consisted of routine and always the same operations, repeated all the time, and followed by use of the same registration means, they are appropriate for complete automatization. The project of records management supports activities of chancellery and registry. Connecting records disposal, implemented in registry, with existing records management application, whole procedure becomes much simpler. Workers in the chancellery enter data on specific items into data bases, according to the expactations of the Records Management Ordinance. Existing data are being extended by specific data helpful for registry workers to implement records disposal. A dossier is accompanied by orientational retention period, and each file by specific retention period expressed by years of retention or by particular date. Records planned to be disposed are followed by syntetical and analitical disposition lists. According to the analitical list it is possible to implement records disposal with <b>precision</b> at a <b>file</b> level, including all documents inside the file, and not, as up to now, at a class level. Insight into the file gives an information on its disposition status: it confirms if the file is archived, showing its retention period, or it shows that the file is disposed, indicating a date of its destruction. The application of the project in the City of Zagreb administrative units, same as in part of the Zagreb county administrative units, and in the financial and customs administrative units of the whole country, created precious arranged data holding, submissible to any automated processing system, depending on available technology. Creation of records exchange electronic system, legalization of electronic documents and complete applicability of digital signature are the basic preconditions for the achievement of electronic governement. Electronic government will once make clients possible to submit their applications in digital form, getting answers in the same way, too. That is a direction in which the project will be developed in the future...|$|R

