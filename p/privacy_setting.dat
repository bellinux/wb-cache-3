41|130|Public
50|$|The <b>privacy</b> <b>setting</b> {{allows users}} to {{disclose}} certain information to the circles of their choice. Users can also see their profile visitors.|$|E
50|$|After {{spending}} all of 2009 working on Silver Bullets, Swanberg finished seven features in 2010: Uncle Kent, Caitlin Plays Herself, The Zone, Art History, Silver Bullets, <b>Privacy</b> <b>Setting</b> and Autoerotic (co-directed with horror filmmaker Adam Wingard). Uncle Kent premiered at the Sundance Film Festival in January 2011 and Silver Bullets and Art History premiered at the Berlinale in February. The {{rest of the}} 2010 films premiered theatrically in 2011 after screenings at film festivals. Four of these were later included in Joe Swanberg: Collected Films 2011, a DVD boxed set from the music and video label Factory 25.|$|E
5000|$|Until Safari 6.0, it {{included}} a built-in web feed aggregator {{that supported the}} RSS and Atom standards. Current features include Private Browsing (a mode in which no record {{of information about the}} user's web activity is retained by the browser), an [...] "Ask websites not to track me" [...] <b>privacy</b> <b>setting,</b> the ability to archive web content in WebArchive format, the ability to email complete web pages directly from a browser menu, the ability to search bookmarks, and the ability to share tabs between all Mac and iOS devices running appropriate versions of software via an iCloud account.|$|E
40|$|AbstractUtilizing a {{typology}} {{for space}} filling into {{what we call}} “soft” and “hard” methods, we introduce the central notion of “privacy sets” {{for dealing with the}} latter. This notion provides a unifying framework for standard designs without replication, Latin hypercube designs, and Bridge designs, among many others. We introduce a heuristic algorithm based on <b>privacy</b> <b>sets</b> and compare its performance on some well-known examples. For instance, we demonstrate that for the computation of Bridge designs this algorithm performs significantly better than the state-of-the-art method. Moreover, the application of <b>privacy</b> <b>sets</b> is not restricted to cuboid design spaces and promises improvements for many other situations...|$|R
5000|$|On October 29, 2008, the Global Network Initiative (GNI) {{was founded}} upon its [...] "Principles of Freedom of Expression and Privacy". The Initiative was {{launched}} in the 60th anniversary year of the Universal Declaration of Human Rights (UDHR), {{and is based on}} internationally recognized laws and standards for human rights on freedom of expression and <b>privacy</b> <b>set</b> out in the UDHR, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR).|$|R
50|$|Since 1974, {{numerous}} {{federal laws}} have been passed in the United States to specify the privacy rights and protections of patients, physicians, and other covered entities to medical data. The most comprehensive law passed is the Health Insurance Portability and Accountability Act of 1996 (HIPAA), which was later revised after the Final Omnibus Rule in 2013. HIPAA provides a federal minimum standard for medical <b>privacy,</b> <b>sets</b> standards for uses and disclosures of protected health information (PHI), and provides civil and criminal penalties for violations.|$|R
50|$|Twitter {{allows people}} to share {{information}} with their followers. Any messages that are not switched from the default <b>privacy</b> <b>setting</b> are public, and thus can be viewed by anyone with a Twitter account. The most recent 20 tweets are posted on a public timeline.Despite Twitter's best efforts to protect their users privacy, personal information can still be dangerous to share. There have been incidents of leaked tweets on Twitter. Leaked tweets are tweets that have been published from a private account but have been made public. This occurs when friends of someone with a private account retweet, or copy and paste, that person's tweet {{and so on and}} so forth until the tweet is made public. This can make private information public, and could possibly be dangerous.|$|E
5000|$|In June 2012, Facebook {{removed all}} {{existing}} email addresses from user profiles, {{and added a}} new @facebook.com email address. Facebook claimed {{this was part of}} adding a [...] "new setting that gives people the choice to decide which addresses they want to show on their timelines". However, this setting was redundant to the existing [...] "Only Me" [...] <b>privacy</b> <b>setting</b> which was already available to hide addresses from timelines. Users complained the change was unnecessary, they did not want an @facebook.com email address, and they did not receive adequate notification their profiles had been changed. The change in email address was synchronised to phones due to a software bug, causing existing email addresses details to be deleted. The facebook.com email service was retired in February 2014.|$|E
5000|$|Facebook's {{policy on}} death {{is to turn}} the {{deceased}} user's profile into a memorial, [...] "as {{a place where people}} can save and share their memories of those who've passed." [...] Memorializing of a profile involves: the deceased user no longer showing up in the [...] "Suggestions" [...] box on the right-hand side of the homepage; the <b>privacy</b> <b>setting</b> is altered so that only confirmed friends can view the profile and search for it; contact information and status updates are removed; no one is able to log into the account in the future. Deletion of an account entails the complete removal of the deceased user's data from the online platform, however Facebook holds the legal right to sustain the user's credentials for up to 90 days after request of deletion.|$|E
5000|$|On October 29, 2008 the Global Network Initiative (GNI) {{was founded}} upon its [...] "Principles on Freedom of Expression and Privacy". The Initiative was {{launched}} in the 60th Anniversary year of the Universal Declaration of Human Rights (UDHR) {{and is based on}} internationally recognized laws and standards for human rights on freedom of expression and <b>privacy</b> <b>set</b> out in the UDHR, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR). Participants in the Initiative include the Electronic Frontier Foundation, Human Rights Watch, Google, Microsoft, Yahoo, other major companies, human rights NGOs, investors, and academics.|$|R
50|$|BCRs {{typically}} form a stringent, intra-corporate global <b>privacy</b> policies, <b>set</b> of practices, {{processes and}} guidelines that satisfies EU standards {{and may be}} available as an alternative means of authorizing transfers of personal data (e.g., customer databases, HR information, etc.) outside of Europe.|$|R
50|$|In New Zealand, the <b>Privacy</b> Act 1993 <b>sets</b> out {{principles}} {{in relation to}} the collection, use, disclosure, security and access to personal information.|$|R
5000|$|For {{sites that}} do {{encourage}} information disclosure, {{it has been}} noted that majority of the users have no trouble disclosing their personal information to {{a large group of}} people. [...] In 2005, a study was performed to analyze data of 540 Facebook profiles of students enrolled at Carnegie Mellon University. It was revealed that 89% of the users gave genuine names, and 61% gave a photograph of themselves for easier identification. Majority of users also had not altered their <b>privacy</b> <b>setting,</b> allowing a large number of unknown users to have access to their personal information (the default setting originally allowed friends, friends of friends, and non-friends of the same network to have the full view of a user's profile). It is possible for users to block other users from locating them on Facebook, but this must be done by individual basis, and would, therefore, appear not to be commonly used for a wide number of people. Most users do not realize that while they may make use of the security features on Facebook the default setting is restored after each update. All of this has led to many concerns that users are displaying far too much information on social networking sites which may have serious implications on their privacy. Facebook was criticized due to the perceived laxity regarding privacy in the default setting for users.|$|E
40|$|Facebook (FB) is {{a popular}} social {{networking}} site that becomes a basic tool for social interaction and networking among users. Users are able to access their friends profile information. Users can also setting their personal information either visible privately or open for public. However, there are users who {{are not aware of}} the <b>privacy</b> <b>setting.</b> Therefore, others can see their status updates, photos and other things they post in Facebook. A preliminary study has been conducted in order to identify students’ awareness on Facebook privacy. The findings of this study revealed that most of the students aware about the <b>privacy</b> <b>setting</b> in Facebook. Therefore, most of them have not experiencing their Facebook account being hacked by others. Future research need to be conducted with a bigger sample size in order to better understand students’ awareness on social media privacy...|$|E
40|$|The ever {{increasing}} popularity of Online Social Networks {{has left a}} wealth of personal data on the web, accessible for broad and automatic retrieval. Protection from undesired recipients and harvesting by crawlers is implemented by access control, manually configured by the user in his privacy settings. Privacy unfriendly default settings and the user unfriendly <b>privacy</b> <b>setting</b> interfaces cause an unnoticed over-sharing. We propose C 4 PS- Colors for Privacy Settings, a concept for future <b>privacy</b> <b>setting</b> interfaces. We developed a mockup for privacy settings in Facebook as a proof of concept, applying color coding for different privacy visibilities, providing {{easy access to the}} privacy settings, and generally following common, well known practices. We evaluated this mockup in a lab study and show in the results that the new approach increases the usability significantly. Based on the results we provide a Firefox plug-in implementing C 4 PS for the new Facebook interface. ...|$|E
5000|$|IM Catcher: This feature allows {{to catch}} all {{unwanted}} messages from unknown users {{that are not}} in your contact list and without <b>setting</b> <b>Privacy</b> mode.|$|R
5000|$|<b>Privacy</b> Act 1993: <b>sets</b> out various <b>privacy</b> {{principles}} {{including those}} on the collection, use and disclosure of personal information. Personal information includes information held about employees; ...|$|R
30|$|The energy {{consumption}} dataset {{is provided by}} the local energy company, SET, that manages almost the entire electrical network over the Trentino territory. SET uses around 180 primary (medium voltage) distribution lines to bring energy from the national grid (high voltage) to Trentino’s consumers. To ensure the <b>privacy</b> of <b>SET’s</b> customers, their locations and the geometry of the 180 primary distribution lines is not explicitly exposed.|$|R
40|$|An {{examination}} {{of current and}} proposed regulatory initiatives relating to data privacy shows a tendency of extraterritorial jurisdictional claims. While there is nothing novel about extraterritorial jurisdictional claims as such, the impact {{they have in the}} data <b>privacy</b> <b>setting</b> is largely unexplored. This paper discusses extraterritorial jurisdictional claims found in a selection of current and proposed regulatory initiatives relating to data privacy. Special attention is given to how such claims affect, and are affected by, modern use of information and communication technologies...|$|E
40|$|AbstractSocial Networking Sites (SNSs) {{have become}} a global experience, with {{communities}} such as Facebook, MySpace and Friendster which they are reporting user figures {{in the hundreds of}} millions. People, have been invited into or chosen to join these communities are able to publish multimedia content about themselves, their interests and concerns. With intensive information been shared, privacy concern are rising up because Social networks privacy is different from the classic <b>privacy</b> <b>setting</b> that cause a lot of problems such as exposing sensitive information. On the other hand privacy model should be designed to facilitate user's requirements, which mean that users are, supposed to set up their own <b>privacy</b> <b>setting.</b> Since it's still not well defined and need to be improved and studied further. However, to solve this is by providing a more private platform with the ability for users to set their own privacy settings. The privacy model can be so general to more complicated, but all of them are still not clear for users to be aware off. The new model we are proposing will solve the privacy issues since it's done by users and monitored by developers. This research will point out some problems, and how to solve these problems by proposing a new user oriented privacy model for social networks...|$|E
30|$|Users add new {{locations}} to OpenLitterMap simply {{by being the}} first to upload a geotagged photo of litter from that location. OpenLitterMap reverse geocodes each set of GPS coordinates to get the latest OpenStreetMap address at each location and uses this information to populate its list of locations dynamically. To incentivize early adoption, this process is redeemable in Littercoin. Each layer has a “Created By” stamp, {{which is used to}} display either an anonymous user (the default <b>privacy</b> <b>setting</b> of every user) or it will reveal a given name and/or username for history to remember.|$|E
50|$|The Privacy Act was amended in 2000 {{to cover}} the private sector. Schedule 3 of the <b>Privacy</b> Act <b>sets</b> out a {{significantly}} different <b>set</b> of <b>privacy</b> principles (the National Privacy Principles) which apply to private sector organisations (including not for profit organisations) with a turnover exceeding three million dollars, other than health service providers or traders in personal information. These principles extend to the transfer of personal information out of Australia.|$|R
40|$|When {{datasets}} {{are distributed}} on different sources, finding out matched data while preserving {{the privacy of}} the datasets is a widely required task. In this paper, we address two matching problems against the private datasets on N (N≥ 2) parties. The first one is the <b>Privacy</b> Preserving <b>Set</b> Intersection (PPSI) problem, in which each party wants to learn the intersection of the N private datasets. The second one is the <b>Privacy</b> Preserving <b>Set</b> Matching (PPSM) problem, in which each party wants to learn whether its elements can be matched in any private set of the other parties. For the two problems we propose efficient protocols based on a threshold cryptosystem which is additive homomorphic. In a comparison with the related work in [18], the computation and communication costs of our PPSI protocol decrease by 81 % and 17 % respectively, and the computation and communication costs of our PPSM protocol decrease by 80 % and 50 % respectively. In practical utilities both of our protocols save computation time and communication bandwidth. Yingpeng Sang, Hong Shen, Yasuo Tan and Naixue Xion...|$|R
40|$|As multi-agent systems {{become more}} {{numerous}} and more data-driven, novel forms of privacy {{are needed in}} order to protect data types that are not accounted for by existing privacy frameworks. In this paper, we present a new form of privacy for set-valued data which extends the notion of differential <b>privacy</b> to <b>sets</b> which users want to protect. While differential privacy is typically defined in terms of probability distributions, we show that it is more natural here to define <b>privacy</b> for <b>sets</b> over their capacity functionals, which capture the probability of a random set intersecting some other set. In terms of sets' capacity functionals, we provide a novel definition of differential privacy for set-valued data. Based on this definition, we introduce the Laplacian Perturbation Mechanism (so named because it applies random perturbations to sets), and show that it provides ?-differential privacy as prescribed by our definition. These theoretical results are supported by numerical results, demonstrating the practical applicability of the developments made. Comment: 14 pages, 3 figures; Submitted to ACC 201...|$|R
40|$|Existing online social {{networks}} (OSNs) only allow a single user to restrict access to her/his data but cannot provide any mechanism to enforce privacy concerns over data associated with multiple users. This situation leaves privacy conflicts largely unresolved {{and leads to}} the potential disclosure of users ’ sensitive information. To ad-dress such an issue, a MultiParty Access Control (MPAC) model was recently proposed, including a systematic approach to iden-tify and resolve privacy conflicts for collaborative data sharing in OSNs. In this paper, we take another step to further study the prob-lem of analyzing the strategic behavior of rational controllers in multiparty access control, where each controller aims to maximize her/his own benefit by adjusting her/his <b>privacy</b> <b>setting</b> in collab-orative data sharing in OSNs. We first formulate this problem as a multiparty control game and show the existence of unique Nash Equilibrium (NE) which is critical because at an NE, no controller has any incentive to change her/his <b>privacy</b> <b>setting.</b> We then present algorithms to compute the NE and prove that the system can con-verge to the NE {{in only a few}} iterations. A numerical analysis is also provided for different scenarios that illustrate the interplay of controllers in the multiparty control game. In addition, we con-duct user studies of the multiparty control game to explore the gap between game theoretic approaches and real human behaviors...|$|E
30|$|Twitter {{was created}} in 2006 {{and it has been}} rapidly growing, {{attracting}} 255 M monthly active users [38]. In Twitter, the users share content composed from 140 -character text messages called tweets. Users can choose whom to follow - a social relationship in Twitter is not necessarily mutual. Hence, topologically, a Twitter network is a directed graph: an individual has a number of ‘followees’ whom he follows and ‘followers’ who follow him. A user will receive all tweets posted by his followees. Unless a user sets his <b>privacy</b> <b>setting</b> as ‘private’ explicitly, all tweets he posts are visible to the public by default.|$|E
40|$|Understanding {{the privacy}} {{implication}} of adopting a certain <b>privacy</b> <b>setting</b> {{is a complex}} task for the users of social network systems. Users need tool support to articulate potential access scenarios and perform policy analysis. Such a need is particularly acute for Facebook-style Social Network Systems (FSNSs), in which semantically rich topology-based policies are used for access control. In this work, we develop a prototypical tool for Reflective Policy Assessment (RPA) — {{a process in which}} a user examines her profile from the viewpoint of another user in her extended neighbourhood in the social graph. We verify the utility and usability of our tool in a within-subject user study...|$|E
40|$|Abstract. When {{datasets}} {{are distributed}} on different sources, finding out matched data while preserving {{the privacy of}} the datasets is a widely required task. In this paper, we address two matching problems against the private datasets on N (N ≥ 2) parties. The first one is the <b>Privacy</b> Preserving <b>Set</b> Intersection (PPSI) problem, in which each party wants to learn the intersection of the N private datasets. The second one is the <b>Privacy</b> Preserving <b>Set</b> Matching (PPSM) problem, in which each party wants to learn whether its elements can be matched in any private set of the other parties. For the two problems we propose efficient protocols based on a threshold cryptosystem which is additive homomorphic. In a comparison with the related work in [18], the computation and communication costs of our PPSI protocol decrease by 81 % and 17 % respectively, and the computation and communication costs of our PPSM protocol decrease by 80 % and 50 % respectively. In practical utilities both of our protocols save computation time and communication bandwidth...|$|R
5000|$|Administration {{applications}} include {{tools to}} customize the look & feel, name and URL of the Online Campus, to <b>set</b> <b>privacy</b> levels {{of activities and}} content, to set admission policies, to invite and administer students and teachers.|$|R
40|$|Recently, a novel {{class of}} {{incentive}} mechanisms is proposed to attract extensive users to truthfully participate in crowd sensing applications with a given budget constraint. The class mechanisms also bring good service quality for the requesters in crowd sensing applications. Although {{it is so}} important, there still exists many verification and privacy challenges, including users' bids and subtask information privacy and identification <b>privacy,</b> winners' <b>set</b> <b>privacy</b> of the platform, and {{the security of the}} payment outcomes. In this paper, we present a privacy-preserving verifiable incentive mechanism for crowd sensing applications with the budget constraint, not only to explore how to protect the privacies of users and the platform, but also to make the verifiable payment correct between the platform and users for crowd sensing applications. Results indicate that our privacy-preserving verifiable incentive mechanism achieves the same results as the generic one without privacy preservation...|$|R
40|$|Privacy {{has been}} a big concern for users of social network {{services}} (SNS). On recent criticism about privacy protection, most SNS now provide fine privacy controls, allowing users to set visibility levels for almost every profile item. However, this also creates a number of difficulties for users. First, SNS providers often set most items by default to the highest visibility to improve the utility of social network, which may conflict with users 2 ̆ 7 intention. It is often formidable for a user to fine-tune tens of privacy settings towards the user desired settings. Second, tuning privacy settings involves an intricate tradeoff between privacy and utility. When you turn off the visibility of one item to protect your privacy, the social utility of that item is turned off as well. It is challenging for users to make a tradeoff between privacy and utility for each <b>privacy</b> <b>setting.</b> We propose a framework for users to conveniently tune the privacy settings towards the user desired privacy level and social utilities. It mines the privacy settings {{of a large number of}} users in a SNS, e. g., Facebook, to generate latent trait models for the level of privacy concern and the level of utility preference. A tradeoff algorithm is developed for helping users find the optimal privacy settings for a specified level of privacy concern and a personalized utility preference. We crawl a large number of Facebook accounts and derive the privacy settings with a novel method. These <b>privacy</b> <b>setting</b> data are used to validate and showcase the proposed approach...|$|E
30|$|Context plays a {{critical}} role in social engineering attacks because this determines the complexity of the attack, especially for the operator. It has been argued that in social networking sites, there are three main sources in the user’s profile that cybercriminals use to reach their victims, content, friendship connections, and privacy settings [20]. A network’s privacy and security settings are important measures to protect the user. Even with the limited functionality of current social network security and privacy preferences [21], if users adjust the network’s <b>privacy</b> <b>setting</b> and prevent non-friends from accessing their account, the attacker {{would not be able to}} use the account to gather the information required to conduct indirect attacks.|$|E
40|$|Social media gives users an {{efficient}} way to communicate and network with each other on an unprecedented scale and at rates unseen in traditional media. As his social network expands, a user’s privacy protection goes beyond his <b>privacy</b> <b>setting</b> and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest <b>privacy</b> <b>setting</b> guarantee a secure protection? Given the open nature of a social networking site, is it possible to manage one’s privacy protection? With the diversity of one’s social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user’s perspective as dependent on whether or not the user’s friends’ security and privacy settings protect the friend and the individual’s network of friends (which includes the user). A single vulnerable friend in a user’s social network can place all friends at risk. Using experiments, we demonstrate how much security an individual user can improve by unfriending a vulnerable friend. We also show how security and privacy weakens if newly accepted friends are unguarded or unprotected. This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset. A new perspective for reasoning about social networking security is presented and discussed. When a user accepts a new friend, the user should ensure the new friend is not an increased security risk with the potential of negatively impacting the entire friend network. Additionally, by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends, it is possible to further improve security and privacy without changing the social networking site’s existing architecture...|$|E
5000|$|Virtual Gift: Camfrog Gift is {{a virtual}} gift that can send to any Camfrog users. It's also allows to send a private message to any user on Camfrog even their status is <b>set</b> <b>privacy</b> modem and still can see your message.|$|R
30|$|The {{previous}} Section  3 and Section  4 how we {{can apply}} the privacy-by-design methodology for guaranteeing individual <b>privacy</b> in a <b>setting</b> {{where we have a}} central trusted aggregation center that collects data and before releasing it can apply a privacy transformation strategy to enable collective analyses in a privacy-aware fashion.|$|R
40|$|One of {{the main}} {{challenges}} for privacy-aware locationbased systems is {{to strike a balance}} between <b>privacy</b> preferences <b>set</b> by users and location accuracy needed by Location-Based Services (LBSs). To this end, two key requirements must be satisfied: the availability of techniques providing for different degrees of user location privacy and the possibility of quantifying such privacy degrees. To address the first requirement, we describe two obfuscation techniques. For the second requirement, we introduce the notion of relevance as the estimator for the degree of location obfuscation. This way, location obfuscation can be adjusted to comply with both user preferences and LBS accuracy requirements...|$|R
