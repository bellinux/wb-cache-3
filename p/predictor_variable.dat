1113|5771|Public
2500|$|Consider fitting a {{line with}} one <b>predictor</b> <b>variable.</b> Define i as {{an index of}} each of the n {{distinct}} x values, j as an index of the response variable observations for a given x value, and n'i as the number of y values associated with the i th x value. [...] The value of each response variable observation can be represented by ...|$|E
2500|$|Linearity. [...] This {{means that}} {{the mean of the}} {{response}} variable is a linear combination of the parameters (regression coefficients) and the predictor variables. [...] Note that this assumption is much less restrictive than it may at first seem. [...] Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. [...] The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying <b>predictor</b> <b>variable</b> can be added, each one transformed differently. [...] This trick is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a <b>predictor</b> <b>variable.</b> This makes linear regression an extremely powerful inference method. [...] In fact, models such as polynomial regression are often [...] "too powerful", in that they tend to overfit the data. [...] As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. [...] Common examples are ridge regression and lasso regression. [...] Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as [...] special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.) ...|$|E
2500|$|The {{notion of}} a [...] "unique effect" [...] is {{appealing}} when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that {{is linked to the}} value of a <b>predictor</b> <b>variable.</b> However, {{it has been argued that}} in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design. A commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.|$|E
25|$|Principal {{component}} regression (PCR) is {{used when}} the number of <b>predictor</b> <b>variables</b> is large, or when strong correlations exist among the <b>predictor</b> <b>variables.</b> This two-stage procedure first reduces the <b>predictor</b> <b>variables</b> using principal component analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the <b>predictor</b> <b>variables</b> should lie among the dominant principal components of the multivariate distribution of the <b>predictor</b> <b>variables.</b> The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|R
40|$|We present <b>predictor</b> <b>variables</b> and R and Stan {{code for}} {{simulating}} and analyzing counts of Missouri Ozark herpetofauna {{in response to}} three forest management strategies. Our code performs four primary purposes: import <b>predictor</b> <b>variables</b> from spreadsheets; simulate synthetic response variables based on imported <b>predictor</b> <b>variables</b> and user-supplied values for data-generating parameters; format synthetic data for export to Stan; and analyze synthetic data...|$|R
30|$|ANOVA of {{multiple}} regression with temperature, turbidity and DO as selected <b>predictor</b> <b>variables</b> yield regression equations {{for the two}} species of Planktoniella as response variables. The equation in case of P. blanda predicts that with each unit decrease in temperature, P. blanda count can increase by 0.425 considering the interaction of other <b>predictor</b> <b>variables.</b> Similarly, in case of P. sol, a single unit decrease in temperature can increase P. sol count by 0.302 in consistence with other <b>predictor</b> <b>variables.</b>|$|R
2500|$|The very {{simplest}} case of {{a single}} scalar <b>predictor</b> <b>variable</b> x and a single scalar response variable y is known as simple linear regression. [...] The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression. [...] Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased {{in terms of the}} multiple regression model. [...] Note, however, that in these cases the response variable y is still a scalar. Another term multivariate linear regression refers to cases where y is a vector, i.e., the same as general linear regression.|$|E
2500|$|A fitted linear {{regression}} {{model can be}} used to identify the relationship between a single <b>predictor</b> <b>variable</b> x'j and the response variable y when all the other predictor variables in the model are [...] "held fixed". Specifically, the interpretation of β'j is the expected change in y for a one-unit change in x'j when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to x'j. This is sometimes called the unique effect of x'j on y. In contrast, the marginal effect of x'j on y can be assessed using a correlation coefficient or simple {{linear regression}} model relating only x'j to y; this effect is the total derivative of y with respect to x'j.|$|E
2500|$|The {{meaning of}} the {{expression}} [...] "held fixed" [...] may depend on how {{the values of the}} predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been [...] "held fixed" [...] by the experimenter. Alternatively, the expression [...] "held fixed" [...] can refer to a selection that takes place in the context of data analysis. In this case, we [...] "hold a variable fixed" [...] by restricting our attention to the subsets of the data that happen to have a common value for the given <b>predictor</b> <b>variable.</b> This is the only interpretation of [...] "held fixed" [...] {{that can be used in}} an observational study.|$|E
40|$|This paper {{considers}} a panel data model for predicting a binary outcome. The conditional {{probability of a}} positive response is obtained by evaluating a given distribution function (F) at a linear combination of the <b>predictor</b> <b>variables.</b> One of the <b>predictor</b> <b>variables</b> is unobserved. It is a random effect that varies across individuals but is constant over time. The semiparametric aspect is that the conditional distribution of the random effect, given the <b>predictor</b> <b>variables,</b> is unrestricted. Copyright 2010 The Econometric Society. ...|$|R
40|$|Description Analyzing {{regression}} {{data with}} many and/or highly collinear <b>predictor</b> <b>variables,</b> by simultaneously reducing the <b>predictor</b> <b>variables</b> {{to a limited}} number of components and regressing the criterion variables on these components. Several rotation options are provided in this package, as well as model selection options...|$|R
40|$|Even though stock {{returns are}} not highly autocorrelated, {{there is a}} {{spurious}} regression bias in predictive regressions for stock returns related to the classic studies of Yule (1926) and Granger and Newbold (1974). Data mining for <b>predictor</b> <b>variables</b> interacts with spurious regression bias. The two effects reinforce each other, because more highly persistent series {{are more likely to}} be found significant in the search for <b>predictor</b> <b>variables.</b> Our simulations suggest that many of the regressions in the literature, based on individual <b>predictor</b> <b>variables,</b> may be spurious...|$|R
2500|$|Lack {{of perfect}} {{multicollinearity}} in the predictors. [...] For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as perfect multicollinearity in the predictor variables. [...] This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same <b>predictor</b> <b>variable</b> is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. [...] At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. [...] Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.|$|E
50|$|ExampleIf the {{variance}} inflation factor of a <b>predictor</b> <b>variable</b> were 5.27 (√5.27 = 2.3) {{this means that the}} standard error for the coefficient of that <b>predictor</b> <b>variable</b> is 2.3 times as large as it would be if that <b>predictor</b> <b>variable</b> were uncorrelated with the other predictor variables.|$|E
5000|$|Large {{changes in}} the {{estimated}} regression coefficients when a <b>predictor</b> <b>variable</b> is added or deleted ...|$|E
5000|$|If all the <b>predictor</b> <b>variables</b> are uncorrelated, {{the matrix}} [...] is the {{identity}} matrix and [...] simply equals , {{the sum of}} the squared correlations with the dependent <b>variable.</b> If the <b>predictor</b> <b>variables</b> are correlated among themselves, the inverse of the correlation matrix [...] accounts for this.|$|R
5000|$|The {{square of}} the {{coefficient}} of multiple correlation can be computed using the vector [...] of correlations [...] between the <b>predictor</b> <b>variables</b> [...] (independent variables) and the target variable [...] (dependent variable), and the correlation matrix [...] of correlations between <b>predictor</b> <b>variables.</b> It is given by ...|$|R
40|$|Concatenative speech {{synthesis}} systems attempt to minimize audible discontinuities between two successive concatenated units. In unit selection concatenative synthesis, a join cost is calculated that {{is intended to}} predict the extent of audible discontinuity introduced by the concatenation of two specific units. A study was conducted that used human perceptual data on the detectability of mid-vowel concatenation discontinuities to train and to test several models for predicting perceptually-based join costs. Both linear regression (LR) and {{classification and regression tree}} (CART) models were used. Each was trained on several different sets of <b>predictor</b> <b>variables.</b> All LR and some CART models used strictly acoustic <b>predictor</b> <b>variables,</b> some CART models used acoustic plus phonetic categorical variables, and one CART model used strictly phonetic predictors. Results from tests of LR and CART models showed that, when trained with the same acoustic <b>predictor</b> <b>variables,</b> the two models achieved very similar results in predicting human detection rates. Euclidean cepstral distances were superior to VQ cepstral distances as <b>predictor</b> <b>variables.</b> Categorical phonetic <b>predictor</b> <b>variables</b> in CART models greatly improved the accuracy of prediction of concatenation discontinuities. 1...|$|R
5000|$|... #Caption: Cox {{proportional}} hazards regression output for melanoma data. <b>Predictor</b> <b>variable</b> is sex 1: female, 2: male.|$|E
5000|$|In {{the case}} where the {{independent}} (<b>predictor)</b> <b>variable</b> [...] is [...] and the dependent (outcome) variable is binary, Somers’ D equals ...|$|E
5000|$|Suppose {{that the}} <b>predictor</b> <b>{{variable}}</b> [...] takes three values, , , or , and outcome variable [...] takes two values, [...] or [...] The table below contains observed combinations of [...] and : ...|$|E
5000|$|LMM 2.6 - Frequency Histograms for <b>Predictor</b> <b>Variables</b> (August 2009) ...|$|R
50|$|Multicollinearity: Predictive {{power can}} {{decrease}} {{with an increased}} correlation between <b>predictor</b> <b>variables.</b>|$|R
2500|$|Weak exogeneity. [...] This {{essentially}} {{means that}} the <b>predictor</b> <b>variables</b> x can be treated as fixed values, rather than random variables. [...] This means, for example, that the <b>predictor</b> <b>variables</b> {{are assumed to be}} error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.|$|R
50|$|Each neuron in {{the input}} layer {{represents}} a <b>predictor</b> <b>variable.</b> In categorical variables, N-1 neurons are used {{when there are}} N number of categories. It standardizes {{the range of the}} values by subtracting the median and dividing by the interquartile range. Then the input neurons feed the values to each of the neurons in the hidden layer.|$|E
50|$|It {{may seem}} counter-intuitive that {{noise in the}} <b>predictor</b> <b>{{variable}}</b> x induces a bias, but noise in the outcome variable y does not. Recall that linear regression is not symmetric: the {{line of best fit}} for predicting y from x (the usual linear regression) {{is not the same as}} the line of best fit for predicting x from y.|$|E
5000|$|Consider fitting a {{line with}} one <b>predictor</b> <b>variable.</b> Define i as {{an index of}} each of the n {{distinct}} x values, j as an index of the response variable observations for a given x value, and ni as the number of y values associated with the i th x value. The value of each response variable observation can be represented by ...|$|E
40|$|Weighted {{least squares}} estimators, {{such as those}} arising from certain {{variance}} stabilizing transformations and robust regression procedures, alter the multicollinear structure of the original matrix of <b>predictor</b> <b>variables.</b> We investigate the effects of weighted least squares on the eigenvalues and the spectral condition number of the original correlation matrix of <b>predictor</b> <b>variables.</b> Biased estimation robust regression spectral condition number...|$|R
50|$|Weak exogeneity. This {{essentially}} {{means that}} the <b>predictor</b> <b>variables</b> x can be treated as fixed values, rather than random variables. This means, for example, that the <b>predictor</b> <b>variables</b> {{are assumed to be}} error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.|$|R
5000|$|The model relates a {{univariate}} response variable, Y, to some <b>predictor</b> <b>variables,</b> xi. An exponential family {{distribution is}} specified for Y (for example normal, binomial or Poisson distributions) {{along with a}} link function g (for example the identity or log functions) relating the expected value of Y to the <b>predictor</b> <b>variables</b> via a structure such as ...|$|R
5000|$|Input layer: One neuron {{appears in}} the input layer for each <b>predictor</b> <b>variable.</b> In the case of {{categorical}} variables, N-1 neurons are used where N {{is the number of}} categories. The input neurons standardizes the value ranges by subtracting the median and dividing by the interquartile range. The input neurons then feed the values to each of the neurons in the hidden layer.|$|E
5000|$|Logistic {{regression}} {{is one way}} {{to generalize}} the odds ratio beyond two binary variables. Suppose we have a binary response variable Y and a binary <b>predictor</b> <b>variable</b> X, and in addition we have other predictor variables Z1, ..., Zp {{that may or may not}} be binary. If we use multiple logistic regression to regress Y on X, Z1, ..., Zp, then the estimated coefficient [...] for X is related to a conditional odds ratio. Specifically, at the population level ...|$|E
50|$|Consider fitting a {{straight}} line for the relationship of an outcome variable y to a <b>predictor</b> <b>variable</b> x, and estimating {{the slope of the}} line. Statistical variability, measurement error or random noise in the y variable cause uncertainty in the estimated slope, but not bias: on average, the procedure calculates the right slope. However, variability, measurement error or random noise in the x variable causes bias in the estimated slope (as well as imprecision). The greater the variance in the x measurement, the closer the estimated slope must approach zero instead of the true value.|$|E
3000|$|... [...]. The binary CoD {{measures}} the relative decrease in prediction error when using <b>predictor</b> <b>variables</b> {{to estimate the}} target variable, as opposed to using no <b>predictor</b> <b>variables.</b> The closer it is to one, the tighter the regulation of the target <b>variable</b> by the <b>predictor</b> <b>variables</b> is, whereas the closer it is to zero, the looser the regulation is. The CoD will correctly produce low values {{in cases where the}} no-predictor error is already small, or when adding predictors does not contribute to a significant decrease in error. The CoD is a function only of the joint distribution between predictors and target, thus it characterizes the regulatory relationship among them.|$|R
30|$|In the {{logistic}} regression model, the classification table shows the accuracy in {{the prediction of}} a sample in a group. The classification percentage {{was found to be}} 52  % without the application of the <b>predictor</b> <b>variables</b> where membership of a sample to group is based on the majority (null model). After the application of the <b>predictor</b> <b>variables,</b> the predictive model gave us the classification rate as 90  %, which implies that the classification of a read request into correct read (1) or incorrect read category (0) was accurate to the extent of 90  % after the application of the two <b>predictor</b> <b>variables.</b> This means that the classification model is statistically significant.|$|R
40|$|For each {{of three}} samples of 207 Caucasians, 61 Mexican-Ameri-cans, and 53 Negroes in a nursing {{training}} program at a very large metropolitan hospital, validity coefficients of 15 <b>predictor</b> <b>variables</b> available prior to training and of 13 <b>predictor</b> <b>variables</b> obtained concurrently with training were calculated relative to each of five subtests of a state board certification examination for nurses. In addition, stepwise multiple regression analyses relative {{to each of the}} same five criterion measures as well as with respect to an average score on the five subtests of the certification examination were car-ried out for selected sets of <b>predictor</b> <b>variables</b> {{in each of the three}} samples. Standardized test measures involving reading skills were the most valid of the <b>predictor</b> <b>variables</b> obtained prior to nursing train-ing, whereas total grade point average in program courses as well as scores on the National League for Nursing (NLN) Achievement Tests provided highest validity coefficients for each ethnic group. I...|$|R
