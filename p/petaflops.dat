294|124|Public
5|$|On September 16, 2007, due {{in large}} part to the {{participation}} of PlayStation 3 consoles, the Folding@home project officially attained a sustained performance level higher than one native <b>petaFLOPS,</b> becoming the first computing system of any kind to do so. Top500's fastest supercomputer at the time was BlueGene/L, at 0.280 <b>petaFLOPS.</b> The following year, on May 7, 2008, the project attained a sustained performance level higher than two native <b>petaFLOPS,</b> followed by the three and four native <b>petaFLOPS</b> milestones on August 20 and September 28, 2008 respectively. On February 18, 2009, Folding@home achieved five native <b>petaFLOPS,</b> and was the first computing project to meet these five levels. In comparison, November 2008's fastest supercomputer was IBM's Roadrunner at 1.105 <b>petaFLOPS.</b> On November 10, 2011, Folding@home's performance exceeded six native <b>petaFLOPS</b> with the equivalent of nearly eight x86 <b>petaFLOPS.</b> In mid-May 2013, Folding@home attained over seven native <b>petaFLOPS,</b> with the equivalent of 14.87 x86 <b>petaFLOPS.</b> It then reached eight native <b>petaFLOPS</b> on June 21, followed by nine on September 9 of that year, with 17.9 x86 <b>petaFLOPS.</b> On May 11, 2016 Folding@home announced that it was moving towards reaching the 100 x86 <b>petaFLOPS</b> mark.|$|E
5|$|In 2007, Guinness World Records {{recognized}} Folding@home as {{the most}} powerful distributed computing network. As of September 30, 2014, the project has 107,708 active CPU cores and 63,977 active GPUs for a total of 40.190 x86 <b>petaFLOPS</b> (19.282 native <b>petaFLOPS).</b> At the same time, the combined efforts of all distributed computing projects under BOINC totals 7.924 <b>petaFLOPS.</b> In November 2012, Folding@home updated its accounting of FLOPS, especially for GPUs, and now reports the number of active processor cores and physical processors. Using the Markov state model method, Folding@home achieves strong scaling across its user base and gains a linear speedup for every added processor. This network allows Folding@home to do work that was formerly impractical computationally.|$|E
5|$|Jaguar had {{received}} various upgrades since its creation. It {{began with the}} Cray XT3 platform that yielded 25 teraFLOPS. By 2008, Jaguar had been expanded with more cabinets and upgraded to the XT4 platform, reaching 263 teraFLOPS. In 2009, it was upgraded to the XT5 platform, hitting 1.4 <b>petaFLOPS.</b> Its final upgrades brought Jaguar to 1.76 <b>petaFLOPS.</b>|$|E
5000|$|Design and {{analysis}} of a 3-dimensional cluster multicomputer architecture using optical interconnection for <b>petaFLOP</b> computing ...|$|R
50|$|The HPC Challenge (High-performance {{computers}} challenge) {{is part of}} the project. An HPCS goal is {{to create}} a multi <b>petaflop</b> systems.|$|R
50|$|Progress in {{the first}} decade of the 21st century was {{dramatic}} and supercomputers with over 60,000 processors appeared, reaching <b>petaflop</b> performance levels.|$|R
5|$|Plans {{to create}} a {{supercomputer}} capable of 20 <b>petaFLOPS</b> at the Oak Ridge Leadership Computing Facility (OLCF) at Oak Ridge National Laboratory (ORNL) originated {{as far back as}} 2005, when Jaguar was built. Titan will itself be replaced by an approximately 200 <b>petaFLOPS</b> system in 2016 as part of ORNL's plan to operate an exascale (1000 <b>petaFLOPS</b> to 1 exaFLOPS) machine by 2020. The initial plan to build a new 15,000 square meter (160,000 ft2) building for Titan, was discarded in favor of using Jaguar's existing infrastructure. The precise system architecture was not finalized until 2010, although a deal with Nvidia to supply the GPUs was signed in 2009. Titan was first announced at the private ACM/IEEE Supercomputing Conference (SC10) on November 16, 2010, and was publicly announced on October 11, 2011, as {{the first phase of the}} Titan upgrade began.|$|E
5|$|WL-LSMS simulates the {{interactions}} between electrons and atoms in magnetic materials at temperatures other than absolute zero. An earlier version of the code {{was the first to}} perform at greater than one <b>petaFLOPS</b> on Jaguar.|$|E
5|$|Bonsai is a {{gravitational}} tree {{code for}} simulation galaxies. It {{has been used}} for the 2014 Gordon Bell prize nomination for simulating the Milky Way Galaxy on a star by star basis, with 200 billion stars. In this application the computer reached a sustained speed of 24.773 <b>PetaFlops.</b>|$|E
5000|$|Tianhe-2 a 33.9 <b>petaflop</b> {{system with}} 32,000 Intel Ivy Bridge chips and 48,000 Intel Xeon Phi chips {{with a total}} of 3.1 million cores ...|$|R
50|$|According to the MIT Technology Review, the Loongson {{processor}} would {{power the}} Dawning supercomputers by 2012, producing {{a line of}} totally Chinese made supercomputers that reach <b>petaflop</b> speeds.|$|R
50|$|On 29 July 2015, President Obama {{signed an}} {{executive}} order creating a National Strategic Computing Initiative calling for the accelerated development of an exascale (1000 <b>petaflop)</b> system and funding research into post-semiconductor computing.|$|R
5|$|Folding@home {{is one of}} the world's fastest {{computing}} systems, with a {{speed of}} approximately 70 <b>petaFLOPS.</b> This performance from its large-scale computing network has allowed researchers to run computationally costly atomic-level simulations of protein folding thousands of times longer than formerly achieved. Since its launch on October1, 2000, the Pande Lab has produced 139 scientific research papers {{as a direct result of}} Folding@home. Results from the project's simulations agree well with experiments.|$|E
5|$|Titan employs AMD Opteron CPUs in {{conjunction}} with Nvidia Tesla GPUs to improve energy efficiency while providing {{an order of magnitude}} increase in computational power over Jaguar. It uses 18,688 CPUs paired with an equal number of GPUs to perform at a theoretical peak of 27 petaFLOPS; in the LINPACK benchmark used to rank supercomputers' speed, it performed at 17.59 <b>petaFLOPS.</b> This was enough to take first place in the November 2012 list by the TOP500 organization, but Tianhe-2 overtook it on the June 2013 list.|$|E
5|$|Titan is a {{supercomputer}} {{built by}} Cray at Oak Ridge National Laboratory {{for use in}} a variety of science projects. Titan is an upgrade of Jaguar, a previous supercomputer at Oak Ridge, that uses graphics processing units (GPUs) in addition to conventional central processing units (CPUs). Titan is the first such hybrid to perform over 10 <b>petaFLOPS.</b> The upgrade began in October 2011, commenced stability testing in October 2012 and it became available to researchers in early 2013. The initial cost of the upgrade was US$60 million, funded primarily by the United States Department of Energy.|$|E
40|$|We {{present a}} {{speculative}} extrapolation {{of the performance}} aspects of an atmospheric general circulation model to ultra-high resolution and describe alternative technological paths to realize integration of such a model in the relatively near future. Due to a superlinear scaling of the computational burden dictated by stability criterion, {{the solution of the}} equations of motion dominate the calculation at ultra-high resolutions. From this extrapolation, it is estimated that a credible kilometer scale atmospheric model would require at least a sustained ten <b>petaflop</b> computer to provide scientifically useful climate simulations. Our design study portends an alternate strategy for practical power-efficient implementations of <b>petaflop</b> scale systems. Embedded processor technology could be exploited to tailor a custom machine designed to ultra-high climate model specifications at relatively affordable cost and power considerations. The major conceptual changes required by a kilometer scale climate model are certain to be difficult to implement. Although the hardware, software, and algorithms are all equally critical in conducting ultra-high climate resolution studies, {{it is likely that the}} necessary <b>petaflop</b> computing technology will be available in advance of a credible kilometer scale climate mode...|$|R
40|$|Abstract—While the HPC {{community}} is working towards {{the development of}} the first Exaflop computer (expected around 2020), after reaching the <b>Petaflop</b> milestone in 2008 still only few HPC applications are able to fully exploit the capabilities of <b>Petaflop</b> systems. In this paper we argue that efforts for preparing HPC applications for Exascale should start before such systems become available. We identify challenges {{that need to be addressed}} and recommend solutions in key areas of interest, including formal modeling, static analysis and op-timization, runtime analysis and optimization, and autonomic computing. Furthermore, we outline a conceptual framework for porting HPC applications to future Exascale computing systems and propose steps for its implementation. I...|$|R
40|$|This paper {{addresses}} the mapping of protein folding to a million-processor array for <b>petaflop</b> performance. A new scalability model is defined which incorporates {{the characteristics of}} highly-distributed multithreaded architectures. The model is {{used to compare the}} scalability of various algorithms and decompositions and facilitates the prototyping of a million node Processing-In-Memory array to achieve <b>petaflop</b> performance during a molecular dynamics simulation typical of protein folding. In addition, simulations are used to validate the model and parallel implementations are demonstrated. Although parallel implementations of MD simulations exist, none have been attempted at the scale that we are proposing and more accurate algorithmic complexity models are needed to assist in such attempt...|$|R
5|$|The yearlong {{conversion}} began October 9, 2011. Between October and December, 96 of Jaguar's 200 cabinets, each containing 24 XT5 blades (two 6-core CPUs per node, four nodes per blade), were {{upgraded to}} XK7 blades (one 16-core CPU per node, four nodes per blade) while {{the remainder of}} the machine remained in use. In December, computation was moved to the 96 XK7 cabinets while the remaining 104 cabinets were upgraded to XK7 blades. ORNL's external ESnet connection was upgraded from 10 Gbit/s to 100 Gbit/s and the system interconnect (the network over which CPUs communicate with each other) was updated. The Seastar design used in Jaguar was upgraded to the Gemini interconnect used in Titan which connects the nodes into a direct 3D torus interconnect network. Gemini uses wormhole flow control internally. The system memory was doubled to 584 TiB. 960 of the XK7 nodes (10 cabinets) were fitted with a Fermi based GPU as Kepler GPUs were not then available; these 960 nodes were referred to as TitanDev and used to test code. This first phase of the upgrade increased the peak performance of Jaguar to 3.3 <b>petaFLOPS.</b> Beginning on September 13, 2012, Nvidia K20X GPUs were fitted to all of Jaguar's XK7 compute blades, including the 960 TitanDev nodes. In October, the task was completed and the computer was finally christened Titan.|$|E
25|$|Ames {{operates}} {{one of the}} world′s fastest supercomputers, Pleiades, {{which will}} be further enhanced and is scheduled to reach 10 <b>petaflops</b> of processing power by 2012.|$|E
25|$|IBM's {{supercomputer}}, IBM Roadrunner, is {{a hybrid}} of General Purpose CISC Opteron as well as Cell processors. This system assumed the #1 spot on the June 2008 Top 500 list as the first supercomputer to run at <b>petaFLOPS</b> speeds, having gained a sustained 1.026 <b>petaFLOPS</b> speed using the standard Linpack benchmark. IBM Roadrunner uses the PowerXCell 8i version of the Cell processor, manufactured using 65nm technology and enhanced SPUs that can handle double precision calculations in the 128-bit registers, reaching double precision 102GFLOPs per chip.|$|E
50|$|In 1983, Bell Labs created niobium/aluminum oxide Josephson {{junctions}} {{that were}} more reliable and easier to fabricate. In 1985, the Rapid single flux quantum logic scheme, which had improved speed and energy efficiency, was developed by researchers at Moscow State University. These advances led to the United States' Hybrid Technology Multi-Threaded project, started in 1997, which sought to beat conventional semiconductors to the <b>petaflop</b> computing scale. The project was abandoned in 2000, however, and the first conventional <b>petaflop</b> computer was constructed in 2008. After 2000, attention turned to superconducting quantum computing. The 2011 introduction of reciprocal quantum logic by Quentin Herr of Northrop Grumman, as well as energy-efficient rapid single flux quantum by Hypres, were seen as major advances.|$|R
50|$|Progress in China {{has been}} rapid, in that China placed 51st on the TOP500 list in June 2003, then 14th in November 2003, and 10th in June 2004 and then 5th during 2005, before gaining {{the top spot}} in 2010 with the 2.5 <b>petaflop</b> Tianhe-I supercomputer.|$|R
40|$|While the HPC {{community}} is working towards {{the development of}} the first Exaflop computer (expected around 2020), after reaching the <b>Petaflop</b> milestone in 2008 still only few HPC applications are able to fully exploit the capabilities of <b>Petaflop</b> systems. In this paper we argue that efforts for preparing HPC applications for Exascale should start before such systems become available. We identify challenges {{that need to be addressed}} and recommend solutions in key areas of interest, including formal modeling, static analysis and optimization, runtime analysis and optimization, and autonomic computing. Furthermore, we outline a conceptual framework for porting HPC applications to future Exascale computing systems and propose steps for its implementation. Comment: 18 th International Conference on Network-Based Information Systems (NBiS 2015). 2 - 4 September 2015 in Tamkang, Taiwa...|$|R
25|$|With {{the help}} of the {{computing}} power of over half a million PlayStation 3 consoles, the distributed computing project Folding@home has been recognized by Guinness World Records as the most powerful distributed network in the world. The first record was achieved on September 16, 2007, as the project surpassed one <b>petaFLOPS,</b> which had never previously been attained by a distributed computing network. Additionally, the collective efforts enabled PS3 alone to reach the <b>petaFLOPS</b> mark on September 23, 2007. In comparison, the world's second-most powerful supercomputer at the time, IBM's BlueGene/L, performed at around 478.2teraFLOPS, which means Folding@home's computing power is approximately twice BlueGene/L's (although the CPU interconnect in BlueGene/L is more than one million times faster than the mean network speed in Folding@home). As of May 7, 2011, Folding@home runs at about 9.3 x86 <b>petaFLOPS,</b> with 1.6petaFLOPS generated by 26,000 active PS3s alone. In late 2008, a cluster of 200 PlayStation 3 consoles was used to generate a rogue SSL certificate, effectively cracking its encryption.|$|E
25|$|In May 2008, an Opteron- and PowerXCell 8i-based super{{computer}}, the IBM Roadrunner system, {{became the}} world's first system to achieve one <b>petaFLOPS,</b> {{and was the}} fastest computer in the world until third quarter 2009. The world's three most energy efficient supercomputers, as represented by the Green500 list, are similarly based on the PowerXCell 8i.|$|E
25|$|As of October 2016, over 4 million {{machines}} {{running the}} open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform {{are members of}} the World Community Grid. One of the projects using BOINC is SETI@home, which was using more than 400,000 computers to achieve 0.828 TFLOPS as of October 2016. As of October 2016 Folding@home, which is not part of BOINC, achieved more than 101 x86-equivalent <b>petaflops</b> on over 110,000 machines.|$|E
5000|$|The TeraGrid {{integrated}} high-performance computers, data {{resources and}} tools, and experimental facilities. Resources included {{more than a}} <b>petaflop</b> of computing capability and more than 30 petabytes of online and archival data storage, with rapid access and retrieval over high-performance computer network connections. Researchers could also access more than 100 discipline-specific databases.|$|R
25|$|The Integrated Computing Network (ICN) is a multi-security level network at the LANL {{integrating}} large host supercomputers, a file server, a batch server, {{a printer}} and graphics output server {{and numerous other}} general purpose and specialized systems. IBM Roadrunner, {{which was part of}} this network, was the first supercomputer to hit <b>petaflop</b> speeds.|$|R
40|$|HTMT is {{an ambitious}} new {{architecture}} combining cutting edge technologies to reach <b>petaflop</b> performance sooner than current technology trends allow. It is a {{massively parallel architecture}} with multi-threaded hardware and a multi-level memory hierarchy. Microservers provide a new perspective for viewing this memory hierarchy whereby memory is actively involved in process execution...|$|R
25|$|In {{conjunction}} with the constellation, Rensselaer operates the Center for Computational Innovations {{which is the result}} of a $100 million collaboration between Rensselaer, IBM, and New York State to further nanotechnology innovations. The center is currently home to the most powerful private-university based supercomputer in the world and it's supercomputer is consistently ranked among the most powerful in the world, capable of performing over 1.1 <b>petaFLOPS.</b> The center's main focus is on reducing the cost associated with the development of nanoscale materials and devices, such as used in the semiconductor industry. The university also utilizes the center for interdisciplinary research in biotechnology, medicine, energy, and other fields. Rensselaer additionally operates a nuclear reactor and testing facility the only university-run reactor in New York State as well as the Gaerttner Linear Accelerator, which is currently being upgraded under a $9.44 million grant from the US Department of Energy.|$|E
500|$|Titan's {{hardware}} has {{a theoretical}} peak performance of 27 <b>petaFLOPS</b> with [...] "perfect" [...] software. On November 12, 2012, the TOP500 organization that ranks the worlds' supercomputers by LINPACK performance, ranked Titan first at 17.59 <b>petaFLOPS,</b> displacing IBM Sequoia. Titan also ranked {{third on the}} Green500, the same 500 supercomputers ranked in terms of energy efficiency. In the June 2013 TOP500 ranking, Titan fell to second place behind Tianhe-2 and to twenty-ninth on the Green500 list. Titan did not re-test for the June 2013 ranking, because it would still have ranked second, at 27 <b>petaFLOPS.</b>|$|E
2500|$|... : LLNL’s Terascale Simulation Facility houses {{one of the}} world’s most {{powerful}} computers, Sequoia. Sequoia occupied the No. 1 position on the Top500 list in June 2012; the current system achieves a Linpack benchmark performance of 16.32 PFlop/s (<b>Petaflops,</b> or quadrillions of calculations per second). Another Blue Gene class machine, Dawn, was installed {{to act as a}} developmental testbed for multi-petaflop computing.|$|E
50|$|The Integrated Computing Network (ICN) is a multi-security level network at the LANL {{integrating}} large host supercomputers, a file server, a batch server, {{a printer}} and graphics output server {{and numerous other}} general purpose and specialized systems. IBM Roadrunner, {{which was part of}} this network, was the first supercomputer to hit <b>petaflop</b> speeds.|$|R
40|$|Protein Folding is {{considered}} one of today’s most significant “grand challenge ” problems and, together with other computational chemistry problems, {{has been one of the}} driving forces for the development of bigger, better, faster supercomputers. IBM is currently proposing to build the “Blue Gene ” [5], a <b>petaflop</b> computer to tackle the protein foldin...|$|R
50|$|On November 19, 2008 Global Language Monitor {{announced}} the most confusing yet frequently cited high tech buzzwords of 2008 to be cloud computing, green washing, and buzzword compliant followed by resonate, de-duping, and virtualization. Rounding out the Top Ten were Web 2.0, versioning, word clouds, and <b>petaflop.</b> The most confusing Acronym for 2008 was SaaS (software as a service).|$|R
