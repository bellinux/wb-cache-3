35|66|Public
40|$|We {{examine the}} neoclassical {{investment}} model using {{a panel of}} U. S. manufacturing firms. The standard model with no financing constraints cannot be rejected for firms with high (<b>pre-sample)</b> dividend payouts. However, it is decisively rejected for firms with low (<b>pre-sample)</b> payouts (firms we expect to face financing constraints). Hem, investment is sensitive to both firm cash flow and macroeconomic credit conditions, holding constant investment opportunities. Sample splits based on firm size or maturity do not produce such distinctions. The latter comparison identifies firms where "free-cash-flow" problems {{might be expected to}} produce correlations between investment and cash flow. ...|$|E
40|$|We {{investigate}} {{the effect of}} environmental policies on innovation under different levels of competition. Using information regarding renewable energy policies, competition and green patents for OECD countries since the late 1970 s, we develop a <b>pre-sample</b> mean count-data econometric specification that accounts for the endogeneity of policies. We find that renewable energy policies are more effective in fostering green innovation in countries with liberalized energy markets. We also find that environmental policies are crucial only in the generation of high-quality green patents, whereas competition enhances the generation of low-quality green patents...|$|E
30|$|For each {{respondent}} 30 repeated {{trials were}} observed. The {{motivation for the}} respondent was, at each trial, to reach the destination {{in time for a}} meeting (or to attend a class). The more punctual the respondents were able to be, the higher the assigned score was (50 points maximum). Moreover, a bonus of 10 points was assigned to each trial if the respondent was able to choose the fastest route (see [13]). The bonus sums up to the standard punctuality-related score. The rewards scheme was chosen by means of a pre-pilot study among ten friends of the SP platform developer (who is {{one of the authors of}} this paper). With this rewards scheme, most of the pre-pilot group were able to achieve almost the same score by acting in two different explicitly requested ways. The first was to consider the fastest-route bonus score as the main goal and the punctuality score as the secondary one, and vice versa for the second way. During the balancing of the rewards scheme only a context without ATIS was applied by the SP platform. Half of the <b>pre-sample</b> was asked to first act with the fastest-route goal and then in the more conservative way; vice versa for the other half of the <b>pre-sample.</b> Because of the balancing of the rewards scheme, we are confident that it does not discourage risk-adverse (preferring punctuality) or risk-seeking (preferring the bonus) route choice behaviour.|$|E
40|$|Over {{the last}} decades, {{computer}} graphics and vision researchers {{have focused on}} developing novel visual effects for computer-generated photo-realistic images. To achieve high-quality output, many state-of-the-art rendering algorithms, which are known as data-driven rendering, pre-process an input three-dimensional scene with complex procedures to obtain necessary data, or pre-capture the real world {{with a set of}} images. The desired visual effects are then recon-structed from the <b>pre-sampled</b> observations for efficient run-time rendering. Nevertheless, with the increasing demand of more and more photo-realistic image synthesis, the amount of <b>pre-sampled</b> data expands accordingly. It not only consumes a great deal of storage space, but also increases data access time and rendering costs at run-time. In order to solve this issue, we can adopt a compact representation to efficiently describe the <b>pre-sampled</b> observations and further apply sophisticated approximation methods {{to reduce the amount of}} data, but achieving real-time performance at the same time is frequently another chal-lenging problem. In this dissertation, we thus focus on data representations and approximation algorithms for real-time rendering of visual data sets. Two novel parametric representations...|$|R
40|$|Abstract Background Since {{it is not}} {{yet clear}} whether it is {{possible}} to satisfactorily avoid sampling-induced stress interference in poultry, more studies on the pattern of physiological response and detailed quantification of stress connected with the first few minutes of capture and <b>pre-sampling</b> handling in poultry are required. This study focused on detection of changes in the corticosterone level and concentrations of other selected biochemical parameters in broilers handled in two different manners during blood sampling (involving catching, carrying, restraint, and blood collection itself) that lasted for various time periods within the interval 30 - 180 seconds. Methods Stress effects of <b>pre-sampling</b> handling were studied in a group (n = 144) of unsexed ROSS 308 broiler chickens aged 42 d. Handling (catching, carrying, restraint, and blood sampling itself) was carried out in a gentle (caught, held and carried carefully in an upright position) or rough (caught by the leg, held and carried with lack of care in inverted position) manner and lasted for 30 s, 60 s, 90 s, 120 s, 150 s, and 180 s. Plasma corticosterone, albumin, glucose, cholesterol, lactate, triglycerides and total protein were measured in order to assess the stress-induced changes to these biochemical indices following handling in the first few minutes of capture. Results <b>Pre-sampling</b> handling in a rough manner resulted in considerably higher plasma concentrations of all biochemical indices monitored when compared with gentle handling. Concentrations of plasma corticosterone after 150 and 180 s of handling were considerably higher (P Conclusions These results indicate that the <b>pre-sampling</b> procedure may be a considerably stressful procedure for broilers, particularly when carried out with lack of care and exceeding 120 seconds. </p...|$|R
5000|$|Miles Tackett is {{best known}} {{around the world as}} bassist/leader of Breakestra, a funk, soul-jazz band whose records & live shows are a throwback to the <b>pre-sampling</b> era of Bronx DJ performances. [...] "Music Man Miles", as he is known among the {{underground}} DJ crowd, also helms the longest-running funky soul party in Los Angeles spinning an all-vinyl set.|$|R
40|$|Particle Induced X-ray Emission is a {{well-established}} technique for quantitative elemental analysis down to trace levels. During microbeam analysis, where the beam is collimated and focused {{into a small}} spot, the beam current reduces to nA or less. The generation of characteristic X-rays is reduced in the same proportion, leading to long data-acquisition times. This can partly be compensated for by using detectors with a large solid angle. In this work, the performance of an annular eight-element silicon drift detector with a total solid angle of 261 msr is described. The initial calibration of the detector was performed using thin elemental standards. Charge measurement was carried out both in a Faraday Cup positioned after the sample and by a <b>pre-sample</b> electrostatic deflection system sampling the beam charge into another Faraday Cup. The two methods were used in parallel and compared during the calibration measurements. A recently installed Versa Module Europe (VME) based data acquisition system equipped with, for example, multi-hit time-to-digital converters, amplifiers, and 32 -channel scalers, was used to record data in event-by-event mode for simultaneous data evaluation on multiple computers. Off-line dead time and pile-up corrections were made on the event data that was sorted into spectra and fitted with the GeoPIXE software. The <b>pre-sample</b> deflection charge measurement gave consistent values for the calibration, {{and this is an}} important observation implying that non-conductive and thick samples will be able to quantify without the use of internal standards...|$|E
40|$|The {{characteristics}} of the thematic mapper (TM) and multispectral scanner (MSS) sensors on LANDSATs 4 and 5 affecting their spatial responses are described, and functions defining {{the response of the}} system to an arbitrary input spatial pattern are derived, i. e., transfer functions (TF) and line spread functions (LSF). These design LSF's and TF's were modified based on prelaunch component and system measurements to provide improved estimates. Prelaunch estimates of LSF/FT's are compared to in-orbit estimates. For the MSS instruments, only limited prelaunch scan direction square-wave response (SWR) data were available. Design estimates were modified by convolving in Gaussian blur till the derived LSF/TF's produced SWR's comparable to the measurements. The two MSS instruments were comparable at their temperatures of best focus; separate calculations were performed for bands 1 and 3, band 2 and band 4. The <b>pre-sample</b> nadir effective instantaneous field's of view (EIFOV's) based on the. 5 modulation transfer function (MTF) criteria vary from 70 to 75 meters in the track direction and 79 to 82 meters in the scan direction. For the TM instruments more extensive prelaunch measurements were available. Bands 1 to 4, 5 and 7, and 6 were handled separately as were the two instruments. Derived MTF's indicate nadir <b>pre-sample</b> EIFOV's of 32 to 33 meter track (bands 1 to 5, 7) and 36 meter scan (bands 1 to 5, 7) and 1245 meter track (band 6) and 141 meter scan (band 6) for both TM's...|$|E
40|$|We {{investigate}} {{the effectiveness of}} policies in favor of innovation in renewable energy under different levels of competition. Using information regarding renewable energy policies, product market regulation and high-quality green patents for OECD countries since the late 1970 s, we develop a <b>pre-sample</b> mean count-data econometric specification that also accounts for the endogeneity of policies. We find that renewable energy policies are significantly more effective in fostering green innovation in countries with deregulated energy markets. We also find that public support for renewable energy is crucial only in the generation of high-quality green patents, whereas competition enhances the generation of green patents irrespective of their quality...|$|E
40|$|Background: Since {{it is not}} {{yet clear}} whether it is {{possible}} to satisfactorily avoid sampling-induced stress interference in poultry, more studies on the pattern of physiological response and detailed quantification of stress connected with the first few minutes of capture and <b>pre-sampling</b> handling in poultry are required. This study focused on detection of changes in the corticosterone level and concentrations of other selected biochemical parameters in broilers handled in two different manners during blood sampling (involving catching, carrying, restraint, and blood collection itself) that lasted for various time periods within the interval 30 - 180 seconds. Methods: Stress effects of <b>pre-sampling</b> handling were studied in a group (n = 144) of unsexed ROSS 308 broiler chickens aged 42 d. Handling (catching, carrying, restraint, and blood sampling itself) was carried out in a gentle (caught, held and carried carefully in an upright position) or rough (caught by the leg, held and carried with lac...|$|R
40|$|The {{amount of}} {{information}} lost in sub-Nyquist sampling of a continuous-time Gaussian stationary process is quantified. We consider a combined source coding and sub-Nyquist reconstruction problem in which the input to the encoder is a noisy sub-Nyquist sampled version of the analog source. We first derive an expression for the mean squared error in {{the reconstruction of the}} process from a noisy and information rate-limited version of its samples. This expression {{is a function of the}} sampling frequency and the average number of bits describing each sample. It is given as the sum of two terms: Minimum mean square error in estimating the source from its noisy but otherwise fully observed sub-Nyquist samples, and a second term obtained by reverse waterfilling over an average of spectral densities associated with the polyphase components of the source. We extend this result to multi-branch uniform sampling, where the samples are available through a set of parallel channels with a uniform sampler and a <b>pre-sampling</b> filter in each branch. Further optimization to reduce distortion is then performed over the <b>pre-sampling</b> filters, and an optimal set of <b>pre-sampling</b> filters associated with the statistics of the input signal and the sampling frequency is found. This results in an expression for the minimal possible distortion achievable under any analog to digital conversion scheme involving uniform sampling and linear filtering. These results thus unify the Shannon-Whittaker-Kotelnikov sampling theorem and Shannon rate-distortion theory for Gaussian sources. Comment: Accepted for publication at the IEEE transactions on information theor...|$|R
40|$|PHASE is a Monte Carlo event generator, under construction, for all Standard Model {{processes}} {{with six}} fermions {{in the final}} state at the LHC. It employs the full set of tree level Feynman diagrams, taking into account fermion masses for b quarks. The program can generate unweighted events for any subset of all six fermion final states in a single run, by making use of dedicated <b>pre-samples.</b> An interface to hadronization is provided. 1...|$|R
40|$|In {{this paper}} {{we examine the}} panel data {{estimation}} of dynamic models for count data that include correlated fixed effects and predetermined variables. Use of a linear feedback model is proposed. A quasi-differenced GMM estimator is consistent for the parameters in the dynamic model, but when series are highly persistent, {{there is a problem}} of weak instrument bias. An estimator is proposed that utilises <b>pre-sample</b> information of the dependent count variable, which is shown in Monte Carlo simulations to possess desirable small sample properties. The models and estimators are applied to data on US patents and R&D expenditure. (C) 2002 Elsevier Science B. V. All rights reserved...|$|E
40|$|Architectural {{simulation}} {{is extremely}} time-consuming given the {{huge number of}} instructions {{that need to be}} simulated for contemporary benchmarks. Sampled simulation that selects a number of samples from the complete benchmark execution yields substantial speedups. However, there is one major issue that needs to be dealt with in order to minimize non-sampling bias, namely the hardware state {{at the beginning of each}} sample. This is well known in the literature as the cold-start problem. The hardware structures that suffer the most from the cold–start problem are cache hierarchies. In this paper, we propose NSL–BLRL, which combines two previously proposed cache hierarchy warmup approaches, namely: no-state-loss (NSL) and boundary line reuse latency (BLRL). The idea of NSL–BLRL is to warmup the cache hierarchy using a hardware state checkpoint that stores a truncated NSL stream. The NSL stream is a least-recently used stream of (unique) memory references in the <b>pre-sample.</b> This NSL stream is then truncated to form the NSL–BLRL warmup checkpoint; this is done by inspecting the sample for determining how far in the <b>pre-sample</b> one needs to go back to accurately warmup the hardware state for the given sample. We show using SPEC CPU 2000 benchmarks that NSL–BLRL is (i) nearly as accurate as BLRL and NSL for sampled processor simulation, (ii) yields simulation time speedups of several orders of magnitude compared to BLRL and (iii) is more space-efficient than NSL. As such, we conclude that NSL–BLRL is a highly efficient and accurate cache warmup strategy for sampled processor simulation...|$|E
40|$|This paper {{examines}} {{the application of}} count data models to firm level panel data on technological innovations. The model the authors propose exhibits dynamic feedback and unobserved heterogeneity. We develop a fixed effects estimator that generalizes the standard Poisson and negative binomial models allowing for dynamic feedback through both the firm's stock of knowledge and its product market power. By using the long <b>pre-sample</b> history of innovation information this 'entry stock' estimator is shown to control for correlated fixed effects and is compared with an alternative nonlinear GMM estimator. We find evidence of history dependence in innovation activity although variables reflecting the company's economic environment are also found {{to play a major}} role. Copyright 1995 by Royal Economic Society. ...|$|E
40|$|The {{topic of}} thesis are reasons for {{discarding}} blood donors. The {{aim of this}} thesis is to evaluace the amount of discarded blood donors {{and the reasons for}} elimination in 2015 and 2016, further implementation of <b>pre-sampling</b> and post-sampling tests in the Blood Bank Department of the Hospital ČB, Ltd. In the theoretical part I attend to important events in the history of transfusion medicine. Next I deal with criteria for receiving blood donors, evaluation of the blood donors and ways, amounts and frequency of blood collections. The important part of my thesis are the reasons for discarding blood donors, which are divided into temporary and permanent. There are registers, in which these donors are registered. In the methodological part I attend to <b>pre-sampling</b> and post-sampling tests, which are performed in the Blood Bank Department of the Hospital ČB, Ltd. At <b>pre-sampling</b> laboratory measurement of hemoglobin amount on HemoCue 201 + is performed. For new blood donors the blood group orientation is examined on the slide. Donors plasma, platelets or in case of chylozity blood at the last sample hematokrit is examined. Further examination post-sampling test, which are include tests of infectious markers and imunohematology. For examination infectious markers are of service analyzer Architect, tests make on this analyzer are: Ab anti HCV, Ag and Ab HIV, Ab against Treponema pallidum and HBsAg. Immunohematology tests are examination with a dispenser Qasar and imagine analyzer Duet Reader. These two devices are jointly involved in the testing of AB 0 blood group and RhD, screening of irregular anti-erythrocyte antibodies and examination of a group of Rh Kell. In 2015 there were 15 048 blood donors and 360 women and 306 men were discarded. In 2016 there were 15 788 blood donors and 286 women and 211 men were discarded. The most blood donors were discarded because of unsufficient amount of hemoglobin...|$|R
5000|$|Breakestra is a funk music project {{founded by}} Miles Tackett and based in Los Angeles, California. Breakestra was first formed in 1997 as a {{strictly}} live ensemble playing [...] "covers" [...] of funk, soul, and jazz breaks {{that had been}} sampled in late 1980s & early 1990s hip-hop seamlessly, blended into {{each other in the}} same style that early hip hop DJs would do in the <b>pre-sampling</b> days of the 1970s when they would DJ records at block parties.|$|R
40|$|In this paper, {{the optimal}} {{sampling}} strategies (uniform or nonuniform) and distortion tradeoffs for Gaussian bandlimited periodic signals with additive white Gaussian noise are studied. Our {{emphasis is on}} characterizing the optimal sampling locations {{as well as the}} optimal <b>pre-sampling</b> filter to minimize the reconstruction distortion. We first show that to achieve the optimal distortion, no <b>pre-sampling</b> filter is necessary for any arbitrary sampling rate. Then, we provide a complete characterization of optimal distortion for low and high sampling rates (with respect to the signal bandwidth). We also provide bounds on the reconstruction distortion for rates in the intermediate region. It is shown that nonuniform sampling outperforms uniform sampling for low sampling rates. In addition, the optimal nonuniform sampling set is robust with respect to missing sampling values. On the other hand, for the sampling rates above the Nyquist rate, the uniform sampling strategy is optimal. An extension of the results for random discrete periodic signals is discussed with simulation results indicating that the intuitions from the continuous domain carry over to the discrete domain. Sparse signals are also considered, where it is shown that uniform sampling is optimal above the Nyquist rate. Comment: Under revie...|$|R
40|$|We {{show that}} {{universities}} in the United States that provide stronger royalty incentives to faculty scientists generate greater license income, controlling for university characteristics. We use <b>pre-sample</b> data on university patenting to control for the potential endogeneity of royalty shares. Faculty responds to royalties both {{in the form of}} cash and research lab support, indicating both pecuniary and intrinsic research motivations. The impact of incentives is larger in private than in public universities, and we provide new survey evidence on the organization and objectives of university licensing offices to explain this difference. Royalty incentives work both by raising faculty effort and sorting scientists across universities. The primary impact of incentives is to increase the quality rather than the quantity of inventions. Copyright (c) 2008, RAND. ...|$|E
40|$|The {{so-called}} type I {{and type}} II fractional Brownian motions are limit distributions {{associated with the}} fractional integration model in which <b>pre-sample</b> shocks are either included in the lag structure, or suppressed. There can be substantial di¤erences between the distributions of these two processes and of functionals derived from them, so that it becomes an important issue to decide which model {{to use as a}} basis for inference. Alternative methods for simulating the type I case are contrasted, and for models close to the nonstationarity boundary, truncating in…nite sums is shown to result in a signi…cant distortion of the distribution. A simple simulation method that overcomes this problem is described and implemented. The approach also has implications for the estimation of type I ARFIMA models, and a new conditional ML estimator is proposed, using the annual Nile minima series for illustration. ...|$|E
40|$|This article hammers out the {{estimation}} of a fixed effects dynamic panel data model extended to include either spatial error autocorrelation or a spatially lagged dependent variable. To overcome the inconsistencies associated with the traditional least-squares dummy estimator, the models are first-differenced to eliminate the fixed effects and then the unconditional likelihood function is derived {{taking into account the}} density function of the first-differenced observations on each spatial unit. When exogenous variables are omitted, the exact likelihood function is found to exist. When exogenous variables are included, the <b>pre-sample</b> values of these variables and thus the likelihood function must be approximated. Two leading cases are considered. the Bhargava and Sargan approximation and the Nerlove and Balestra approximation. As an application, a dynamic demand model for cigarettes is estimated based on panel data from 46 U. S. states over the period from 1963 to 7992...|$|E
40|$|In this paper, an {{advanced}} direct RF sampling receiver architecture is studied for the GNSS environment. The architecture {{is based on}} sampling the signal directly at RF, which in the GNSS case are in the 1. 5 GHz range. The high-frequencies in the signals to be sampled pose then very high demands for the accuracy {{and quality of the}} sampling process, and thus quantization and especially the timing jitter must be considered in detail. The study shows that the quantization and jitter requirements are, however, feasible when the <b>pre-sampling</b> filtering is done properly...|$|R
40|$|This paper {{deals with}} the {{development}} of a multisensory virtual environment with visual, haptic, and aural feedbacks for simulating the five-axis CNC milling process. The paper focuses on the haptic and au-ral rendering of the virtual milling process. Haptic rendering provides the user with kinesthetic and tactile information. Kinesthetic informa-tion is displayed by the cutting force of a milling machine. The tactile information is conveyed by the haptic texturing. Aural rendering sim-ulates the machine sound and provides the aural feedback to the user. Using ideas from the concepts of image-based rendering, haptic and aural rendering are accelerated by <b>pre-sampling</b> related environment’s parameters in a perception-dependent way. ...|$|R
5000|$|... #Caption: X(f) (top blue) and XA(f) (bottom blue) are {{continuous}} Fourier transforms of two [...] functions, x(t) and xA(t) (not shown). When {{the functions}} are sampled at rate fs, the images (green) {{are added to}} the original transforms (blue) when one examines the discrete-time Fourier transforms (DTFT) of the sequences. In this hypothetical example, the DTFTs are identical, which means , even though the original continuous <b>pre-sampled</b> functions are not. If these were audio signals, x(t) and xA(t) might not sound the same. But their samples (taken at rate fs) are identical and would lead to identical reproduced sounds; thus xA(t) is an alias of x(t) at this sample rate.|$|R
30|$|K-means {{is one of}} the {{simplest}} algorithms used to solve the clustering problem. This clustering algorithm can be applied to extract key-frames from short video sequences or shots, but its application to longer video sequences must be done with care taking into account the large processing time and memory requirements of the algorithm. To reduce the number of frames used by the clustering algorithm some authors <b>pre-sample</b> the original video, as proposed in [24]. The quality of the summaries may not be affected by this operation but the sampling rate must be chosen carefully. Although K-means is a popular and well-known clustering algorithm it has some limitations such as the need to pre-establish the number of clusters desired priori {{and the fact that the}} sequential order of the key-frames may not be preserved. Huang et al. [18] used the K-means clustering algorithm for extracting a set of 3 D key-frames to be compared with the output of their key-frame extraction method.|$|E
40|$|Using data on U. S. {{universities}}, we {{show that}} universities that give higher royalty shares to faculty scientists generate greater license income, controlling for university size, academic quality, research funding and other factors. We use <b>pre-sample</b> data on university patenting to control for the potential endogeneity of royalty shares. We find that scientists respond both to cash royalties and to royalties used to support their research labs, suggesting both pecuniary and intrinsic (research) motivations. The incentive effects appear to be larger in private universities than in public ones, and we provide survey evidence indicating this {{may be related to}} differences in the use of performance pay, government constraints, and local development objectives of technology license offices. Royalty incentives work both by raising faculty effort and sorting scientists across universities. The effect of incentives works primarily by increasing the quality (value) rather than the quantity of inventions. academic research; incentives; intellectual property; licensing; royalties; technology transfer...|$|E
40|$|II) Confidence {{sets and}} {{confidence}} intervals. Why? (III) Bayesian interpretation of frequentist data analysis. (IV) A case where Bayesian and frequentist approaches seem aligned: SNLM (V) Cases where they conflict: Unit root time series models; breaks. (VI) MCMC basics (VII) Inference for DSGE’s. 2. BAYESIAN INFERENCE IS A WAY OF THINKING, NOT A BASKET OF “METHODS” • Frequentist inference makes only <b>pre-sample</b> probability assertions. – A 95 % confidence interval contains the true parameter value with probability. 95 only before one {{has seen the}} data. After the data has been seen, the probability is zero or one. – Yet confidence intervals are universally interpreted in practice as guides to post-sample uncertainty. – They often are reasonable guides, but only because they often are close to posterior probability intervals that would emerge from a Bayesian analysis. • People want guides to uncertainty {{as an aid to}} decision-making. They want to characterize uncertainty about parameter values, given the sample that has actually been observed. That it aims to help with this is the distinguishing characteristic of Bayesian inference...|$|E
40|$|WPHACT 2. 0 {{is the new}} fully massive {{version of}} a MC program and {{unweighted}} event generator which computes all Standard Model processes with four fermions in the final state at e + e − colliders. The program can now generate unweighted events for any subset of all four fermion final states in a single run, by making use of dedicated <b>pre-samples</b> which can cover the entire phase space. Improvements with respect to WPHACT 1. 0 include the Imaginary Fermion Loop gauge restoring scheme, new phase space mappings, a new input system, the possibility to compute subsets of Feynman diagrams and options for including ISR via QEDPS, running αQED, CKM mixing, resonances in q¯q channels...|$|R
40|$|In a Digital Matched Filter (DMF) the {{degradation}} of signal quality depends on the <b>pre-sampling</b> filter bandwidth and {{on the number of}} Analogue to Digital Converter (ADC) bits. Digital matched filters are used in the acquisition of Global Navigation Satellite Systems (GNSS) signals. Automatic Gain Control (AGC) in the GNSS receivers is used to determine the maximum level of quantization by the ADC. In [1], the quantization degradation for different pre sampling bandwidths and ADC bits is analysed in the presence of Additive Gaussian white noise (AGWN). Bandwidths of more than once (or twice) the data bit rate contribute only about 0. 4 dB (or (0. 2 dB) to the overall degradation of the DMF. Also 2 bit uniform step quantization with optimum ADC threshold setting is shown to recover most of the digital implementation degradation. In this paper by focusing on <b>pre-sampling</b> filter bandwidth equal to the data bit rate and 2 bit ADC, the effect of quantization on the received GNSS signal quality in the presence of Continuous Wave (CW) radio frequency interference (RFI) is analysed. It is shown that a one bit ADC can have much higher degradation than the 3. 5 dB in the AGWN case. It is also shown that the effect of AGC on the quantization degradation has to do with both power and frequency of CW RFI. In the presence of CW RFI, the optimal value for ratio of maximum threshold of ADC to the effective noise RMS is shown to be about 1. 35 which is higher than the case when RFI does not exist (0. 8) ...|$|R
40|$|National Natural Science Foundation of China [51105309, 51175425]; Aviation Science Foundation [2011 ZA 53015]A novel {{adaptive}} importance {{sampling method}} is proposed {{to estimate the}} structural failure probability. It properly utilizes Markov chain algorithm to form an adaptive importance sampling procedure. The main concept is suggesting the proposal distributions of Markov chain as the importance sampling density. Markov chain states can adaptively populate the important failure regions thus the importance sampling based on them will yield an efficient and accurate estimate of the failure probability. Compared with existent methods, it does not need {{the solution of the}} design point(s) or the <b>pre-sampling</b> in the failure region. Various examples are given to demonstrate the advantages of the proposed method. (C) 2013 Elsevier Masson SAS. All rights reserved...|$|R
40|$|This paper {{reviews the}} di¤erences between the {{so-called}} type I and type II models of fractional Brownian motion, {{corresponding to the}} cases in which <b>pre-sample</b> shocks are either included in the lag structure, or suppressed. It is noted {{that there can be}} substantial di¤erences between the distributions of these two processes, and of functionals derived from them, so that it becomes an important issue to decide which model to use as a basis for approximate inference based on Monte Carlo simulation. The main problem addressed is that of simulating the type I case. For models close to the nonstationarity boundary, the number of in‡uential lags becomes very large, and truncating the sums to a computationally feasible number of terms results in signi…cant distortions of the distribution. A simple method of overcoming this problem is implemented. The distributions of representative statistics for the type I and type II models are compared in Monte Carlo experiments. Also considered is the estimation of type I ARFIMA models, with the annual Nile minima series used for illustration...|$|E
40|$|Abstract Ilestadvannet is a {{dimictic}} lake in Andebu County, Norway, {{with its}} catchment area {{situated in the}} counties of Andebu and Re. One <b>pre-sample</b> was taken in November 2002, while the main sampling was done from February 2003 to January 2004. Samples for qualitative identification of algae were also collected in 2004. The lake is eutrophicated, mesotrophic, and has circumneutral pH, low conductivity and alkalinity. A total of 251 algae species in 136 genera were observed, 178 of these in the lake plankton. The divisions Cyanophyta, Cryptophyta, Chrysophyta, Bacillariophyta, Xanthophyta, Haptophyta, Dinophyta, Euglenophyta, Rhodophyta, Prasinophyta and Chlorophyta were registered. The dominating divisions in winter and spring 2003 were Cryptophyta and Chrysophyta. In the summer the centric diatoms, especially Aulacoseira distans, were the dominating species. Late summer and early autumn Chlorophyta and Dinophyta were also represented. Late fall and winter Cryptophyta and Chrysophyta were again the dominating divisions. In the fall of 2004, a bloom of the cyanobacteria species, Anabaena lemmermannii, Microcystis aeruginosa and Woronichinia naegeliana occurred. An ELISA test confirmed that they produced microcystins. The varying contribution of nutrients from Merkedamselva during flood {{may be one reason}} for these annual variations in the phytoplankton community...|$|E
40|$|In a {{previous}} study, we reported apparently paradoxical facilitation of object recognition memory following infusions of the cholinergic muscarinic receptor antagonist scopolamine into the perirhinal cortex (PRh) of rats. We attributed these effects to the blockade by scopolamine of {{the acquisition of}} interfering information. The present study tested this possibility directly by modifying the spontaneous object recognition memory task to allow the presentation of a potentially interfering object either before the sample phase or in the retention delay between the sample and choice phases. Presentation of an object between the sample and choice phases disrupted subsequent recognition of the sample object (retroactive interference), and intra-PRh infusions of scopolamine prior to {{the presentation of the}} irrelevant object prevented this retroactive interference effect. Moreover, presentation of an irrelevant object prior to the sample phase interfered proactively with sample object recognition, and intra-PRh infusions of scopolamine prior to the presentation of the <b>pre-sample</b> object prevented this proactive interference effect. These results suggest that blocking muscarinic cholinergic receptors in PRh can disrupt the acquisition of potentially interfering object information, thereby facilitating object recognition memory. This finding provides further, strong evidence that acetylcholine is important for the acquisition of object information in PRh...|$|E
40|$|AbstractObjectiveThis study {{aimed to}} check the {{sensitivity}} of multiple newly developed 3 T MRI breast sequences using CAD software, in <b>pre-sampling</b> diagnosis of breast cancer, {{in an attempt to}} minimize unnecessary invasive sampling or surgical procedures. Patients and methodsThis was a prospective study, included 120 female patients, presented with debatable or malignancy suspected mammo-sonographic results. The study protocol was approved by the ethics committee in Al-Mana General Hospital. Results 36 patients’ tumors were reported as benign and 84 were reported as malignant. Biopsy approved 33 tumors as benign and 87 as malignant tumors. These results gave indices of 93. 1 % sensitivity and 90. 9 % specificity. Conclusion 3 T MRI breast with CAD is very sensitive imaging tool, that can help to avoid unnecessary invasive procedures...|$|R
40|$|AbstractObjectiveThis study {{aimed to}} check the {{sensitivity}} of phased array surface coli of 3 T MRI, in <b>pre-sampling</b> diagnosis of prostate cancer, {{in an attempt to}} use it instead of endorectal coil. Patients and methodsThis was a prospective comparative study, included 20 male patients, presented with suspected prostate cancer due to unexplained high PSA. The study protocol was approved by the ethics committee in Al-Mana General Hospital. ResultsProstate cancer was correctly diagnosed by T 2 w sequence within 9 patients, 10 by DW&T 2 w, 13 by T 2 w – DW-DCE and 14 by of T 2 w-DW-DCE-MRS sequences. Conclusion 3 T MRI imaging using phased array surface coil is a useful diagnostic tool for detecting prostate cancer, trustworthy when compared to endorectal approach...|$|R
40|$|Abstract—In this paper, the {{applicability}} of advanced direct RF sampling receiver architecture is studied in the GNSS envi-ronment. The architecture is based on sampling the signal di-rectly at RF, which in the GNSS case is in the 1. 5 GHz range. The high-frequencies in the signal to be sampled pose then very high demands for the accuracy {{and quality of the}} sampling process, and thus quantization and especially the timing jitter aspects must be considered in detail. Both system calculations as well as computer simulations are used to assess the essential requirements for the sampling process. In summary, the study shows that the quantization and jitter requirements are in prin-ciple feasible when the <b>pre-sampling</b> filtering is done properly. Index Terms—Direct RF Sampling, radio receiver design...|$|R
