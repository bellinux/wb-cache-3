43|1|Public
5000|$|Peisistratus or Peisitratos or Pisistratus ( [...] ; Ancient Greek: Πεισίστρατος) {{was king}} of Arcadian Orchomenus {{at the time}} of the Peloponnesian War, who became the object of the hatred of the oligarchical party, and was murdered in an {{assembly}} of the senate. To avoid detection his body was cut to pieces, and the parts of it carried away by the senators under their robes. Tlesimachus, the son of Peisistratus, who was privy to the conspiracy, quieted the populace, who were incensed at the disappearance of their king, by a story of his having appeared to him in a superhuman form after he had left the earth. (Plut. <b>Parall.</b> vol. ii. p. 313, b.) ...|$|E
40|$|I have {{examined}} the Proteinogramm of blood-srums, effusions and vesicate {{on the surface of}} inside fore-arm in cases of pleurisy exdative tuberculosa following its course. Accordant with Processes (exudation, resorption), a definite variation of body fluids and the accleration of the capillary permeability not only at pleural cavity but also in all body were recognized. Thus the conception of so called serous inflammation after Eppinger was assendend. <b>Parall</b> to the clinical figures of exudative Pleurisy and variation of Proteinogramm of body fluids, at kidney, liver, and heart were also found a definite change. I thought these effects were due to the serous inflammation accompanied with local invasions at pleural cavity as mentioned above...|$|E
40|$|PDT is the Parallel DebuggingTool of the Annai {{programming}} environment developedwithin the Joint CSCS-ETH/NEC Collaboration in Parallel Processing. Similarly {{to the other}} components of the integrated environment, PDT provides support for application developers to debug data-parallel programs written in HPF, and message-passingprograms basedon the MPI standard. This paper describes how the PDT source-level debugger addresses three major issues in parallel debugging: execution control (handling of breakpoints, watchpoints, and exceptions), distributed data and data distribution visualization, race detection and deterministic execution replay. We present the algorithms, their implementation, and our latest results regarding intrusiveness and overhead. 1 Introduction As part of the Joint CSCS/NEC Collaborationin Parallel Processing [1], we are currently developing an interactive, source-level debugger for distributed-memory parallel processor (DMPP) programs. The debugger is called <b>Parall</b> [...] ...|$|E
50|$|It <b>paralls</b> {{the eastern}} side of Marsh Glacier for nearly 160 km (100 mi) from Nimrod Glacier in the north to Law Glacier in the south. Mount Markham (4,350 m), is the highest {{elevation}} in the range.|$|R
40|$|A good design, development, and {{maintenance}} methodology for concurrent systems {{must be based}} on a deep recognition and understanding of the intrinsic characteristics of concurrent systems. Based on the author's recognition and understanding of the wholeness principle of concurrent systems and the uncertainty principle in measuring and monitoring concurrent systems, a new design principle, named the "self-measurement principle," for large-scale, long-lived, and highly reliable concurrent systems is proposed. The self-measurement principle requires that a large-scale, long-lived, and highly reliable concurrent system should be constructed by some function components and some (maybe only one) permanent self-measurement components that act concurrently with the function components, measure and monitor the system itself according to some requirements, and pass run-time information about the system's behavior to the outside world of the system. Introduction Concurrent systems, i. e., <b>parall</b> [...] ...|$|E
40|$|One of {{critical}} issues affecting parallel system performance is the scheduling of parallel tasks onto available target processors. A number of algorithms and software tools {{have been presented}} {{to deal with the}} task scheduling problem in parallel processing. This paper studies the static scheduling of general types of applications. We focus on "conditional task scheduling", in which the task model may vary between each program execution, due to conditional branching within tasks. A parallel programming environment, called ATME, is presented to practically tackle this complicated conditional scheduling problem. Keywords: static scheduling, conditional task scheduling, task model, model estimation, multiprocessing, parallel programming environment. 1 Introduction Parallel programming is more complex than its sequential programming counterpart. Issues to be considered include task partition, task communication and synchronization, task scheduling and tuning. The performance of the <b>parall</b> [...] ...|$|E
40|$|Powerful desktop {{multiprocessor}} systems {{based on}} the Intel Architecture (iA) offer a formidable alternative to traditional scientific/engineering workstations for commercial application developers at an attractive costperformance ratio. However, the lack of adequate compiler and runtime library support for multithreading and parallel processing on Windows NT* makes it difficult or impossible to fully exploit the performance advantage of these multiprocessor systems. In this paper we describe the design, development, and initial performance results of the Illinois-Intel Multithreading Library (IML), which aims at providing an efficient and powerful (in terms of types of parallelism it supports) API for multithreaded application developers. IML implements a parallel execution environment, which creates, enqueues, dequeues, binds, and schedules user-level threads on Windows NT* threads and fibers. One of the unique and novel features of IML is its support for both loop-level (data) <b>parall</b> [...] ...|$|E
40|$|Crossover {{designs in}} {{clinical}} trials are very popular for comparing several non-curative treatments for their efficacy, while having each patient as his/her own control. Many investigators have constructed universally optimal designs for certain repeated measurement experiments. However, most of these statistically optimal designs have certain undesirable features from a clinical perspective. In this work, we compare savings in cost and thus the gain in power attainable from employing crossover designs against completely randomized designs. We note that the assumption of a positive within-subject correlation between measurements has been implicit in earlier comparisons and in most models for repeated measures data. We explore other models for repeated measures data that allow for a negative correlation among repeated measurements. Under such models, we will consider some useful crossover designs and compare their cost effectiveness with those of completely randomized designs and <b>parall</b> [...] ...|$|E
40|$|Concurrent {{computing}} on networked {{collections of}} computer systems is rapidly evolving into a viable {{technology that is}} attractive from the economic, performance, and availability perspectives. Several software infrastructures that support such heterogeneous network-based concurrent computing have evolved, and are in use for productionquality high-performance computing. In this paper, we describe such a system, and present our experiences with its use for massively concurrent computing in the application domain of polymer physics. The application involves stochastic simulation of polymer chains for measuring scale-invariant phenomena at critical disorder. The parallelization is achieved through the EcliPSe toolkit, and conducted on a flexible, treestructured virtual machine made up of arbitrary and heterogeneous computing nodes dispersed across the country. These nodes cooperate to perform the simulation and pool results together in real time at a central node which initiates the <b>parall</b> [...] ...|$|E
40|$|Department of Computing Science, Glasgow University Email: ftrinder,kh,simonpjg@dcs. glasgow. ac. uk PW Trinder K Hammond JS Mattson Jr AS Partridge y SL Peyton Jones z Abstract GUM is a portable, {{parallel}} {{implementation of}} the Haskell functional language. Despite sustained research interest in parallel functional programming, GUM {{is one of the}} first such systems to be made publicly available. GUM is message-based, and portability is facilitated by using the PVM communications harness that is available on many multi-processors. As a result, GUM is available for both shared-memory (Sun SPARCserver multiprocessors) and distributed-memory (networks of workstations) architectures. The high message-latency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message. Initial performance figures demonstrate absolute speedups relative to the best sequential compiler technology. To improve the performance of a <b>parall</b> [...] ...|$|E
40|$|In {{this paper}} {{we present a}} new parallelization of an {{efficient}} best-first branch-and-bound algorithm to solve the constrained two-dimensional single stock guillotine cutting stock problem (CSP) to optimality. The underlying sequential branch-and-bound algorithm {{is based on an}} exact version of Wang's heuristic suggested by Viswanathan and Bagchi. In our algorithm we improve the upper bound and introduce duplicate pruning. For an efficient parallelization we developed a new dynamic load-balancing algorithm, because due to the unusual branching strategy and detection of duplicates standard load balancing methods do not work. Our new dynamic load balancing is fully distributed using a direct neighbor strategy. Computational results on two different parallel systems are presented. The implementation is system-independent using the portable parallel branch-and-bound library (PPBB-LIB) developed in Paderborn and can easily be ported to other systems. Keywords: two-dimensional cutting, <b>parall</b> [...] ...|$|E
40|$|In {{this paper}} we are {{comparing}} (via simulations) under real conditions, several routing algorithms based either on oblivious wormhole routing or deflection routing. Although these two techniques differ significally {{in the low}} level structure of information (worms, packets), we derived realistic results using an appropriate simulation model. In our experiments we compared average time, latency and throughput achieved by both deflection and wormhole routing (for both batch and continuous cases). Results show that deflection routing performs better than wormhole routing, for random traffic on the mesh, especially for large networks. Furthermore, although {{it is hard to}} analyze simple versions of wormhole routing, it seems that these versions outperform algorithms that are devised in order to simplify theoretical analysis. Finally we implemented and tested an algorithm that combines both deflection and wormhole routing. 1 Introduction In many applications that run on today's <b>parall</b> [...] ...|$|E
40|$|The side-effect {{problem in}} AND/OR {{parallel}} execution of logic programs {{is complex and}} need to be further investigated. This paper presents a Selective Recomputation(SR) approach for handling side-effects in the OR-forest model which can exploit both AND- and OR- parallelism. In contrast to merely AND- and merely OR- parallel execution models/systems, handling side-effects in AND/OR parallel execution models/systems is much more difficult because of the complex orderings among side-effect built-ins. Firstly, we give {{a brief description of the}} OR-forest model. Secondly, we analyze the side-effect problem in merely AND-parallel, merely OR-parallel and AND/OR parallel models/systems. Thirdly, we describe the Selective Recomputation(SR) approach in detail in the OR-forest model. This approach can solve the side-effect problem with minimum recomputation and maximum parallelism, and it is applicable to other AND/OR parallel execution models/systems as well. Keywords: Logic Programming, <b>Parall</b> [...] ...|$|E
40|$|We {{present a}} set of {{advanced}} program parallelization techniques {{that are able to}} significantly improve the performance of application programs. We present evidence for this improvement in terms of the overall program speedup that we have achieved on the Perfect Benchmarks R fl programs, {{and in terms of the}} performance gains that can be attributed to individual techniques. These numbers were measured on the Cedar multiprocessor at the University of Illinois. This paper extends the findings previously reported in [EHLP 91]. The techniques credited most for the performance gains include array privatization, parallelization of reduction operations, and the substitution of generalized induction variables. We have applied these transformations by hand to the given programs, in a mechanical manner, similar to that of parallelizing compiler. Because of our success with these transformations, we believe that it will be possible to implement many of these techniques in a new generation of <b>parall</b> [...] ...|$|E
40|$|ParaStation is a {{communications}} fabric for connecting off-the-shelf workstations into a supercomputer. The fabric employs technology used in massively parallel machines and scales up to 4096 nodes. ParaStation's user-level message passing software preserves the low latency {{of the fabric}} by taking the operating system out of the communication path, while still providing full protection in a multiprogramming environment. The programming interface presented by ParaStation consists of a UNIX socket emulation and widely used parallel programming environments such as PVM, P 4, and MPI. Implementations of ParaStation using various platforms, such as Digital 's AlphaGeneration workstations and Linux PCs, achieve end-to-end (process-toprocess) latencies as low as 2 ¯s and a sustained bandwidth of up to 15 Mbyte/s per channel, even with small packets. Benchmarks using PVM on ParaStation demonstrate real application performance of 1 GFLOP on an 8 -node cluster. Keywords: Workstation Cluster, <b>Parall</b> [...] ...|$|E
40|$|OF PAPER Evaluating the Performance of Parallel Programs in a Pseudo-Parallel MPI Environment By Erik Demaine This paper {{presents}} {{a system for}} use with the message-passing standard called MPI (Message Passing Interface) that provides a means of automatically simulating a distributed-memory parallel program. This allows one to evaluate a parallel algorithm {{without the use of}} a parallel computer. The system consists of three parts: the network evaluator, logging library, and simulator. The network evaluator is a parallel program that evaluates the network speed of a distributed-memory parallel computer. The logging library, when used, automatically logs the message-passing activity of the running program. The logs are designed so that running the "processors" on a uniprocessor workstation does not affect the contents. The simulator is a serial program that reads a log generated by the logging library and timing results from the network evaluator, and simulates the execution of the <b>parall</b> [...] ...|$|E
40|$|Concurrent Clean is an experimental, lazy, higher-order {{parallel}} {{functional programming}} language based on term graph rewriting. An important difference with other languages is that in Clean graphs are manipulated and not terms. This {{can be used by}} the programmer to control communication and sharing of computation. Cyclic structures can be defined. Concurrent Clean furthermore allows to control the (parallel) order of evaluation to make efficient evaluation possible. With help of sequential annotations the default lazy evaluation can be locally changed into eager evaluation. The language enables the definition of partially strict data structures which make a whole new class of algorithms feasible in a functional language. A powerful and fast strictness analyser is incorporated in the system. The quality of the code generated by the Clean compiler has been greatly improved such that {{it is one of the}} best code generators for a lazy functional language. Two very powerful <b>parall</b> [...] ...|$|E
40|$|Developing {{parallel}} {{software for}} unstructured problems {{continues to be}} a difficult undertaking, particularly for distributed memory machines. Framework and library support are limited for non-standard applications and developers are often forced to code from scratch. This is particularly true for complex, unstructured applications. In this paper, we show that this needn't always be the case. We describe a set of simple primitives which can be combined to provide solutions to a variety of unstructured parallel computing problems. Specifically, we show how a small set of tools can yield efficient parallel algorithms for particle modeling, crash simulations and transferring data between two independent grids in multiphysics simulations. The use of such tools allows the application developer to program at a higher level without sacrificing performance. 1 Introduction As has long been the case, the greatest impediment to the use of parallel computers is the difficulty of writing <b>parall</b> [...] ...|$|E
40|$|In {{this paper}} we {{describe}} a compilation scheme to translate implicitly parallel {{programs in the}} programming language Spar (an extension to Java) to efficient code for distributed-memory parallel computer systems. The compilation scheme is formulated {{as a set of}} transformation rules. In Spar, the language constructs for parallelization have been designed for comfortable use by the programmer, not for ease of compilation. Nevertheless, it is shown that the basic translation and optimization schemes can be implemented with a surprisingly small set of transformation rules. The paper discusses the systematic translation of Spar programs into parallel code by using these rules. 1 Introduction Issues that are of major importance in the design of parallel programming languages and associated compilation schemes for distributed-memory systems include: ffl parallelizability, the extent to which parallelism can either be expressed by the user or deduced by the compiler such that problem <b>parall</b> [...] ...|$|E
40|$|This paper {{presents}} a portable parallel programming environment for Modula- 2 * [...] an explicitly parallel machine-independent extension of Modula- 2. Modula- 2 * offers synchronous and asynchronous parallelism, a global single address space, and automatic data and process distribution. The Modula- 2 * system {{consists of a}} compiler, a debugger, a cross-architecture make, a runtime systems for different machines, {{and a set of}} scalable parallel libraries. Implementations exist for the MasPar MP series of massively parallel processors (SIMD), the KSR- 1 parallel computer (MIMD), heterogeneous LANs of workstations (MIMD), and single workstations (SISD). The paper presents the important components of the Modula- 2 * environment and discusses selected implementation issues. We focus on how we achieve a high degree of portability for our system {{while at the same time}} ensuring efficiency. 1 Introduction The demand for increasing computer performance at reasonable cost leads to rising interest in <b>parall</b> [...] ...|$|E
40|$|This paper proposes {{solutions}} to two important problems with parallel programming environments {{that were not}} previously addressed. The first issue is that current compilers are typically black-box tools with which the user has little interaction. Information gathered by the compiler, although potentially very meaningful for the user, is often inaccessible or hard to decipher. Second, compilation and performance analysis tools are not well integrated. While there are many advanced instruments for gathering and browsing performance results of a program, {{it is difficult to}} relate this information to the source program, to the applied program transformations, and to the compiler's reasoning. The Ursa Minor tool addresses these issues. The tool is designed to help understand the structure of a program and the information gathered by a compiler in an interactive way. It facilitates the comparison of performance results under different environments and the identification of potential <b>parall</b> [...] ...|$|E
40|$|We {{consider}} ß-calculus, amodel of {{concurrent processes}} {{based on the}} notion of naming, extended with probabilistic information. The new language is an evolution of CSP - like stochastic process algebra that we call stochastic ß- calculus. Furthermore, we integrate the semantic description of the language with topology information expressed through axioms. The new formalism is suitable to study behavioural and performance property of distributed systems. In particular, we can compare different allocations of systems on a fixed network architecture with respect to expected performance. 1 Introduction The design and development of concurrent distributed systems is a difficult task due to the large number of parameters that designers must take into account. Hence, the use of formal methods during all the life-cycle of these systems is becoming well-accepted. Particular interest is devoted to process algebras such as CCS [15], CSP [13], Meije [2]. Only few operators (sequential and <b>parall</b> [...] ...|$|E
40|$|We are {{developing}} a lazy, self-optimising parallel library of vector-matrix routines. The aim is to allow users to parallelise certain computationally expensive parts of numerical programs by simply linking with a parallel rather than sequential library of subroutines. The library performs interprocedural data placement optimisation at runtime, which requires the optimiser itself to be very efficient. We achieve this firstly by working from aggregate loop nests which have been optimised in isolation, and secondly by using a carefully constructed mathematical formulation for data distributions and the distribution requirements of library operators, which allows us largely to replace searching with calculation in our algorithm. 1 Introduction This paper describes an approach to interprocedural data placement optimisation {{in the context of}} a parallel numerical library. The idea for such a library, as described in our previous paper [4], is to make it easy for users to <b>parall</b> [...] ...|$|E
40|$|This paper {{describes}} an environment for performance-oriented design of portable parallel software. The environment {{consists of a}} graphical design tool based on the PVM communication library for building parallel algorithms, a state-of-the-art simulation engine and a visualisation tool for animation of program execution and visualisation of platform and network performance measures and statistics. The toolset is used to model a virtual machine composed of a cluster of workstations interconnected by a local area network. The simulation model used is modular and its components are interchangeable which allows easy re-configuration of the platform. The model is validated using experiments on the COMMS 1 benchmark from the Parkbench suite, and a standard image processing algorithm with maximum errors of 1 % and 10 % respectively. 1 Introduction A major obstacle to the widespread adoption of parallel computing in industry is the difficulty in program development due mainly to lack of <b>parall</b> [...] ...|$|E
40|$|In {{this paper}} a {{modification}} of a standard parallel genetic algorithm (SPGA) is introduced which can be run efficiently on different types of parallel computers. The purpose of the algorithm is to find optimal morphological filters for grey scale image processing tasks. The structure of the developed General Parallel Genetic Algorithm (GPGA) {{is based on a}} new subpopulation model which uses an integral load balancing and soft synchronization mechanism. It is designed to lead to good parallelization efficiencies even on distributed workstation clusters with multi-user operating systems. This is especially important for user groups which have no access to massively parallel computers to speed up their algorithms. Run time results from tests on a massively parallel computer and a workstation cluster are shown. R ' ESUM ' E Ce papier pr'esente une modification d'un algorithme g'en'etique standard utilisant des techniques <b>parall</b> `eles. Cet algorithme permet de trouver les solutions optimal [...] ...|$|E
40|$|Systematic parallelization of {{sequential}} programs {{remains a}} major challenge in parallel computing. Traditional approaches using program schemes tend to be narrower in scope, as the properties which enable parallelism are difficult to capture via ad-hoc schemes. In [CTH 98], a systematic approach to parallelization based {{on the notion of}} preserving the context of recursive sub-terms has been proposed. This approach can be used to derive a class of divide-and-conquer algorithms. In this paper, we enhance the methodology by using invariants to guide the parallelization process. The enhancement enables the parallelization of a class of recursive functions with conditional and tupled constructs, which were not possible previously. We further show how such invariants can be discovered and verified systematically, and demonstrate the power of our methodology by deriving a parallel code for maximum segment product. To the best of our knowledge, this is the first systematic <b>parall</b> [...] ...|$|E
40|$|The {{only reason}} to {{parallelize}} a {{program is to}} gain performance. However, the synchronization primitives needed in parallel programs can consume execessive memory bandwidths, can be subject to memory latencies, consume excessive memory, and result in unfair access or even starvation. These problems can overwhelm the performance benefits of parallel execution. Therefore, {{it is necessary to}} understand these performance implications of synchronization primitives in addition to their correctness, liveness, and safety properties. This paper presents a pattern language to assist you in selecting synchronization primitives for parallel programs. This pattern language assumes you have already chosen a locking design, perhaps by using a locking design pattern language [McK 95 b]. 1 Overview A lock-based parallel program uses synchronization primitives to define critical sections of code in which only one CPU at a time may execute concurrently. For example, Figure 1 presents a fragment of <b>parall</b> [...] ...|$|E
40|$|Starting from a {{mathematical}} reinterpretation {{of the classical}} crossover operator, {{a new type of}} crossover is introduced. The proposed new crossover operator gives better performances than the classical 1 point, 2 point or uniform crossover operators. In the paper a theorical investigation of the behaviour of the new crossover is presented. In comparison to the classical crossover operator, it allows a better exploration of the searching space and gives better findings. Some comparative results relative to the optimization of test functions taken from literature are given. Keywords: Convergence, Crossover, Genetic Algorithms. 1 Introduction Genetic Algorithms (GAs) [1 - 3] are a widely used optimization algorithm. They have high insensibility with respect to local minima and very fast starting convergence; they do not require the knowledge of the gradient of the function to optimize; they offer a large amount of large grain parallelism, so they can be efficiently implemented in <b>parall</b> [...] ...|$|E
40|$|A parallel, "across the method" {{implementation}} of a stiff ODE solver is presented. The construction of the methods is outlined and the main implementational issues are discussed. Performance results, using MPI on the IBM SP- 2 are presented and they indicate that a speed-up between 3 and 5 typically can be obtained compared to state-of-the-art sequential codes. AMS subject classification: 65 L 05, 65 L 06, 65 Y 05 Keywords. Stiff ordinary differential equations, Parallel computation, Multi-implicit Runge-Kutta methods, Parallelism across the method 1. Introduction When solving stiff initial value problems the possibilities of using parallel computation "across the method" has often been discussed, [20, 19, 21, 11, 24, 7, 3], but there does however {{seem to be a}} lack of successful implementations. This paper describes the parallel {{implementation of}} a class of stiffly accurate multiimplicit Runge-Kutta methods [...] MIRK methods (not to be confused with monoimplicit RK-methods [10]). The <b>parall</b> [...] ...|$|E
40|$|We present two simpler {{versions}} of the log(n) time parallel recognition of unambiguous context-free languages on CREW PRAM's. The first algorithm combines {{the ideas of the}} algorithms given in [3] and [7]. The result related to that algorithm is an unambiguous decomposition of trees into smaller ones. The analysis of this algorithm is quite simple. The second algorithm is based on unique paths decomposing trees into smaller subtrees. It has a shorter description and it is simpler to explain how it works. However, its correctness is nontrivial. Both algorithms reduce the numbers of processors by a linear factor in the case of general unambiguous cfl's. For linear unambiguous cfl's the number of processors is reduced to O(n 2) by a very simple algorithm. The paper shows how to remove write conflicts in parallel computations on trees by using two kinds of unique decompositions of trees. Finally we show that our results can be generalized for finitely ambiguous cfl's. Keywords: <b>Parall</b> [...] ...|$|E
40|$|Metric {{databases}} are databases where a metric distance {{function is}} defined for pairs of database objects. In such databases, similarity queries {{in the form}} of range queries or k-nearest neighbor queries are the most important queries. In traditional query processing, single queries are issued independently by different users. In many data mining applications, however, the database is typically explored by iteratively asking similarity queries for answers of previous similarity queries. In this paper, we introduce a generic scheme for such data mining algorithms and we develop a method to transform such algorithms {{in a way that they}} can use multiple similarity queries, i. e. sets of queries issued simultaneously. We investigate two orthogonal approaches, reducing I/O cost as well as CPU cost, to speed-up the processing of multiple similarity queries. The proposed techniques apply to any type of similarity query and to an implementation based on an index or using a sequential scan. <b>Parall</b> [...] ...|$|E
40|$|In this paper, we {{consider}} {{the evaluation of the}} memory hierarchy of multiprocessor systems via parallel trace-driven simulation. We study two parallel simulation schemes: a conservative one using an algorithm proposed by Lin et al. [10], whose main characteristic is to insert the shared references from every trace in all other traces, and an optimistic one using a Time Warp-like [9] algorithm. We compare, qualitatively and quantitatively, the major causes of overhead and the overall performance of the two methods. In addition, we discuss the trade-offs in terms of implementation and debugging effort and of application to more general architectural simulation. The optimistic scheme is more complex but, in general, has slightly better performance, is more general, and does not require preprocessing. 3 This work {{was supported in part by}} NSF Grants CCR- 91 - 01541, CCR- 91 - 23308 and CCR- 94 - 01689 1 Introduction The amount of computational cycles needed to simulate the performance of <b>parall</b> [...] ...|$|E
40|$|In {{this paper}} we show a {{progressive}} way for managing abstract algebraic specifications {{in order to}} obtain efficient parallel or distributed algorithms. Our approach is based on TSPP technique (Transformations de Sp cifications de probl mes orient es vers le <b>Parall</b> lisme et le Probabilisme). We claim that the process of transformation of specifications can be done, in many cases, automatically in direction from a high level of abstraction to concrete one using a sequence of derivation steps. The resulting specification, obtained by such a transformation process, is then well suited for execution on target parallel machine. Further we shortly describe the SANDS system (Structured Algebraic Nets Development System) and the TTool - transformation tool package which was developed for performing transformations on specifications. To illustrate the TSPP technique we present here an example developed in SANDS system environment by application of TTool. Finally we discuss the obtained speedup of execution such a derived specification on parallel machine with various number of processors...|$|E
40|$|In {{this paper}} we {{will present a}} massively {{parallel}} SPMD programming model in which the locality of the data is exploited {{in order to obtain}} high execution speeds in real problems. Superlinear speedups are achieved when the mechanisms for the management of the memory hierarchy operate efficiently. A simple model for the prediction of this behavior is introduced and some application examples in matrix algebra and pattern recognition algorithms on the KSR- 1 system are presented. 1. Introduction In the current state of distributed-memory multiprocessor programming technology it is generally assumed that the most efficient way of exploiting large scale parallelism is through the SPMD (Simple Program Multiple Data) programming models. This programming methodology is based on extracting the parallelism associated with the data and will thus not be adequate for certain applications. However, {{there are a variety of}} fields in which the data space is so large and regular that this kind of <b>parall</b> [...] ...|$|E
40|$|In this paper, {{we study}} {{the problem of}} {{efficiently}} executing a parallel program composed of N tasks on an N-node hypercube assuming that ffl communications between tasks are irregular i. e. any pair of tasks may want to communicate at any step of the program; ffl communications between any two tasks occur with a low frequency, i. e. frequency f = O i 1 N log 2 N j. Our probabilistic technique emulates such programs with slowdown O(log log N) with high probability. The problem can be also seen as the problem of emulating a CRCW PRAM program on a distributed memory parallel machine whose interconnection network is the hypercube and our result can be equivalently stated in this model. As {{a part of the}} emulation technique, we develop a Distributed Recombination Algorithm that has independent interest being an efficient way of clustering homogenous information on a hypercube. 1 Introduction The problem of efficiently executing a parallel program on a distributed memory <b>parall</b> [...] ...|$|E
40|$|The {{ability to}} genetically breed {{computer}} programs {{is a relatively}} new approach to problem solving. Recent advances in parallel processing have provided platforms to efficiently carry out this type of computation. In this paper we describe a genetically optimized artificial lifeform. The goal of this work is to breed swarms of artificial organisms that can move over corrupted bitmapped images and correct any errors that are found. We will show that the genetically optimized lifeforms can correct most instances of some forms of errors. In addition, we will show that ability of the organisms to communicate can improve the fitness of an organism. To efficiently carry out this task, an organism exploits two different forms of parallelism. The first form is an implementation specific form of parallelism that makes use of the massively parallel processing paradigm to evaluate an organism's performance in parallel for each generation. The second form is derived from the independent and <b>parall</b> [...] ...|$|E
