0|10000|Public
40|$|In the {{declarative}} <b>programming</b> <b>approach</b> <b>of</b> property models, a dataflow constraint system {{manages the}} behavior of a user interface. The dataflow constraint system captures the user-interface logic as a set of variables and dependencies between those variables. This thesis builds on the prior work that realizes the property models approach as a concrete library for web development called HotDrink. This thesis evaluates the effectiveness <b>of</b> the declarative <b>programming</b> <b>approach</b> <b>of</b> property models, describes the experience of implementing a medium-size web application following the approach, and compares property models with existing web frameworks. A particular focus is on how programming with property models helps programmers to avoid defects related to asynchronous execution of responses to user events...|$|R
40|$|This paper disscuses {{the minimal}} area {{rectangular}} packing {{problem of how}} to pack a set of specified, non-overlapping rectangels into a rectangular container of minimal area. We investigate different mathematical <b>programming</b> <b>approaches</b> <b>of</b> this and introduce a novel approach based on non-linear optimization and the...|$|R
40|$|Abstract. We {{apply the}} logic-based {{declarative}} <b>programming</b> <b>approach</b> <b>of</b> Model Expansion (MX) to a phylogenetic inference task. We axiomatize the task in multi-sorted first-order logic with cardinality constraints. Using the model expansion solver MXG and SAT+cardinality solver MXC, we compare {{the performance of}} several MX axiomatizations on a challenging set of test instances. Our methods perform orders of magnitude faster than previously reported declarative solutions. Our best solution involves polynomial-time pre-processing, redundant axioms, and symmetry-breaking axioms. We also discuss our method of test instance generation, {{and the role of}} pre-processing in declarative programming. Keywords: Phylogeny, Declarative Programming, Model Expansion. ...|$|R
40|$|Abstract. Robust {{optimization}} is one <b>of</b> {{the fundamental}} <b>approaches</b> {{to deal with}} uncertainty in combinatorial optimization. This paper considers the robust span-ning tree problem with interval data, which arises {{in a variety of}} telecommunica-tion applications. It proposes a constraint satisfaction approach using a combina-torial lower bound, a pruning component that removes infeasible and suboptimal edges, as well as a search strategy exploring the most uncertain edges first. The resulting algorithm is shown to produce very dramatic improvements over the mathematical <b>programming</b> <b>approach</b> <b>of</b> Yaman et al. and to enlarge consider-ably the class of problems amenable to effective solutions. ...|$|R
40|$|Abstract. A {{calculus}} {{for developing}} programs from specifications written as predicates that describe {{the relationship between}} the initial and final state is proposed. Such specifications are well known from the specification language Z. All elements of a simple sequential programming notation are defined in terms of predicates. Hence programs form a subset of specifications. In particular, sequential composition is defined by ’demonic composition’, nondeterministic choice by ’demonic disjunction’, and iteration by fixed points. Laws are derived which allow proving equivalence and refinement of specifications and programs. The weakest precondition is expressed by sequential composition. The approach is compared to the predicative <b>programming</b> <b>approach</b> <b>of</b> E. Hehner and to other refinement calculi. ...|$|R
40|$|Robust {{optimization}} is one <b>of</b> {{the fundamental}} <b>approaches</b> {{to deal with}} uncertainty in combinatorial optimization. This paper considers the robust spanning tree problem with interval data, which arises {{in a variety of}} telecommunication applications. It proposes a constraint satisfaction approach using a combinatorial lower bound, a pruning component that removes infeasible and suboptimal edges, as well as a search strategy exploring the most uncertain edges first. The resulting algorithm is shown to produce very dramatic improvements over the mathematical <b>programming</b> <b>approach</b> <b>of</b> Yaman et al. and to enlarge considerably the class of problems amenable to effective solutionsComment: Appears in Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI 2002...|$|R
40|$|In {{this paper}} we {{describe}} a new logic-based stochastic modeling language. It {{is an extension}} of the Bayesian logic <b>programming</b> <b>approach</b> <b>of</b> Kersting and De Raedt [1]. We specialize the Kirsting and De Raedt formalism by suggesting that product distributions are an effective combining rule for Horn clause heads. We use a refinement of Pearl’s loopy belief propagation [2] for the inference algorithm. We also extend the Kirsting and De Raedt language by adding learnable distributions. We propose a message passing algorithm based on Expectation Maximization for estimating the learned parameters in the general case of models built in our system. We have also added some additional utilities to our logic language including second order unification and equality predicates. ...|$|R
40|$|Despite the government’s credit <b>program</b> <b>approach,</b> access <b>of</b> poor {{households}} to microfinancial {{services has}} remained limited. This paper explains the microfinance policy {{environment in the}} Philippines and evaluates the institutional and financial capacity/performance constraints of MFIs. This also addresses four areas that will allow MFIs to be self-sustaining financial institutions for the poor. microfinance, poverty alleviation, microfinance institutions...|$|R
40|$|We {{describe}} a new logic-based stochastic modeling language called Loopy Logic. It {{is an extension}} of the Bayesian logic <b>programming</b> <b>approach</b> <b>of</b> Kersting and De Raedt [2000]. We specialize the Kersting and De Raedt formalism by suggesting that product distributions are an effective combining rule for Horn clause heads. We use a refinement of Pearl’s loopy belief propagation [Pearl, 1998] for the inference algorithm. We also extend the Kersting and De Raedt language by adding learnable distributions. We propose a message passing algorithm based on Expectation Maximization [Dempster et al., 1977] for estimating the learned parameters in the general case of models built in our system. We have also added some additional utilities to our logic language including second order unification and equality predicates...|$|R
40|$|The {{study of}} {{stochastic}} queueing networks is quite important {{due to the}} many applications including transportation, telecommunication, and manufacturing industries. Since there is often no explicit solution to these types of control problems, numerical methods are needed. Following the method of Boué-Dupuis, we use a Dynamic <b>Programming</b> <b>approach</b> <b>of</b> opti-mization on a controlled Markov Chain that simulates {{the behavior of a}} fluid limit of the original process. The search for an optimal control in this case involves a Skorokhod problem to describe the dynamics on the boundary of closed, convex domain. Using relaxed stochastic controls we show that the approximating numerical solution converges to the actual solution as the size of the mesh in the discretized state space goes to zero, and illustrate with an example...|$|R
40|$|In these notes, {{we address}} bounds for error-correcting codes. Our {{approach}} {{is from the}} viewpoint of algebraic graph theory. We therefore begin with a review of the algebraic structure of the Hamming graph, focusing on the binary case. We then derive Delsarte’s linear programming bound and explore some applications of it. In {{the second part of the}} notes, we introduce Terwilliger’s subconstituent algebra and explore its connection to the semidefinite <b>programming</b> <b>approach</b> <b>of</b> Schrijver. Throughout, our focus is on the binary case. The three steps presented here are a summary of the structure of the Terwilliger algebra as presented by Go, a surprising connection to the biweight enumerator of a binary code, and a full characterization of the positive semidefinite cone of the algebra, given by Visentin and Martin...|$|R
40|$|In this paper, we {{formulate}} {{the joint}} Internet gateway allocation, routing, and scheduling problem in wireless ad hoc networks {{with the goal}} of minimizing the average packet delay in a space–time-division multiple-access (STDMA network. We first propose a mathematical <b>programming</b> <b>approach</b> consisting <b>of</b> two steps: the minimization of the weighted hop count (mWHC) subject to scheduling constraints, followed by average delay minimization for the previously computed routes. Since the computational complexity <b>of</b> this <b>approach</b> is prohibitive for larger networks, we also formulate a genetic algorithm (GA) that can be applied to larger networks and mobile networks. We analyze the performance <b>of</b> both <b>approaches</b> by means <b>of</b> simulations and compare the solution that they provide to a simple hop-countbased routing and gateway selection solution. It is shown that the performance of the GA is comparable with the mathematical <b>programming</b> <b>approach</b> in terms <b>of</b> delay and packet delivery ratio (PDR) at lower complexity and is significantly better than the hop-count-based solution...|$|R
40|$|Wind power {{trading in}} pool-based {{electricity}} markets is a decision-making problem and is generally modeled using a multi-stage stochastic <b>programming</b> <b>approach</b> because <b>of</b> the implicit uncertainty of wind input. In any stochastic <b>programming</b> <b>approach,</b> representation <b>of</b> random input {{process is a}} major issue. Due to uncertainty in wind availability, generated power by wind turbines is stochastic and is represented by possible values with corresponding probability of occurrence or scenarios. Accurate representation of uncertainty generally requires the consideration of large number of scenarios, thus necessitating the need for scenario-reduction techniques. This article presents simplified algorithms for wind power scenario generation and reduction. A time series based auto regressive moving average model is used for scenario generation, and probability distance based backward reduction is used for scenario reduction. The algorithms have been implemented for next-day scenario generation of wind farm located at Barnstable, Massachusetts, USA. The results prove {{the ability of the}} proposed algorithms in wind uncertainty modeling. These algorithms can successfully be utilized to generate optimal wind power bids for trading in electricity markets. © 2013 Copyright Taylor and Francis Group, LLC...|$|R
40|$|Abstract. VeriJava {{is a novel}} {{programming}} system, {{which extends}} Java language, {{with just a few}} new language constructs, to support adding contracts to Java. VeriJava consists of an object-oriented programming language called Veri-Java, a compiler for compiling VeriJava programs to Java bytecode, and a static verifier for assuring that the VeriJava code is consistent with its specifications. This paper discusses the goal and the overall <b>programming</b> <b>approach</b> <b>of</b> VeriJava. The paper also gives examples of VeriJava programs. On one hand, VeriJava provides a way with assertions (pre- and postconditions, and class invariants), to support runtime checking such as debugging and testing. On the other hand, VeriJava also offers the possibility of automatic compile-time verifications such as checking the code of a method against its specification, checking that the specification of a subclass is compatible with the specification of its superclass. ...|$|R
40|$|Abstract. It {{has been}} 5 years that our {{department}} set DSP as a bilingual model course. However, both our teaching {{team and the}} students majoring the course encountered many difficulties during this period. To motivate students to overcome language and obscure concept problem, we adopted a graphical <b>programming</b> <b>approach</b> instead <b>of</b> the traditional and commonly used text-based <b>programming</b> <b>approach</b> in our DSP teaching process, and achieved good feedback from students. In this paper, some considerations and strategies for the combining LabVIEW with bilingual teaching are proposed. Moreover, two interesting combination examples are presented to make description understandable...|$|R
40|$|Probabilistic {{software}} analysis aims at quantifying {{the probability of}} a target event to occur during a program execution, given a probabilistic usage profile. The probability distributions over the input values specified by a profile can be used to quantify the probability of satisfying a branch condition (and its complement) of sequential programs. On the other hand, the behavior of multithreaded programs is affected not only by users inputs, but also by the choices of the scheduler assigning the CPU to each of the ready threads. A problem for multithreaded programs is to identify a scheduler that minimizes or maximizes the probability of the target event to occur. In [4] an <b>approach</b> reminiscent <b>of</b> dynamic <b>programming</b> is used for this purpose. The goal of this seminar is to replace the dynamic <b>programming</b> <b>approach</b> <b>of</b> [4] with an ant-optimization strategy, able to find a (near-) optimal scheduler via a partial exploration of the execution tree...|$|R
40|$|A {{calculus}} {{for developing}} programs from specifications written as predicates that describe {{the relationship between}} the initial and final state is proposed. Such specifications are well known from the specification language Z. All elements of a simple sequential programming notation are defined in terms of predicates. Hence programs form a subset of specifications. In particular, sequential composition is defined by 'demonic composition', nondeterministic choice by 'demonic disjunction', and iteration by fixed points. Laws are derived which allow proving equivalence and refinement of specifications and programs. The weakest precondition is expressed by sequential composition. The approach is compared to the predicative <b>programming</b> <b>approach</b> <b>of</b> E. Hehner and to other refinement calculi. 1 Introduction We view a specification as a predicate which describes the admissible final state of a computing machine with respect to some initial state. A program is a predicate restricted to operat [...] ...|$|R
40|$|We {{developed}} a dynamic <b>programming</b> <b>approach</b> <b>of</b> computing common sequence structure patterns among two RNAs given their primary sequences and their secondary structures. Common patterns between two RNAs are defined {{to share the}} same local sequential and structural properties. The locality {{is based on the}} connections of nucleotides given by their phosphodiester and hydrogen bonds. The idea of interpreting secondary structures as chains of structure elements leads us to develop an efficient dynamic <b>programming</b> <b>approach</b> in time O(nm) and space O(nm), where n and m are the lengths of the RNAs. The biological motivation is given by detecting common, local regions of RNAs, although they do not necessarily share global sequential and structural properties. This might happen if RNAs fold into different structures but share a lot of local, stable regions. Here, we illustrate our algorithm on Hepatitis C virus internal ribosome entry sites. Our method is useful for detecting and describing local motifs as well. An implementation in C++ is available and can be obtained by contacting one of the authors. Key words: local patterns, similar RNAs, related RNAs, sequence structure, RNA motifs...|$|R
25|$|Möbius ladders {{have also}} been used in {{computer}} science, as part <b>of</b> integer <b>programming</b> <b>approaches</b> to problems <b>of</b> set packing and linear ordering. Certain configurations within these problems {{can be used to}} define facets of the polytope describing a linear programming relaxation of the problem; these facets are called Möbius ladder constraints.|$|R
40|$|This {{paper is}} devoted to the study of {{algorithms}} for sequential optimization of approximate inhibitory rules relative to the length, coverage and number of misclassifications. Theses algorithms are based on extensions <b>of</b> dynamic <b>programming</b> <b>approach.</b> The results <b>of</b> experiments for decision tables from UCI Machine Learning Repository are discussed. © 2013 Springer-Verlag...|$|R
50|$|Möbius ladders {{have also}} been used in {{computer}} science, as part <b>of</b> integer <b>programming</b> <b>approaches</b> to problems <b>of</b> set packing and linear ordering. Certain configurations within these problems {{can be used to}} define facets of the polytope describing a linear programming relaxation of the problem; these facets are called Möbius ladder constraints.|$|R
40|$|AbstractAn (n, d) {{set in the}} {{projective}} geometry PG(r, q) {{is a set of}} n points, no d of which are dependent. The packing problem is that of finding n(r, q, d), the largest size of an (n, d) set in PG(r, q). The packing problem for PG(r, 3) is considered. All of the values of n(r, 3, d) for r ⩽ 5 are known. New results for r = 6 are n(6, 3, 5) = 14 and 20 ⩽ n(6, 3, 4) ⩽ 31. In general, upper bounds on n(r, q, d) are determined using a slightly improved sphere-packing bound, the linear <b>programming</b> <b>approach</b> <b>of</b> coding theory, and an orthogonal (n, d) set with the known extremal values of n(r, q, d) —values when r and d are close to each other. The BCH constructions and computer searches are used to give lower bounds. The current situation for the packing problem for PG(r, 3) with r ⩽ 15 is summarized in a final table...|$|R
40|$|Abstract. In {{this paper}} we discuss a visual {{programming}} environment for design and rapid prototyping of web-based applications, securely connected to remote Location-Based Services. The visual <b>programming</b> <b>approach</b> <b>of</b> this research is based on computation as data transformation within a dataflow, and on visual composition of web services. The VisPro environment uses a very simple approach to service composition: (a) the developer takes a set of web widgets from a library, (b) builds interactively a user interface by drag and drop, (c) builds the application logic of the web service by drawing the connections between boxes (standing for suitable data transformations) and widgets (standing for user interaction). The development session produces, in presentation mode, a web page where the user may trigger, and interact with, the novel data mining and related computation. A successful GUI (and logic) is abstracted as a new service, characterized by a new widget, and stored in the widget library...|$|R
40|$|The {{rectilinear}} Steiner Tree problem {{asks for}} a shortest tree connecting given points in the plane with rectilinear distance. The best theoretically analysed algorithms for this problem base on dynamic programming and have a running time of O(n 2 Δ 2 : 62 n) (Ganley/Cohoon) resp. n O(p n) (Smith). The first algorithm can solve problems of size 27, the second one is highly impractical {{because of the large}} constant in the exponent. The best implementations perform poorly even on small problem instances; the best practical results can be reached using a Branch & Bound approach (Salowe/Warme); this implementation can solve random problems of size 35 within a day, while the dynamic <b>programming</b> <b>approach</b> <b>of</b> Ganley/Cohoon can handle only 27 points examples. In this paper we improve the theoretical worst-case time bound to O(n 2 Δ 2 : 38 n), for random problem instances we prove a running time of ff n with a constant ff ! 2. We implemented our algorithms and can now solve [...] ...|$|R
40|$|There is {{a number}} of OODB {{optimization}} techniques proposed recently, such as the translation of path expressions into joins and query unnesting, that may generate {{a large number of}} implicit joins even for simple queries. Unfortunately, most current commercial query optimizers are still based on the dynamic <b>programming</b> <b>approach</b> <b>of</b> System R, and cannot handle queries of more than ten tables. There {{is a number}} of recent proposals that advocate the use of combinatorial optimization techniques, such as iterative improvement and simulated annealing, to deal with the complexity of this problem. These techniques, though, fail {{to take advantage of the}} rich semantic information inherent in the query specification, such as the information available in query graphs, which gives a good handle to choose which relations to join each time. This paper presents a polynomial-time algorithm that generates a good quality order of relational joins. It can also be used with minor modifications to sort OODB a [...] ...|$|R
40|$|Screening is {{a helpful}} process of {{multiple}} criteria decision aid (MCDA) to reduce a larger set of alternatives into a smaller one containing the best alternatives; thereby, decision makers {{are able to}} concentrate on evaluating alternatives within a smaller set. Therefore, determining how to assist decision makers in screening {{is an important issue}} for MCDA. This study proposes an extended case-based distance approach incorporating the advantages of a case-based distance method, a mixed-integer <b>programming</b> <b>approach</b> <b>of</b> discriminant analysis, and a multidimensional scaling technique to help decision makers screen alternatives visually in MCDA. The proposed approach can screen alternatives by evaluating sets of cases selected by decision makers, providing visual aids to observe decision context, reducing the number of misclassifications, and improving multiple solution problems. An interactive screening procedure is also developed to provide flexibility so that decision makers can check and adjust screening results iteratively. Decision making Multi-criteria Screening Case-based reasoning Visualization...|$|R
40|$|Ongoing {{empirical}} {{research on the}} drivers <b>of</b> <b>project-aid</b> effectiveness relies on World Bank evaluation ratings across heterogeneous aid sectors. This leads to two problems. First, {{it is difficult to}} identify which dimension of project performance World Bank evaluation ratings are measuring precisely. Second, only project management variables, which are sector independent, can be included in the analysis. This study concentrating on an analysis of 150 water supply projects enables us to work with a more precise and objective performance measure by defining sector-specific indicators of improved water supply. Moreover, we are able to analyze the impact of project design in addition to project management. We find that evaluation ratings and indicators of improved water supply are positively but weakly correlated. Project management variables have a higher impact on evaluation ratings whereas project design variables have a higher impact on improving water supply to the target group. Various independent variables even change sign if indicators of improved water supply instead of evaluation ratings are chosen as a performance measure <b>of</b> <b>project-aid</b> effectiveness. Taking into account project design in addition to project management and country characteristics considerably increases the share of variation in project performance that can be explained...|$|R
40|$|This {{qualitative}} {{case study}} is exploratory in nature. The {{purpose of the study}} is to look at higher education organizational change processes related to accountability and the implementation of an assessment management system. Specifically, the study focuses on how one teacher education <b>program</b> <b>approaches</b> the process <b>of</b> implementing an assessment management system through the lens of effective organizational change...|$|R
40|$|The {{paper is}} {{concerned}} with the problem of automatic detection and correction of errors into massive data sets. As customary, erroneous data records are detected by formulating a set of rules. Such rules are here encoded into linear inequalities. This allows to check the set of rules for inconsistencies and redundancies by using a polyhedral mathematics approach. Moreover, it allows to correct erroneous data records by introducing the minimum changes through an integer linear <b>programming</b> <b>approach.</b> Results <b>of</b> a particularization of the proposed procedure to a real-world case of census data correction are reported...|$|R
40|$|AbstractWe study {{integrated}} prefetching and caching {{in single}} and parallel disk systems. In {{the first part}} of the paper we investigate approximation algorithms for the single disk problem. There exist two very popular approximation algorithms called Aggressive and Conservative for minimizing the total elapsed time. We give a refined analysis of the Aggressive algorithm, improving the original analysis by Cao et al. We prove that our new bound is tight. Additionally we present a new family of prefetching and caching strategies and give algorithms that perform better than Aggressive and Conservative. In the second part of the paper we investigate the problem of minimizing stall time in parallel disk systems. We present a polynomial time algorithm for computing a prefetching/caching schedule whose stall time is bounded by that of an optimal solution. The schedule uses at most 2 (D- 1) extra memory locations in cache. This is the first polynomial time algorithm that, using a small amount of extra resources, computes schedules whose stall times are bounded by that of optimal schedules not using extra resources. Our algorithm is based on the linear <b>programming</b> <b>approach</b> <b>of</b> [Journal of the ACM, 47 (2000) 969]. However, in order to achieve minimum stall times, we introduce the new concept of synchronized schedules in which fetches on the D disks are performed completely in parallel...|$|R
40|$|Abstract — This paper {{concerns}} with portfolio problem with logarithmic utility which is maximizing the expected {{utility of the}} terminal wealth. The stock price is modeled as a stochastic differential equation whose coefficients evolve according to a correlated diffusion factor. Using dynamic <b>programming</b> <b>approach,</b> explicit representations <b>of</b> the value function and corresponding optimal strategies are derived. Index Terms—Hamilton-Jacobi-Bellman equation, utility maximization, stochastic factor, logarithmic utility I...|$|R
40|$|Coronary {{heart disease}} {{has been proved}} to be {{associated}} with a "high-risk" diet and with elevated blood cholesterol levels. The National Cholesterol Education Program has embarked on a campaign based on intensive medical treatment of 60 million Americans with high blood cholesterol levels, but the degree of benefit of dietary change or pharmaceutical intervention or both to reduce blood cholesterol values remains a subject of disagreement within the scientific community. Evidence from comparative international studies suggests that to lower coronary heart disease mortality substantially, dietary alterations and general societal changes must be greater than those possible under the National Cholesterol Education <b>Program's</b> <b>approach</b> <b>of</b> physician-centered patient counseling. The nation's priority to prevent coronary heart disease should be a public policy <b>approach,</b> the goal <b>of</b> which is to reduce for the entire population all coronary disease risk factors. In the dietary area, three proposals to reduce the availability of atherogenic foods are the use of warning labels on atherogenic foods, the prohibition of advertising for such high-risk foods, and the imposition of an excise tax on the same foods. We must confront the "TV-auto-supermarket society" that underlies our nation's high rate of coronary heart disease...|$|R
40|$|International audienceDevelopment of {{user-friendly}} {{and flexible}} scientific programs {{is a key}} to their usage, extension and maintenance. This paper presents an OOP (Object-Oriented <b>Programming)</b> <b>approach</b> for design <b>of</b> finite element analysis programs. General organization of the developed software system, called FER/SubDomain, is given which includes the solver and the pre/post processors with a friendly GUI (Graphical User Interfaces). A case study with graphical representations illustrates some functionalities of the program...|$|R
40|$|Available in IEEE Xplore digital library. The {{problem of}} writing machine-independent APL interpreters is solved by means <b>of</b> a systems <b>programming</b> <b>approach</b> making use <b>of</b> an {{intermediate}} level language specially designed for that purpose. This paper describes the language, {{as well as}} the procedure used to build universal interpreters. Three compilers that translate this language for three different machines have been written so far, and an APL interpreter has been finishe...|$|R
40|$|Summary: The {{capacity}} development {{approach was}} proposed by UNDP and European donors {{as a new}} approach based upon the African aid failure in the 1980 s {{and the end of}} the Cold War instead <b>of</b> conventional aid <b>approach.</b> In this paper, capacity development approach comes into collision with institutional studies in social sciences on purpose to accelerate knowledge evolution. This paper presents a new perspective on the development process and on aid policy. It is named an <b>approach</b> <b>of</b> “capacity development and institutional change”. By using this new <b>approach</b> <b>of</b> “capacity develop-ment and institutional change”, capacity development is able to cover not only technical cooperation but also lending matters. Moreover, the <b>program</b> <b>approach</b> is realized into development strategy and aid policy. The <b>program</b> <b>approach</b> indicates criteria <b>of</b> selectivity and priority of allocation of development resources including aid resources. <b>Program</b> <b>approach</b> concentrates more on the policy making process or on the top down (upstream) approach. Furthermore, this paper shows the importance of field experiences, meaning the advantages of Japanese aid compared to European aid, especially with regards to making the <b>program</b> <b>approach</b> more e#ective. The micro (field experience) and macro (top down) loop is a critical factor for aid e#ectiveness. 1...|$|R
50|$|STU {{made great}} efforts {{to carry out}} the Advanced Undergraduate Education reform that mainly {{includes}} “Integrative Thinking” training, integrated education model, adaptable academic programm ing, as well as evaluation and continuous improvement of teaching quality. It has developed a series of featured teaching models such as Credit System of International Standard, English Enhancement <b>Program,</b> CDIO <b>approach</b> <b>of</b> engineering education and system-based integrated medical curriculum. In quest of a unique educational model, STU made tremendous efforts to integrate such educational aspects as community service, liberal arts and sportsmanship into its holistic approach to talent training.|$|R
