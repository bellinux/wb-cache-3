283|10000|Public
25|$|Particle swarm {{optimization}} {{is another}} algorithm widely used {{to solve problems}} related to swarms. It was developed in 1995 by Kennedy and Eberhart and was first aimed at simulating the social behaviour and choreography of bird flocks and fish schools. The algorithm was simplified and it was observed to be performing optimization. The system initially seeds a population with random solutions. It then searches in the problem space through successive generations using stochastic optimization {{to find the best}} solutions. The solutions it finds are called particles. Each particle stores its position as well as the best solution it has achieved so far. The <b>particle</b> <b>swarm</b> <b>optimizer</b> tracks the best local value obtained so far by any particle in the local neighbourhood. The remaining particles then move through the problem space following the lead of the optimum particles. At each time iteration, the particle swarm optimiser accelerates each particle toward its optimum locations according to simple mathematical rules. Particle swarm optimization has been applied in many areas. It has few parameters to adjust, and a version that works well for a specific applications can also work well with minor modifications across a range of related applications. A book by Kennedy and Eberhart describes some philosophical aspects of particle swarm optimization applications and swarm intelligence. An extensive survey of applications is made by Poli.|$|E
40|$|The <b>Particle</b> <b>Swarm</b> <b>Optimizer,</b> {{like many}} other {{evolutionary}} and classical minimization methods, suffers the problem of occasional convergence to local minima, especially in multimodal and scattered landscapes. In this work we propose a modification of the <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> that makes use of a new technique, named Function "Stretching", to alleviate the local minima problem. Function "Stretching" consists of a two [...] stage transformation of the objective function that eliminates local minima, while preserving global ones. Experiments indicate that the <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> equipped with the "Stretching" technique exhibits good performance and results in finding global minima reliably and predictably. ...|$|E
30|$|Solving {{large-scale}} global optimization using improved <b>particle</b> <b>swarm</b> <b>optimizer</b> (EPUS-PSO) [52].|$|E
30|$|Multi-objective <b>particle</b> <b>swarm</b> <b>optimizers</b> is a meta-heuristic {{stemmed from}} PSO method of optimization. The {{difference}} between the two methods is in detection of the best position of the particle and particle local memory [6].|$|R
40|$|Abstract — In {{this paper}} we propose a new method, called {{multiple}} <b>particle</b> <b>swarm</b> <b>optimizers</b> with diversive curiosity (MPSOα/DC), for improving the search performance of the convenient multiple <b>particle</b> <b>swarm</b> <b>optimizers.</b> It has three outstanding features: (1) Implementing plural <b>particle</b> <b>swarms</b> simultaneously to search; (2) Exploring the most suitable solution in a small limited space by a localized random search for correcting the solution found by each particle swarm; (3) Introducing diversive curiosity into the whole <b>particle</b> <b>swarms</b> to comprehensively deal with premature convergence and stagnation. To demonstrate the effectiveness of the proposed method, computer experiments on a suite of benchmark problems are carried out. We investigate the characteristics of the proposed method, and compare the search performance with other methods such as EPSO, OPSO, and RGA/E. The experimental results indicate that the search performance of MPSOα/DC is superior to EPSO, OPSO, and RGA/E for the given benchmark problems...|$|R
40|$|This paper {{presents}} {{a method to}} employ <b>particle</b> <b>swarms</b> <b>optimizers</b> in a cooperative configuration. This is achieved by splitting the input vector into several sub-vectors, each which is optimized cooperatively in its own swarm. The application of this technique to neural network training is investigated, with promising results. Keywords: <b>Particle</b> <b>swarms,</b> cooperative learning, optimization Computing Review Categories: G. 1. 6, I. 2. 6...|$|R
3000|$|Self-organizing {{hierarchical}} <b>particle</b> <b>swarm</b> <b>optimizer</b> with time-varying acceleration coefficients (HPSO-TVAC) [66], [...]...|$|E
40|$|Abstract — This paper {{presents}} a dynamic {{particle swarm optimization}} based search for optimal fusion configuration of sensors in distributed detection network in presence of a nonstationary binary symmetric channel. The wireless channel in sensor networks is a non-stationary random process, which moves the optima of the original problem, otherwise static. The optimal fusion configuration minimizes the probability of error and involves optimal setting of the decision thresholds for the sensors and the optimal fusion rule used at the fusion center. The optimal decision thresholding {{of a group of}} sensors in a distributed detection network is a proven intractable problem. Previously a <b>particle</b> <b>swarm</b> <b>optimizer</b> has been used to solve this problem. In this paper a dynamic <b>particle</b> <b>swarm</b> <b>optimizer</b> is used to evolve the optimal decision thresholds of the group of sensors in presence of a non-stationary channel. Performance of the dynamic <b>particle</b> <b>swarm</b> <b>optimizer</b> is compared to the simple <b>particle</b> <b>swarm</b> <b>optimizer</b> in which the particle swarm optimization is restarted after detection of a change. The results demonstrate the effectiveness of the particle swarm in achieving the optima in highly dynamic environments. Dynamic adaptation in the particle swarms towards changes in fitness landscape makes particle swarm optimization an ideal algorithm for multi objective optimization in non-stationary sensor networks. I...|$|E
40|$|This is {{the author}} {{accepted}} manuscript. This article explores {{the application of a}} wind farm layout optimization framework using a <b>particle</b> <b>swarm</b> <b>optimizer</b> to three benchmark test cases. The developed framework introduces an increased level of detail characterizing the impact that the wind farm layout can have on the levelized cost of energy by modelling the wind farm’s electrical infrastructure, annual energy production, and cost as functions of the wind farm layout. Using this framework, this paper explores the application of a <b>particle</b> <b>swarm</b> <b>optimizer</b> to the wind farm layout optimization problem considering three different levels of wind farm constraint faced by modern wind farm developers. The <b>particle</b> <b>swarm</b> <b>optimizer</b> is found to yield improvements in the layout with respect to the levelized cost of energy for the three benchmark cases when compared to two past studies. This highlights both applicability of the <b>particle</b> <b>swarm</b> <b>optimizer</b> to the problem {{and the ways in which}} a wind farm developer could make use of the present framework in the development and design of future wind farms. This work is funded in part by the Energy Technologies Institute (ETI) and RCUK energy program for IDCORE (EP/J 500847 / 1) and supported by EDF Energy R&D UK Centre...|$|E
40|$|This paper {{presents}} {{a method to}} employ <b>particle</b> <b>swarms</b> <b>optimizers</b> in a cooperative configuration. This is achieved by splitting the input vector into several sub-vectors, each which is optimized cooperatively in its own swarm. The application of this technique to neural network training is investigated, with promising results. Keywords: <b>Particle</b> <b>swarms,</b> cooperative learning, optimization Computing Review Categories: G. 1. 6, I. 2. 6 1 Introduction <b>Particle</b> <b>Swarm</b> <b>Optimizers</b> (PSOs) have previously been used to train neural networks[6, 10] and generally met with success. The advantage of the PSO over {{many of the other}} optimization algorithms is its relative simplicity. This paper aims to improve the performance of the basic PSO by partitioning the input vector into several subvectors. Each sub-vector is then allocated its own swarm. In Section 2, a brief overview of PSOs is presented, followed by a discussion of how cooperative behavior can be implemented through a splitting technique i [...] ...|$|R
40|$|This paper {{shows that}} a novel hybrid algorithm, Breeding Swarms, {{performs}} equal to, or better than, Genetic Algorithms and <b>Particle</b> <b>Swarm</b> <b>Optimizers</b> when training recurrent neural networks. The algorithm {{was found to be}} robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and <b>Particle</b> <b>Swarm</b> Optimization in 79 of 80 tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks. Categories and Subject Descriptor...|$|R
40|$|Abstract. — In this paper, the {{performances}} of the quasi-Newton BFGS algorithm, the NEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), the Differential Evolution (DE) algorithm and <b>Particle</b> <b>Swarm</b> <b>Optimizers</b> (PSO) are compared experimentally on benchmark functions reflecting important challenges encountered in real-world optimization problems. Dependence of {{the performances}} in the conditioning {{of the problem and}} rotational invariance of the algorithms are in particular investigated. ...|$|R
30|$|As a {{extension}} of their main work, they proposed a query based method and employed a <b>Particle</b> <b>Swarm</b> <b>optimizer</b> to attack classification model {{in a black}} box model.|$|E
40|$|A {{based on}} Rapidly-exploring Random Tree(RRT) and <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> (PSO) for path {{planning}} of the robot is proposed. First the grid method is built {{to describe the}} working space of the mobile robot,then the Rapidly-exploring Random Tree algorithm is used to obtain the global navigation path,and the <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> algorithm is adopted to get the better path. Computer experiment results demonstrate that this novel algorithm can plan an optimal path rapidly in a cluttered environment. The successful obstacle avoidance is achieved,and the model is robust and performs reliably. </p...|$|E
40|$|In this paper, {{new models}} and {{algorithms}} {{are used for}} control and optimization of a class in Hierarchical Heterogeneous Wireless Network, under real-world physical constraints. Flocking Algorithm and a <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> are implemented in this paper. We first develop a non-convex mathematical model for Hierarchical Heterogeneous Wireless Networks. Second, we propose a new Flocking algorithm for self-organization {{and control of the}} backbone nodes in this network by collecting local information from end users. Third, we employ <b>Particle</b> <b>Swarm</b> <b>Optimizer,</b> a widely used artificial intelligence algorithm, to directly optimize the Hierarchical Heterogeneous Wireless Networks by collecting global information from the entire system. A comprehensive evaluation measurement during the optimization process is developed. In addition, the relationship between network and Flocking Algorithm, Our novel framework is examined in various dynamic scenarios. Experimental results demonstrate that Flocking algorithm and <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> both outperform current algorithms for the self-organization and optimization of Hierarchical Heterogeneous Wireless Networks...|$|E
40|$|In this paper, the {{performances}} of the quasi-Newton BFGS algorithm, the NEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), the Differential Evolution (DE) algorithm and <b>Particle</b> <b>Swarm</b> <b>Optimizers</b> (PSO) are compared experimentally on benchmark functions reflecting important challenges encountered in real-world optimization problems. Dependence of {{the performances}} in the conditioning {{of the problem and}} rotational invariance of the algorithms are in particular investigated. Comment: 8 th International Symposium on Experimental Algorithms, Dortmund : Germany (2009...|$|R
40|$|PSO (ARPSO) are {{methods for}} {{artificial}} injection of diversity into <b>particle</b> <b>swarm</b> <b>optimizers</b> that {{are intended to}} encourage converged swarms to engage in exploration. While simple to implement, effective when tuned correctly, and benefiting from intuitive appeal, SEPSO behavior can be improved by adapting its radius and bounce parameters in response to collisions. In fact, adaptation can allow SEPSO to compete with and outperform ARPSO. The adaptation strategies presented here are simple to implement, easy to tune, and retain SEPSO’s intuitive appeal...|$|R
40|$|Spatial Extension PSO (SEPSO) and Attractive-Repulsive PSO (ARPSO) are {{methods for}} {{artificial}} injection of diversity into <b>particle</b> <b>swarm</b> <b>optimizers</b> that {{are intended to}} encourage converged swarms to engage in exploration. While simple to implement, effective when tuned correctly, and benefiting from intuitive appeal, SEPSO behavior can be improved by adapting its radius and bounce parameters in response to collisions. In fact, adaptation can allow SEPSO to compete with and outperform ARPSO. The adaptation strategies presented here are simple to implement, easy to tune, and retain SEPSO’s intuitive appeal...|$|R
40|$|The barebones {{differential}} evolution (BBDE) is a new, almost parameter-free {{optimization algorithm}} {{that is a}} hybrid of the barebones <b>particle</b> <b>swarm</b> <b>optimizer</b> and differential evolution. Differential evolution is used to mutate, for each particle, the attractor associated with that particle, defined as a weighted average of its personal and neighborhood best positions. The performance of the proposed approach is investigated and compared with differential evolution, a Von Neumann <b>particle</b> <b>swarm</b> <b>optimizer</b> and a barebones <b>particle</b> <b>swarm</b> <b>optimizer.</b> The experiments conducted show that the BBDE provides excellent results with the added advantage of little, almost no parameter tuning. Moreover, {{the performance of the}} barebones differential evolution using the ring and Von Neumann neighborhood topologies is investigated. Finally, the application of the BBDE to the real-world problem of unsupervised image classification is investigated. Experimental results show that the proposed approach performs very well compared to other state-of-the-art clustering algorithms in all measured criteria. Evolutionary computation Differential evolution Particle swarm optimization Optimization...|$|E
30|$|Another {{factor is}} inertia weight, which is {{introduced}} by Shi and Eberhart (1998). If {{you are interested}} in it, please refer to their paper in 1998. (Title: A modified <b>particle</b> <b>swarm</b> <b>optimizer).</b>|$|E
40|$|In {{this paper}} a new {{effective}} optimization algorithm called hybrid <b>particle</b> <b>swarm</b> <b>optimizer</b> with breeding and subpopulation is presented. This algorithm is essentially, as PSO and GA, a population-based heuristic search technique, now in {{use for the}} optimization of electromagnetic structures, modeled on the concepts of natural selection and evolution (GA) but also based on cultural and social rules derived from {{the analysis of the}} swarm intelligence and from the interaction among particles (PSO). The optimized design of multibeam antennas arrays is reported with numerical results. General Terms Hybrid model, global approach, genetic algorithm, <b>particle</b> <b>swarm</b> <b>optimizer,</b> multibeam pattern, antenna arrays...|$|E
40|$|Abstract- Biomechanics is {{a science}} of {{examining}} {{the internal and external}} forces on the human body. In biomechanics, forward dynamics simulation models can be used to study optimal control of the human muscu-loskeletal system. An example application is the soc-cer kick, where the optimal recruitment pattern of the muscles of the lower extremity can result in a high speed shot on net. We used evolutionary algorithms— <b>particle</b> <b>swarm</b> <b>optimizers</b> and evolution strategies—to adjust muscle control parameters for a soccer kick. In this paper we describe our implementation of the soccer kick project, optimization experiments performed with the soccer kick and comparing the results from <b>Particle</b> <b>Swarm</b> Optimization and Evolution Strategies. ...|$|R
40|$|This article {{presents}} statistical techniques {{for the design}} and analysis of evolution strategies. These techniques {{can be applied to}} other search heuristics such as genetic algorithms, simulated annealing or <b>particle</b> <b>swarm</b> <b>optimizers.</b> It provides guidelines for the comparison of di#erent algorithms on artifical test functions and on real-world optimization problems. Statistical experimental design techniques to improve the integrity and comparability of experiments are proposed. Interpreting the run of an optimization algorithm as an experiment, design of experiments (DOE), response surface methods (RSM), and tree-based regression methods can be applied to analyze and to improve its performance...|$|R
40|$|Abstract. <b>Particle</b> <b>Swarm</b> Optimization (PSO) has {{received}} increased {{attention in the}} optimization research community since its first appearance. Regarding multi-objective optimization, {{a considerable number of}} algorithms based on Multi-Objective <b>Particle</b> <b>Swarm</b> <b>Optimizers</b> (MOP-SOs) {{can be found in the}} specialized literature. Unfortunately, no experimental comparisons have been made in order to clarify which version of MOPSO shows the best performance. In this paper, we use a benchmark composed of three well-known problem families (ZDT, DTLZ, and WFG) with the aim of analyzing the search capabilities of six representative state-of-the-art MOPSOs, namely, NSPSO, SigmaMOPSO, OMOPSO, AMOPSO, MOPSOpd, and CLMOPSO. We additionally propose a new MOPSO algorithm, called SMPSO, characterized by including a velocity constraint mechanism, obtaining promising results where the rest perform inadequately...|$|R
40|$|Although the {{original}} <b>particle</b> <b>swarm</b> <b>optimizer</b> (PSO) method and its related variant methods show some effectiveness for solving optimization problems, it may easily get trapped into local optimum especially when solving complex multimodal problems. Aiming {{to solve this}} issue, this paper puts forward a novel method called parallel and cooperative <b>particle</b> <b>swarm</b> <b>optimizer</b> (PCPSO). In case that the interacting of the elements in D-dimensional function vector 				X=[x 1,x 2,…,xd,…,xD] is independent, cooperative <b>particle</b> <b>swarm</b> <b>optimizer</b> (CPSO) is used. Based on this, the PCPSO is presented to solve real problems. Since the dimension cannot be split into several lower dimensional search spaces in real problems because of the interacting of the elements, PCPSO exploits the cooperation of two parallel CPSO algorithms by orthogonal experimental design (OED) learning. Firstly, the CPSO algorithm is used to generate two locally optimal vectors separately; then the OED is used to learn the merits of these two vectors and creates a better combination of them to generate further search. Experimental studies {{on a set of}} test functions show that PCPSO exhibits better robustness and converges much closer to the global optimum than several other peer algorithms...|$|E
40|$|Particle swarm {{inversion}} of large neural networks is a com-putationally intensive process. By the implementing a modified <b>particle</b> <b>swarm</b> <b>optimizer</b> and neural network in reconfigurable hardware, {{many of the}} computations can be preformed simultaneously, significantly reducing computa-tion time compared to a conventional computer. 1...|$|E
40|$|Abstract—For random {{high density}} {{distribution}} in {{wireless sensor networks}} in this article have serious redundancy problems. In order to maximize the cost savings network resources for wireless sensor networks, extend the life network, this paper proposed a algorithm for the minimal k-covering set based on <b>particle</b> <b>swarm</b> <b>optimizer.</b> Firstly, the network monitoring area is divided {{into a number of}} grid points. Utilization rate and the node minimum are used as optional objective, and a combinatorial optimization mathematical model is established. Then using <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> to solve optimization model, thus the optimal network coverage and the utilization od sensor nodes are obtained. Simulation results that algorithm has reduced node redundancy and the energy consumption, and improved the network coverage effectively...|$|E
40|$|Abstract. This paper {{considers}} {{the use of}} randomly generated directed graphs as neighborhoods for <b>particle</b> <b>swarm</b> <b>optimizers</b> (PSO) using fully informed particles (FIPS), together with dynamic changes to the graph during an algorithm run as a diversity-preserving measure. Different graph sizes, constructed with a uniform out-degree were studied {{with regard to their}} effect on the performance of the PSO on optimization problems. Comparisons were made with a static random method, as well as with several canonical PSO and FIPS methods. The results indicate that under appropriate parameter settings, the use of random directed graphs with a probabilistic disruptive re-structuring of the graph produces the best results on the test functions considered. ...|$|R
40|$|Abstract: This paper {{presents}} an efficient and reliable swarm intelligence-based approach, namely elitist-mutated <b>particle</b> <b>swarm</b> optimization �EMPSO � technique, to derive reservoir operation policies for multipurpose reservoir systems. <b>Particle</b> <b>swarm</b> <b>optimizers</b> are inherently distributed algorithms, {{in which the}} solution for a problem emerges from the interactions between many simple individuals called particles. In this study the standard <b>particle</b> <b>swarm</b> optimization �PSO � algorithm is further improved by incorporating a new strategic mechanism called elitist-mutation to improve its performance. The proposed approach is first tested on a hypothetical multireservoir system, used by earlier researchers. EMPSO showed promising results, when compared with other techniques. To show practical utility, EMPSO is then applied to a realistic case study, the Bhadra reservoir system in India, which serves multiple purposes, namely irrigation and hydropower generation. To handle multiple objectives of the problem, a weighted approach is adopted. The results obtained demonstrate that EMPSO is consistently performing better than the standard PSO and genetic algorithm techniques. It is see...|$|R
40|$|Abstract — In {{this paper}} we propose a newly {{multiple}} <b>particle</b> <b>swarm</b> <b>optimizers</b> with inertia weight with diversive curiosity (MPSOIWα/DC) for improving the search performance and intelligent processing of a plain MPSOIW. It has the following outstanding features: (1) Decentralization in multi-swarm exploration with hybrid search (MPSOIWα), (2) Concentration in evaluation and behavior control with diversive curiosity (DC), and (3) Their effective combination. For inspecting the effectiveness of the proposal, computer experiments on a suite of 5 -dimensional benchmark problems are carried out. We examine its intrinsic characteristics, and compare the search ability with other methods. The obtained results indicate that the search performance of the MPSOIWα/DC is superior to the PSOIW/DC, EPSOIW, PSOIW, OPSO, and RGA/E for the given problems...|$|R
40|$|Premature {{convergence}} {{has been}} {{recognized as one of}} the major drawbacks of particle swarm optimization (PSO) algorithms. In particular, the lack of diversity in PSO performance is an essential cause that commonly results in high susceptibility to prematurely converge to local optima especially in complex multimodal problems with high dimensionality. This paper presents a new PSO operational strategy based on gravity concept to address the aforementioned drawback and it is named as gravity-based <b>particle</b> <b>swarm</b> <b>optimizer</b> (GPSO). In addition, GPSO is further modified by adopting the cooperation concept of the conventional cooperative <b>particle</b> <b>swarm</b> <b>optimizer</b> (CPSO) to develop an extended version of GPSO called cooperative gravity-based <b>particle</b> <b>swarm</b> <b>optimizer</b> (CGPSO). Simulation results manifest that CGPSO performs satisfactorily on unimodal functions while it generally performs better on multimodal functions than GPSO and other conventional PSO variants. Finally, the proposed GPSO and CGPSO are applied into the problem of optimizing the detection performance of soft decision fusion for cooperative spectrum sensing in cognitive radio networks. For this problem, computer simulations show that the proposed CGPSO outperforms all other PSO variants in terms of quality of solutions whereas GPSO is found to be the best when the computational cost is taken into account...|$|E
40|$|The {{modification}} of the <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> {{has been shown to}} be effective in locating a changing extrema. In this paper we investigate the effectiveness of the modified PSO in tracking changing extrema over time. We demonstrate that a modified PSO is reliable and accurate in tracking a continuously changing solution. 1...|$|E
40|$|Abstract. The {{widespread}} {{usage of}} optimization heuristics such as <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> (PSO) imposes huge challenges on parameter adaption. One variant of PSO is Comprehensive Learning <b>Particle</b> <b>Swarm</b> <b>Optimizer</b> (CLPSO), which uses all individuals ’ best information to update their velocity. The novel strategy of CLPSO enables population {{to read from}} exemplars for specified generations which is called refreshing gap m. In this paper, we devel-op two classes of Learning Automata (LA) in order to study the learning ability of automata for CLPSO refreshing gap tuning. In the first class, a learning au-tomaton is assigned to the population {{and in the second}} one each particle has its own personal automaton. We also compare the proposed algorithm with CLPSO and CPSO-H algorithms. Simulation results show that our algorithms outperform their counterpart algorithms in term of performance, robustness and convergence speed...|$|E
40|$|This paper {{presents}} an efficient and reliable swarm intelligence-based approach, namely elitist-mutated <b>particle</b> <b>swarm</b> optimization (EMPSO) technique, to derive reservoir operation policies for multipurpose reservoir systems. <b>Particle</b> <b>swarm</b> <b>optimizers</b> are inherently distributed algorithms, {{in which the}} solution for a problem emerges from the interactions between many simple individuals called particles. In this study the standard <b>particle</b> <b>swarm</b> optimization (PSO) algorithm is further improved by incorporating a new strategic mechanism called elitist-mutation to improve its performance. The proposed approach is first tested on a hypothetical multireservoir system, used by earlier researchers. EMPSO showed promising results, when compared with other techniques. To show practical utility, EMPSO is then applied to a realistic case study, the Bhadra reservoir system in India, which serves multiple purposes, namely irrigation and hydropower generation. To handle multiple objectives of the problem, a weighted approach is adopted. The results obtained demonstrate that EMPSO is consistently performing better than the standard PSO and genetic algorithm techniques. It is seen that EMPSO is yielding better quality solutions with less number of function evaluations. ...|$|R
40|$|AbstractBoundary {{conditions}} in <b>particle</b> <b>swarm</b> <b>optimizers</b> have been, quite convincingly, shown {{as an important}} algorithmic part which restricts the particles within the solution space thereby increasing their scope of participation in solution finding process. As a common practice, velocities of the particles are regulated {{to bring them back}} to solution space; however, a different fundamental approach of regulating the position of particles is taken here, particles which go out of solution place are relocated to a position where it was in its past (i. e. hysteretic position) iterative cycle inside the boundary. A six element linear antenna array is taken as the optimization target for the reduction of side lobes in the radiation pattern. Results show better performance of this boundary condition over other established ones in terms of quicker convergence and obtaining better optimization solution...|$|R
40|$|In this paper, we {{consider}} the scenario that a population-based algorithm is applied to a numerical optimization problem and a solution needs to be presented within a given time budget. Although {{a wide range of}} population-based algorithms, such as evolutionary algorithms, <b>particle</b> <b>swarm</b> <b>optimizers,</b> and differential evolution, have been developed and studied under this scenario, the performance of an algorithm may vary significantly from problem to problem. This implies that there is an inherent risk associated with the selection of algorithms. We propose that, instead of choosing an existing algorithm and investing the entire time budget in it, it would be less risky to distribute the time among multiple different algorithms. A new approach named population-based algorithm portfolio (PAP), which takes multiple algorithms as its constituent algorithms, is proposed based upon this idea. PAP runs eac...|$|R
