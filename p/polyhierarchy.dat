10|4|Public
5000|$|Although codes remain 5-bytes in length, the {{hierarchical}} relationship between codes {{is no longer}} represented through the codes themselves but rather through a separate table listing all binary parent-child relations; this allows for a <b>polyhierarchy</b> of indefinite depth.|$|E
5000|$|Both concept {{codes and}} term codes have a release status, thus {{allowing}} authoring errors to be corrected: concepts and terms may {{be moved to}} different places in the <b>polyhierarchy</b> between releases, or retired from the scheme (and hierarchy) altogether.|$|E
40|$|A new {{approach}} {{to the construction of}} general persistent polyhierarchical classifications is proposed. It is based on implicit description of category <b>polyhierarchy</b> by a generating <b>polyhierarchy</b> of classification criteria. Similarly to existing approaches, the classification categories are defined by logical functions encoded by attributive expressions. However, the generating hierarchy explicitly predefines domains of criteria applicability, and the semantics of relations between categories is invariant to changes in the universe composition, extending variety of criteria, and increasing their cardinalities. The generating <b>polyhierarchy</b> is an independent, compact, portable, and re-usable information structure serving as a template classification. It can be associated with one or more particular sets of objects, included in more general classifications as a standard component, or used as a prototype for more comprehensive classifications. The approach dramatically simplifies development and unplanned modifications of persistent hierarchical classifications compared with tree, DAG, and faceted schemes. It can be efficiently implemented in common DBMS, while considerably reducing amount of computer resources required for storage, maintenance, and use of complex polyhierarchies. Comment: 11 pages; 1 figure; to be presented at the "Artificial Intelligence and Applications 2004 " conferenc...|$|E
50|$|Technical {{development}} of CONA began in 2010. The editorial system and online search screens for CONA {{are similar to}} those used for the existing Getty vocabularies. The data is compiled and edited in an editorial system that was custom-built by Getty technical staff to meet the unique requirements of compiling data from many contributors, building complex and changing <b>polyhierarchies,</b> merging, moving, and publishing in various formats. Final editorial control of CONA is maintained by the Getty Vocabulary Program, using well-established editorial rules.|$|R
50|$|Editorial {{work has}} been managed by the Getty since 1983. In 1987 the Getty created a {{department}} dedicated to compiling and distributing terminology called the Vocabulary Coordination Group, {{now known as the}} Getty Vocabulary Program, which was within the Getty Information Institute. The data is compiled and edited in an editorial system that was custom-built by Getty technical staff to meet the unique requirements of compiling data from many contributors, building complex and changing <b>polyhierarchies,</b> merging, moving, and publishing in various formats. Final editorial control of the Vocabularies are maintained by the Getty Vocabulary Program, using well-established editorial rules. They are now published in automated formats only, in both a searchable online Web interface and in data files available for licensing.|$|R
40|$|Nursing Vocabulary Summit {{participants}} were challenged {{to consider whether}} reference terminology and information models might {{be a way to}} move toward better capture of data in electronic medical records. A requirement of such reference models is fidelity to representations of domain knowledge. This article discusses embedded structures in three different approaches to organizing domain knowledge: scientific reasoning, expertise, and standardized nursing languages. The concept of pressure ulcer is presented {{as an example of the}} various ways lexical elements used in relation to a specific concept are organized across systems. Different approaches to structuring information—the clinical information system, minimum data sets, and standardized messaging formats—are similarly discussed. Recommendations include identification of the <b>polyhierarchies</b> and categorical structures required within a reference terminology, systematic evaluations of the extent to which structured information accurately and completely represents domain knowledge, and modifications or extensions to existing multidisciplinary efforts...|$|R
40|$|This paper brings {{together}} {{some of the}} common themes which have been described, including: vocabulary content, concept orientation, concept permanence, nonsemantic concept identifiers, <b>polyhierarchy,</b> formal definitions, rejection of "not elsewhere classified" terms, multiple granularities, multiple consistent views, context representation, graceful evolution, and recognized redundancy. Standards developers are beginning to address recognize these desiderata and adapt their offerings to meet them...|$|E
40|$|Classification {{theory is}} divided into two areas: {{analysis}} of conceptual structure and file organization, and the primacy of the first is stressed, A model for conceptual structure in terms of concept coordination and <b>polyhierarchy</b> is sketched, Some problems of file organization, namely post-coordination vs. pre-coordination and synthetic vs. enumerative schemes are discussed in relation to this model. A model for a classification scheme for different kinds of file organization is then proposed. The scheme would consist of a "core classification scheme " made up of elemental concepts and an "extended classification scheme " made up of combinations of elemental concepts. While the core scheme would be universal, extended schemes would be developed as needed in a specific application. This would make for flexibility while maintaining inter-system compatibility. - 1 -...|$|E
40|$|There {{are many}} methodologies and {{techniques}} for easing {{the task of}} ontology building. Here we describe the intersection of two of these: ontology normalisation and fully programmatic ontology development. The first of these describes a standardized organisation for an ontology, with singly inherited self-standing entities, {{and a number of}} small taxonomies of refining entities. The former are described and defined in terms of the latter and used to manage the <b>polyhierarchy</b> of the self-standing entities. Fully programmatic development is a technique where an ontology is developed using a domain-specific language within a programming language, meaning that as well defining ontological entities, it is possible to add arbitrary patterns or new syntax within the same environment. We describe how new patterns can be used to enable a new style of ontology development that we call hypernormalisation...|$|E
5000|$|TGN {{was founded}} under the {{management}} of Eleanor Fink (head of what was then called the Vocabulary Coordination Group, and subsequently Director of the Art History Information Program, later called the Getty Information Institute). TGN has been constructed {{over the years by}} numerous members of the user community and an army of dedicated editors, under the supervision of several managers. Technical support for the TGN was provided by the Getty. TGN was first published in 1997 in machine-readable files. Given the growing size and frequency of changes and additions to the TGN, hard-copy publication was deemed to be impractical. It is currently published in both a searchable online Web interface and in data files available for licensing. The data for the TGN is compiled and edited in an editorial system that was custom-built by Getty technical staff to meet the unique requirements of compiling data from many contributors, building complex and changing <b>polyhierarchies,</b> merging, moving, and publishing in various formats. Final editorial control of the TGN is maintained by the Getty Vocabulary Program, using well-established editorial rules. The current managers of the TGN are Patricia Harpring, Managing Editor, and Murtha Baca, Head, Vocabulary Program and Digital Resource Management.|$|R
40|$|The {{question}} of the dimensionality of intelligent performance has kept researchers occupied for decades. We investigate this question {{in the context of}} learning elementary arithmetic. Our assumption of a <b>polyhierarchy</b> of skills in arithmetic (HiSkA) predicts a multidimensional structure of test data. This seems to contradict findings that data collected to validate the HiSkA conformed to the Rasch model. To resolve this seeming contradiction, we analysed test data from two samples of third graders with a number of methods ranging from factor analysis and Rasch analysis to multidimensional item response theory (MIRT). Additionally we simulated data sets based on different unidimensional and multidimensional models and compared the results of some of the analyses that were also applied to the empirical data. Results show that a multidimensional generating structure can produce data conforming to the Rasch model under certain conditions, that a general factor explains a substantial amount of variance in the empirical data, but that the HiSkA is capable of explaining much of the residual variance...|$|E
40|$|Ontologies {{such as the}} Gene Ontology (GO) {{and their}} use in {{annotations}} make cross species comparisons of genes possible, along {{with a wide range}} of other activities. Tools, such as AmiGO, allow exploration of genes based on their GO annotations. This human driven exploration and querying of GO is obviously useful, but by taking advantage of the ontological representation we can use these annotations to create a rich <b>polyhierarchy</b> of gene products for enhanced querying. This also opens up possibilities for exploring GO annotations (GOA) for redundancies and defects in annotations. To do this we have created a set of OWL classes for mouse genes and their GOA. Each gene is represented as a class, with appropriate relationships to the GO aspects with which it has been annotated. We then use defined classes to query these gene product classes and to build a complex hierarchy. This standard use of OWL affords a rich interaction with GO annotations to give a fine partitioning of the gene products in the ontology...|$|E
40|$|In {{the early}} years of the 20 th century, Julius Otto Kaiser (1868 â 1927), a special {{librarian}} and indexer of technical literature, developed a method of knowledge organization (KO) known as systematic indexing. Certain elements of the method - its stipulation that all indexing terms be divided into fundamental categories "concretes", "countries", and "processes", which are then to be synthesized into indexing "statements" formulated according to strict rules of citation order - have long been recognized as precursors to key principles of the theory of faceted classification. However, other, less well-known elements of the method may prove no less interesting to practitioners of KO. In particular, two aspects of systematic indexing seem to prefigure current trends in KO: (1) a perspectivist outlook that rejects universal classifications in favor of information organization systems customized to reflect local needs and (2) the incorporation of index terms extracted from source documents into a polyhierarchical taxonomical structure. Kaiserâ s perspectivism anticipates postmodern theories of KO, while his principled use of <b>polyhierarchy</b> to organize terms derived from the language of source documents provides a potentially fruitful model that can inform current discussions about harvesting natural-language terms, such as tags, and incorporating them into a flexibly structured controlled vocabulary...|$|E
40|$|As {{the pace}} of {{publishing}} and the volume of content rapidly increase on the web, citizen journalism and data journalism have threatened the traditional role of institutional newsmaking. Legacy publishers, as well as digital-native outlets and aggregators, are beginning to adapt to this new news landscape, in part by drawing newfound value from archival stories and reusing older works. However, this trend's potential remains limited by technical challenges and institutional inertia. In this thesis I propose a framework for considering the news institution of the digital era as a linked archive: equal parts news provider and information portal, the linked archive places historical context on the same footing as new content, and emphasizes the journalist's role as news explainer and verifier. Informed by a theoretical, historical, and technical understanding of the web's structural affordances and limitations, and especially by the untapped networking power of the hyperlink, I suggest how publishers can offer an archive-oriented model of structured, sustainable, and scalable journalism. I draw from concepts and lessons learned in library and computer science, such as link analysis, network theory, and <b>polyhierarchy,</b> to offer an archivally-focused journalistic model that can save time for reporters and improve the research and reading process for journalists and audiences alike. This allows for a treatment of news items {{as part of a}} dynamic conversation rather than a static box or endless feed, revitalizing the news archive and putting the past in fuller and richer dialogue with the present. by Liam Phalen Andrew. Thesis: S. M., Massachusetts Institute of Technology, Department of Comparative Media Studies, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 159 - 172) ...|$|E

