463|10000|Public
50|$|IPoAC {{has been}} {{successfully}} implemented, but for only nine packets of data, with a <b>packet</b> <b>loss</b> <b>ratio</b> of 55% (due to operator error), and a response time ranging from 3000 seconds (≈54 minutes) to over 6000 seconds (≈1.77 hours). Thus, this technology suffers from poor latency. Nevertheless, for large transfers, avian carriers are capable of high average throughput when carrying flash memory devices, effectively implementing a sneakernet. During the last 20 years, the information density of storage media and thus the bandwidth of an avian carrier has increased 3 {{times as fast as}} the bandwidth of the Internet. IPoAC may achieve bandwidth peaks of orders of magnitude more than the Internet when used with multiple avian carriers in rural areas. For example: If 16 homing pigeons are given eight 512 GB SD cards each, and take an hour to reach their destination, the throughput of the transfer would be 145.6 Gbit/s, excluding transfer to and from the SD cards.|$|E
30|$|Average <b>packet</b> <b>loss</b> <b>ratio.</b> The average <b>packet</b> <b>loss</b> <b>ratio</b> is {{the number}} of packets {{received}} unsuccessfully by all receivers versus the total number of packets sent out by all senders.|$|E
30|$|<b>Packet</b> <b>loss</b> <b>ratio</b> [%]: 0, 0.75, 1.5, and 3.|$|E
30|$|We {{evaluate}} {{the performance of}} our DSS-TA method through importing the real channel realizations into the simulation model. Simulation results show that, compared to conventional DSS methods, the newly proposed DSS method achieves up to 12.9 % reduction in terms of <b>packet</b> <b>loss</b> <b>ratios.</b>|$|R
30|$|Video {{multicast}} {{is becoming}} more and more popular in wireless multimedia applications, in which one major challenge is to offer heterogeneous users with a graceful degradation against varying <b>packet</b> <b>loss</b> <b>ratios</b> and channel noise. In this paper, we propose a multi-scale compressed sensing-based wireless video multicast scheme, abbreviated as MCS-cast. The encoder of MCS-cast decomposes each video frame through a discrete wavelet transform (DWT) and explores an optimized compressed sensing (CS) rate to sample/measure each DWT level. The CS measurements are then packed in such a way that all packets are made as equally important as possible, while each packet includes different percentages of different DWT levels. Finally, the packets are transmitted via an analog-like modulator with mapping of the measurements into a very dense constellation. We demonstrate that because of larger percentages of more important DWT levels in each <b>packet,</b> <b>packet</b> <b>loss</b> leads to a much reduced influence on the reconstruction quality. Experimental results show that our MCS-cast preserves the property of graceful degradation for heterogeneous users and can outperform the state-of-the-art SoftCast by up to 3 dB in PSNR at high <b>packet</b> <b>loss</b> <b>ratios</b> (over the same noisy channel).|$|R
40|$|Congestions in {{wireless}} sensor networks (WSNs) {{could potentially}} cause <b>packet</b> <b>loss,</b> throughput impairment and energy waste. To address this issue, a hop-by-hop cross-layer congestion control scheme (HCCC) built on contention-based MAC protocol is proposed in this paper. According to MAC-layer channel information including buffer occupancy ratio and congestion degree of local node, HCCC dynamically adjusts channel access priority in MAC layer and data transmission rate of the node to tackle the problem of congestion. Simulations have been conducted to compare HCCC against closely-related existing schemes. The results show that HCCC exhibits considerable superiority in terms of <b>packets</b> <b>loss</b> <b>ratio,</b> throughput and energy efficiency...|$|R
40|$|This paper proposes an {{adaptive}} proportional integral (API) algorithm for active queue management. API uses mean <b>packet</b> <b>loss</b> <b>ratio</b> and mean queue length as control error to adaptively adjust packet drop probability {{in order to}} expedite congestion control. The extensive simulations validate that API achieves faster control response and improved performance in terms of goodput, average queue length, and <b>packet</b> <b>loss</b> <b>ratio...</b>|$|E
30|$|From Figure 8 {{it follows}} that, with a {{selected}} value of beacon sending frequency and time headway (0.7 s), the undershoot of velocity {{for the last}} vehicle increases as the <b>packet</b> <b>loss</b> <b>ratio</b> increases, {{which means that the}} platoon becomes more string unstable. It can also be observed that for a selected value of <b>packet</b> <b>loss</b> <b>ratio,</b> the string stability becomes worse as the beacon sending frequency decreases. A vehicle always uses the acceleration value which is most recently received from its preceding vehicle as the input of the CACC controller. Therefore, a higher beacon sending frequency for the preceding vehicle results in a higher possibility of receiving fresh information under a constant <b>packet</b> <b>loss</b> <b>ratio.</b> Besides, lower <b>packet</b> <b>loss</b> <b>ratio</b> can also result in a higher possibility of receiving fresh information under a constant beacon sending frequency. For a BSF of 25 Hz packet loss has little effect because vehicles can still easily receive sufficiently fresh information. It should be noted, however, that at 25 Hz the wireless channel capacity will become a limiting factor when considering larger numbers of vehicles [10], so a low PLR is not always achievable.|$|E
30|$|In both Figures 16 and 17, {{it can be}} {{observed}} that {{from the point of}} string stability performance, CACC strongly outperforms ACC. In particular, for a beaconing <b>packet</b> <b>loss</b> <b>ratio</b> of 0 % the CACC string stability overshoot and undershoot are more than 10 times smaller than those measured on the ACC system. Even for a beaconing <b>packet</b> <b>loss</b> <b>ratio</b> of 50 % the CACC string stability overshoot and undershoot are more than 5 times smaller than those measured on the ACC system.|$|E
40|$|Abstract — In this paper, we {{introduce}} a novel wavelet-based multiple description image coder, {{referred to as}} the feature-oriented MDC (FO-MDC). The proposed multiple description coder exploits the statistics of the wavelet coefficients, and identifies the subsets of samples those are sensitive to <b>packet</b> <b>loss.</b> A joint optimization between tree-pruning and quantizer selection in the rate-distortion sense is used in order to allocate more bits to these sensitive coefficients. When compared with the state-of-the-art multiple description scalar quantization coder, the proposed FO-MDC yields a more efficient central-side distortion tradeoff control mechanism. Furthermore, it proves to be more robust for image transmission even with high <b>packet</b> <b>loss</b> <b>ratios.</b> I...|$|R
40|$|This paper {{applies the}} {{recently}} proposed provisioning-delivery hysteresis for Quality of Experience (QoE) {{to the case}} of video. The study is based on evaluations using the Structural Similarity Metric (SSIM) for different versions of a video in terms of resolution on one hand, and suffering from different <b>packet</b> <b>loss</b> <b>ratios</b> on the other hand. Upon translation of the SSIM into MOS, the QoE plotted versus the effective throughput shows the predicted behaviour: a controlled quality and throughput reduction leads to a better user perceived quality than the quality degradation due to <b>packet</b> <b>loss.</b> The results clearly quantify the necessity to control quality, instead of "getting hit" in an uncontrolled way...|$|R
30|$|The limited {{subjective}} tests {{conducted by}} Clark showed {{that most of}} the time VQmon predicts with acceptable accuracy subjective rating of time-varying speech quality. In our opinion, the key shortcoming of VQmon resides in its incapability to accurately estimate RI value under bursty <b>packet</b> <b>loss</b> behavior. In fact, VQmon quantifies the effect of a bursty <b>packet</b> <b>loss</b> process solely using PLR value. As such, there is no subtle characterization and specification of the burstiness of the <b>packet</b> <b>loss</b> processes. This could lead to a wrong judgment of perceived quality because it has been subjectively observed that two distinct bursty <b>packet</b> <b>loss</b> patterns with identical PLR may lead to an obvious difference in the perceived quality [7]. Moreover, the rapidity of the exponential decay/growing is hold static independently of the duration of preceding Good or Bad state and the magnitude variation of previous and current <b>packet</b> <b>loss</b> <b>ratios.</b>|$|R
40|$|PhDInternet-based {{applications}} {{and services are}} pervading everyday life. Moreover, {{the growing popularity of}} real-time, time-critical and mission-critical applications set new challenges to the Internet community. The requirement for reducing response time, and therefore latency control is increasingly emphasized. This thesis seeks to reduce queueing delay through active queue management. While mathematical studies and research simulations reveal that complex trade-off relationships exist among performance indices such as throughput, <b>packet</b> <b>loss</b> <b>ratio</b> and delay, etc., this thesis intends to find an improved active queue management algorithm which emphasizes delay control without trading much on other performance indices such as throughput and <b>packet</b> <b>loss</b> <b>ratio.</b> The thesis observes that in TCP/IP network, <b>packet</b> <b>loss</b> <b>ratio</b> is a major reflection of congestion severity or load. With a properly functioning active queue management algorithm, traffic load will in general push the feedback system to an equilibrium point in terms of <b>packet</b> <b>loss</b> <b>ratio</b> and throughput. On the other hand, queue length is a determinant factor on system delay performance while has only a slight influence on the equilibrium. This observation suggests the possibility of reducing delay while maintaining throughput and <b>packet</b> <b>loss</b> <b>ratio</b> relatively unchanged. The thesis also observes that queue length fluctuation is a reflection of both load changes and natural fluctuation in arriving bit rate. Monitoring queue length fluctuation alone cannot distinguish the difference and identify congestion status; and yet identifying this difference is crucial in finding out situations where average queue size and hence queueing delay can be properly controlled and reasonably reduced. However, many existing active queue management algorithms only monitor queue length, and their control policies are solely based on this measurement. In our studies, our novel finding is that the arriving bit rate distribution of all sources contains information which can be a better indication of congestion status and has a correlation with traffic burstiness. And this thesis develops a simple and scalable way to measure its two most important characteristics, namely the mean ii and the variance of the arriving rate distribution. The measuring mechanism is based on a Zombie List mechanism originally proposed and deployed in Stabilized RED to estimate the number of flows and identify misbehaving flows. This thesis modifies the original zombie list measuring mechanism, makes it capable of measuring additional variables. Based on these additional measurements, this thesis proposes a novel modification to the RED algorithm. It utilizes a robust adaptive mechanism to ensure that the system reaches proper equilibrium operating points in terms of <b>packet</b> <b>loss</b> <b>ratio</b> and queueing delay under various loads. Furthermore, it identifies different congestion status where traffic is less bursty and adapts RED parameters in order to reduce average queue size and hence queueing delay accordingly. Using ns- 2 simulation platform, this thesis runs simulations of a single bottleneck link scenario which represents an important and popular application scenario such as home access network or SoHo. Simulation results indicate that there are complex trade-off relationships among throughput, <b>packet</b> <b>loss</b> <b>ratio</b> and delay; and in these relationships delay can be substantially reduced whereas trade-offs on throughput and <b>packet</b> <b>loss</b> <b>ratio</b> are negligible. Simulation results show that our proposed active queue management algorithm can identify circumstances where traffic is less bursty and actively reduce queueing delay with hardly noticeable sacrifice on throughput and <b>packet</b> <b>loss</b> <b>ratio</b> performances. In conclusion, our novel approach enables the application of adaptive techniques to more RED parameters including those affecting queue occupancy and hence queueing delay. The new modification to RED algorithm is a scalable approach and does not introduce additional protocol overhead. In general it brings the benefit of substantially reduced delay at the cost of limited processing overhead and negligible degradation in throughput and <b>packet</b> <b>loss</b> <b>ratio.</b> However, our new algorithm is only tested on responsive flows and a single bottleneck scenario. Its effectiveness on a combination of responsive and non-responsive flows as well as in more complicated network topology scenarios is left for future work...|$|E
40|$|Packet loss {{can have}} a {{destructive}} effect on the reconstructed video which makes the presentation displeasing to human eyes. with the availability of Forward Error Correction (FEC), the error of packet losses in streaming applications for both video and audio data can be better managed and controlled [...] Different FEC packet sizes can cause variation in packet loss and <b>packet</b> <b>loss</b> <b>ratio.</b> Selecting appropriate packet size is important. In this paper we examine how the FEC packet size effects on the packet loss and <b>packet</b> <b>loss</b> <b>ratio.</b> Ns 2 simulator is used to evaluate FEC packet size and find optimal FEC packet size...|$|E
40|$|This paper {{presents}} a new overlay-based congestion management mechanism {{that provides the}} proportional fairness among competing aggregates and the improvement in <b>packet</b> <b>loss</b> <b>ratio</b> for Differentiated Services (DiffServ) network. In the proposed scheme, a QoS control packet is sent from the ingress to the egress router every fixed time interval. The ingress router employs a simple additive increase and explicitly decreases algorithm to adjust the aggregate’s sending rate according to the QoS information reflected back from the egress routers. For the performance evaluation of the proposed scheme, we present the simulation of the proportional fairness of aggregates traffic and <b>packet</b> <b>loss</b> <b>ratio.</b> 1...|$|E
30|$|One remark is {{necessary}} before we conclude the paper: {{although we did}} not consider a real multicast scenario that needs to define exactly how many users {{are included in the}} multicast group, it has been mimicked closely by allowing different <b>packet</b> <b>loss</b> <b>ratios</b> and channel noise levels, because each ratio/noise combination truly represents a specific user. As these combinations can be many, our MCS-cast becomes fully scalable in serving an arbitrary number of users in a multicast group or even multiple multicast groups, where each individual user receives a number of noise-corrupted packets (depending on its channel conditions) and then runs its own reconstruction independently.|$|R
3000|$|Recent {{development}} in wireless technology enables communication between vehicles. The concept of co-operative {{adaptive cruise control}} (CACC)--which uses wireless communication between vehicles--aims at string stable behavior in a platoon of vehicles. [...] "String stability" [...] means any non-zero position, speed, and acceleration errors of an individual vehicle in a string do not amplify when they propagate upstream. In this article, we will discuss the string stability of CACC and evaluate its performance under varying <b>packet</b> <b>loss</b> <b>ratios,</b> beacon sending frequencies, and time headway settings in simulation experiments. The simulation framework is built up with a controller prototype, a traffic simulator, and a network simulator.|$|R
30|$|The main {{contributions}} {{of this work}} is the theoretical analysis and practical design of the quantized control signals. In particular, we propose to combine a recent robust control strategy known as (quantized) packet predictive control (PPC) [8 – 11] with a joint source-channel coding strategy based on multiple description (MD) coding [12, 13]. We provide computable upper bounds on the operational bit rate required for coding the quantized control signals (descriptions) and provide a practical design based on our theoretical analysis. The simulation study shows {{that the combination of}} MDs and PPC provides a significant improvement over PPC in the case of large <b>packet</b> <b>loss</b> <b>ratios.</b>|$|R
30|$|Two {{groups of}} {{experiments}} are conducted. The {{first one is}} to show the performance of LLSE in our framework. The second compares our scheme with SoftCast under the same CSNR and <b>packet</b> <b>loss</b> <b>ratio.</b>|$|E
30|$|Nevertheless, {{with the}} {{increase}} in the number of retransmissions, there is still a significant increase of the <b>packet</b> <b>loss</b> <b>ratio</b> once a node sends packets to the sink over more than 4 or 5 hops.|$|E
30|$|In PMIPv 6, the {{involvement}} of LMA in all handoff and communication processes may lead to significantly increase the load on LMA. Consequently, the high load of LMA results in longer queuing delays and increases the <b>packet</b> <b>loss</b> <b>ratio.</b>|$|E
40|$|We {{propose a}} novel error {{concealment}} algorithm {{to be used}} at the receiver side of a lossy image transmission system. Our algorithm involves hiding the edge map of the original image at the transmitter within itself using a robust watermarking scheme. At the receiver, wherever a lost block is detected, the extracted edge information is used as border constraint for the spatial smoothing employing the intact neighboring blocks in order to conceal errors. Simulation results show the superiority of our technique over existing methods even in case of high <b>packet</b> <b>loss</b> <b>ratios</b> in the communication network. Comment: To appear in Proceeding of ICCET 201...|$|R
40|$|Recent {{development}} in wireless technology enables {{communication between vehicles}}. The concept of Co-operative Adaptive Cruise Control (CACC) - which uses wireless communication between vehicles - aims at string stable behaviour in a platoon of vehicles. “String Stability” means any non-zero position, speed, and acceleration errors of an individual vehicle in a string do not amplify when they propagate upstream. In this paper, we will discuss the string stability of CACC and evaluate its performance with various <b>packet</b> <b>loss</b> <b>ratios,</b> beacon sending frequencies and time headway in simulations. The simulation framework is built up with a controller prototype, a traffic simulator, and a network simulator. ...|$|R
40|$|Recent {{literature}} {{highlights the}} multiple description coding (MDC) as a promising method {{to solve the}} problem of resilient image coding over error-prone networks, where <b>packet</b> <b>losses</b> occur. In this paper, we introduce a novel multiple description wavelet-based image coding scheme using fractal. This scheme exploits the fractal’s ability, which is to describe the different resolution scale similarity (redundancy) among wavelet coefficient blocks. When one description is lost, the lost information can be reconstructed by the proposed iterated function system (IFS) recovering scheme with the similarity and some introduced information. Compared with the referenced methods, the experimental results suggest that the proposed scheme can achieve better performance. Furthermore, it is substantiated to be more robust for images transmission and better subjective quality in reconstructed images even with high <b>packet</b> <b>loss</b> <b>ratios...</b>|$|R
40|$|Abstract Accurate network {{monitoring}} {{is vital}} for the operation of Grids. The <b>packet</b> <b>loss</b> <b>ratio</b> {{is among the most}} important metrics for identifying poor network conditions, since it highly affects data throughput performance and the overall end-to-end data transfer quality. In this paper, we present a scalable and non-intrusive technique based on passive network monitoring for estimating the <b>packet</b> <b>loss</b> <b>ratio</b> between different measurement points. The proposed approach is complementary to current active monitoring techniques and can be easily incorporated into the network monitoring components of Grid systems. We describe the design and implementation of the technique, outline its integration within a Grid environment, and present experimental evaluation results, including measurements with real Grid application traffic...|$|E
30|$|For {{the first}} scenario, at time step 8, 000, {{we let the}} leading vehicle {{decelerate}} with an acceleration of - 9 m/s 2, until the leading vehicle reaches the speed of 15 m/s. For the second scenario, at time step 8, 000, we let the leading vehicle accelerate with acceleration 2 m/s 2 until {{the speed of the}} leading vehicle reaches the value of 25 m/s. For experiments in this section, the <b>packet</b> <b>loss</b> <b>ratio</b> (PLR) and beacon sending frequency (BSF) are varied. The chosen values of <b>packet</b> <b>loss</b> <b>ratio</b> are 0 %, 10 %, 20 %, 30 %, 40 %, 50 % and that of beacon sending frequency are 25 Hz, 20 Hz, 15 Hz, 10 Hz, 5 Hz. We also simulate the case with a default beacon sending frequency and <b>packet</b> <b>loss</b> <b>ratio</b> of 15 Hz and 20 %, and different time headway (TH) values: 2 s, 1.5 s, 1.0 s, 0.9 s, 0.8 s, 0.7 s, 0.6 s, and 0.5 s. Note that in all these experiments packet loss is artificially accomplished in a random fashion according to a uniform distribution, see [23]. Moreover, the dropping of a packet is independent from that of other packets.|$|E
30|$|Finally, the {{proposed}} CSIT predictor is compared with nonpredictive CR in LTE-A {{in terms of}} five performance measures, i.e., spectrum utilization, sensing energy, channel switching rate, <b>packet</b> <b>loss</b> <b>ratio</b> (PLR), and average instantaneous throughput. In addition, the limitation {{of the study is}} also presented.|$|E
40|$|Loss {{measurements}} {{are widely used}} in today's networks. There are existing standards and commercial products to perform these measurements. The missing element is a rigorous statistical methodology for their analysis. Indeed, most existing tools ignore the correlation between <b>packet</b> <b>losses</b> and severely underestimate the errors in the measured <b>loss</b> <b>ratios.</b> In this paper, we present a rigorous technique for analyzing performance measurements, in particular, for estimating confidence intervals of <b>packet</b> <b>loss</b> measurements. The task is challenging because Internet <b>packet</b> <b>loss</b> <b>ratios</b> are typically small and the <b>packet</b> <b>loss</b> process is bursty. Our approach, SAIL, is motivated by some simple observations about the mechanism of <b>packet</b> <b>losses.</b> <b>Packet</b> <b>losses</b> occur when the buffer in a switch or router fills, when there are major routing instabilities, or when the hosts are overloaded, and so we expect <b>packet</b> <b>loss</b> to proceed in episodes of loss, interspersed with periods of successful packet transmission. This can be modeled as a simple on/off process, and in fact, empirical measurements suggest that an alternating renewal process is a reasonable approximation to the real underlying loss process. We use this structure to build a hidden semi-Markov model (HSMM) of the underlying loss process and, from this, to estimate both <b>loss</b> <b>ratios</b> and confidence intervals on these <b>loss</b> <b>ratios.</b> We use both simulations {{and a set of}} more than 18 000 hours of real Internet measurements (between dedicated measurement hosts, PlanetLab hosts, Web and DNS servers) to cross-validate our estimates and show that they are better than any current alternative. Hung X. Nguyen and Matthew Rougha...|$|R
40|$|Abstract—Loss {{measurements}} {{are widely used}} in today’s net-works. There are existing standards and commercial products to perform these measurements. The missing element is a rigorous statistical methodology for their analysis. Indeed, most existing tools ignore the correlation between <b>packet</b> <b>losses</b> and severely underestimate the errors in the measured <b>loss</b> <b>ratios.</b> In this paper, we present a rigorous technique for analyzing performance measurements, in particular, for estimating confidence intervals of <b>packet</b> <b>loss</b> measurements. The task is challenging because Internet <b>packet</b> <b>loss</b> <b>ratios</b> are typically small and the <b>packet</b> <b>loss</b> process is bursty. Our approach, SAIL, is motivated by some simple observations about the mechanism of <b>packet</b> <b>losses.</b> <b>Packet</b> <b>losses</b> occur when the buffer in a switch or router fills, when there are major routing instabilities, or when the hosts are overloaded, and so we expect <b>packet</b> <b>loss</b> to proceed in episodes of loss, in-terspersed with periods of successful packet transmission. This can be modeled as a simple ON/OFF process, and in fact, empirical measurements suggest that an alternating renewal process is a reasonable approximation to the real underlying loss process. We use this structure to build a hidden semi-Markov model (HSMM) of the underlying loss process and, from this, to estimate both <b>loss</b> <b>ratios</b> and confidence intervals on these <b>loss</b> <b>ratios.</b> We use both simulations {{and a set of}} more than 18 000 hours of real Internet measurements (between dedicated measurement hosts, PlanetLab hosts, Web and DNS servers) to cross-validate our estimates and show that they are better than any current alternative. Index Terms—Accuracy, confidence interval, loss measurement, hidden semi-Markov models (HSMMs). I...|$|R
40|$|This paper {{reports the}} results of a psychometric {{experiment}} aimed at investigating the visibility and annoyance of packet-loss artifacts, with a focus on understanding to what extent their presence influences viewing behavior. The rationale behind this study is that <b>packet</b> <b>loss</b> artifacts might 'distract' visual attention, creating visual saliency themselves, thereby becoming more visible. In turn, higher artifact visibility might impact their annoyance. Our study involved seven videos, compressed at a very high bitrate (thus free of spatial artifacts), impaired by discarding packets at different <b>packet</b> <b>loss</b> <b>ratios.</b> We tracked the observers' eye-movements while (1) they assessed the annoyance of impaired videos and (2) looked freely at pristine videos. Our results show that the viewing behavior significantly changes from pristine to impaired videos. This change is related to both properties of the video and to the annoyance of the artifacts presented. (c) 2013 IEEE...|$|R
30|$|In this article, {{the string}} {{stability}} of a CACC controller {{in the presence}} of imperfect communication has been investigated. For that purpose, a simulation environment integrating a time-driven controller and traffic simulations (in Simulink and SUMO, respectively) has been combined with event-driven wireless communication simulation (in MiXiM/OMNeT++). We observed that beacon sending frequency and <b>packet</b> <b>loss</b> <b>ratio</b> have significant influence on the performance of the evaluated CACC controller. Lower beacon sending frequencies and/or higher packet loss ratios, which prevent vehicles from receiving fresh information from preceding vehicles, will lower the CACC controller's performance on string stability. Therefore, given a required time headway, strict requirements with respect to beacon sending frequency and <b>packet</b> <b>loss</b> <b>ratio</b> have to be set in order to guarantee string stability.|$|E
40|$|We {{assess the}} {{performance}} of a bottleneck buffer in a multimedia network. The buffer temporarily stores packets of different video streams, awaiting for transmission on a single output link. These streams are encoded in a scalable way such that the bottleneck buffer may drop packets of enhancement layers to ensure delivery of base layer packets. In particular, we here adopt Partial Buffer Sharing to reduce the <b>packet</b> <b>loss</b> <b>ratio</b> of base layer packets. The arrival process of the video packets is modelled by means of a two-class discrete batch Markovian arrival process. Using a matrix-analytic approach, we retrieve various performance measures such as the <b>packet</b> <b>loss</b> <b>ratio</b> and the moments of the packet delay. We then illustrate our approach by means of some numerical examples...|$|E
40|$|We {{describe}} a simple protocol that enhances the communication between end nodes and "the network". Other than {{the majority of}} QoS signaling systems, it achieves scalability by avoiding per-flow state. We also show {{how it can be}} used to decrease an adaptive multimedia application's <b>packet</b> <b>loss</b> <b>ratio...</b>|$|E
40|$|B n C n This paper tackles low delay {{adaptive}} {{video streaming}} over error-prone networks. Our framework {{consists of an}} encoding station, an edge server {{and a set of}} clients with various access rates. The edge server is capable of performing simple error concealment operations on the incoming data before forwarding the adapted media to its clients. We study two encoding scenarios: versions (multiple encodings at various output rates) and layers. We develop a unified end-to-end distortion model, which we use to derive the optimal coding strategy for both scenarios. Finally we analyze the performance of MPEG- 4 coded versions against MPEG- 4 FGS-coded layers in rate-constrained lossy environments. Experiments show that versions perform better than layers when the constraint on the aggregate rate is somewhat relaxed, for low to medium <b>packet</b> <b>loss</b> <b>ratios.</b> ...|$|R
30|$|In WLANs, the {{prioritized}} {{services are}} inherently supported {{through the use}} of EDCA. For guaranteed services, a user defined QoS framework, such as CAPS, is needed. Using the EDCA mechanism, the video traffic is usually given a priority level of 4 or 5 (video access category). This priority level uses smaller contention window and shorter AIFS, resulting in higher access probability, but lower network capacity. Although the higher access probability yields favorably lower average delay for the video traffic, the jitter is still high for video. To examine this fact, we simulated a typical video communication scenario in home WLAN environments using OPNET and observed the delay performance of EDCA and CAPS mechanisms to determine the <b>packet</b> <b>loss</b> <b>ratios.</b> The WLAN used for these simulations was an 802.11 e network with an 802.11 b PHY layer (maximum PHY rate of 11 [*]Mbps).|$|R
40|$|The {{objective}} {{of this paper is}} to study the performance of handoff procedures for an integrated wireless LAN/WAN infrastructure, both qualitatively and quantitatively. We make use of a proposed hierarchical mobility management scheme to evaluate the performance of a handoff procedure with a hierarchy level of one. Also, we propose a design and perform the analysis for a two-level hierarchy architecture that is applicable for integrated 802. 11 -based WLAN/WAN scenarios. The focus of our analysis is on performance measures that are suitable for real-time multimedia applications. In particular, we observe and analyze the improvement in handoff latencies and <b>packet</b> <b>loss</b> <b>ratios</b> that are obtained by employing hierarchical mobility schemes for UDP based traffic sessions under different network scenarios. We also study the scalability of the proposed schemes by evaluating the impact of the number of mobile nodes on the above performance measures...|$|R
