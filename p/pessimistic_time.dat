13|30|Public
5000|$|Times {{necessary}} to complete each task between events are estimated {{in such a}} way as to give an appropriate measure of uncertainty. The most likely time, optimistic time, and <b>pessimistic</b> <b>time,</b> ... " ...|$|E
50|$|Absence is {{the third}} album by {{alternative}} hip hop group Dälek, released by Ipecac Recordings in 2005. The album, according to the group, was recorded during a very dark and <b>pessimistic</b> <b>time</b> period for the group, which in turn resulted in the recording's dark, bleak sound.|$|E
50|$|In the {{following}} example there are seven tasks, labeled A through G. Some tasks can be done concurrently (A and B) while others cannot be done until their predecessor task is complete (C cannot begin until A is complete). Additionally, each task has three time estimates: the optimistic time estimate (o), the most likely or normal time estimate (m), and the <b>pessimistic</b> <b>time</b> estimate (p). The expected time (te) is computed using the formula (o + 4m + p) ÷ 6.|$|E
50|$|In BioShock and BioShock 2, fortune teller {{machines}} called Epstein the Swami {{will give}} out <b>pessimistic</b> fortunes every <b>time</b> they are used.|$|R
5|$|Von Neumann's {{assessment}} that the Soviets had a lead in missile technology, considered <b>pessimistic</b> at the <b>time,</b> was soon proven correct in the Sputnik crisis.|$|R
30|$|The {{proposed}} WMD model {{follows a}} pattern of greedy algorithm optimization for all the nodes of the mesh. The steps performed in that pattern include: (a) calculation of the energy (Equations 1 and 2) in all the neighbor locations of a given node; (b) definition of the location with the lowest energy as a new location for the given node; (c) repetition of steps (a) and (b) for all nodes of the mesh, until the overall energy continues to decrease. From those steps {{we can see that}} the execution time of the segmentation task depends linearly on the number of the nodes in the mesh and the necessary number of algorithm repetitions (calculation of new position for each node of the mesh). Furthermore, the number of necessary iterations depends strictly on the distance which the nodes need to travel during the optimization step. In the WMD model, we have focused on decreasing this value as much as possible. As a result, the general complexity of WMD and TAV models is similar but the <b>pessimistic</b> execution <b>time</b> of the WMD model is severely lower than the <b>pessimistic</b> execution <b>time</b> of the TAV-based algorithm.|$|R
50|$|In the {{following}} table there are seven tasks, labeled a through g. Some tasks can be done concurrently (a and b) while others cannot be done until their predecessor task is complete (c and d cannot begin until a is complete). Additionally, each task has three time estimates: the optimistic time estimate (O), the most likely or normal time estimate (M), and the <b>pessimistic</b> <b>time</b> estimate (P). The expected time (TE) is estimated using the beta probability distribution for the time estimates, using the formula (O + 4M + P) ÷ 6.|$|E
40|$|The {{research}} introduces {{and develops}} a mathematical modeling technique with linearized Taylor’s first order expansion and solve {{by using the}} simplex method. The main objective is to minimize the <b>pessimistic</b> <b>time</b> of the activity which is lie on the critical path by investing additional amounts of money to the project. 7 different amounts of money which is $ 5000, $ 10000, $ 15000, $ 20000, $ 25000, $ 30000 and $ 35000 will be invest to the project, to show the increase amount of money invest in the project will tend to minimize the <b>pessimistic</b> <b>time</b> to decrease the expected time and project duration. Then at the same time, it is also reduces its variance and standard deviation. As {{the result of the}} research, it will bring to the increase of the probability or percentage of completing the project on or before the completion time. The PERT and normal distribution will display the differences between of the amounts of money that will invest to the project. </p...|$|E
40|$|This paper {{presents}} the <b>pessimistic</b> <b>time</b> complexity {{analysis of the}} parallel algorithm for minimizing the fleet size in the pickup and delivery problem with time windows. We show how to estimate the pessimistic complexity step by step. This approach can be easily adopted to other parallel algorithms for solving complex transportation problems. Comment: 4 pages, presented at the Work in Progress Session at PDP 201...|$|E
50|$|Bergman's {{reporting}} for ABC was {{noted for its}} direct style. In contrast to the more avuncular style of CBS anchor Walter Cronkite, Bergman's reporting took a very serious tone, and was very direct (to the point of seeming <b>pessimistic</b> at <b>times)</b> about the possible consequences of any mishaps or accidents that took place during a spaceflight, such as the Apollo 13 accident. In order to more fully understand the astronauts and their missions, Bergman often {{took part in the}} same training and simulations that the astronauts did.|$|R
25|$|In the 1970s, Illich {{was popular}} among leftist intellectuals in France, his thesis having been {{discussed}} in particular by André Gorz. However, his influence declined after the 1981 election of François Mitterrand as Illich was considered too <b>pessimistic</b> at a <b>time</b> when the French Left {{took control of the}} government.|$|R
50|$|The {{critical}} path is aceg {{and the critical}} time is 19.51 work days. It {{is important to note}} that there can be more than one {{critical path}} (in a project more complex than this example) or that the critical path can change. For example, let's say that activities d and f take their <b>pessimistic</b> (b) <b>times</b> to complete instead of their expected (TE) times. The critical path is now adf and the critical time is 22 work days. On the other hand, if activity c can be reduced to one work day, the path time for aceg is reduced to 15.34 work days, which is slightly less than the time of the new critical path, beg (15.67 work days).|$|R
40|$|Abstract: The paper {{aspires to}} {{conclude}} whether Arithmetic Progression (A. P) {{in a particular}} case will substantiate a network or not. A huge network is considered in a specific way with 124 activities which are formed with 94 nodes. A. P is implemented on <b>pessimistic</b> <b>time</b> estimate (m) among the three time estimates namely optimistic, most likely and pessimistic. An algorithmic study has been performed on the network. Elite noteworthy results are launched. All float values are also ciphered. Critical path is delineated and project analysis has been followed up. Periodical analysis is attained with standard normal distribution curves...|$|E
40|$|Under {{the title}} “The Tree of Knowledge” (1911), Pío Baroja’s novel expresses a very <b>pessimistic</b> <b>time</b> under the {{influence}} of Schopenhauer, who was also the face of a noventayochismo and historical particularity of the colonial disaster. Considering the family as the natural and fundamental group unit of society, in this work we try to analyze their role, position and shape in the context of time and space on the novel of Basque writer. The end of the nineteenth century, a time in which the novel developed, and Spain, the spice in which the characters are presented and the contrast between the city (Madrid) and people (Alcolea) serve as a complex prism through which contemplate the family types on the one hand, and family relations, on the other. The aim of our research is to determine the temporal and spatial relationships that concretize the fundamental features of the family and its influence on the main characters in the novel...|$|E
40|$|Thesis {{presents}} {{the activities that}} have taken place during 2011 with the maintenance of Šoštanj's thermal power plant block 5. Early part of thesis describes components of a thermal block. It is followed by the description of action course analysis methods needed for completion of the project's maintenance. With Critical Path Method we can easily determine the longest path throughout the project as durations of activities have exact estimations. Method PERT identifies likely range sizes of activity duration, and acts as an upgrade for CPM analysis. We decided to use beta distribution, for which we needed three estimates of activity duration; optimistic, most likely and <b>pessimistic</b> <b>time.</b> In the second part of thesis we briefly describe the software Pertmaster. We made the project plan and analyzed the risk of project's completion. Resulting critical paths were put through simulation with software programs Pertmaster and Mathematica. Results from the program Pertmaster had minor deviations from the results of Mathematica according to simulations with beta pert distribution. To verify the accuracy of the data, we made extra simulations in both programs with triangular distribution...|$|E
40|$|The {{phenomenon}} of underpricing {{initial public offerings}} is documented for 53 Egyptian share issue privatizations (SIPs) between 1994 and 1998. In my study, which covers five years, I find mixed results; SIPs sustain their positive performance and provide investors with positive abnormal returns over a one-year period, however, my results document negative abnormal returns over 3 - and 5 -year horizons. The initial excess returns are determined by ex-ante uncertainty and demand multiplier, while the aftermarket abnormal returns are explained, mainly, by initial excess return and price-earning ratio, in addition to ex-ante uncertainty and demand multiplier. The empirical findings of this paper {{are consistent with the}} initial public offerings market in which investors are overoptimistic towards the performance of these issues but grow more <b>pessimistic</b> over <b>time.</b> 2...|$|R
40|$|Round-Robin {{scheduling}} is {{the most}} popular time triggered scheduling policy, and has been widely used in communication networks for the last decades. It is an efficient scheduling technique for integration of unrelated system parts, but the worst-case timing depends on the system properties in a very complex way. The existing works on response time analysis of task scheduled under Round-Robin determine very <b>pessimistic</b> response <b>time</b> bounds, without considering in detail the interactions between tasks. This may lead to a degradation of the efficiency of Round-Robin scheduling algorithm, and becomes a practical obstacle to its application in realtime systems. In this paper we present an approach to compute much tighter best-case and worst-case response time bounds of tasks scheduled under preemptive Round-Robin, including also the effects of the scheduling algorithm...|$|R
40|$|We {{document}} {{the phenomenon of}} under-pricing initial public offerings (IPOs) for 47 Gulf firms that went public between 2001 and 2006. The IPOs had, on average, initial abnormal returns of 290 percent, far exceeding those documented for both developed and emerging markets. In aftermarket performance, we find that these IPOs provide investors with negative abnormal returns over a one-year period, {{which seems to be}} consistent with findings in other industrial and emerging markets. The empirical models fail, however, to provide us with a satisfactory explanation using the common independent variables employed in the literature. Nevertheless, it appears that country- and industry-specific characteristics, in addition to the timing of the offers, {{play a key role in}} explaining IPO behavior in the region. This paper's empirical findings support the hypothesis that investors are initially over-optimistic about an IPO's performance, but grow more <b>pessimistic</b> over <b>time.</b> IPOs Under-pricing Aftermarket performance GCC...|$|R
40|$|In {{this paper}} new {{algorithm}} for calculating power indices is described. The complexity {{class of the}} problem is #P-complete and even calculating power index of the biggest player is NP-hard task. Constructed algorithm is a mix of ideas of two algorithms: Klinz & Woeginger partitioning algorithm and Mann & Shapley generating functions algorithm. Time and space complexities of the algorithm are analysed and compared with other known algorithms for the problem. Constructed algorithm has <b>pessimistic</b> <b>time</b> complexity O(n 2 ^(n/ 2)) and pseudopolynomial complexity O(nq), where q is quota of the voting game. This paper also solves open problem stated by H. Aziz and M. Paterson - existence of the algorithm for calculating Banzhaf power indices of all players with time complexity lower than O(n^ 2 2 ^(n/ 2)). Not only is the answer positive but this can be done keeping the pseudopolynomial complexity of generating functions algorithm in case weights are integers. New open problems are stated. Comment: 15 pages, algorithm pessimistic complexity O(n 2 ^(n/ 2)), pseudopolynomial complexity O(nq), calculates Banzhaf indices of all players, #P-complete problem. Minor errors corrected. Explicit explanation of general (non-integer) case without pseudopolynomial complexit...|$|E
40|$|AbstractEstimating {{expected}} completion {{probability of}} any repetitive construction project with a specified/certain duration including repetitive identical activities by using program {{evaluation and review}} technique is the most essential part in construction areas since the activities were had optimistic, most likely and pessimistic durations. This paper focuses on the calculation of expected completion probability of any repetitive construction project within a specified/certain duration (contract duration) by using Line Of Balance technique (LOB) in case of single or multiple number of crews integrated with Program Evaluation and Review Technique (PERT). Repetitive-Projects Evaluation and Review Technique (RPERT), which is a simplified software, will generate the expected project completion probability of a specified/certain duration (contract duration). RPERT software is designed by java programming code system to provide {{a number of new}} and unique capabilities, including: (1) Viewing the expected project completion probability according to a set of specified durations per each identical activity (optimistic time, most likely time, and <b>pessimistic</b> <b>time)</b> in the analyzed project; (2) Providing seamless integration with available project time calculations. In order to provide the aforementioned capabilities of RPERT, the system is implemented and developed in four main modules: (1) A user interface module; (2) A database module; (3) A running module; and (4) A processing module. At the end, an illustrative example will be presented to demonstrate and verify the applications of proposed software (RPERT), by using probabilistic calculations for repetitive construction projects...|$|E
40|$|There {{is a broad}} {{category}} of Operations Management problems {{having to do with}} the management of project type operations. Such operations are typically illustrated by the example of some large-scale, one-time activity such as the design and production of a new prototype machine, the construction of a new facility, or the design and manufacturing of a new system to serve a specific or general propose. The basic approach to all project scheduling is to form an actual or implied network that graphically portrays the tasks and milestone in the project. There are several techniques evolves in the late 1950 s for organizing and representing this basic information. Best known today are PERT (Program Evaluation and Review Techniques) and CPM (Critical Path Method). Other network techniques such as PERT/Cost, GERT and Decision CPM are largely extension and modifications of these original two. It is well known and reported in the literature that CPM is best used for situations with a deterministic nature; on the other hand, PERT is best used for stochastic situations. However, although PERT is able to deal with uncertainty in activities times by using three point estimates (optimistic time, most likely time and <b>pessimistic</b> <b>time),</b> the estimate of activity times are clearly subjective and rely solely on judgment. This research paper and through a real life construction project propose a Monte Carlo simulation approach to deal with the short coming problems inherited in the PERT. The results obtained from our comprehensive simulation approach shows that project managers are able to estimate the probability of project completion time and thus allocate contingency plan for the project. In addition, project managers are able to consider the possibility of sub-critical path(s) that might eventually become the critical path as the project progresses through time. The obvious contribution of this research work is the opportunity that provides for project managers to carefully examine their estimation of the project activities times and the sub-critical path(s) leading to a much improved approach for monitoring and controlling the project...|$|E
40|$|The underpricing {{of initial}} public offerings (IPOs) is {{documented}} for 53 share issue privatizations in Egypt between 1994 and 1998. Over several intervals (up to five years), I find mixed results: share issue privatizations sustain their positive performance and provide investors with positive abnormal returns over a one-year period; however, my results document negative abnormal returns over three- and five-year horizons. The initial excess returns are determined by ex ante uncertainty and oversubscription, whereas the aftermarket abnormal returns over a one-year period are driven by ex ante uncertainty and the price-earnings ratio. However, over three- and five-year periods, abnormal returns are significantly affected by initial excess returns, the price-earnings ratio, and, to a lesser extent, oversubscription. The empirical {{findings are consistent with}} IPO markets in which investors are overoptimistic about the performance of these issues but grow more <b>pessimistic</b> over <b>time.</b> 2005 The Southern Finance Association and the Southwestern Finance Association. ...|$|R
40|$|Simulations were {{performed}} for the anticipated GEOS-C laser network stations at Goddard, Bermuda, and Florida {{to predict how}} well survey and orbit will be recovered. Lasers were added {{one at a time}} at Grand Turk, Antigua, and Panama to estimate the contribution from these additional sites. Time tag biases of 50 microseconds, survey uncertainties of 10 meters in each coordinate, laser range biases and noise estimates of 20 cm each, and conventional gravity uncertainties were included in the simulations. The results indicate that survey can be recovered to about 1 meter and Grand Turk can be recovered better than Antigua or Panama. Reducing the probably <b>pessimistic</b> assumed <b>time</b> tag biases and gravity field uncertainties improves the results. Using these survey recovery estimates, the short arc GEOS-C satellite heights for altimeter intercomparison orbits can be recovered within the calibration area to better than the required two meters...|$|R
40|$|Stochastic {{analysis}} {{techniques for}} real-time systems model the execution time of tasks as random variables. These techniques constitute {{a very powerful}} tool to study the behaviour of real-time systems. However, as they can not avoid all the timing bugs in the implementation, they must be combined with measurement techniques {{in order to gain}} more confidence in the implemented system. In this pa-per, a set of tools to measure, analyze and visualize traces of real-time systems is presented. These tools are driven by stochastic models. In order to find bugs in the timing be-haviour of the system, two metrics, called “pessimism ” and “optimism”, are proposed. They are based on two random variables, the optimistic and the <b>pessimistic</b> execution <b>time,</b> which are also introduced in this paper. These metrics are used in the debugging tools to compare the model and the measured system in order to find errors. The metrics are examined in three case studies. ...|$|R
40|$|Hard {{real-time}} {{systems are}} typically written to execute either on bare metal or {{on a small}} real-time executive that offers no memory protection. This model scales poorly as systems become more complex and integrated, as is the trend in industry today. Designing hard real-time systems on a protected OS is often avoided due to the difficulty in predicting its response time. Hard real-time systems with full virtual memory and memory protection have been proposed previously. However, to our knowledge, no such system has determined safe upper bounds on the latency introduced by this protection. This paper proposes that hard real-time systems can be constructed confidently and cost-effectively using an operating system providing full memory protection and virtual memory. We contend that a carefully written microkernel providing these mechanisms {{has the ability to}} be used in a hard real-time system without overly <b>pessimistic</b> response <b>time</b> guarantees. We use the seL 4 microkernel as a case study, investigating how the features of seL 4 ’s design enable a highly accurate WCET analysis...|$|R
40|$|We study an {{investment}} {{model in which}} agents have the wrong beliefs about the dynamic properties of fundamentals. Specifically, we assume that agents under-estimate the rate of mean reversion. The model exhibits the following six properties. (1) Beliefs are excessively optimistic in good <b>times</b> and excessively <b>pessimistic</b> in bad <b>times.</b> (2) Asset prices are too volatile. (3) Excess returns are negatively autocorrelated. (4) High levels of corporate profits predict negative future excess returns. (5) Real economic activity is excessively volatile; the economy experiences amplified investment cycles. (6) Corporate profits are positively autocorrelated in the short-run and negatively autocorrelated in the medium run. The paper provides a formal model of animal spirits, amplified business cycles, and excess volatility...|$|R
40|$|Abstract: In {{real-time}} systems, temporal behaviour is {{as important}} as functional behaviour, so several techniques have been especially developed for these systems. Stochastic analysis techniques model the execution time of tasks as random variables and constitute a very powerful tool to study the temporal behaviour of real-time systems. However, as they can not avoid all the timing bugs in the implementation, they must be combined with measurement techniques in order to gain more confidence in the implemented system. This paper presents a monitoring tool which can measure real-time systems developed using POSIX. The corresponding analysis and a visualization tool that makes it possible to find errors easily is also introduced. In order to find bugs in the timing behaviour of the system when an stochastic analysis technique is used, two metrics, called “pessimism ” and “optimism”, are proposed. They are based on two random variables, the optimistic and the <b>pessimistic</b> execution <b>time,</b> which are also introduced in this paper. These metrics are used in the debugging tools to compare the model and the measured system in order to find errors. The metrics are examined in four case studies...|$|R
5000|$|Selgas belongs {{among the}} minor writers. His repute depends upon his lyrics and his short tales rather than upon his more ambitious novels. The {{best of his}} verse, which is {{generally}} marked by a gentle melancholy, will {{be found in the}} two collections, [...] "La Primavera" [...] and [...] "El Estio", both put forth in 1850. After his death there appeared the volume of poems entitled [...] "Flores y Espinas". Of his longer novels there may be mentioned the [...] "Dos Rivales" [...] and [...] "Una Madre", both rather tedious compositions. In his short tales he is most successful when he indulges in the sentimental; he is less attractive when he gives utterance to his <b>pessimistic</b> feeling. At <b>times</b> his sentimentalism and pessimism become even morbid.|$|R
40|$|The vehicle {{navigation}} problem {{studied in}} Bell (2009) is revisited and a time-dependent reverse Hyperstar algorithm is presented. This minimises the expected {{time of arrival}} at the destination, and all intermediate nodes, where expectation {{is based on a}} pessimistic (or risk-averse) view of unknown link delays. This may also be regarded as a hyperpath version of the Chabini and Lan (2002) algorithm, which itself is a time-dependent A* algorithm. Links are assigned undelayed travel times and maximum delays, both of which are potentially functions of the time of arrival at the respective link. Probabilities for link use are sought that minimise the driver's maximum exposure to delay on the approach to each node, leading to the determination of a <b>pessimistic</b> expected <b>time</b> of arrival at the destination and all intermediate nodes. Since the context considered is vehicle navigation, the probability of link use measures link attractiveness, so a link with a zero probability of use is unattractive while a link with a probability of use equal to one will have no attractive alternatives. A solution algorithm is presented and proven to solve the problem provided the node potentials are feasible and a FIFO condition applies to undelayed link travel times. The paper concludes with a numerical example. (C) 2012 Published by Elsevier Ltd...|$|R
50|$|Troisi's first work as {{a writer}} was the book L'ulivo nella sabbia (The Olive Tree in the Sand) and was written in 1951. The story {{revolves}} around {{the relationship between a}} judge and a man charged with hiding a cachet of weapons. The two were formerly friends while serving together in northern Africa during World War II. The plot is primarily a psychological one—contrasting the two men and how they dealt with the war. Innocente delitto (Innocent Crime) was written in 1960 and dealt with the problems between the generations of post-war Italy. Diario di un giudice (Diary of a Judge), drawing heavily from his experience as a judge, was written in 1962 and deals with the difficulty of the job. Troisi felt that there was a deep mistrust of the Italian judicial system. Troisi's writings, while very <b>pessimistic</b> at <b>times,</b> were filled with challenges to morality and philosophy. His style was marked by a concentration on dialogue, and left little room for scenery description or plot development. Although Troisi's style has been compared to that of the existentialist Albert Camus, it is more accurate {{to say that he was}} a moralist, showing his concerns for the social and ideological issues of his time.|$|R
40|$|Strength {{testing of}} {{standard}} cured cubes {{does not give}} an accurate representation of early age in-situ strength. Such test results may particularly underestimate early age insitu strength of concrete made with GGBS. This adversely affects site practice through adherence to potentially <b>pessimistic</b> formwork striking <b>times.</b> This paper presents a proposed methodology to enable a contractor make an optimal determination of striking time for repetitive elements in a project, with particular relevance to GGBS concretes. Once a reliable methodology to evaluate in-situ strength is initially verified the decision process to strike formwork can be simplified in follow-on pours of similar concrete elements. A project-specific test program for early age strength determination is reduced to a primarily desktop exercise using standard cube strength data, a relationship between initial testing {{and the application of}} a maturity method...|$|R
40|$|The fork-join queue models {{parallel}} resources where arriving jobs {{divide into}} various number of sub-tasks that {{are assigned to}} unique devices within the parallel resource. Each device in the parallel resource is modeled  ¢¡ £  ¢¡¥ ¤ by queueing servers. A job completes execution and departs the parallel resource after all its sub-tasks complete execution. This paper analyzes ¦-server fork-join queues where arriving jobs divide into ¤¨§�©�§ are assigned to unique servers of the fork-join queue. There is no known closed-form solution for ¦��� � fork-join queues. The paper presents an O(log K) algorithm for computing the mean response <b>time</b> <b>pessimistic</b> and optimistic bounds and for computing the mean response time approximation of the fork-join queue. The error bounds for the response time bounds and approximation are presented. Index Terms: fork-join synchronization, performance evaluation, parallel computer and storage systems. ...|$|R
40|$|To ensure {{temporally}} predictable execution behaviour of an embedded hard real [...] time {{computer control}} system, layer [...] by [...] layer predictability {{of the system}} must be provided. Based on a simple structured programming language, a programming environment for hard real [...] time applications is under construction designed to function temporally predictably, and to support an experimental hardware platform {{as well as a}} corresponding operating system. A compiler with an integrated analyser for execution [...] time analysis of tasks is used to determine usable, realistic and not too <b>pessimistic</b> run [...] <b>time</b> estimates. Keywords. Embedded hard real [...] time systems, safety [...] critical systems, execution time analysis, temporally predictable system behaviour, real [...] time programming languages, compilers. 1. INTRODUCTION For embedded hard [...] real time systems, being able to ensure that a request from the embedding environment will be served within a predefined time frame is of the utmost importance. In multiprogra [...] ...|$|R
40|$|The {{number of}} {{vehicles}} on the road (worldwide) is constantly increasing, causing traffic jams and congestion especially in city traffic. Anticipatory vehicle routing techniques have thus far been applied to fairly small networked traffic scenarios and uniform traffic. We note here a number of limitations of these techniques and present a routing strategy on the assumption of a city map that has {{a large number of}} nodes and connectivity and where the vehicles possess highly varying speed capabilities. A scenario of operation with such characteristics has not previously been sufficiently studied in the literature. Frequent short-term planning is preferred as compared with infrequent planning of the complete map. Experimental results show an efficiency boost when single-lane overtaking is allowed, traffic signals are accounted for and every vehicle prefers to avoid high traffic density on a road by taking an alternative route. Comparisons with optimistic routing, <b>pessimistic</b> routing and <b>time</b> message channel routing are give...|$|R
40|$|This paper {{documents}} {{the phenomenon of}} underpricing initial public offerings (IPOs) for 47 firms that went public between 2001 and 2006 in the equity markets of the six Gulf Cooperation Council (GCC) countries. The average initial abnormal returns of 290 percent exceed {{those found in the}} existing literature for both developed and emerging markets IPOs. Although the IPOs'' returns over the one-year horizon beat the market index benchmark, they present negative abnormal returns once initial returns are excluded, which is consistent with findings in other industrial and emerging markets. The empirical models reject the hypothesis that the IPOs'' performance is driven by the common independent variables employed in the literature. On the contrary, {{in the case of the}} GCC, country- and industry-specific characteristics, in addition to the timing of the offers, play key roles in explaining the abnormal returns of IPOs. This paper''s empirical findings support the hypothesis that investors initially tend to be over-optimistic about the performance of IPOs, but grow more <b>pessimistic</b> over <b>time.</b> Stock prices;Stock markets;statistics, equity markets, statistic, equation, independent variables, stock price, financial economics, stock price indices, stock market, skewness, stock exchange, standard deviation, kurtosis, descriptive statistics, samples, regression analysis, international financial statistics, financial statistics, dummy variable, equity market, stock exchanges, basic descriptive statistics, stock returns, covariance, dummy variables, financial market, autocorrelation, stock price index, financial sector, normal distribution, international financial markets, standard errors, statistical tests, explanatory power, statistical techniques, overvaluation, probability, deposit rates, equations, financial markets, deposit rate, outliers, estimation period, missing observations, standard deviations...|$|R
40|$|Multimedia-dominated {{consumer}} electronics devices (such as cellular phone, digital camera, etc.) operate under soft real-time constraints. Overly <b>pessimistic</b> worst-case execution <b>time</b> analysis techniques borrowed from hard real-time systems domain {{are not particularly}} suitable in this context. Instead, the execution time distribution of a task provides a more valuable input to the system-level performance analysis frameworks. Both program inputs and underlying architecture contribute to the execution time variation of a task. But existing probabilistic execution time analysis approaches mostly ignore architectural modeling. In this paper, we {{take the first step}} towards remedying this situation through instruction cache modeling. We introduce the notion of probabilistic cache states to model the evolution of cache content during program execution over multiple inputs. In particular, we estimate the mean and variance of execution time of a program across inputs in the presence of instruction cache. The experimental evaluation confirms the scalability and accuracy of our probabilistic cache modeling approach. Categories and Subject Descriptors C. 3 [Special-purpose and application-based systems]: Real-time and embedded systems...|$|R
