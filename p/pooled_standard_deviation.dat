80|10000|Public
2500|$|Here [...] is the <b>pooled</b> <b>standard</b> <b>deviation</b> for n=n1=n2 and [...] and [...] are the {{unbiased}} estimators of the variances {{of the two}} samples. The denominator of t is {{the standard}} error {{of the difference between}} two means.|$|E
2500|$|... is an {{estimator}} of the <b>pooled</b> <b>standard</b> <b>deviation</b> {{of the two}} samples: it {{is defined}} in this way so that its square is an unbiased estimator of the common variance {{whether or not the}} population means are the same. In these formulae, [...] is the number of degrees of freedom for each group, and the total sample size minus two (that is, [...] ) is the total number of degrees of freedom, which is used in significance testing.|$|E
50|$|The <b>pooled</b> <b>standard</b> <b>deviation</b> {{follows from}} the square-root of a pooled variance.|$|E
40|$|Range {{restriction}} corrections {{require the}} predictor <b>standard</b> <b>deviation</b> in the applicant pool of interest. Unfortunately, {{this information is}} frequently not available in applied contexts. The common strategy {{in this type of}} situations is to use national-norm <b>standard</b> <b>deviation</b> estimates. This study used data from 8, 276 applicants applying to nine jobs in German governmental organizations to compare applicant <b>pool</b> <b>standard</b> <b>deviations</b> for two cognitive ability tests with national-norm <b>standard</b> <b>deviation</b> estimates, and <b>standard</b> <b>deviations</b> for the total group of governmental applicants. Results revealed that job- and organizational context-specific applicant <b>pool</b> <b>standard</b> <b>deviations</b> were on average about 10 - 12 % smaller than estimates from national norms, and about 4 - 6 % smaller than <b>standard</b> <b>deviations</b> for the total group of governmental applicants...|$|R
25|$|Microarray {{data may}} require further {{processing}} {{aimed at reducing}} the dimensionality of the data to aid comprehension and more focused analysis. Other methods permit analysis of data consisting of a low number of biological or technical replicates; for example, the Local Pooled Error (LPE) test <b>pools</b> <b>standard</b> <b>deviations</b> of genes with similar expression levels {{in an effort to}} compensate for insufficient replication.|$|R
50|$|The {{hypothesized}} {{effect size}} in the population can either be expressed as a correlation coefficient (rho) or a standardized difference in means (delta) for a t-test. The standardized difference {{is equal to the}} absolute value of the difference between two population means (mu's), divided by the <b>pooled</b> population <b>standard</b> <b>deviation</b> (sigma).|$|R
5000|$|Jacob Cohen defined s, the <b>pooled</b> <b>standard</b> <b>deviation,</b> as (for two {{independent}} samples): ...|$|E
50|$|The {{square root}} of a pooled {{variance}} estimator {{is known as}} a <b>pooled</b> <b>standard</b> <b>deviation</b> (also known as combined, composite, or overall standard deviation).|$|E
50|$|This {{technique}} {{was developed by}} Ronald Fisher in 1935 and is used most commonly after a null hypothesis {{in an analysis of}} variance (ANOVA) test is rejected (assuming normality and homogeneity of variances). A significant ANOVA test only reveals that not all the means compared in the test are equal. Fisher's LSD is basically a set of individual t-tests, differentiated only in the calculation of the standard deviation. In each t-test, a <b>pooled</b> <b>standard</b> <b>deviation</b> is computed from only the two groups being compared, while the Fisher's LSD test computes the <b>pooled</b> <b>standard</b> <b>deviation</b> from all groups - thus increasing power. Fisher's LSD does not correct for multiple comparisons.|$|E
40|$|Standard Shewhart X {{control charts}} with {{estimated}} control limits {{are widely used}} in practice. There are four ways to estimate the <b>standard</b> <b>deviation</b> generally, as which we call the average range, the average sample <b>standard</b> <b>deviation,</b> the <b>pooled</b> sample <b>standard</b> <b>deviation</b> and the average absolute deviation, respectively. We give three new estimators which based on median in order to estimate <b>standard</b> <b>deviation</b> in this paper, and we get their means through simulated compu-tation. The simulated results are then used to discuss their properties when the data is from a "-contaminated normal distribution. At last, we simulate the in-control and out-of-control average run length of Shewhart X control charts when the process <b>standard</b> <b>deviation</b> is estimated by seven different ways including three absolute deviations to median mentioned in the paper. 42...|$|R
50|$|A {{method for}} quality control in higher-dimensional space {{is to use}} {{probability}} binning with bins fit to the whole data set <b>pooled</b> together.Then the <b>standard</b> <b>deviation</b> {{of the number of}} cells falling in the bins within each sample can be taken as a measure of multidimensional similarity, with samples that are closer to the norm having a smaller standard deviation.With this method, higher <b>standard</b> <b>deviation</b> can indicate outliers, although this is a relative measure as the absolute value depends partly on the number of bins.|$|R
5000|$|Morphological {{characters}} that sample a continuum may contain phylogenetic signal, but {{are hard to}} code as discrete characters. Several methods have been used, {{one of which is}} gap coding, and there are variations on gap coding. In the original form of gap coding:group means for a character are first ordered by size. The <b>pooled</b> within-group <b>standard</b> <b>deviation</b> is calculated ... and differences between adjacent means ... are compared relative to this <b>standard</b> <b>deviation.</b> Any pair of adjacent means is considered different and given different integer scores ... if the means are separated by a [...] "gap" [...] greater than the within-group <b>standard</b> <b>deviation</b> ... times some arbitrary constant. If more taxa are added to the analysis, the gaps between taxa may become so small that all information is lost. Generalized gap coding works around that problem by comparing individual pairs of taxa rather than considering one set that contains all of the taxa.|$|R
5000|$|Here [...] is the <b>pooled</b> <b>standard</b> <b>deviation</b> for n=n1=n2 and [...] and [...] are the {{unbiased}} estimators of the variances {{of the two}} samples. The denominator of t is {{the standard}} error {{of the difference between}} two means.|$|E
5000|$|... is an {{estimator}} of the <b>pooled</b> <b>standard</b> <b>deviation</b> {{of the two}} samples: it {{is defined}} in this way so that its square is an unbiased estimator of the common variance {{whether or not the}} population means are the same. In these formulae, [...] is the number of degrees of freedom for each group, and the total sample size minus two (that is, [...] ) is the total number of degrees of freedom, which is used in significance testing.|$|E
40|$|The use of <b>pooled</b> <b>standard</b> <b>deviation</b> {{can reduce}} the {{efficiency}} loss in optimum allocation when strata stan-dard deviations are estimated and several of them are equal. Also shown is that the <b>pooled</b> <b>standard</b> <b>deviation</b> is useful in optimum allocation under a multivariate setting. In addition to theoretical development, we provide the result of simulation study to support the theory...|$|E
40|$|Background/Aims: Many {{cognitive}} screening instruments (CSI) {{are available}} to clinicians to assess cognitive function. The optimal method comparing the diagnostic utility of such tests is uncertain. The effect size (Cohen’s d), calculated as the difference {{of the means of}} two groups divided by the weighted <b>pooled</b> <b>standard</b> <b>deviations</b> of these groups, may permit such comparisons. Methods: Datasets from five pragmatic diagnostic accuracy studies, which ex-amined the Mini-Mental State Examination (MMSE), the Mini-Mental Parkinson (MMP), the Six-Item Cognitive Impairment Test (6 CIT), the Montreal Cognitive Assessment (MoCA), the Test Your Memory test (TYM), and the Addenbrooke’s Cognitive Examination-Revised (ACE-R), were analysed to calculate the effect size (Cohen’s d) for the diagnosis of dementia versus no dementia and for the diagnosis of mild cognitive impairment versus no dementia (subjective memory impairment). Results: The effect sizes for dementia versus no dementia diagnosis were large for all six CSI examined (range 1. 59 – 1. 87). For the diagnosis of mild cognitive im-pairment versus no dementia, the effect sizes ranged from medium to large (range 0. 48 – 1. 45), with MoCA having the largest effect size. Conclusion: The calculation of the effect size (Co-hen’s d) in diagnostic accuracy studies is straightforward. The routine incorporation of effect size calculations into diagnostic accuracy studies merits consideration in order to facilitate the comparison of the relative value of CSI. © 2014 S. Karger AG, Base...|$|R
40|$|This paper studies design {{schemes for}} the control chart under non-normality. Different estimators of the <b>standard</b> <b>deviation</b> are {{considered}} {{and the effect of}} the estimator on the performance of the control chart under non-normality is investigated. Two situations are distinguished. In the first situation, the effect of non-normality on the control chart is investigated by using the control limits based on normality. In the second situation we incorporate the knowledge of non-normality to correct the limits of the control chart. The schemes are evaluated by studying the characteristics of the in-control and the out-of-control run length distributions. The results indicate that when the control limits based on normality are applied the best estimator is the <b>pooled</b> sample <b>standard</b> <b>deviation</b> both under normality and under non-normality. When the control limits are corrected for non-normality, the estimator based on Gini's mean sample differences is the best choice...|$|R
40|$|Traditional assays of the {{coagulation}} {{status of}} patients, bleeding time assessment (BT) and light transmission aggregometry (LTA), {{are useful in}} clinical drug development. However, these assays are both labor intensive and expensive. BT results can be operator dependent and by its nature can inhibit subject enrollment in a clinical trial. The preparation of platelet-rich plasma necessary for LTA requires specialized training and laboratory support. Alternatives to these methods are desirable. The goal {{of this study was}} identification of a quantitative, easy-to-use, point-of-care device with minimal technical variables that could facilitate assessment of platelet aggregation in clinical drug development. This was a double-blind, placebo-controlled, randomized, three-period cross-over study in healthy volunteers designed to compare the abilities of BT, LTA, and three point-of-care devices, Multiplate®, Platelet Function Analyzer- 100 ®, and VerifyNow® to quantitate the effects on platelet function of 3 days of treatment with aspirin, clopidogrel, or placebo. The effect size (difference in treatment means divided by the <b>pooled</b> <b>standard</b> <b>deviations</b> [SD]) of the three point-of-care devices was greater than or similar to BT and LTA for all treatment comparisons examined. VerifyNow® had the highest effect size comparing ASA to placebo. Multiplate® had the highest effect size comparing clopidogrel to placebo. From this study, we conclude that any one of the three simple-to-use point-of-care devices can reliably assess the treatment effect of ASA and CLP on platelet function in comparison with BT or LTA at the study population level (ClinicalTrials. gov: NCT 01108588). status: publishe...|$|R
30|$|Step 1 : For each {{dependent}} measure, {{the standardized}} scores were computed using zi[*]=[*](xi[*]−[*]X)/SD, where X {{refers to the}} overall mean and SD refers to the <b>pooled</b> <b>standard</b> <b>deviation.</b>|$|E
30|$|In {{the present}} study, method {{detection}} limit for each metal was estimated by digesting six analytical blanks with the optimized procedure for both ginger and soil samples. Triplicate analyses of six blank samples for all elements were performed and the <b>pooled</b> <b>standard</b> <b>deviation</b> {{of the six}} blank reagents was calculated. The detection limits were obtained by multiplying the <b>pooled</b> <b>standard</b> <b>deviation</b> of the reagent blank (Sblank) by three (MDL[*]=[*] 3 x Sblank, n[*]=[*] 6). The method detection limit of each metal was[*]≤[*] 5  μg/g which indicated that the method was applicable to the determination of metals at trace levels in both the ginger and soils samples.|$|E
30|$|In this study, the {{precision}} of the results were evaluated by the <b>pooled</b> <b>standard</b> <b>deviation,</b> and relative standard deviation {{of the results of}} nine measurements for a given bulk sample (i.e. three samples (n[*]=[*] 3) and triplicate readings for each sample).|$|E
40|$|Standardized {{effect size}} {{measures}} typically employed in behavioral {{and social sciences}} research in the multi-group case (e. g., 2, f 2) evaluate between-group variability in terms of either total or within-group variability, such as variance or <b>standard</b> <b>deviation</b> -' that is, measures of dispersion about the mean. In contrast, the definition of Cohen's d, the effect size measure typically computed in the two-group case, is incongruent due to a conceptual difference between the numerator -' which measures between-group variability by the intuitive and straightforward raw {{difference between the two}} group means -' and the denominator - which measures within-group variability in terms of the difference between all observations and the group mean (i. e., the <b>pooled</b> within-groups <b>standard</b> <b>deviation,</b> SW). Two congruent alternatives to d, in which the root square or absolute mean difference between all observation pairs is substituted for SW as the variability measure in the denominator of d, are suggested and their conceptual and statistical advantages and disadvantages are discussed...|$|R
40|$|In {{stratified}} {{sampling design}} when {{the cost of}} measuring the units is not significant in each stratum, the estimation of population mean or total constructed from a selected sample according to Neyman allocation is advisable. In general the practical use of Neyman allocation suffers {{from a number of}} limitations, when there is no information about strata <b>standard</b> <b>deviations</b> except about the equality of <b>standard</b> <b>deviations</b> between some of the strata, then the precision of the estimate may be increased by pooling the strata with equal <b>standard</b> <b>deviations</b> as a single stratum and the problem of allocation is resolved by using Neyman and proportional allocations simultaneously. In this paper the case of multiple <b>pooling</b> of the <b>standard</b> <b>deviations</b> of the estimates in a multivariate stratified sampling for more than three strata. The problem is formulated as a Multiobjective Nonlinear Programming Problem and its solution procedure is suggested by using Fuzzy Programming approach...|$|R
40|$|Lake Chad at {{the border}} of the Sahara desert in central Africa, is well known for its high {{sensitivity}} to hydroclimatic events. Gaps in in situ data have so far prevented a full assessment of the response of Lake Chad to the ongoing prolonged drought that started {{in the second half of}} the 20 th century. Like many other wetlands and shallow lakes, the 'Small' Lake Chad includes large areas of water under aquatic vegetation which needs to be accounted for to obtain the total inundated area. In this paper, a methodology is proposed that uses Meteosat thermal maximum composite data (Tmax) to account for water covered by aquatic vegetation and provide a consistent monthly time series of total inundated area estimates for Lake Chad. Total inundation patterns in Lake Chad were reconstructed for a 15 -yr period (1986 – 2001) which includes the peak of the drought (86 – 91) and therefore provides new observations on the hydrological functioning of the 'Small' Lake Chad. During the study period, Lake Chad remained below 16, 400 km 2 (third quartile ∼ 8800 km 2). The variability of the inundated area observed in the northern <b>pool</b> (<b>standard</b> <b>deviation</b> σnorthern <b>pool</b> = 1980 km 2) is about 60 % greater than that of the southern pool (σsouthern pool = 1250 km 2). The same methodology could be applied to other large wetlands and shallow lakes in semi-arid or arid regions elsewehere using Meteosat (e. g. Niger Inland Delta, Sudd in Sudan, Okavango Delta) and other weather satellites (e. g., floodplains of the Lake Eyre Basin in Australia and Andean Altiplano Lakes in South America) ...|$|R
30|$|MANOVAs {{were carried}} out to test whether {{independent}} variables, such as gender, age group (18 – 23  years/ 24 – 30  years), marital status (single/partner), level of education (lower education/high school/college or university), work/education ability versus work/education disability, adherence (MMAS score < 6 /MMAS score ≥ 6) and depression (MDI score < 20 /MDI score ≥ 20) {{were associated with the}} twelve domains of CFQ-R. To control for increasing Type I error rate, statistically significant MANOVAs were followed up by univariate ANOVAs to test domain-specific differences. To estimate the magnitude of the association between symptoms of depression, ability to work and HRQoL, effect sizes were calculated by subtracting the means and dividing the result by the <b>pooled</b> <b>standard</b> <b>deviation,</b> resulting in Cohen’s d. Given the variable and small sample sizes, calculation of the <b>pooled</b> <b>standard</b> <b>deviation</b> was adjusted with weight for sample sizes according to Hedges and Olkin (Lakens 2013).|$|E
30|$|Following the {{recommendations}} of Lipsey and Wilson (2001), Cohen's d was calculated as the measure of effect size for all analyses. For between-group analyses, Cohen's d reflects {{the difference between the}} means of the hope intervention and control groups after treatment, divided by the <b>pooled</b> <b>standard</b> <b>deviation</b> of the sample, and controlling for sample size (i.e., the standard mean difference). For studies without control groups, Cohen's d reflects the difference between pre- and post-treatment means divided by the <b>pooled</b> <b>standard</b> <b>deviation</b> (i.e., the standardized mean gain; Cohen 1988). In most cases, Cohen's d was calculated directly from means and standard deviations reported in the articles; in some cases, Cohen's d was estimated using other statistics (e.g., results of t tests). If descriptive statistics were not provided, and the article reported nonsignificant findings, an effect size of zero was used (Lipsey & Wilson, 2001). In all cases, positive d statistics indicate superior functioning in the intervention group compared to the control group or superior functioning posttreatment compared to pretreatment (e.g., higher hope and life satisfaction, or lower psychological distress associated with the intervention).|$|E
40|$|Propensity score {{matching}} is {{a method}} to reduce bias in non-randomized and observational studies. Propensity score matching is mainly applied to two treatment groups rather than multiple treatment groups, because some key issues affecting its application to multiple treatment groups remain unsolved, such as the matching distance, the assessment of balance in baseline variables, and the choice of optimal caliper width. The primary objective {{of this study was}} to compare propensity score matching methods using different calipers and to choose the optimal caliper width for use with three treatment groups. The authors used caliper widths from 0. 1 to 0. 8 of the <b>pooled</b> <b>standard</b> <b>deviation</b> of the logit of the propensity score, in increments of 0. 1. The balance in baseline variables was assessed by standardized difference. The matching ratio, relative bias, and mean squared error (MSE) of the estimate between groups in different propensity score-matched samples were also reported. The results of Monte Carlo simulations indicate that matching using a caliper width of 0. 2 of the <b>pooled</b> <b>standard</b> <b>deviation</b> of the logit of the propensity score affords superior performance in the estimation of treatment effects. This study provides practical solutions for the application of propensity score matching of three treatment groups...|$|E
5000|$|In {{terms of}} its {{algebraic}} form, Fisher's original ICC is the ICC that most resembles the Pearson correlation coefficient. One key {{difference between the two}} statistics is that in the ICC, the data are centered and scaled using a <b>pooled</b> mean and <b>standard</b> <b>deviation,</b> whereas in the Pearson correlation, each variable is centered and scaled by its own mean and <b>standard</b> <b>deviation.</b> This <b>pooled</b> scaling for the ICC makes sense because all measurements are of the same quantity (albeit on units in different groups). For example, in a paired data set where each [...] "pair" [...] is a single measurement made for each of two units (e.g., weighing each twin in a pair of identical twins) rather than two different measurements for a single unit (e.g., measuring height and weight for each individual), the ICC is a more natural measure of association than Pearson's correlation.|$|R
40|$|Abstract Background Most western {{countries}} have disability benefit schemes ostensibly based upon requiring (1) a work inhibiting functional limitation that (2) {{can be attributed}} to a diagnosable condition, injury or disease. The present paper examines to what extent current practice matches the core premises of this model by examining how much poorer the perceived health of disability benefit recipients is, compared to the employed and the unemployed, and further to examine to what extent any poorer perceived health among benefit recipients {{can be attributed to}} mental or somatic illness and symptoms. Methods Information on disability benefit recipiency was obtained from Norwegian registry data, and merged with health information from the Hordaland Health Study (HUSK) in Western Norway, 1997 – 99. Participants (N = 14 946) aged 40 – 47 were assessed for perceived physical and mental health (Short Form- 12), somatic symptoms, mental health, and self reported somatic conditions and diseases treated with medication. Differences associated with employment status were tested in chi-square and t-tests, as well as multivariate and univariate regression models to adjust for potential confounders. Results Recipients of disability benefits (n = 1 351) had poorer perceived physical and mental health than employees (n = 13 156); group differences were 1. 86 and 0. 74 <b>pooled</b> <b>standard</b> <b>deviations</b> respectively. Self reported somatic diagnoses, mental health and symptoms accounted for very little of this difference in perceived health. The unemployed (n = 439) were comparable to the employed rather than the recipients of disability benefits. Conclusion Recipients of disability benefits have poor perceived health compared to both the employed and the unemployed. Surprisingly little of this difference can be ascribed to respondents' descriptions of their illnesses and symptoms. Even allowing for potential underascertainment of condition severity, this finding supports the increasing focus on non-disease oriented contributing factors. Rehabilitation efforts aiming at return to work should have a strong focus on the patients' perceptions of their health in addition to symptom relief and social factors. </p...|$|R
40|$|PURPOSE: To {{evaluate}} the repeatability of Fourier-domain {{optical coherence tomography}} (OCT) measurements of the thickness of femtosecond laser-created laser in situ keratomileusis (LASIK) flaps. SETTING: Doheny Eye Institute, University of Southern California, Los Angeles, California, USA. DESIGN: Case series, evaluation of diagnostic technology. METHODS: in this consecutive series, Fourier-domain OCT (RTVue) {{was used to measure}} flap thickness 1 week after LASIK. Flaps were created with a Pulsion 60 kHz femtosecond laser programmed for 110 mu m flap thickness. Each eye was scanned 2 times with a radial pachymetry pattern and 1 time with a horizontal line scan. Flap thicknesses were measured at 6 positions across the corneal flap (ie, +/- 0. 5 mm, +/- 1. 5 mm, and +/- 2. 5 mm from the center on the horizontal and vertical meridians). the within-grader flap thickness repeatability and between-grader reproducibility were calculated by <b>pooled</b> <b>standard</b> <b>deviations</b> (SDs). RESULTS: Twenty-one eyes were measured. the mean flap thickness measurements were highly predictable at all positions. Thickness SDs varied from 5. 3 to 9. 5 mu m and uniformity, from 121. 7 to 126. 5 mu m. the within-grader repeatability was 3. 3 to 6. 4 mu m based on the same image measured at different times and 4. 7 to 7. 4 mu m for different images. the between-grader reproducibility was 4. 0 to 9. 0 mu m. There was no statistically significant asymmetry between the nasal side and the temporal side, the superior side and the inferior side, or the pericentral area and the central area of the corneal flap. CONCLUSIONS: the femtosecond laser created LASIK flaps with uniform and predictable thicknesses. Fourier-domain OCT gave highly repeatable flap-thickness measurements. Optovue, Inc. NIHResearch to Prevent Blindness, Inc., New York, New York, USAUniv So Calif, Doheny Eye Inst, Ctr Ophthalm Opt & Lasers, Los Angeles, CA USAUniversidade Federal de São Paulo, São Paulo, BrazilUniversidade Federal de São Paulo, São Paulo, BrazilNIH: R 01 EY 018184 Web of Scienc...|$|R
40|$|Background: The {{diagnosis}} of occupational asthma requires objective confirmation. Analysis of serial mea-surements of peak expiratory flow (PEF) {{is usually the}} most convenient {{first step in the}} diagnostic process. A new method of analysis originally developed to detect late asthmatic reactions following specific inhalation testing is described. This was applied to serial PEF measurements made over many days in the workplace to supplement existing methods of PEF analysis. Methods: 236 records from workers with independently diagnosed occupational asthma and 320 records from controls with asthma were available. The <b>pooled</b> <b>standard</b> <b>deviation</b> for rest day measurements was obtained from an analysis of variance by time. Work day PE...|$|E
40|$|This paper aims to {{identify}} problems in estimating and {{the interpretation of}} the magnitude of intervention-related change over time or responsiveness assessed with health outcome measures. Responsiveness is a problematic construct and there is no consensus on how to quantify the appropriate index to estimate change over time between baseline and post-test designs. This paper gives an overview of several responsiveness indices. Thresholds for effect size (or responsiveness index) interpretation were introduced some thirty years ago by Cohen who standardised the difference-scores (d) with the <b>pooled</b> <b>standard</b> <b>deviation</b> (d /SDpooled). However, many effect sizes (ES) have been introduced since Cohen's original work and in the formula of one of these ES, the mean change scores are standardised with the SD of those change scores (d /SDchange). When health outcome questionnaires are used, this effect size is applied on a wide scale and is represented as the Standardized Response Mean (SRM). However, its interpretation is problematic when it is used as an estimate of magnitude of change over time and interpreted with the thresholds, set by Cohen for effect size (ES) which is based on SDpooled. Thus, in the case of using the SRM, application of these well-known cut-off points for <b>pooled</b> <b>standard</b> <b>deviation</b> units namely: ‘trivial’ (ES Consequently, taking Cohen's thresholds for granted for every version of effect size indices as estimates of intervention-related magnitude of change, may lead to over- or underestimation of this magnitude of intervention-related change over time. For those researchers who use Cohen's thresholds for SRM interpretation, this paper demonstrates a simple method to avoid over-or underestimation...|$|E
30|$|Precision can be {{determined}} by standard deviation, variance, coefficient of variance, relative standard deviation, and range of series measurements (Miller and Miller 2000). In this study the precision of the results were evaluated by the <b>pooled</b> <b>standard</b> <b>deviation</b> and relative standard deviation {{of the results of}} triplicate samples and three reading (n =  9) obtained for each sample. The result of analysis was reported with corresponding standard deviation at 95  % confidence limit and relative standard deviation. It can be seen that the values of percentage relative standard deviations (% RSD) are less than 10  % for all the mean concentrations. This shows the precision of the results obtained by this method is good and acceptable.|$|E
40|$|Background: Most western {{countries}} have disability benefit schemes ostensibly based upon requiring (1) a work inhibiting functional limitation that (2) {{can be attributed}} to a diagnosable condition, injury or disease. The present paper examines to what extent current practice matches the core premises of this model by examining how much poorer the perceived health of disability benefit recipients is, compared to the employed and the unemployed, and further to examine to what extent any poorer perceived health among benefit recipients {{can be attributed to}} mental or somatic illness and symptoms. Methods: Information on disability benefit recipiency was obtained from Norwegian registry data, and merged with health information from the Hordaland Health Study (HUSK) in Western Norway, 1997 – 99. Participants (N = 14 946) aged 40 – 47 were assessed for perceived physical and mental health (Short Form- 12), somatic symptoms, mental health, and self reported somatic conditions and diseases treated with medication. Differences associated with employment status were tested in chi-square and t-tests, as well as multivariate and univariate regression models to adjust for potential confounders. Results: Recipients of disability benefits (n = 1 351) had poorer perceived physical and mental health than employees (n = 13 156); group differences were 1. 86 and 0. 74 <b>pooled</b> <b>standard</b> <b>deviations</b> respectively. Self reported somatic diagnoses, mental health and symptoms accounted for very little of this difference in perceived health. The unemployed (n = 439) were comparable to the employed rather than the recipients of disability benefits. Conclusion: Recipients of disability benefits have poor perceived health compared to both the employed and the unemployed. Surprisingly little of this difference can be ascribed to respondents' descriptions of their illnesses and symptoms. Even allowing for potential underascertainment of condition severity, this finding supports the increasing focus on non-disease oriented contributing factors. Rehabilitation efforts aiming at return to work should have a strong focus on the patients' perceptions of their health in addition to symptom relief and social factors. </p...|$|R
40|$|We {{revisit the}} {{dramatic}} failure of monetary models in explaining exchange rate movements. Using the information content from 98 countries, we find strong evidence for cointegration between nominal exchange rates and monetary fundamentals. We also find fundamentalsbased models {{very successful in}} beating a random walk in out-of-sample prediction. Exchange rates;Forecasting models;random walk, inflation, monetary model, cointegration, statistics, forecasting, statistic, bootstrap, monetary models, correlation, money supplies, equation, econometrics, estimation period, equations, time series, samples, money supply, predictions, prediction, standard errors, significance levels, finite sample, predictability, statistical significance, adjustment parameter, financial statistics, dummy variable, explanatory power, covariance, monetary approach, sample size, monetary fund, monetary exchange, coefficient of adjustment, monetary analysis, monetary statistics, significance level, outlier, combination of variables, calibrations, hypothesis testing, survey, autocorrelation, instrumental variable, time series analysis, statistical inference, demand for money, statistical model, central bank, covariances, <b>pooled</b> time series, <b>standard</b> <b>deviation...</b>|$|R
40|$|Previous {{results from}} our {{laboratory}} suggest a combined effect of anger-suppression and {{family history of}} cardiovascular disorders in determining cardiovascular responses to mental stress. The present {{study was designed to}} determine the effect sizes in cardiovascular reactivity associated with biological risk, psychological risk and the combination of these risk factors using meta-analytical techniques. Results from three independent studies with almost identical experimental procedures provided the basis for the calculation of d, the difference between the means of two groups, divided by the <b>pooled</b> within-group <b>standard</b> <b>deviation.</b> Effect sizes were calculated for the comparison of high versus low biological hypertension risk, high versus low anger suppression, and high versus low combined risk. The results show the largest effect sizes for the comparison of high versus low combined risk. The effect sizes associated with the combination of risk factors were larger than the sum of the effect sizes associated with either factor alone. We conclude that the combination of biological and psychological risk factors in determining cardiovascular reactivity to mental stress is {{more than the sum of}} its parts. These findings are discussed in terms of a better understanding of the over-additive effects of multiple cardiovascular risk factors on cardiovascular morbidity and mortality...|$|R
