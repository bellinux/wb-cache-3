839|866|Public
25|$|Find {{the optimal}} grammar <b>parse</b> <b>tree</b> (CYK algorithm).|$|E
25|$|Two {{types of}} ambiguities can be distinguished. <b>Parse</b> <b>tree</b> {{ambiguity}} and structural ambiguity. Structural ambiguity {{does not affect}} thermodynamic approaches as the optimal structure selection {{is always on the}} basis of lowest free energy scores. <b>Parse</b> <b>tree</b> ambiguity concerns the existence of multiple parse trees per sequence. Such an ambiguity can reveal all possible base-paired structures for the sequence by generating all possible parse trees then finding the optimal one. In the case of structural ambiguity multiple parse trees describe the same secondary structure. This obscures the CYK algorithm decision on finding an optimal structure as the correspondence between the <b>parse</b> <b>tree</b> and the structure is not unique. Grammar ambiguity can be checked for by the conditional-inside algorithm.|$|E
25|$|Picking a {{different}} order of expansion will produce {{a different}} derivation, but the same <b>parse</b> <b>tree.</b>|$|E
50|$|In gene {{expression}} programming the linear chromosomes {{work as the}} genotype and the <b>parse</b> <b>trees</b> as the phenotype, creating a genotype/phenotype system. This genotype/phenotype system is multigenic, thus encoding multiple <b>parse</b> <b>trees</b> in each chromosome. This means that the computer programs created by GEP are composed of multiple <b>parse</b> <b>trees.</b> Because these <b>parse</b> <b>trees</b> {{are the result of}} {{gene expression}}, in GEP they are called expression trees.|$|R
5000|$|Hybrid {{approaches}} - for instance, <b>parse</b> <b>trees</b> + suffix trees {{can combine}} the detection capability of <b>parse</b> <b>trees</b> {{with the speed}} afforded by suffix trees, a type of string-matching data structure.|$|R
5000|$|<b>Parse</b> <b>Trees</b> - {{build and}} compare <b>parse</b> <b>trees.</b> This allows higher-level {{similarities}} to be detected. For instance, tree comparison can normalize conditional statements, and detect equivalent constructs as similar to each other.|$|R
25|$|The <b>parse</b> <b>tree</b> {{will only}} change if we pick a {{different}} rule {{to apply at}} some position in the tree.|$|E
25|$|Several {{algorithms}} {{dealing with}} aspects of PCFG based probabilistic models in RNA structure prediction exist. For instance the inside-outside algorithm and the CYK algorithm. The inside-outside algorithm is a recursive dynamic programming scoring algorithm that can follow expectation-maximization paradigms. It computes the total probability of all derivations {{that are consistent}} with a given sequence, based on some PCFG. The inside part scores the subtrees from a <b>parse</b> <b>tree</b> and therefore subsequences probabilities given an PCFG. The outside part scores the probability of the complete <b>parse</b> <b>tree</b> for a full sequence. CYK modifies the inside-outside scoring. Note that the term 'CYK algorithm' describes the CYK variant of the inside algorithm that finds an optimal <b>parse</b> <b>tree</b> for a sequence using a PCFG. It extends the actual CYK algorithm used in non-probabilistic CFGs.|$|E
25|$|The {{resulting}} {{of multiple}} parse trees per grammar denotes grammar ambiguity. This {{may be useful}} in revealing all possible base-pair structures for a grammar. However an optimal structure is the one where there is one and only one correspondence between the <b>parse</b> <b>tree</b> and the secondary structure.|$|E
40|$|This work {{presents}} {{a type of}} parser that takes the process of chunking to the stage of producing full <b>parse</b> <b>trees.</b> This type of parser, denoted Thin Parsers (TP) in this work has the characteristics of: following a given grammar, creating full <b>parse</b> <b>trees,</b> producing {{only a limited number}} of full <b>parse</b> <b>trees,</b> <b>parsing</b> in linear time of sentence length. Performance standards on the Penn Tree Bank show results slightly under that of stochastic parsers but faster performance. Various types of Thi...|$|R
40|$|We {{describe}} an approach for training a semantic role labeler through cross-lingual projection between {{different types of}} <b>parse</b> <b>trees.</b> After applying an existing semantic role labeler to <b>parse</b> <b>trees</b> in a resource-rich language (English), we partially project the semantic information to the <b>parse</b> <b>trees</b> of the corresponding target sentences, based on word alignment. From this precision-oriented projection, which focuses on similarities between source and target sentences and <b>parse</b> <b>trees,</b> a large set of features describing target predicates, roles and predicate-role connections is generated, independently from the type of tree annotation (phrase structure or dependencies). These features describe tree paths starting at or connecting nodes. We train classifiers in order to add candidate predicates and roles to the ones resulting from projection. We investigate whether these candidates facilitate the alignment of translation divergences and annotational divergences between <b>parse</b> <b>trees.</b> status: publishe...|$|R
40|$|<b>Parse</b> <b>trees</b> are {{indispensable}} {{to the existing}} tree-based translation models. However, there exist two major challenges in utilizing parse trees: 1) For most language pairs, {{it is hard to}} get <b>parse</b> <b>trees</b> {{due to the lack of}} syntactic resources for training. 2) Numerous <b>parse</b> <b>trees</b> are not compatible with word alignment which is generally learned by GIZA++. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the <b>parse</b> <b>trees</b> and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic parser. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using <b>parse</b> <b>trees...</b>|$|R
25|$|In {{computer}} science, {{an ambiguous}} grammar is a context-free grammar {{for which there}} exists a string that can {{have more than one}} leftmost derivation or <b>parse</b> <b>tree,</b> while an unambiguous grammar is a context-free grammar for which every valid string has a unique leftmost derivation or <b>parse</b> <b>tree.</b> Many languages admit both ambiguous and unambiguous grammars, while some languages admit only ambiguous grammars. Any non-empty language admits an ambiguous grammar by taking an unambiguous grammar and introducing a duplicate rule or synonym (the only language without ambiguous grammars is the empty language). A language that only admits ambiguous grammars is called an inherently ambiguous language, and there are inherently ambiguous context-free languages. Deterministic context-free grammars are always unambiguous, and are an important subclass of unambiguous grammars; there are non-deterministic unambiguous grammars, however.|$|E
25|$|The {{role of the}} {{parentheses}} in {{the definition}} {{is to ensure that}} any formula can only be obtained in one way by following the inductive definition (in other words, there is a unique <b>parse</b> <b>tree</b> for each formula). This property is known as unique readability of formulas. There are many conventions for where parentheses are used in formulas. For example, some authors use colons or full stops instead of parentheses, or change the places in which parentheses are inserted. Each author's particular definition must be accompanied by a proof of unique readability.|$|E
25|$|Sometimes, the {{structure}} of sentiments and topics is fairly complex. Also, the problem of sentiment analysis is non-monotonic in respect to sentence extension and stop-word substitution (compare THEY would not let my dog stay in this hotel vs I would not let my dog stay in this hotel). To address this issue a number of rule-based and reasoning-based approaches have been applied to sentiment analysis, including defeasible logic programming. Also, there is a number of tree traversal rules applied to syntactic <b>parse</b> <b>tree</b> to extract the topicality of sentiment in open domain setting.|$|E
40|$|We {{investigate}} {{the feasibility of}} aligning Chinese and English <b>parse</b> <b>trees</b> by examining cases of incompatibility between Chinese-English parallel <b>parse</b> <b>trees.</b> This work {{is done in the}} context of an annotation project wherewe construct a parallel treebank by doingword and phrase alignments simultaneously. We discuss the most common incompatibility patterns identified within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel <b>parse</b> <b>trees</b> with somemodification of existing syntactic annotation guidelines. We believe this has implications for the use of parallel <b>parse</b> <b>trees</b> as an important resource for Machine Translation models. ...|$|R
50|$|<b>Parse</b> <b>trees</b> concretely {{reflect the}} syntax of the input language, making them {{distinct}} from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, <b>parse</b> <b>trees</b> do not use distinct symbol shapes for different types of constituents.|$|R
40|$|The {{automatic}} {{extraction of}} relations between entities expressed in natural language text is an important problem for IR and text understanding. In this paper we show how different kernels for <b>parse</b> <b>trees</b> can be combined to improve the relation extraction quality. On a public benchmark dataset {{the combination of a}} kernel for phrase grammar <b>parse</b> <b>trees</b> and for dependency <b>parse</b> <b>trees</b> outperforms all known tree kernel approaches alone suggesting that both types of trees contain complementary information for relation extraction. ...|$|R
2500|$|But can a {{different}} <b>parse</b> <b>tree</b> still {{produce the same}} terminal string, ...|$|E
2500|$|Let's {{look at this}} in more detail. [...] Consider the <b>parse</b> <b>tree</b> of this derivation: ...|$|E
2500|$|In {{order to}} score a CM model the inside-outside {{algorithms}} are used. CMs use a slightly different implementation of CYK. Log-odds emission scores for the optimum <b>parse</b> <b>tree</b> - [...] - are calculated out of the emitting states [...] Since these scores are a function of sequence length a more discriminative measure to recover an optimum <b>parse</b> <b>tree</b> probability score- [...] - is reached by limiting the maximum length of the sequence to be aligned and calculating the log-odds relative to a null. The computation time of this step is linear to the database size and the algorithm has a memory complexity of [...]|$|E
40|$|In {{this paper}} we present an {{approach}} to extracting subject-predicate-object triplets from English sentences. To begin with, four different well known syntactical parsers for English are used for generating <b>parse</b> <b>trees</b> from the sentences, followed by extraction of triplets from the <b>parse</b> <b>trees</b> using parser dependent techniques. 1...|$|R
50|$|<b>Parse</b> <b>trees</b> {{are usually}} {{constructed}} based {{on either the}} constituency relation of constituency grammars (phrase structure grammars) or the dependency relation of dependency grammars. <b>Parse</b> <b>trees</b> may be generated for sentences in natural languages (see natural language processing), as well as during processing of computer languages, such as programming languages.|$|R
25|$|Rank {{and score}} the <b>parse</b> <b>trees</b> {{for the most}} {{plausible}} sequence.|$|R
2500|$|This tree {{is called}} a <b>parse</b> <b>tree</b> or [...] "concrete syntax tree" [...] of the string, by {{contrast}} with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string ...|$|E
2500|$|The inside {{algorithm}} calculates [...] probabilities for all [...] of a parse subtree rooted at [...] for subsequence [...] Outside algorithm calculates [...] {{probabilities of}} a complete <b>parse</b> <b>tree</b> for sequence [...] from root excluding the calculation of [...] The variables [...] and [...] refine the estimation of probability parameters of an PCFG. It is possible to reestimate the PCFG algorithm by finding the expected number of times a state is used in a derivation through summing all the products of [...] and [...] divided by the probability for a sequence [...] given the model [...] It is also possible to find the expected number of times a production rule is used by an expectation-maximization that utilizes the values of [...] and [...] The CYK algorithm calculates [...] {{to find the most}} probable <b>parse</b> <b>tree</b> [...] and yields [...]|$|E
2500|$|... (once again picking S as {{the start}} symbol). This {{alternative}} grammar will produce x + y * z with a <b>parse</b> <b>tree</b> {{similar to the}} left one above, i.e. implicitly assuming the association (x + y) * z, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.|$|E
2500|$|Recursively {{generate}} <b>parse</b> <b>trees</b> of {{the possible}} structures using the grammar.|$|R
5000|$|Rank {{and score}} the <b>parse</b> <b>trees</b> {{for the most}} {{plausible}} sequence.|$|R
5000|$|Recursively {{generate}} <b>parse</b> <b>trees</b> of {{the possible}} structures using the grammar.|$|R
2500|$|A {{weighted}} {{context-free grammar}} (WCFG) {{is a more}} general category of context-free grammar, where each production has a numeric weight associated with it. The weight of a specific <b>parse</b> <b>tree</b> in a WCFG is the product (or sum [...] ) of all rule weights in the tree. [...] Each rule weight is included as often as the rule {{is used in the}} tree. [...] A special case of WCFGs are PCFGs, where the weights are (logarithms of [...] ) probabilities.|$|E
2500|$|Chomsky {{developed}} a formal theory of grammar where [...] transformations manipulated {{not just the}} surface strings, but the <b>parse</b> <b>tree</b> associated with them, making transformational grammar a system of tree automata. A transformational-generative (or simply transformational) grammar thus involved two types of productive rules: phrase structure rules, such as [...] "S → NP VP" [...] (meaning that a sentence may consist of a noun phrase followed by a verb phrase) etc., {{which could be used}} to generate grammatical sentences with associated parse trees (phrase markers, or P markers); and transformational rules, such as rules for converting statements to questions or active to passive voice, which acted on the phrase markers to produce further grammatically correct sentences. (For more details see the Transformations section below.) ...|$|E
2500|$|The {{types of}} various {{structure}} {{that can be}} modeled by a PCFG include long range interactions, pairwise structure and other nested structures. However, pseudoknots can not be modeled. PCFGs extend CFG by assigning probabilities to each production rule. A maximum probability <b>parse</b> <b>tree</b> from the grammar implies a maximum probability structure. Since RNAs preserve their structures over their primary sequence; RNA structure prediction can be guided by combining evolutionary information from comparative sequence analysis with biophysical knowledge about a structure plausibility based on such probabilities. Also search results for structural homologs using PCFG rules are scored according to PCFG derivations probabilities. Therefore, building grammar to model the behavior of base-pairs and single-stranded regions starts with exploring features of structural multiple sequence alignment of related RNAs.|$|E
40|$|We {{present a}} new {{algorithm}} for transforming dependency <b>parse</b> <b>trees</b> into phrase-structure <b>parse</b> <b>trees.</b> We cast {{the problem as}} struc-tured prediction and learn a statistical model. Our algorithm is faster than traditional phrase-structure parsing and achieves 90. 4 % English parsing accuracy and 82. 4 % Chinese parsing accuracy, near {{to the state of}} the art on both benchmarks. ...|$|R
40|$|An {{important}} step {{for understanding the}} semantic content of text is the extraction of semantic relations between entities in natural language documents. Automatic extraction techniques {{have to be able}} to identify different versions of the same relation which usually may be expressed in a great variety of ways. Therefore these techniques benefit from taking into account many syntactic and semantic features, especially <b>parse</b> <b>trees</b> generated by automatic sentence parsers. Typed dependency <b>parse</b> <b>trees</b> are edge and node labeled <b>parse</b> <b>trees</b> whose labels and topology contains valuable semantic clues. This information can be exploited for relation extraction by the use of kernels over structured data for classification. In this paper we present new tree kernels for relation extraction over typed dependency <b>parse</b> <b>trees.</b> On a public benchmark data set we are able to demonstrate a significant improvement in terms of relation extraction quality of our new kernels over other state-of-the-art kernels...|$|R
40|$|Mango is a parser {{generator}} that {{is included in}} Release 3. 0 of the Self system. Mango goes beyond LEX/YACC in several respects. First, Mango grammars are structured, thus easier to read and map onto <b>parse</b> <b>trees.</b> Second, Mango parsers automatically build <b>parse</b> <b>trees</b> rather than merely provide hooks for calling low-level reduce actions during parsing. Third, Mango automatically maintains invariance {{of the structure of}} <b>parse</b> <b>trees,</b> even while grammars are transformed to enable LR parsing. Fourth, Mango and the parsers it generates are completely integrated in the Self object world. In particular, a parser is an object. Unlike YACC, several independent parsers can co-exist in a single program. We show how to generate a Mango parser and how to use it by means of an example: a simple expression language. Furthermore, we show how to add semantic properties to the <b>parse</b> <b>trees</b> that the Mango parser produces. Mango is a realistic tool. A parser for full ANSI C was built with Mango. email addr [...] ...|$|R
