1|27|Public
40|$|Abstract. The {{research}} actuality {{and development}} of roller module replacement technology are talked over in this paper. Positioning scheme of one plane and two pins with the special rounded edge diamond pin that is suitable for the longer roller is discussed, which is meeting the <b>parallelism</b> <b>tolerance</b> requirement for the roller within the full length; thus reducing manufacturing tolerance requirement for the spacing between pin axes and reducing manufacturing difficulty. And on this basis, adaptable interface design of roller module rapid replacement system which is suitable for the longer roller has been completed. The invention patent on the roller module rapid replacement system has already been applied for, and the precise positioning of roller module can be realized...|$|E
40|$|Modularity and <b>parallelism</b> provide <b>tolerance</b> to faults {{and high}} {{throughput}} capacity. System, called MAX, is network of interconnected computers. MAX cluster consists of group of modules - each semiautonomous computer. Modules connected {{to each other}} and to other clusters by global bus and circuit-switched communication mesh...|$|R
30|$|The test {{procedure}} can be schematized as it follows: (i) {{the acquisition of}} the environmental temperature and relative humidity; (ii) the laser signal acquisition to verify the parallelism between the acquisition system and the panel (<b>parallelism</b> dimensional <b>tolerance</b> ≤ 0.5  mm); (iii) the acquisition of the laser signal (scanning speed ~  17.00  ±  0.02  mm/s) for detecting the marker edges, {{in order to measure}} the relative distances between each pair of contiguous markers.|$|R
40|$|There {{has been}} {{considerable}} interest in implementing a multithreaded program execution and architecture model on a multiprocessor whose primary processors consist of today's off-the-shelf microprocessors. Unlike some custom-designed multithreaded processor architectures, which can interleave multiple threads concurrently, conventional processors can only execute one thread at a time. This presents {{a unique and}} challenging problem to the compiler: partition a program into threads so that it executes both correctly and in minimal time. We present a new heuristic algorithm based on an interesting extension of the classical list scheduling algorithm. Based on a cost model, our algorithm groups instructions into threads by considering the trade-offs among <b>parallelism,</b> latency <b>tolerance,</b> thread switching costs and sequential execution efficiency. The proposed algorithm has been implemented, and its performance measured through experiments {{on a variety of}} architecture parameters a [...] ...|$|R
40|$|Abstract. MapReduce {{has been}} {{prevalent}} for running data-parallel applications. By hiding other non-functionality parts such as <b>parallelism,</b> fault <b>tolerance</b> and load balance from programmers, MapReduce significantly simplifies the programming of large clusters. Due to the mentioned features of MapReduce above, {{researchers have also}} explored the use of MapReduce on other application domains, such as machine learning, textual retrieval and statistical translation, among others. In this paper, we study the feasibility of running typical supercomputing applications using the MapReduce framework. We port two applications (Water Spatial and Radix Sort) from the Stanford SPLASH- 2 suite to MapReduce. By completely evaluating them in Hadoop, an open-source MapReduce framework for clusters, we analyze the major performance bottleneck {{of them in the}} MapReduce framework. Based on this, we also provide several suggestions in enhancing the MapReduce framework to suite these applications. ...|$|R
40|$|A way of {{incorporating}} the concepls of fU 2. ZY sets into layered neural networks has been described. The inpul {{can be provided}} to quanlitative or linguistic fOnTIS while Ihe outpUI may be modeled as membership values. Logical operators, viz., I-norm T and r-conorm S involving And and Or neurons. are employed al the neuronal level, and the conventional back propagarion algorilhm is accordingly modified using various fuzzy implication operalors. TIle usefulness of Ihe model fOT classification is demonstrated {{on a set of}} vowel data by developing various melhods along wilh Iheir comparison. Effects of fuzZificalion al the Input and outpUI are also investigated. 1. INTRODUCIlON Anificial neural networks 1 - 9 are signal processing systems that try to emulate the human brain, i. e., the behaviour of biological nervous systems, by providing a mathematical model of combination of numerous connected in a network. These models are reputed to have the following characteristics adaptivity (the ability to adjust when given new information), speed (via massive <b>parallelism),</b> fault <b>tolerance</b> (to missing, confusing and/or noisy data) and optimality (as regards error rates I...|$|R
40|$|In {{the past}} few decades, as a new tool for {{analysis}} of the tough geotechnical problems, artificial neural networks (ANNs) have been successfully applied to address a number of engineering problems, including deformation due to tunnelling in various types of rock mass. Unlike the classical regression methods in which a certain form for the approximation function must be presumed, ANNs do not require the complex constitutive models. Additionally, it is traced that the ANN prediction system {{is one of the}} most effective ways to predict the rock mass deformation. Furthermore, it could be envisaged that ANNs would be more feasible for the dynamic prediction of displacements in tunnelling in the future, especially if ANN models are combined with other research methods. In this paper, we summarized the state-of-the-art and future research challenges of ANNs on the tunnel deformation prediction. And the application cases as well as the improvement of ANN models were also presented. The presented ANN models can serve as a benchmark for effective prediction of the tunnel deformation with characters of nonlinearity, high <b>parallelism,</b> fault <b>tolerance,</b> learning, and generalization capability...|$|R
40|$|LH*g is a high-availability {{extension}} of the LH* Scalable Distributed Data Structure. An LH*g file scales up with constant key search and insert performance, while surviving any single-site unavailability (failure). Highavailability is achieved through new principle of record grouping. A group is a logical structure of up to k records, where k is a file parameter. Every group contains a parity record allowing the reconstruction of an unavailable member. The basic schema may be generalized to support unavailability of any number of sites, {{at the expense of}} storage and messaging. Other known high-availability schemes are static, or require more storage, or provide worse search performance. Keywords Scalability, distributed systems, distributed data structures, high-availability, fault <b>tolerance,</b> <b>parallelism,</b> multicomputers 1...|$|R
40|$|Spark {{has been}} {{established}} as an attractive platform for big data analysis, since it manages to hide most of the complexities related to <b>parallelism,</b> fault <b>tolerance</b> and cluster setting from developers. However, this comes {{at the expense of}} having over 150 configurable parameters, the impact of which cannot be exhaustively examined due to the exponential amount of their combinations. The default values allow developers to quickly deploy their applications but leave the question as to whether performance can be improved open. In this work, we investigate the impact of the most important of the tunable Spark parameters on the application performance and guide developers on how to proceed to changes to the default values. We conduct a series of experiments with known benchmarks on the MareNostrum petascale supercomputer to test the performance sensitivity. More importantly, we offer a trial-and-error methodology for tuning parameters in arbitrary applications based on evidence from {{a very small number of}} experimental runs. We test our methodology in three case studies, where we manage to achieve speedups of more than 10 times. Comment: full version of paper accepted in the 2 nd INNS Conference on Big Data 201...|$|R
40|$|AbstractÐLH*g is a high-availability {{extension}} of the LH * Scalable Distributed Data Structure. An LH*g file scales up with constant key search and insert performance, while surviving any single-site unavailability (failure). We achieve high-availability through a new principle of record grouping. A group is a logical structure of up to k records, where k is a file parameter. Every group contains a parity record allowing for the reconstruction of an unavailable member. The basic scheme may be generalized to support the unavailability of any number of sites, {{at the expense of}} storage and messaging. Other known high-availability schemes are static, or require more storage, or provide worse search performance. Index TermsÐScalability, distributed systems, distributed data structures, highavailability, fault <b>tolerance,</b> <b>parallelism,</b> multicomputers. ...|$|R
40|$|With the {{increasing}} availability of heterogeneous networks, {{the coordination of}} distributed services, the communication between different applications, {{and the development of}} distributed programs {{is becoming more and more}} important. In previous work we proposed coordination extensions for existing sequential programming languages like Prolog and C, leading to Prolog&Co 1 (Prolog plus Coordination) and C&Co (C plus Coordination). The coordination extensions provide fine and coarse grained <b>parallelism,</b> software fault <b>tolerance</b> through function replication, and a high-level, reliable communication via communication objects. The semantics of the base language are not destroyed but only slightly expanded by introducing a few new language constructs. The realization of these new coordination languages employs a coordination kernel running in the background and handling all requests concerning communication with other sites. So far, we have implemented the kernel on single proces [...] ...|$|R
40|$|As {{multiprocessor}} systems {{become more}} complex, their reliability {{will need to}} increase as well. In this {{paper we propose a}} novel technique which is applicable {{to a wide variety of}} distributed real-time systems, especially those exhibiting data <b>parallelism.</b> System-level fault <b>tolerance</b> involves reliability techniques incorporated within the system hardware and software whereas application-level fault tolerance involves reliability techniques incorporated within the application software. We assert that, for high reliability, a combination of system-level fault tolerance and application-level fault tolerance works best. In many systems, application-level fault tolerance can be used to bridge the gap when system-level fault tolerance alone does not provide the required reliability. We exemplify this with the RTHT target tracking benchmark and the ABF beamforming benchmark. Keywords: distributed real-time systems, fault tolerance, checkpointing, imprecise computation, target tracking, be [...] ...|$|R
40|$|We {{present a}} {{preliminary}} {{study of a}} thalamo-cortico-thalamic (TCT) implementation on SpiNNaker (Spiking Neural Network architecture), a brain inspired hardware platform designed to incorporate the inherent biological properties of <b>parallelism,</b> fault <b>tolerance</b> and energy efficiency. These attributes make SpiNNaker an ideal platform for simulating biologically plausible computational models. Our focus in this work is to design a TCT framework that can be simulated on SpiNNaker to mimic dynamical behavior similar to Electroencephalogram (EEG) time and power-spectra signatures in sleep-wake transition. The scale of the model is minimized for simplicity in this proof-of-concept study; thus {{the total number of}} spiking neurons is â�� 1000 and represents a "mini-column" of the thalamocortical tissue. All data on model structure, synaptic layout and parameters is inspired from previous studies and abstracted at a level that is appropriate to the aims of the current study as well as computationally suitable for model simulation on a small 4 -chip SpiNNaker system. The initial results from selective deletion of synaptic connectivity parameters in the model show similarity with EEG power spectra characteristics of sleep and wakefulness. These observations provide a positive perspective and a basis for future implementation of a very large scale biologically plausible model of thalamo-cortico-thalamic interactivity-the essential brain circuit that regulates the biological sleep-wake cycle and associated EEG rhythms. Â© 2014 Bhattacharya, Patterson, Galluppi, Durrant and Furber...|$|R
40|$|Today’s Big Data phenomenon, {{characterized}} by huge volumes of data produced {{at very high}} rates by heterogeneous and geographically dispersed sources, is fostering the employment of large-scale distributed systems in order to leverage <b>parallelism,</b> fault <b>tolerance</b> and locality awareness {{with the aim of}} delivering suitable performances. Among the several areas where Big Data is gaining increasing significance, the protection of Critical Infrastructure {{is one of the most}} strategic since it impacts on the stability and safety of entire countries. Intrusion detection mechanisms can benefit a lot from novel Big Data technologies because these allow to exploit much more information in order to sharpen the accuracy of threats discovery. A key aspect for increasing even more the amount of data at disposal for detection purposes is the collaboration (meant as information sharing) among distinct actors that share the common goal of maximizing the chances to recognize malicious activities earlier. Indeed, if an agreement can be found to share their data, they all have the possibility to definitely improve their cyber defenses. The abstraction of Semantic Room (SR) allows interested parties to form trusted and contractually regulated federations, the Semantic Rooms, for the sake of secure information sharing and processing. Another crucial point for the effectiveness of cyber protection mechanisms is the timeliness of the detection, because the sooner a threat is identified, th...|$|R
40|$|Recent {{advances}} {{in the development of}} a fine-grain multi-threaded program execution model based on off-the-shelf microprocessor technology has created an interesting challenge: how to partition a node program into threads that can exploit machine <b>parallelism,</b> achieve latency <b>tolerance,</b> and maintain reasonable locality of reference? A successful algorithm must produce a thread partition that best utilize multiple execution units on a single processing node and handles long and unpredictable latencies. In this paper, we introduce a new thread partitioning algorithm that can meet the above challenge for a range of machine architecture models. A quantitative affinity heuristic is introduced to guide the placement of operations into threads. This heuristic addresses the trade-off between exploiting parallelism and preserving locality. The main algorithm is surprisingly simple and clean due to the use of a time-ordered event list to account for the multiple execution unit activiti [...] ...|$|R
40|$|Artificial neural {{networks}} (ANN), {{due to their}} inherent parallelism and potential fault tolerance offer an attractive paradigm for robust and efficient implementations of functional modules for symbol processing. This paper presents designs of such ANN modules for simulating deterministic finite automata (DFA) and deterministic pushdown automata (DDPA). The designs use an implementation of a class of partially recurrent ANN (modified Elman networks) constructed using a general-purpose binary mapping module (BMP) {{which in turn is}} synthesized from multi [...] layer perceptrons. The paper also discusses some relevant mathematical properties of multi-layer perceptrons that facilitate automated synthesis of BMP modules and points out several potential applications of ANN implementations of DFA and DDPA. 1. Introduction Artificial {{neural networks}} (ANN), due to their inherent <b>parallelism</b> and fault <b>tolerance</b> offer an attractive paradigm for fast and robust implementations of functional m [...] ...|$|R
40|$|Data-flow {{networks}} - or object flow {{networks in}} their more general form - are a suitable coordination language for distributed applications. Their advantages include {{easy to understand}} diagrams, natural <b>parallelism</b> and fault <b>tolerance.</b> Object-flow networks are used for coordination at user level {{in the context of}} the WOS. In addition, the implementation is based on WOS services and resources. A basic framework based on attribute schemes, attributes, WebComs (Web Components) and channels is introduced. WOS specific questions such as usage of warehouses, versioning, service and resource lookup will be addressed as well. As this paper reflects the state of an ongoing project, an outlook presents the future directions. Keywords: Web Operating System, Flow Based Programming, Coordination, Distributed Systems, Concurrency, Communication Models. 1 Introduction Data flow networks are widely recognized as a suitable coordination language for distributed applications. Their advantages that i [...] ...|$|R
40|$|Using Java for {{high-performance}} {{distributed computing}} aggravates a well-known problem: {{the choice between}} efficient message passing environments and more convenient Distributed Shared Memory systems which often provide additional functionalities like adaptive <b>parallelism</b> or fault <b>tolerance</b> [...] -with the latter being imperative for Web-based computing. This paper proposes an extension to the DSM-based Charlotte system that incorporates advantages from both approaches. Annotations are {{used to describe the}} data dependencies of parallel routines. With this information, the runtime system can improve the communication efficiency while still guaranteeing the correctness of the shared memory semantics. If the correctness of these annotations can be relied upon, additional optimizations are possible, ultimately sharing primitive data types such as int across a network, making the overhead associated with accessing and sharing objects unnecessary. In this case, the annotations can be regarded as [...] ...|$|R
40|$|This paper explores some {{algorithms}} for automatic quantization of real-valued datasets using thermometer {{codes for}} pattern classification applications. Experimental {{results indicate that}} a relatively simple randomized thermometer code generation technique can result in quantized datasets that when used to train simple perceptrons, can yield generalization on test data that is substantially better than that obtained with their unquantized counterparts. 1 Introduction Artificial neural networks offer a particularly attractive framework {{for the design of}} pattern classification and inductive knowledge acquisition systems {{for a number of reasons}} including their potential for <b>parallelism</b> and fault <b>tolerance.</b> A single threshold logic unit (TLU), also known as perceptron, is a simple neural network that can be trained to classify a set of input patterns into one of two classes. A TLU is an elementary processing unit that computes a function of the weighted sum of its inputs. Assuming that the [...] ...|$|R
40|$|Spark {{has been}} {{established}} as an attractive platform for big data analysis, since it manages to hide most of the complexities related to <b>parallelism,</b> fault <b>tolerance</b> and cluster setting from developers. However, this comes {{at the expense of}} having over 150 configurable parameters, the impact of which cannot be exhaustively examined due to the exponential amount of their combinations. The default values allow developers to quickly deploy their applications but leave the question as to whether performance can be improved open. In this work, we investigate the impact of the most important tunable Spark parameters with regards to shuffling, compression and serialization on the application performance through extensive experimentation using the Spark-enabled Marenostrum III (MN 3) computing infrastructure of the Barcelona Supercomputing Center. The overarching aim is to guide developers on how to proceed to changes to the default values. We build upon our previous work, where we mapped our experience to a trial-and-error iterative improvement methodology for tuning parameters in arbitrary applications based on evidence from {{a very small number of}} experimental runs. The main contribution of this work is that we propose an alternative systematic methodology for parameter tuning, which can be easily applied onto any computing infrastructure and is shown to yield comparable if not better results than the initial one when applied to MN 3; observed speedups in our validating test case studies start from 20 %. In addition, the new methodology can rely on runs using samples instead of runs on the complete datasets, which render it significantly more practical. Peer ReviewedPostprint (author's final draft...|$|R
30|$|Programming {{languages}} {{have long}} impacted {{the development of}} distributed systems. While much middleware and distributed systems code continues to be developed today using mainstream languages such as Java and C++, several forces have recently combined to drive {{a renewed interest in}} other programming languages. The result of these forces has been an increase in the use of programming languages such as Erlang, Scala, Haskell, and Clojure that allow programming at a higher level of abstraction affording better modularity, enhanced speed of development, and added power of reasoning about systems being developed. Such languages {{can also be used to}} develop embedded domain specific languages that can expressively and succinctly model issues inherent in distributed systems including concurrency, <b>parallelism,</b> and fault <b>tolerance.</b> In this paper, we first present a history of programming languages and distributed systems, and then explore several alternative languages along with modern systems built using them. We focus on language and application features, how problems of distribution are addressed, concurrency issues, code brevity, extensibility, and maintenance concerns. Finally, we speculate about the possible influences today’s alternative programming languages could have on the future of middleware and distributed systems development.|$|R
40|$|Parallel Database Management {{systems are}} the {{dominant}} technology used for large scale data-analysis. The experience of query evaluation techniques used by Database Management Systems {{combined with the}} processing power offered by parallelism {{are some of the}} reasons for the wide use of the technology. On the other hand, MapReduce is a new technology which is quickly spreading and becoming a commonly used tool for processing of large portions of data. The fault <b>tolerance,</b> <b>parallelism</b> and scalability, are only some of the characteristics that the framework can provide to any system based on it. The basic idea behind this work is to modify the query evaluation techniques used by parallel database management systems in order to use the Hadoop MapReduce framework as the underlying execution engine. For the purposes of this work we have focused on join evaluation. We have designed and implemented three algorithms which modify the data-flow of the MapReduce framework in order to simulate the data-flow that parallel Database Management Systems use in order to execute query evaluation. More specifically, we have implemented three algorithms that execute parallel hash join: Simple Hash Join is the implementatio...|$|R
40|$|Energy {{consumption}} in data centers has recently {{become a major}} concern due to the rising operational costs and scalability issues. Recent solutions to this problem propose the principle of energy proportionality, i. e., {{the amount of energy}} consumed by the server nodes must be proportional to the amount of work performed. For data <b>parallelism</b> and fault <b>tolerance</b> purposes, most common file systems used in Map Reduce-type clusters maintain a set of replicas for each data block. A covering set is a group of nodes that together contain at least one replica of the data blocks needed for performing computing tasks. In this work, we develop and analyze algorithms to maintain energy proportionality by discovering a covering set that minimizes energy consumption while placing the remaining nodes in low power standby mode. Our algorithms can also discover covering sets in heterogeneous computing environments. In order to allow more data parallelism, we generalize our algorithms so that it can discover k-covering sets, i. e., a set of nodes that contain at least k replicas of the data blocks. Our experimental results show that we can achieve substantial energy saving without significant performance loss in diverse cluster configurations and working environments...|$|R
40|$|International audienceNowadays, {{large scale}} {{distributed}} systems gather thousands of nodes with hierarchical memory models. They are heterogeneous, volatile and geographically distributed. The efficient exploitation of such systems requires the conception and adaptation of appropriate numerical methods, {{the definition of}} new programming paradigms, new metrics for performance prediction, etc. The modern hybrid numerical methods are well adapted {{to this kind of}} systems. This is particularly because of their multi-level <b>parallelism</b> and fault <b>tolerance</b> property. However the programming of these methods for these architectures requires concurrent reuse of sequential and parallel code. But the currently existing numerical libraries aren't able to exploit the multi-level parallelism offered by theses methods. A few linear algebra numerical libraries make use of object oriented approach allowing modularity and extensibility. Nevertheless, those which offer modularity,sequential and parallel code reuse are almost non-existent. In this paper, we analyze the lacks in existing libraries and propose a design based on a component approach and the strict separation between computation operations, data management and communication control of an application. We present then an application of this design using YML scientific workflow environment ([URL] jointly with the object oriented LAKe (Linear Algebra Kernel) library. Some numerical experiments on GRID 5000 platform validate our approach and show its efficiency...|$|R
40|$|The recent {{proliferation}} {{of computers and}} communication networks {{has made it possible}} for individuals around the world to access a wide variety of information sources through the Internet. However, effective use of these information sources requires fairly sophisticated tools or software agents for locating, classifying, selectively retrieving and extracting knowledge from data. This dissertation addresses several related research issues in the design of such intelligent agents for information retrieval and knowledge discovery from distributed data and knowledge sources;Artificial neural networks, because of their potential for massive <b>parallelism</b> and fault <b>tolerance,</b> offer an attractive approach to the design of intelligent agents. Our work extended several single layer perceptrons and constructive neural networks of perceptrons in order to handle multi-category, real-valued patterns. In particular, we designed DistAl, a novel constructive neural network learning algorithm based on inter-pattern distance. DistAl is significantly faster than conventional neural network algorithms and has been demonstrated to perform well on a broad variety of benchmark data-driven knowledge discovery problems. The performance of DistAl was further improved by using it in conjunction with a genetic algorithm for automated selection of features used to encode the data. DistAl was also used for data-driven refinement of incomplete or inaccurate domain knowledge. Some of these algorithms were used in a design of a multi-agent system consisting of multiple cooperating customizable intelligent mobile agents for selective information retrieval and knowledge discovery from distributed data sources...|$|R
40|$|A {{distributed}} parallel {{technique for}} shared counting that is constructed, {{in a manner}} similar to counting network, from simple one-input two-output computing elements called balancers that are connected to one another by wires to form a balanced binary tree. One can view a balancer as a toggle mechanism, that given a stream of input tokens, repeatedly sends one token to the left output wire and one to the right, effectively balancing the number of tokens that have been output. However, to overcome the problem of sequential bottleneck in the root of the tree, a ”prism ” mechanism is created in front of the toggle bit to diffract each independent pair, one to the left and one to the right, without even having to toggle the shared bit. By distributing the prism over many locations and ensuring that each pair of tokens use different locations, we can get a highly parallel balancer with very low contention. The diffraction mechanism uses randomization to ensure high collision/diffraction rates on the prism, and the tree structure guarantees correctness of the output values. Diffracting trees thus combine the high degree <b>parallelism</b> and fault <b>tolerance</b> of the counting networks with the logarithmic depth, and the beneficial utilization of collisions of a combing tree. 9. 2 Trees That Count A counting tree balancer is a computing element with one input wire and two output wires. We denote by x the number of input tokens ever received on the balancer’s input wire, and by yi,i ∈ { 0, 1 } the number of tokens ever output o...|$|R
40|$|Today, many {{distributed}} systems are deployed in high-performance computing environments {{such as a}} multi-core architecture or a managed network like a data center. As the new computing architectures require more parallelism to improve performance and responsiveness, implementing distributed applications that work consistently in parallel architectures without causing any deadlock or data race issues have become a challenging task. Even more, data center applications must handle fault-tolerance as well because random or correlated crash-restart failures can happen in data centers. Many approaches to solve these issues have been proposed independently to make data center applications to be concurrent, fault-tolerant, or both. Popular applications like graph computing systems or non-relational database systems have their own mechanism to handle concurrency and failures. There are even more generic frameworks that provide both <b>parallelism</b> and fault <b>tolerance</b> in data computing frameworks, message-passing interfaces, and software transactional memory systems. However, making a data center application that works in these generic frameworks may require major restructuring or learning a new paradigm. ^ In this dissertation, we present a solution that provides parallelism, and another solutions that provides fault-tolerance, and both in an event-driven system framework transparently. First, we present InContext, a concurrent event execution model that runs events in parallel by associating access behaviors with the shared variables. Second, we present Ken, an uncoordinated rollback recovery protocol for event-driven systems that can mask crash-restart failures and guarantee composable reliability. We also present MaceKen, integrated with Mace frameworks, that transparently provides crash-restart fault-tolerance for legacy Mace applications. Finally, we propose MultiKen, a combined framework for parallelism and fault-tolerance in event-driven systems. ...|$|R
40|$|Today’s Big Data phenomenon, {{characterized}} by huge volumes of data produced {{at very high}} rates by heterogeneous and geographically dispersed sources, is fostering the employment of large-scale distributed systems in order to leverage <b>parallelism,</b> fault <b>tolerance</b> and locality awareness {{with the aim of}} delivering suitable performances. Among the several areas where Big Data is gaining increasing significance, the protection of Critical Infrastructure {{is one of the most}} strategic since it impacts on the stability and safety of entire countries. Intrusion detection mechanisms can benefit a lot from novel Big Data technologies because these allow to exploit much more information in order to sharpen the accuracy of threats discovery. A key aspect for increasing even more the amount of data at disposal for detection purposes is the collaboration (meant as information sharing) among distinct actors that share the common goal of maximizing the chances to recognize malicious activities earlier. Indeed, if an agreement can be found to share their data, they all have the possibility to definitely improve their cyber defenses. The abstraction of Semantic Room (SR) allows interested parties to form trusted and contractually regulated federations, the Semantic Rooms, for the sake of secure information sharing and processing. Another crucial point for the effectiveness of cyber protection mechanisms is the timeliness of the detection, because the sooner a threat is identified, the faster proper countermeasures can be put in place so as to confine any damage. Within this context, the contributions reported in this thesis are threefold * As a case study to show how collaboration can enhance the efficacy of security tools, we developed a novel algorithm for the detection of stealthy port scans, named R-SYN (Ranked SYN port scan detection). We implemented it in three distinct technologies, all of them integrated within an SR-compliant architecture that allows for collaboration through information sharing: (i) in a centralized Complex Event Processing (CEP) engine (Esper), (ii) in a framework for distributed event processing (Storm) and (iii) in Agilis, a novel platform for batch-oriented processing which leverages the Hadoop framework and a RAM-based storage for fast data access. Regardless of the employed technology, all the evaluations have shown that increasing the number of participants (that is, increasing the amount of input data at disposal), allows to improve the detection accuracy. The experiments made clear that a distributed approach allows for lower detection latency and for keeping up with higher input throughput, compared with a centralized one. * Distributing the computation over a set of physical nodes introduces the issue of improving the way available resources are assigned to the elaboration tasks to execute, with the aim of minimizing the time the computation takes to complete. We investigated this aspect in Storm by developing two distinct scheduling algorithms, both aimed at decreasing the average elaboration time of the single input event by decreasing the inter-node traffic. Experimental evaluations showed that these two algorithms can improve the performance up to 30 %. * Computations in online processing platforms (like Esper and Storm) are run continuously, and the need of refining running computations or adding new computations, together with the need to cope with the variability of the input, requires the possibility to adapt the resource allocation at runtime, which entails a set of additional problems. Among them, the most relevant concern how to cope with incoming data and processing state while the topology is being reconfigured, and the issue of temporary reduced performance. At this aim, we also explored the alternative approach of running the computation periodically on batches of input data: although it involves a performance penalty on the elaboration latency, it allows to eliminate the great complexity of dynamic reconfigurations. We chose Hadoop as batch-oriented processing framework and we developed some strategies specific for dealing with computations based on time windows, which are very likely to be used for pattern recognition purposes, like in the case of intrusion detection. Our evaluations provided a comparison of these strategies and made evident the kind of performance that this approach can provide...|$|R

