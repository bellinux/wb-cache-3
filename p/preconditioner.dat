3454|1598|Public
2500|$|Truncating the sum {{results in}} an [...] "approximate" [...] inverse which may be useful as a <b>preconditioner.</b> Note that a {{truncated}} series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies ...|$|E
50|$|One {{should have}} in mind, however, that {{constructing}} an efficient <b>preconditioner</b> is very often computationally expensive. The increased cost of updating the <b>preconditioner</b> can easily override {{the positive effect of}} faster convergence.|$|E
5000|$|The Jacobi <b>preconditioner</b> {{is one of}} the {{simplest}} forms of preconditioning, in which the <b>preconditioner</b> is chosen to be the diagonal of the matrix [...] Assuming , we get [...] It is efficient for diagonally dominant matrices [...]|$|E
40|$|Introduction A {{decade ago}} Pravin Vaidya {{proposed}} an intriguing family of <b>preconditioners</b> for M-matrices [4]. He presented his ideas in a scientific meeting but never published {{a paper on}} the topic. His <b>preconditioners</b> were never implemented or tested experimentally. We have implemented Vaidya's <b>preconditioners.</b> We experimentally compare the e#ectiveness of Vaidya's <b>preconditioners</b> to that of incomplete-factorization <b>preconditioners,</b> including no-fill and drop-tolerance <b>preconditioners,</b> both modified and unmodified. Our results indicate that Vaidya's <b>preconditioners</b> are often superior to drop-tolerance incomplete-factorization <b>preconditioners,</b> including modified and unmodified, and with various matrix orderings. Vaidya's <b>preconditioners</b> are particularly e#ective on di#cult 2 D problems (e. g. isotropic elliptic problems with Neumann boundary conditions). Vaidya's <b>preconditioners</b> are also more robust. They are not as e#ective on 3 D problems. Vaidya proposed constructing a prec...|$|R
40|$|We {{show that}} a novel class of <b>preconditioners,</b> {{designed}} by Pravin Vaidya in 1991 but never before implemented, is remarkably robust and can outperform incomplete-Cholesky <b>preconditioners.</b> Our test suite includes problems arising from finitedi #erences discretizations of elliptic PDEs in two and three dimensions. On 2 D problems, Vaidya's <b>preconditioners</b> often outperform drop-tolerance incomplete-Cholesky <b>preconditioners</b> with similar amounts of fill and sometimes outperform modified drop-tolerance incomplete <b>preconditioners.</b> Vaidya's <b>preconditioners</b> {{do not appear to}} be e#ective on 3 D problems. Vaidya's <b>preconditioners</b> are robust in the sense that they are insensitive to the boundary conditions of the PDE or to the original ordering of the mesh. 1...|$|R
40|$|In {{this paper}} we present an {{experimental}} study of SPAI (SParse Aproximation of the Inverse) <b>preconditioners.</b> We are specially interested in the SPAI <b>preconditioners</b> performance for convective problems. Two different techniques to generate SPAI <b>preconditioners</b> are analyzed: those based on the minimisation of the residual matrix norm and those based in the nonsymmetric Lanczos method. The effectivity of both SPAI <b>preconditioners</b> is analyzed {{in front of the}} classical incomplete factorisation <b>preconditioners.</b> An elliptic scalar convection-diffusion operator is used to generate the different test cases. As conclusion we determinate that the computational cost of SPAI <b>preconditioners</b> is too large for convective problems. This computational cost makes SPAI <b>preconditioners</b> unusable for practical simulations...|$|R
50|$|One {{can obtain}} a more {{accurate}} <b>preconditioner</b> by allowing some level of extra fill in the factorization. A common choice {{is to use the}} sparsity pattern of A2 instead of A; this matrix is appreciably more dense than A, but still sparse over all. This <b>preconditioner</b> is called ILU(1). One can then generalize this procedure; the ILU(k) <b>preconditioner</b> of a matrix A is the incomplete LU factorization with the sparsity pattern of the matrix Ak+1.|$|E
50|$|The <b>preconditioner</b> matrix M {{has to be}} {{symmetric}} positive-definite and fixed, i.e., cannot {{change from}} iteration to iteration. If any of these assumptions on the <b>preconditioner</b> is violated, {{the behavior of the}} preconditioned conjugate gradient method may become unpredictable.|$|E
5000|$|In {{order for}} the <b>preconditioner</b> to be effective, row and/or column {{permutation}} is usually necessary to move “heavy” elements of [...] close to the diagonal {{so that they are}} covered by the <b>preconditioner.</b> This can be accomplished by computing the weighted spectral reordering of [...]|$|E
40|$|The linear systems {{associated}} with large, sparse, symmetric, positive definite matrices are often solved iteratively using the preconditioned conjugate gradient method. We {{have developed a}} new class of <b>preconditioners,</b> support tree <b>preconditioners,</b> that are based on the connectivity of the graphs corresponding to the matrices and are well-structured for parallel implementation. In this paper, we evaluate the performance of support tree <b>preconditioners</b> by comparing them against two common types of preconditioners: diagonal scaling, and incomplete Cholesky. Support tree <b>preconditioners</b> require less overall storage and less work per iteration than incomplete Cholesky <b>preconditioners.</b> In terms of total execution time, support tree <b>preconditioners</b> outperform both diagonal scaling and incomplete Cholesky <b>preconditioners.</b> ...|$|R
40|$|Circulant {{matrices}} can {{be effective}} <b>preconditioners</b> for linear systems of equations with a Toeplitz matrix. Several approaches to construct such <b>preconditioners</b> have been described in the literature. This paper focuses on the superoptimal circulant <b>preconditioners</b> proposed by Tyrtyshnikov, and investigates a generalization obtained by allowing generalized circulant matrices. Numerical examples illustrate that the new <b>preconditioners</b> so obtained can give faster convergence than available <b>preconditioners</b> based on circulant and generalized circulant matrices. © 2013 IMACS...|$|R
40|$|Abstract. We compare {{a number}} of {{efficient}} <b>preconditioners,</b> published in recent papers, with our own developed SILU method. Some <b>preconditioners</b> are modified {{in order to get}} a more efficient implementation. All <b>preconditioners</b> are applied to a couple of standard benchmark problems. The final goal is to select the best <b>preconditioners</b> for large three- dimensional engineering problems. 1...|$|R
50|$|In applied mathematics, {{symmetric}} successive over-relaxation (SSOR), is a <b>preconditioner.</b>|$|E
5000|$|The SPIKE {{algorithm}} {{can also}} {{function as a}} <b>preconditioner</b> for iterative methods for solving linear systems. To solve a linear system [...] using a SPIKE-preconditioned iterative solver, one extracts center bands from [...] to form a banded <b>preconditioner</b> [...] and solves linear systems involving [...] in each iteration with the SPIKE algorithm.|$|E
50|$|An {{example of}} a {{commonly}} used <b>preconditioner</b> is the incomplete Cholesky factorization.|$|E
40|$|In {{this paper}} two <b>preconditioners</b> for the saddle point problem are analysed: {{one based on}} the {{augmented}} Lagrangian approach and another involving artificial compressibility. Eigenvalue analysis shows that with these <b>preconditioners</b> small condition numbers can be achieved for the preconditioned saddle point matrix. The <b>preconditioners</b> are compared with commonly used <b>preconditioners</b> from literature for the Stokes and Oseen equation and an ocean flow problem. The numerical results confirm the analysis: the <b>preconditioners</b> are a good alternative to existing ones in fluid flow problems. ...|$|R
40|$|Key words: Block <b>preconditioners</b> Runge-Kutta methods, order-optimal methods Abstract. Recently, {{the authors}} {{presented}} different block <b>preconditioners</b> for implicit Runge-Kutta discretization {{of the heat}} equation. The <b>preconditioners</b> were block Jacobi and block Gauss-Seidel preconditoners where the blocks reused existing <b>preconditioners</b> for the implicit Euler discretization of the same equation. In this paper we will introduce similar block <b>preconditioners</b> for the implicit Runge-Kutta discretization of the Bidomain equation. We will, by numerical experiments, show {{the properties of the}} preconditoners, and that higher-order Runge-Kutta discretization of the Bidomain equation may be superior to lower-order in some cases. ...|$|R
40|$|We {{present a}} class of domain {{decomposition}} (DD) <b>preconditioners</b> for the solution of elliptic linear [...] quadratic optimal control problems. Our DD <b>preconditioners</b> are extensions of Neumann-Neumann DD <b>preconditioners,</b> which have been successfully applied to the solution of single partial differential equations...|$|R
5000|$|Choose initial guess , {{two other}} vectors [...] and [...] and a <b>preconditioner</b> ...|$|E
50|$|The SPIKE {{algorithm}} can {{be generalized}} by not restricting the <b>preconditioner</b> to be strictly banded. In particular, the diagonal block in each partition {{can be a}} general matrix and thus handled by a direct general linear system solver rather than a banded solver. This enhances the <b>preconditioner,</b> and hence allows better chance of convergence and reduces the number of iterations.|$|E
5000|$|... {{called the}} {{eigenvector}} residual. If a <b>preconditioner</b> [...] is available, it {{is applied to}} the residual giving vector ...|$|E
40|$|We {{consider}} symmetric {{saddle point}} matrices. We analyze block <b>preconditioners</b> {{based on the}} knowledge of a good approximation for both the top left block and the Schur complement resulting from its elimination. We obtain bounds on the eigenvalues of the preconditioned matrix that depend only {{of the quality of}} these approximations, as measured by the related condition numbers. Our analysis applies to indefinite block diagonal <b>preconditioners,</b> block triangular <b>preconditioners,</b> inexact Uzawa <b>preconditioners,</b> block approximate factorization <b>preconditioners,</b> and a further enhancement of these <b>preconditioners</b> based on symmetric block Gauss-Seidel-type iterations. The analysis is unified and allows the comparison of these different approaches. In particular, it reveals that block triangular and inexact Uzawa <b>preconditioners</b> lead to identical eigenvalue distributions. These theoretical results are illustrated on the discrete Stokes problem. It turns out that the provided bounds allow one to localize accurately both real and nonreal eigenvalues. quality of the different types of <b>preconditioners</b> is also as expected from the theory. © 2014 Society for Industrial and Applied Mathematics. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|AbstractIn this paper, {{we study}} {{the problem of}} reconstructing a {{high-resolution}} image from multiple undersampled, shifted, degraded frames with subpixel displacement errors from multisensors. Preconditioned conjugate gradient methods with cosine transform based <b>preconditioners</b> and incomplete factorization based <b>preconditioners</b> are applied to solve this image reconstruction problem. Numerical examples are given to demonstrate the efficiency of these <b>preconditioners.</b> We find that cosine transform based <b>preconditioners</b> are effective {{when the number of}} shifted low-resolution frames are large, but are less effective when the number is small. However, incomplete factorization based <b>preconditioners</b> work quite well independent of the number of shifted low-resolution frames...|$|R
40|$|We study <b>preconditioners</b> for a model problem {{describing}} the coupling of two elliptic subproblems posed over domains with different topological dimension by a parameter dependent constraint. A pair of parameter robust and efficient <b>preconditioners</b> is proposed and analyzed. Robustness {{and efficiency of}} the <b>preconditioners</b> is demonstrated by numerical experiments...|$|R
5000|$|For a {{symmetric}} {{positive definite matrix}} [...] the <b>preconditioner</b> [...] {{is typically}} chosen to be symmetric positive definite as well. The preconditioned operator [...] is then also symmetric positive definite, but {{with respect to the}} -based scalar product. In this case, the desired effect in applying a <b>preconditioner</b> is to make the quadratic form of the preconditioned operator [...] with respect to the -based scalar product to be nearly spherical http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf.|$|E
5000|$|If the {{original}} matrix can be split into diagonal, lower and upper tridiagonal as [...] then SSOR <b>preconditioner</b> matrix {{is defined as}} ...|$|E
5000|$|The <b>preconditioner</b> [...] must be {{symmetric}} positive definite. Note {{that the}} residual vector here {{is different from}} the residual vector without preconditioning.|$|E
40|$|In this paper, {{we propose}} {{approximate}} inverse-free <b>preconditioners</b> for solving Toeplitz systems. The <b>preconditioners</b> are constructed {{based on the}} famous Gohberg-Semencul formula. We show that if a Toeplitz matrix is generated by a positive bounded function and its entries enjoys the off-diagonal decay property, then the eigenvalues of the preconditioned matrix are clustered around one. Experimental {{results show that the}} proposed <b>preconditioners</b> are superior to other existing <b>preconditioners</b> in the literature. © 2011 Elsevier Inc. All rights reserved. link_to_subscribed_fulltex...|$|R
40|$|Incomplete {{factorization}} preconditioning {{is one of}} {{the most}} efficient and rebust preconditioning techniques in the conjugate gradient method for solving systems of linear equations. To improve the performance of the incomplete factorization <b>preconditioners</b> for difficult problems, extra fill-ins are often used. In this thesis, we study the effect of using different ordering schemes and more accurate incomplete factorization <b>preconditioners</b> in the solution process. It is found that the incomplete factorization <b>preconditioners</b> that drop fill-in by its magnitude are often better than <b>preconditioners</b> that drop fill-in by the size of their residual. Furthermore, the relative preformance of different orderings may change as the accuracy of the <b>preconditioners</b> improves...|$|R
40|$|Abstract. We {{present a}} {{preconditioning}} technique, called support-graph preconditioning, {{and use it}} to analyze two classes of <b>preconditioners.</b> The technique was first described in a talk by Pravin Vaidya, who did not formally publish his results. Vaidya used the technique to devise and analyze a class of novel <b>preconditioners.</b> The technique was later extended by Gremban and Miller, who used it in the development and analysis of yet another class of new <b>preconditioners.</b> This paper extends the technique further and uses it to analyze a class of existing <b>preconditioners,</b> modified incomplete Cholesky. The paper also contains a presentation of Vaidya’s <b>preconditioners,</b> which was previously missing from the literature...|$|R
50|$|In {{numerical}} linear algebra, {{an incomplete}} LU factorization of a matrix is a sparse approximation of the LU factorization {{often used as}} a <b>preconditioner.</b>|$|E
5000|$|Typically {{there is}} a {{trade-off}} {{in the choice of}} [...] Since the operator [...] must be applied at each step of the iterative linear solver, it should have a small cost (computing time) of applying the [...] operation. The cheapest <b>preconditioner</b> would therefore be [...] since then [...] Clearly, this results in the original linear system and the <b>preconditioner</b> does nothing. At the other extreme, the choice [...] gives [...] which has optimal condition number of 1, requiring a single iteration for convergence; however in this case [...] and applying the <b>preconditioner</b> is as difficult as solving the original system. One therefore chooses [...] as somewhere between these two extremes, in an attempt to achieve a minimal number of linear iterations while keeping the operator [...] as simple as possible. Some examples of typical preconditioning approaches are detailed below.|$|E
5000|$|Both {{of systems}} {{give the same}} {{solution}} as the original system {{so long as the}} <b>preconditioner</b> matrix [...] is nonsingular. The left preconditioning is more common.|$|E
40|$|In this paper, {{we study}} {{the problem of}} reconstructing a {{high-resolution}} image from multiple undersampled, shifted, degraded frames with subpixel displacement errors from multisensors. Preconditioned conjugate gradient methods with cosine transform based <b>preconditioners</b> and incomplete factorization based <b>preconditioners</b> are applied to solve this image reconstruction problem. Numerical examples are given to demonstrate the efficiency of these <b>preconditioners.</b> We find that cosine transform based <b>preconditioners</b> are effective {{when the number of}} shifted low-resolution frames are large, but are less effective when the number is small. However, incomplete factorization based <b>preconditioners</b> work quite well independent of the number of shifted low-resolution frames. © 2004 Elsevier Inc. All rights reserved. link_to_subscribed_fulltex...|$|R
40|$|Abstract. In this paper, {{based on}} the <b>preconditioners</b> {{presented}} by Wang [On hybrid preconditioning methods for large sparse saddle-point problems, Linear Algebra Appl., 434 (2011) 2353 - 2366], we introduce and study the block symmetric SOR (BSSOR) and the modified block symmetric SOR (MBSSOR) <b>preconditioners</b> for the block traingular product approximation to a 2 -by- 2 block matrix and these <b>preconditioners</b> are the generalization of BSGS and MBSGS <b>preconditioners</b> presented by Wang. Moreover, we analyse {{the properties of the}} corresponding preconditioned matrices...|$|R
40|$|This paper {{analyses}} a novel {{method for}} constructing <b>preconditioners</b> for diagonally dominant symmetric positive-de nite matrices. The method discussed here {{is based on}} a simple idea: we construct M by simply dropping o diagonal non-zeros from A and modifying the diagonal elements to maintain a certain row-sum property. The <b>preconditioners</b> are extensions of Vaidya’s augmented maximum-spanning-tree <b>preconditioners.</b> The <b>preconditioners</b> presented here were also mentioned by Vaidya in an unpublished manuscript, but without a complete analysis. The <b>preconditioners</b> that we present have only O(n + t²) nonzeros, where n is the dimension of the matrix and 16 t 6 n is a parameter that one can choose. Their construction is efficient and guarantees that the condition number of the preconditioned system is O(n²/t²) if the number of nonzeros per row in the matrix is bounded by a constant. We have developed an efficient algorithm to construct these <b>preconditioners</b> and we have implemented it. We used our implementation to solve a simple model problem; we show the combinatorial structure of the <b>preconditioners</b> and we present encouraging convergence results...|$|R
