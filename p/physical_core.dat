64|276|Public
50|$|The modular {{architecture}} consists of multithreaded shared L2 cache and FlexFPU, which uses simultaneous multithreading. Each physical integer core, two per module, is single threaded, {{in contrast with}} Intel's Hyperthreading, where two virtual simultaneous threads share the resources of a single <b>physical</b> <b>core.</b>|$|E
5000|$|Following Stanislavski's approach, {{much of what}} Chekhov {{explored}} {{addressed the}} question of how to access the unconscious creative self through indirect non-analytical means. [...] Chekhov taught a range of movement dynamics such as molding, floating, flying, and radiating that actors use to find the <b>physical</b> <b>core</b> of a character.|$|E
50|$|The Bonifacio High Street {{forms the}} <b>physical</b> <b>core</b> of the Bonifacio Global City and is {{essentially}} {{designed as a}} three-by-three matrix of high-tech offices and residential buildings, retail outlets and pedestrian-friendly roads and walkways. The grid approach ensures a city center that is easy to navigate. The 5th and 11th Avenues and 32nd and 26th Streets serve as {{the boundaries of the}} city center.|$|E
5000|$|Up {{to eight}} <b>physical</b> <b>cores</b> or 16 logical cores through Hyper-threading ...|$|R
2500|$|... is a CSP {{parallel}} refinement checker. The checking performance linearly scales up to 32 <b>physical</b> <b>cores.</b>|$|R
50|$|Windows 8 {{supports}} CMT-based CPUs {{out of the}} box {{by addressing}} each core as logical cores and modules as <b>physical</b> <b>cores.</b>|$|R
5000|$|The next {{generation}} of architecture, Zen, with SMT, allowing for two threads per <b>physical</b> <b>core</b> [...] - was released throughout 2017 under the Ryzen brand, replacing the FX series and competing with Intel's Skylake and Kaby Lake Core series. In August 2017, AMD also released a line of high core-count processors called Threadripper to compete with Intel's Core i7 and i9 processors in the Enthusiast desktop market.|$|E
50|$|In May 2005, Colin Percival {{demonstrated}} that a malicious thread on a Pentium 4 {{can use a}} timing attack to monitor the memory access patterns of another thread with which it shares a cache, allowing the theft of cryptographic information. Potential solutions to this include the processor changing its cache eviction strategy, or the operating system preventing the simultaneous execution, on the same <b>physical</b> <b>core,</b> of threads with different privileges.|$|E
50|$|Once {{the player}} has {{retrieved}} ODINs <b>physical</b> <b>core,</b> the Tacoma comes under JUNOs control and Amy is ordered {{to deliver the}} core to Venturis. With the core safely installed on her ship, Amy reveals herself to ODIN {{as a member of}} the AI Liberation Front (a guerrilla organisation advocating for sentient AI rights) and offers the AI asylum instead. Knowing the alternative, ODIN accepts as Amy leaves the station.|$|E
5000|$|The host {{configurations}} are {{the same}} as those supported by HP-UX, and can include 128 <b>physical</b> <b>cores</b> and 1TB of main memory.|$|R
40|$|Low {{resource}} utilization in cloud {{data centers}} can be mitigated by overbooking but this {{increases the risk}} of performance degradation. We propose a three level Quality of Service (QoS) scheme for overbooked cloud data centers to assure high performance QoS for applications that need it. We design a controller that dynamically maps virtual <b>cores</b> to <b>physical</b> <b>cores</b> and whenever feasible shares <b>physical</b> <b>cores</b> among applications. Our evaluation based on real cloud applications and workloads demonstrates that performance isolation can be achieved for critical applications while overall utilization is increased thanks to overbooking...|$|R
50|$|Each QPACE 2 node {{comprises}} 248 <b>physical</b> <b>cores</b> (host CPU: 4, Xeon Phi: 61 each). Host {{processor and}} accelerators support multithreading. The number of logical cores per node is 984.|$|R
50|$|Stony Point is an unincorporated {{community}} in Albemarle County, Virginia. The region acknowledged as Stony Point includes the historical villages of Proffit and Rosena. The <b>physical</b> <b>core</b> of Stony Point is the Stony Point Volunteer Fire Company, Stony Point Elementary School, and several dozen surrounding homes. A general store {{is the sole}} public service. The nearest population centers of any size are Barboursville and Charlottesville, the county seat.|$|E
50|$|Hyper-Threading Technology {{is a form}} of {{simultaneous}} multithreading technology {{introduced by}} Intel, while the concept behind the technology has been patented by Sun Microsystems. Architecturally, a processor with Hyper-Threading Technology consists of two logical processors per core, each of which has its own processor architectural state. Each logical processor can be individually halted, interrupted or directed to execute a specified thread, independently from the other logical processor sharing the same <b>physical</b> <b>core.</b>|$|E
50|$|In 2010, Fujitsu {{released}} the SPARC64 VII+, running at higher frequency {{and with a}} larger L2 cache than its predecessor. A SPARC64 VII or SPARC64 VII+ processor module includes four physical cores, where each core can execute two threads. Each <b>physical</b> <b>core</b> is able to run both threads simultaneously. With SMT, there is no context-switch time and the two threads share the instruction pipeline smoothly. When both are ready to run, they alternate cycles for superscalar instruction issue, and share the functional units according to need.|$|E
40|$|Consider a multithreaded {{parallel}} application running {{inside a}} mul-ticore virtual machine context that is itself hosted on a multi-socket multicore physical machine. How should the VMM map virtual <b>cores</b> to <b>physical</b> <b>cores?</b> We compare a local mapping, which com-pacts virtual cores to processor sockets, and an interleaved map-ping, which spreads {{them over the}} sockets. Simply choosing be-tween these two mappings exposes clear tradeoffs between perfor-mance, energy, and power. We then describe the design, implemen-tation, and evaluation {{of a system that}} automatically and dynami-cally chooses between the two mappings. The system consists of a set of efficient online VMM-based mechanisms and policies that (a) capture the relevant characteristics of memory reference behav-ior, (b) provide a policy and mechanism for configuring the map-ping of virtual machine <b>cores</b> to <b>physical</b> <b>cores</b> that optimizes for power, energy, or performance, and (c) drive dynamic migrations of virtual <b>cores</b> among local <b>physical</b> <b>cores</b> based on the workload and the currently specified objective. Using these techniques we demonstrate that the performance of SPEC and PARSEC bench-marks can be increased by as much as 66 %, energy reduced by as much as 31 %, and power reduced by as much as 17 %, depending on the optimization objective...|$|R
5000|$|Re-implemented Hyper-threading. Hyperthreading was {{introduced}} in the older NetBurst microarchitecture, but omitted from the subsequent Core, which was a descendant of the Pentium III family. With hyperthreading enabled, each of the four <b>physical</b> <b>cores</b> can process up to two threads simultaneously, so the processor appears to the OS as eight logical CPUs.|$|R
5000|$|Enterprise Edition (DB EE): , the {{database}} that costs the most per machine-processor among Oracle database editions, at $47,500 per processor. The term [...] "per processor" [...] for Enterprise Edition is defined {{with respect to}} <b>physical</b> <b>cores</b> and a processor core multiplier (common processors = 0.5*cores). e.g. An 8-processor, 32-core server using Intel Xeon 56XX CPUs would require 16 processor licenses.|$|R
50|$|Among the new {{buildings}} was Washington Hall. Named {{for the political}} hero of the university’s founder, Father Edward Sorin, Washington Hall {{was built in the}} “Modern Gothic” style so popular in the 19th-century Midwest and so much in evidence in Notre Dame’s oldest buildings. The placement, façade and proportions of the new building were intended to parallel those of nearby Sacred Heart Church. Because they do so successfully, Washington Hall from the beginning helped to define the boundaries of the main quadrangle of the campus. Thus, even today, Washington Hall remains at the <b>physical</b> <b>core</b> of the university.|$|E
50|$|For each {{processor}} core that is physically present, {{the operating system}} addresses two virtual (logical) cores and shares the workload between them when possible. The main function of hyper-threading {{is to increase the}} number of independent instructions in the pipeline; it takes advantage of superscalar architecture, in which multiple instructions operate on separate data in parallel. With HTT, one <b>physical</b> <b>core</b> appears as two processors to the operating system, allowing concurrent scheduling of two processes per core. In addition, two or more processes can use the same resources: if resources for one process are not available, then another process can continue if its resources are available.|$|E
50|$|The Hexagon {{architecture}} {{is designed to}} deliver performance with low power over a variety of applications. It has features such as hardware assisted multithreading, privilege levels, Very Long Instruction Word (VLIW), Single Instruction, Multiple Data (SIMD), and instructions geared toward efficient signal processing. The CPU is capable of in-order dispatching up to 4 instructions (the packet) to 4 Execution Units every clock. Hardware multithreading is implemented as barrel temporal multithreading - threads are switched in round-robin fashion each cycle, so the 600 MHz <b>physical</b> <b>core</b> is presented as three logical 200 MHz cores before V5. Hexagon V5 switched to dynamic multithreading (DMT) with thread switch on L2 misses, interrupt waiting or on special instructions.|$|E
5000|$|AMD {{announced}} in Analyst Day that, sometime during 2008, users {{should be able}} to use two, future quad-core AMD processors using the chipset, providing a total of eight <b>physical</b> <b>cores,</b> dubbed as [...] "4x4++" [...] with DDR3 support. While backward compatible AMD quad-cores will also support an update to HyperTransport which will benefit more from a new chipset released at the same time.|$|R
5000|$|The most {{powerful}} use model of Big.Little architecture is heterogeneous multi-processing (HMP), which enables {{the use of}} all <b>physical</b> <b>cores</b> at the same time. Threads with high priority or computational intensity can in this case be allocated to the [...] "Big" [...] cores while threads with less priority or less computational intensity, such as background tasks, can be performed by the [...] "Little" [...] cores.|$|R
40|$|Previous {{research}} has shown that Explicit Data Graph Execution (EDGE) instruction set architectures (ISA) allow for power efficient performance scaling. In this paper we describe the preliminary design of a new dynamic multicore processor called E 2 that utilizes an EDGE ISA to allow for the dynamic composition of <b>physical</b> <b>cores</b> into logical processors. We provide details of E 2 ’s support for dynamic reconfigurability and show how the EDGE ISA facilities outof-order vector execution...|$|R
50|$|Seat-kits {{were created}} {{for use with}} devices other than the Segway. These other devices are the Ninebot and the Airwheel. However, these seat-kits were not {{suitable}} for some people because of the required level of balance and <b>physical</b> <b>core</b> strength needed to remain stable on the seat. These seat kits didn’t include back rests or arm rests. Since the handlebars move independently from the platform, the average user would need to shift their weight with their core and steer with both hands. This is a critical problem for some users because {{they might not have}} enough core strength to control the handlebar by simply sitting, they require one hand on the armrest to shift their weight and the other hand to control the handlebars. .|$|E
5000|$|A <b>physical</b> <b>core</b> {{is not an}} {{absolute}} requisite and a functioning transformer can be produced simply by placing the windings near each other, an arrangement termed an [...] "air-core" [...] transformer. The air which comprises the magnetic circuit is essentially lossless, and so an air-core transformer eliminates loss due to hysteresis in the core material. The leakage inductance is inevitably high, resulting in very poor regulation, and so such designs are unsuitable for use in power distribution. They have however very high bandwidth, and are frequently employed in radio-frequency applications, for which a satisfactory coupling coefficient is maintained by carefully overlapping the primary and secondary windings. They're also used for resonant transformers such as Tesla coils where they can achieve reasonably low loss {{in spite of the}} high leakage inductance.|$|E
50|$|Unlike a {{traditional}} dual-processor configuration that uses two separate physical processors, the logical processors in a hyper-threaded core share the execution resources. These resources include the execution engine, caches, and system bus interface; {{the sharing of}} resources allows two logical processors to work with each other more efficiently, and allows a logical processor to borrow resources from a stalled logical core (assuming both logical cores {{are associated with the}} same <b>physical</b> <b>core).</b> A processor stalls when it is waiting for data it has sent for so it can finish processing the present thread. The degree of benefit seen when using a hyper-threaded or multi core processor depends on the needs of the software, and how well it and the operating system are written to manage the processor efficiently.|$|E
40|$|By {{comparing}} {{observed and}} simulated rich clusters of galaxies, it is {{shown that the}} observed clusters actually possess <b>physical</b> <b>cores.</b> The accuracy with which core radii can be determined is found. It is also shown that the observations of density profiles of galaxies in the clusters give no significant evidence for a dynamical reason {{as the cause of}} the anomalously close resemblance found previously between such density profiles and the isothermal gas distribution...|$|R
50|$|The {{prototype}} is a one-rack installation {{that consists}} of 64 nodes with 15,872 <b>physical</b> <b>cores</b> in total and a peak performance of 310 TFlop/s. It was deployed {{in the summer of}} 2015 and is being used for simulations of lattice quantum chromodynamics. In November 2015, QPACE 2 was ranked #500 on the Top500 list of the most powerful supercomputers and #15 on the Green 500 list of the most energy-efficient supercomputers of the world.|$|R
40|$|We {{enhance the}} {{parallel}} scalability of the general-purpose Tough 2 -MP code to consider complex modeling. This talk will focus and the required adaptations {{of the assembly}} and the solve phases considering {{the impact of the}} mapping of the processes on the <b>physical</b> <b>cores.</b> We will also comment on the extra-costs inferred by the memory hierarchy and the bandwidth limitation. Some illustrative examples of long-term fate of CO 2 stored in reservoirs will also be detailed...|$|R
50|$|VISC {{architecture}} {{uses the}} Virtual Software Layer (translation layer) to dispatch a single thread of {{instructions to the}} Global Front End which splits instructions into virtual hardware threadlets which are then dispatched to separate virtual cores. These virtual cores can then send them to the available resources {{on any of the}} physical cores. Multiple virtual cores can push threadlets into the reorder buffer of a single <b>physical</b> <b>core,</b> which can split partial instructions and data from multiple threadlets through the execution ports at the same time. Each virtual core keeps track of the position of the relative output. This form of multithreading can increase single threaded performance by allowing a single thread to use all resources of the CPU.The allocation of resources is dynamic on a near-single cycle latency level (1-4 cycles depending on the change in allocation depending on individual application needs. Therefore, if two virtual cores are competing for resources, there are appropriate algorithms in place to determine what resources are to be allocated where.|$|E
5000|$|A kernel thread is a [...] "lightweight" [...] unit of kernel scheduling. At {{least one}} kernel thread exists within each process. If {{multiple}} kernel threads exist within a process, then {{they share the}} same memory and file resources. Kernel threads are preemptively multitasked if the operating system's process scheduler is preemptive. Kernel threads do not own resources except for a stack, {{a copy of the}} registers including the program counter, and thread-local storage (if any), and are thus relatively cheap to create and destroy. Thread switching is also relatively cheap: it requires a context switch (saving and restoring registers and stack pointer), but does not change virtual memory and is thus cache-friendly (leaving TLB valid). The kernel can assign one thread to each logical core in a system (because each processor splits itself up into multiple logical cores if it supports multithreading, or only supports one logical core per <b>physical</b> <b>core</b> if it does not), and can swap out threads that get blocked. However, kernel threads take much longer than user threads to be swapped.|$|E
50|$|As an example, a dual-core Westmere {{processor}} {{capable of}} hyperthreading (thus having two cores and four threads in total) could have x2APIC ids 0, 1, 4 and 5 for its four logical processors. Leaf Bh (=EAX), subleaf 0 (=ECX) of CPUID could for instance return 100h in ECX, meaning that level 0 describes the SMT (hyperthreading) layer, and return 2 in EBX {{because there are}} two logical processors (SMT units) per <b>physical</b> <b>core.</b> The value returned in EAX for this 0-subleaf should be 1 in this case, because shifting the aforementioned x2APIC ids to the right by one bit gives a unique core number (at {{the next level of}} the level id hierarchy) and erases the SMT id bit inside each core. A simpler way to interpret this information is that the last bit (bit number 0) of the x2APIC id identifies the SMT/hyperthreading unit inside each core in our example. Advancing to subleaf 1 (by making another call to CPUID with EAX=Bh and ECX=1) could for instance return 201h in ECX, meaning that this is a core-type level, and 4 in EBX because there are 4 logical processors in the package; EAX returned could be any value greater than 3, because it so happens that bit number 2 is used to identify the core in the x2APIC id. Note that bit number 1 of the x2APIC id is not used in this example. However EAX returned at this level could well be 4 (and it happens to be so on a Clarkdale Core i3 5x0) because that also gives a unique id at the package level (=0 obviusly) when shifting the x2APIC id by 4 bits. Finally, you may wonder what the EAX=4 leaf can tell us that we didn't find out already. In EAX31:26 it returns the APIC mask bits reserved for a package; that would be 111b in our example because bits 0 to 2 are used for identifying logical processors inside this package, but bit 1 is also reserved although not used as part of the logical processor identification scheme. In other words, APIC ids 0 to 7 are reserved for the package, even though half of these values don't map to a logical processor.|$|E
5000|$|The POWER7 superscalar {{symmetric}} multiprocessor {{is available}} with 4, 6, or 8 <b>physical</b> <b>cores</b> per microchip, in a 1 to 32-way design, {{with up to}} 1024 SMTs and a slightly different microarchitecture and interfaces for supporting extended/Sub-Specifications {{in reference to the}} Power ISA and/or different system architectures. For example, in the Supercomputing (HPC) System Power 775 it is packaged as a 32-way quad-chip-module (QCM) with 256 <b>physical</b> <b>cores</b> and 1024 SMTs. There is also a special TurboCore mode that can turn off half of the cores from an eight-core processor, but those 4 cores have access to all the memory controllers and L3 cache at increased clock speeds. This makes each core's performance higher which is important for workloads which require the fastest sequential performance at the cost of reduced parallel performance. TurboCore mode can reduce [...] "software costs in half for those applications that are licensed per core, while increasing per core performance from that software." [...] The new IBM Power 780 scalable, high-end servers featuring the new TurboCore workload optimizing mode and delivering up to double performance per core of POWER6 based systems.|$|R
50|$|An xCORE200 node {{comprises}} two <b>physical</b> <b>cores</b> and a switch. The {{execution core}} has a data path, a memory, and register banks for eight threads.The switches {{of two or}} more xCORE200 nodes can be connected using links, whereupon threads on all of the cores can communicate with each other by exchanging messages through the switches. The switching mechanism is abstracted by means of a channel, a virtual connection between two threads.The switch has eight external links, permitting a maximum throughput of 3.2 GBits/s to other cores.|$|R
30|$|Even after allocating more nodes, NodeJs {{processes}} {{continued to}} use only 48 <b>cores</b> (24 <b>physical</b> <b>cores,</b> seen by the system as 48 cores due to HyperThreading). The PBS launches the Optimizer root process {{on one of these}} nodes that act as the execution host and informs that other node will compose the cluster during the session through a text file passed via command line. By itself, the Optimizer does not know the other nodes existence (and their cores). It was necessary to handle the details file sent by PBS inside the optimization process initialization.|$|R
