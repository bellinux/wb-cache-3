20|1359|Public
50|$|Every {{declaration}} has a parametric equivalent. All <b>procedure</b> <b>parameter</b> types must be specified. Any procedure {{passed as}} a parameter has its full type specified (in contrast to Pascal) {{and the same}} is true for a structure class.|$|E
5000|$|The {{procedure}} {{begins with}} a mandatory heading part to hold the procedure name and optionally the <b>procedure</b> <b>parameter</b> list. Next come the declarative, executable and exception-handling parts, as in the PL/SQL Anonymous Block. A simple procedure might look like this: ...|$|E
50|$|Although {{it would}} be {{possible}} to infer <b>procedure</b> <b>parameter</b> and return types by examining where the procedure is called, S-algol does require parameter and return types to be specified. This is a practical decision, since it should be possible to understand a procedure without examining its calls.|$|E
5000|$|Stored Procedure Group Map OPCdata to {{and from}} SQL stored <b>procedure</b> <b>parameters.</b>|$|R
5000|$|Dynamic extents for arrays and strings with {{inheritance}} of extents by <b>procedure</b> <b>parameters.</b>|$|R
40|$|Abstract. We {{introduce}} a new algebraic model for program variables, suitable for reasoning about recursive <b>procedures</b> with <b>parameters</b> and local variables in a mechanical verification setting. We give a predicate transformer semantics to recursive procedures and prove refinement rules for introducing recursive <b>procedure</b> calls, <b>procedure</b> <b>parameters,</b> and local variables. We also prove, based on the refinement rules, Hoare total correctness rules for recursive <b>procedures,</b> and <b>parameters.</b> We {{introduce a}} special form of Hoare specification statement which alone is enough to fully specify a procedure. Moreover, we prove that this Hoare specification statement is equivalent to a refinement specification. We implemented this theory in the PVS theorem prover...|$|R
50|$|As the {{detection}} of errors in data is achieved through duplicating all variables and adding consistency checks after every read operation, special considerations have to be applied according to the procedure interfaces. Parameters passed to procedures, as well as return values, {{are considered to be}} variables. Hence, every <b>procedure</b> <b>parameter</b> is duplicated, as well as the return values. A procedure is still called only once, but it returns two results, which must hold the same value. The source listing to the right shows a sample implementation of function parameter duplication.|$|E
50|$|SCL is {{designed}} to allow both line-at-a-time interactive use from a console or from a command file, and creation of executable scripts or programs (when the language is compiled into object module format {{in the same way}} as any other VME programming language). The declaration of a procedure within SCL also acts as the definition of a simple form or template allowing the procedure to be invoked from an interactive terminal, with fields validated according to the data types of the underlying procedure parameters or using the default <b>procedure</b> <b>parameter</b> values.|$|E
5000|$|Speaking {{as someone}} who has delved into the intricacies of PL/I, I am sure that only Real Men could have written such a machine-hogging, cycle-grabbing, all-encompassing monster. Allocate an array and free the middle third? Sure! Why not? Multiply a {{character}} string times a bit string and assign the result to a float decimal? Go ahead! Free a controlled variable <b>procedure</b> <b>parameter</b> and reallocate it before passing it back? Overlay three different types of variable on the same memory location? Anything you say! Write a recursive macro? Well, no, but Real Men use rescan. How could a language so obviously designed and written by Real Men not be intended for Real Man use? ...|$|E
40|$|The {{last chapter}} {{described}} {{how to create}} <b>procedures,</b> pass <b>parameters,</b> and allocate and access local variables. This chapter picks up where that one left off and describes how to access non-local variables in other procedures, pass <b>procedures</b> as <b>parameters,</b> and implement some user-defined control structures...|$|R
40|$|We {{present a}} shallow {{embedding}} in PVS of a predicate transformer semantics of an imperative language suitable for reasoning about recursive <b>procedures</b> with <b>parameters</b> and local variables. We use the PVS dependent type mechanism for implementing program variables of different types. We use an uninterpreted state space and define the program variables behavior {{by means of}} certain tree functions {{that are supposed to}} satisfy some axioms. Unlike in the implementations mentioned in the literature, we do not need to change the state space when adding local variables or <b>procedure</b> <b>parameters.</b> ...|$|R
50|$|The {{configuration}} of processes, monitors, and classes in a Concurrent Pascal program is normally {{established at the}} start of execution, and is not changed thereafter. The communication paths between these components are established by variables passed in the init statements, since class and monitor instance variables cannot be used as <b>procedure</b> <b>parameters.</b>|$|R
5000|$|Some {{programming}} languages {{that have}} this feature may allow or require a complete type declaration for each procedural parameter f, including {{the number and}} type of its arguments, {{and the type of}} its result, if any. For example, in the C programming language the example above could be written as int P ( [...] int f (int a, int b) [...] ) { return f(6,3) * f(2,1); }In principle, the actual function actf that is passed as argument when P is called must be type-compatible with the declared type of the <b>procedure</b> <b>parameter</b> f. This usually means that actf and f must return the same type of result, must have the same number of arguments, and corresponding arguments must have the same type. The names of the arguments need not be the same, however, as shown by the plus and quot examples above. However, some programming languages may me more restrictive or more liberal in this regard.|$|E
40|$|Evidence of meat {{trade in}} the form of table cuts {{suggests}} that consumer preferences and tastes vary across meat cuts. Unlike previous studies, this paper estimates demand elasticities at the table cut level from a Mexican survey of household incomes and expenditures, which is a stratified sample. The study uses the two-step estimation of a censored demand system proposed by Shonkwiler and Yen (1999) but incorporates stratification variables into the estimation <b>procedure.</b> <b>Parameter</b> estimates are reported and its standard errors are approximated by using the bootstrap procedure. censored demand system, two-step estimation procedure, stratified sampling, Mexican meat demand, elasticities, adult equivalent scales, bootstrap standard errors, Consumer/Household Economics, Demand and Price Analysis, Food Consumption/Nutrition/Food Safety, Q 11,...|$|E
40|$|A {{parameter}} estimation {{problem for a}} condition-based maintenance model is considered. We model a failing system that can be in a healthy or unhealthy operational state, or in a failure state. System deterioration is assumed to follow a hidden, three-state continuous time Markov process. Vector autoregressive data are obtained through condition monitoring at discrete time points, which gives partial information about the unobservable system state. Two kinds of data histories are considered: histories that end with observable system failure and histories that end when the system is suspended from operation but has not failed. Maximum likelihood estimates of the model parameters are obtained using the EM algorithm and a closed form expression for the pseudo-likelihood function is derived. Numerical results are provided which illustrate the estimation <b>procedure.</b> <b>Parameter</b> estimation Partially observable failing systems Hidden Markov modeling Vector time series EM algorithm...|$|E
30|$|The signal’s {{occurrence}} {{rates from}} any automatic detection algorithm {{depend on the}} selection <b>procedure,</b> <b>parameters</b> of detection, and the interference level at an observatory. After the set of parameters is fixed, a valid procedure should lead to results that coincide qualitatively with known from other methods, {{at least for a}} well-known object like daytime Pc 3 pulsations.|$|R
40|$|This paper {{presents}} {{a model for}} a network of communicating processes. We extend well known ideas in sequentia] programming such as <b>procedures,</b> <b>parameter</b> passing and binding, and recursion to distributed programs. We stress the notion of implementation-hiding, i. e. the invoker of a process or procedure has no knowledge of the implementation of the invoked computation. I...|$|R
5000|$|... the {{principle}} of correspondence : The rules governing names should be uniform and apply everywhere. This primarily applies to correspondence between declarations and <b>procedure</b> <b>parameters,</b> including consideration of all parameter passing modes. This principle was examined by R. D. Tennent in conjunction with Pascal, and {{has its roots in}} work by Peter Landin and Christopher Strachey.|$|R
40|$|This article {{considers}} {{an agricultural}} production model of sequential nitrogen application under risk. Because of random shocks between subsequent production stages, optimal fertilization decisions {{depend on the}} magnitude of farmers ’ risk aversion (risk premium), and the possibility for farmers to process information (value of information). We propose a joint estimation procedure of technology and risk aversion parameters, using a structural, simulation-based econometric <b>procedure.</b> <b>Parameter</b> estimates allow to compute both the value of information and the risk premium, which account for about 30 % of fertilizer cost for Midwest corn producers. Key words: agricultural production, generalized method-of-moments estimation, information value, production risk, risk aversion. Because of high nitrate concentration levels in drinking water, there is an increased need for policies aimed at reducing nitrogen con-tamination of groundwater via agricultural sources. Much empirical evidence has emerged recently to support the fact that non...|$|E
40|$|The {{purpose of}} this study was to {{evaluate}} 18 F-FDG PET studies of primary and recurrent sarcomas for diagnosis and correlation with grading. Methods: The evaluation included 56 patients, 43 with histologically proven malignancies and 13 with benign lesions. Seventeen patients were referred with suspicion on a primary tumor, and the remaining 39 were referred with suspicion on a recurrent tumor. The FDG studies were accomplished as a dynamic series for 60 min. The evaluation of the FDG kinetics was performed using the following parameters: standardized uptake value (SUV), global influx, computation of the transport constants K 1 –k 4 with consideration of the distribution volume (VB) according to a two-tissue-compartment model, and fractal dimension based on the box-counting <b>procedure</b> (<b>parameter</b> for the inhomogeneity of the tumors). Results: Visual evaluation revealed a sensitivity of 76. 2 %, a specificity o...|$|E
40|$|Plotkin [Theor. Comp. Sci. 1975] {{showed that}} the lambda {{calculus}} is a good model of the evaluation process for call-by-name functional programs. Reducing programs to constants or lambda abstractions according to the leftmost-outermost strategy exactly mirrors execution on an abstract machine like Landin's SECD machine. The machine-based evaluator returns a constant or the token closure {{if and only if}} the standard reduction sequence starting at the same program will end in the same constant or in some lambda abstraction. However, the calculus does not capture the sharing of the evaluation of arguments that lazy implementations use to speed up the execution. More precisely, a lazy implementation evaluates procedure arguments only when needed and then only once. All other references to the formal <b>procedure</b> <b>parameter</b> re-use the value of the first argument evaluation. The mismatch between the operational semantics of the lambda calculus and the actual behavior of the prototypical implement [...] ...|$|E
50|$|Some {{protection}} from SQL injection attacks: Stored procedures {{can be used}} to protect against injection attacks. Stored <b>procedure</b> <b>parameters</b> will be treated as data even if an attacker inserts SQL commands. Also, some DBMS will check the parameter's type. However, a stored procedure that in turn generates dynamic SQL using the input is still vulnerable to SQL injections unless proper precautions are taken.|$|R
40|$|The Gompertz {{model was}} applied for {{describing}} inactivation, assuming a typical pasteurisation temperature profile at food surface. In order to test the regression analysis <b>procedure</b> <b>parameters</b> were estimated {{on the basis of}} pseudo-experimental data. Results were compared to the ones obtained assuming processes at constant temperature. Care should be taken, if parameters estimated under isothermal conditions are going to be used for predicting inactivation under dynamic conditions...|$|R
5000|$|The {{scope of}} ISO/IEC JTC 1/SC 6 is “Standardization {{in the field}} of {{telecommunications}} dealing with the exchange of information between open systems including system functions, <b>procedures,</b> <b>parameters</b> as well as the conditions for their use. The standardization encompasses protocols and services of lower layers, including physical, data link, network, and transport {{as well as those of}} upper layers including but not limited to Directory and ASN.1.” ...|$|R
40|$|Abstract A {{discrete}} Bass model, {{which is}} a discrete analog of the Bass model, is proposed. This discrete Bass model {{is defined as a}} difference equation that has an exact solution. The difference equation and the solution respectively tend to the differential equation which the Bass model is defined as and the solution when the time interval tends t o zero. The discrete Bass model conserves the characteristics of the Bass model because the difference equation has an exact solution. Therefore, the discrete Bass model enables us to forecast the innovation diffusion of products and services without a continuous-time Bass model. The parameter estimations of the discrete Bass model are very simple and precise. The difference equation itself can be used for the ordinary least squares <b>procedure.</b> <b>Parameter</b> estimation using the ordinary least squares procedure is equal to that using the nonlinear least squares procedure in the discrete Bass model. The ordinary least squares procedures in the discrete Bass model overcome the three shortcomings of the ordinary least squares procedure in the continuous Bass model: the time-interval bias, standard error, and multicollinearity. 1...|$|E
40|$|This work {{presents}} a paradigm {{application of a}} new methodology for simultaneously calibrating (adjusting) model parameters and responses, through assimilation of experimental data, to the benchmark transient thermal-hydraulic experiment IC 1, performed at London's Imperial College. Following {{the description of the}} experimental setup, the corresponding mathematical model is developed and solved numerically. The sensitivities of typically important responses (e. g., temperatures, pressures) to model parameters are computed by applying both the forward and the adjoint sensitivity analysis procedures. These sensitivities not only identify the most important model parameters but also propagate, within the data assimilation <b>procedure,</b> <b>parameter</b> uncertainties for obtaining predictive best-estimate quantities, with reduced best-estimate uncertainties (i. e., “smaller” values for the variance-covariance matrices). This assimilation procedure also provides a quantitative indication of the degree of agreement between computations and experiments. In particular, the paradigm application presented in this work indicates the path for validating and calibrating thermal-hydraulic computational models used for reactor safety analyses. The concluding remarks highlight several important open issues, the resolution of which would significantly advance the area of predictive best-estimate modeling, while opening new avenues for applications in nuclear reactor engineering and safety...|$|E
40|$|Abstract Objective: To {{develop a}} {{simulator}} for training in fluoroscopy-guided facet joint injections and {{to evaluate the}} learning curve for this procedure among radiology residents. Materials and Methods: Using a human lumbar spine as a model, we manufactured five lumbar vertebrae made of methacrylate and plaster. These vertebrae were assembled {{in order to create}} an anatomical model of the lumbar spine. We used a silicon casing to simulate the paravertebral muscles. The model was placed into the trunk of a plastic mannequin. From a group of radiology residents, we recruited 12 volunteers. During simulation-based training sessions, each student carried out 16 lumbar facet injections. We used three parameters to assess the learning curves: procedure time; fluoroscopy time; and quality of the procedure, as defined by the positioning of the needle. Results: During the training, the learning curves of all the students showed improvement in terms of the procedure and fluoroscopy times. The quality of the <b>procedure</b> <b>parameter</b> also showed improvement, as evidenced by a {{decrease in the number of}} inappropriate injections. Conclusion: We present a simple, inexpensive simulation model for training in facet joint injections. The learning curves of our trainees using the simulator showed improvement in all of the parameters assessed...|$|E
5000|$|... {{procedures}} {{with local}} variables (indeed, all variables in BASIC09 are local to <b>procedures)</b> and <b>parameter</b> passing by reference ...|$|R
40|$|Abstract: Precision angular {{scales are}} {{considered}} from common point of view, {{and principles of}} their construction are formulated. Methods of mutual calibration of circular scales are developed to meet these principles {{taking into account the}} redundancy factor. Multidimensional classifica-tion of the methods is proposed. The family of designs is constructed, and relations of <b>procedure</b> <b>parameters</b> of a scale carrier calibration with required accuracy of the procedure results is established...|$|R
40|$|High-level {{language}} {{program compilation}} strategies can be proven correct by modelling {{the process as}} a series of refinement steps from source code to a machine-level description. We show how this can be done for programs containing recursively-defined procedures in the well-established predicate transformer semantics for refinement. To do so the formalism is extended with an abstraction of the way stack frames are created at run time for <b>procedure</b> <b>parameters</b> and variables...|$|R
40|$|This {{dissertation}} analyses {{whether a}} five-year time limit on welfare participation encourages behaviors {{that lead to}} less reliance on public assistance. Using a dynamic discrete choice model, it describes young women 2 ̆ 7 s decisions about childbearing, schooling, working and welfare participation over time. The parameters underlying these decisions are estimated by fitting the model to data on observed choices obtained from the National Longitudinal Survey of Youth. Estimation is done using a simulated maximum likelihood <b>procedure.</b> <b>Parameter</b> estimates are used to simulate choices under current policy and under a policy of a five-year time limit on welfare participation. This provides a way of assessing effects of the time limit. The results suggest that a time limit has {{a significant impact on}} the behavior of those at highest risk for welfare participation. When faced with a time limit, they have fewer children, work and attend school more and rely on less welfare. In fact, a time limit leads to shorter-term welfare participation, apart from the purely 2 ̆ 2 mechanical 2 ̆ 2 effect of making long-term recipients categorically ineligible. Thus a time limit does encourage self-sufficiency in the sense that individuals rely more on other sources of support. At the same time, it appears that the shift occurs towards sources of support other than full-time work to a greater extent than towards full-time school attendance or full-time work. ...|$|E
40|$|This paper {{studies the}} {{implications}} of the imperfect credibility of an exchange rate target zone on the term structure of forward premia. The relationship between spot and forward exchange rates of different maturities reflects the possibility of repeated realignments of the exchange rate band. The credibility of the commitment to the target zone implicit in forward market data can be extracted by estimating the model. Application to French/German data indicates that the model is capable of matching observed patterns of interest rate differentials during the EMS, while yielding estimates of the credibility parameters that accord with the experience of the FF/DM exchange rate during the 1980 s. Target zones;Exchange rate realignments;Economic models;exchange rate, exchange rates, probability, spot exchange rate, spot exchange rates, probabilities, equation, forward exchange, exchange rate band, samples, covariance, exchange rate target, correlation, forward exchange rate, standard errors, forward exchange rates, exchange rate target zones, computation, exchange rate dynamics, exchange rate bands, statistic, statistics, equations, probability density, exchange rate targets, adjustment parameter, exchange rate data, linear models, regression equation, standard deviation, exchange rate target zone, current exchange rate, autocorrelation, exchange rate determination, foreign exchange, optimization, frequency distributions, floating exchange rates, nominal exchange rate, linear regression, probability density function, diffusion process, home currency, rate of change, lagrange multiplier tests, exchange risk, statistical models, exchange markets, estimation <b>procedure,</b> <b>parameter</b> vector, freely floating exchange rates, exchange rate positions, foreign exchange risk, integral, fourier series, difference equation, fortran program, empirical model, significance levels, foreign exchange markets, poisson process, probability distribution, survey, linear model, exchange rate management...|$|E
40|$|Groundwater flow {{models are}} {{important}} tools in assessing baseline conditions and investigating management alternatives in groundwater systems. The usefulness of these models, however, is often hindered by insufficient knowledge regarding {{the magnitude and}} spatial distribution of the spatially-distributed parameters, such as hydraulic conductivity (K), that govern the response of these models. Proposed parameter estimation methods frequently are demonstrated using simplified aquifer representations, when in reality the groundwater regime in a given watershed is influenced by strongly-coupled surface-subsurface processes. Furthermore, parameter estimation methodologies that rely on a geostatistical structure of K often assume the parameter values of the geostatistical model as known or estimate these values from limited data. In this study, we investigate {{the use of a}} data assimilation algorithm, the Ensemble Smoother, to provide enhanced estimates of K within a catchment system using the fully-coupled, surface-subsurface flow model CATHY. Both water table elevation and streamflow data are assimilated to condition the spatial distribution of K. An iterative procedure using the ES update routine, in which geostatistical parameter values defining the true spatial structure of K are identified, is also presented. In this <b>procedure,</b> <b>parameter</b> values are inferred from the updated ensemble of K fields and used in the subsequent iteration to generate the K ensemble, with the process proceeding until parameter values are converged upon. The parameter estimation scheme is demonstrated via a synthetic three-dimensional tilted v-shaped catchment system incorporating stream flow and variably-saturated subsurface flow, with spatio-temporal variability in forcing terms. Results indicate that the method is successful in providing improved estimates of the K field, and that the iterative scheme can be used to identify the geostatistical parameter values of the aquifer system. In general, water table data have a much greater ability than streamflow data to condition K. Future research includes applying the methodology to an actual regional study site...|$|E
40|$|Abstract. Most of {{classification}} methods require model learning <b>procedure</b> or optimal <b>parameters</b> selection using {{large number of}} training samples. In this paper, we propose a novel classification approach using l 1 -minimization based sparse representation which does not need any learning <b>procedure</b> or <b>parameters</b> selection. The proposed approach is based on l 1 minimization becaus...|$|R
3000|$|It is {{observed}} that RLS converges very {{fast in the}} <b>procedure</b> of <b>parameter</b> tuning. The approximate optimal average cost [...]...|$|R
40|$|Abstract. High-level {{language}} {{program compilation}} strategies can be proven correct by modelling {{the process as}} a series of refinement steps from source code to a machine-level description. We show how this can be done for programs containing recursively-defined procedures in the well-established predicate transformer semantics for refinement. To do so the formalism is extended with an abstraction of the way stack frames are created at run time for <b>procedure</b> <b>parameters</b> and variables...|$|R
