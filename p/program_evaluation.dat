3591|4297|Public
5|$|A {{month after}} the InSurv, Iowa failed an Operation Propulsion <b>Program</b> <b>Evaluation.</b> A short time later, the ship retook and passed the evaluation. In July 1987, Captain Larry Seaquist assumed command of the ship.|$|E
5|$|A {{somewhat}} different DAG-based formulation of scheduling constraints {{is used by}} the <b>program</b> <b>evaluation</b> and review technique (PERT), a method for management of large human projects {{that was one of the}} first applications of DAGs. In this method, the vertices of a DAG represent milestones of a project rather than specific tasks to be performed. Instead, a task or activity is represented by an edge of a DAG, connecting two milestones that mark the beginning and completion of the task. Each such edge is labeled with an estimate for the amount of time that it will take a team of workers to perform the task. The longest path in this DAG represents the critical path of the project, the one that controls the total time for the project. Individual milestones can be scheduled according to the lengths of the longest paths ending at their vertices.|$|E
5|$|On 15 January 2014, a {{spending}} bill {{passed by the}} House appropriated $100 million for the GCV program, even though the Army had requested $592 million for the program for FY 2014. The Army planned to spend 80 percent of its ground vehicle modernization budget on the GCV over the next 5 years, with costs ranging from $29-$34 billion depending on overruns and setbacks. Several options were being considered to make the program more affordable, including reducing the squad size from their optimum goal of nine men and using new emerging, and undeveloped, technologies to reduce {{the weight of the}} vehicle to 30 tons for operations in urban environments. The two contractors would run out of money for development of their prototype vehicles by June 2014 unless the Army funded the rest of the technology development phase. The Pentagon and Army tried to find ways to continue the program, without actually starting vehicle production, through new technologies like advanced fire control systems and hybrid engines. Although the Army wanted 1,894 Ground Combat Vehicles with a target price of $9–10.5 million per unit, the Pentagon's Office of Cost Assessment and <b>Program</b> <b>Evaluation</b> estimated a unit cost of up to $17 million. The 83-percent cut in funding essentially scaled back the GCV program to a research effort. The program had declined in support over the past months with the Army determining that the desired vehicle was no longer feasible in the near term due to budget reductions, suspicion from the contractors that the program would not move past technology development, and Congress's believing it would not succeed.|$|E
40|$|The {{development}} of this strategic plan was guided by major policy documents and <b>program</b> <b>evaluations.</b> In May 2001, the Administration issued its National Energy Policy which had several recommendations for the Department of Energy. The Administrations Nuclear Posture Review of 2002 revised the Nations nuclear weapons policy affecting the Departments weapons programs. Focused internally, the Department conducted major top-to-bottom <b>program</b> <b>evaluations</b> of the environmental management and fossil energy programs and has {{had the benefit of}} <b>program</b> <b>evaluations</b> conducted by the Departments Inspector General and the General Accounting Office. There are additional discussions of <b>program</b> <b>evaluations</b> with the resulting goals below. ...|$|R
50|$|There is an {{interesting}} evaluation {{in the context of}} clinical trials and <b>program</b> <b>evaluations.</b>|$|R
40|$|Although {{evaluation}} {{offers many}} benefits to gifted pro-gram administrators, facilitating change in evaluation practices is frequently hampered by insufficient training (Callahan, Tomlinson, Hunsaker, Bland, & Moon, 1995), {{a lack of}} release time for planning and carrying out <b>program</b> <b>evaluations</b> (Moon, 1996), and a failure to devote suffi-cient resources to evaluation efforts (VanTassel-Baska, 2004). When <b>program</b> <b>evaluations</b> have been undertaken, results were infrequently used to drive program decisio...|$|R
25|$|Individuals {{involved}} in medical research, financial or management audits, or <b>program</b> <b>evaluation</b> {{have access to}} the medical record. They are not allowed access to any identifying information, however.|$|E
25|$|Many {{new project}} {{management}} techniques were introduced {{during the development}} of the Polaris missile program, to deal with the inherent system complexity. This includes the use of the <b>Program</b> <b>Evaluation</b> and Review Technique (PERT). This technique replaced the simpler Gantt chart methodology which was largely employed prior to this program.|$|E
25|$|In {{the field}} of evaluation, and in {{particular}} educational evaluation, the Joint Committee on Standards for Educational Evaluation has published three sets of standards for evaluations. The Personnel Evaluation Standards was published in 1988, The <b>Program</b> <b>Evaluation</b> Standards (2nd edition) was published in 1994, and The Student Evaluation Standards was published in 2003.|$|E
40|$|In this paper, {{the authors}} provide an {{overview}} of the recent international experience with active labor market programs (ALMPs). Basing the evidence on the growing body of <b>program</b> <b>evaluations,</b> it focuses on the impacts of ALMPs on the subsequent employment and earnings of participants. This paper provides an update to earlier assessments by incorporating the results of the more recent <b>program</b> <b>evaluations.</b> It also extends these previous reviews by explicitly considering the impacts of ALMPs in developing and transition countries. While most rigorous <b>program</b> <b>evaluations</b> continue to be undertaken in industrialized countries, this gives a significant number of evaluations from transition and, to a lesser extent, developing countries. Environmental Economics&Policies,Labor Standards,Poverty Impact Evaluation,ICT Policy and Strategies,Banks&Banking Reform...|$|R
40|$|This {{document}} presents {{summaries of}} code and utility building <b>program</b> <b>evaluations</b> reviewed {{as the basis}} for the information presented in Energy-Efficient Buildings <b>Program</b> <b>Evaluations,</b> Volume 1 : Findings and Recommendations, DOE/EE/OBT- 11569, Vol. 1. The main purpose of this volume is to summarize information from prior <b>evaluations</b> of similar <b>programs</b> that may be useful background for designing and conducting an evaluation of the BSGP. Another purpose is to summarize an extensive set of relevant evaluations and provide a resource for program designers, mangers, and evaluators...|$|R
50|$|Scriven {{agreed that}} EE contributed to {{improvements}} in internal staff <b>program</b> <b>evaluations</b> and that empowerment evaluation {{could make a}} contribution to evaluation if combined with third-party evaluation.|$|R
25|$|An {{emerging}} {{and growing}} practice of program design is <b>program</b> <b>evaluation.</b> Evaluation {{can be seen}} as a cycle which involves the ongoing systematic assessment of a community-based program by collecting data from it, reviewing the data, changing the program as the data recommends, and then collecting data again. Program designers often choose to incorporate evaluation into design in order to check program processes, determine impact, build a base of support, and/or justify replication/expansion.|$|E
25|$|On 25 June 2014, the Senate {{confirmed}} Morin to be Director of Cost Assessment and <b>Program</b> <b>Evaluation.</b> He {{served in}} that role {{until the end of}} the Obama Administration on 20 January, 2017. During his tenure as CAPE director, Morin was associated with efforts to develop program plans and cost estimates for the B-21 bomber and Ground Based Strategic Deterrent ICBM program, to curtail production of the Littoral Combat Ship and instead produce a more powerful frigate, to use special groups like the Strategic Capabilities Office to develop innovative upgrades to existing systems, and to modernize DOD contract cost reporting methods.|$|E
25|$|One {{advantage}} is {{a learning experience}} between a consumer and a social services provider. One dis{{advantage is}} a limited availability of resources. The models {{that can be used}} for it are the social-ecological model, which provides a framework for program design, the logic model, which is a graphical depiction of logical relationships between the resources, activities, outputs and outcomes of a program, the social action model, whose objectives are to recognize the change around a community in order to preserve or improve standards, understand the social action process/model is a conceptualization of how directed change takes place, and understand how the social action model can be implemented as a successful community problem solving tool, and <b>program</b> <b>evaluation,</b> which involves the ongoing systematic assessment of community-based programs.|$|E
40|$|The {{constant}} rate discounted {{utility model}} {{is commonly used}} to represent intertemporal preferences in health care <b>program</b> <b>evaluations.</b> This paper examines the appropriateness of this model, and argues that the model fails both normatively and descriptively as a representation of individual intertemporal preferences for health outcomes. Variable rate discounted utility models are more flexible, but still require restrictive assumptions and may lead to dynamically inconsistent behaviour. The paper concludes by considering two ways of incorporating individual intertemporal preferences in health care <b>program</b> <b>evaluations</b> that allow for complementarity of health outcomes in different time periods...|$|R
25|$|The {{efficacy}} of aerial resupply operations in Laos having been proven, a six-man support section {{was set up}} in the embassy's <b>Programs</b> <b>Evaluation</b> Office on 16 October 1958 to manage further airlifts.|$|R
50|$|These presentations for parents, teachers, {{educators}} {{and young people}} utilises current research, Top Blokes Foundation experiences, <b>program</b> <b>evaluations</b> and case studies to educate and empower the audience through presentations and interactive workshops.|$|R
25|$|Today, in the U.S., {{about half}} of {{licensed}} psychologists are trained in the Scientist-Practitioner Model of Clinical Psychology (PhD)—a model that emphasizes both research and clinical practice and is usually housed in universities. The other half are being trained within a Practitioner-Scholar Model of Clinical Psychology (PsyD), which focuses on practice (similar to professional degrees for medicine and law). A third training model called the Clinical Scientist Model emphasizes training in clinical psychology research. Outside of coursework, graduates of both programs generally are required to have had 2 to 3 years of supervised clinical experience, {{a certain amount of}} personal psychotherapy, and the completion of a dissertation (PhD programs usually require original quantitative empirical research, whereas the PsyD equivalent of dissertation research often consists of literature review and qualitative research, theoretical scholarship, <b>program</b> <b>evaluation</b> or development, critical literature analysis, or clinical application and analysis).|$|E
500|$|Similarly, topological orderings of DAGs {{can be used}} {{to order}} the {{compilation}} operations in a makefile. The <b>program</b> <b>evaluation</b> and review technique uses DAGs to model the milestones and activities of large human projects, and schedule these projects to use as little total time as possible. Combinational logic blocks in electronic circuit design, and the operations in dataflow programming languages, involve acyclic networks of processing elements. DAGs can also represent collections of events and their influence on each other, either in a probabilistic structure such as a Bayesian network or as a record of historical data such as family trees or the version histories of [...] distributed revision control systems. DAGs can also be used as a compact representation of sequence data, such as the directed acyclic word graph representation of a collection of strings, or the binary decision diagram representation of sequences of binary choices. More abstractly, ...|$|E
500|$|On 20 February 1988, Iowa {{departed}} from the Persian Gulf, transited the Suez Canal, and set sail for the United States, arriving at Norfolk on 10 March for routine maintenance. In April, she participated in the annual Fleet Week celebrations before returning to Norfolk for an overhaul. On 26 May, Fred Moosally replaced Larry Seaquist as Captain of Iowa. After the overhaul, Moosally took Iowa on a shakedown cruise around Chesapeake Bay on 25 August. Encountering difficulty in conning the ship through shallow water, Moosally narrowly missed colliding with the frigate , destroyer , and the cruiser [...] before running aground in soft mud outside the bay's main ship channel near the Thimble Shoals. After one hour, Iowa was able to extricate herself without damage and return to port. Iowa continued with sea trials throughout August and September, then began refresher training in the waters around Florida and Puerto Rico in October, during which the ship passed an Operation Propulsion <b>Program</b> <b>Evaluation.</b>|$|E
40|$|Parallel {{processing}} {{techniques are}} considered {{today as the}} standard approach for high performance computing. Various parallel processor complexes have been constructed, along with algorithms that cover {{a wide range of}} science and engineering domains, taking full advantage of the underlying organization. Efficient and fair at the same time techniques or methods for parallel <b>programs</b> <b>evaluation</b> can be hardly found. The goal herein is to study, evaluate and propose fairer parameters for parallel <b>programs</b> <b>evaluation.</b> Two new parameters are introduced, namely, sizeup and generalized speedup. Especially, the latter parameter considers sizeup and traditional speedup as special cases. The relative speedup parameter has been also set under microscope. Results from this work may be further used for the evaluation of absolute speedup, as well as other parameters, as far as parallel <b>programs</b> performance <b>evaluation</b> is concerned...|$|R
2500|$|As <b>program</b> <b>evaluations</b> {{have become}} a higher {{priority}} for the U.S. government, quantifiable program results are beginning to appear. In 2008 USAID funding towards Peace and Security in Sudan resulted in the following: ...|$|R
25|$|Subsequently, {{right wing}} Assembly members {{organized}} against {{the newly elected}} communists. Also, the <b>Programs</b> <b>Evaluation</b> Office in the American embassy gained an aerial delivery section; this {{was the beginning of}} extensive air operations in Laos.|$|R
2500|$|<b>Program</b> <b>evaluation</b> {{and review}} technique: a {{statistical}} tool {{which was designed}} to analyze and represent the tasks involved in completing a given project ...|$|E
2500|$|In {{the field}} of psychometrics, the Standards for Educational and Psychological Testing place {{standards}} about validity and reliability, along with errors of measurement and {{issues related to the}} accommodation of individuals with disabilities. [...] The third and final major topic covers standards related to testing applications, credentialing, plus testing in <b>program</b> <b>evaluation</b> and public policy.|$|E
2500|$|According to CACREP, an {{accredited}} {{school counseling program}} offers coursework in Professional Identity and Ethics, Human Development, Counseling Theories, Group Work, Career Counseling, Multicultural Counseling, [...] Assessment, Research and <b>Program</b> <b>Evaluation,</b> and Clinical Coursework—a 100-hour practicum and a 600-hour internship under supervision of a school counseling faculty member and a certified school counselor site supervisor (CACREP, 2001).|$|E
50|$|June 1987 - August 1989, Chief of Tactical Command and Control Communication Systems, Directorate of <b>Programs</b> and <b>Evaluation,</b> later, Executive Officer for the Deputy Director of <b>Programs</b> and <b>Evaluation,</b> Headquarters U.S. Air Force, Washington, D.C.|$|R
50|$|Subsequently, {{right wing}} Assembly members {{organized}} against {{the newly elected}} communists. Also, the <b>Programs</b> <b>Evaluation</b> Office in the American embassy gained an aerial delivery section; this {{was the beginning of}} extensive air operations in Laos.|$|R
50|$|Thompson, M. S., DiCerbo, K., Mahoney, K. S., & MacSwan, J. (2002). ¿Éxito en California? A {{validity}} {{critique of}} language <b>program</b> <b>evaluations</b> {{and analysis of}} English learner test scores. Education Policy Analysis Archives, 10(7), entire issue.|$|R
2500|$|Through the program's tie to land-grant {{institutions}} of higher education, 4-H academic staff are responsible for advancing the field of youth development. [...] Professional academic staff are committed to innovation, {{the creation of new}} knowledge, and the dissemination of new forms of program practice and research on topics like University of California's study of thriving in young people. [...] Youth development research is undertaken in a variety of forms including <b>program</b> <b>evaluation,</b> applied research, and introduction of new programs.|$|E
2500|$|A key {{component}} to strategic management {{which is often}} overlooked when planning is evaluation. There {{are many ways to}} evaluate whether or not strategic priorities and plans have been achieved, one such method is Robert Stake's Responsive Evaluation. [...] provides a naturalistic and humanistic approach to <b>program</b> <b>evaluation.</b> In expanding beyond the goal-oriented or pre-ordinate evaluation design, responsive evaluation takes into consideration the program’s background (history), conditions, and transactions among stakeholders. It is largely emergent, the design unfolds as contact is made with stakeholders.|$|E
2500|$|Jamie Michael Morin (born 23 May 1975) was {{a senior}} {{official}} in the United States Department of Defense. [...] He was a private sector economist and research consultant before earning a Doctorate degree in political science from Yale University. [...] He then served as a professional staff member on the United States Senate Committee on the Budget. [...] Morin was Director of Cost Assessment and <b>Program</b> <b>Evaluation</b> at the Department of Defense. Previously, he was Assistant Secretary of the Air Force (Financial Management & Comptroller), and served concurrently as Acting Under Secretary of the Air Force from 3 July 2012 to 28 April 2013. He joined The Aerospace Corporation in 2017 {{as executive director of}} the Center for Space Policy and Strategy and vice president of Defense Systems Operations.|$|E
50|$|MAAG Laos was {{preceded by}} the <b>Programs</b> <b>Evaluation</b> Office, {{established}} on 15 December 1955. Due to the limitations emplaced by international treaty, the PEO was set up with civilian personnel instead of a MAAG with a military staff. When political changes superseded the treaty, MAAG Laos was established in 1961 to replace the <b>Programs</b> <b>Evaluation</b> Office in its support of the Royal Lao Army's fight against the communist Pathet Lao. On July 23, 1962, several interested countries agreed in Geneva to guarantee the neutrality and independence of Laos. As such, the US removed the MAAG, {{replacing it with a}} Requirements Office, which served as a convenient cover for the CIA activities.|$|R
40|$|University of Minnesota Ph. D. dissertation. May 2008. Major: Educational Psychology. Advisor: Frances Lawrenz. 1 {{computer}} file (PDF); x, 206 pages. This study explores {{the validity of}} using citation analysis methods {{as a way of}} assessing the influence of <b>program</b> <b>evaluations</b> conducted within the areas of science, technology, engineering, and mathematics (STEM). Interest in the broad influence of evaluations has caught the attention of evaluation theorists, practitioners, and funders recently. However, methods for measuring the influence of evaluations have yet to be developed and validated. Citation analysis is widely used within scientific research communities to measure the relative influence of scientific research and/or specific scientists. This study explores the applicability of citation analysis for understanding the broad impact of STEM education <b>program</b> <b>evaluations.</b> Nine assumptions regarding the validity of using citation analysis methods to assess STEM education evaluation product influence are examined using data from four sources: (1) citation analysis data, (2) the opinions of an expert panel, (3) data from a survey of primary investigators and evaluators from local projects connected with four national <b>program</b> <b>evaluations,</b> and (4) a review of relevant literatures. The data collected for the validation study suggest that citation analysis methods provide data to help understand, to a limited extent, the influence of large-scale <b>program</b> <b>evaluations</b> on the fields of STEM education and evaluation. In particular, citation data can be used to understand and compare patterns of influence of multi-site STEM <b>program</b> <b>evaluations.</b> Citations, however, are only one among many possible measures of one limited type of influence arising from the dissemination of evaluation products. Additionally, citation data {{do not appear to be}} useful for precisely quantifying the actual level of influence of any one evaluation. Moreover, the examination of the content of citations is critical. Without understanding the content of the citations, judgments cannot be made about whether citations are actually measuring influence. Consequently, it is important to stress that citations are only one measure of one possible influence arising from an evaluation and are limited and should be interpreted as such...|$|R
40|$|In mid term <b>program</b> <b>evaluations</b> evaluators {{are often}} {{confronted with the}} double task of {{retrospectively}} judging the program's merit and worth {{while at the same}} time advising decision makers concerning future adjustments in courses of action. In such cases, it can be argued that it is particularly important that evaluators take into account the divergent views and needs of different stakeholder groups. In principle, <b>program</b> theory <b>evaluation</b> can constitute a sound basis for dealing with the double objective of retrospective judgment and proactive program improvement. However, as argued in the paper, current approaches in <b>program</b> theory <b>evaluation</b> may not be sufficiently equipped to systematically deal with divergent stakeholder values. Taking into account lessons from the literature on stakeholder values in evaluation, an alternative methodological framework is presented. The framework combines <b>program</b> theory <b>evaluation</b> with elements of multicriteria decision aid. An example is used to illustrate the framework. ...|$|R
