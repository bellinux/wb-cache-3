36|10000|Public
40|$|Following the <b>processing</b> <b>and</b> <b>validation</b> of JEFF- 3. 1 {{performed}} in 2006 and presented in ND 2007, {{and as a}} consequence of the latest updated of this library (JEFF- 3. 1. 2) in February 2012, a new <b>processing</b> <b>and</b> <b>validation</b> of JEFF- 3. 1. 2 cross section library is presented in this paper. The processed library in ACE format at ten different temperatures was generated with NJOY- 99. 364 nuclear data processing system. In addition, NJOY- 99 inputs are provided to generate PENDF, GENDF, MATXSR and BOXER formats. The library has undergone strict QA procedures, being compared with other available libraries (e. g. ENDF/B-VII. 1) and processing codes as PREPRO- 2000 codes. A set of 119 criticality benchmark experiments taken from ICSBEP- 2010 has been used for validation purposes...|$|E
40|$|Abstract. Gives {{the method}} of 3 D CAD {{parametric}} design {{under the guidance of}} the theory of reverse engineering. Describes the application of the method development process of complex-shaped surface mold. Analysis of the mold characteristics, coordinate measuring equipment accurate and efficient access to the basic outline of the data, the integration of CAD software to design mold forms, re-use processing module directly form processing code, complete mold <b>processing</b> <b>and</b> <b>validation...</b>|$|E
40|$|Instruments of the Earth Radiation Budget Experiment (ERBE) are {{operating}} on three different Earth orbiting spacecrafts: the Earth Radiation Budget Satellite (ERBS), NOAA- 9, and NOAA- 10. An overview is presented of the ERBE mission, in-orbit environments, and instrument design and operational features. An overview of science data <b>processing</b> <b>and</b> <b>validation</b> procedures is also presented. In-flight operations are described for the ERBE instruments aboard the ERBS and NOAA- 9. Calibration and other operational procedures are described, and operational and instrument housekeeping data are presented and discussed...|$|E
40|$|Website {{requirements}} specification (WRS) is {{a document that}} describes the characteristics expected of a web site to be developed or modified, {{in order to ensure}} compliance. This is particularly important when site development is outsourced. This paper analyzes in detail the procedure for <b>processing,</b> structure <b>and</b> <b>validation,</b> with special attention to the functional requirements of a website...|$|R
40|$|Accurate, consistent, <b>and</b> {{transparent}} data <b>processing</b> <b>and</b> analysis are integral and critical parts of proteomics workflows {{in general and}} for biomarker discovery in par-ticular. Definition of common standards for data repre-sentation and analysis {{and the creation of}} data reposito-ries are essential to compare, exchange, and share data within the community. Current issues in data <b>processing,</b> analysis, <b>and</b> <b>validation</b> are discussed together with op-portunities for improving the process in the future and for defining alternative workflows. Molecular & Cellular Proteomics 5 : 1921 – 1926, 2006. Proteomics has undergone tremendous advances over the past few years, and technologies have noticeably matured. Despite these developments, biomarker discovery remains a very challenging task due to the complexity of the sample...|$|R
40|$|AbstractLabel-free {{quantitative}} LC–MS profiling {{of complex}} body fluids {{has become an}} important analytical tool for biomarker and biological knowledge discovery in the past decade. Accurate <b>processing,</b> statistical analysis <b>and</b> <b>validation</b> of acquired data diversified by {{the different types of}} mass spectrometers, mass spectrometer parameter settings and applied sample preparation steps are essential to answer complex life science research questions and understand the molecular mechanism of disease onset and developments. This review provides insight into the main modules of label-free data processing pipelines with statistical analysis <b>and</b> <b>validation</b> <b>and</b> discusses recent developments. Special emphasis is devoted to quality control methods, performance assessment of complete workflows and algorithms of individual modules. Finally, the review discusses the current state and trends in high throughput data <b>processing</b> <b>and</b> analysis solutions for users with little bioinformatics knowledge...|$|R
40|$|A {{computational}} {{procedure is}} developed {{that uses a}} moving zonal grid concept to model complex flexible aerospace vehicles. The Euler/Navier-Stokes equations are used to model the flow, and computations are made using efficient methods based on both central and upwind schemes. The structure is represented by a finite element method which can model general aerospace vehicles. Provisions are made to accommodate other disciplines such as controls and thermal loads. The code is capable of computing unsteady flows on flexible wings with vortical flows. Adaptation of this procedure for parallel <b>processing</b> <b>and</b> <b>validation</b> for complete aerospace configurations is in progress...|$|E
40|$|A {{guide for}} using the data {{products}} from the Stratospheric Aerosol and Gas Experiment 1 (SAGE 1) for scientific investigations of stratospheric chemistry related to aerosol, ozone, nitrogen dioxide, dynamics, and climate change is presented. A {{detailed description of the}} aerosol profile tape, the ozone profile tape, and the nitrogen dioxide profile tape is included. These tapes are the SAGE 1 data products containing aerosol extinction data and ozone and nitrogen dioxide concentration data for use in the different scientific investigations. Brief descriptions of the instrument operation, data collection, <b>processing,</b> <b>and</b> <b>validation,</b> and some of the scientific analyses that were conducted are also included...|$|E
30|$|In the {{numerical}} model section, production {{of at least}} 10 years of sea state data, with wave resource taken as stationary, is required. The minimum frequency for sea state data is one dataset recorded at intervals of 3 h. Guidelines regarding the configuration of boundary conditions, data <b>processing</b> <b>and</b> <b>validation</b> of numerical modelling as well as dealing with missing data are mentioned. The practical implications of the report has been evaluated through comparison with the Biscay Marine Energy Platform (BiMEP) facility which is situated on the Basque coast. Consequently, the case study may be of relevance to the Government of Mauritius or any organizations considering {{the implementation of a}} wave farm in the identified site of Roches Noires.|$|E
40|$|We {{describe}} {{a system that}} enhances the readability of scanned picture books. Motivated by our website of children’s books in the International Children's Digital Library, the system separates textual from visual content which decreases {{the size of the}} image files (since their quality can be lower) while increasing the quality of the text by displaying it as computer-generated text instead of an image. This text-background separation combines image <b>processing</b> <b>and</b> human <b>validation</b> in an efficient manner and results in a system that not only is more readable, but also accessible, searchable, and translatable. Categories and Subject Descriptors H 5. 2 [Information interfaces and presentation]: User Interfaces. - Graphical user interfaces...|$|R
40|$|Abstract—In this paper, {{we propose}} a data {{transformation}} pattern to transform sequential data into {{a set of}} binary/categorical features and numerical features to enable data analysis. These features capture both structural and temporal information inherent in sequential data. I. CATEGORIZATION Assuming data analysis consists of four phases: data cleansing, data transformation, data <b>processing,</b> <b>and</b> result <b>validation,</b> the proposed pattern is a data transformation pattern. II. INTENT Extract features from sequential data composed of events {{for the purpose of}} data analysis. The features should capture event ordering information inherent in the data. If the events are structured, the features should capture structural information inherent in events...|$|R
40|$|I {{describe}} {{a system that}} enhances the readability of scanned picture books. Motivated by our website of children’s books in the International Children's Digital Library, the system separates textual from visual content which decreases {{the size of the}} image files (since their quality can be lower) while increasing the quality of the text by displaying it as computer-generated text instead of an image. This text-background separation combines image <b>processing</b> <b>and</b> human <b>validation</b> in an efficient manner and results in a system that not only is more readable, but also accessible, searchable, and translatable. Categories and Subject Descriptors H 5. 2 [Information interfaces and presentation]: User Interfaces. - Graphical user interfaces...|$|R
40|$|In {{order to}} manage the global data sets {{required}} to understand the earth as a system, the EOS Data and Information System (EOSDIS) will collect and store satellite, aircraft, and in situ measurements and their resultant data products, and will distribute the data conveniently. EOSDIS will also provide product generation and science computing facilities to support the development, <b>processing,</b> <b>and</b> <b>validation</b> of standard EOS science data products. The overall architecture of EOSDIS, and how the Distributed Active Archive Centers fit into that structure, are shown. EOSDIS will enable users to query data bases nationally, make use of keywords and other mnemonic identifiers, and see graphic images of subsets of available data prior to ordering full (or selected pieces of) data sets for use in their 'home' environment...|$|E
40|$|Automatic brain MRI segmentations {{methods are}} useful but {{computationally}} intensive tools in medical image computing. Deploying them on grid infrastructures {{can provide an}} efficient resource for data handling and computing power. In this study, an efficient implementation of a brain MRI segmentation method through a grid-interfaced workflow enactor is proposed. The deployment of the workflow enables simultaneous <b>processing</b> <b>and</b> <b>validation.</b> The importance of parallelism is shown with concurrent analysis of several MRI subjects. The results obtained from the grid have been compared to the results computed locally on only one computer. Thanks {{to the power of}} the grid, method’s parameter influence on the resulting segmentations has also been assessed given the best compromise between algorithm speed and results accuracy. This deployment highlights also the grid issue of a bottleneck effect. ...|$|E
40|$|ABSTRACT—TransLink {{currently}} has several liabilities in its security policy making it vulnerable to {{attacks in the}} form of fraud and fare evasion. Exploitation of the newly celebrated U-pass program may be approached from several levels. The economic implications of these losses significantly impact TransLink’s business model. Hence, to drive revenue opportunities while upgrading quality of service, a smart card TransLink system is proposed. Smart cards are by evolution more functionally secure and scalable than magnetic stripe technologies currently utilized by TransLink. They also address other transit challenges such as rate of passenger <b>processing</b> <b>and</b> <b>validation</b> of fare. An implementation of a secure smart card system will both transform a legacy security system and enhance Translink’s value proposition to customers. Index Terms — Magnetic stripe, smart card, transit security...|$|E
40|$|Label-free {{quantitative}} LC-MS profiling {{of complex}} body fluids {{has become an}} important analytical tool for biomarker and biological knowledge discovery in the past decade. Accurate <b>processing,</b> statistical analysis <b>and</b> <b>validation</b> of acquired data diversified by {{the different types of}} mass spectrometers, mass spectrometer parameter settings and applied sample preparation steps are essential to answer complex life science research questions and understand the molecular mechanism of disease onset and developments. This review provides insight into the main modules of label-free data processing pipelines with statistical analysis <b>and</b> <b>validation</b> <b>and</b> discusses recent developments. Special emphasis is devoted to quality control methods, performance assessment of complete workflows and algorithms of individual modules. Finally, the review discusses the current state and trends in high throughput data <b>processing</b> <b>and</b> analysis solutions for users with little bioinformatics knowledge. (c) 2010 Elsevier B. V. All rights reserved...|$|R
40|$|Given {{a set of}} {{data points}} with both spatial {{coordinates}} andnon-spatial attributes, point a location-dependently dominates point b {{with respect to a}} query point q if a is closer to q than b and meanwhile a dominates b. A location-dependent skyline query (LDSQ) issued at point q is to retrieve all the points that are not location-dependently dominated by other points with regard to q. In this paper, we focus on the query <b>processing</b> <b>and</b> result <b>validation</b> of LDSQ overstatic objects. Two algorithms, namely brute-forth and ?-scanning, are proposed. The former serves as the baseline algorithm while the latter significantly improves the performance via space pruning. We further conduct a comprehensive simulation to demonstrate the performance of proposed algorithms...|$|R
50|$|Peiker Acustic's main {{clients include}} {{well-known}} automotive and mobile phone manufacturers, along with various industrial firms, {{police and fire}} departments, emergency medical services, government agencies and official bodies and Public transit agencies. The company's specialties include the fields of acoustics (with analog <b>and</b> digital signal <b>processing),</b> <b>validation,,</b> <b>and</b> the manufacturing, construction and development of hardware and software for in-vehicle communications devices.|$|R
40|$|URL] audienceAutomatic brain MRI segmentations {{methods are}} useful but {{computationally}} intensive tools in medical image computing. Deploying them on grid infrastructures {{can provide an}} efficient resource for data handling and computing power. In this study, an efficient implementation of a brain MRI segmentation method through a grid-interfaced workflow enactor is proposed. The deployment of the workflow enables simultaneous <b>processing</b> <b>and</b> <b>validation.</b> The importance of parallelism is shown with concurrent analysis of several MRI subjects. The results obtained from the grid have been compared to the results computed locally on only one computer. Thanks {{to the power of}} the grid, method's parameter influence on the resulting segmentations has also been assessed given the best compromise between algorithm speed and results accuracy. This deployment highlights also the grid issue of a bottleneck effect...|$|E
40|$|This article {{describes}} the collecting, <b>processing</b> <b>and</b> <b>validation</b> of a large balanced corpus for Romanian. The annotation types {{and structure of the}} corpus are briefly reviewed. It was constructed at the Research Institute for Artificial Intelligence of the Romanian Academy {{in the context of an}} international project (METANET 4 U). The processing covers tokenization, POS-tagging, lemmatization and chunking. The corpus is in XML format generated by our in-house annotation tools; the corpus encoding schema is XCES compliant and the metadata specification is conformant to the METANET recommendations. To the best of our knowledge, this is the first large and richly annotated corpus for Romanian. ROMBAC is intended to be the foundation of a linguistic environment containing a reference corpus for contemporary Romanian and a comprehensive collection of interoperable processing tools...|$|E
40|$|Extensive {{and highly}} {{detailed}} real-world road networks obtained through mobile mapping and spatial data processing build {{a basis for}} development and evaluation of advanced driver assistant and automation systems nowadays. Such road networks, as used in driving/traffic simulation and test vehicles, are provided in specialised description formats - one of which being OpenDRIVE. Sparse tool support makes generation, <b>processing</b> <b>and</b> <b>validation</b> of OpenDRIVE cumbersome. The GIS domain provides well-established and convenient tools for spatial data processing, but does not yet offer support for OpenDRIVE data. This paper describes {{an extension of the}} free and open-source Geospatial Data Abstraction Library (GDAL) with OpenDRIVE as missing link between the domains of driving simulation and geographic information systems. By bringing both domains closer together we hope to stimulate promising development of scenario generation and synthesis of reality-based road networks for driving simulator applications...|$|E
40|$|Sentinel- 3 A is {{scheduled}} for launch in Oct. 2015, with Sentinel- 3 B to follow 18 months later. Together these missions are to take oceanographic remote-sensing into a new operational realm. To achieve this {{a large number of}} <b>processing,</b> calibration <b>and</b> <b>validation</b> tasks have to be applied to their data in order to assess for quality, absolute bias, short-term changes and long-term drifts. ESA has funded the Sentinel- 3 Mission Performance Centre (S 3 MPC) to carry out this evaluation on behalf of ESA and EUMETSAT. The S 3 MPC is run by a consortium led by ACRI [1] and this paper describes the work on the calibration/validation (cal/val) of the Surface Topography Mission (STM), which is co-ordinated by CLS and PML...|$|R
40|$|This paper {{describes}} the Imperial College near infrared spectroscopy neuroimaging analysis (ICNNA) software tool for functional near infrared spectroscopy neuroimaging data. ICNNA is a MATLAB-based object-oriented framework encompassing an {{application programming interface}} and a graphical user interface. ICNNA incorporates reconstruction based on the modified Beer-Lambert law <b>and</b> basic <b>processing</b> <b>and</b> data <b>validation</b> capabilities. Emphasis {{is placed on the}} full experiment rather than individual neuroimages as the central element of analysis. The software offers three types of analyses including classical statistical methods based on comparison of changes in relative concentrations of hemoglobin between the task and baseline periods, graph theory-based metrics of connectivity and, distinctively, an analysis approach based on manifold embedding. This paper presents the different capabilities of ICNNA in its current version...|$|R
40|$|The German-Dutch {{atmospheric}} science instrument SCIAMACHY on the European Earth observation mission ENVISAT provides {{new insights into}} the Earth's atmosphere. The book describes the SCIAMACHY mission in detail. It starts with the requirements for doing {{atmospheric science}} from space, followed by outlining the SCIAMACHY host, i. e. ENVISAT. Several chapters deal with SCIAMACHY instrument aspects: design, operations and calibration & monitoring. The system related part of the book finishes with a description of SCIAMACHY's first years in orbit. The second part of the book is devoted to the area of science data, including retrieval algorithms, data <b>processing</b> <b>and</b> product <b>validation.</b> SCIAMACHY's view of the Earth's atmosphere, a tour through the atmosphere from bottom to top presenting many exciting SCIAMACHY results, finally concludes the publication...|$|R
40|$|Maximum 200 words) Instruments of the Earth Radiation Budget Experiment (ERBE) are {{operating}} on three different Earth-orbiting spacecraft. The Earth Radiation Budget Satellite (ERBS) is {{operated by the}} National Aeronautics and Space Administration (NASA), and the NOAA 9 and NOAA 10 weather satellites are operated by the National Oceanic and Atmospheric Administration (NOAA). This paper is the second in a series that describes the ERBE mission, in-orbit environments, instrument design and operational features, and data <b>processing</b> <b>and</b> <b>validation</b> procedures. This paper decribes the spacecraft and instrument operations for the second full year of in-orbit operations, which extends from February 1986 through January 1987. Validation and archival of radiation measurements made by ERBE instruments during this second year of operation were completed in July 1991. This period includes the only time, November 1986 through January 1987, during which all ERBE instruments aboard the ERBS, NOAA 9, [...] ...|$|E
40|$|Proteomics {{has come}} a long way from the initial {{qualitative}} analysis of proteins present in a given sample at a given time ("cataloguing") to large-scale characterization of proteomes, their interactions and dynamic behavior. Originally enabled by breakthroughs in protein separation and visualization (by two-dimensional gels) and protein identification (by mass spectrometry), the discipline now encompasses a large body of protein and peptide separation, labeling, detection and sequencing tools supported by computational data processing. The decisive mass spectrometric developments and most recent instrumentation news are briefly mentioned accompanied by a short review of gel and chromatographic techniques for protein/peptide separation, depletion and enrichment. Special emphasis is placed on quantification techniques: gel-based, and label-free techniques are briefly discussed whereas stable-isotope coding and internal peptide standards are extensively reviewed. Another special chapter is dedicated to software and computing tools for proteomic data <b>processing</b> <b>and</b> <b>validation.</b> A short assessment of the status quo and recommendations for future developments round up this journey through quantitative proteomics...|$|E
40|$|Instruments of the Earth Radiation Budget Experiment (ERBE) are {{operating}} on three different Earth-orbiting spacecraft. The Earth Radiation Budget Satellite (ERBS) is operated by NASA, and NOAA 9 and NOAA 10 weather satellites are {{operated by the}} National Oceanic and Atmospheric Administration (NOAA). This paper is the second in a series that describes the ERBE mission, and data <b>processing</b> <b>and</b> <b>validation</b> procedures. This paper describes the spacecraft and instrument operations for the second full year of in-orbit operations, which extend from February 1986 through January 1987. Validation and archival of radiation measurements made by ERBE instruments during this second year of operation were completed in July 1991. This period includes the only time, November 1986 through January 1987, during which all ERBE instruments aboard the ERBE, NOAA 9, and NOAA 10 spacecraft were simultaneously operational. This paper covers normal and special operations of the spacecraft and instruments, operational anomalies, and {{the responses of the}} instruments to in-orbit and seasonal variations in the solar environment...|$|E
40|$|The MODIS Information, Data, and Control System (MIDACS) Specifications and Conceptual Design Document {{discusses}} system level requirements, {{the overall}} operating {{environment in which}} requirements must be met, and a breakdown of MIDACS into component subsystems, which include the Instrument Support Terminal, the Instrument Control Center, the Team Member Computing Facility, the Central Data Handling Facility, and the Data Archive and Distribution System. The specifications include sizing estimates for the <b>processing</b> <b>and</b> storage capacities of each data system element, as well as traffic analyses of data flows between the elements internally, and also externally across the data system interfaces. The specifications for the data system, {{as well as for}} the individual planning and scheduling, control and monitoring, data acquisition <b>and</b> <b>processing,</b> calibration <b>and</b> <b>validation,</b> <b>and</b> data archive and distribution components, do not yet fully specify the data system in the complete manner needed to achieve the scientific objectives of the MODIS instruments and science teams. The teams have not yet been formed; however, it was possible to develop the specifications and conceptual design based on the present concept of EosDIS, the Level- 1 and Level- 2 Functional Requirements Documents, the Operations Concept, and through interviews and meetings with key members of the scientific community...|$|R
30|$|The {{classical}} biobanking {{activities are}} to be mirrored by a similar network of imaging biobanks. Modern radiology and nuclear medicine can also provide multiple imaging biomarkers of the same patient, using quantitative data derived from all sources of digital imaging, such as CT, MRI, PET, SPECT, US, x-ray, etc. [20, 21]. Imaging biobanks are infrastructures with massive storage and computing capacity. High-performance computing resources are needed to facilitate image <b>processing</b> comparison, standardisation <b>and</b> <b>validation.</b> Integration of resources and services through a platform that manages the information flow <b>and</b> image <b>processing</b> is a step needed {{in the development of}} imaging biobanks.|$|R
40|$|The MODIS Information, Data, and Control System (MIDACS) Operations Concepts Document {{provides}} {{a basis for}} the mutual understanding between the users and the designers of the MIDACS, including the requirements, operating environment, external interfaces, and development plan. In defining the concepts and scope of the system, how the MIDACS will operate as an element of the Earth Observing System (EOS) within the EosDIS environment is described. This version follows an earlier release of a preliminary draft version. The individual operations concepts for planning and scheduling, control and monitoring, data acquisition <b>and</b> <b>processing,</b> calibration <b>and</b> <b>validation,</b> data archive <b>and</b> distribution, and user access do not yet fully represent the requirements of the data system needed to achieve the scientific objectives of the MODIS instruments and science teams. The teams are not yet formed; however, it is possible to develop the operations concepts based on the present concept of EosDIS, the level 1 and level 2 Functional Requirements Documents, and through interviews and meetings with key members of the scientific community. The operations concepts were exercised through the application of representative scenarios...|$|R
40|$|As {{the need}} for {{increased}} productivity, faster delivery, and new electronic products — without a loss of quality or an increase in cost — becomes more pressing, scholarly publishers are faced with challenges unique to their rapidly changing environment. eXtyles provides powerful and flexible solutions to these problems through a suite of editorial and XML-production tools that are customized to meet the individual requirements of each publisher. eXtyles operates as an extension to the familiar Microsoft Word environment. Within Word, it efficiently streamlines documents into a standard editorial and visual style, automates reference <b>processing</b> <b>and</b> <b>validation,</b> and produces well-formed or valid XML with {{the push of a}} button. eXtyles is currently used by a number of scholarly book publishers and by more than 350 journals worldwide, including the Cell Press journals, Nature, Science, and the New England Journal of Medicine. All of these organizations have achieved significant improvements in productivity, substantial cost savings, and enhanced quality of content. Why eXtyles? With eXtyles, editors work in a “What You See Is What You Get ” (WYSIWYG) Word environment rathe...|$|E
40|$|A {{biological}} assay {{is designed to}} set up a rapid and robust drug-screening system on a small scale. An assay is considered as a single unit of a platform to screen various compounds for aiding in drug discovery. Each assay is carried out in a 96 -well plate, each of whose wells consists of the biological component called the Spheroids. The value of each assay lies in it facilitating for versatile screening applications. The spheroid is considered as a micro-structural product. And the addition of various compounds for testing is performed in each well (consisting of the spheroids). The focus has been to put forth the production principles and validation strategies to run the {{biological assay}} and test its efficacy to be used for screening in high volumes. The assay development illustrates <b>processing</b> <b>and</b> <b>validation</b> techniques. The goal is to develop optimized standards to process the assay, addressing various quality control issues, from the raw material to the end-product stage. Such an approach also brings interesting analogies of biological process in a manufacturing scenario. The developed system incorporates a value stream approach, by pulling the product from the customer end. The process involves simply encapsulating HUVECs (Human Umbelical Vei...|$|E
40|$|Copyright: © 2008 Thiele H, et al. This is an open-access article {{distributed}} {{under the}} terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. With the large variety of Proteomics workflows, as well as the large variety of instruments and data-analysis software available, researchers today face major challenges validating and comparing their Proteomics data. It is the expectation that Human Proteome Organisation (HUPO) related standardization initiatives with its standardized data formats but also with its efforts in standardized <b>processing</b> <b>and</b> <b>validation</b> will lead to field-generated data of greater accuracy, reproducibility and comparability. Here we present a new generation of the ProteinScapeTM bioinformatics platform, now enabling researchers to manage Proteomics data from the generation and data warehousing to a central data repository with a strong focus on the improved accuracy, reproducibility and comparability demanded by many researchers in the field. It addresses scientists current needs in proteomics identification, quantification, validation and biomarker dis-covery. Offering comprehensive solutions for qualitative and quantitative LC-MS/MS and gel-based protein analysis, this proteomics data warehousing and project management software supports various discovery workflow...|$|E
40|$|Circular dichroism (CD) {{spectroscopy}} {{is widely}} used in structural biology as a technique for examining the structure, folding and conformational changes of proteins. A new server, ValiDichro, has been developed for checking the quality and validity of CD spectral data and metadata, both {{as an aid to}} data collection <b>and</b> <b>processing</b> <b>and</b> as a <b>validation</b> procedure for spectra to be included in publications. ValiDichro currently includes 25 tests for data completeness, consistency and quality. For each test that is done, not only is a validation report produced, but the user is also provided with suggestions for correcting or improving the data. The ValiDichro server is freely available a...|$|R
40|$|This paper {{summarizes}} the major events occurred since {{the launch of}} ERS- 1 and ERS- 2 Wind Scatterometer, and shows {{the results from the}} monitoring of the instrument by looking at the telemetry data stream (e. g. working modes, currents, voltages and temperatures of AMI instrument) and the fast delivery product (e. g. doppler, noise and calibration information). The latter allows also the monitoring of the On-Ground <b>Processing</b> <b>and</b> the geophysical <b>validation</b> of the products. This paper also reports the results of the commissioning phase and describes the calibration status of both ERS- 1 and ERS- 2 Wind Scatterometer. This monitoring activity is conducted within the Produc...|$|R
40|$|A {{biomarker}} is an {{accurately and}} reproducibly quantifiable biological characteristic that provides an objective measure of health status or disease. Benefits of biomarkers include identification of therapeutic targets, monitoring of clinical interventions, {{and development of}} personalized (or precision) medicine. Challenges {{to the use of}} biomarkers include optimizing sample collection, <b>processing</b> <b>and</b> storage, <b>validation,</b> <b>and</b> often the need for sophisticated laboratory and bioinformatics approaches. Biomarkers offer better understanding of disease processes and should benefit the early detection, treatment, and management of multiple noncommunicable diseases (NCDs). This review will consider the utility of biomarkers in patients with allergic and other immune-mediated diseases in childhood. Typically, biomarkers are used currently to provide mechanistic insight or an objective measure of disease severity, with their future role in risk stratification/disease prediction speculative at best. There are many lessons to be learned from the biomarker strategies used for cancer in which biomarkers are in routine clinical use and industry-wide standardized approaches have been developed. Biomarker discovery <b>and</b> <b>validation</b> in children with disease lag behind those in adults; given the early onset and therefore potential lifelong effect of many NCDs, there should be more studies incorporating cohorts of children. Many pediatric biomarkers are at the discovery stage, with a long path to evaluation and clinical implementation. The ultimate challenge will be optimization of prevention strategies that can be implemented in children identified as being at risk of an NCD through the use of biomarkers...|$|R
