5|50|Public
50|$|The {{solution}} {{is to provide a}} large-volume plenum chamber between the inlet and the cylinders. This has two benefits: it evens out the difference in <b>path</b> <b>restriction</b> between cylinders (distribution across space), secondly it provides a large-volume buffer against pressure changes (distribution over time).|$|E
5000|$|Production of the GT-Four (or {{previously}} {{known as}} All-Trac in the US), continued for the Japanese, Australian, European, and British markets. This ST205 version {{was to be}} the most powerful Celica produced to date, producing [...] (export version) or [...] (Japanese market) from an updated 3S-GTE engine. Influenced strongly by Toyota Team Europe, Toyota's factory team in the World Rally Championship, {{the final version of the}} GT-Four included improvements such as an all-aluminum hood to save weight, four-channel ABS (with G-force sensor), an improved turbocharger (incorrectly known by enthusiasts as the CT20B), and Super Strut Suspension. The 2500 homologation cars built to allow Toyota to enter the GT-Four as a Group A car in the World Rally Championship also sported extras such as all of the plumbing required to activate an anti-lag system, a water spray bar for the Intercooler's front heat exchanger, a water injection system for detonation protection, a hood spoiler mounted in front of the windscreen to stop hood flex at high speed and the standard rear spoiler mounted on riser blocks. The car proved to be quite competitive in the 1995 World Championship. However, the team was banned from competition for a year after the car's single victory due to turbocharger fixing - a device that meant there was no air <b>path</b> <b>restriction</b> on the intake - when the jubilee clip was undone this would flick back into place so as to go un-noticed by inspectors. Toyota has always claimed that they knew nothing of the fix - but opponents say it was one very cleverly engineered device. In some respects this car is a true sports car; in order to qualify for rallying it has a lot of special features and a unique strut arrangement.|$|E
40|$|We define {{extensions}} of the full branching-time temporal logic CTL ∗ in which the path quantifiers are relativised by formal languages of infinite words, and consider its natural fragments obtained by extending the logics CTL and CTL+ in the same way. This yields a small and two-dimensional hierarchy of temporal logics parametrised by the class of languages used for the <b>path</b> <b>restriction</b> on one hand, {{and the use of}} temporal operators on the other. We motivate the study of such logics through two application scenarios: in abstraction and refinement they offer more precise means for the exclusion of spurious traces; and they may be useful in software synthesis where decidable logics without the finite model property are required. We study the relative expressive power of these logics as well as the complexities of their satisfiability and model-checking problems...|$|E
25|$|There is a {{decrease}} in faunal species from the Belts to the Gulf of Bothnia. The decreasing salinity along this <b>path</b> causes <b>restrictions</b> in both physiology and habitats. The lack of tides has affected the marine species {{as compared with the}} Atlantic.|$|R
40|$|Mobile {{telecommunication}} systems {{establish a}} large number of communication links with a limited number of available frequencies; reuse of the same or adjacent frequencies on neighboring links causes interference. The task to find an assignment of frequencies to channels with minimal interference is the frequency assignment problem. The frequency assignment problem is usually treated as a graph coloring problem where the number of colors is minimized, but this approach does not model interference minimization correctly. We give in this paper a new integer programming formulation of the frequency assignment problem, the orientation model, and develop a heuristic two-stage method to solve it. The algorithm iteratively solves an outer and an inner optimization problem. The outer problem decides for each pair of communication links which link gets the higher frequency and leads to an acyclic subdigraph problem with additional longest <b>path</b> <b>restrictions.</b> The inner problem to find an optimal assignment respecting an orientation leads to a min-cost flow problem...|$|R
40|$|It {{is known}} that both the number of Dyck paths with $ 2 n$ steps and $k$ peaks, {{and the number of}} Dyck paths with $ 2 n$ steps and $k$ steps at odd height follow the Narayana distribution. In this paper we present a {{bijection}} which explicitly illustrates this equinumeracy. Moreover, we extend this bijection to bilateral Dyck <b>paths.</b> The <b>restriction</b> to Dyck <b>paths</b> preserves the number of contacts. Comment: revised versio...|$|R
40|$|The {{application}} of automated vehicles in logistics can efficiently {{reduce the cost}} of logistics and reduce the potential risks in the last mile. Considering the <b>path</b> <b>restriction</b> in the initial stage of the {{application of}} automated vehicles in logistics, the conventional model for a vehicle routing problem (VRP) is modified. Thus, the automated vehicle routing problem with time windows (AVRPTW) model considering path interruption is established. Additionally, an improved particle swarm optimisation (PSO) algorithm is designed to solve this problem. Finally, a case study is undertaken to test the validity of the model and the algorithm. Four automated vehicles are designated to execute all delivery tasks required by 25 stores. Capacities of all of the automated vehicles are almost fully utilised. It is of considerable significance for the promotion of automated vehicles in last-mile situations to develop such research into real problems arising in the initial period...|$|E
40|$|As {{important}} as navigation is to human performance in virtual worlds, {{it is an}} often overlooked problem in the design process. This dissertation reports an experiment intended to show that real-world wayfinding and environmental design principles are effective in designing virtual worlds which support skilled wayfinding behaviour. The design principles are adapted from both the cognitive psychology literature and urban and architectural design methodologies. There are two categories of design principles, those that guide the organizational structure of the environment, and those that guide the use and presentation of maps. The study measures subject performance on a complex searching task {{in a number of}} virtual worlds with differing environmental cues. The environments are extremely large and consist of open sea, land, and ships which are used as targets for search tasks. The worlds are augmented with either a radial grid, a map, both, or neither. For each trial, the subject's viewpoint position and orientation was sampled approximately once per second. A verbal protocol with accompanying video was used to elicit information about the search strategies employed. A map drawing exercise followed each trial in order to gain insight to the subject's spatial knowledge (i. e. cognitive map) of the environment.  The results show that subjects in the treatment without any additional cues were often disoriented and had extreme difficulty completing the task. The grid was shown to provide superior directional information but both treatments using the map were superior overall due to the geocentric perspective provided. Behaviors associated with each treatment indicate that the cues had a strong effect on both search performance and search strategy. The results suggest that users of large-scale virtual worlds require structure in order to effectively navigate. Augmentations such as direction indicators, maps, and <b>path</b> <b>restriction</b> can all greatly improve both wayfinding performance and overall user satisfaction. ...|$|E
40|$|This thesis {{focuses on}} {{approximation}} algorithms and complexity assessments concerning network flows. It deals with various network flow problems with <b>path</b> <b>restrictions.</b> These restrictions cover {{the number of}} paths {{that are used to}} route commodities as well as the amount of flow that is routed along a single path or the path's length. Concerning the first restriction we study the unsplittable flow problem-a generalization of the NP-hard edge-disjoint paths problem. Given a network with commodities that must be routed from their sources to their sinks, the unsplittable flow problem forbids each commodity to use more than one path. For this problem we prove a new lower bound on the performance guarantee of randomized rounding which so far belongs to the best approximation algorithms known for this problem. Further, we present an interesting relation between unsplittable flows and classical (splittable) multicommodity flows in the case that all commodities share a common source: Each single source multicommodity flow can be represented as a convex combination of unsplittable flows of congestion at most 2. Further, we combine different <b>path</b> <b>restrictions</b> from the ones mentioned above. In the k-splittable flow problem with path capacities, we study the NP-hard problem that each commodity may be sent along a limited number of paths while the flow value of each path is bounded. This yields a generalization of the unsplittable flow problem, but we show how one can obtain the same asymptotic approximation ratios. For the length-bounded k-splittable flow problem, we consider the single commodity case and develop a constant factor approximation algorithm. A crucial characteristic of network flows occurring in real-world applications is flow variation over time and the fact that flow does not travel instantaneously through a network but requires {{a certain amount of time}} to travel through each arc. Both characteristics are captured by "flows over time" which specify a flow rate for each arc and each point in time. We consider the quickest single commodity k-splittable flow problem and give a constant factor approximation algorithm for it. So far only results for k-splittable flows as well as for length-bounded flows and flows over time have been known, but nothing was known for combinations of them. Bounding the flow value of each path is also interesting in the classical maximum s-t-flow problem. We study the case that each path may carry at most one unit of flow and prove that this restriction makes the maximum s-t-flow problem strongly NP-hard. In contrast to the classical maximum s-t-flow problem, the fractional and the integral problem diverge strongly with the new restriction. For the integral problem, we even prove APX-hardness. We develop an FPTAS for the fractional problem and an O(log m) -approximation algorithm for the integral one. (Here, m is the number of arcs in the network under consideration.) Similar results emerge for the multicommodity case. For the objective to find a maximum integral multicommodity flow our asymptotic approximation ratio of O(m^{ 0. 5 }) is proven to be best possible, unless P = NP...|$|R
40|$|This paper speci 8 ̆ 5 es {{how to do}} policy {{simulations}} {{with alternative}} instrument-rate paths in DSGE models such as Ramses, the Riksbanks main model for policy analysis and fore-casting. The new element is that these alternative instrument-rate paths are anticipated by the private sector. Such simulations correspond to situations where the Riksbank transpar-ently announces that it plans to implement a particular instrument-rate path and where this announcement is believed by the private sector. Previous methods have instead implemented al-ternative instrument-rate paths by adding unanticipated shocks to an instrument rule, as in the method of modest interventions by Leeper and Zha (2003). This corresponds to a very di¤erent situation where the Riksbank would nontransparently and secretly plan to implement deviations from an announced instrument rule. Such deviations are in practical simulations normally both serially correlated and large, which seems inconsistent {{with the assumption that}} they would remain unanticipated by the private sector. Simulations with anticipated instrument-rate paths seem more relevant for the transparent exible ination targeting that the Riksbank conducts. We provide an algorithm for the computation of policy simulations with arbitrary restrictions on nominal and real instrument-rate paths for an arbitrary number of periods after which a given policy rule, including targeting rules and explicit, implicit, or forecast-based instrument rules is implemented. When ination projections are su ¢ ciently sensitive to the real interest-rate <b>path,</b> <b>restrictions</b> on real interest-rate paths provide more intuitive and robust results, whereas restrictions on nominal interest-rate path may provide somewhat counter-intuitive results. JEL Classi 8 ̆ 5 cation: E 52, E 5...|$|R
40|$|In simple – but {{selected}} – quantum systems, {{the probability}} distribution {{determined by the}} ground state wave function is infinitely divisible. Like all simple quantum systems, the Euclidean temporal extension leads to a system that involves a stochastic variable and which can be characterized by a probability distribution on continuous <b>paths.</b> The <b>restriction</b> of the latter distribution to sharp time expectations recovers the infinitely divisible behavior of the ground state probability distribution, {{and the question is}} raised whether or not the temporally extended probability distribution retains the property of being infinitely divisible. A similar question extended to a quantum field theory relates to whether or not such systems would have nontrivial scattering behavior. Introduction, Discussion & Proposition Preliminary detail...|$|R
40|$|In {{experiments}} where failure {{times are}} sparse, degradation analysis {{is useful for}} the analysis of fail-ure time distributions in reliability studies. This re-search investigates the link between a practitioner’s selected degradation model and the resulting lifetime model. Simple additive and multiplicative mod-els with single random effects are featured. Re-sults show that seemingly innocuous assumptions of the degradation <b>path</b> create surprising <b>restrictions</b> on the lifetime distribution. These constraints are described in terms of failure rate and distribution classes...|$|R
40|$|Photograph of La Cañada Flintridge {{real estate}} sign, Southern California, 1926. "'Distinction' to Reside in Flintridge; Fire and Police Protection, Schools, Five Golf Courses, Two Riding Academies in {{immediate}} Vicinity, Twenty Miles Bridle <b>Paths,</b> Protective <b>Restrictions,</b> On All Lots, Reasonable Prices and Terms; Comparative Distances, New Los Angeles Civic Center and Flintridge; Burbank, Oakmont Country Club, Flintridge, Glendale, Rose Bowl Stadium, Brookside Park, Amandale Country Club, L. A. Municipal Golf Course, Universal City, Cahuenga Pass, Midwick Country Club, L. A. Civic Center, Wilshire Country Club, Caballero Country Club, Encino Country Club, Hollywood Country Club, Beverly Hills Hotel, L. A. Country Club, Holmby Hills, Westwood; Hillcrest Country Club, Rancho Country Club, Brentwood Country Club, The Riviera, Topanga, Santa Monica Canyon, Palms, Culver City, Ocean Park, Venice, Inglewood, Pacific Ocean, Altadena, Pasadena High School, San Marino, San Gabriel, Monrovia, Montebello, Whittier, Hermosa" [...] on sign...|$|R
40|$|The work {{reported}} here {{is part of}} the PROGRES (PROgrammed Graph Rewriting Systems) project. PROGRES is a very high level multi paradigm language for the specification of complex structured data types and their operations. The data structures are modelled as directed, attributed, node and edge labeled graphs (diane graphs). The basic programming constructs of PROGRES are graph rewriting rules (productions and tests) and derived relations on nodes (<b>paths</b> and <b>restrictions).</b> Although graph rewriting systems have successfully been used for specification purposes in many application areas since about 20 years, there was no sufficient tool available for the execution and implementation of graph grammar specifications. Especially, the problem of efficiently searching for a redex for an arbitrary given rewrite rule has been unsolved for a long time. In this paper we propose a new, heuristic, graph based algorithm solving this graph pattern matching problem. This algorithm has been implemented [...] ...|$|R
40|$|The work {{reported}} here {{is part of}} the PROGRES (PROgrammed Graph Rewriting Systems) project. PROGRES is a very high level multi paradigm language for the specification of complex structured data types and their operations. The data structures are modelled as directed, attributed, node and edge labelled graphs (diane graphs). The basic programming constructs of PROGRES are graph rewriting rules (productions and tests) and derived relations on nodes (<b>paths</b> and <b>restrictions).</b> These basic operations may be combined to build partly imperative, partly rule based, complex graph transformations by special control structures which regard the nondeterministic nature of graph rewriting rules. In order to use PROGRES not only as a specification language but also for building rapid prototypes and even for the derivation of final implementations, {{we have to be able}} to execute PROGRES specifications efficiently. The central problem in executing a graph rewriting system is to match the left-hand side [...] ...|$|R
40|$|The work {{reported}} here {{is part of}} the PROGRES (PROgrammed Graph Rewriting Systems) project. PROGRES is a very high level multi paradigm language for the specification of complex structured data types and their operations. The data structures are modelled as directed, attributed, node and edge labelled graphs (diane graphs). The basic programming constructs of PROGRES are graph rewriting rules (productions and tests) and derived relations on nodes (<b>paths</b> and <b>restrictions).</b> These basic operations may be combined to build partly imperative, partly rule based, complex graph transformations by special control structures which regard the nondeterministic nature of graph rewriting rules. PROGRES offers its users a convenient, partly textual, partly graphical concrete syntax and a rich system of consistency checking rules for the underlying calculus of programmed diane-graph rewriting systems. This paper presents the key techniques used for the execution of PROGRES programs. We will discuss a [...] ...|$|R
40|$|Lattice paths {{effectively}} model {{phenomena in}} chemistry, physics and probability theory. Asymptotic enumeration of lattice paths is linked with entropy {{in the physical}} systems being modeled. Lattice paths restricted to {{different regions of the}} plane are well suited to a functional equation approach for exact and asymptotic enumeration. This thesis surveys results on lattice <b>paths</b> under various <b>restrictions,</b> with an emphasis on lattice paths in the quarter plane. For these paths, we develop an original systematic combinatorial approach providing direct access to the exponential growth factors of the asymptotic expressions. Comment: Master's thesis of the author, 97 page...|$|R
40|$|Abstract. Recently {{more and}} more {{structured}} data in form of RDF triples have been published and integrated into Linked Open Data (LOD). While the current LOD contains hundreds of data sources with billions of triples, it has {{a small number of}} distinct relations compared with the large number of entities. On the other hand, Web pages are growing rapidly, which results in much larger number of textual contents to be exploited. With the popularity and wide adoption of open information extraction technology, extracting entities and relations among them from text at the Web scale is possible. In this paper, we present an approach to extract the subject individuals and the object counterparts for the relations from text and determine the most appropriate domain and range as well as the most confident dependency path patterns for the given relation based on the EM algorithm. As a preliminary results, we built a knowledge base for relations extracted from Chinese encyclopedias. The experimental results show the effectiveness of our approach to extract relations with reasonable domain, range and <b>path</b> pattern <b>restrictions</b> as well as high-quality triples...|$|R
40|$|We {{define and}} study a metaplectically {{covariant}} class of pseudo-differential operators acting on functions on symplectic space and generalizing a modified {{form of the}} usual Weyl calculus. This construction requires a precise calculation of the twisted Weyl symbol of a class of generators of the metaplectic group {{and the use of}} a Conley–Zehnder type index for symplectic <b>paths,</b> defined without <b>restrictions</b> on the endpoint. Our calculus is related the usual Weyl calculus using a family of isometries of L 2 (R n) on closed subspaces of L 2 (R 2 n) and to an irreducible representation of the Heisenberg algebra distinct from the usual Schrödinger representation...|$|R
40|$|We {{consider}} {{a class of}} lattice <b>paths</b> with certain <b>restrictions</b> on their ascents and down steps and use them as building blocks to construct various families of Dyck paths. We let every building block $P_j$ take on $c_j$ colors and count all of the resulting colored Dyck paths of a given semilength. Our approach is to prove a recurrence relation of convolution type, which yields a representation in terms of partial Bell polynomials that simplifies the handling of different colorings. This allows us to recover multiple known formulas for Dyck paths and related lattice paths in an unified manner. Comment: 10 pages. Submitted for publicatio...|$|R
40|$|We look at {{walks of}} n+k unit {{distance}} steps into North and East direction {{starting at the}} origin (0; 0) and ending at (n; k). The number of such <b>paths</b> without further <b>restrictions</b> is n + k k as exactly k of the n + k steps are eastward steps. (n,k) This number can also be interpreted {{as the number of}} walks of n + k unit distance steps on the real axis that start at 0 and end at n k. Now we consider the case where n = k and look only at those paths that remain under or on the diagonal. The number of such paths is the n-th Catalan numbe...|$|R
40|$|Random {{field with}} <b>paths</b> given as <b>restrictions</b> of {{holomorphic}} functions to Euclidean space-time can be Wick-rotated by pathwise analytic continuation. Euclidean symmetries of the correlation functions then {{go over to}} relativistic symmetries. As a concrete example, convoluted point processes with interactions motivated from quantum field theory are discussed. A general scheme {{for the construction of}} Euclidean invariant infinite volume measures for systems of continuous particles with ferromagnetic interaction is given and applied to the models under consideration. Connections with Euclidean quantum field theory, Widom-Rowlinson and Potts models are pointed out. For the given models, pathwise analytic continuation and analytically continued correlation functions are shown to exist and to expose relativistic symmetries. Comment: 18 page...|$|R
40|$|We {{perform a}} {{comprehensive}} theoretical and experimental {{analysis of the}} use of all-different constraints. We prove that generalized arc-consistency on such constraints lies between neighborhood inverse consistency and, under a simple <b>restriction,</b> <b>path</b> inverse consistency on the binary representation of the problem. By generalizing the arguments of Kondrak and van Beek, we prove that a search algorithm that maintains generalized arc-consistency on all-different constraints dominates a search algorithm that maintains arc-consistency on the binary representation. Our experiments show the practical value of achieving these high levels of consistency. For example, we can solve almost all benchmark quasigroup completion problems up to order 25 with just a few branches of search. These results demonstrate the benefits of using non-binary constraints like all-different to identify structure in problems. ...|$|R
40|$|Cataloged from PDF {{version of}} article. All-optical Wavelength Division Multiplexing networks, {{providing}} extremely large bandwidths, {{are among the}} most promising solutions for the increasing need for high-speed data transport. In all-optical networks, data is transmitted solely in the optical domain along lightpaths from source to destination without being converted into the electronic form, and each lightpath is restricted to use the same wavelength on all the links along its <b>path.</b> This <b>restriction</b> is known as the wavelength continuity constraint. Optical wavelength conversion can increase the performance and capacity of optical networks by removing this restriction and relaxing the wavelength continuity constraint. However, optical wavelength conversion is a difficult and expensive technology. In this study, we analyze the problem of placing limited number of wavelength converting nodes in a multi- fiber network with static traffic demands. Optimum placement of wavelength converting nodes is an NP-complete problem. We propose a tabu search based heuristic algorithm for this problem. The objective of the algorithm is to achieve the performance of full wavelength conversion in terms of minimizing the total number of fibers used in the network by placing minimum number of wavelength converting nodes. Numerical results comparing the performance of the algorithm with the optimum solutions are presented. The proposed algorithm gives quite satisfactory results, it also has a relatively low computational complexity making it applicable to large scale networks. Şengezer, NamıkM. S...|$|R
40|$|The Trans-diagnostic Model (TM) {{of eating}} {{pathology}} describes how {{one or more}} of four hypothesized mechanisms (i. e., mood intolerance, core low self-esteem, clinical perfectionism and interpersonal difficulties) may interrelate {{with each other and with}} the core psychopathology of eating disorders (i. e., over-evaluation of weight and shape) to maintain the disordered behaviors. Although a cognitive behavioral treatment based on the TM has shown to be effective in treating eating disorders, the model itself has undergone only limited testing. This is the first study to both elaborate and test the validity of the TM in a large sample (N= 605) of undergraduate men. Body mass index was controlled within structural equation modeling analyses. Although not all expected associations for the maintenance variables were significant, overall the validity of the model was supported. Concern about shape and weight directly led to exercise behaviors. There was a direct path from binge eating to exercise and other forms of compensatory behaviors (i. e., purging); but no significant <b>path</b> from <b>restriction</b> to binge eating. Of the maintaining factors, mood intolerance was the only maintaining variable directly linked to men's eating disorder symptoms. The other three maintaining factors of the TM indirectly impacted restriction through concerns about shape and weight, whereas only interpersonal difficulties predicted low self-esteem and binge eating. Potential implications for understanding and targeting eating disturbances in men are discussed...|$|R
40|$|As {{manufacturers}} are pushing their {{research and development}} toward more simulation based and computer aided methods, vehicle dynamics modeling and simulation become more important than ever. The challenge lies in how to utilize the new technology to its fullest, delivering the best possible performance given certain objectives and current restrictions. Here, optimization methods in different forms can be a tremendous asset. However, the solution to an optimization problem will always rely on the problem formulation, where model validity plays a crucial role. The main emphasis in this thesis lies within methodology and analysis of optimal control oriented topics for safety-critical road-vehicle maneuvers. A crucial element here is the vehicle models. This is investigated as a first study, evaluating {{the degree to which}} different model configurations can represent the lateral vehicle dynamics in critical maneuvers, where it is shown that even the low-complexity models describe the most essential vehicle characteristics surprisingly well. How to formulate the optimization problems and utilize optimal control tools is not obvious. Therefore, a methodology for road-vehicle maneuvering in safety-critical driving scenarios is presented, and used in evaluation studies of various vehicle model configurations and different road-surface conditions. It was found that the overall dynamics is described similarly for both the high- and low-complexity models, as well as for various road-surface conditions. If more information about the surroundings is available, the best control actions might differ from the ones in traditional safety systems. This is also studied, where the fundamental control strategies of classic electronic stability control is compared to the optimal strategy in a safety-critical scenario. It is concluded that the optimal braking strategy not only differs from the traditional strategies, but actually counteracts the fundamental intentions from which the traditional systems are based on. In contrast to passenger cars, heavy trucks experience other characteristics due to the different geometric proportions. Rollover is one example, which has to be considered in critical maneuvering. Model configurations predicting this phenomenon are investigated using optimal control methods. The results show that the simple first go-to models have to be constrained very conservatively to prevent rollover in more rapid maneuvers. In vehicle systems designed for path following, which has become a trending topic with the expanding area of automated driving, the requirements on vehicle modeling can be very high. These requirements ultimately depend on several various properties, where the <b>path</b> <b>restrictions</b> and <b>path</b> characteristics are very influential factors. The interplay between these path properties and the required model characteristics is here investigated. In situations where a smooth path is obtained, low-complexity models can suffice if path deviation tolerances are set accordingly. In more rapid and tricky maneuvers, however, vehicle properties such as yaw inertia are found to be important. Several of the included studies indicate that vehicle models of lower complexity can describe the overall dynamics sufficiently in critical driving scenarios, which is a valuable observation for future development...|$|R
40|$|Although the {{intention}} of RDF is to provide an open, minimally constraining way for representing information, there exists an increas-ing number of applications for which guarantees on the structure and values of an RDF data set become desirable if not essential. The RDF Data Description Language (RDD) was designed to tackle this problem. With RDDs, data maintainers can define a rich set of integrity constraints (such as keys, cardinality <b>restrictions,</b> <b>path</b> constraints, etc.) to tie RDF data sets to quality guarantees, akin to schemata of relational databases, or to DTDs in XML. Making constraints explicit {{by means of an}} associated RDD de-scription (which is coupled to a specific RDF data set or SPARQL endpoint) not only helps in maintaining data quality, but also eases the formulation of precise queries for users and system developers. This handbook documents the RDD Checker v 2. 11, a tool tha...|$|R
40|$|A timed {{automaton}} {{is said to}} {{have the}} simple path property (SPP) if any given sequence of states is traversable if and only if it is traversable by a simple <b>path.</b> With this <b>restriction</b> it becomes simpler to do iterative timing verification and the problem of language emptiness becomes decidable regardless of the form of the timing constraints (e. g., non-linear inequalities among multiple clocks are allowed). Unfortunately, it is hard to determine if an automaton has the SPP, and the SPP may not be preserved upon composition of automata. In this study, we present an algorithm that makes full use of the SPP without needing to determine if the automaton possesses this property. No restrictions are posed on the form of the timing constraints. If the automaton does not have the SPP, our algorithm is still a complete decision procedure in all cases where iterative algorithms exist. Otherwise, it becomes a semi-decision procedure. The iterative timing verification algorithms in the [...] ...|$|R
40|$|When {{consumers}} have constant or non-existent planning costs 8 ̆ 5 rms can always achieve maximum pro 8 ̆ 5 ts {{with a single}} 8 ̆ 5 xed price. However, when planning costs vary with con-sumersvaluations a single 8 ̆ 5 xed price is not always su ¢ cient for pro 8 ̆ 5 t maximization. In such cases it is optimal for the 8 ̆ 5 rm to provide an array of purchasing options. High valuation consumers will purchase with certainty at a high price while lower valuation consumers will opt for a lottery which gives some probability of being allowed to purchase at a discounted price and no opportunity to purchase otherwise. In general this type of mechanism may be di ¢ cult to implement, but in markets for time-speci 8 ̆ 5 c goods any menu of lotteries can be realized by a descending price <b>path</b> with quantity <b>restrictions</b> at the discount prices. John Hegeman 2 9 / 5 / 2006...|$|R
50|$|In 1937, {{developer}} E.P. Taylor, {{who designed}} the Don Mills community, purchased a large plot of land north of the Bridle Path. The estate, named Windfields by his wife, is occupied today by the Canadian Film Centre. The park through which Wilket Creek flows behind this parcel of land is known as Windfields Park. In the late forties, Taylor's business partner George Montegu Black, Jr (father of Conrad Black) moved into the area and built a large mansion on Park Lane Circle. In an effort to control who his future neighbours would be, Black took over the company that owned the rolling farmland that was to become the Bridle <b>Path,</b> and set <b>restrictions</b> in place through the North York zoning by-law; only single-family dwellings could be built, and they only on minimum lot sizes of 2 acres. The area was subdivided into about 50 lots, each selling for $25,000 at the time, and through the Fifties it began to take shape.|$|R
40|$|The energy {{efficiency}} evaluation of trawlers {{becomes more and}} more relevant as the trend of the government laws follows the <b>path</b> of <b>restrictions</b> to new constructions to encourage improvement to existing vessels, often equipped with outdated technologies. The two conditions a trawler need to meet during its operations, namely the sailing and the trawling phases, require in principle very different performances. Despite the higher speed reached when cruising from ports to fishing grounds, this type of ships experiences increased resistance as well as heavy loaded propellers during the trawling operations at low speed. In the light of the specific tasks of such a kind of vessel, a preliminary assessment of the {{energy efficiency}} of the ship-system is of crucial interest to optimize both the operating costs and the impact on the sea environment. In this context, hydrodynamic performances and the propulsion system have direct consequences on the fuel consumption, hence on the operational costs of the vessel. In the proposed model, the engine fuel consumption is evaluated by suitable statistic regression to account for the prime mover performance into the propulsion chain. Ship resistance in calm water and added resistance due to rough sea conditions are computed by a Boundary Element Method. Both are then integrated into the global efficiency assessment method. An application of energy efficiency assessment and optimization of a hard-chine 18 meters trawler is presented. The final results are shown as part of a wider decision support system to optimize the overall performance of the vessel operation in different environmental conditions...|$|R
40|$|We {{introduce}} {{a model for}} the computation of a multicommodity flow together with back-up flows for each commodity after any of a given set of failure states occurs in a network. We call such a total routing strategy a restorable multicommodity flow. One advantage of restorable flows over classical network protection methods, such as disjoint paths, is that the commodities may effectively share capacity in the network for their back-up flows. We consider the problem of finding a minimum expectedcost restorable flow, under a given probability distribution on network element failures. The underlying stochastic optimization problem can be modelled as a large-scale linear programming (LP) problem that explicitly incorporates the network failure scenarios. The size of the LPs grows swiftly, however, in the size of network and number of failure states. Our focus is on developing a scalable combinatorial algorithm for this problem. This leads us to specialize the problem to the case where traffic flows for each commodity are restricted to a (pre-computed) collection of disjoint <b>paths.</b> This <b>restriction</b> also makes it easier to restore traffic after network failures; specifically, restoration can be performed by the commodity’s endpoints without rerouting any of the traffic which was not disturbed by the network failure. In this setting, devise an approximation scheme for the feasibility problem that is based only on repeatedly pushing flow along cheapest pairs of paths. The algorithms are easily described and readily implementable. We give computational results relating how our approach scales for large networks in comparison with general-purpose LP solvers. An earlier version of this paper, with identical results, appeared in [9]...|$|R
40|$|As long {{as there}} are taxes, the {{incentive}} for evasion will exist as well. Studies for tax evasion are of interest in different fields like that of economics, public finance, personal finance, business administration, business finance, financial accounting, in the banking system etc. However, {{there are only a few}} studies about the internal causes and financial incentives that oblige decision-makers of small firms towards tax evasion. When we refer to business tax evasion, always brings to mind sensational cases of large businesses that evade taxes, but business tax evasion is a widespread phenomenon even to small firms. To be more competitive, small business must have a consistently entrepreneurial orientation, but limited financing prevents this, therefore the business savings from tax evasion is believed to be an internally funding <b>path.</b> The <b>restrictions</b> on small firms financing often make them orientate more toward internal generation of funds, which also has few alternatives. As a way to internal saving, firms often find tax evasion, which is not only a deviant and unethical behavior, but also puts firms into many difficulties in the long run. Consequently, the study aims to discuss the phenomenon of tax evasion in the managerial practice of small business in an Albanian region, as well as its financial cause as is perceived by the small business. The study findings report that the deviant behavior of businesses from taxes considers the need for internal financing as an important reason. The study concluded that savings from tax evasion is an alternative of internal financing primarily for small and early-staged firms, and that if entrepreneurs are capable of a good business model and competitive strategy, they will not need to make evasion...|$|R
30|$|Macroscopic rail models {{describe}} the network {{by means of}} a graph whose nodes indicate the various stations and whose links usually define the frequency and travel times of the various trains. The main advantages of macro-approaches lie in the fact that they require a limited set of input data and a low computational effort. As this makes them able to deal with large-size networks in a reasonable computation time, they are usually implemented in planning phases to carry out strategic evaluations related to different infrastructure scenarios or solve routing problems which consist in defining train <b>paths</b> without time <b>restrictions.</b> However, the low degree of detail adopted in the network representation affects the accuracy of results: they usually contain far fewer nodes and links compared to the microscopic model and consider infrastructure in a more abstract manner. This means that such models are unable to reproduce some aspects such as signalling equipment installed and layout of station tracks and thus are unable to detect train conflicts or provide a reliable estimation of running times.|$|R
40|$|Abstract. In {{this report}} we {{describe}} how both, memory and time re-quirements for stochastic model checking of SPDL (stochastic propo-sitional dynamic logic) formulae can significantly be reduced. SPDL is the stochastic {{extension of the}} multi-modal program logic PDL. SPDL provides means to specify path-based properties with or without timing <b>restrictions.</b> <b>Paths</b> can be characterised by so-called programs, essentially regular expressions, where the executability can be made dependent on the validity of test formulae. For model-checking SPDL path formulae {{it is necessary to}} build a product transition system (PTS) between the system model and the program automaton belonging to the path formula that is to be verified. In many cases, this PTS can be drastically reduced during the model checking procedure, as the program restricts the num-ber of potentially satisfying paths. Therefore, we propose an approach that directly generates the reduced PTS from a given SPA specification and an SPDL path formula. The feasibility of this approach is shown through a selection of case studies, which show enormous state space reductions, at no increase in generation time. ...|$|R
40|$|The {{purpose of}} this paper is to develop the LOGIT type {{stochastic}} (equilibrium) assignment that satisfies three requirements: first, the path set for the loading should be defined by the simple <b>paths</b> without any <b>restrictions,</b> which do not depend on the travel cost pattern; secondly, the model should give small probabilities for the paths with mazy structure, while the number of paths defined by the first requirement itself is enormous; and finally, the model should be efficiently computable even in large scale networks. To achieve the purpose, we employ two strategies: first, we incorporate not only the conventional travel times but also the geometric attributes of paths into the model; and secondly, we construct the algorithm making fully use of the Markov property of LOGIT model. The incorporation of the geometric aspects into the model is not only natural to satisfy the second requirement but also effectively utilized to achieve the first requirement. Moreover, the strategy combined with the Markov chain theory enables the model to inherit the computational efficiency of Markov Chain Assignment (MCA) developed by SASAKI (1965), BELL (1995) and AKAMATS...|$|R
