7|38|Public
5000|$|In {{addition}} to using Akamai's own servers, Akamai delivers content from other end-users' computers, {{in a form}} of peer-to-peer networking. When users request a download of some large files served by this system, it prompts them to download and install [...] "Akamai NetSession Interface," [...] a download manager used to reduce download time and to increase quality. However, this software operates not merely as a download manager (delivering content from the Internet to the user's computer) {{but also as a}} <b>peer-to-peer</b> <b>server,</b> delivering content cached on the user's computer to other users' computers.|$|E
40|$|Observations on game server {{discovery}} mechanisms Networked First Person Shooter (FPS) {{games are}} amongst {{the most popular}} multiuser applications on the Internet today. At any given time, {{there are thousands of}} servers available to a potential player. We describe and analyse the existing mechanisms for locating these game servers. The mechanisms are found to be inefficient and do not scale well. We propose and describe a distributed <b>peer-to-peer</b> <b>server</b> discovery mechanism. This is a work in progress...|$|E
40|$|SHORE (Scalable Heterogeneous Object REpository) is a {{persistent}} object system under {{development at the}} University of Wisconsin. SHORE represents a merger of objectoriented database and file system technologies. In this paper we give the goals and motivation for SHORE, and describe how SHORE provides features of both technologies. We also describe some novel aspects of the SHORE architecture, including a symmetric <b>peer-to-peer</b> <b>server</b> architecture, server customization through an extensible value-added server facility, and support for scalability on multiprocessor systems. An initial version of SHORE is already operational, and we expect a release of Version 1 in mid- 1994. 1 Introduction SHORE (Scalable Heterogeneous Object REpository) is a new persistent object system under development at the University of Wisconsin that represents a merger of object-oriented database (OODB) and file system technologies. While {{the past few years}} have seen significant progress in the OODB area, most ap [...] ...|$|E
2500|$|Attackers {{have found}} a way to exploit a number of bugs in <b>peer-to-peer</b> <b>servers</b> to {{initiate}} DDoS attacks. The most aggressive of these peer-to-peer-DDoS attacks exploits DC++. With peer-to-peer there is no botnet and the attacker does not have to communicate with the clients it subverts. Instead, the attacker acts as a [...] "puppet master," [...] instructing clients of large peer-to-peer file sharing hubs to disconnect from their peer-to-peer network and to connect to the victim's website instead.|$|R
40|$|Distance {{estimation}} {{is important}} to many Internet applications. It can aid a WWW client when selecting among several potential candidate servers, or among candidate <b>peer-to-peer</b> <b>servers.</b> It can also aid in building efficient overlay or peer-to-peer networks that dynamically react {{to change in the}} underlying Internet. One of the approaches to distance (i. e., time delay) estimation in the Internet is based on placing Tracer stations in key locations and conducting measurements between them. The Tracers construct an approximated map of the Internet after processing the information obtained from these measurements...|$|R
50|$|OpenNap is a <b>peer-to-peer</b> service <b>server</b> software. It {{was created}} as an open source Napster server, {{extending}} the Napster protocol to allow sharing of any media type, and adding {{the ability to}} link servers together.|$|R
40|$|The Shortest Remaining Processing Time (SRPT) {{scheduling}} {{policy was}} proven, in the 1960 s, to yield the smallest mean response time, and recently it was proven its performance gain over Processor Sharing (PS) usu-ally {{does not come}} at the expense of large jobs. How-ever, despite the many advantages of SRPT scheduling, it is not widely applied. One important reason for the spo-radic application of SRPT scheduling is that accurate job size information is often unavailable. Our previ-ous work addressed the performance and fairness issues of SRPT scheduling when job size information is inaccu-rate. We found that SRPT (and FSP) scheduling outper-forms PS as long as there exists a (rather small) amount of correlation between the estimated job size and the ac-tual job size. In the work we summarize here, we have developed job size estimation techniques to support the ap-plication of SRPT to web server and <b>Peer-to-Peer</b> <b>server</b> side scheduling. We have evaluated our techniques with ex-tensive simulation studies and real world implementation and measurement. 1...|$|E
40|$|Summary: OmicBrowse is a browser {{to explore}} {{multiple}} datasets coordinated in the multidimensional omic space integrating omics knowledge ranging from genomes to phenomes and connecting evolutional correspondences among multiple species. OmicBrowse integrates multiple data servers {{into a single}} omic space through secure <b>peer-to-peer</b> <b>server</b> communications, so that a user can easily obtain an integrated view of distributed data servers, e. g. an integrated view of numerous whole-genome tiling-array data retrieved from a user’s in-house private-data server, along with various genomic anno-tations from public internet servers. OmicBrowse is especially appro-priate for positional-cloning purposes. It displays both genetic maps and genomic annotations within wide chromosomal intervals and assists a user to select candidate genes by filtering their annotations or associated documents against user-specified keywords or ontology terms. We also show that an omic-space chart effectively represents schemes for integrating multiple datasets of multiple species. Availability: OmicBrowse is developed by the Genome-Phenome Superbrain Project and is released as free open-source softwar...|$|E
40|$|Abstract — A <b>peer-to-peer</b> <b>server</b> {{network system}} {{consists}} {{of a large number}} of autonomous servers logically connected in a peer-to-peer way where each server maintains a collection of documents. When a query of storing new documents is received by the system, a distributed search process determines the most relevant servers and redirects the documents to them for processing (compressing and storing at the right document base). In this paper, we model this distributed search process as a distributed sequential decision making problem using a set of interactive Markov Decision Processes (MDP), a specific stochastic game approach, which represent each server’s decision making problem. The relevance of a server to a document is regarded as a reward considering the capacity of the storage and the goodness score of a server. We show that using a central MDP to derive an optimal policy of how to distribute documents among servers leads to high complexity and is inappropriate to the distributed nature of the application. We present then interactive MDPs approach transforming this problem into a decentralized decision making process. I...|$|E
40|$|Abstract—Distance {{estimation}} {{is important}} to many Internet applications. It can aid a World Wide Web client when selecting among several potential candidate servers, or among candidate <b>peer-to-peer</b> <b>servers.</b> It can also aid in building efficient overlay or peer-to-peer networks that dynamically react {{to change in the}} underlying Internet. One of the approaches to distance (i. e., time delay) estimation in the Internet is based on placing tracer stations in key locations and conducting measurements between them. The tracers construct an approximated map of the Internet after processing the information obtained from these measurements. This work presents a novel algorithm, based on algebraic tools, that computes additional distances, which are not explicitly measured. As such, the algorithm extracts more information from the same amount of measurement data. Our algorithm has several practical impacts. First, it can reduce the number of tracers and measurements without sacrificing information. Second, our algorithm is able to compute distance estimates between locations where tracers cannot be placed. To evaluate the algorithm’s performance, we tested it both on randomly generated topologies and on real Internet measurements. Our results show that the algorithm computes up to 50 %– 200 % additional distances beyond the basic tracer-to-tracer measurements. Index Terms—Delay inference, end-to-end measurements, network tomography...|$|R
40|$|Distance {{estimation}} {{is important}} to many Internet applications. It can aid a WWW client when selecting among several potential candidate servers, or among candidate <b>peer-to-peer</b> <b>servers.</b> It can also aid in building efficient overlay or peer-to-peer networks that dynamically react {{to change in the}} underlying Internet. One of the approaches to distance (i. e., time delay) estimation in the Internet is based on placing Tracer stations in key locations and conducting measurements between them. The Tracers construct an approximated map of the Internet after processing the information obtained from these measurements. This work presents a novel algorithm, based on Algebraic tools, that computes additional distances, which are not explicitly measured. As such, the algorithm extracts more information from the same amount of measurement data. Our algorithm has several practical impacts. First, it can reduce the number of Tracers and measurements without sacrificing information. Second, our algorithm is able to compute distance estimates between locations where Tracers cannot be placed. To evaluate the algorithm’s performance, we tested it both on randomly generated topologies and on real Internet measurements. Our results show that the algorithm computes up to 50 - 200 % additional distances beyond the basic Tracer-to-Tracer measurements. A. Background I...|$|R
50|$|Unlike <b>peer-to-peer</b> transfers, XDCC <b>servers</b> {{are often}} hosted on {{connections}} with very high upstream bandwidth, sometimes {{in excess of}} 100 Mbit. Often FTP servers are also running on the XDCC servers to facilitate uploading of materials to them. Many XDCC servers run on security compromised computers.|$|R
50|$|Amaryllo was {{the first}} to {{establish}} global <b>Peer-to-Peer</b> (P2P) <b>server</b> based on WebRTC protocol in smart home service. Amaryllo Live is a plug-in-free H.264-based browser service allowing consumers to access their cameras anywhere anytime. It runs on WebRTC protocol and Firefox is the first browser company to support Amaryllo Live. Other browser companies have vowed to support WebRTC H.264-based codec. Amaryllo server measures each communications channel and dynamically adjusts the video format based on available bandwidth to effectively alleviate video latency and results in a better video communications experience. It also permits a true two-way audio talk.|$|R
40|$|Organizations {{increasingly}} {{define their}} software development projects as “virtual project teams”, where project members {{from within the}} organization cooperate with outside experts and therefore build a “community”, which in many cases o 1 perates as a highly distributed team. Process modeling,- composition and – configuration are substantial ingredients for team activities. This paper analyses the needs for process awareness for virtual teams and derives the requirements for location-transparent distributed and mobile collaboration. It describes the design of virtual project teams that contain mobile users, their artifacts, and processes. The paper distills a framework for process aware virtual project teams that goes beyond current technologies such as <b>peer-to-peer,</b> multiple <b>servers</b> and clients, as well as Workflow Management and Groupware systems...|$|R
50|$|The {{two-level}} (client and <b>server)</b> <b>peer-to-peer</b> {{network architecture}} offered {{a balance between}} centralized systems like Napster, and decentralized systems like Gnutella. Where Napster ultimately proved to be vulnerable was its centralized server cluster, which was a stable target for legal action. Gnutella's original design, featuring total elimination of the server network in favor of purely peer-to-peer searching, quickly proved to be infeasible due to massive search traffic overhead between peers.|$|R
40|$|Nowadays, all {{universities}} {{provide their}} own dedicated intranet systems. However, only {{few of them}} provide students and teachers features like internal communica-tion system, context aware information in the campus and perhaps, all of this acces-sible from a smartphone. We propose an Android OS application dedicated to the students of Aalborg Uni-versity, capable to achieve two main func-tionalities accessible from a smartphone: a Chat System and an Indoor Geolocation System. The Chat {{is based on a}} <b>Peer-To-Peer</b> ar-chitecture, no <b>server</b> either administrator...|$|R
40|$|Conventional load {{balancing}} schemes are efficient at increasing {{the utilization of}} CPU, memory, and disk I/O resources in a Distributed environment. Most of the existing load-balancing schemes ignore network proximity and heterogeneity of nodes. Load balancing becomes more challenging as load variation is very large and the load on each server may change dramatically over time, by the time when a server {{is to make the}} load migration decision, the collected load status from other servers may no longer be valid. This will affect the accuracy, and hence the performance, of the {{load balancing}} algorithms. All the existing methods neglect the heterogeneity of nodes and contextual load balancing. In this seminar, context based load balancing and task allocation with network proximity of heterogeneous nodes will be studied. Index Terms: Proximity-aware, <b>peer-to-peer,</b> virtual <b>server,</b> load balancing, distributed algorithms, random walks...|$|R
40|$|In our {{previous}} work,we introduced the Wigan <b>Peer-to-Peer</b> database <b>server,</b> {{which is based}} on the popular BitTorrent file-sharing protocol. In Wigan, users (peers) cache the results of queries they receive and make these available to future users. A central component, known as the Tracker, keeps a record of which users have submitted which queries and uses this record to provide a new user submitting a query with a list of one or more peers that already have these query results. In this paper, we describe the query matching process which occurs at the Tracker, thus highlighting the differences between query matching in a P 2 P database and file matching in a P 2 P filesharing system and the challenges these differences posed. © 2010 University of Newcastle upon Tyne. Printed and published by the University of Newcastle upon Tyne...|$|R
5000|$|DPMS saw {{its debut}} in beta {{versions}} of DR DOS [...] "Panther" [...] in October 1992, which, besides others, came with DPMS-enabled versions of the Super PC-Kwik disk cache, the SuperStor disk compression, and DEBUG as [...] "stealth" [...] protected mode system debugger. The PCMCIA card services CS in PalmDOS were DPMS-enabled as well. Later retail products such as Novell DOS 7 and Personal NetWare 1.0 in December 1993 also came with many DPMS-enabled drivers such as the file deletion tracking component DELWATCH 2.00, the adaptive disk cache NWCACHE 1.00, NWCDEX 1.00, a CD-ROM redirector extension, the <b>peer-to-peer</b> networking <b>server</b> SERVER 1.20, and STACKER 3.12, the disk compression component. DPMS was also provided by Caldera OpenDOS 7.01, DR-DOS 7.02 and 7.03, which, {{at least in some}} releases, added DPMS-enabled issues of DRFAT32 (a FAT32 redirector extension), LONGNAME (VFAT long filename support) and VDISK (virtual RAM disk). DR-DOS 7.03 contains the latest version of DPMS 1.44.|$|R
50|$|Bitcoin uses {{proof of}} work to {{maintain}} consensus in its peer-to-peer network. Nodes in the bitcoin network attempt to solve a cryptographic proof-of-work problem, where probability of finding the solution {{is proportional to the}} computational effort, in hashes per second, expended, and the node that solves the problem has their version of the block of transactions added to the <b>peer-to-peer</b> distributed timestamp <b>server</b> accepted by all of the other nodes. As any node in the network can attempt to solve the proof-of-work problem, a Sybil attack becomes unfeasible unless the attacker has over 50% of the computational resources of the network.|$|R
40|$|Computer-supported {{collaborative}} learning (CSCL) is an emerging branch of learning science concerned with studying {{how people can}} learn together {{with the help of}} computers. As an indispensable ingredient, computer mediation evolves through couples of phases, e. g. centralized <b>server,</b> <b>peer-to-peer</b> network, grid computing. Nevertheless, with daily rising trend on requirement’s dynamic changes in service, existing models fall short to respond on demand. Thus, in this paper, by taking advantage of cloud computing, we propose a feasible CSCL platform and the experiments show that it can not only fulfill the basic requirement of CSCL, but also respond to learner’s dynamic need on demand...|$|R
40|$|In {{recent times}} {{peer-to-peer}} networking has gained popularity since users can share files {{without having to}} communicate with a dedicated <b>server.</b> <b>Peer-to-peer</b> network have a scalable and fault-tolerant system that can locate nodes on a network with no need to maintain routing state {{having a lot of}} information. As the bandwidth of internet connection has increased as well as the low cost, it has resulted in increase use of peer-to-peer networks. Peerto-Peer networks have the ability to deliver contents and services to other users easily. In this paper we discuss about the various architecture of peer-to-peer network...|$|R
40|$|Abstract. Peer-to-Peer (P 2 P) {{computing}} {{is widely}} recognized as a promising paradigm for building next generation distributed applications. However, the autonomous, heterogeneous, and decentralized nature of participating peers introduces the following challenge for resource sharing: how to make peers profitable in the untrusted P 2 P environment? To address the problem, we present a self-policing and distributed approach by combining two models: PET, a personalized trust model, and M-CUBE, a multiple-currency based economic model, {{to lay a foundation}} for resource sharing in untrusted P 2 P computing environments. PET is a flexible trust model that can adapt to different requirements, and provides the solid support for the currency management in M-CUBE. M-CUBE provides a novel self-policing and quality-aware framework for the sharing of multiple resources, including both homogeneous and heterogeneous resources. We evaluate the efficacy and performance of this approach {{in the context of a}} real application, a <b>peer-to-peer</b> Web <b>server</b> sharing. Our results show that our approach is flexible enough to adapt to different situations and effective to make the system profitable, especially for systems with large scale...|$|R
40|$|Abstract – {{peer-to-peer}} architectures derives to a {{large extent}} from their ability to function, scale and selforganize {{in the presence of a}} highly transient population of nodes, network and computer failures, without the need of a central <b>server,</b> <b>Peer-to-peer</b> networks are capable of being wounded to peers who cheat, propagate malicious code, leech on the network, or simply do not cooperate. The traditional security techniques developed for the centralized distributed systems like client-server networks are insufficient for P 2 P networks by the virtue of their centralized nature. The absence of a central authority in a P 2 P network poses unique challenges for identity management in the network. These challenges include Reputation of the peers, secure directory data management, Sybil attacks...|$|R
40|$|In {{the past}} few years, {{peer-to-peer}} networks have become an extremely popular mechanism for large-scale content sharing. Unlike traditional client-server applications, which centralize the management of data in a few highly reliable <b>servers,</b> <b>peer-to-peer</b> systems distribute the burden of data storage, computation, communications and administration among thousands of individual client workstations. While the popularity of this approach, exemplified by systems such as Gnutella [3], was driven by the popularity of unrestricted music distribution, newer work has expanded the potential application base to generalized distributed file systems [1, 4], persistent anonymous publishing [5], as well as support for high-quality video distribution [2]. The widespread attraction of the peer-to-peer model arises primarily from its potential for both low-cost scalability and enhanced availability. Ideally a peer-to-peer system could efficiently multiplex the resources and connectivity o...|$|R
40|$|Abstract—Many {{solutions}} {{have been}} proposed to tackle the load balancing issue in DHT-based P 2 P systems. However, all these solutions either ignore the heterogeneity nature of the system, or reassign loads among nodes without considering proximity relationships, or both. In this paper, we present an efficient, proximity-aware load balancing scheme by using the concept of virtual servers. To {{the best of our}} knowledge, this is the first work to use proximity information in load balancing. In particular, our main contributions are: 1) Relying on a self-organized, fully distributed k-ary tree structure constructed on top of a DHT, load balance is achieved by aligning those two skews in load distribution and node capacity inherent in P 2 P systems—that is, have higher capacity nodes carry more loads; 2) proximity information is used to guide virtual server reassignments such that virtual servers are reassigned and transferred between physically close heavily loaded nodes and lightly loaded nodes, thereby minimizing the load movement cost and allowing load balancing to perform efficiently; and 3) our simulations show that our proximity-aware load balancing scheme reduces the load movement cost by 11 - 65 percent for all the combinations of two representative network topologies, two node capacity profiles, and two load distributions of virtual servers. Moreover, we achieve virtual server reassignments in Oðlog NÞ time. Index Terms—Proximity-aware, <b>peer-to-peer,</b> virtual <b>server,</b> load balancing. ...|$|R
40|$|Abstract—To {{guarantee}} the streaming quality in live peerto-peer (P 2 P) streaming channels, it {{is preferable to}} provision adequate levels of upload capacities at dedicated streaming servers, compensating for peer instability and time-varying peer upload bandwidth availability. Most commercial P 2 P streaming systems have resorted {{to the practice of}} over-provisioning a fixed amount of upload capacity on streaming servers. In this paper, we have performed a detailed analysis on 10 months of run-time traces from UUSee, a commercial P 2 P streaming system, and observed that available server capacities are not able {{to keep up with the}} increasing demand by hundreds of channels. We propose a novel online server capacity provisioning algorithm that proactively adjusts server capacities available to each of the concurrent channels, such that the supply of server bandwidth in each channel dynamically adapts to the forecasted demand, taking into account the number of peers, the streaming quality, and the channel priority. The algorithm is able to learn over time, has full ISP awareness to maximally constrain P 2 P traffic within ISP boundaries, and can provide differentiated streaming qualities to different channels by manipulating their priorities. To evaluate its effectiveness, our experiments are based on an implementation of the algorithm which replays real-world traces. Index Terms—Distributed applications, <b>peer-to-peer</b> streaming, <b>server</b> bandwidth provisioning, multiple channels I...|$|R
25|$|Maintenance {{requires}} sufficient servers and bandwidth, and {{a dedicated}} support staff. Insufficient resources for maintenance lead to lag and frustration for the players, and can severely damage {{the reputation of}} a game, especially at launch. Care {{must also be taken}} to ensure that player population remains at an acceptable level by adding or removing <b>servers.</b> <b>Peer-to-peer</b> MMORPGs could theoretically work cheaply and efficiently in regulating server load, but practical issues such as asymmetrical network bandwidth, CPU-hungry rendering engines, unreliability of individual nodes, and inherent lack of security (opening fertile new grounds for cheating) can make them a difficult proposition. The hosted infrastructure for a commercial-grade MMORPG requires the deployment of hundreds (or even thousands) of servers. Developing an affordable infrastructure for an online game requires developers to scale large numbers of players with less hardware and network investment.|$|R
40|$|National audienceIn 3 D {{collaborative}} environments, users needs interactivity and real-time updates. With web-based applications, such requirement {{implies that}} conventional client-server -alone- {{is no longer}} enough. To overcome this unmet need, we propose a hybrid client <b>server</b> <b>peer-to-peer</b> (P 2 P) communication model based on pluginless web standards enabling users to design collaboratively 3 D scenes. The client part includes a WebGL editor to visualize and edit 3 D scenes while the server side provides data and ensure persistence. Using the WebRTC protocol, a P 2 P mesh is generated to transmit directly the updates through a scenes working group. The feasibility of our approach is demonstrated with a web-based prototype submitted to a qualitative evaluation highlighting the usage of WebRTC for direct 3 D data transmission with low latency and high throughput, and WebGL for 3 D rendering...|$|R
50|$|Maintenance {{requires}} sufficient servers and bandwidth, and {{a dedicated}} support staff. Insufficient resources for maintenance lead to lag and frustration for the players, and can severely damage {{the reputation of}} a game, especially at launch. Care {{must also be taken}} to ensure that player population remains at an acceptable level by adding or removing <b>servers.</b> <b>Peer-to-peer</b> MMORPGs could theoretically work cheaply and efficiently in regulating server load, but practical issues such as asymmetrical network bandwidth, CPU-hungry rendering engines, unreliability of individual nodes, and inherent lack of security (opening fertile new grounds for cheating) can make them a difficult proposition. The hosted infrastructure for a commercial-grade MMORPG requires the deployment of hundreds (or even thousands) of servers. Developing an affordable infrastructure for an online game requires developers to scale large numbers of players with less hardware and network investment.|$|R
40|$|Live {{streaming}} {{is used in}} today’s Internet {{to broadcast}} TV channels and radio stations, usually by deploying streaming <b>servers.</b> <b>Peer-to-peer</b> applications have become popular {{in recent years to}} overcome the limitations of centralized servers. This thesis proposes StreamTorrent, a peer-to-peer-based live streaming protocol. In contrast to most protocols, StreamTorrent provides robustness, efficiency, and scalability by combining different strategies. StreamTorrent has the efficiency of typical tree-based and the robustness of more random protocols. An overlay with small diameter and localityawareness reduces latency and network load. And because peer-to-peer computing is about collaboration among peers, incentives are given to peers to share their resources to ensure good playback quality. The StreamTorrent protocol has been implemented and can be used both to perform simulations and to stream in the real world. The simulations allow to evaluate the protocol and to perform automated tests for a large number of peers. The StreamTorren...|$|R
40|$|SUMMARY The massively {{multiplayer}} {{online game}} (MMOG) industry {{is suffering from}} huge outgoing traffic from centralized servers. To accommodate this traffic, game companies claim large bandwidth to Internet Data Centers (IDCs), and several months’ payment for that bandwidth is likely to even exceed the cost for MMOG servers. In this paper, we propose a MMOG server architecture to reduce outgoing bandwidth consumption from MMOG servers. The proposed architecture distributes some functions of servers to selected clients, and those clients {{are in charge of}} event notification to other clients {{in order to reduce the}} outgoing traffic from servers. The clients with server functions communicate with each other in peer-to-peer manner. We analyze traffic reduction as a function of cell-daemonable ratio of clients, and the results show that up to 80 % of outgoing traffic from servers can be reduced using the proposed architecture when 10 % of clients are cell-daemonable. key words: MMOG, <b>server,</b> <b>peer-to-peer,</b> hybrid, traffic 1...|$|R
40|$|This paper {{investigates the}} {{assignment}} of audio mixing operations to a geographically distributed set of servers to provide an immersive audio communication environment for massively multiplayer online games. The immersive audio communication service enables each avatar to hear a realistic audio mix of the conversations in its audible range. There are three primary delivery architectures for this service, namely, <b>peer-to-peer,</b> central <b>server</b> and distributed servers. We focus on the distributed server architecture, which partitions the virtual world into regions or locales and then assigns the computation associated {{with the creation of}} audio scenes for all avatars in each locale to a server. Our aim is to find the optimal way to partition the virtual world into locales and then choose the locale servers {{in such a way that}} reduces the total delay perceived by all avatars. We have produced a mathematical formulation for the optimal partitioning and server assignment and developed a heuristics approach based on a graph algorithm. We have developed a simulation environment that creates both the physical world (geographic distribution of participants and the Internet topology model) and the virtual world (distribution of avatars based on different avatar aggregation behaviors). We have solved the problem exactly as well as using the heuristics algorithm for a range of simulated virtual and physical worlds. In many cases, the heuristics results were within 5 % of the optimal. Our algorithms and simulation study will be of benefit to future immersive audio communication service providers in the design of a cost effective delivery architecture for this service...|$|R
40|$|In this paper, a {{distributed}} proxy {{architecture is}} introduced for the provisioning of an immersive audio communication service to massively multi-player online games. The immersive audio communication service enables each avatar {{to hear a}} realistic audio mix of conversations in its hearing range. In our earlier work, <b>peer-to-peer</b> and central <b>server</b> architectures have been proposed for this service. In this paper, a distributed proxy architecture with either using network multicast or unicast between proxies is introduced to address {{the limitations of the}} previous architectures. The main focus {{of this paper is to}} evaluate the bandwidth cost saving of network multicast in the distributed proxy architecture in different avatar grouping behaviours and distribution of game player scenarios. In addition, the effect of varying the number of proxy servers on communication delays and network bandwidth usages are investigated. We have developed a simulation environment that creates both the physical world (geographic distribution of participants and the Internet topology model) and the virtual world (distribution of avatars based on different avatar aggregation behaviors). Based on the simulation study, we provide recommendations on a cost-effective delivery architecture for this service...|$|R
40|$|A Peer-to-peer mode is a {{desirable}} operation mode for SIP based VoIP system since it will pose a least traffic load to the network. However, the operation will usually fail when {{one or both}} of the clients are behind a NAT or firewall. Using a server-client mode operation can usually solve the NAT problem, but the transmission delay reduction is a challenging problem when the server load is high. In order for the VoIP system to operate efficiently, it is necessary to keep the system in the peer-to-peer mode whenever possible. In this research, a multi-server cluster VOIP system is proposed. This server cluster consists of a <b>peer-to-peer</b> based master <b>server</b> with a group of slave servers. The slave servers will normally operate in the client-server mode. For a service that does not need client-server operation, the master server will directly handle the necessary SIP signaling and leave the clients in the peer-to-peer mode for their data stream transmission to reduce the network load. If a service needs to operate in a client-server mode, one of the slave server will be chosen for the operation. Depending on the server load and other factors, the number of the slave servers can be changed accordingly...|$|R
40|$|Video-on-Demand (VoD) is a {{compelling}} application, but costly due to the load it places on <b>servers.</b> <b>Peer-to-peer</b> (P 2 P) techniques hold the potential to reduce centralized costs by sharing data between peers. There are many difficult design issues associated with P 2 P for VoD. Viewing the problem as designing a large distributed cache, {{many of the issues}} can be expressed in terms of caching algorithms. In an earlier paper [6], we studied the performance of Grid-Cast, a P 2 P VoD system deployed on CERNET. From system traces, we found that departure misses are the major cause of server load. Motivated by this finding, this paper examines how to use replication to decrease departure misses and thereby further reduce server load. This paper proposes and evaluates a framework for lazy replication. Lazy replication postpones replication, trying to make efficient use of bandwidth. In our framework, two predictors are plugged in to create the working replication algorithm. Lazy replication with several predictors is compared with a naïve eager replication algorithm. We find that lazy replication is more efficient than eager replication, even when using two simple predictors. With these two simple predictors, lazy replication can decrease server load by 15 % from multivideo caching with only a minor increase in network traffic. 1...|$|R
