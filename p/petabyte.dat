338|780|Public
25|$|A {{significant}} portion of the Saudi Aramco workforce consists of geophysicists and geologists. Saudi Aramco has been exploring for oil and gas reservoirs since 1982. Most of this process takes place at the Exploration and Petroleum Engineering Center (EXPEC). Originally, Saudi Aramco used Cray Supercomputers (CRAY-1M) in its EXPEC Computer Center (ECC) to assist in processing the colossal quantity of data obtained during exploration and in 2001, ECC decided to use Linux clusters as a replacement for the decommissioned Cray systems. ECC installed a new supercomputing system in late 2009 with a disk storage capacity of 1,050 terabytes (i.e, exceeding one <b>petabyte),</b> the largest storage installation in Saudi Aramco's history to support its exploration in the frontier areas and the Red Sea.|$|E
500|$|U2 {{developed}} {{a style of}} editing in their previous concert films that involved fast cutting between shots, which Owens wanted to retain in U2 3D. Because fast cutting in 3D would lead to motion sickness or eye strain, the film was edited to incorporate dissolves of at least four frames between shots. Many of the transitions were created by layering several frames of footage {{on top of one}} another into composite images. Each of the layered frames featured a different depth of field to enhance the 3D effects, and up to five images were layered together in a single shot. This made U2 3D the first 3D film to feature composite images with more than two layers, and the first to be edited specifically to prevent the viewer from experiencing motion sickness or eye strain. Software did not exist at the time to layer the 3D images, so new software had to be developed. [...] Because the project was captured in high-definition video, each frame used nearly 20 megabytes of data on 3ality Digital's servers, and the entire film used almost a <b>petabyte</b> (1015 bytes). The 3D editing process took longer than Owens expected, and consequently, the project went over budget, costing $15million to produce. Video editing took 17months, and the final film was cut to a length of 85minutes—seven shorter than originally announced.|$|E
500|$|The lead {{visual effects}} company was Weta Digital in Wellington, New Zealand, {{at one point}} {{employing}} 900 people {{to work on the}} film. Because of the huge amount of data which needed to be stored, cataloged and available for everybody involved, even {{on the other side of}} the world, a new cloud computing and Digital Asset Management (DAM) system named Gaia was created by Microsoft especially for Avatar, which allowed the crews to keep track of and coordinate all stages in the digital processing. To render Avatar, Weta used a [...] server farm making use of 4,000 Hewlett-Packard servers with 35,000 processor cores with 104 terabytes of RAM and three petabytes of network area storage running Ubuntu Linux, Grid Engine cluster manager, and 2 of the animation software and managers, Pixar's RenderMan and Pixar's Alfred queue management system. The render farm occupies the 193rd to 197th spots in the TOP500 list of the world's most powerful supercomputers. A new texturing and paint software system, called Mari, was developed by The Foundry in cooperation with Weta. Creating the Na'vi characters and the virtual world of Pandora required over a <b>petabyte</b> of digital storage, and each minute of the final footage for Avatar occupies 17.28 gigabytes of storage. Often, it would take each frame of the movie several hours to render. To help finish preparing the special effects sequences on time, a number of other companies were brought on board, including Industrial Light & Magic, which worked alongside Weta Digital to create the battle sequences. ILM was responsible for the visual effects for many of the film's specialized vehicles and devised a new way to make CGI explosions. Joe Letteri was the film's visual effects general supervisor.|$|E
5000|$|Telecommunications (capacity): The world's {{effective}} capacity to exchange information through two-way telecommunication networks was 281 <b>petabytes</b> {{of information in}} 1986, 471 <b>petabytes</b> in 1993, 2,200 <b>petabytes</b> in 2000, and 65,000 <b>petabytes</b> in 2007 (this is the informational equivalent to every person exchanging 6 newspapers per day).|$|R
5000|$|Internet: Google {{processed}} about 24 <b>petabytes</b> of data {{per day in}} 2009. The BBC's iPlayer {{is reported}} to have transferred up to 7 <b>petabytes</b> each month in 2010. Imgur transfers about 4 <b>petabytes</b> of data per month.|$|R
50|$|The Cheyenne {{supercomputer}} {{was built}} by Silicon Graphics International Corporation (SGI) in coordination with centralized file system and data storage components provided by DataDirect Networks (DDN). The SGI high-performance computer is a 5.34-petaflops system, meaning it can carry out 5.34 quadrillion calculations per second. The new data storage system for Cheyenne is integrated with NCAR’s existing GLADE file system. The DDN storage provides an initial capacity of 20 <b>petabytes,</b> expandable to 40 <b>petabytes</b> {{with the addition of}} extra drives. This, combined with the current 16 <b>petabytes</b> of GLADE, totals 36 <b>petabytes</b> of high-speed storage as of February 2017.|$|R
2500|$|Other {{databases}} {{available only}} {{from within the}} library include Nature, IEEE and Wiley science journals, Wall Street Journal archives, and Factiva. [...] Overall, the digital holdings for the Library consist {{of more than a}} <b>petabyte</b> of data as of 2015.|$|E
2500|$|The {{detector}} generates unmanageably {{large amounts}} of raw data: about 25 megabytes per event (raw; zero suppression reduces this to 1.6 MB), multiplied by 40 million beam crossings per second {{in the center of}} the detector. This produces a total of 1 <b>petabyte</b> of raw data per second. [...] The trigger system uses simple information to identify, in real time, the most interesting events to retain for detailed analysis. [...] There are three trigger levels. The first is based in electronics on the detector while the other two run primarily on a large computer cluster near the detector. [...] The first-level trigger selects about 100,000 events per second. [...] After the third-level trigger has been applied, a few hundred events remain to be stored for further analysis. [...] This amount of data still requires over 100megabytes of disk space per second – at least a <b>petabyte</b> each year.|$|E
2500|$|The {{limits of}} {{specific}} approaches {{continue to be}} tested, as boundaries are reached through large scale experiments, e.g., in 2011 IBM ended its participation in the Blue Waters petaflops project at the University of Illinois. The Blue Waters architecture {{was based on the}} IBM POWER7 processor and intended to have 200,000 cores with a <b>petabyte</b> of [...] "globally addressable memory" [...] and 10 petabytes of disk space. The goal of a sustained petaflop led to design choices that optimized single-core performance, and hence a lower number of cores. The lower number of cores was then expected to help performance on programs that did not scale well to a large number of processors. The large globally addressable memory architecture aimed to solve memory address problems in an efficient manner, for the same type of programs. Blue Waters had been expected to run at sustained speeds of at least one petaflop, and relied on the specific water-cooling approach to manage heat. In the first four years of operation, the National Science Foundation spent about $200 million on the project. IBM released the Power 775 computing node derived from that project's technology soon thereafter, but effectively abandoned the Blue Waters approach.|$|E
30|$|One more {{example is}} Kaiser Permanente medical network based in California. It {{has more than}} 9 million members, {{estimated}} to manage large volumes of data ranging from 26.5 <b>Petabytes</b> to 44 <b>Petabytes.</b> [7].|$|R
50|$|The {{system can}} scale {{between one and}} six 19-inch rack cabinets. It can hold a maximum of 2,048 SAS {{high-density}} 2.5-inch drives for 1.2 <b>petabytes</b> of capacity, or 1,280 3.5-inch SATA drives for a maximum capacity of 2.5 <b>petabytes.</b>|$|R
5000|$|As a result, only {{working with}} less than 0.001% of the sensor stream data, the data flow from all four LHC {{experiments}} represents 25 <b>petabytes</b> annual rate before replication (as of 2012). This becomes nearly 200 <b>petabytes</b> after replication.|$|R
5000|$|The <b>petabyte</b> is a {{multiple}} of the unit byte for digital information. The prefix peta indicates the fifth power of 1000 and means 1015 in the International System of Units (SI), and therefore 1 <b>petabyte</b> is one quadrillion (short scale) bytes, or 1 billiard (long scale) bytes. The unit symbol for the <b>petabyte</b> is PB.|$|E
5000|$|... 2010 - Penguin Computing Cluster named Pacman with 2080 CPUs and 89 TB Filesystem, Sun SPARC Enterprise T5440 Server named Bigdipper with 7 <b>Petabyte</b> Storage Capacity, Cray XE6 named Chugach with 11648 CPUs and 330 TB Filesystem, Sun SPARC Enterprise T5440 Server named Wiseman with 7 <b>Petabyte</b> Storage Capacity, Cray XE6 named Tana with 256 CPUs and 2.36 TFLOPS ...|$|E
5000|$|Magic Pocket - Dropbox's {{file system}} that powers their Diskotech <b>petabyte</b> storage {{machines}} ...|$|E
50|$|MareNostrum 4 has a {{disk storage}} {{capacity}} of 14 <b>Petabytes</b> and {{is connected to}} BSC’s big data facilities, which have a total capacity of 24.6 <b>Petabytes.</b> Like its predecessors, MareNostrum 4 will also be connected to European research centres and European universities via the RedIris and Géant networks.|$|R
5000|$|Digital archives: The Internet Archive surpassed 15 <b>petabytes,</b> [...]|$|R
5000|$|DNA: stores {{information}} in DNA nucleotides. It was first done in 2012 when researchers achieved {{a rate of}} 1.28 <b>petabytes</b> per gram of DNA. In March 2017 scientists reported that a new algorithm called a DNA fountain achieved 85% of the theoretical limit, at 215 <b>petabytes</b> per gram of DNA.|$|R
50|$|Already, over 1 <b>Petabyte</b> of {{data are}} {{transferred}} every day via the GÉANT backbone network.|$|E
5000|$|Examples {{of the use}} of the <b>petabyte</b> to {{describe}} data sizes in different fields are: ...|$|E
5000|$|... {{virtualization}} technology provides {{external storage}} device support to enable tiered storage up to 16 <b>Petabyte</b> ...|$|E
5000|$|... the Wayback Machine {{reportedly}} contained around 15 <b>petabytes</b> of data.|$|R
25|$|This {{results in}} more than 33 <b>petabytes</b> (33000 terabytes) of storage.|$|R
5000|$|Folding@home (Scientific Data): Folding@home has {{generated}} 0.5 <b>petabytes</b> of simulated data.|$|R
50|$|Petascale {{can also}} refer to very large storage systems where the {{capacity}} exceeds one <b>petabyte</b> (PB).|$|E
5000|$|The binary {{approximation}} of the peta-, or 1,000,000,000,000,000 multiplier. 1,125,899,906,842,624 bytes = 1 <b>petabyte</b> (or pebibyte).|$|E
5000|$|Optical jukebox - hold {{massive amounts}} of data on {{multiple}} discs allowing scalability into the <b>petabyte</b> range.|$|E
5000|$|... {{approximately}} six <b>petabytes</b> of raw {{disk storage}} with high input/output bandwidth; ...|$|R
50|$|GlusterFS: Clustered Distributed Filesystem having {{ability to}} scale up to several <b>petabytes.</b>|$|R
50|$|In 2010 two <b>petabytes</b> {{of photos}} were stored on the Amazon S3 service.|$|R
50|$|Version 16 {{brings a}} {{re-engineered}} column store for extreme, <b>petabyte</b> scale, data volumes, and more extreme data compression.|$|E
50|$|The {{explosion}} of video {{on the net}} is another disruptive element. The Amesterdam Internet exchange (AMS-IX), which handles approximately 20% of Europe’s traffic, saw its aggregate data traffic increase from 1.75 <b>Petabyte</b> per day in November 2007 to an expected 4 <b>Petabyte</b> per day in November 2009. Much of this rapid increase in traffic is driven by widespread use of voice and, in particular, video over the Internet.|$|E
50|$|The {{facility}} {{also contains}} a <b>petabyte</b> High Performance Storage System (HPSS) that currently stores upwards of 29 petabytes of data.|$|E
5000|$|Databases: Teradata Database 12 has a {{capacity}} of 50 <b>petabytes</b> of compressed data.|$|R
5000|$|Games: World of Warcraft uses 1.3 <b>petabytes</b> {{of storage}} to {{maintain}} its game.|$|R
50|$|Mobile {{data traffic}} doubled {{between the end}} of 2011 (~620 <b>Petabytes</b> in Q4 2011) and the end of 2012 (~1280 <b>Petabytes</b> in Q4 2012). This traffic growth is and will {{continue}} to be driven by large increases in the number of mobile subscriptions and by increases in the average data traffic per subscription due to increases in the number of smartphones being sold, the use of more demanding applications and in particular video, and the availability and deployment of newer 3G and 4G technologies capable of higher data rates. By 2018 total mobile broadband traffic is expected to increase by a factor of 12 to roughly 13,000 <b>PetaBytes.</b>|$|R
