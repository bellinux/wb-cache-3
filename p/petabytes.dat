816|306|Public
5|$|The {{effective}} capacity to exchange information worldwide through two-way telecommunication networks grew from 281 <b>petabytes</b> of (optimally compressed) information in 1986, to 471 <b>petabytes</b> in 1993, to 2.2 (optimally compressed) exabytes in 2000, and to 65 (optimally compressed) exabytes in 2007. This is the informational equivalent of two newspaper pages {{per person per}} day in 1986, and six entire newspapers {{per person per day}} by 2007. Given this growth, telecommunications play an increasingly important role in the world economy and the global telecommunications industry was about a $4.7 trillion sector in 2012. The service revenue of the global telecommunications industry was estimated to be $1.5 trillion in 2010, corresponding to 2.4% of the world’s gross domestic product (GDP).|$|E
5|$|UM {{maintains}} one of {{the largest}} centralized academic cyber infrastructures in the country with numerous assets. The Center for Computational Science High Performance Computing group has been in continuous operation since 2007. Over that time the core has grown from a zero HPC cyberinfrastructure to a regional high-performance computing environment that currently supports more than 1,200 users, 220 TFlops of computational power, and more than 3 <b>Petabytes</b> of disk storage. The center's latest system acquisition, an IBM IDataPlex system, was ranked at number 389 on the November 2012 Top 500 Supercomputers.|$|E
25|$|This {{results in}} more than 33 <b>petabytes</b> (33000 terabytes) of storage.|$|E
5000|$|The <b>petabyte</b> is a {{multiple}} of the unit byte for digital information. The prefix peta indicates the fifth power of 1000 and means 1015 in the International System of Units (SI), and therefore 1 <b>petabyte</b> is one quadrillion (short scale) bytes, or 1 billiard (long scale) bytes. The unit symbol for the <b>petabyte</b> is PB.|$|R
5000|$|... 2010 - Penguin Computing Cluster named Pacman with 2080 CPUs and 89 TB Filesystem, Sun SPARC Enterprise T5440 Server named Bigdipper with 7 <b>Petabyte</b> Storage Capacity, Cray XE6 named Chugach with 11648 CPUs and 330 TB Filesystem, Sun SPARC Enterprise T5440 Server named Wiseman with 7 <b>Petabyte</b> Storage Capacity, Cray XE6 named Tana with 256 CPUs and 2.36 TFLOPS ...|$|R
5000|$|Magic Pocket - Dropbox's {{file system}} that powers their Diskotech <b>petabyte</b> storage {{machines}} ...|$|R
25|$|Data {{produced}} by LHC, {{as well as}} LHC-related simulation, was estimated at approximately 15 <b>petabytes</b> per year (max throughput while running not stated) - a major challenge {{in its own right}} at the time.|$|E
25|$|The term CERN is {{also used}} to refer to the laboratory, which in 2016 had 2,500 scientific, technical, and {{administrative}} staff members, and hosted about 12,000 users. In the same year, CERN generated 49 <b>petabytes</b> of data.|$|E
25|$|As of March 2008, NMCI {{included}} more than 363,000 computers, serving more than 707,000 Sailors, Marines and civilians in 620 {{locations in the}} continental United States, Hawaii, and Japan, making it the largest internal computer network in the world. The network's 4,100 servers handle over 2.3 <b>petabytes</b> of data.|$|E
50|$|Already, over 1 <b>Petabyte</b> of {{data are}} {{transferred}} every day via the GÉANT backbone network.|$|R
5000|$|Examples {{of the use}} of the <b>petabyte</b> to {{describe}} data sizes in different fields are: ...|$|R
5000|$|... {{virtualization}} technology provides {{external storage}} device support to enable tiered storage up to 16 <b>Petabyte</b> ...|$|R
25|$|Enterprise: SQL Server Enterprise Edition {{includes}} both the core database engine and add-on services, {{with a range}} of tools for creating and managing a SQL Server cluster. It can manage databases as large as 524 <b>petabytes</b> and address 12 terabytes of memory and supports 640 logical processors (CPU cores).|$|E
25|$|By 2012 {{data from}} over 6 {{quadrillion}} (6 x 1015) LHC proton-proton collisions had been analysed, LHC collision data was being produced at approximately 25 <b>petabytes</b> per year, and the LHC Computing Grid {{had become the}} world's largest computing grid (as of 2012), comprising over 170 computing facilities in a worldwide network across 36 countries.|$|E
25|$|Computer {{information}} systems {{researchers have also}} shown interest in eBay. Michael Goul, Chairman of the Computer Information Systems department of the W. P. Carey School of Business at Arizona State University, published an academic case based on eBay’s big data management and use in which he discusses how eBay is a data-driven company that processes 50 <b>petabytes</b> of data a day.|$|E
50|$|Petascale {{can also}} refer to very large storage systems where the {{capacity}} exceeds one <b>petabyte</b> (PB).|$|R
5000|$|The binary {{approximation}} of the peta-, or 1,000,000,000,000,000 multiplier. 1,125,899,906,842,624 bytes = 1 <b>petabyte</b> (or pebibyte).|$|R
5000|$|Optical jukebox - hold {{massive amounts}} of data on {{multiple}} discs allowing scalability into the <b>petabyte</b> range.|$|R
25|$|The LHC's {{computing}} grid is a {{world record}} holder. Data from collisions was produced at an unprecedented rate for the time of first collisions, tens of <b>petabytes</b> per year, a major challenge at the time, to be analysed by a grid-based computer network infrastructure connecting 170 computing centres in 42 countries as of 2017 – by 2012 the Worldwide LHC Computing Grid was also the world's largest distributed computing grid, comprising over 170 computing facilities in a worldwide network across 36 countries.|$|E
25|$|In {{the month}} of October 2009, it was {{revealed}} that the site experienced 70 million requests and transferred seven <b>petabytes</b> of data. Television formed about two thirds of all requests, with radio making up the rest. Most TV was streamed from pre-recorded footage, whereas live streaming was preferred of radio. Eighty-five percent of requests were from computers, with much of the rest coming from iPods, iPhones and PS3s (from a total of 15 platforms). The most popular TV programme of 2009 was Top Gear, and the most popular radio was that reporting The Ashes.|$|E
25|$|Unlike the K computer, the Tianhe-1A {{system uses}} a hybrid {{architecture}} and integrates CPUs and GPUs. It uses more than 14,000 Xeon general-purpose processors {{and more than}} 7,000 Nvidia Tesla general-purpose graphics processing units (GPGPUs) on about 3,500 blades. It has 112 computer cabinets and 262 terabytes of distributed memory; 2 <b>petabytes</b> of disk storage is implemented via Lustre clustered files. Tianhe-1 uses a proprietary high-speed communication network to connect the processors. The proprietary interconnect network {{was based on the}} Infiniband QDR, enhanced with Chinese made FeiTeng-1000 CPUs. In the case of the interconnect the system is twice as fast as the Infiniband, but slower than some interconnects on other supercomputers.|$|E
50|$|Version 16 {{brings a}} {{re-engineered}} column store for extreme, <b>petabyte</b> scale, data volumes, and more extreme data compression.|$|R
50|$|The {{explosion}} of video {{on the net}} is another disruptive element. The Amesterdam Internet exchange (AMS-IX), which handles approximately 20% of Europe’s traffic, saw its aggregate data traffic increase from 1.75 <b>Petabyte</b> per day in November 2007 to an expected 4 <b>Petabyte</b> per day in November 2009. Much of this rapid increase in traffic is driven by widespread use of voice and, in particular, video over the Internet.|$|R
5000|$|Proven {{scalability}} to over 54 billion triples (LUBM 200K benchmark) with scalability to the 8 <b>petabyte</b> {{limit of}} Oracle Database.|$|R
25|$|In June 2012, Google {{announced}} that it has captured 20 <b>petabytes</b> of data for Street View, comprising photos taken along 5 million miles of roads, covering 39 countries and about 3,000 cities. Coverage includes much of North and South America, from Cambridge Bay, Nunavut to Half Moon Island in the South Shetland Islands. Maps also include panoramic views taken under water such as in West Nusa Tenggara underwater coral, in the Grand Canyon, inside museums, and Liwa Desert in United Arab Emirates which are viewed from camelback. In a ten-day trek with Apa Sherpa, Google documented Khumbu, Nepal with its Mount Everest, Sherpa communities, monasteries and schools.|$|E
25|$|To {{find the}} Higgs boson, a {{powerful}} particle accelerator was needed, because Higgs bosons {{might not be}} seen in lower-energy experiments. The collider needed {{to have a high}} luminosity in order to ensure enough collisions were seen for conclusions to be drawn. Finally, advanced computing facilities were needed to process the vast amount of data (25 <b>petabytes</b> per year as of 2012) produced by the collisions. For the announcement of 4 July 2012, a new collider known as the Large Hadron Collider was constructed at CERN with a planned eventual collision energy of 14 TeV—over seven times any previous collider—and over 300 trillion (3×1014) LHC proton–proton collisions were analysed by the LHC Computing Grid, the world's largest computing grid (as of 2012), comprising over 170 computing facilities in a worldwide network across 36 countries.|$|E
25|$|The concerts were {{digitally}} {{recorded by}} 28 HD cameras, both human operated and robotic, collecting about 500 gigabytes of audio/video per hour—roughly a terabyte (TB) per show. To handle their data storage needs, U2 used several products from EMC Corporation, {{the first time}} the company had a musical client. To archive uncompressed footage and access it on-demand during the shows' production, the tour staff used an EMC VNXe3200 portable flash storage unit worth about US$25,000; it was configured with 22.9 TB of storage but was expandable up to 450 TB. After each show, tour staff used an EMC Data Domain 2500 system to back up footage; with storage up to 6.6 <b>petabytes</b> and an hourly throughput of 13.4 TB, the Data Domain system could complete a nightly backup before the crew disassembled the stage. On previous tours, U2 relied on USB flash drives for storage. Video imagery was loaded on the set's video screens with two d3 Technologies d3 4×4 media servers. Due to the need to load video on the fly, all storage was locally networked, as a cloud storage configuration would have increased latency. EMC's solution satisfied certain requirements dictated by the band, such as: mobility through a flight case form factor; expandable storage; and the capability to handle the large data loads from many cameras.|$|E
5000|$|Music: One <b>petabyte</b> {{of average}} MP3-encoded songs (for mobile, roughly one {{megabyte}} per minute), would require 2000 years to play.|$|R
50|$|During 2010, CeSViMa {{acquire a}} new massive storage system with 1 <b>petabyte</b> of {{capacity}} {{in parallel with}} the own storage of Magerit.|$|R
5000|$|... 2007 - Sun Opteron Cluster named Midnight with 2312 CPUs and 12.02 TFLOPS, StorageTek SL8500 Robotic Tape Library with 3+ <b>PetaByte</b> capacity.|$|R
500|$|The lead {{visual effects}} company was Weta Digital in Wellington, New Zealand, {{at one point}} {{employing}} 900 people {{to work on the}} film. Because of the huge amount of data which needed to be stored, cataloged and available for everybody involved, even {{on the other side of}} the world, a new cloud computing and Digital Asset Management (DAM) system named Gaia was created by Microsoft especially for Avatar, which allowed the crews to keep track of and coordinate all stages in the digital processing. To render Avatar, Weta used a [...] server farm making use of 4,000 Hewlett-Packard servers with 35,000 processor cores with 104 terabytes of RAM and three <b>petabytes</b> of network area storage running Ubuntu Linux, Grid Engine cluster manager, and 2 of the animation software and managers, Pixar's RenderMan and Pixar's Alfred queue management system. The render farm occupies the 193rd to 197th spots in the TOP500 list of the world's most powerful supercomputers. A new texturing and paint software system, called Mari, was developed by The Foundry in cooperation with Weta. Creating the Na'vi characters and the virtual world of Pandora required over a petabyte of digital storage, and each minute of the final footage for Avatar occupies 17.28 gigabytes of storage. Often, it would take each frame of the movie several hours to render. To help finish preparing the special effects sequences on time, a number of other companies were brought on board, including Industrial Light & Magic, which worked alongside Weta Digital to create the battle sequences. ILM was responsible for the visual effects for many of the film's specialized vehicles and devised a new way to make CGI explosions. Joe Letteri was the film's visual effects general supervisor.|$|E
2500|$|There {{has been}} growing trend to migrate from PACS to a Cloud Based RIS. A recent article by Applied Radiology said, [...] "As the digital-imaging realm is embraced across the {{healthcare}} enterprise, the swift transition from terabytes to <b>petabytes</b> of data has put radiology {{on the brink of}} information overload. Cloud computing offers the imaging department of the future the tools to manage data much more intelligently." ...|$|E
2500|$|Development on Hyperion {{started in}} 2011 and {{was based upon}} {{research}} into multi-bounce complex global illumination originally conducted at Disney Research in Zürich. Disney, in turn, had to assemble a new super-computing cluster just to handle Hyperion's immense processing demands, which consists of over 2,300 Linux workstations distributed across four data centers (three in Los Angeles and one in San Francisco). Each workstation, as of 2014, included a pair of 2.4GHz Intel Xeon processors, 256 GB of memory, {{and a pair of}} 300 GB solid-state drives configured as a RAID Level 0 array (i.e., to operate as a single 600 GB drive). [...] This was all backed by a central storage system with a capacity of five <b>petabytes,</b> which holds all digital assets as well as archival copies of all 54 Disney Animation films.|$|E
50|$|As of 6 May 2013, {{more than}} a <b>petabyte</b> of {{anonymous}} data had been synced between users, with over 70 terabytes synced daily.|$|R
5000|$|In May 2013 a 380 <b>Petabyte</b> HPSS {{installation}} entered {{service at}} the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign.|$|R
5000|$|TrueCar's [...] "massive up-to-the-minute {{database}} of selling prices" [...] {{is an example}} of big data at the <b>petabyte</b> level and is run on Apache Hadoop.|$|R
