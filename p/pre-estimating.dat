21|164|Public
40|$|Abstract. A new {{wideband}} DOA {{estimation method}} based on Khatri-Rao subspace using uniform focusing {{is presented in}} this paper. Due to using the uniform focusing matrix which does not require <b>pre-estimating</b> the DOAs of signals in advance, its resolution is improved obviously and its computation complexity is reduced greatly. Meanwhile, it can also resolve more signals {{than the number of}} array sensors. Theoretical analysis and simulation results demonstrate the effectiveness and efficiency of the method...|$|E
40|$|In this paper, a {{constrained}} moving horizon estimation (MHE) {{strategy for}} linear systems is proposed. Recently, {{the use of}} a <b>pre-estimating</b> linear observer in the forward prediction equations in the MHE cost function has been proposed {{in order to reduce the}} effects of uncertainty. Here we introduce state constraints within this formulation, and investigate stability properties in the presence of bounded disturbances and noise. The robustness and performance of the proposed observer is demonstrated with a simulation example...|$|E
40|$|In {{this paper}} we propose a unified {{framework}} for joint motion estimation/kinetic image reconstruction from gated dynamic PET data. The method is a generalisation of previous work to include gated data. The kinetic and motion parameters are estimated jointly by maximisation of the penalised likelihood. Kinetic parameters are estimated with an optimisation transfer approach, and the non-rigid motion is estimated with a quasi-Newton algorithm. Results on synthetic phantom data show that there is an advantage in jointly estimating motion and kinetics compared to <b>pre-estimating</b> the motion field for motion-compensated kinetic image reconstruction...|$|E
5000|$|... iii) Whether {{a genuine}} <b>pre-estimate</b> of damage is ascertainable ...|$|R
5000|$|... #Subtitle level 5: Uncalibrated, <b>pre-estimated</b> {{demographic}} data-free - PRAM ...|$|R
50|$|Each set of {{contract}} conditions contains in its Annexure of Schedule {{a space for}} inserting a rate for liquidated damages. This rate is a genuine <b>pre-estimate</b> of damages that the owner will incur if the project is not completed by the authorised date for completion. Once contractually accepted the rate will apply whether the actual damages are higher or lower than the <b>pre-estimate.</b> Liquidated damages are always calculated on calendar days.|$|R
40|$|In this paper, {{a moving}} horizon {{estimation}} (MHE) strategy for detectable linear systems is proposed. Like {{the idea of}} ’prestabilizing’ model predictive control, the states are estimated by a forward simulation with a <b>pre-estimating</b> observer in the MHE formulation. Compared with standard linear MHE approaches, it has more degrees of freedom to optimize the noise filtering. Tuning parameters are chosen to minimize the effects of measurement noise and model errors, which is implemented by finding tightest estimation error bounds. The performance of the proposed observer is demonstrated on one linear discrete-time example...|$|E
40|$|Context An {{uncomplicated}} and easy-to-use {{method of}} <b>pre-estimating</b> {{the severity of}} gallstone pancreatitis shortly after admission was {{required in order to}} direct clinicians for monitoring and/or transferring to a specialized center. Objective To determine the role of brief assessment in <b>pre-estimating</b> the severity of gallstone pancreatitis at admission. Patients Fifty-eight patients with gallstone pancreatitis were consecutively followed regarding the course of complications. Main outcome variables Sensitivity analysis of the Biliary Ranson score (> 3), the modified Imrie score (> 3), the acute physiology and chronic health evaluation II (APACHE-II) score (> 5), white blood cell count (> 14. 5 x 103 /dL), blood urea nitrogen (> 12 mg/dL), random blood sugar (> 150 mg/dL), pulse rate (> 100 beats/min) and combinations of the four brief assessments were compared using the z-test. Two-tailed P values less than 0. 05 were considered statistically significant. Results Both the Biliary Ranson score> 3 and the modified Imrie score> 3 had a sensitivity of 96. 4 % and a specificity of 96. 7 %. Both the APACHE-II score> 5 and random blood sugar> 150 mg/dL had a sensitivity of 89. 3 % which is comparable to a Biliary Ranson score> 3 and a modified Imrie score> 3. Conclusion As compared to a Biliary Ranson score> 3, a modified Imrie score> 3 and an APACHE-II score> 5, random blood sugar> 150 mg/dL can be considered as an oversimplified and effective prognostic indicator at admission in patients with gallstone pancreatitis...|$|E
30|$|It can be {{seen from}} Fig.  3 that the RSS-MUSIC [3] {{algorithm}}, the RSS-L 1 SVD [7] algorithm, and the FRFT-SSMUSIC [13] algorithm are all invalid. The main reason for the failure of RSS-MUSIC and RSS-L 1 SVD is that the CBF algorithm has a large error in <b>pre-estimating</b> the angles, due to source angle interval is less than the beam width of the array. In addition, the lower SNR also makes the estimation error larger. Although FRFT-SSMUSIC algorithm can remove the signal coherence using spatial smoothing, its angle resolution is not good. However, the proposed algorithm can estimate the angles of coherent wideband LFM signals very well and has high angular resolution.|$|E
30|$|Since a {{destination}} in vehicular networks may change its location at any moment, data packet delivered {{to its original}} position might not reach it. The <b>pre-estimated</b> position is the predicted current position of the destination, even if the destination already traveled a substantial distance from its initially known location. In Figure[*] 6, the distance of <b>pre-estimated</b> position P and the last known position C is S[*]=[*]TS[*]×[*]v, where the TS signifies the time elapsed since the destination at C, and v recorded in the evolutional state indicates the destination’s velocity. Obviously, sending data packets towards the predictive positions enhances the efficiency of data packet delivering process, reducing the path length and delay.|$|R
40|$|In this paper, {{we study}} the semidefinite inverse {{eigenvalue}} problem of reconstructing a real n-by-n matrix C {{such that it}} is nearest to the original <b>pre-estimated</b> real n-by-n matrix Co in the Frobenius norm and satisfies the measured partial eigendata, where the required matrix C should preserve the symmetry, positive semidefiniteness, and the prescribed entries of the <b>pre-estimated</b> matrix Co. We propose the alternating direction method of multipliers for solving the semidefinite inverse eigenvalue problem, where three related iterative algorithms are presented. We also extend our method {{to the case of}} lower bounds. Numerical experiments are reported to illustrate the efficiency of the proposed method for solving semidefinite inverse problems...|$|R
50|$|The {{motivation}} for <b>pre-estimate</b> aggregation {{may be that}} no ground truth data are available. This may occur in situations where diagnostics {{does a good job}} in detecting faults that are resolved (through maintenance) before system failure occurs. Therefore, there are hardly any run-to-failure data. However, there is incentive to know better when a system would fail to better leverage the remaining useful life {{while at the same time}} avoiding unscheduled maintenance (unscheduled maintenance is typically more costly than scheduled maintenance and results in system downtime). Garga et al. REF describe conceptually a <b>pre-estimate</b> aggregation hybrid approach where domain knowledge is used to change the structure of a neural network, thus resulting in a more parsimonius representation of the network. Another way to accomplish the <b>pre-estimate</b> aggregation is by a combined off-line process and on-line process: In the off-line mode, one can use a physics-based simulation model to understand the relationships of sensor response to fault state; In the on-line mode, one can use data to identify current damage state, then track the data to characterize damage propagation, and finally apply an individualized data-driven propagation model for remaining life prediction.|$|R
40|$|Proceedings of the IEEE International Conference on Telecommunications, 2010, p. 266 - 271 In this paper, we {{investigate}} the optimization problem of resource allocation in downlink of multiuser MISO-OFDM system. Multiple users with different BER and minimum transmission rate requirements are considered. We propose a novel heuristic allocation algorithm (HAA) of radio resource, which minimizes the total transmit {{power of the}} base station while meeting individual users'QoS requirements. The proposed algorithm combines antenna selection, subcarrier, bit and power allocation together, <b>pre-estimating</b> number of subcarriers assigned to each user and number of bits loaded for each subcarrier to reduce search number, reducing about 8 dB average bit SNR comparing with fixed allocation algorithm (FAA), and acquiring asymptotic average bit SNR of optimal allocation algorithm (OAA) with much lower complexity. © 2009 IEEE. published_or_final_versio...|$|E
40|$|A stereovision {{algorithm}} is proposed for visual odometry to estimate motion of mobile robot by providing feature pair sequence. It is composed of feature extracting, matching and tracking. Firstly, corners are extracted as features by Harris operator and grid-based optimizing. In feature matching and tracking, serious problems are caused by variable illumination between stereo images. An improved Moravec&# 39;s Normalized Cross Correlation (MNCC) {{algorithm is}} presented to reduce illumination affect in computing correspondence of corners. On current stereo image pair, extracted corners are matched by correlation-based bidirectional algorithm and outliers are rejected by epipolar constraint. Matched corners are tracked in pre-estimated search windows. The computational cost is greatly reduced by limiting number of corners, <b>pre-estimating</b> search window and feature local-updating. Simulation results validate that our algorithm is efficient and reliable...|$|E
40|$|We {{propose a}} {{real-time}} and accurate {{solution to the}} Perspective-n-Point (PnP) prob-lem –estimating the pose of a calibrated camera from n 3 D-to- 2 D point correspondences– that exploits {{the fact that in}} practice the 2 D position of not all 2 D features is estimated with the same accuracy. Assuming a model of such feature uncertainties is known in advance, we reformulate the PnP problem as a maximum likelihood minimization ap-proximated by an unconstrained Sampson error function, which naturally penalizes the most noisy correspondences. The advantages of this approach are clearly demonstrated in synthetic experiments where feature uncertainties are exactly known. <b>Pre-estimating</b> the features uncertainties in real experiments is, though, not easy. In this paper we model feature uncertainty as 2 D Gaussian distributions representing the sensitivity of the 2 D feature detectors to different camera viewpoints. When using these noise models with our PnP formulation we still obtain promising pose estimation results that outperform the most recent approaches. ...|$|E
3000|$|Note that {{performance}} analysis of filtering efficiency is usually carried out under {{an assumption that}} noise characteristics are known in advance (or accurately <b>pre-estimated)</b> and filter parameters are set according to certain recommendations [6, 9, 31]. Recall that, in our case, the thresholds for blocks are set to [...]...|$|R
40|$|A non-recursive {{version of}} Nonlinear Least Squares Fitting for {{frequency}} estimation is presented. This problem yields a closed-form solution exploiting a Taylor's series expansion. Respecting some conditions, the computational complexity is reduced, but equally the method assures that the accuracy reaches the Cramer-Rao Bound. The proposed method requires a frequency <b>pre-estimate.</b> A series of simulations {{has been made}} to determine how accurate the <b>pre-estimate</b> should be in order to ensure the achievement of the Cramer-Rao Bound in various conditions for different periodic signals. The execution time of the proposed algorithm is smaller compared to a single iteration cycle of the standard approach. The proposed method is useful in applications that require a high accuracy fitting of periodic signals, especially when limited computational resources are available or a real-time evaluation is needed...|$|R
40|$|In {{the actual}} order {{allocation}} process of Logistics Service Supply Chain (LSSC), Functional Logistics Service Providers (FLSPs) are strategic: they will <b>pre-estimate</b> the order allocation results {{to decide whether}} or not to participate in order allocation. Considering a two-echelon Logistics Service Supply Chain (LSSC) consisting of one Logistics Service Integrator (LSI) and several competitive FLSPs, we establish an order allocation optimization model of LSSC based on the <b>pre-estimate</b> and competitive behavior of FLSPs. The model considers three objectives: to minimize the cost of LSI, to maximize the order satisfaction of FLSPs and to match the different logistics capacities of FLSPs as much as possible. Numerical analysis is performed to discuss the effects of the competition among FLSPs on the order allocation results. The results show that with the rational expectations equilibrium, competitions among FLSPs help improve the comprehensive performance of LSSC...|$|R
40|$|Increase of {{drilling}} safety and reduction {{of drilling}} operation costs, especially improvement of drilling efficiency, are two important considerations {{in the oil}} and gas industry. The rate of penetration (ROP, alternatively called as drilling speed) is a critical drilling parameter to evaluate and improve drilling safety and efficiency. ROP estimation has an important role in drilling optimization as well as interpretation of all stages of the well life cycle. In this paper, we use a moving horizon estimation (MHE) method to estimate ROP as well as other drilling parameters. In the MHE formulation the states are estimated by a forward simulation with a <b>pre-estimating</b> observer. Moreover, it considers the constraints of states/outputs in the MHE problem. It is shown that the estimation error is with input-to-state stability. Furthermore, the ROP optimization (to achieve minimum drilling cost/drilling energy) concerning with the e cient hole cleaning condition and downhole environmental stability is presented. The performance of the methodology is demonstrated by one case study...|$|E
40|$|The paper {{proposes a}} {{technique}} for {{speeding up the}} search of the optimal set of features in classification problems where the input variables are discrete or nominal. The approach {{is based on the}} definition of an upper bound on the mutual information between the target and a set of d input variables. This bound is derived {{as a function of the}} mutual information of its subsets of d - 1 cardinality. The rationale of the algorithm is to proceed to evaluate the mutual information of a subset only if the respective upper bound is sufficiently promising. The computation of the upper bound can thus be seen as a pre-estimation of a subset. We show that the principle of <b>pre-estimating</b> allows to explore a much higher number of combinations of inputs than the classical algorithm of forward selection by preserving the same computational complexity. Some preliminary results showing the effectiveness of the proposed technique with respect to the classical forward search are reported. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|E
30|$|At the WZ decoder, an {{interpolated}} {{version of}} the current WZ frame is produced from the neighboring reconstructed frames. The BiMESS motion-compensated temporal interpolation technique introduced in [14] {{is used for the}} Discover DVC codec. The MCTI BiMESS performances are improved using a hierarchical coarse-to-fine approach in bidirectional motion estimation [15] and subpixel precision for motion search [16]. The interpolated frame is then DCT-transformed: these DCT coefficients represent the side information for the Wyner-Ziv decoder. The WZ DCT coefficients are modeled as the input of a virtual channel and the side information as its output. For the turbo decoding process, a Laplacian model is assumed for this virtual channel. The estimation of the Laplacian distribution parameter α is based on the online correlation noise modeling technique developed by Brites and Pereira [17]: parameter α is estimated for each coefficient of each DCT band. Alternative on-the-fly estimation methods are proposed in [18] and [19] to track the unpredictable and dynamic temporal changes within a video sequence. The correlation noise parameter is refined iteratively during the decoding instead of <b>pre-estimating</b> this parameter before decoding [17].|$|E
3000|$|... [...]) {{even with}} a large {{observation}} time. According to Eq. (15), the noise variance should be <b>pre-estimated</b> in order to perform the normalization. That means our proposed detectors {{are sensitive to the}} estimation of the noise variance. In this section, the impact of the NU on the robustness of our proposed detectors is evaluated.|$|R
30|$|Motion {{compensated}} gated PET image reconstruction {{methods include}} joint-reconstruction (JR) and indirect reconstruction (IR) with <b>pre-estimated</b> motion from MRI (MRI-IR). JR suffers from poor PET data quality whereas MRI-IR requires high-quality MRI volumes at each gate. We propose a penalised maximum-likelihood approach combining JR and MRI-IR. Our method {{is referred to}} as minimal MRI prior JR (MP-JR).|$|R
3000|$|... } in (22), (23) {{should be}} {{specified}} by an observer o <b>pre-estimated</b> invoking, for example, the VA inspired resolution-over-noise-suppression balancing method developed in [10, Section 3]. In the latter case, {{the result of}} the enhancement-fusion becomes a balanced tradeoff between the gained spatial resolution and noise suppression in the resulting fused enhanced image with the POCS-based regularizing stabilizer.|$|R
40|$|Abstract—The torque {{coefficient}} of any wind turbine must have highest limit. In this paper, an analytical expression of {{torque coefficient}} associated with {{tip speed ratio}} and airfoil lift drag ratio of wind turbine with ideal chord has been deduced by integrating along the blade wingspan using the blade element-momentum theory, {{which can be used}} for <b>pre-estimating</b> torque coefficient of actual wind turbine in design. Further, considering ideal fluid environment (the drag coefficient is close to 0), an expression of the highest performance of torque only associated with tip speed ratio has been deduced too, which is the highest boundary of torque coefficient of any actual wind turbine with same tip speed ratio. The results show that for the wind turbine in steady operation state when the tip speed ratio is about 0. 635, there is a theoretical limit of the torque coefficient, 0. 401; if the tip speed ratio is greater than 6, the torque coefficient is unlikely to exceed 0. 1. Keywords-wind energy utilization; horizontal axis wind turbine; torque coefficient; torque limit; highest performance; lift drag ratio; tip speed ratio I...|$|E
40|$|Severe shield jamming {{events have}} been {{reported}} during excavation of Uluabat tunnel through adverse geological conditions, which resulted in several stoppages at advancing a single shielded tunnel boring machine (TBM). To study the jamming mechanism, three-dimensional (3 D) simulation of the machine and surrounding ground was implemented using the finite difference code FLAC 3 D. Numerical analyses were performed for three sections along the tunnel with a higher risk for entrapment due to the combination of overburden and geological conditions. The computational results including longitudinal displacement contours and ground pressure profiles around the shield allow {{a better understanding of}} ground behavior within the excavation. Furthermore, they allow realistically assessing the impact of adverse geological conditions on shield jamming. The calculated thrust forces, which are required to move the machine forward, are in good agreement with field observations and measurements. It also proves that the numerical analysis can effectively be used for evaluating the effect of adverse geological environment on TBM entrapments and can be applied to prediction of loads on the shield and <b>pre-estimating</b> of the required thrust force during excavation through adverse ground conditions...|$|E
40|$|Abstract: We {{prove that}} the linear step-up {{procedure}} ϕLSU considered by Benjamini and Hochberg (1995) controls the false discovery rate (FDR) {{in the case of}} dependent p-values whose dependency structure is deter-mined by an Archimedean copula. In fact, the FDR of ϕLSU is under the assumption of an Archimedean p-value copula upper-bounded by the same constant {{as in the case of}} independent p-values. Namely, the upper bound is given by m 0 q/m, where m denotes the total number of hypotheses, m 0 the number of true null hypotheses, and q the nominal FDR level. Furthermore, we establish a sharper upper bound for the FDR of ϕLSU as well as a non-trivial lower bound. Application of the sharper upper bound to parametric subclasses of Archimedean p-value copulae allows us to increase the power of ϕLSU by <b>pre-estimating</b> the copula parameter and adjusting q. Based on the lower bound, a sufficient condition is obtained under which the FDR of ϕLSU is exactly equal to m 0 q/m. Finally, we deal with high-dimensional multiple test problems with exchangeable test statistics by proving that the dependency structure of the corresponding vector of p-values can always be expressed by an Archimedean copula. The theoretical results are applied to important copula families, including Clayton copulae and Gumbel copulae...|$|E
50|$|To provide {{quick and}} easily provable {{relief in the}} event of breach of contract, {{contracts}} often include penalty clauses or other similar clauses (<b>pre-estimates</b> of damages and forfeiture clauses). Clauses falling within the scope of the Conventional Penalties Act are enforceable but subject to reduction on equitable grounds. A penalty clause excludes a claim for damages.|$|R
30|$|DeVore [11] and Feng et al. [12] {{have proved}} the image space belong to Besov space. Chambolle et al. [13] {{proposed}} a threshold in Besov space. Based on Chambolle threshold, {{this article presents}} a novel shrinkage threshold which is optimal in spherical coordinates. The new threshold does not require <b>pre-estimated</b> noise intensity. It avoids the incomplete de-noising problem caused by inaccurate estimates.|$|R
30|$|Apparently, vehicles’ {{moving speed}} and {{direction}} may change in realistic environments. For example, a node may {{move back to}} prior road in the opposite direction, rendering prediction error of node’s trajectory. Thus, when the data packet reaches the <b>pre-estimated</b> position, the actual position of the destination can be far away. There are three cases when the applied predictive information goes wrong for forwarding data packets.|$|R
40|$|We are {{considered}} with the false discovery rate (FDR) of the linear step-up test φ^LSU considered by Benjamini and Hochberg (1995). It {{is well known}} that φ^LSU controls the FDR at level m_ 0 q / m if the joint distribution of p-values is multivariate totally positive of order 2. In this, m denotes the total number of hypotheses, m_ 0 the number of true null hypotheses, and q the nominal FDR level. Under the assumption of an Archimedean p-value copula with completely monotone generator, we derive a sharper upper bound for the FDR of φ^LSU as well as a non-trivial lower bound. Application of the sharper upper bound to parametric subclasses of Archimedean p-value copulae allows us to increase the power of φ^LSU by <b>pre-estimating</b> the copula parameter and adjusting q. Based on the lower bound, a sufficient condition is obtained under which the FDR of φ^LSU is exactly equal to m_ 0 q / m, {{as in the case of}} stochastically independent p-values. Finally, we deal with high-dimensional multiple test problems with exchangeable test statistics by drawing a connection between infinite sequences of exchangeable p-values and Archimedean copulae with completely monotone generators. Our theoretical results are applied to important copula families, including Clayton copulae and Gumbel copulae...|$|E
40|$|Climate induced natural {{disasters}} and extreme events are escalating {{with the increased}} variability of climatic parameters due to climate change. For example, climate change has aggravated extremes such as droughts and flood events. Intense rainfall leading to devastating floods is likely to severely affect people practicing indigenous subsistence agriculture. In the southern plain areas of Nepal, floods are the most frequent and devastating {{natural disasters}}. This study assesses the flood adaptation strategies that are applicable at the community level in two Terai districts of Nepal. The data were collected through three focus group discussions and 210 household surveys. The study revealed that flood forecasting practices at community level included monitoring the extent of rainfall in upper catchments and identifying the position of clouds. Prior to the monsoon season, community people gather and prepare using a range of flood adaptation strategies such as developing and refining management plan, updating contact information, <b>pre-estimating</b> flood risk and developing human resources/train manpower against flood. 'Perfect communication'; 'take care the affected people'; and 'select the appropriate location to stay' were most preferred strategies during the flood. Similarly, the community practised post flood adaptation strategies such as exchanging helps with each other, preparing temporary settlement, co-ordinating with government and other agencies and distribution of available resources. This paper argues that the identification and assessment of locally-relevant flood adaptation strategies will help governments to choose adaptation strategies that are both effective and preferred by local people in vulnerable communities. ...|$|E
40|$|Trabajo presentado a la 25 th British Machine Vision Conference (BMVC), celebrada en Nottingham (UK) del 1 al 5 de septiembre de 2014. [...] Este ítem (excepto textos e imágenes no creados por el autor) está sujeto a una licencia de Creative Commons: Attribution-NonCommercial-NoDerivs 3. 0 Spain. We {{propose a}} {{real-time}} and accurate {{solution to the}} Perspective-n-Point (PnP) problem [...] estimating the pose of a calibrated camera from n 3 D-to- 2 D point correspondences [...] that exploits {{the fact that in}} practice the 2 D position of not all 2 D features is estimated with the same accuracy. Assuming a model of such feature uncertainties is known in advance, we reformulate the PnP problem as a maximum likelihood minimization approximated by an unconstrained Sampson error function, which naturally penalizes the most noisy correspondences. The advantages of this approach are clearly demonstrated in synthetic experiments where feature uncertainties are exactly known. <b>Pre-estimating</b> the features uncertainties in real experiments is, though, not easy. In this paper we model feature uncertainty as 2 D Gaussian distributions representing the sensitivity of the 2 D feature detectors to different camera viewpoints. When using these noise models with our PnP formulation we still obtain promising pose estimation results that outperform the most recent approaches. This work has been partially funded by Spanish government under projects DPI 2011 - 27510, IPT- 2012 - 0630 - 020000, IPT- 2011 - 1015 - 430000 and CICYT grant TIN 2012 - 39203; by the EU project ARCAS FP 7 -ICT- 2011 - 28761; and by the ERA-Net Chistera project ViSen PCIN- 2013 - 047. Peer Reviewe...|$|E
5000|$|However, the {{approbation}} by the Privy Council did {{not move}} the editors of Chitty on Contracts {{in relation to the}} point of law. [...] Subsequent to the publication of that edition of Chitty the Court of Appeal expressly replicated that statement. [...] But even the editors of Chitty have acknowledged that their own preferred test of the requirement of genuine <b>pre-estimate</b> of loss has become very flexible.|$|R
30|$|The {{first task}} is {{intended}} to reduce noise and <b>pre-estimate</b> the vessels boundaries, from which the initial contours will be calculated. In so-doing, adaptive segmentation is performed, subtracting a heavy diffused version of the retinal image itself followed by a threshold by a fixed value, obtaining a binary map. To ensure that we are outside of the vessels location, some erosions are applied. The final image contains the initial contours.|$|R
3000|$|... {{tended to}} set all the {{coefficients}} of image details to zero, especially when N approaches infinite. At the mean while, {{the standard deviation}} of noise must be <b>pre-estimated.</b> Bias existed between estimated value and real value will reduce the effect of image de-noising. DeVore [11] and Feng et al. [12] believe the image space belongs to Besov space. In 1998, Chambolle et al. [13] proposed a threshold in Besov space.|$|R
