911|598|Public
25|$|In penalized regression, 'L1 penalty' and 'L2 penalty' {{refer to}} penalizing either the L1 norm of a solution's vector of {{parameter}} values (i.e. {{the sum of}} its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a <b>penalty</b> <b>term</b> that is a combination of the L1 norm and the L2 norm of the parameter vector.|$|E
50|$|The {{individual}} test compares each <b>penalty</b> <b>term</b> in the objective function with the critical {{values of the}} normal distribution. If the -th <b>penalty</b> <b>term</b> is outside the 95% confidence interval of the normal distribution, then {{there is reason to}} believe that this measurement has a gross error.|$|E
50|$|When fitting models, it is {{possible}} to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a <b>penalty</b> <b>term</b> for the number of parameters in the model; the <b>penalty</b> <b>term</b> is larger in BIC than in AIC.|$|E
30|$|Security {{constraints}} {{are satisfied}} by adding <b>penalty</b> <b>terms</b> to the objective function in this paper.|$|R
3000|$|... 2 are {{parameters}} weighting {{the importance}} of their respective <b>penalty</b> <b>terms.</b> The contour needs to be evolved until E [...]...|$|R
40|$|A popular {{method for}} {{handling}} state and output constraints {{in a model}} predictive control (MPC) algorithm is to use 'soft constraints', in which <b>penalty</b> <b>terms</b> are added directly to the objective function. Improved closed loop performance can be obtained for plants with nontninimum phase zeros by modifying the MPC formulation to include suitably-designed time-dependent weights on the <b>penalty</b> <b>terms</b> associated with the state and output constraints. When the <b>penalty</b> <b>terms</b> are written {{in terms of the}} 'worst-ease' l-infinity norm, incorporating the appropriate time dependence into the weights provides much better closed loop performance. The approach is illustrated using two multivariable plants with nonminimum phase transmission zeros, where the time-dependent weights cause the open loop predictions to coincide with closed loop predictions, which results in a reduction of output constraint violations...|$|R
5000|$|The elastic net extends lasso {{by adding}} an {{additional}} [...] <b>penalty</b> <b>term</b> giving ...|$|E
5000|$|Elastic net regularization {{offers an}} {{alternative}} to pure [...] regularization. The problem of lasso (...) regularization involves the <b>penalty</b> <b>term</b> , which is not strictly convex. Hence, solutions to [...] where [...] is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an [...] norm regularization penalty. For example, one can consider the problemwhere For [...] the <b>penalty</b> <b>term</b> [...] is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small , the additional <b>penalty</b> <b>term</b> [...] acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.|$|E
50|$|Viewed differently, the {{unconstrained}} {{objective is}} the Lagrangian of the constrained problem, {{with an additional}} <b>penalty</b> <b>term</b> (the augmentation).|$|E
30|$|The {{evaluation}} of the second part is performed by applying the DG discretization for the DG and <b>penalty</b> <b>terms.</b> The standard Gaussian quadrature implies full integration (two Gauss points in 2 D and 4 Gauss points in 3 D) on the interior element boundaries for both DG and <b>penalty</b> <b>terms</b> (see Fig.  2 (left)). From now on, we illustrate the numerical schemes only for the 2 D case. In analogy, the same concepts are applied in the 3 D setting.|$|R
30|$|However, {{having two}} {{integration}} {{points on the}} interior element boundaries for both DG and <b>penalty</b> <b>terms</b> results in a high total number of quadrature points. This implies that an element surrounded by four other elements will have four quadrature points in the area integral and eight {{for each of the}} DG and <b>penalty</b> <b>terms</b> on the entire element boundaries. It is well-known that this may lead to an artificial stiffening of the element behavior known as “locking phenomenon”.|$|R
40|$|The {{problem with}} {{introducing}} multiscale domains when modelling mi- cromagnetic processes {{is that some}} phenomena cannot be resolved in the coarser mesh. When using a structured grid, with an interface between the meshes, these phenomena reflect back into the fine mesh as a non- physical result of the discretization. Landau-Lifshitz is solved by using a new, higher order, energy con- serving, finite difference, method. The particular issue of reflections is handled by weakly damping the high frequencies of spin waves, travelling across an interface, using <b>penalty</b> <b>terms</b> close to the interface. A couple of ways of imposing these <b>penalty</b> <b>terms</b> are implemented, which results in the damping of high frequencies, and creates unwanted reflections from the low frequencies that travel across the interface. The result is pre- sented and other ways of implementing the <b>penalty</b> <b>terms</b> at the interface is discussed...|$|R
50|$|Adding a {{weighted}} <b>penalty</b> <b>term</b> to the Lyapunov drift and minimizing the sum {{leads to the}} drift-plus-penalty algorithm for joint network stability and penalty minimization.|$|E
5000|$|... and maximizing this {{marginal}} likelihood towards θ {{provides the}} complete specification of the Gaussian process f. One can briefly note {{at this point}} that the first term corresponds to a <b>penalty</b> <b>term</b> for a model's failure to fit observed values and the second term to a <b>penalty</b> <b>term</b> that increases proportionally to a model's complexity. Having specified θ making predictions about unobserved values [...] at coordinates x* is then only a matter of drawing samples from the predictive distribution [...] where the posterior mean estimate A is defined as ...|$|E
50|$|To {{overcome}} this drawback, a refined orientation model {{is defined in}} which the data term reduces the effect of noise and improves accuracy while the second <b>penalty</b> <b>term</b> with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.|$|E
40|$|The {{traditional}} {{collaborative filtering}} (CF) suffers from two key challenges, namely, the normal assumption {{that it is}} not robust, {{and it is difficult to}} set in advance the <b>penalty</b> <b>terms</b> of the latent features. We therefore propose a hierarchical Bayesian model-based CF and the related inference algorithm. Specifically, we impose a Gaussian-Gamma prior on the ratings, and the latent features. We show the model is more robust, and the <b>penalty</b> <b>terms</b> can be adapted automatically in the inference. We use Gibbs sampler for the inference and provide a statistical explanation. We verify the performance using both synthetic and real dataset...|$|R
40|$|We {{develop an}} {{approach}} for simulating acousto-elastic wave phenomena, including scattering from fluid-solid boundaries, where the solid {{is allowed to}} be anisotropic, with the Discontinuous Galerkin method. We use a coupled first-order elastic strain-velocity, acoustic velocity-pressure formulation, and append <b>penalty</b> <b>terms</b> based on interior boundary continuity conditions to the numerical (central) flux so that the consistency condition holds for the discretized Discontinuous Galerkin weak formulation. We incorporate the fluid-solid boundaries through these <b>penalty</b> <b>terms</b> and obtain a stable algorithm. Our approach avoids the diagonalization into polarized wave constituents {{such as in the}} approach based on solving elementwise Riemann problems. Comment: 43 pages, 30 figure...|$|R
40|$|We {{present a}} novel {{classification}} and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression (supervised training), {{to yield a}} new family of costlcomplexity <b>penalty</b> <b>terms.</b> Some improved generalization properties are demonstrated on real-world problems. ...|$|R
5000|$|After {{the sparse}} coding task, the next is {{to search for}} a better {{dictionary}} [...] However, finding the whole dictionary all at a time is impossible, so the process is to update only one column of the dictionary [...] each time, while fixing [...] The update of the -th column is done by rewriting the <b>penalty</b> <b>term</b> as ...|$|E
50|$|Augmented Lagrangian {{methods are}} a certain class of {{algorithms}} for solving constrained optimization problems. They have similarities to penalty methods {{in that they}} replace a constrained optimization problem {{by a series of}} unconstrained problems and add a <b>penalty</b> <b>term</b> to the objective; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a Lagrange multiplier. The augmented Lagrangian {{is not the same as}} the method of Lagrange multipliers.|$|E
50|$|In penalized regression, 'L1 penalty' and 'L2 penalty' {{refer to}} penalizing either the L1 norm of a solution's vector of {{parameter}} values (i.e. {{the sum of}} its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a <b>penalty</b> <b>term</b> that is a combination of the L1 norm and the L2 norm of the parameter vector.|$|E
40|$|In {{this paper}} {{we deal with}} Morozov’s {{discrepancy}} principle as an a-posteriori parameter choice rule for Tikhonov regularization with general convex <b>penalty</b> <b>terms</b> Ψ for non-linear inverse problems. It is shown that a regularization parameter α fulfilling the discprepancy principle exists, whenever the operator F satisfies some basic conditions, and that for this parameter choice rule holds α → 0, δ q /α → 0 as the noise level δ goes to 0. It is illustrated that for suitable <b>penalty</b> <b>terms</b> this yields convergence of the regularized solutions to the true solution in the topology induced by Ψ. Finally, we establish convergence rates {{with respect to the}} generalized Bregman distance and a numerical example is presented...|$|R
40|$|We {{introduce}} {{a method for}} the problem of learning {{the structure of a}} Bayesian network using the quantum adiabatic algorithm. We do so by introducing an efficient reformulation of a standard posterior-probability scoring function on graphs as a pseudo-Boolean function, which is equivalent to a system of 2 -body Ising spins, as well as suitable <b>penalty</b> <b>terms</b> for enforcing the constraints necessary for the refor-mulation; our proposed method requires O(n 2) qubits for n Bayesian network variables. Furthermore, we prove lower bounds on the necessary weighting of these <b>penalty</b> <b>terms.</b> The logical structure result-ing from the mapping has the appealing property that it is instance-independent for a given number of Bayesian network variables, as well as being independent of the number of data cases. ...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedWe develop an approach for simulating acousto-elastic wave phenomena, including scattering from fluid-solid boundaries, where the solid {{is allowed to}} be anisotropic, with the Discontinuous Galerkin method. We use a coupled first-order elastic strain-velocity, acoustic velocity-pressure formulation, and append <b>penalty</b> <b>terms</b> based on interior boundary continuity conditions to the numerical (central) flux so that the consistency condition holds for the discretized Discontinuous Galerkin weak formulation. We incorporate the fluid-solid boundaries through these <b>penalty</b> <b>terms</b> and obtain a stable algorithm. Our approach avoids the diagonalization into polarized wave constituents {{such as in the}} approach based on solving elementwise Riemann problems. This work {{was supported in part by}} the members, BGP, ExxonMobil, PGS, Statoil, and Total, of the Geo-Mathematical Imaging Group...|$|R
5000|$|The {{quadratic}} <b>penalty</b> <b>term</b> {{makes the}} loss function strictly convex, and it therefore {{has a unique}} minimum. The elastic net method includes the LASSO and ridge regression: in other words, each {{of them is a}} special case where [...] or [...] Meanwhile, the naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed [...] it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by [...]|$|E
50|$|After a {{distinguished}} athletic career she retired from running {{but wanted to}} become the first woman ever to win gold medals at both the Summer and Winter Olympics, by competing in and winning the inaugural two-woman bobsleigh event at the 2002 Winter Olympics. In late 2001, however, she was found guilty of having recently used banned drugs and barred from competition for two years. Her admission of drug use, though only during the recent part of her bobsleigh effort, made her a very controversial person in Sweden and considering that she had tested positive for banned drugs once before, during her days as a Soviet runner, and had sustained a ban (which was appealed and lifted after a while) some alleged that she had been using performance-enhancing substances regularly all the time, a claim {{for which there is no}} evidence. The <b>penalty</b> <b>term</b> ended in December 2003, but Engquist has not returned to competition since then.|$|E
5000|$|If no gross errors {{exist in}} the set of {{measured}} values, then each <b>penalty</b> <b>term</b> in the objective function is a random variable that is normally distributed with mean equal to 0 and variance equal to 1. By consequence, the objective function is a random variable which follows a chi-square distribution, {{since it is the}} sum of the square of normally distributed random variables. Comparing the value of the objective function [...] with a given percentile [...] of the probability density function of a chi-square distribution (e.g. the 95th percentile for a 95% confidence) gives an indication of whether a gross error exists: If , then no gross errors exist with 95% probability. The chi square test gives only a rough indication about the existence of gross errors, and it is easy to conduct: one only has to compare the value of the objective function with the critical value of the chi square distribution.|$|E
3000|$|We analyze {{discontinuous}} Galerkin methods with <b>penalty</b> <b>terms,</b> namely, symmetric interior penalty Galerkin methods, {{to solve}} nonlinear Sobolev equations. We construct finite element spaces {{on which we}} develop fully discrete approximations using extrapolated Crank-Nicolson method. We adopt an appropriate elliptic-type projection, which leads to optimal [...]...|$|R
40|$|Abstract. In this paper, {{we develop}} a {{symmetric}} Galerkin method with interior <b>penalty</b> <b>terms</b> to construct fully discrete approximations {{of the solution}} for nonlinear Sobolev equations. To analyze the convergence of discontinuous Galerkin approximations, we introduce an appropriate pro-jection and derive the optimal L 2 error estimates. 1...|$|R
40|$|The stereo {{matching}} method semi-global matching (SGM) relies on consistency constraints during the cost aggregation which are enforced by so-called <b>penalty</b> <b>terms.</b> This paper proposes new and evaluates four penalty functions for SGM. Due to mutual dependencies, {{two types of}} matching cost calculation, census and rank transform, are considered. Performance is measured using original and degenerated images exhibiting radiometric changes and noise from the Middlebury benchmark. The two best performing penalty functions are inversely proportional and negatively linear to the intensity gradient and perform equally with 6. 05 % and 5. 91 % average error, respectively. The experiments also show that adaptive <b>penalty</b> <b>terms</b> are mandatory when dealing with difficult imaging conditions. Consequently, for highest algorithmic performance in real-world systems, selection of a suitable penalty function and thorough parametrization {{with respect to the}} expected image quality is essential...|$|R
5000|$|Most significantly, the LACE {{system is}} far heavier than a pure rocket engine {{having the same}} thrust (air-breathing engines of almost all types have {{relatively}} poor thrust-to-weight ratios compared to rockets), {{and the performance of}} launch vehicles of all types is particularly affected by increases in vehicle dry mass (such as engines) that must be carried all the way to orbit, as opposed to oxidizer mass that would be burnt off {{over the course of the}} flight. Moreover, the lower thrust-to-weight ratio of an air-breathing engine as compared to a rocket significantly decreases the launch vehicle's maximum possible acceleration, and increases gravity losses since more time must be spent to accelerate to orbital velocity. Also, the higher inlet and airframe drag losses of a lifting, air-breathing vehicle launch trajectory as compared to a pure rocket on a ballistic launch trajectory introduces an additional <b>penalty</b> <b>term</b> [...] into the rocket equation known as the air-breather's burden. This term implies that unless the lift-to-drag ratio [...] and the acceleration of the vehicle as compared to gravity [...] are both implausibly large for a hypersonic air-breathing vehicle, the advantages of the higher I of the air-breathing engine and the savings in LOx mass are largely lost.|$|E
5000|$|... {{where the}} design matrix [...] and {{covariate}} vector [...] {{have been replaced}} by a collection of design matrices [...] and covariate vectors , one for each of the J groups. Additionally, the <b>penalty</b> <b>term</b> is now a sum over [...] norms defined by the positive definite matrices [...] If each covariate is in its own group and , then this reduces to the standard lasso, while if there is only a single group and , it reduces to ridge regression. Since the penalty reduces to an [...] norm on the subspaces defined by each group, it cannot select out only some of the covariates from a group, just as ridge regression cannot. However, because the penalty is the sum over the different subspace norms, as in the standard lasso, the constraint has some non-differential points, which correspond to some subspaces being identically zero. Therefore, it can set the coefficient vectors corresponding to some subspaces to zero, while only shrinking others. However, it is possible to extend the group lasso to the so-called sparse group lasso, which can select individual covariates within a group, by adding an additional [...] penalty to each group subspace. Another extension, group lasso with Overlap allows covariates to be shared between different groups, e.g. if a gene were to occur in two pathways.|$|E
40|$|In this paper, {{we study}} first- and second-order {{necessary}} conditions for nonlinear programming problems {{from the viewpoint}} of exact penalty functions. By applying the variational description of regular subgradients, we first establish necessary and sufficient conditions for a <b>penalty</b> <b>term</b> to be of KKT-type by using the regular subdifferential of the <b>penalty</b> <b>term.</b> In terms of the kernel of the subderivative of the <b>penalty</b> <b>term,</b> we also present sufficient conditions for a <b>penalty</b> <b>term</b> to be of KKT-type. We then derive a second-order necessary condition by assuming a second-order constraint qualification, which requires that the second-order linearized tangent set is included in the closed convex hull of the kernel of the parabolic subderivative of the <b>penalty</b> <b>term.</b> In particular, for a <b>penalty</b> <b>term</b> with order, by assuming the nonpositiveness of a sum of a second-order derivative and a third-order derivative of the original data and applying a third-order Taylor expansion, we obtain the second-order necessary condition. Department of Applied Mathematic...|$|E
30|$|DTS: The Direct Tabu Search {{algorithm}} is an improved algorithm which exploits {{the idea of}} not revisiting regions of the search space that have been already explored. For achieving that, some <b>penalty</b> <b>terms</b> {{are added to the}} objective function. In this work, we use the implementation detailed in [19] with the parameters recommended in there.|$|R
30|$|Although {{the form}} of (16) {{is similar to the}} hybridizable {{symmetric}} interior penalty method (HSIP) [12], the derivation and motivation of the two approaches are different. For the discretization (16), the numerical flux is directly approximated by the solution gradient. For HSIP method, the jumps are introduced as <b>penalty</b> <b>terms</b> to guarantee the stability of the method.|$|R
40|$|This paper investigates fault {{tolerance}} in feedforward neural networks, for a realistic fault model based on analog hardware. In our previous work with synaptic weight noise [26] we showed significant {{fault tolerance}} enhancement over standard training algorithms. We proposed that when introduced into training, weight noise distributes the network computationmore evenly across the weights and thus enhances fault tolerance. Here we compare those results with an approximation to the mechanisms induced by stochastic weight noise, incorporated into training deterministicallyvia <b>penalty</b> <b>terms.</b> The <b>penalty</b> <b>terms</b> are an approximation to weight saliency and therefore, in addition, we assess {{a number of}} other weight saliency measures and perform comparison experiments. The results show that the first term approximation is an incomplete model of weight noise in terms of fault tolerance. Also the error hessian is shown to be the most accurate measure of weight saliency. 1 Introduction The presenc [...] ...|$|R
