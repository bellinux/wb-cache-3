0|20|Public
40|$|The IDAMS Processor is {{a package}} of task {{routines}} and support software that performs convolution filtering, image expansion, fast Fourier transformation, and other operations on a digital image tape. A unique task control card for that program, together with any necessary <b>parameter</b> <b>cards,</b> selects each processing technique {{to be applied to}} the input image. A variable number of tasks can be selected for execution by including the proper task and <b>parameter</b> <b>cards</b> in the input deck. An executive maintains control of the run; it initiates execution of each task in turn and handles any necessary error processing...|$|R
50|$|Additional <b>parameters,</b> {{including}} the <b>card’s</b> {{country of issue}} and its previous payment history, are also used to gauge the probability of the transaction being approved.|$|R
40|$|This thesis {{deals with}} {{measuring}} with USB card 6009 from National Industries company. The {{first part of}} the project deals with attributes and <b>parameters</b> measuring <b>card.</b> The second part of the project is aimed to programming environment LabView. In the third part of the project is descriptioned program to reading and generating curves, preferences and limits of this program, method of use and its options. In {{the last part of the}} project is valorization executed measures...|$|R
40|$|A manual is {{presented}} for correctly submitting program runs in aerodynamics on the UNIVAC 1108 computer system. All major program modules are included. Control cards are documented for the user's convenience, and <b>card</b> <b>parameters</b> {{are included in}} order to provide some idea as to reasonable time estimates for the program modules...|$|R
50|$|If a {{card issuer}} wants to update a card post {{issuance}} it can send commands to the card using issuer script processing. Issuer scripts are encrypted between {{the card and}} the issuer, so are meaningless to the terminal. Issuer script {{can be used to}} block cards, or change <b>card</b> <b>parameters.</b>|$|R
30|$|Note {{that the}} main {{difference}} between TTR 1.0 and 1.1 is that TTR 1.0 performs the ordering of the domain, P, that is the parameters are ordered according to the amount of values they have: from the highest to the lowest quantity. For example, considering Fig.  1 and this input order: bank, function, and card. In version 1.0, parameters are stored in an ordered way: the first <b>parameter</b> becomes <b>card</b> (3 values), the second parameter is bank (2 values) and the last parameter is function (2 values). In version 1.1, there is no such ordering and this explains why bank and function generate the first rows (t-tuples) of Θ (see Table  2).|$|R
40|$|Here, we {{describe}} the potential utility of the card sorting method for structuring and refining map symbol sets. Card sorting has been proposed as a method for delineating categories by researchers and practitioners {{in a variety of}} disciplines due to its ability to identify and explicate real or perceived structures in an information space; however, there is little reported application of card sorting within Cartography. To span this gap, we offer a framework that prescribes the appropriate methodological <b>parameters</b> for <b>card</b> sorting according to the stage in the design process and the goals of the study. We then illustrate the utility of card sorting for Cartography by describing a closed sorting study we conducted on the ANCI INCITS 415 - 2006 emergency mapping symbol standard. Our approached helped us identify several barriers to using the symbol standard, including areas of conceptual overlap among the categories in the standard, potentially missing categories from the standard, and individual symbols in the standard that are consistently misclassified. 1...|$|R
40|$|This Bachelor thesis {{deals with}} the {{acceleration}} of function calculations, using parallel computing mediated by NVDIA graphics cards via CUDA technology. The theoretical part describes the general principles of parallel computing and the basic characteristics and <b>parameters</b> of graphics <b>cards</b> NVDIA. The theoretical part also deals with basic principles of CUDA technology. End of the theoretical part focuses on FFTW and cuFFT libraries. The practical part {{deals with the}} comparison of the performance between GPU and CPU functions filter 2 D and Canny and practical possibilities of accelerating fast convolution calculation. The practical part also describes sample code {{that was used to}} compare the performance between GPU and CPU. The results of this program are then plotted and evaluated...|$|R
40|$|The {{power supply}} {{transient}} signal (I DDT) methods that we propose for defect detection and localization analyze regional signal variations introduced by defects at {{a set of}} the power supply ports on the chip under test (CUT). A significant detractor to the successful application of such methods {{is dealing with the}} signal variations introduced by process and probe <b>card</b> <b>parameter</b> variations. In this paper, we describe several calibration techniques designed to reduce the impact of these types of “non-defect ” related chip and testing environment variations on the defect detection sensitivity of I DDT testing methods. More specifically, calibration methods are proposed that calibrate for signal variations introduced by performance differences and by changes in the probe <b>card</b> RLC <b>parameters.</b> Th...|$|R
40|$|Class of 2015 AbstractObjectives: The study {{aimed to}} {{increase}} EPS risk factor assessment when prescribers order prophylactic anticholinergics with antipsychotics. An evidence-based pharmacist checklist card {{was developed to}} aid in this decision making process. Methods: A retrospective chart review of patients admitted to the acute inpatient psychiatry units at an academic medical center was conducted to determine baseline prophylactic anticholinergic prescribing habits over a two-month period. Charts were included if the patient was at least 18 years old and ordered at least one scheduled antipsychotic during the admission. An educational intervention session introduced the pharmacist checklist card and shared baseline findings. Post-intervention data was collected during a two-month period following the intervention. The percentage of prophylactic anticholinergic orders based upon pharmacist checklist <b>card</b> <b>parameters</b> pre and post-intervention was analyzed using chi-square test. Results: There was {{a significant decrease in}} the total percentage of orders for prophylactic anticholinergics from 72. 7...|$|R
40|$|International audienceMajor {{evolutions}} {{have been}} implemented in the three main CDS databases in 2006. SIMBAD 4, {{a new version of}} SIMBAD developed with Java and PostgreSQL, has been released. Il is much more flexible than the previous version and offers in particular full search capabilities on all <b>parameters.</b> Wild <b>card</b> can also be used in object names, which should ease searching for a given object in the frequent case of 'fuzzy' nomenclature. New information is progressively added, in particular a set of multiwavelength magnitudes (in progress), and other information from the Dictionnary of Nomenclature such as the list of object types attached to each object name (available), or hierarchy and associations (in progress). A new version of VizieR, also in the open source PostgreSQL DBMS, has been completed, in order to simplify mirroring. The master database at CDS currently remains in the present Sybase implementation. A new simplified interface will be demonstrated, providing a more user-friendly navigation while retaining the multiple browsing capabilities. A new release of the Aladin Sky Atlas offers new capabilities, like the management of multipart FITS files and of data cubes, construction and execution of macros for processing a list of targets, and improved navigation within an image plane. This new version also allows easy and efficient manipulation of very large (> 108 pixels) images, support for solar images display, and direct access to SExtractor to perform source extraction on displayed images...|$|R
40|$|A {{digitally}} simulated electronic watershed analog {{has been}} developed {{for the analysis of}} the hydrologic regime of a watershed. Individual electrical circuits were designed to synthesize the physical characteristics of the hydrologic components of a watershed: interception, surface storage, runoff, infiltration, and subsurface storage. These circuits were related to pertinent empirical studies of significance to each component. Electrical circuit analogies, despite advantages inherent in their direct physical correspondence to hydrologic systems, have fallen into disuse due to the inflexibility of fixed component networks. A digital simulation program developed by the electrical engineering profession to provide flexibility in the design of electronic circuitry has been adapted for the simulation of the electronic watershed analog. The typical digital circuit analysis program is "canned" and the user need not understand its intricacies. Input {{is in the form of}} circuit <b>parameters</b> on punched <b>cards.</b> The output is in numeric or graphic form. Using digital simulation methodology, the electronic watershed analog has been used to analyze a 1. 63 acre forested watershed...|$|R
40|$|Events are an {{emerging}} paradigm for composing applications in an open, heterogeneous distributed world. In Cambridge {{we have developed}} scalable event handling based on a publish-register-notify model with event object classes and server-side filtering based on parameter templates. After experience in using this approach in a home-built RPC system we have extended CORBA, an open standard for distributed object computing, to handle events in this way. In this paper, we present the design of COBEA - a COrba-Based Event Architecture. A service that {{is the source of}} (parameterised) events publishes in a Trader the events it is prepared to notify, along with its normal interface specification. For scalability, a client must register interest (by invoking a register method with appropriate <b>parameters</b> or wild <b>cards)</b> at the service, at which point an access control check is carried out. Subsequently, whenever a matching event occurs, the client is notified. We outline the requirements on the [...] ...|$|R
40|$|The VLSI {{electronic}} circuit designs have steadily grown in their capacity and complexity through the years. MOSIS fabrication services provide test data that designers can used to simulate their circuit designs. The provided test results are extracted from various lot wafers and BSIM 3 or BSIM 4 model <b>card</b> <b>parameters</b> {{in addition to}} technology parameters are provided. It may be cumbersome to ensure design functionality over {{the wide range of}} model set of parameters. In this paper, it is proposed to utilize the average model parameters to validate circuit design functionality. It can be shown through device characterization and simple circuit simulations that the average model parameters can provide a good representation of the wide range of supplied model parameters. This is specially attractive for students of circuit design classes where classroom and graduate research work were computing resources are limited. Utilizing average model parameters alleviate the need to run simulations over the large set of models from the fabrication facility...|$|R
40|$|The {{present study}} exposes an analytical, numerical, and {{experimental}} investigation on the failure mechanism, {{pertaining to the}} stable mode of collapse (Mode I), of thin-walled composite truncated conical tubes subjected to axial loading in order to predict the mean loads and total displacements during collapse. The analysis is based on previous research results with the attempt to eliminate some simplifications dictated by experimental evidence and improve the modeling from the mathematical point of view. The theoretical modeling identifies the main internal absorbing energy contributions and equals their sum to the work done by the external load. From this {{it is possible to}} explicit the average load, that is a function of several variables. From a minimization process it is possible to obtain the values of the variables that identify the crushing behavior. Together with the theoretical analysis, a finite element modeling is conducted through an explicit dynamic code, such as LS-DYNA. It is possible to capture also numerically the crushing phenomenon with a particular attention during the setting of the numerical <b>parameters</b> and <b>card</b> definitions. The model’s accuracy can be estimated only through an experimental campaign; therefore circular frusta in CFRP material varying some geometrical parameters, such as wall thickness, mean radius, and slope, were tested under axial dynamic loadings. Comparison between analytical, numerical, and experimental investigation as regards mean load and final crushing is good, indicating that the proposed strategy can be a valid approach to predict the energy-absorbing capability of the axially collapsing composite shells, despite the complexity of the phenomenon...|$|R
40|$|This thesis {{studies the}} impact of {{hardware}} features of graphics cards on performance of GPU computing using GPGPU-Sim simulation software tool. GPU computing is a growing topic {{in the world of}} computing, and could be an important milestone for computers. Therefore, such a study that seeks to identify the performance bottlenecks of the program with respect to hardware parameters of the devvice can be considered an important step towards tuning devices for higher efficiency. In this work we selected convolution algorithm - a typical GPGPU application - and conducted several tests to study different performance parameters. These tests were performed on two simulated graphics cards (NVIDIA GTX 480, NVIDIA Tesla C 2050), which are supported by GPGPU-Sim. By changing the hardware <b>parameters</b> of graphics <b>card</b> such as memory cache sizes, frequency and the number of cores, we can make a fine-grained analysis on the effect of these parameters on the performance of the program. A graphics card working on a picture convolution task releis on the L 1 cache but has the worst performance with a small shared memory. Using this simulator to run performance tests on a theoretical GPU architecture could lead to better GPU design for embedded systems...|$|R
40|$|In {{the article}} the payment card {{problems}} are {{connected with a}} gap and damage in the security system. The main purpose of the article is to evaluate security level of the payment card system and find its security gap and damage. Tasks of the article are as follows: to evaluate the damage level of the payment card system for the card system members, to measure the payment card security features, security of the work organization and systems, to examine and evaluate the merits and demerits of security models used in European Banks, to propose the solution how to minimize the damage in payment card system. The Method of research: analysis of nonfiction, continuous census of fraud cases, information analysis using statistical methods, comparative analysis, experts opinion, analysis of material of case investigation and reports, analysis according to the dominating fraud types and indicators, document process analysis. The article's research shows the weakness of physical card security features, nonsecure customer financial - credit possibility and <b>card</b> <b>parameters</b> management, outworn equipment and methods of system analysis and monitoring, the lack of co-operation between {{the members of the}} payment card system. The big part of indicated demerits could be covered by migrating to the new card system and equipment. In addition, it was proposed to create the co-operation schemes between system members and third party service providers, to set the loyalty programme for the merchants and police, to introduce more secure authorisation system modules...|$|R
40|$|This multi-national, double-blind, randomized, parallel-group study {{compared}} the efficacy and tolerability of fluticasone propionate 500 μg twice daily propelled {{either by the}} non-chlorofluorocarbon (CFC) propellant, hydrofluoroalkane (HFA) 134 a, or the CFC propellants 11 and 12 used in the established pressurized metered dose inhaler (pMDI). The study period was 12 months and involved 412 subjects with moderate to severe asthma (HFA 134 a pMDI: n = 203; CFC pMDI: n = 209). For the first 3 months, subjects kept a daily record card and attended the clinic every 4 weeks. Thereafter, they kept daily diaries for 2 weeks before each clinic assessment, which were performed {{at the end of}} 6, 9 and 12 months. Mean morning peak expiratory flow (PEF) increased during the first week in both treatment groups. By the end of week 12 the adjusted mean increase from baseline in morning PEF was 21 and 23 l min− 1 in the HFA 134 a and CFC pMDI groups, respectively, and this increase was maintained throughout the 12 -month study period. Similar improvements were detected in other diary <b>card</b> <b>parameters</b> and in clinic lung function measurements. The two groups were shown to be clinically equivalent in terms of all efficacy variables and there were no differences in tolerability. There were few reports of low serum cortisol levels during the 12 -month study period, and serum cortisol levels were similar at baseline and after 12 weeks and 12 months of treatment in the two groups. In conclusion, the new HFA 134 a fluticasone propionate pMDI is as effective and safe as the established CFC fluticasone propionate pMDI when used at a dosage of 1 mg day− 1...|$|R
40|$|Abstract Background Aortic {{calcification}} is a {{major risk}} factor for death from cardiovascular disease. We investigated the relationship between mortality and the composite markers of number, size, morphology and distribution of calcified plaques in the lumbar aorta. Methods 308 postmenopausal women aged 48 - 76 were followed for 8. 3 ± 0. 3 years, with deaths related to cardiovascular disease, cancer, or other causes being recorded. From lumbar X-rays at baseline the number (NCD), size, morphology and distribution of aortic calcification lesions were scored and combined into one Morphological Atherosclerotic Calcification Distribution (MACD) index. The hazard ratio for mortality was calculated for the MACD and for three other commonly used predictors: the EU SCORE card, the Framingham Coronary Heart Disease Risk Score (Framingham score), and the gold standard Aortic Calcification Severity score (AC 24) developed from the Framingham Heart Study cohorts. Results All four scoring systems showed increasing age, smoking, and raised triglyceride levels were the main predictors of mortality after adjustment for all other metabolic and physical <b>parameters.</b> The SCORE <b>card</b> and the Framingham score resulted in a mortality hazard ratio increase per standard deviation (HR/SD) of 1. 8 (1. 51 - 2. 13) and 2. 6 (1. 87 - 3. 71), respectively. Of the morphological x-ray based measures, NCD revealed a HR/SD > 2 adjusted for SCORE/Framingham. The MACD index scoring the distribution, size, morphology and number of lesions revealed the best predictive power for identification of patients at risk of mortality, with a hazard ratio of 15. 6 (p Conclusions This study shows {{that it is not}} just the extent of aortic calcification that predicts risk of mortality, but also the distribution, shape and size of calcified lesions. The MACD index may provide a more sensitive predictor of mortality from aortic calcification than the commonly used AC 24 and SCORE/Framingham point card systems. </p...|$|R
40|$|Background: Aortic {{calcification}} is a {{major risk}} factor for death from cardiovascular disease. We investigated the relationship between mortality and the composite markers of number, size, morphology and distribution of calcified plaques in the lumbar aorta. Methods: 308 postmenopausal women aged 48 - 76 were followed for 8. 3 ± 0. 3 years, with deaths related to cardiovascular disease, cancer, or other causes being recorded. From lumbar X-rays at baseline the number (NCD), size, morphology and distribution of aortic calcification lesions were scored and combined into one Morphological Atherosclerotic Calcification Distribution (MACD) index. The hazard ratio for mortality was calculated for the MACD and for three other commonly used predictors: the EU SCORE card, the Framingham Coronary Heart Disease Risk Score (Framingham score), and the gold standard Aortic Calcification Severity score (AC 24) developed from the Framingham Heart Study cohorts. Results: All four scoring systems showed increasing age, smoking, and raised triglyceride levels were the main predictors of mortality after adjustment for all other metabolic and physical <b>parameters.</b> The SCORE <b>card</b> and the Framingham score resulted in a mortality hazard ratio increase per standard deviation (HR/SD) of 1. 8 (1. 51 - 2. 13) and 2. 6 (1. 87 - 3. 71), respectively. Of the morphological x-ray based measures, NCD revealed a HR/SD > 2 adjusted for SCORE/Framingham. The MACD index scoring the distribution, size, morphology and number of lesions revealed the best predictive power for identification of patients at risk of mortality, with a hazard ratio of 15. 6 (p < 0. 001) for the 10 % at greatest risk of death. Conclusions: This study shows {{that it is not}} just the extent of aortic calcification that predicts risk of mortality, but also the distribution, shape and size of calcified lesions. The MACD index may provide a more sensitive predictor of mortality from aortic calcification than the commonly used AC 24 and SCORE/Framingham point card systems...|$|R

