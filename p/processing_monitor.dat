34|530|Public
25|$|On April 7, 1964, IBM {{announced}} the first computer system family, the IBM System/360. It spanned the complete range {{of commercial and}} scientific applications from large to small, allowing companies {{for the first time}} to upgrade to models with greater computing capability without having to rewrite their applications. It was followed by the IBM System/370 in 1970. Together the 360 and 370 made the IBM mainframe the dominant mainframe computer and the dominant computing platform in the industry throughout this period and into the early 1980s. They, and the operating systems that ran on them such as OS/VS1 and MVS, and the middleware built on top of those such as the CICS transaction <b>processing</b> <b>monitor,</b> had a near-monopoly-level hold on the computer industry and became almost synonymous with IBM products due to their marketshare.|$|E
5000|$|UNIVAC Transaction Interface Package (TIP) - 1970s. A {{transaction}} <b>processing</b> <b>monitor</b> for UNIVAC 1100/2200 series computers.|$|E
50|$|Application Control Management System (Application Control and Management System) (ACMS) is a {{transaction}} <b>processing</b> <b>monitor</b> software system from HP for computers running the OpenVMS operating system.|$|E
50|$|Transaction <b>processing</b> <b>monitors</b> {{provides}} {{tools and}} an environment {{to develop and}} deploy distributed applications.|$|R
50|$|In 1981, WESTI was {{considered}} the main competitor to CICS, holding second place in market share for IBM mainframe transaction <b>processing</b> <b>monitors.</b>|$|R
50|$|Database access {{services}} are often characterised as middleware. Some {{of them are}} language specific implementations and support heterogeneous features and other related communication features. Examples of database-oriented middleware include ODBC, JDBC and transaction <b>processing</b> <b>monitors.</b>|$|R
50|$|Sigma 5 and 8 systems {{lack the}} memory map feature,The Sigma 5 is {{supported}} by the Basic Control Monitor (BCM) and the Batch <b>Processing</b> <b>Monitor</b> (BPM). The Sigma 8 can run the Real-time Batch Monitor (RBM) as well as BPM/BTM.|$|E
50|$|A {{teleprocessing}} monitor (also, Transaction <b>Processing</b> <b>Monitor</b> or TP Monitor) is a control program that monitors {{the transfer of}} data between multiple local and remote terminals {{to ensure that the}} transaction processes completely or, if an error occurs, to take appropriate actions.|$|E
50|$|The 1978 {{multiprocessor}} {{technology was}} introduced. The operating system {{had the ability}} to cope with a processor failure. At the same time the new technology considerably extended the performance range of the system.In 1979 a transaction <b>processing</b> <b>monitor,</b> the Universal Transaction Monitor (UTM), was introduced, providing support for online transaction processing as an additional operating mode.|$|E
50|$|The Strategic Planning and Market Intelligence {{section is}} {{responsible}} for enhancing STB’s strategic planning capabilities through gathering, <b>processing,</b> <b>monitoring,</b> analyzing and reporting on various forms of market intelligence and data related to the tourism industry.|$|R
40|$|The article {{considers}} {{the methods and}} means of knowledge management in industrial activities to create knowledge management systems (KMS). The author gives approaches {{to the construction of}} KMS architecture on the basis of model representation, obtaining, exchange, <b>processing,</b> <b>monitoring</b> the state of KMS knowledge as agents’ activities (staff and intelligent software agents) ...|$|R
40|$|Methods {{derived from}} time-series {{analysis}} are proposed for <b>processing</b> <b>monitoring</b> data. The necessity {{for the use}} of these methods is demonstrated. In a case study time-series analysis was applied {{to assess the impact of}} the closure of the Grevelingen estuary (S. W. Netherlands) in 1971 on a local wintering population of Oystercatchers (Haematopus ostralegus) in the adjacent Oosterschelde estuary. ...|$|R
5000|$|Tmax (Transaction MAXimization) is a TP-Monitor which {{facilitates}} and oversees {{transaction processing}} in distributed systems that consist of multiple machines and properly handles system and network errors. A TP-Monitor (Transaction <b>Processing</b> <b>Monitor),</b> often called middleware, watches transactions across multiple systems and databases and ensures integrity of data across the resources {{involved in the}} transaction, regardless of access protocol.|$|E
50|$|As {{a member}} of The Open Group, Transarc also {{developed}} the DFS distributed filesystem component of the Distributed Computing Environment (DCE) that was sold by Open Group members (including Transarc). Other products included the distributed transaction <b>processing</b> <b>monitor</b> Encina (a basis for IBM's UNIX-based CICS products; included in IBM's TXSeries and later WebSphere), and the Solaris binary distribution of DCE.|$|E
50|$|ALCS is a {{transaction}} <b>processing</b> <b>monitor</b> for the IBM System/360, System/370, ESA/390, and zSeries mainframes. It is {{a variant of}} TPF specially designed to provide all the benefits of TPF (very high speed, high volume, and high availability in transaction processing) but with the advantages such as easier integration into the data center offered by running on a standard IBM operating system platform.|$|E
50|$|The primary {{language}} used for developing both the VME operating system itself and other system software such as compilers and transaction <b>processing</b> <b>monitors</b> is S3. This {{is a high}} level language based in many ways on Algol 68, but with data types and low-level functions and operators aligned closely with {{the architecture of the}} 2900 series.|$|R
40|$|Modern TOF-PET scanner systems require {{high-speed}} {{computing resources}} for efficient data <b>processing,</b> <b>monitoring</b> and image reconstruction. In {{this article we}} present the data flow and software architecture for the novel TOF-PET scanner developed by the J-PET collaboration. We discuss the data acquisition system, reconstruction framework and image reconstruction software. Also, the concept of computing outside hospitals in the remote centers such as Świerk Computing Centre in Poland is presented. Comment: 8 pages, 2 figure...|$|R
40|$|As {{more and}} more {{object-oriented}} transactional <b>processing</b> <b>monitors</b> are being developed, users in industries such as banking and telecommunications need systematic and critical evaluations of {{the strengths and weaknesses}} of these products. This paper presents the Middleware Evaluation Project (MEP) which aims to provide an impartial evaluation based on rigorously derived tests and benchmarks. The evaluation framework based on TPC’s benchmark C will firstly be presented followed by discussions on the set of evaluation criteria. Preliminary results on the OTM product OrbixOTM will also be given. 1...|$|R
50|$|Scientific Data Systems/Xerox Data Systems {{developed}} several {{operating systems}} for the Sigma series of computers, such as the Basic Control Monitor (BCM), Batch <b>Processing</b> <b>Monitor</b> (BPM), and Basic Time-Sharing Monitor (BTM). Later, BPM and BTM were succeeded by the Universal Time-Sharing System (UTS); {{it was designed to}} provide multi-programming services for online (interactive) user programs in addition to batch-mode production jobs, It was succeeded by the CP-V operating system, which combined UTS with the heavily batch-oriented Xerox Operating System (XOS).|$|E
50|$|For {{many years}} the large {{majority}} of VME users wrote applications in COBOL, usually making use of the IDMS database and the TPMS transaction <b>processing</b> <b>monitor.</b> Other programming languages included Fortran, Pascal, ALGOL 68RS, Coral 66 and RPG2, but these served minority interests. Later, in the mid 1980s, compilers for C became available, both within and outside the Unix subsystem, largely to enable porting of software such as relational database systems. It is interesting that a PL/I subset compiler was written by the EEC, to assist in porting programs from IBM to ICL hardware.|$|E
50|$|OS/VS1 was {{intended}} to manage a medium-sized work load (for the 1970s) consisting only of batch processing applications, running within a fixed number of operating system partitions via the batch job management system Job Entry Subsystem 1 (JES1). This was in contrast to OS/VS2 which {{was intended}} to handle larger work loads consisting of batch applications, online interactive users (using the Time Sharing Option, or TSO), {{or a combination of}} both. However, OS/VS1 could, and often did, support interactive applications and users by running IBM's CICS transaction <b>processing</b> <b>monitor</b> as a job within one of its partitions.|$|E
40|$|Transaction {{monitors}} (a. k. a. transaction <b>processing</b> <b>monitors,</b> a. k. a. TP monitors) {{were the}} first kind of middleware to support distributed transaction processing, and thus also the first kind of application servers. They were (originally) designed for environments with very high processing demands, such as banks or airlines, {{that could not be}} properly supported by 2 -tier architectures and by DBMS alone. Architectures using transaction monitors as middleware for transaction control are also called TP-heavy transaction processing environments, because this solution is much heavier to implement. ...|$|R
50|$|Manufacturing {{processes}} require {{precision and}} reproducibility. For laser materials <b>processing</b> the <b>monitoring</b> of laser power is beneficial {{as it can}} avoid scrap production and yield high quality products.|$|R
40|$|Abstract. An {{underwater}} {{ultrasound system}} for detecting flooding {{in the hollow}} sub-sea structures of offshore steel oil-rigs is presented. A sensor, attached to a subsea structure and powered by seawater, transmits underwater ultrasound chirpencoded signals to a real-time digital signal <b>processing</b> <b>monitoring</b> system at surface level. Experiments performed using a jointed steel pipe structure 1 ton in weight, with dimensions of 7 m × 0. 5 m × 16 mm, completely immersed in seawater showed excellent performance using a central excitation frequency of 38 kHz, over distances of 100 m. 1...|$|R
50|$|The {{remaining}} models initially ran the Batch <b>Processing</b> <b>Monitor</b> (BPM), later augmented with a timesharing option (BTM); {{the combined}} system was usually {{referred to as}} BPM/BTM. The Universal Time-Sharing System (UTS) became available in 1971, supporting much enhanced time-sharing facilities. A compatible upgrade (or renaming) of UTS, Control Program V (CP-V) became available starting in 1973 and added real-time, remote batch, and transaction processing. A dedicated real-time OS, Control Program for Real-Time (CP-R) was also available for Sigma 9 systems. The Xerox Operating System (XOS), intended as an IBM DOS replacement, also runs on Sigma 6/7/9 systems, but never gained real popularity.|$|E
50|$|Based on {{earnings}} {{reported for}} IBM's 1Q13, annualized revenue for IBM's middleware software unit increased to $14 billion (up $7 billion from 2011). License and maintenance revenue for IBM middleware products reached $7 billion in 2011. In 2012, IBM expected {{an increase in}} both market share and total market increase of ten percent. The worldwide application infrastructure and middleware software market grew 9.9 percent in 2011 to $19.4 billion, according to Gartner. Gartner reported that IBM continues to be number one in other growing and key areas including the Enterprise Service Bus Suites, Message Oriented Middleware Market, the Transaction <b>Processing</b> <b>Monitor</b> market and Integration Appliances.|$|E
50|$|In the X/Open XA architecture, a {{transaction}} manager or transaction <b>processing</b> <b>monitor</b> (TP monitor) coordinates the transactions across multiple {{resources such as}} databases and message queues. Each resource has its own resource manager. The resource manager typically has its own API for manipulating the resource, for example the JDBC API to work with relational databases. In addition, the resource manager allows a TP monitor to coordinate a distributed transaction between its own and other resource managers. Finally, there is the application which communicates with the TP monitor to begin, commit or roll back the transactions. The application also communicates with the individual resources using their own API to modify the resource.|$|E
40|$|This paper {{addresses}} {{the problem of}} <b>processing</b> range <b>monitoring</b> queries, each of which continuously retrieves moving objects that are currently located within a given query range. In particular, this paper focuses on <b>processing</b> range <b>monitoring</b> queries in the road network, where movements of the objects are constrained by a predefined set of paths. One {{of the most important}} challenges of <b>processing</b> range <b>monitoring</b> queries is how to minimize the wireless communication cost and the server computation cost, both of which are heavily dependent on the amount of location-update stream generated by moving objects. The traditional centralized methods for range monitoring queries assume that moving objects periodically send location-updates to the server. However, when the number of moving objects becomes increasingly large, such an assumption may no longer be acceptable because the amount of location-update stream becomes enormous. Recently, some distributed methods have been proposed, where moving objects utilize their available computational capabilities for sending location-updates to the server only when necessary. Unfortunately, the existing distributed methods only deal with the objects moving in Euclidean space, and thus they cannot be extended to <b>processing</b> range <b>monitoring</b> queries over the objects moving along the road network. In this paper, we propose the distributed method for <b>processing</b> range <b>monitoring</b> queries in the road network. To utilize the computational capabilities of moving objects, we introduce the concept of vicinity region. A vicinity region, assigned to each moving object o, makes o monitor whether or not it {{should be included in the}} results of nearby queries. The proposed method includes (i) a new spatial index structure, called the Segment-based Space Partitioning tree (SSP-tree) whose role is to efficiently search the appropriate vicinity regions for moving objects based on their heterogeneous computational capabilities and (ii) the details of the communication strategy between the server and moving objects, which significantly reduce the wireless communication cost as well as the server computation cost. Through simulations, we verify the effectiveness for <b>processing</b> range <b>monitoring</b> queries over a large number of moving objects (up to 100, 000) in the road network (modeled as an undirected graph) ...|$|R
40|$|International Telemetering Conference Proceedings / October 13 - 15, 1981 / Bahia Hotel, San Diego, CaliforniaThe data {{system for}} the {{processing}} of telemetry and telecommand data for the German space research program {{is in the process}} of being implemented. It consists of data <b>processing,</b> <b>monitoring</b> and transmission facilities including hardware and software at the ground station with various antennas and at the mission control center. The system is designed on distributed processing techniques based on processors of various sizes. The software is modular and can easy be adjusted for the various mission requirements. Data transmission is based on. standard protocolls...|$|R
40|$|Background: Pain is a {{commonly}} reported symptom following surgery {{that is more}} likely to occur in individuals psychologically distressed prior to surgery. <b>Monitoring</b> <b>processing</b> style, a cognitive tendency to focus on health-related threats, has been associated with increased reporting of somatic symptoms, but no studies have specifically addressed the link between this cognitive style and pain. This prospective clinical study aimed to investigate whether <b>monitoring</b> <b>processing</b> style predicted post-surgical pain in women undergoing breast surgery, controlling for pre-surgical psychological distress. Methods: Women scheduled to undergo breast cancer surgery (N[*]=[*] 106) completed pre-surgical assessments of <b>monitoring</b> <b>processing</b> style (Miller Behavioral Style Scale) and psychological distress (Depression Anxiety Stress Scales- 21). Demographic and medical characteristics were documented. Self-reported neuropathic pain (Neuropathic Pain Scale) was assessed at 3 months post surgery. Results: Post-surgical neuropathic pain levels were low to moderate (M[*]=[*] 19. 3, SD[*]=[*] 21. 1). Higher pre-surgical <b>monitoring</b> <b>processing</b> style scores significantly predicted higher post-surgical neuropathic pain (β[*]=[*] 0. 23, p[*]=[*] 0. 023), over and above psychological stress (β[*]=[*] 0. 22, p[*]=[*] 0. 020) and age (β[*]=[*]− 0. 25, p[*]=[*] 0. 011). Conclusions: Pre-surgical <b>monitoring</b> <b>processing</b> style was an independent predictor of post-surgical neuropathic pain, even when accounting for pre-surgical psychological distress. Since the reduction of post-surgical pain is a key goal of healthcare, efforts should be made prior to breast cancer surgery to counsel and support individuals with high <b>monitoring</b> <b>processing</b> styles irrespective of their level of distress. 8 page(s...|$|R
5000|$|The {{original}} Tuxedo team comprised {{members of}} the LMOS team, including Juan M. Andrade, Mark T. Carges, Terrence Dwyer, and Stephen Felts.In 1993 Novell acquired the Unix System Laboratories (USL) division of AT&T which {{was responsible for the}} development of Tuxedo at the time. In September 1993 it was called the [...] "best known" [...] distributed transaction <b>processing</b> <b>monitor,</b> running on 25 different platforms.In February 1996, BEA Systems made an exclusive agreement with Novell to develop and distribute Tuxedo on non-NetWare platforms, with most Novell employees working with Tuxedo joining BEA.In 2008, Oracle Corporation acquired BEA Systems, and TUXEDO was marketed as part of the Oracle Fusion Middleware product line.|$|E
5000|$|The Universal Time-Sharing System (UTS) was an {{operating}} {{system for the}} XDS Sigma series of computers, succeeding Batch <b>Processing</b> <b>Monitor</b> (BPM)/Batch Time-Sharing Monitor (BTM). UTS was announced in 1966, but because of delays did not actually ship until 1971. It was designed to provide multi-programming services for online (interactive) user programs in addition to batch-mode production jobs, symbiont (spooled) I/O, and critical real-time processes. System Daemons, called [...] "ghost jobs" [...] were used to run monitor code in user space. The final release, D00, shipped in January, 1973. It was succeeded by the CP-V operating system, which combined UTS with the heavily batch-oriented Xerox Operating System (XOS).|$|E
50|$|On April 7, 1964, IBM {{announced}} the first computer system family, the IBM System/360. It spanned the complete range {{of commercial and}} scientific applications from large to small, allowing companies {{for the first time}} to upgrade to models with greater computing capability without having to rewrite their applications. It was followed by the IBM System/370 in 1970. Together the 360 and 370 made the IBM mainframe the dominant mainframe computer and the dominant computing platform in the industry throughout this period and into the early 1980s. They, and the operating systems that ran on them such as OS/VS1 and MVS, and the middleware built on top of those such as the CICS transaction <b>processing</b> <b>monitor,</b> had a near-monopoly-level hold on the computer industry and became almost synonymous with IBM products due to their marketshare.|$|E
40|$|Tyt. z nagłówka. Bibliogr. s. 61 - 62. The {{amount of}} textual {{information}} {{published on the}} Internet {{is considered to be}} in billions of web pages, blog posts, comments, social media updates and others. Analyzing such quantities of data requires high level of distribution – both data and computing. This is especially true in case of complex algorithms, often used in text mining tasks. The paper presents a prototype implementation of CLUO – an Open Source Intelligence (OSINT) system, which extracts and analyzes significant quantities of openly available information. Dostępny również w formie drukowanej. KEYWORDS: text mining, Big Data, OSINT, Natural Language <b>Processing,</b> <b>monitoring...</b>|$|R
40|$|Abstract- In {{this paper}} we present {{algorithms}} for building and maintaining efficient collection trees {{that provide the}} conduit to disseminate data required for <b>processing</b> <b>monitoring</b> queries in a wireless sensor network. We introduce and formalize the notion of event monitoring queries and demonstrate that they can capture a large class of monitoring applications. We then show techniques which, using a small set of intuitive statistics, can compute collection trees that minimize important resources such {{as the number of}} messages exchanged among the nodes or the overall energy consumption. Our experiments demonstrate that our techniques can organize the data collection process while utilizing significantly lower resources than prior approaches. I...|$|R
40|$|In {{the article}} the problems, which appear under the {{creation}} of monitoring systems concerning the condition of informatization of general educational institutions, such as definition of monitoring object and list of parameters that will be traced during the monitoring, technologies of obtaining and actualization of data parameters, {{that are to be}} monitored, formats of data submission and ways of its <b>processing,</b> <b>monitoring</b> time period etc. are considered. In the article some decision of these problems are offered. Here is also mentioned the data of some characteristics and possibilities of {{the creation of}} monitoring systems concerning the condition of informatization of general educational institutions in Ukraine...|$|R
