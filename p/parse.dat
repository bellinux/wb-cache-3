5594|10000|Public
5|$|Many music {{theorists have}} used Parsifal to explore {{difficulties}} {{in analyzing the}} chromaticism of late 19th century music. Theorists such as David Lewin and Richard Cohn have explored the importance of certain pitches and harmonic progressions both in structuring and symbolizing the work. The unusual harmonic progressions in the leitmotifs which structure the piece, {{as well as the}} heavy chromaticism of act 2, make it a difficult work to <b>parse</b> musically.|$|E
5|$|Watson's basic working {{principle}} is to <b>parse</b> keywords in a clue while searching for related terms as responses. This gives Watson some {{advantages and disadvantages}} compared with human Jeopardy! players. Watson has deficiencies in understanding the contexts of the clues. As a result, human players usually generate responses faster than Watson, especially to short clues. Watson's programming prevents it from using the popular tactic of buzzing before it is sure of its response. Watson has consistently better reaction time on the buzzer once it has generated a response, and is immune to human players' psychological tactics, such as jumping between categories on every clue.|$|E
25|$|Two {{types of}} ambiguities can be distinguished. <b>Parse</b> tree {{ambiguity}} and structural ambiguity. Structural ambiguity {{does not affect}} thermodynamic approaches as the optimal structure selection {{is always on the}} basis of lowest free energy scores. <b>Parse</b> tree ambiguity concerns the existence of multiple <b>parse</b> trees per sequence. Such an ambiguity can reveal all possible base-paired structures for the sequence by generating all possible <b>parse</b> trees then finding the optimal one. In the case of structural ambiguity multiple <b>parse</b> trees describe the same secondary structure. This obscures the CYK algorithm decision on finding an optimal structure as the correspondence between the <b>parse</b> tree and the structure is not unique. Grammar ambiguity can be checked for by the conditional-inside algorithm.|$|E
50|$|LR <b>parsing</b> extends LL <b>parsing</b> {{to support}} a larger range of grammars; in turn, {{generalized}} LR <b>parsing</b> extends LR <b>parsing</b> {{to support a}}rbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL <b>parsing</b> and LR <b>parsing,</b> respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR <b>parsing</b> {{was developed in the}} 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR <b>parsing</b> up to the present day.|$|R
40|$|This paper {{presents}} {{our work}} {{for participation in}} the 2012 CIPS-SIGHAN shared task of Traditional Chinese <b>Parsing.</b> We have adopted two multilingual <b>parsing</b> models – a factored model (Stanford <b>Parser)</b> and an unlexicalized model (Berkeley <b>Parser)</b> for <b>parsing</b> the Sinica Treebank. This paper also proposes a new Chinese unknown word model and integrates it into the Berkeley <b>Parser.</b> Our experiment gives the first result of adapting existing multilingual <b>parsing</b> models to the Sinica Treebank and shows that the <b>parsing</b> accuracy can be improved by our suggested approach...|$|R
40|$|We {{present a}} novel {{framework}} that combines strengths from surface syntactic <b>parsing</b> and deep syntactic <b>parsing</b> to increase deep <b>parsing</b> accuracy, specifically by combining dependency and HPSG <b>parsing.</b> We show that by using surface dependencies to constrain {{the application of}} wide-coverage HPSG rules, we can benefit {{from a number of}} <b>parsing</b> techniques designed for highaccuracy dependency <b>parsing,</b> while actually performing deep syntactic analysis. Our framework results in a 1. 4 % absolute improvement over a state-of-the-art approach for wide coverage HPSG <b>parsing.</b> ...|$|R
25|$|Most prominently, {{behavior}} on <b>parse</b> errors differ. A fatal <b>parse</b> error in XML (such as an incorrect tag structure) causes document processing to be aborted.|$|E
25|$|The {{resulting}} {{of multiple}} <b>parse</b> trees per grammar denotes grammar ambiguity. This {{may be useful}} in revealing all possible base-pair structures for a grammar. However an optimal structure is the one where there is one and only one correspondence between the <b>parse</b> tree and the secondary structure.|$|E
25|$|Find {{the optimal}} grammar <b>parse</b> tree (CYK algorithm).|$|E
40|$|Most {{previous}} studies {{need to learn}} a complex object model for <b>parsing</b> a specific object instance. This paper directly learns the general <b>parsing</b> patterns from the set of <b>parsed</b> objects and formalizes the <b>parsing</b> patterns {{as a series of}} <b>parsing</b> templates instead of learning the complex object model. Moreover, a novel hierarchical structure is presented to represent an object by using the <b>parsing</b> templates, which implicitly contains the multi-scale object parts and their relationships. For a single object, the <b>parsing</b> process is equivalent to establishing its hierarchical representation and determining the <b>parsing</b> template for each node. We combine the top-down decomposing scheme and the bottom-up composing scheme to infer the <b>parsing</b> process and formalize the inference as an energy minimization problem. The effect of our method is demonstrated by <b>parsing</b> the human body with aggressive pose variations. Compared with the state-of-the-art methods, the <b>parsing</b> results are more satisfying...|$|R
40|$|We propose an {{algebraic}} {{method for}} the design of tabular <b>parsing</b> algorithms which uses <b>parsing</b> schemata [7]. The <b>parsing</b> strategy is expressed in a tree algebra. A <b>parsing</b> schema is derived from the tree algebra by means of algebraic operations such as homomorphic images, direct products, subalgebras and quotient algebras. The latter yields a tabular interpretation of the <b>parsing</b> strategy. The proposed method allows simpler and more elegant correctness proofs by using general theorems and is not limited to left-right <b>parsing</b> strategies, unlike current automaton-based approaches. Furthermore, it allows to derive <b>parsing</b> schemata for linear indexed grammars (LIG) from <b>parsing</b> schemata for context-free grammars by means of a correctness preserving algebraic transformation. A new bottom-up head corner <b>parsing</b> schema for LIG is constructed to demonstrate the method...|$|R
40|$|We {{investigated}} the performance e#cacy of beam search <b>parsing</b> and deep <b>parsing</b> techniques in probabilistic HPSG <b>parsing</b> using the Penn treebank. We first tested the beam thresholding and iterative <b>parsing</b> developed for PCFG <b>parsing</b> with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid <b>parsing</b> with a CFG chunk parser. The {{contributions of the}} large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total <b>parsing</b> performance. The precision, recall and average <b>parsing</b> time for the Penn treebank (Section 23) were 87. 85 %, 86. 85 %, and 360 ms, respectively...|$|R
25|$|Rank {{and score}} the <b>parse</b> trees {{for the most}} {{plausible}} sequence.|$|E
25|$|For example, {{there are}} many {{families}} of graphs that are close enough analogues of formal languages {{that the concept of}} a calculus is quite easily and naturally extended to them. Indeed, many species of graphs arise as <b>parse</b> graphs in the syntactic analysis of the corresponding families of text structures. The exigencies of practical computation on formal languages frequently demand that text strings be converted into pointer structure renditions of <b>parse</b> graphs, simply as a matter of checking whether strings are well-formed formulas or not. Once this is done, {{there are many}} advantages to be gained from developing the graphical analogue of the calculus on strings. The mapping from strings to <b>parse</b> graphs is called parsing and the inverse mapping from <b>parse</b> graphs to strings is achieved by an operation that is called traversing the graph.|$|E
25|$|Picking a {{different}} order of expansion will produce {{a different}} derivation, but the same <b>parse</b> tree.|$|E
40|$|GML data {{is widely}} used for model building, data exchanging, etc. The GML data <b>parsing</b> is the base of {{handling}} other operation of GML. The <b>parsing</b> technology of XML {{can be used for}} GML <b>parsing.</b> But the XML <b>parsing</b> technology is deficient in <b>parsing</b> semantic information on geography information. This paper tries to build a semantic information database (SIDB) of GML and design GML core schema-based <b>parsing</b> engine which based on SIDB. Ultimately actualize GML data <b>parsing.</b> The results of the study are verified by GML test data in the paper. And more, this study provides a new way to <b>parsing</b> semantic information in other fields...|$|R
40|$|We present Pro 3 Gres, a fast robust broad-coverage and deep-linguistic parser {{that has}} been applied to and {{evaluated}} on unrestricted amounts of text from unrestricted domains. We show that it is largely cognitively adequate. We argue that Pro 3 Gres contributes to closing the gap between psycholinguistics and language engineering, between probabilistic <b>parsing</b> and formal grammar-based <b>parsing,</b> between shallow <b>parsing</b> and full <b>parsing,</b> and between deterministic <b>parsing</b> and non-deterministic <b>parsing.</b> We also describe a successful application of Pro 3 Gres for <b>parsing</b> research texts from the BioMedical domain...|$|R
40|$|<b>Parsing</b> schemata are {{high-level}} {{descriptions of}} <b>parsing</b> algorithms. This paper {{is concerned with}} <b>parsing</b> schemata for different grammar formalisms. We separate the description of <b>parsing</b> steps from that of grammatical properties by means of abstract <b>parsing</b> schemata. We define an abstract Earley schema and prove it correct. We obtain Earley schemata for several grammar formalisms by specifying the grammatical properties in the abstract Earley schema. Our approach offers a clear and well-defined interface between a <b>parsing</b> algorithm and a grammar. Moreover, it provides a precise criterion for the classification of <b>parsing</b> algorithms...|$|R
25|$|The <b>parse</b> tree {{will only}} change if we pick a {{different}} rule {{to apply at}} some position in the tree.|$|E
25|$|Several {{algorithms}} {{dealing with}} aspects of PCFG based probabilistic models in RNA structure prediction exist. For instance the inside-outside algorithm and the CYK algorithm. The inside-outside algorithm is a recursive dynamic programming scoring algorithm that can follow expectation-maximization paradigms. It computes the total probability of all derivations {{that are consistent}} with a given sequence, based on some PCFG. The inside part scores the subtrees from a <b>parse</b> tree and therefore subsequences probabilities given an PCFG. The outside part scores the probability of the complete <b>parse</b> tree for a full sequence. CYK modifies the inside-outside scoring. Note that the term 'CYK algorithm' describes the CYK variant of the inside algorithm that finds an optimal <b>parse</b> tree for a sequence using a PCFG. It extends the actual CYK algorithm used in non-probabilistic CFGs.|$|E
25|$|The {{simplicity}} of FASTA format {{makes it easy}} to manipulate and <b>parse</b> sequences using text-processing tools and scripting languages like R, Python, Ruby, and Perl.|$|E
40|$|This paper {{suggests}} {{two ways}} of improving transition-based, non-projective dependency <b>parsing.</b> First, we add a transition to an existing non-projective <b>parsing</b> algorithm, so it can perform either projective or non-projective <b>parsing</b> as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic <b>parses</b> used as features. The new addition to the algorithm shows a clear advantage in <b>parsing</b> speed. The bootstrapping technique gives a significant improvement to <b>parsing</b> accuracy, showing near state-of-theart performance with respect to other <b>parsing</b> approaches evaluated on the same data set. ...|$|R
40|$|<b>Parsing</b> schemata are de ned as an {{intermediate}} level of abstraction between contextfree grammars and parsers. Clear, concise speci cations of radically di erent <b>parsing</b> algorithms can be expressed as <b>parsing</b> schemata. Moreover, because of the uniformityofthese speci cations, relations between di erent <b>parsing</b> algorithms can be formally established. This article gives {{an introduction to the}} <b>parsing</b> schemata framework. ...|$|R
40|$|In this work, {{we address}} the {{challenging}} video scene <b>parsing</b> problem by developing effective representation learning methods given limited <b>parsing</b> annotations. In particular, we contribute two novel methods that constitute a unified <b>parsing</b> framework. (1) Predictive feature learning from nearly unlimited unlabeled video data. Different from existing methods learning features from single frame <b>parsing,</b> we learn spatiotemporal discriminative features by enforcing a <b>parsing</b> network to predict future frames and their <b>parsing</b> maps (if available) given only historical frames. In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene <b>parsing,</b> without requiring extra manual annotations. (2) Prediction steering <b>parsing</b> architecture that effectively adapts the learned spatiotemporal features to scene <b>parsing</b> tasks and provides strong guidance for any off-the-shelf <b>parsing</b> model to achieve better video scene <b>parsing</b> performance. Extensive experiments over two challenging datasets, Cityscapes and Camvid, {{have demonstrated the}} effectiveness of our methods by showing significant improvement over well-established baselines. Comment: 15 pages, 7 figures, 5 tables, currently v...|$|R
25|$|Researchers {{have shown}} that this problem is intimately linked {{with the ability to}} <b>parse</b> language, and that those words that are easy to segment due to their high {{transitional}} probabilities are also easier to map to an appropriate referent. This serves as further evidence of the developmental progression of language acquisition, with children requiring an understanding of the sound distributions of natural languages to form phonetic categories, <b>parse</b> words based on these categories, and then use these parses to map them to objects as labels.|$|E
25|$|Software libraries, such as libexif for C and Adobe XMP Toolkit or Exiv2 for C++, Metadata Extractor for Java, PIL/Pillow for Python or ExifTool for Perl, <b>parse</b> Exif {{data from}} files and read/write Exif tag values.|$|E
25|$|On {{the other}} hand, the editor often {{displays}} code during its creating, {{while it is}} incomplete or incorrect, and the strict parsers (like ones used in compiles) would fail to <b>parse</b> the code most of the time.|$|E
40|$|Previous {{studies in}} {{data-driven}} dependency <b>parsing</b> {{have shown that}} tree transformations can improve <b>parsing</b> accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages/treebanks and parsers, focusing on pseudo-projective <b>parsing,</b> {{as a way of}} capturing non-projective dependencies, and transformations used to facilitate <b>parsing</b> of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective <b>parsing</b> is independent of <b>parsing</b> strategy but sensitive to language or treebank specific properties. By contrast, the construction specific transformations appear to be more sensitive to <b>parsing</b> strategy but have a constant positive effect over several languages...|$|R
40|$|<b>Parsing</b> {{plays an}} {{important}} role in semantic role labeling (SRL) because most SRL systems infer semantic relations from 1 -best <b>parses.</b> Therefore, <b>parsing</b> errors inevitably lead to labeling mistakes. To alleviate this problem, we propose to use packed forest, which compactly encodes all <b>parses</b> for a sentence. We design an algorithm to exploit exponentially many <b>parses</b> to learn semantic relations efficiently. Experimental results on the CoNLL- 2005 shared task show that using forests achieves an absolute improvement of 1. 2 % in terms of F 1 score over using 1 -best <b>parses</b> and 0. 6 % over using 50 -best <b>parses...</b>|$|R
40|$|International audienceThis paper {{presents}} both <b>parsing</b> systems used by project ATOLL (INRIA) for the EASy <b>parsing</b> evaluation campaign (December, 2004). We give a few {{quantitative results}} {{in terms of}} coverage and <b>parsing</b> time. These experiments will allow the comparison of <b>parsing</b> results on huge corpus, but also show that deep <b>parsing</b> techniques can cope with large corpus while preserving their linguistic expressive power...|$|R
25|$|There are {{currently}} just {{over one hundred}} thousand British Japanese, mostly in London; but unlike other Nikkei communities elsewhere in the world, these Britons do not conventionally <b>parse</b> their communities in generational terms as Issei, Nisei, or Sansei.|$|E
25|$|Three of {{the four}} former Maverick AHCA All-Americans – Hoggan (2002 second team), <b>Parse</b> (2006 first team and 2007 second team), and Zanon (2001 and 2002 second teams) – have all gone on {{to play in the}} NHL.|$|E
25|$|The {{ability to}} {{appropriately}} generalize to whole classes of yet unseen words, {{coupled with the}} abilities to <b>parse</b> continuous speech and keep track of word-ordering regularities, may be the critical skills necessary to develop proficiency with and knowledge of syntax and grammar.|$|E
40|$|In <b>parsing</b> theory, LL <b>parsing</b> and LR <b>parsing</b> are {{regarded}} {{to be two}} distinct methods. In this paper the relation between these methods is clarified. As shown in literature on <b>parsing</b> theory, for every context-free grammar, a so-called non-deterministic LR(0) automaton can be constructed. Here, we show, that traversing this automaton in a special way is equivalent to LL(1) <b>parsing.</b> This automaton can {{be transformed into a}} deterministic LR-automaton. The description of a method to traverse this automaton results into a new formulation of the LR <b>parsing</b> algorithm. Having obtained in this way a relationship between LL and LR <b>parsing,</b> the LL(1) class is characterised, using several LR-classes. 1 Introduction In the theory of <b>parsing,</b> the two main methods are LL and LR <b>parsing</b> respectively. The LL method is implemented mostly by the recursive descent technique. LR <b>parsing</b> is implemented mainly as a stack algorithm, governed by a so-called action/goto-matrix representing the LR aut [...] ...|$|R
40|$|We {{develop an}} {{improved}} form of left-corner chart <b>parsing</b> for large context-free grammars, introducing improvements {{that result in}} signicant speed-ups compared to previously-known variants of left-corner <b>parsing.</b> We also compare our method to several other major <b>parsing</b> approaches, and nd that our improved left-corner <b>parsing</b> method outperforms each of these {{across a range of}} grammars. Finally, we also describe a new technique for minimizing the extra information needed to eÆciently recover <b>parses</b> from the data structures built in the course of <b>parsing.</b> ...|$|R
40|$|<b>Parsing</b> schemata {{provide a}} general {{framework}} for specication, analysis and comparison of (sequential and/or parallel) <b>parsing</b> algorithms. A grammar specifies implicitly what the valid <b>parses</b> {{of a sentence}} are; a <b>parsing</b> algorithm specifies explicitly how to compute these. <b>Parsing</b> schemata form a well-defined level of abstraction in between grammars and <b>parsing</b> algorithms. A <b>parsing</b> schema specifies the types of intermediate results that can be computed by a parser, and the rules that allow to expand a given set of such results with new results. A <b>parsing</b> schema does not specify the data structures, control structures, and (in case of parallel processing) communication structures {{that are to be}} used by a parser. Part I, Exposition, gives a general introduction to the ideas that are worked out in the following parts. Part II, Foundation, unfolds a mathematical theory of <b>parsing</b> schemata. Different kinds of relations between <b>parsing</b> schemata are formally introduced and illustrated with examples drawn from the <b>parsing</b> literature. Part III, Application, discusses a series of applications of <b>parsing</b> schemata. - Feature percolation in unification grammar <b>parsing</b> can be described in an elegant, legible notation. - Because of the absence of algorithmic detail, <b>parsing</b> schemata can be used to get a formal grip on highly complicated algorithms. We give substance to this claim by means of a thorough analysis of Left-Corner and Head-Corner chart <b>parsing.</b> - As an example of structural similarity of parsers, despite differences in form and appearance, we show that the underlying <b>parsing</b> schemata of Earley's algorithm and Tomita's algorithm are virtually identical. Using this structural correspondence we can obtain a novel parallel parser by cross-fertilizing a parallel Earley parser with Tomita's graph-structured stack. - <b>Parsing</b> schemata can be implemented straightforwardly by boolean circuits. This means that, in principle, <b>parsing</b> schemata can be coded directly into hardware. Part IV, Perspective, discusses the prospects for natural language <b>parsing</b> applications and draws some conclusions. An important observation is that the theoretical and practical part of the book reinforce each other. The proposed framework is abstract enough to allow a thorough mathematical treatment and practical enough to allow rewriting a variety of real <b>parsing</b> algorithms (i. e. seriously proposed in the literature, not toy examples) in a clear and coherent way...|$|R
