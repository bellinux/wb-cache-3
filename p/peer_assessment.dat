1060|360|Public
25|$|The 2013 U.S. News & World Report rankings' <b>peer</b> <b>assessment</b> portion {{gives the}} school {{a score of}} 4.4, tied with University of Pennsylvania, Dartmouth, Northwestern, and University of Michigan.|$|E
25|$|Washington University's {{undergraduate}} {{program is}} ranked 18th {{in the nation}} in the 2018 U.S. News & World Report National Universities ranking. Additionally, 19 undergraduate disciplines are ranked among the top 10 programs in the country. In 2013, the ranking's <b>peer</b> <b>assessment</b> score was 4.1.|$|E
500|$|For 2013, Florida Atlantic University was {{classified}} as a second-tier university by the U.S. News & World Report's rankings of [...] "Best Colleges." [...] U.S. News ranks universities into one of two tiers, with one being the highest, based on how they compare with other colleges in a <b>peer</b> <b>assessment,</b> retention rates, student selectivity, faculty resources, financial resources, graduation rates, {{and the amount of}} alumni giving. The university was named one of the 146 [...] "Best Southeastern Colleges" [...] in the United States by the Princeton Review. The Review also recognized FAU's business program by naming the College of Business to their list of [...] "Best 296 Business Schools" [...] for 2009. For 2011, Florida Atlantic was ranked 249th in the nation by Washington Monthly. The magazine based its rankings on the following three criteria: [...] "how well a university performs as an engine of social mobility (ideally helping the poor to get rich rather than the very rich to get very, very rich), how well a university does in fostering scientific and humanistic research, and how well a university promotes an ethic of service to country." [...] The university was also ranked 28th in the United States and fourth in Florida by The Hispanic Outlook in Higher Education magazine for awarding 738 bachelor's degrees to Hispanic students during the 2006–2007 academic year.|$|E
5000|$|Publishing {{critical}} {{reports and}} conducting <b>peer</b> <b>assessments</b> {{to raise the}} bar on quality, ...|$|R
2500|$|Peer assessments: {{members of}} a group {{evaluate}} and appraise the performance of their fellow group members. [...] There it is common for a graphic rating scale to be used for self-assessments. [...] Positive leniency tends to be a problem with self-assessments. <b>Peer</b> <b>assessments</b> from multiple {{members of a}} group are often called crowd-based performance reviews, and solve many problems with <b>peer</b> <b>assessments</b> from only one member.|$|R
5000|$|Ashland University {{was named}} {{as one of}} the top 15 over-performing colleges in the nation by U.S. News & World Report in December 2012. The {{publication}} looked at data from its Best Colleges 2013, then took <b>peer</b> <b>assessments</b> and compared the information to the actual rankings. Ashland University's performance is 58 places above its <b>peer</b> <b>assessments,</b> placing it among the top 15 over-performing schools in the country.Ashland University was ranked by Forbes in 2013 as number 20 on the [...] "Twenty-Five Colleges With the Worst Return on Investment" [...] list.|$|R
2500|$|In 1926 McQueen {{introduced}} some <b>peer</b> <b>assessment,</b> whereby girls' {{estimates of}} each other's work and worth {{was taken into}} consideration in the allocation of prizes. As McQueen said: ...|$|E
2500|$|In January 1997, then-president of Alma College, Alan Stone, asked 480 {{colleges to}} boycott the U.S. News & World Report Rankings due to the <b>peer</b> <b>assessment</b> survey which counts for 22.5% of a college's ranking. According to the Chronicle of Higher Education, in 1996, Alma College {{surveyed}} 158 colleges about the rankings. The result of the survey indicated that [...] "84 per cent of the respondents admitted that they were unfamiliar {{with some of the}} institutions they had been asked to rank. Almost 44 per cent indicated that they 'tended to leave responses for unfamiliar schools blank.' [...] " [...] Stone stated, [...] "this makes me wonder just how many votes are being considered for each school's academic-reputation ranking." ...|$|E
2500|$|While {{assessment}} can {{be performed}} along reporting relationships (usually top-down), net assessment can include peer and self-assessment. [...] <b>Peer</b> <b>assessment</b> is when assessment is performed by colleagues along both horizontal (similar function) and vertical (different function) relationship. [...] Self-assessments are when individuals evaluate themselves. There are three common methods of peer assessments. [...] Peer nomination involves each group member nominating who he/she believes to be the [...] "best" [...] on a certain dimension of performance. [...] Peer ratings has each group member rate each other {{on a set of}} performance dimensions. [...] Peer ranking requires each group member rank all fellow members from [...] "best" [...] to [...] "worst" [...] on one or more dimensions of performance.|$|E
40|$|Children's <b>peer</b> <b>assessments</b> of {{aggressive}} and withdrawn behavior are fundamentally related to developmental {{changes in their}} understanding of others. This article synthesizes research relevant to the thesis that <b>peer</b> <b>assessments</b> are dependent on children's ability both to recall the previous behavior of their peers and to predict their likely future behavior. Social schema theory, borrowed from adult social psychology, is highly relevant to such recall and prediction. Age differences, affective biases, and gender roles may color children's assessments of their peers' social behavior. Such influences {{should be taken into}} account when conceptualizing interventions aimed at enhancing children's peer status, and in measuring the success of these interventions...|$|R
40|$|Nowadays, the {{educational}} methodology known as “peer assessment” constitutes {{one of the}} pillars of formative assessment at the different levels of {{the educational}} system, particularly at the University level. In fact, in recent years, it has been increasingly used to enhance students' meaningful learning, as it {{is considered to be}} an element of social learning, in which students benefit from the lessons learned by other classmates, and draw upon the ability to assess the quality of the learning, contrasting it with the level of knowledge that each has about the subject/course being evaluated, and using common evaluation criteria. In this regard, this paper represents the experience of two groups of students. It allows us to determine how many <b>peer</b> <b>assessments</b> should be required of students in a particular course in order to constitute a serious, reliable activity. On the other hand, {{from the point of view}} of the student, the assessments are evaluated to the extent that they are seen as a required and mandatory exercise that must be carried out by students simply to pass the course. In the latter case, the activity can become extremely trivial and banal. Statistical analysis of the results indicates that three <b>peer</b> <b>assessments</b> per student appraised represents an adequate number. On the other hand, more than thirty <b>peer</b> <b>assessments</b> fail to contribute to learning, nor do they represent serious activitie...|$|R
50|$|An MoU {{was signed}} with the International Laboratory Accreditation Cooperation (ILAC) on 12 November 2006 whereby ILAC and OIML would {{cooperate}} {{in the use of}} the OIML Mutual Acceptance Arrangement (MAA) tool and both organisations would cooperate in the field of harmonisation of accreditation by ILAC full members and <b>peer</b> <b>assessments</b> organized by the BIML. On 28 October 2007, during the ILAC/IAF General Assembly in Sydney, the MoU was extended to include the International Accreditation Forum (IAF) whereby the whereby ILAC and OIML would cooperate {{in the use of the}} OIML Mutual Acceptance Arrangement (MAA) tool and both organisations would cooperate in the field of harmonisation of accreditation by ILAC full members and <b>peer</b> <b>assessments</b> organized by the document Guide for the application of ISO/IEC Guide 65 to legal metrology was to be revised to bring it into line with other ISO standards, notable ISO 17021 and ISO 9001.|$|R
2500|$|On 22 June 2007, U.S. News and World Report editor Robert Morse {{issued a}} {{response}} {{in which he}} argued, [...] "in terms of the <b>peer</b> <b>assessment</b> survey, we at U.S. News firmly believe the survey has significant value because {{it allows us to}} measure the [...] "intangibles" [...] of a college that we can't measure through statistical data. Plus, the reputation of a school can help get that all-important first job and plays a key part in which grad school someone {{will be able to get}} into. The peer survey is by nature subjective, but the technique of asking industry leaders to rate their competitors is a commonly accepted practice. The results from the peer survey also can act to level the playing field between private and public colleges." ...|$|E
2500|$|In 2007, {{members of}} the Annapolis Group {{discussed}} a letter to college presidents asking them {{not to participate in}} the US News [...] "reputation survey". A majority of the approximately 80 presidents at the meeting agreed not to participate, although the statements were not binding. Members pledged to develop alternative web-based information formats in conjunction with several collegiate associations. US News responded that their <b>peer</b> <b>assessment</b> survey helps them measure a college's [...] "intangibles" [...] such as the ability of a college's reputation to help a graduate win a first job or entrance into graduate school. An article by Nicholas Thompson in Washington Monthly criticized the U.S. News rankings as [...] "confirming the prejudices of the meritocracy" [...] by tuning their statistical algorithms to entrench the reputations of a handful of schools, while failing to measure how much students learn. Thompson described the algorithms as being [...] "opaque enough that no one outside the magazine can figure out exactly how they work, yet clear enough to imply legitimacy." ...|$|E
2500|$|The 2011 US News and World Report rankings place RIT at #7 {{under the}} Regional Universities (North) category, where it {{received}} {{the second highest}} <b>peer</b> <b>assessment</b> score, which is a survey of presidents, provosts and deans from other universities judging a school’s academic excellence. RIT is also ranked #2 in the [...] "Great School, Great Prices" [...] category for Regional Universities (North). The 2013 America's Best Colleges ranked by Forbes.com placed RIT at #349 out of 650 colleges, while the 2011 Webometrics Ranking of World Universities rank the school at #161 {{out of the top}} 12000 institutions, and #1 under the Regional Universities (Northeastern) category. RIT's undergraduate engineering programs have been ranked in the top 64 in the country by the US News and World Report. The E. Philip Saunders College of Business was ranked #58 in the 2008 Business Week Best Undergrad B-Schools and was included in the 2009 Business Week Best Undergrad B-Schools as well. It was named one of the [...] "Great Schools for Accounting Majors!" [...] in The Princeton Review's [...] "The Best 368 Colleges." [...] and is featured in Princeton Review's [...] "The Best 290 Business Schools" [...] 2009 edition. RIT's undergraduate education is also {{recognized as one of the}} nation's best in the 2009 edition of Princeton Review's [...] "The Best 369 Colleges". It is also one of the best Northeastern Colleges and in the 2010 edition of Princeton Review's [...] "The Best 371 Colleges", RIT is ranked in the top 20 for [...] "best career services".|$|E
40|$|AbstractProblem-based {{learning}} {{has been in}} use {{in the education of}} medicine, law, and engineering students. The aim {{of this study is to}} put forth that PBL could be used equally successful in the education faculties, hence to analyze the effects of problem-based learning on academic achievement and self-regulated learning skills together, and to undertake self-peer assessments that have been long neglected in PBL studies. Senior students (treatment= 36, control= 21) of Middle East Technical University Faculty of Education Department of Foreign Language Education took part in the study. An achievement test, an open-ended-application exam, a scale on self-regulation in learning, and self-peer assessment forms were developed and used to gather quantitative data. Statistical procedures like independent samples t-test, ANOVA for repeated measures, and Pearson correlation technique were administered to quantitative data. The quantitative results revealed that PBL was effective on students’ academic achievements, but it had no significant effects on self-regulated skills. Students were consistent in their self and <b>peer</b> <b>assessments,</b> but their self- assessments were lower than their <b>peer</b> <b>assessments.</b> Suggestions parallel to findings are given in the end...|$|R
40|$|Here {{we discuss}} a {{framework}} (OpenAnswer) providing {{support to the}} teacher's activity of grading answers to open ended questions. OpenAnswer implements a teacher mediated peer-evaluation approach: the marking results obtained from <b>peer</b> <b>assessments</b> are tuned by the grades explicitly assigned by the teacher, the teacher grades only {{a subset of the}} answers, suggested by the system. When a termination criterion is met, for the process managing the amount of teacher grading work, the remaining answers are automatically graded. A Bayesian Network is designed to represent the information related to students' models, <b>peer</b> <b>assessments,</b> and teacher's grading. The model parameters are many, here we report the results of investigations on a particularly tricky aspect of the framework, that is the modeling and optimization of the Conditional Probability Tables that {{are an important part of}} the Bayesian underlying model. In fact, they express the hypothesized relation between items of information that are relevant for evidence propagation through the network. Results suggest that this optimization improves OpenAnswer's performance, i. e. its capability to infer correct grades. We also show evidence of the influence of the teacher's assessing style on the grading process...|$|R
40|$|Hutchins, Yuan, M., and Santangelo (2015) {{proposed}} the Relative Citation Ratio (RCR) {{as a new}} field-normalized impact indicator. This study investigates the RCR by correlating it {{on the level of}} single publications with established field-normalized indicators and assessments of the publications by peers. We find that the RCR correlates highly with established field-normalized indicators, but the correlation between RCR and <b>peer</b> <b>assessments</b> is only low to medium. Comment: Accepted for publication in the Journal of the Association for Information Science and Technolog...|$|R
2500|$|On 22 June 2007, U.S. News & World Report editor Robert Morse {{issued a}} {{response}} {{in which he}} argued, [...] "in terms of the <b>peer</b> <b>assessment</b> survey, we at U.S. News firmly believe the survey has significant value because {{it allows us to}} measure the [...] "intangibles" [...] of a college that we can't measure through statistical data. Plus, the reputation of a school can help get that all-important first job and plays a key part in which grad school someone {{will be able to get}} into. The peer survey is by nature subjective, but the technique of asking industry leaders to rate their competitors is a commonly accepted practice. The results from the peer survey also can act to level the playing field between private and public colleges." [...] In reference to the alternative database discussed by the Annapolis Group, Morse also argued, [...] "It's important to point out that the Annapolis Group's stated goal of presenting college data in a common format has been tried before [...] U.S. News has been supplying this exact college information for many years already. And it appears that NAICU will be doing it with significantly less comparability and functionality. U.S. News first collects all these data (using an agreed-upon set of definitions from the Common Data Set). Then we post the data on our website in easily accessible, comparable tables. In other words, the Annapolis Group and the others in the NAICU initiative actually are following the lead of U.S. News." ...|$|E
2500|$|Former {{president}} of Sarah Lawrence College, Michele Tolela Myers, responded to Michael Skube's rebuttal in the 12 July 2007 Letter to the Editor for the Los Angeles Times, [...] "Argument {{may be a}} rank disgrace." [...] On the general topic of U.S. News methodology, she states, [...] "what many of us dispute is the validity of a single score computed by using [...] "data points" [...] to which weights are arbitrarily ascribed (why should retention count for 20% instead of 30%; why is <b>peer</b> <b>assessment</b> 25% instead of 10%; and who decides?). How can a single measure be valid when, in some cases, values are made up {{when they are not}} provided (the case of the missing SATs at Sarah Lawrence — the point of my Washington Post Op-Ed)? However, that's exactly what U.S. News does each year. Professional statisticians have reported that the methodology used by the magazine is seriously flawed and cannot be trusted." [...] She also responds to Skube's discussion of Sarah Lawrence's decision not to consider SAT or ACT scores by stating, [...] "Skube says he knows 'all he needs to know about Sarah Lawrence' because the college does not use SAT scores in its admission process, and therefore he infers we don't take aptitude seriously. Perhaps he doesn't know the research showing that SAT tests do not measure aptitude and at best provide a guess about academic performance {{in the first year of}} college. I do not think Elon University's SAT scores tell all there is to know about Elon. To think so would be falling into the trap of using one single measure as a proxy for the complex nature of any college. Which is precisely why the rankings are flawed." ...|$|E
50|$|Assessing <b>Peer</b> <b>Assessment.</b>|$|E
40|$|Assessment {{methods in}} Social Work {{education}} have changed considerably {{in recent years}} and continue to evolve and develop, reflecting changes in the nature of assessment in higher education more generally (Cree, 2000). Key changes have included moves from written examinations to coursework assignments and more emphasis on student participation in <b>assessment</b> (self and <b>peer</b> <b>assessments),</b> processes rather than products, and on competencies rather than content (Brown et al., 1997). Even the more traditional forms of assessment such as essays and examinations have undergone considerable innovations...|$|R
40|$|Evaluative {{bibliometrics}} {{compares the}} citation impact of researchers, research groups and institutions {{with each other}} across time scales and disciplines. Both factors - discipline and period - have an influence on the citation count which is independent {{of the quality of}} the publication. Normalizing the citation impact of papers for these two factors started in the mid- 1980 s. Since then, a range of different methods have been presented for producing normalized citation impact scores. The current study uses a data set of over 50, 000 records to test which of the methods so far presented correlate better with the assessment of papers by <b>peers.</b> The <b>peer</b> <b>assessments</b> come from F 1000 Prime - a post-publication peer review system of the biomedical literature. Of the normalized indicators, the current study involves not only cited-side indicators, such as the mean normalized citation score, but also citing-side indicators. As the results show, the correlations of the indicators with the <b>peer</b> <b>assessments</b> all turn out to be very similar. Since F 1000 focuses on biomedicine, it is important that the results of this study are validated by other studies based on datasets from other disciplines or (ideally) based on multi-disciplinary datasets. Comment: Accepted for publication in the Journal of Informetric...|$|R
40|$|Developing {{effective}} {{methods for}} improving student learning {{in higher education}} is a priority. Recent findings have shown that feedback on student work can effectively facilitate learning if students are engaged as active participants in the feedback cycle; where they seek, generate and use {{feedback in the form}} of dialogue. This novel study investigates the use of <b>peer</b> dialogue <b>assessment</b> as an assessment for learning tool used in an existing undergraduate physical education course. Our findings demonstrate that when thirty six undergraduate physical education students were provided with instruction and practice using <b>peer</b> dialogue <b>assessment</b> after consecutive teaching performances, they exhibit significant improvements in perceived teaching confidence and competence, and teaching self-efficacy. Process evaluation results implying thatembedding <b>peer</b> dialogue <b>assessment</b> in higher education courses may be a feasible approach for facilitating learning, and that students were satisfied with using peer dialogue as a feedback method for improving teaching practices...|$|R
50|$|Contact North / Contact Nord, <b>Peer</b> <b>Assessment</b> and More with peerScholar.|$|E
50|$|A {{number of}} digital tools exists to help {{teachers}} facilitate <b>peer</b> <b>assessment.</b>|$|E
50|$|Academic Matters Journal, Zen and the Art of Metacognition: Quality-Based Discrimination, <b>Peer</b> <b>Assessment</b> & Technology.|$|E
5000|$|Nationally he {{has been}} a Member of Expert Committee-University Grants Commission, <b>Peer</b> Committee-National <b>Assessment</b> and Accreditation Council (NAAC), Joint Consultative Committee, High Power Committee-Higher Educational Policies, In-house Committee-Higher Education, ...|$|R
40|$|Rater {{training}} is fundamental in reducing rater variability in self- and <b>peer</b> <b>assessments</b> practice within {{the paradigm of}} assessment as learning (AaL). Since Malaysian education system is examination-oriented in which assessment of learning (summative) and assessment for learning (formative) dominate, ESL learners are rarely asked to rate themselves or their peers as the system is still sceptical in entrusting learners {{with the role of}} assessors. Learners are normally perceived as unable to (1) assess accurately, (2) assess consistently and (3) discriminate oral proficiency components in their performance. Therefore, this study attempts to gauge learners' rating skills on these three assumptions through the use of Common European Framework of Reference (CEFR) oral assessment criteria. Quantitative analysis was conducted using the Rasch model while a short semi-structured interview was used to support the quantitative results obtained. Findings from this study suggest that ESL learners were generally able to rate accurately and consistently after rater training. The rater training had also somewhat sensitized these learners to CEFR oral proficiency components but they were still grappling in confidently rating range and accuracy. Based on these findings, it indicates that ESL learners' are ready for AaL paradigm shift and this will possibly launch a platform for self- and <b>peer</b> <b>assessments</b> practice in ESL classrooms. Such assessments will promote active learning and effective learner-centred classroom...|$|R
40|$|In this paper, the {{proposed}} model is described {{to demonstrate how}} to mix students in a heterogonous way and equally balance groups {{in terms of the}} educational background and assessment of students. A cyclic genetic algorithm (CGA) is employed in the model to mimic the natural process of evolution to achieve the optimized solution. In order to keep population diversity in the CGA, a particular cycle shift operator and self-crossover operator are presented. The model {{can be used as a}} starting point for considering both educational background and <b>peer</b> <b>assessments</b> in the formation of heterogeneous groups of students...|$|R
5000|$|Formal {{assessment}} functions, such as examinations, essay submission, or {{presentation of}} projects. this now frequently includes components to support <b>peer</b> <b>assessment</b> ...|$|E
50|$|Journal of Computer Assisted Learning, Peering {{into large}} lectures: {{examining}} peer and expert mark agreement using peerScholar, an online <b>peer</b> <b>assessment</b> tool.|$|E
50|$|Employing self or <b>peer</b> <b>assessment</b> allows {{teachers}} {{to manage their}} time more effectively while having students grade each other’s papers results in a more efficient classroom setting.|$|E
40|$|The role of {{emergent}} {{leaders in}} shaping the norms of an initially leaderless problem solving team was examined {{in the context of}} a field study of autonomous groups. Leaders were found to strongly influence the norms that a team adopted around problem solving behaviours, such that when staff started out with low expectations of collaborative problem solving behaviours, a leader who had initially high problem solving behaviour expectations was able to significantly raise the problem solving norms established in the team. Team problem solving norms significantly impacted <b>peer</b> <b>assessments</b> of individual team member problem solving behaviours...|$|R
5000|$|Academics {{are usually}} {{employed}} {{in research and}} development activities that are less well understood than those of professionals, and therefore submit themselves to <b>peer</b> review <b>assessment</b> by other appropriately qualified individuals. See also perceived_safety ...|$|R
40|$|This paper {{begins with}} a {{description}} of several learning goals of the two-course Introductory Statistics sequence for undergraduate business students at the International University of Monaco, and then focuses on the goals of communicating statistical results and becoming critical consumers of statistical information. As our students are business students, we aim for them to become both producers and literate consumers of statistical analysis. In line with reform movements in Statistics Education and the GAISE guidelines, we are working to implement teaching strategies and assessment methods that align instruction and assessment with our learning goals. One of the main instructional tools we use is group projects with elements of <b>peer</b> and self <b>assessment.</b> This paper describes how peer evaluations are carried out, how they are summarized and why we believe that explicitly incorporating these self and <b>peer</b> <b>assessments</b> has improved student learning both in communicating and in consuming statistical information...|$|R
