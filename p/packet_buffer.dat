71|531|Public
50|$|It {{is normal}} for IP {{protocol}} stack software to keep many copies of different IP packets, for transmission, reception {{and to keep}} copies in case {{they need to be}} resent. uIP is economical in its use of memory because it uses only one <b>packet</b> <b>buffer.</b> First, it uses the <b>packet</b> <b>buffer</b> in a half-duplex way, using it in turn for transmission and reception. Also, when uIP needs to retransmit a packet, it calls the application code in a way that requests for the previous data to be reproduced.|$|E
50|$|Traffic {{policing}} requires {{maintenance of}} numerical statistics and measures for each policed traffic flow, {{but it does}} not require implementation or management of significant volumes of <b>packet</b> <b>buffer.</b> Consequently, it is significantly less complex to implement than traffic shaping.|$|E
5000|$|Direct {{memory access}} (DMA) is where some other device {{other than the}} CPU assumes control of the system bus to move data to or from the NIC to memory. This removes load from the CPU but {{requires}} more logic on the card. In addition, a <b>packet</b> <b>buffer</b> on the NIC may not be required and latency can be reduced.|$|E
30|$|If big {{electronic}} routers {{required only}} a few dozen <b>packet</b> <b>buffers,</b> it could reduce their complexity making them easier to build and easier to scale. A typical 10 [*]Gbps router line-card today contains about one million <b>packet</b> <b>buffers,</b> using many external DRAM chips. The board space occupied by the DRAMs the pins they require and the power they dissipate all limit {{the capacity of the}} router [8].|$|R
5000|$|In {{computer}} networks, random walks can {{model the}} number of transmission <b>packets</b> <b>buffered</b> at a server [...]|$|R
40|$|All packet {{switches}} contain <b>packet</b> <b>buffers</b> to hold <b>packets</b> {{during times of}} congestion. The capacity of a high performance router is often dictated by the speed of its <b>packet</b> <b>buffers.</b> This is particularly true for a shared memory switch where the memory needs to operate at times the line rate, where {{is the number of}} ports in the system. Even input queued switches must be able to <b>buffer</b> <b>packets</b> at the rate at which they arrive. And so as link rates increase memory bandwidth requirements grow. With today's DRAM technology and for an OC 192 c (10 Gb/s) link, it is barely possible to write packets to (read packets from) memory at the rate at which they arrive (depart). As link rates increase, the problem will get harder. There ar...|$|R
5000|$|... 7500E series: Modular chassis with a VOQ fabric {{supporting}} up to 4 or 8 {{store and}} forward line cards delivering line-rate non-blocking 10GbE, 40GbE, and 100GbE performance in a 30Tbit/s fabric supporting a maximum of 1152 10GbE ports with 144GB of <b>packet</b> <b>buffer.</b> Each 100GbE ports can also operate as 3x40G or 12x10G ports, thus effectively providing 120Gb of line-rate capacity per port.|$|E
5000|$|Another {{issue is}} that its single <b>packet</b> <b>buffer</b> can have {{substantial}} throughput problems because a PC host usually delays the [...] "ACK" [...] packet, waiting for more packets. In slow, serial port implementations, the ack-throughput can be fixed by modifying uIP to send every packet as two half-packet fragments. uIP systems with fast ethernet or WiFi can modify the hardware driver to send every packet twice.|$|E
50|$|Polling is an {{alternative}} to interrupt-based processing. The kernel can periodically check {{for the arrival of}} incoming network packets without being interrupted, which eliminates the overhead of interrupt processing. Establishing an optimal polling frequency is important, however. Too frequent polling wastes CPU resources by repeatedly checking for incoming packets that have not yet arrived. On the other hand, polling too infrequently introduces latency by reducing system reactivity to incoming packets, and it may result in the loss of packets if the incoming <b>packet</b> <b>buffer</b> fills up before being processed.|$|E
5000|$|FlexE has low added latency as {{compared}} to regular Ethernet. The multiplexing is accomplished using time division multiplexing instead of <b>packet</b> <b>buffers.</b> This type of multiplexing delivers deterministic latency that is near the minimum needed to deliver the bandwidth ...|$|R
3000|$|... 3 <b>packets</b> <b>buffered</b> in the space-limited {{receiver}} buffer {{and cause}} a severe hand over delay problem. Worse still for the time-sensitive multimedia streaming services, the delayed delivery of multimedia packets {{may lead to}} some of these received packets becoming unplayable.|$|R
40|$|Abstract. Active queue {{management}} (AQM) is {{an important}} function in today’s core routers that will be required in the future optical internet core. A recently reported novel architecture for optical <b>packet</b> <b>buffers</b> is extended by implementing necessary AQM functions. The suggested AQM scheme is validated and explore via simulations. ...|$|R
5000|$|By placing {{limits on}} the number of packets that can be {{transmitted}} or received at any given time, a sliding window protocol allows an unlimited number of packets to be communicated using fixed-size sequence numbers.The term [...] "window" [...] on the transmitter side represents the logical boundary {{of the total number of}} packets yet to be acknowledged by the receiver. The receiver informs the transmitter in each acknowledgment packet the current maximum receiver buffer size (window boundary). The TCP header uses a 16 bit field to report the receive window size to the sender. Therefore, the largest window that can be used is 216 = 64 kilobytes. In slow-start mode, the transmitter starts with low packet count and increases the number of packets in each transmission after receiving acknowledgment packets from receiver. For every ack packet received, the window slides by one packet (logically) to transmit one new packet. When the window threshold is reached, the transmitter sends one packet for one ack packet received. If the window limit is 10 packets then in slow start mode the transmitter may start transmitting one packet followed by two packets (before transmitting two packets, one packet ack has to be received), followed by three packets and so on until 10 packets. But after reaching 10 packets, further transmissions are restricted to one packet transmitted for one ack packet received. In a simulation this appears as if the window is moving by one packet distance for every ack packet received. On the receiver side also the window moves one packet for every packet received. The sliding window method ensures that traffic congestion on the network is avoided. The application layer will still be offering data for transmission to TCP without worrying about the network traffic congestion issues as the TCP on sender and receiver side implement sliding windows of <b>packet</b> <b>buffer.</b> The window size may vary dynamically depending on network traffic.|$|E
30|$|Consider the two users {{case with}} a single <b>packet</b> <b>buffer</b> size. The Semi-Greedy Policy is the optimal among all {{policies}} in terms of maximizing the average capacity.|$|E
40|$|Abstract — High-performance routers need {{to store}} {{temporarily}} {{a large number}} of packets in response to congestion. DRAM is typically used to implement the needed packet buffers, but DRAM devices are too slow to match the bandwidth requirements. To bridge the bandwidth gap, a number of hybrid SRAM/DRAM <b>packet</b> <b>buffer</b> architectures have been proposed [7]–[10]. These <b>packet</b> <b>buffer</b> architectures assume a very general model where the buffer consists of many logically separated FIFO queues that may be accessed in random order. For example, virtual output queues (VOQs) are used in crossbar routers, where each VOQ corresponds to a logical queue corresponding to a particular output. Depending on the scheduling algorithm used, the access pattern to these logical queues may indeed be at random. However, for a number of router architectures, this worst-case random access assumption is unnecessary since packet departure times are deterministic. One architecture is the switch-memory-switch router architecture [3], [4] that efficiently mimics an output queueing switch. Another architecture is the load-balanced router architecture [1], [2] that has interesting scalability properties. In these architectures, for best-effort routing, the departure times of packets can be deterministically calculated before inserting packets into packet buffers. In this paper, we describe a novel <b>packet</b> <b>buffer</b> architecture based on interleaved memories that takes advantage of the known packet departure times to achieve simplicity and determinism. The number of interleaved DRAM banks required to implement the proposed <b>packet</b> <b>buffer</b> architecture is independent of the number of logical queues, yet the proposed architecture can achieve the performance of an SRAM implementation. I...|$|E
40|$|This paper {{discusses}} {{the performance of}} traffic flow over Local Area Networks (LAN) utilizing buffers to avoid any irrelevant traffic that clusters the network using sniffer pro. The study applies buffer technology to filter out unnecessary information so the system captures only required information <b>packets.</b> <b>Buffers</b> that capture the protocols are identified. These are...|$|R
40|$|Abstract [...] Internet routers and Ethernet {{switches}} contain <b>packet</b> <b>buffers</b> to hold <b>packets</b> {{during times}} of congestion. <b>Packet</b> <b>buffers</b> {{are at the heart}} of every packet switch and router, which have a combined annual market of tens of billions of dollars, and equipment vendors spend hundreds of millions of dollars on memory each year. Designing <b>packet</b> <b>buffers</b> used to be easy: DRAM was cheap, low power and widely used. But something happened at 10 Gb/s when packets started to arrive and depart faster than the access time of a DRAM. Alternative memories were needed, but SRAM is too expensive and powerhungry. A caching solution is appealing, with a hierarchy of SRAM and DRAM, as used by the computer industry. However, in switches and routers it is not acceptable to have a “miss-rate ” as it reduces throughput and breaks pipelines. In this paper we describe how to build caches with 100 % hit-rate under all conditions, by exploiting the fact that switches and routers always store data in FIFO queues. We describe a number of different ways to do it, with and without pipelining, with static or dynamic allocation of memory. In each case, we prove a lower bound on how big the cache needs to be, and propose an algorithm that meets, or comes close, to the lower bound. These techniques are practical and have been implemented in fast silicon; as a result, we expect the techniques to fundamentally change the way switches and routers use external memory. I...|$|R
40|$|Epidemic routing [18] {{has been}} {{proposed}} as an approach for routing in sparse and/or highly mobile networks in which {{there may not be}} a contemporaneous path from source to destination. Epidemic routing adopts a so-called “store-carry-forward ” paradigm – a node receiving a <b>packet</b> <b>buffers</b> and carries that packet as it moves, passing the packet on to new nodes that it encounters. Analogous to the spread of infectiou...|$|R
40|$|In this paper, {{we address}} {{the design of}} a future {{high-speed}} router that supports line rates as high as OC- 3072 (160 Gb/s), around one hundred ports and several service classes. Building such a high-speed router would raise many technological problems, one of them being the <b>packet</b> <b>buffer</b> design, mainly because in router design it is important to provide worst-case bandwidth guarantees and not just average-case optimizations. A previous <b>packet</b> <b>buffer</b> design provides worst-case bandwidth guarantees by using a hybrid SRAM/DRAM approach. Next-generation routers need to support hundreds of interfaces (i. e., ports and service classes). Unfortunately, high bandwidth for hundreds of interfaces requires the previous design to use large SRAMs which become a bandwidth bottleneck. The key observation we make is that the SRAM size is proportional to the DRAM access time but we can reduce the effective DRAM access time by overlapping multiple accesses to different banks, allowing us to reduce the SRAM size. The key challenge is that to keep the worst-case bandwidth guarantees, we need to guarantee that there are no bank conflicts while the accesses are in flight. We guarantee bank conflicts by reordering the DRAM requests using a modern issue-queue-like mechanism. Because our design may lead to fragmentation of memory across <b>packet</b> <b>buffer</b> queues, we propose to share the DRAM space among multiple queues by renaming the queue slots. To the best of our knowledge, the design proposed in this paper is the fastest buffer design using commodity DRAM to be published to date. Peer ReviewedPostprint (published version...|$|E
3000|$|Proof. Intuitively, {{under our}} setting {{of a single}} <b>packet</b> <b>buffer</b> size, each packet should be {{transmitted}} uncoded at least once. Then, it could be retransmitted in coded packets. To maximize the average capacity, an optimal policy would minimize the additional uncoded retransmissions. Clearly, the semi-greedy scheme obtains this by sending each packet uncoded only once and sending all retransmissions coded. In addition, from a symmetry reasoning, at state S [...]...|$|E
40|$|In {{this paper}} we address {{the design of a}} future {{high-speed}} router that supports line rates as high as OC- 3072 (160 Gb/s), around one hundred ports and several service classes. Building such a high-speed router would raise many technological problems, one of them being the <b>packet</b> <b>buffer</b> design, mainly because in router design it is important to provide worst-case bandwidth guarantees and not just average-case optimizations. A previou...|$|E
40|$|Reducing {{the power}} {{consumption}} of core Internet routers {{is important for}} both Internet Service Providers (ISPs) and router vendors. ISPs can reduce their Carbon footprint and operational costs, while router manufacturers can achieve higher switching capacity per rack space. In this work, we {{examine the impact of}} <b>packet</b> <b>buffers</b> on the power consumption of backbone router line-cards. We argue that Gigabytes of always-on SRAM and DRAM buffers account for around 10 % of the power, but are actively used only during transient periods of congestion. We propose a simple and practical algorithm for activating buffers incrementally as needed and putting them to sleep when not in use. We evaluate our algorithm on traffic traces from carrier and enterprise networks, via simulations in ns 2, and by implementing it on a programmable-router test-bed. Our study shows that much of the energy associated with off-chip <b>packet</b> <b>buffers</b> can be eliminated with negligible impact on traffic performance. Dynamic adjustment of active router buffer size provides a low-complexity low-risk mechanism of saving energy that is amenable for incremental deployment in networks today. 1...|$|R
40|$|Abstract. With the Internet {{services}} increased explosively, {{the requirement}} for network bandwidth is rigorous. Upon the extraordinary development of process capability, Memory access control has become a key factor that impacts the performance of network processor. The paper proposed a storage management for fast <b>packet</b> <b>buffers</b> in network processor, which can enhance the utilization of bandwidth. Experiment results shows that this approach improved the rates of accessing to memory system in network processor remarkably...|$|R
50|$|In {{computer}} networking, micro-bursting is {{a behavior}} seen on fast packet-switched networks, where rapid bursts of data packets are sent in quick succession, leading to periods of full line-rate transmission that can overflow <b>packet</b> <b>buffers</b> {{of the network}} stack, both in network endpoints and routers and switches inside the network. It can be mitigated by the network scheduler. In particular, micro-bursting is often caused {{by the use of}} the TCP protocol on such a network.|$|R
40|$|High-speed routers rely on well-designed packet buffers {{that support}} {{multiple}} queues, provide large capacity and short response times. Some researchers suggested combined SRAM/DRAM hierarchical buffer architectures {{to meet these}} challenges. However, these architectures suffer from either large SRAM requirement or high time-complexity in the memory management. In this paper, we present scalable, efficient, and novel distributed <b>packet</b> <b>buffer</b> architecture. Two fundamental issues {{need to be addressed}} to make this architecture feasible: 1) how to minimize the overhead of an individual packet buffer; and 2) how to design scalable packet buffers using independent buffer subsystems. We address these issues by first designing an efficient compact buffer that reduces the SRAM size requirement by (k - 1) /k. Then, we introduce a feasible way of coordinating multiple subsystems with a load-balancing algorithm that maximizes the overall system performance. Both theoretical analysis and experimental results demonstrate that our load-balancing algorithm and the distributed <b>packet</b> <b>buffer</b> architecture can easily scale to meet the buffering needs of high bandwidth links and satisfy the requirements of scale and support for multiple queues...|$|E
40|$|EMC-Y {{is a new}} {{processing}} element {{for highly}} parallel computers designed to achieve high performance paral-lel computation by fusing a dataflow mechanism and a von Neumann execution pipeline. We have already de-veloped EMC-R, which is the processing element used in the EM- 4 prototype. EM C-Y improves on EMC-R’s packet communication performance, allowing it to tolerate a more network traffic. This paper presents the architecture of EMC-Y, concentrating on the prin-ciples of packet communication. EMC-Y uses an output <b>packet</b> <b>buffer</b> and optimal packet routing to improve the performance of packet sending and transferring. EMC-Y changes the memory access priority for input <b>packet</b> <b>buffer</b> operation to improve the performance of receiv-ing packets. Since the EMC-Y processor not only im-proves the performance of packet input and output but also balances them, it can tolerate {{a large amount of}} trafic and can improve the execution performance. We evaluate the improvements of EMC-Y architecture us-ing a clock level simulator. The results show that EMC-Y improves performance by 50 ’? 10 to 7070 in several prc-grams over EMC-R at the same clock speed. ...|$|E
40|$|The {{computer}} engineering group at Linköping University has {{parts of their}} research dedicated to networks-on-chip and components used in network components and terminals. This research has among others resulted in the SoCBUS NOC and a flow based network protocol processor. The main objective of this project was to integrate these components into an IP router with two or more Gigabit Ethernet interfaces. A working system has been designed and found working. It consists of three main components, the input module, the output module and a <b>packet</b> <b>buffer.</b> Due to the time constraint {{and the size of}} the project the <b>packet</b> <b>buffer</b> could not be designed to be as efficient as possible, thus reducing the overall performance. The SoCBUS also has negative impact on performance, although this could probably be reduced with a revised system design. If such a project is carried out it could use the input and output modules from this project, which connect to SoCBUS and can easily be integrated with other packet buffers and system designs...|$|E
40|$|AbstractStore-and-forward {{deadlock}} (SFD) {{occurs in}} packet switched computer networks when, among some cycle of <b>packets</b> <b>buffered</b> by the communication system, each packet in the cycle {{waits for the}} use of the buffer currently occupied by the next packet in the cycle. In this paper, a deadlock-free algorithm as well as a livelock-free algorithm for packet switching is obtained using the strategy of the banker's algorithm. Furthermore, the solution obtained is interpreted for the hyper-fast banker's problem...|$|R
5000|$|One common {{simplification}} of selective-repeat is {{so called}} SREJ-REJ ARQ. This operates with wr=2 and <b>buffers</b> <b>packets</b> following a gap, but only allows a single lost packet; {{while waiting for}} that packet, wr=1 and if a second packet is lost, no more <b>packets</b> are <b>buffered.</b> This gives most of the performance benefit of the full selective-repeat protocol, with a simpler implementation.|$|R
30|$|Throughput {{is defined}} by the average payload bits which are {{transmitted}} successfully in a time slot divided by the duration of the time slot, i.e. Tslot. We mainly consider saturation throughput where there is always a <b>packet</b> in the <b>buffer</b> of each station ready for transmission. However, in Section IV-C non-saturation throughput and impact of the presence of <b>packets</b> in <b>buffers</b> on the throughput are studied.|$|R
40|$|High-speed routers rely on well-designed packet buffers {{that support}} {{multiple}} queuing, large capacity and short response times. Some researchers suggested a combined SRAM/DRAM hierarchical buffer architecture {{to meet these}} challenges. However, both the SRAM and DRAM need to maintain {{a large number of}} dynamic queues which is a real challenge in practice and limits the scalability of these approaches. In this paper, we present a scalable, efficient and novel distributed <b>packet</b> <b>buffer</b> architecture. Two fundamental issues need to be addressed to make this feasible: (a) how to design scalable packet buffers using independent buffer subsystems; and (b) how to dynamically balance the workload among multiple buffer subsystems without any blocking. We address these issues by first designing a basic framework that allows flows to dynamically switch from one subsystem to another without any blocking. Based on this framework, we further devise a load-balancing algorithm to meet the overall system requirements. Both theoretical analysis and experimental results demonstrate that our load-balancing algorithm and the distributed <b>packet</b> <b>buffer</b> architecture can easily scale to meet the buffering needs of high bandwidth links with large number of active connections. © 2010 IEEE...|$|E
40|$|Massive {{data centers}} {{are being built}} {{around the world to}} provide various cloud {{computing}} services. As a result, data center networking has recently been a hot research topic in both academia and industry. A fundamental challenge in this area is the design of the data center network that interconnects the massive number of servers, and provides an efficient and robust platform. In response to this challenge, the research community has begun exploring novel interconnection network topologies. One approach is to use commodity electronic switches or servers to scale out the network, such as Portland, VL 2, DCell, BCube, FiConn, etc. The other approach is to exploit optical devices to build high-capacity switches, such as OSA, Helios, HyPaC, PETASW, Data Vortex, OSMOSIS, etc. Understandably, this research is still in its infancy. For the first approach, the solutions proposed so far either scale too slow or suffer from performance bottlenecks, are server-location dependent, inherit poor availability, can be too complex/expensive to be constructed. For the second approach, where the entire interconnection network can be regarded as a “giant” switch, its performance heavily relies on well-designed packet buffers that support multiple queues, provide large capacity and short response time. In this thesis, five algorithms/architectures are presented, addressing on both of these issues respectively. Using commodity switches only, we propose two cost-effective and gracefully scalable Data Center Interconnection networks (DCIs) called HyperBCube and FlatNet which yield robust performance and inherit low-time-complexity routing based on simple network topologies. On the other hand, aiming at scalable packet buffers, we propose three <b>packet</b> <b>buffer</b> architectures along with their memory management algorithms based on distributed, parallel and hierarchical memory structures respectively. Both mathematical analysis and simulation results indicate that the proposed <b>packet</b> <b>buffer</b> architectures outperform the traditional <b>packet</b> <b>buffer</b> architectures significantly in terms of low time complexity, short access delay and guaranteed performance. Keywords: Data Center, Interconnection Network, Router memory, SRAM / DRAM...|$|E
40|$|The routing {{problem for}} a {{high-speed}} network in which a single <b>packet</b> <b>buffer</b> is associated with each output port of each node is considered. Conflicts occur whenever two or more packets are sent through the same output port. An algorithm to solve such situations is stated. It yields a suboptimal solution for the combinatorial local decision problem. A coordination among the local decision problems is performed by making each node pass aggregate information regarding the congestion of the downstream portion of the network to its predecessors...|$|E
30|$|Extending timeout values: During {{the route}} {{recovery}} process, the <b>packets</b> are <b>buffered</b> {{along the path}} from the source to the PN to avoid retransmission of packets on route re-establishment. It is possible that timeout occurs for the <b>buffered</b> <b>packets.</b> Therefore, {{it is necessary to}} increase transmission timeout values to avoid timeout events. For ease of implementation, the proposed scheme just doubles the timeout values.|$|R
30|$|Avoiding {{unnecessary}} {{requests for}} fast retransmission: On route restoration, the destination can notify the source about the lost packets. In response, the source simply retransmits the lost <b>packets.</b> The <b>packets</b> <b>buffered</b> {{along the path}} from the source to the PN may arrive at their destination earlier than the retransmitted packets, but the destination continues to send duplicate ACK until the expected packets arrive at the destination (via the fast retransmit method adopted by TCP-Reno). In TCP-BuS, these unnecessary request packets for fast retransmission are avoided.|$|R
40|$|Input-queued {{crossbars}} are {{the common}} building blocks in Internet routers, datacenter and high-performance computing interconnects, and on-chip networks. These crossbars often contain no buffers, which saves valuable chip area. Arriving packets issue requests {{to a central}} scheduler. While waiting for the scheduler to grant their requests, packets wait at input <b>packet</b> <b>buffers</b> {{in front of the}} crossbar. To isolate traffic for different outputs, these input buffers are often organized as virtual output queues (VOQs). A VOQ crossbar’s trade-off of speed and switching efficiency depends on its crossba...|$|R
