2|5|Public
40|$|Novel weighted-least-squares {{approaches}} {{to the design of}} digital filters for SAR applications are presented. The filters belong to three different categories according to their combinations of minimax passband, least-squares stopband, minimax stopband, and maximally-flat passband. For real-time applications, it is important to design the sets of digital filter coefficient tables in an offline environment; the appropriate <b>precomputed</b> <b>filter</b> is then selected for each SAR signal-processing function, as a function of both mode and mapping geometry during real-time processing...|$|E
30|$|The most {{time-consuming}} part {{of computing}} a SIRT-FBP filter is performing the iterations of Eq. (12). In each iteration, a single forward projection {{and a single}} backprojection operation are needed, similar to the standard iterative SIRT method. The total required time for computing a single filter is therefore similar to {{the time it takes}} to compute a SIRT reconstruction for a single slice. Note, however, that the filter can be precomputed for a specific acquisition geometry (i.e., number of projections and detector column pixels, and size of the reconstruction grid). The <b>precomputed</b> <b>filter</b> can be stored and used for future SIRT-FBP reconstructions using the same acquisition geometry. Furthermore, filters for multiple numbers of iterations can be computed in a single run by storing the filter at each chosen number of iterations. Finally, a full three-dimensional tomographic reconstruction using SIRT-FBP can be computed slice-by-slice, with each slice using the same filter.|$|E
30|$|In {{advanced}} tomographic experiments, large detector {{sizes and}} {{large numbers of}} acquired datasets can {{make it difficult to}} process the data in a reasonable time. At the same time, the acquired projections are often limited in some way, for example having a low number of projections or a low signal-to-noise ratio. Direct analytical reconstruction methods are able to produce reconstructions in very little time, even for large-scale data, but the quality of these reconstructions can be insufficient for further analysis in cases with limited data. Iterative reconstruction methods typically produce more accurate reconstructions, but take significantly more time to compute, which limits their usefulness in practice. In this paper, we present the application of the SIRT-FBP method to large-scale real-world tomographic data. The SIRT-FBP method is able to accurately approximate the simultaneous iterative reconstruction technique (SIRT) method by the computationally efficient filtered backprojection (FBP) method, using <b>precomputed</b> experiment-specific <b>filters.</b> We specifically focus on the many implementation details that are important for application on large-scale real-world data, and give solutions to common problems that occur with experimental data. We show that SIRT-FBP filters can be computed in reasonable time, even for large problem sizes, and that <b>precomputed</b> <b>filters</b> can be reused for future experiments. Reconstruction results are given for three different experiments, and are compared with results of popular existing methods. The results show that the SIRT-FBP method is able to accurately approximate iterative reconstructions of experimental data. Furthermore, they show that, in practice, the SIRT-FBP method can produce more accurate reconstructions than standard direct analytical reconstructions with popular filters, without increasing the required computation time.|$|R
30|$|Our {{implementation}} supports both {{single precision}} (32 -bit) and double precision (64 -bit) IEEE 754 {{floating point numbers}} (double precision is only supported on devices with compute capability 2.0 or newer due to limitations in {{the maximum amount of}} shared memory available per multiprocessor). We generate the filter bank of directional filters using the Fourier-domain approach from [9], where directional filters are designed as Meyer-type window functions in the Fourier domain. Since this step only needs to be run once and does not depend on the image dimensions, we <b>precompute</b> these directional <b>filters</b> using the original MATLAB implementation.|$|R
40|$|Increasingly {{data on the}} Web {{is stored}} {{in the form of}} Semantic Web data. Because of today’s {{information}} overload, it becomes very important to store and query these big datasets in a scalable way and hence in a distributed fash-ion. Cloud Computing offers such a distributed environment with dynamic reallocation of computing and storing resources based on needs. In this work we introduce a scalable distributed Semantic Web database in the Cloud. In order {{to reduce the number of}} (unnecessary) intermediate results early, we apply bloom filters. Instead of computing bloom filters, a time-consuming task during query processing as it has been done traditionally, we <b>precompute</b> the bloom <b>filters</b> as much as possible and store them in the indices besides the data. The experimental results with data sets up to 1 billion triples show that our approach speeds up query processing significantly and sometimes even reduces the processing time to less than half. TYPE OF PAPER AND KEYWORD...|$|R
40|$|When drawing images onto a {{computer}} screen, {{the information in}} the scene is typically more detailed than can be displayed. Most objects, however, will not be close to the camera, so details have to be filtered out, or anti-aliased, when the objects are drawn on the screen. I describe new methods for filtering images and shapes with high fidelity while using computational resources as efficiently as possible. Vector graphics are everywhere, from drawing 3 D polygons to 2 D text and maps for navigation software. Because of its numerous applications, having a fast, high-quality rasterizer is important. I developed a method for analytically rasterizing shapes using wavelets. This approach allows me to produce accurate 2 D rasterizations of images and 3 D voxelizations of objects, which {{is the first step in}} 3 D printing. I later improved my method to handle more filters. The resulting algorithm creates higher-quality images than commercial software such as Adobe Acrobat and is several times faster than the most highly optimized commercial products. The quality of texture filtering also has a dramatic impact on the quality of a rendered image. Textures are images that are applied to 3 D surfaces, which typically cannot be mapped to the 2 D space of an image without introducing distortions. For situations in which it is impossible to change the rendering pipeline, I developed a method for <b>precomputing</b> image <b>filters</b> over 3 D surfaces. If I can also change the pipeline, I show that it is possible to improve the quality of texture sampling significantly in real-time rendering while using the same memory bandwidth as used in traditional methods...|$|R
40|$|Autonomous orbital {{rendezvous}} with an orbiting sample (OS) {{is seen as}} an enabling technology for a Mars Sample Return (MSR) mission, so several demonstrations have been planned. With CNES cooperation a proposed rendezvous demonstration was governed by ITAR restrictions, and a guidance and navigation system was designed using a <b>Precomputed</b> Gain Kalman <b>filter</b> and targeting algorithms. Having lost CNES participation, the opportunity now exists to use a full Extended Kalman filter with onboard targeting algorithms on a new demonstration using the Mars Telecommunications Orbiter (MTO). This creates an impetus to compare the Precomputed Gain system with the Extended system to determine their relative performance. This thesis aims to compare the Precomputed Gain and Extended Kalman filters and associated targeting algorithms using a Monte-Carlo analysis, and based on quantitative performance metrics including: total change in velocity required, navigation errors, target pointing errors. In addition, other aspects of the algorithms will be studied including: technology readiness level (TRL) data uplink requirements, and complexity and computational burden for the onboard algorithms. Monte-Carlo analysis will reveal that the Extended system modestly outperforms the Precomputed Gain system in total change in velocity required, navigation error, and target pointing error, with a larger performance envelope. The Extended system will also be found to have a greater technology readiness and require substantially less data uplink. The Precomputed Gain system will be found to be a significantly less complex algorithm for the onboard flight computer. by Andrew Thomas Vaughan. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2004. Includes bibliographical references (p. 235 - 236) ...|$|R

