9|13|Public
40|$|This paper {{presents}} a new cryptographic <b>protocol</b> <b>analyser</b> for key correlation detection. This analyser {{is based on}} network information collection and analysis by abstract interpretation. K. C. S. or Key Correlation System is mainly composed of two modules: Information collection sniffers or sensors and information analyser {{that is based on}} formal interpretation as developed by N. El Kadhi. The main idea and goal of K. C. S is to verify coherence and freshness of cryptographic keys used throughout SSL (Secure Socket Layer) or SSH (Secure SHell) sessions. KCS is also able to verify secret preservation of crucial information by propagating specific constraints. This paper presents KCS global architecture and KCS operator mode, it also includes significant results and experiments...|$|E
40|$|Consider {{the problem}} of verifying {{security}} properties of a cryptographic protocol coded in C. We propose an automatic solution that needs neither a pre-existing protocol description nor manual annotation of source code. First, symbolically execute the C program to obtain symbolic descriptions for the network messages sent by the protocol. Second, apply algebraic rewriting to obtain a process calculus description. Third, run an existing <b>protocol</b> <b>analyser</b> (ProVerif) to prove security properties or find attacks. We formalise our algorithm and appeal to existing results for ProVerif to establish computational soundness under suitable circumstances. We analyse only a single execution path, so our results are limited to protocols with no significant branching. The results in this paper provide the first computationally sound verification of weak secrecy and authentication for (single execution paths of) C code. 1...|$|E
40|$|Network {{attacks have}} become {{prominent}} in the modern-day web activities and the black hat community have also gain more sophistication with the tools used to penetrate poorly guarded or unguarded networks. Network security administrators have also moved swiftly to counter the threats posed by the attacker with different network intrusion detection and monitoring tools. Low interaction honeypots were developed to entice hackers without causing any serious downtime to the production network, so that their activities {{and the way they}} access the network can be studied with a minimal setup cost. In this work, a low interaction virtual honeypot using the Honeyd daemon to lure attackers to the network and alert the attacker's activities in the network using the Snort IDS. The data captured is analysed based on the protocol and port used. It is then validated by analysing the attacker's activities once it is logged and accessed through Wireshark <b>protocol</b> <b>analyser...</b>|$|E
50|$|Transputers {{also found}} use in <b>protocol</b> <b>analysers</b> {{such as the}} Siemens/Tektronix K1103.|$|R
50|$|<b>Protocol</b> <b>analysers</b> are {{connected}} to BTSs, BSCs, and MSCs {{for a period of}} time to check for problems in the cellular network. When a problem is discovered the staff can record it and it can be analysed.|$|R
5000|$|The QoS in {{industry}} is also measured {{from the perspective}} of an expert (e.g. teletraffic engineer). This involves assessing the network to see if it delivers the quality that the network planner has been required to target. Certain tools and methods (<b>protocol</b> <b>analysers,</b> drive tests and Operation and Maintenance measurements), are used for this QoS measurement: ...|$|R
40|$|Packetman is an X- 11 based <b>protocol</b> <b>analyser</b> {{that can}} be used to {{retrospectively}} analyse packets on an ethernet LAN. Packetman can be used for diagnosis, monitoring and troubleshooting of network protocols under a Unix environment. The Packetman display is divided into 3 sections ala TcpView and the Sniffer: The top section is a sequential trace of captured data; the second provides detailed analysis of a packet; and the bottom shows a hex dump of the packet. Several well used protocols can be decoded including:. NFS (Network File System) v 2. NIS (Network Information Service) v 2. Mount Protocol v 1. Yppasswd v 1. Sun Portmapper v 2. ICMP. Telnet. Arp Packetman fully decodes Ethernet/TCP/UDP/IP headers and resolves ethernet/IP addresses to host names. RPC headers, including the authentication, are also fully decoded with the RPC replies being matched to the calls by their transaction IDs. This enables RPC reply packets of recognised protocols to be fully decoded. Sniffer format fil [...] ...|$|E
40|$|Hardware-based packet {{classification}} and capture {{can be a}} useful feature for a high-speed networked device, or a useful debugging aid for NetFPGA projects. This paper presents the design and implementation details of a drop-in module for the NetFPGA framework which provides a simple but nevertheless highly flexible system for matching patterns {{in one or more}} packet headers and/or payloads and diverting such packets to the host system via DMA for inspection or recording. The module is implemented {{in such a way as}} to never act as a bottleneck to the NetFPGA pipeline, and can classify packets at wire speed. We also present an extensible software framework which allows filters to be specified and implemented by the user in a simple manner according to built-in knowledge of common protocols, provides display of captured packets via the Wireshark <b>protocol</b> <b>analyser</b> and optionally further distributes captured packets to custom processes via a publish/subscribe system for analysis and/or storage. The hardware module and associated software will be available to the NetFPGA community under a free license...|$|E
30|$|In practice, it {{is assumed}} that when the QoS is determined, the packet streams from an RTP session are {{collected}} by a <b>protocol</b> <b>analyser</b> and then passed on to a suitable evaluation tool. The new MP 3 Model is such a tool. It works on the following principles: all network impairments are collected and processed in the first block of the diagram. The effects of jitter and out-of-order packet delivery are converted into losses, not forgetting that some errors can be offset with the aid of the jitter buffer. The values attained in this block and the packet losses from the network are passed on to the second block where total losses and burst size are determined. For the MP 3 Model, the Markov property “memorylessness”, which is widely used in analyses of networks, has been assumed. Going off this assumption, it is possible to determine the likelihood of a packet being lost, depending on whether the packet immediately before it was received or lost. The ensuing recalculated parameters are passed on to the third and final block. Further inputs for the third block include information about the number of frames in one RTP packet and which encoding rate is being used. These data are gained from measuring the RTP streams. The last block, called “cognitive model” calculates and outputs the MP 3 factor as a value on the MOS(LQO) scale [13].|$|E
40|$|We {{present a}} model {{checking}} technique for security protocols {{based on a}} reduction to propositional logic. At {{the core of our}} approach is a procedure that, given a description of the protocol in a multi-set rewriting formalism and a positive integer k, builds a propositional formula whose models (if any) correspond to attacks on the protocol. Thus, finding attacks on protocols boils down to checking a propositional formula for satisfiability, problem that is usually solved very efficiently by modern SAT solvers. Experimental results indicate that the approach scales up to industrial strength security protocols with performance comparable with (and in some cases superior to) that of other state-of-the-art <b>protocol</b> <b>analysers...</b>|$|R
40|$|Abstract ⎯ Security {{protocols}} {{require more}} rigorous and detailed verification than normal communication protocols before their deployment because even a trivial flaw in their architecture may produce drastic results. These verification procedures {{are based upon}} the abstract formal methods producing analytical rules to show if a given protocol is secure or not. The technical difficulties to master formal techniques by non-mathematicians {{have given rise to}} online validation tools which are easy to use and produce a human readable output. These online tools differ in many parameters. Our work is aimed to present a qualitative comparison of two automatic security <b>protocol</b> <b>analysers</b> Hermes and AVISPA for the some pertinent parameters. Keywords ⎯ Formal validation, security protocols, Hermes, AVISPA, EVA, HLPSL...|$|R
40|$|Most model {{checking}} {{techniques for}} security protocols make {{a number of}} simplifying assumptions on the protocol and/or on its execution environment that prevent their appli-cability in some important cases. For instance, most tech-niques assume that communication between honest princi-pals is controlled by a Dolev-Yao intruder, i. e. a malicious agent capable to overhear, divert, and fake messages. Yet we {{might be interested in}} establishing the security of a pro-tocol that relies on a less unsecure channel (e. g. a confiden-tial channel provided by some other protocol sitting lower in the protocol stack). In this paper we propose a general model for security protocols based on the set-rewriting for-malism that, coupled with the use of LTL, allows for the specification of assumptions on principals and communica-tion channels as well as complex security properties that are normally not handled by state-of-the-art <b>protocol</b> <b>analysers.</b> By using our approach {{we have been able to}} formalise all the assumptions required by the ASW protocol for optimistic fair exchange as well as some of its security properties. Be-sides the previously reported attacks on the protocol, we report a new attack on a patched version of the protocol. 1...|$|R
40|$|Käesolev töö käsitleb ühte võimalust Peeter Laua ja Ilja Tšahhirovi loodud krüptograafiliste protokollide analüsaatori protokolliesitusest tsüklite eemaldamiseks. Protokolliesituseks kasutatakse analüsaatoris sõltuvusgraafe, millesse transformatsioonide tulemusel võivad tekkida tsüklid. Analüsaatoris on olemas teisendused, mis võimaldavad eemaldada tsükleid, kus on maksimaalselt üks mitterange tipp. Lahendada on vaja rohkemaid mitterangeid tippe sisaldavate tsüklite eemaldamise ülesanne. The aim of {{this paper}} was to {{describe}} a cryptographic <b>protocol</b> <b>analyser,</b> specially its protocol representation, and implement a transformation to simplify protocol representation. The analyser was first described in the doctoral thesis of Ilja Tšahhirov and was afterwards implemented by Peeter Laud and Ilja Tšahhirov. The analyser is designed for static protocol analysis to check confidentiality and integrity of secret messages. All cryptographic primitives used in the protocols have to be strong in computational model in order for the analysis to be correct. Protocols are represented by dependency flow graphs that indicate possible data and control flows in protocol execution. Like any directed graphs, they may contain directed cycles that make understanding and drawing graphs more difficult. In general, all nodes in graph can be divided to two groups - strict and non-strict nodes. Strict nodes only compute their value if they have received all their inputs, on the other hand, non-strict nodes compute their value as soon as they have sufficient inputs to do so. All cycles in graph can be divided to groups depending on which non-strict nodes they contain and how many of them are present. The analyser already has transformations to cut cycles with none or just one non-strict nodes. This paper described a way based on depth-first search trees to find all cycles and transform them so that they contain only one non-strict node and can therefore be removed by cutting an edge in the cycle...|$|E
40|$|This wide-ranging study {{explored}} various {{parameters of}} visual search {{in relation to}} computer screen displays. Its ultimate goal was to help identify factors which could result in improvements in commercially available displays within the 'real world’. Those improvements are generally reflected in suggestions for enhancing efficiency of locatabolity of information through an acknowledgement of the visual and cognitive factors involved. The thesis commenced by introducing an ergonomics approach to the presentation of information on VDUs. Memory load and attention were discussed. In the second chapter, literature on general and theoretical aspects of visual search (with particular regard for VDUs) was reviewed. As an experimental starting point, three studies were conducted involving locating a target within arrays of varying configurations. A model concerning visual lobes was proposed. Two text-editing studies were then detailed showing superior user performances where conspicuity {{and the potential for}} peripheral vision are enhanced. Relevant eye movement data was combined with a keystroke analysis derived from an automated <b>protocol</b> <b>analyser.</b> Results of a further search task showed icons to be more quickly located within an array than textual material. Precise scan paths were then recorded and analyses suggested greater systematicity of search strategies for complex items. This led on to a relatively 'pure' search study involving materials of varying spatial frequencies. Results were discussed in terms of verbal material generally being of higher spatial frequencies and how the ease of resolution and greater cues available in peripheral vision can result in items being accessed more directly. In the final (relatively applied) study, differences in eye movement indices were found across various fonts used. One main conclusion was that eye movement monitoring was a valuable technique within the visual search/VDU research area in illuminating precise details of performance which otherwise, at best, could only be inferred...|$|E
40|$|Video Conferencing {{systems have}} become highly {{popular with the}} {{explosion}} of bandwidth and computing power. This has made high quality video conference possible on embedded hand held devices. This has created an ecosystem of companies developing applications which are powered by video conferencing libraries and a parallel ecosystem of companies who make those video conferencing libraries. An application developer has to design his system {{in a way that}} the video conferencing library (which is the backbone of the application) is efficiently integrated and is performing optimally. In order to do that, the developer must be acutely aware of features such as platform compatibility, resource usage, performance boundaries and bottlenecks of the library in order to make an informed decision. Hence the developer needs a test-bench which can evaluate the above mentioned features. This thesis mainly discusses the need to arrive at one such test framework followed by the framework itself. The framework designed in this thesis consists of a network emulator which is combined with a popular network <b>protocol</b> <b>analyser.</b> It profiles the library and monitors variables essential to Quality of Service (QoS) like bandwidth usage, frame rates, frame/packet sizes and audio/video delay under varying network conditions. By observing the change in these variables, the QoS offered by the library can be determined. This information can also be extrapolated to understand the challenges in guaranteeing a minimum quality of experience (QoE). This thesis applies the framework on one chosen library (Library 1) and Google Hangouts and discusses the results. The performance of the library under varying network conditions (bandwidth, packet loss and latency) is observed, enabling Presence Displays BV to extract information about how the library works. Presence Displays BV is building a video conferencing enabled product and uses the framework discussed in this thesis to analyse video conferencing libraries in order to build the application around it. It has been observed that Library 1 favours video quality above other parameters and only compromises video quality to accommodate more users participating in a call. Google Hangouts favours audio, motion content in the video, interactivity and robustness over video quality. This information can be used to configure Library 1 in a way such that it delivers maximum QoE in the available resources. MSc Embedded SystemsComputer EngineeringElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|ABSTRACT. Most model {{checking}} {{techniques for}} security protocols make {{a number of}} simplifying assumptions on the protocol and/or on its execution environment that greatly complicate or even prevent their applicability in some important cases. For instance, most techniques assume that communication between honest principals is controlled by a Dolev-Yao intruder, i. e. a malicious agent capable to overhear, divert, and fake messages. Yet we {{might be interested in}} establishing the security of a protocol that relies on a less unsecure channel (e. g. a confidential channel provided by some other protocol sitting lower in the protocol stack). In this paper we propose a general model for security protocols based on the set-rewriting formalism that, coupled with the use of LTL, allows for the specification of assumptions on principals and communication channels as well as of complex security properties that are normally not handled by state-of-the-art security <b>protocol</b> <b>analysers.</b> By using our approach {{we have been able to}} formalise all the assumptions required by the ASW protocol for optimistic fair exchange and some of its key security properties. Besides the previously reported attacks on the protocol, we report a new attack on a patched version of the protocol...|$|R
40|$|Within Ericsson AB, {{integration}} and verification activities {{is done on}} the network level {{in order to secure}} the functionality of the network. <b>Protocol</b> <b>analysers</b> are used to capture the traffic in the network. This results in many log files, which needs to be analysed. To do this, a protocol decoder tool called Scapy/LHC is used. Scapy/LHC is a framework that allows the users to write their own script to retrieve the data they need from the log files. The Scapy/LHC framework is incrementally developed as open source within Ericsson when there are needs for more functionality. This is often done by the users, outside normal working tasks. Because of this, there is almost no testing done to verify that old and new functionality works as expected, and there is no formal test strategy in use today. The goal of this master’s thesis is to evaluate test strategies that are possible to use on the Scapy/LHC framework. To make the time needed for the testing process as short as possible, the test strategy needs to be automated. Therefore, possible test automation tools shall also be evaluated. Two possible test strategies and two possible test automation tools are evaluated in this thesis. A test strategy, where the scripts that are written by the users are used, is then selected for implementation. The two test automation tools are also implemented. The evaluation of the implemented test strategy shows {{that it is possible to}} find defects in the Scapy/LHC framework in a time efficient way with help of the implemented test strategy and any of the implemented test automation tools...|$|R
40|$|The SPADE theory uses {{linguistic}} formalisms {{to model}} the program planning and debugging processes. The theory begins with a taxonomy of basic planning concepts covering strategies for identification, decomposition and reformulation. A handle is provided for recognizing interactions between goals and deriving a lincnr solution. A complementary taxonomy of rational bugs and associated repair techniques is also provided. SPA OK. introduces a new data structure to facilitate debugging [...] the derivation tree of the program. SPADE generalizes recent work in Artificial Intelligence by Suasman and Sacerdoti on automatic programming, and extends The theory of program design developed by the Structured Programming movement. It provides a more structured information processing model of human problem solving than the production systems of Newell and Simon, and articulates the type of problem solving curriculum advocated by Papert's Logo Project. 1. A Multi-Faceted Approach The SPADE theory is being developed in three contexts: 1. Education: an editor called SPADEE- 0 has bern implemented that encourages students to define and debug programs in terms of explicit SPADE design choices, thereby providing a highly structured programming environment. 2. AI: an automatic programmer called PATN has been designed using an augmented transition network embodiment of the SPADE theory. This results in a framework which unifies recent work on planning and debugging by Sacerdoti [75] and Sussrnan [75]. 3. Psychology: a parser called PAZATN has been designed that applies the SPADE theory {{to the analysis of}} programming protocols. PAZATN produces a parse of the protocol that delineates the planning and debugging strategics employed by the problem solver. PAZATN extends the series of automatic <b>protocol</b> <b>analysers</b> develope...|$|R
50|$|Provides {{assessment}} of the visual field where glaucomatous damage is often seen. It compares five corresponding and mirrored areas in the superior and inferior visual fields. The result of either 'Outside Normal Limits' (significant difference in superior and inferior fields), 'Borderline' (suspicious differences) or 'Within Normal Limits' (no differences) is only considered when the patient has, or is a suspect for, glaucoma. This is only available in 30-2 and 24-2 <b>Analyser</b> <b>protocol.</b>|$|R
40|$|Abstract: Three commercially {{available}} bioimpedance spectroscopy analysers were compared for technical performance {{and for their}} design purpose of prediction of body composition. All three analysers were electronically stable, remaining in calibration over a year, and provided highly reproducible (coefficients of variation < 0. 5 %) and accurate (within 0. 5 % of component values) measurements of impedances of a test circuit. Whole body impedances in humans were highly cor-related between all three instruments although significant biases between instruments were observed, particularly for the measurement of intracellular resistance. However, when the measured impedances, and using instrument-specific resist-ivity coefficients, were used to predict fat-free mass of the subjects, the difference between instruments was approxi-mately 1. 7 kg fat-free mass, a value comparable with that observed for the error associated with reference methods such as multi-compartment models of body composition. It is concluded that, with appropriate regard to standardisation of meas-urement <b>protocol,</b> all three <b>analysers</b> are suitable for their design purpose of estimating body composition in humans...|$|R
40|$|AVS plays a {{critical}} role in the diagnostic workup of primary aldosteronism (PAL) as it is the most reliable means of differentiating unilateral forms (e. g. aldosterone-producing adenoma) correctable by unilateral adrenalectomy, from bilateral forms usually treated with aldosterone antagonist medications. Examination of the adrenal/peripheral venous (AV/PV) cortisol ratio permits assessment of the adequacy of AVS. Ratios of 3 indicate adequate sampling. The right adrenal vein (RAV) is often harder to locate than the left (LAV) as it usually is smaller and empties into the inferior vena cava (IVC) rather than the renal vein at a level ranging from upper T 11 to mid L 1. Thus, even in highly experienced hands, the RAV cannulation success rate (87 % at Princess Alexandra Hospital) is lower than that for LAV (94 %). Use of contrast CT prior to AVS has contributed to high success rates achieved in our institutions by permitting visualization of the RAV at its point of entry into the IVC. We recently instituted an on-the-spot method of measuring plasma cortisol that permits determination of AV levels within 12 min of collection. Rapid cortisol estimation was performed by competitive fluorescence polarization assay using a TDx analyser and the TDx reagent system for cortisol. The standard assay for cortisol was modified by reducing the original 16 min incubation time to 6 min by following a test <b>protocol</b> on the <b>analyser</b> originally used for measuring ethosuximide. The requirement for only 50 L sample volumes allowed rapid centrifugation (4 min). Measurement of RAV and simultaneously collected PV cortisol levels was undertaken while the radiologist collected samples from the LAV, resulting in minimal or no prolongation of the AVS procedure. Cortisol levels of 1500 nmol/L could be estimated accurately, permitting reliable assessment of cannulation success provided PV levels were 500 nmol/L (which was almost always the case). This method proved accurate when compared with an established competitive chemiluminescent immunoassay (ADVIA Centaur). This approach offers a means of definitively establishing, at the time of AVS, whether AV cannulation has been successful, and thereby promises {{to reduce the number of}} samples required and the need for repeat procedures...|$|R
40|$|Human {{activity}} in the Peruvian Amazon causes native vegetation fragmentation into smaller units resulting on the increase of agricultural systems. Understanding the level, the structure and the origin of morphologic within and among populations variation is essential for planning better management strategies aimed at sustainable use and conservation of Inga edulis Mart. species. We evaluated the genetic variability in wild and domestic population to unfold cultivation changes over the species genetic resources. We have studied 400 adult trees: 200 cultivated on arable land and 200 wild growing in untouched lowland rain forest. The individuals were randomly selected. Sampling sites were selected and defined {{on the basis of}} the geographical coordinates: longitude, latitude and altitude. Phenotypic variation was monitored using the proposed descriptor of qualitative and quantitative features (e. g., weight of hundred seeds). For each individual a voucher specimen was kept. The total genomic DNA was extracted from young leaves, conserved in silica gel, with INVITEK, Invisorb ®Spin Plant Mini Kit. Samples were then genotyped with five microsatellite (SSR) loci. One locus (Pel 5) was cross-transferred, developed previously for Pithecellobium elegans. The remaining four loci (Inga 03, 05, 08, 33) were previously developed for the species. Polymerase chain reaction (PCR) was made using a Biometra® T 1 Thermocycler using the following profile: 95 °C for 2 min; 95 °C for 15 s, 55 / 59 °C for 30 s, 72 °C for 30 s, 30 cycles; 72 °C for 15 min. The PCR products were fluorescently labelled. The visualization of fragments was carried out according to standard <b>protocols</b> on genetic <b>analyser,</b> ABI PRISM® 310 (Applied Biosystems), using ABI GENESCAN and GENOTYPER software. The phenotypic and genotypic results of wild versus domestic populations are under evaluation to verify if cultivation is altering the allelic variation considering that morphology is considerably changed. Czech Development Cooperation Project entitled “Sustainable use of natural resources in Peruvian Amazon” Project No. 23 /MZe/B/ 07 - 10; The Academy of Science of The Czech Republic and the National Council for Science, Technology and Technological Innovation, Peru, binational project entitled “Morphological and genetic diversity of indigenous tropical trees in the Amazon – model study of Inga edulis Mart. in Peruvian Amazon”; Foundation “Nadace Nadání Josefa, Marie a Zdeňky Hlávkových”, Czech Republi...|$|R

