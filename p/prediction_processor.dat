7|34|Public
40|$|The {{prediction}} of the SRB and ET impact areas requires six separate processors. The SRB impact <b>prediction</b> <b>processor</b> computes the impact areas and related trajectory data for each SRB element. Output from this processor is stored on a secure file accessible by the SRB impact plot processor which generates the required plots. Similarly the ET RTLS impact <b>prediction</b> <b>processor</b> and the ET RTLS impact plot processor generates the ET impact footprints for return-to-launch-site (RTLS) profiles. The ET nominal/AOA/ATO impact <b>prediction</b> <b>processor</b> and the ET nominal/AOA/ATO impact plot processor generate the ET impact footprints for non-RTLS profiles. The SRB and ET impact processors compute {{the size and}} shape of the impact footprints by tabular lookup in a stored footprint dispersion data base. The location of each footprint is determined by simulating a reference trajectory and computing the reference impact point location. To insure consistency among all flight design system (FDS) users, much input required by these processors will be obtained from the FDS master data base...|$|E
40|$|Abstract — The {{architecture}} and VLSI implementation of an epileptic seizure prediction microsystem are presented. The microsystem comprises a neural recording interface and a seizure <b>prediction</b> <b>processor.</b> The two functional blocks have been prototyped in a 0. 35 µm CMOS technology and experimentally characterized. The integrated microsystem is validated {{in predicting the}} onsets of seizures off line in an in vitro epilepsy model of recurrent spontaneous seizures in the hippocampus of mice. I...|$|E
40|$|Speculative {{multithreading}} (SpMT) is {{a dynamic}} program parallelisation technique that promises dramatic speedup of irregular, pointer-based {{programs as well as}} numerical, loop-based programs. We present the design and implementation of software-only SpMT for Java at the virtual machine level. We take the full Java language into account and we are able to run and analyse real world benchmarks in reasonable execution times on commodity multiprocessor hardware. We provide an experimental analysis of benchmark behaviour, uncovered parallelism, the impact of return value <b>prediction,</b> <b>processor</b> scalability, and a breakdown of overhead costs. ...|$|E
50|$|The {{objective}} is to provide more concurrency if extra resources are available. This approach is employed {{in a variety of}} areas, including branch <b>prediction</b> in pipelined <b>processors,</b> value <b>prediction</b> for exploiting value locality, prefetching memory and files, and optimistic concurrency control in database systems.|$|R
30|$|The {{elements}} of f̂_ℓ - 1 (n) are fed into a forward <b>prediction</b> reference-orthogonalization <b>processor</b> (ROP) {{in order to}} predict the {{elements of}} bℓ− 1 (n− 1) and to produce the stage output back prediction error vector bℓ(n). The elements of b̂_ℓ - 1 (n) are similarly fed into a ROP to perform M-channel joint process estimation and to produce the stage output estimation error vector eℓ(n). Subsequently, the elements of b̂_ℓ - 1 (n) are delayed and are also fed into another ROP to obtain the stage output forward prediction error vector fℓ(n).|$|R
50|$|A {{variety of}} {{situations}} {{can cause a}} pipeline stall, including jumps (conditional and unconditional branches) and data cache misses.Some processors have a instruction set architecture with certain features designed to reduce the impact of pipeline stalls -- delay slot, conditional instructions such as FCMOV and branch <b>prediction,</b> etc.Some <b>processors</b> {{spend a lot of}} energy and transistors in the microarchitecture on features designed to reduce the impact of pipeline stalls -- branch prediction and speculative execution, out-of-order execution, etc.Some optimizing compilers try to reduce the impact of pipeline stalls by replacing some jumps with branch-free code, often at the cost of increasing the binary file size.|$|R
40|$|This {{report is}} {{to fulfill the}} partial {{requirement}} for the final project for cse/ese 560 m. <b>Prediction</b> <b>Processor</b> is the final project I have done based on the research interest of applying financial decision making strategies to engineering field. Resource allocation is the special area I would like to explore the application first. Since this is a computer architecture class, a parallel proprietary architecture is proposed and implemented to accommodate the idea. All the components are developed from scratch except the register file and sram modules. Through the software algorithm design and hardware circuit design, a balancing development cycle is adopted to provide scalability, flexibility and performance. The idea o...|$|E
40|$|Accurate {{and timely}} {{prediction}} of weather phenomena, such as hurricanes and flash floods, require high-fidelity compute intensive simulations of multiple finer regions of interest within a coarse simulation domain. Current weather applications execute these nested simulations sequentially using {{all the available}} processors, which is sub-optimal due to their sub-linear scalability. In this work, we present a strategy for parallel execution of multiple nested domain simulations based on partitioning the 2 -D processor grid into disjoint rectangular regions associated with each domain. We propose a novel combination of performance <b>prediction,</b> <b>processor</b> allocation methods and topology-aware mapping of the regions on torus interconnects. Experiments on IBM Blue Gene systems using WRF show that the proposed strategies result in performance improvement of up to 33 % with topology-oblivious mapping and up to additional 7 % with topology-aware mapping over the default sequential strategy...|$|E
40|$|Abstract—Accurate {{and timely}} {{prediction}} of weather phenomena, such as hurricanes and flash floods, require highfidelity compute intensive simulations of multiple finer regions of interest within a coarse simulation domain. Current weather applications execute these nested simulations sequentially using {{all the available}} processors, which is sub-optimal due to their sublinear scalability. In this work, we present a strategy for parallel execution of multiple nested domain simulations based on partitioning the 2 -D processor grid into disjoint rectangular regions associated with each domain. We propose a novel combination of performance <b>prediction,</b> <b>processor</b> allocation methods and topology-aware mapping of the regions on torus interconnects. Experiments on IBM Blue Gene systems using WRF show that the proposed strategies result in performance improvement of up to 33 % with topology-oblivious mapping and up to additional 7 % with topology-aware mapping over the default sequential strategy. Index Terms—weather simulation; performance modeling; processor allocation; topology-aware mapping; I...|$|E
40|$|Abstract—Conventional thermal {{management}} techniques are reactive, {{as they take}} action after temperature reaches a threshold. Such approaches do not always minimize and balance the temperature, and they control temperature at a noticeable performance cost. This paper investigates how to use predictors for forecasting temperature and workload dynamics, and proposes proactive {{thermal management}} techniques for multiprocessor system-on-chips. The predictors we study include autoregressive moving average modeling and lookup tables. We evaluate several reactive and predictive techniques on an UltraSPARC T 1 processor and an architecture-level simulator. Proactive methods achieve significantly better thermal profiles and performance in comparison to reactive policies. Index Terms—Energy management, <b>prediction</b> methods, <b>processor</b> scheduling, temperature control, thermal factors. I...|$|R
50|$|Without branch <b>prediction,</b> the <b>processor</b> {{would have}} to wait until the {{conditional}} jump instruction has passed the execute stage before the next instruction can enter the fetch stage in the pipeline. The branch predictor attempts to avoid this waste of time by trying to guess whether the conditional jump is most likely to be taken or not taken. The branch that is guessed to be the most likely is then fetched and speculatively executed. If it is later detected that the guess was wrong then the speculatively executed or partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.|$|R
40|$|In this paper, {{we examine}} the {{application}} of simple neural processing elements {{to the problem of}} dynamic branch <b>prediction</b> in high-performance <b>processors.</b> A single neural network model is considered: the Perceptron. We demonstrate that a predictor based on the Perceptron can achieve a prediction accuracy in excess of that given by conventional Two-level Adaptive Predictors and suggest that neural predictors merit further investigation...|$|R
40|$|Abstract—Many modern {{applications}} have {{a significant}} operating system (OS) component. The OS execution affects various architectural states, including the dynamic branch predictions, which are widely used in today’s high-performance microprocessor designs to improve performance. This impact tends to become more significant as the designs become more deeply pipelined and more speculative. In this paper, {{we focus on the}} issues of understanding the OS effects on the branch predictions and designing architectural support to alleviate the bottlenecks that are created by misprediction. In this work, we characterize the control flow transfer of several emerging applications on a commercial OS. It was observed that the exception-driven, intermittent invocation of OS code and user/OS branch history interference increased misprediction in both user and kernel code. We propose two simple OS-aware control flow prediction techniques to alleviate the destructive impact of user/OS branch interference. The first consists of capturing separate branch correlation information for user and kernel code. The second involves using separate branch prediction tables for user and kernel code. We demonstrate in this paper that OS-aware branch predictions require minimal hardware modifications and additions. Moreover, the OS-aware branch predictions can be integrated with many existing schemes to further improve their performance. We studied the improvement contributed by OS-aware techniques to various branch prediction schemes ranging from the simple Gshare to the more advanced Agree, Multi-Hybrid, and Bi-Mode predictors. On the 32 K-entry predictors, incorporating the OS-aware techniques yields up to 34 percent, 23 percent, 27 percent, and 9 percent prediction accuracy improvement on the Gshare, Multi-Hybrid, Agree, and Bi-Mode predictors, respectively. Index Terms—Pipeline processors, branch <b>prediction,</b> <b>processor</b> architectures, hardware/software interfaces, computer system implementation, performance of systems...|$|E
40|$|Abstract – Architectural {{simulators}} {{used for}} microprocessor design study and optimization can require large amount computational time and/or resources. In such cases, models {{can be a}} fast alternative to lengthy simulations, and can help reach a designer near-optimal system configuration. However, The nonlinear characteristics of a processor system make the modeling task quite challenging. The models not only need to incorporate the micro-architectural parameters but also the dynamic behavior of programs. This paper presents a hybrid (hardware/software), non-linear model for processors. The model provides accurate <b>predictions</b> of <b>processor</b> throughput {{for a wide range}} of design space. Index Terms – Processor throughput, instructions per cycle (IPC), processor model, micro-architecture simulation, basic blocks. I...|$|R
40|$|In general, prosody {{of speech}} {{contains}} various information. For example, in Japanese, accent information {{is used for}} distinguishing homonyms and identifying word boundaries. In this paper, we propose a combination method of phonetic and prosodic information in speech applications, that is, an input <b>prediction</b> front end <b>processor</b> for dictation. From a few morae inputs, completion candidates that are sorted by input history and by the accent pattern are listed up. We examined two accent usage methods for both registered words and unregistered words and implemented an input prediction system combining a speech recognizer, a prediction server and an accent usage module. 1...|$|R
40|$|A {{thermometer}} that rapidly predicts {{body temperature}} {{based on the}} temperature signals received from a temperature sensing probe when it comes {{into contact with the}} body. The logarithms {{of the differences between the}} temperature signals in a selected time frame are determined. A line is fit through the logarithms and the slope of the line is used as a system time constant in predicting the final temperature of the body. The time constant in conjunction with predetermined additional constants are used to compute the predicted temperature. Data quality in the time frame is monitored and if unacceptable, a different time frame of temperature signals is selected for use in <b>prediction.</b> The <b>processor</b> switches to a monitor mode if data quality over a limited number of time frames is unacceptable. Determining the start time on which the measurement time frame for prediction is based is performed by summing the second derivatives of temperature signals over time frames. When the sum of second derivatives in a particular time frame exceeds a threshold, the start time is established...|$|R
40|$|Without special {{handling}} branch instructions would {{disrupt the}} smooth flow of instructions into the mi-croprocessor pipeline. To eliminate this disruption, many modern systems attempt {{to predict the}} out-come of branch instructions, and use this prediction to fetch, decode and even evaluate future instruc-tions. Recently, researchers have realized that the task of branch <b>prediction</b> for <b>processor</b> optimization {{is similar to the}} task of symbol prediction for data compression. Substantial {{progress has been made in}} developing approximations to asymptotically op-timal compression methods, while respecting the limited resources available within the instruction prefetching phase of the processor pipeline. Not only does the infusion of data compression ideas result in a theoretical fortication of branch pre-diction, it results in real and signicant empirical improvement in performance, as well. We present an overview of branch prediction, beginning with early techniques through more recent data compres-sion inspired schemes. A new approach is described which uses a non-parametric probability density es-timator similar to the LZ 77 compression scheme [23]. Results are presented comparing the branch prediction accuracy of several schemes with those achieved by our new approach. ...|$|R
40|$|Dynamic branch <b>prediction</b> in {{high-performance}} <b>processors</b> is {{a specific}} instance of a general Time Series Prediction problem that occurs {{in many areas of}} science. In contrast, most branch prediction research focuses on Two-Level Adaptive Branch Prediction techniques, a very specific solution to the branch prediction problem. An alternative approach is to look to other application areas and fields for novel solutions to the problem. In this paper, we examine the application of neural networks to dynamic branch prediction. Two neural networks are considered: a Learning Vector Quantisation (LVQ) Network and a Backpropagation Network. We demonstrate that a neural predictor can achieve misprediction rates comparable to conventional Two-level Adaptive Predictors and suggest that neural predictors merit further investigation...|$|R
40|$|Multi-core and many-core {{were already}} major trends {{for the past}} six years and are {{expected}} to continue for the next decade. With these trends of parallel computing, it becomes increasingly difficult to decide on which processor to run a given application, mainly because the programming of these processors has become increasingly challenging. In this work, we present a model to predict the performance of a given application on a multi-core or many-core processor. Since programming these processors can be challenging and time consuming, our model does not require source code to be available for the target processor. This is in contrast to existing performance prediction techniques such as mathematical models and simulators, which require code to be available and optimized for the target architecture. To enable performance prediction prior to algorithm implementation, we classify algorithms using an existing algo- rithm classification. For each class, we create a specific instance of the roofline model, resulting in a new class-specific model. This new model, named the boat hull model, enables performance <b>prediction</b> and <b>processor</b> selection prior to the development of architecture specific code. We demonstrate the boat hull model using GPUs and CPUs as target architectures. We show that performance is accurately predicted for an example real-life applicatio...|$|R
40|$|The pattern {{prediction}} algorithm {{has been}} shown in past research to provide substantial improvements in energy consumption, without sacrificing performance when used to predict access to the filter cache in the instruction memory hierarchy. Since, the pattern predictor and the filter cache are an overhead to the existing cache architecture, it is imperative that they should together not lower the energy efficiency of the existing architecture. Thus, the extra energy consumed by the predictor should be minimal as compared to the overall energy reduction achieved by the introduction of the filter cache in the instruction memory hierarchy. In this paper, we present the hardware architecture of a low-power, single-cycle pattern predictor, which can be used in conjunction with the filter cache. The predictor has been functionally validated against results from simplescalar simulations on the MediaBench benchmark suite. The predictor is capable of performing single cycle <b>predictions</b> for <b>processor</b> clocks of over 0. 5 GHz (in 0. 35 u process technology) and adds an area overhead of a mere 1500 to 2700 gates to the silicon area depending on the configuration chosen. The predictor is area and power efficient, and does not add any significant overheads to the processor's power and area budget. Thus providing an elegant and attractive solution for inclusion in modern embedded processors...|$|R
40|$|High-performance {{multiprocessor}} {{systems are}} built around out-of-order processors with aggressive branch predictors. Despite their relatively high branch <b>prediction</b> accuracies, these <b>processors</b> execute many memory instructions on mispredicted paths. Previous studies {{that focused on}} uniprocessors systems showed that these wrong-path memory references may pollute the caches by bringing in data that are not needed on the correct execution path and by evicting useful data or instructions. Additionally, they may also {{increase the amount of}} cache and memory traffic. On the positive side, however, they may have a prefetching effect for memory references on the correct path. For multiprocessor systems, the performance implications of these wrong-path memory references are more widespread, including, but not limited to, more cache-to-cache transfers, write-backs, and cache block state transitions. In this thesis, we investigated the effects of wrong path memor...|$|R
40|$|Dynamic branch <b>prediction</b> in {{high-performance}} <b>processors</b> is {{a specific}} instance of a general Time Series Prediction problem that occurs {{in many areas of}} science. In contrast, most current branch prediction research focuses on Two-Level Adaptive Branch Prediction techniques, a very specific solution to the branch prediction problem. An alternative approach is to look to other application areas and fields for novel solutions to the problem. In this paper we examine the application of neural networks to dynamic branch prediction. Two neural networks are considered, a Learning Vector Quantisation (LVQ) Network and a Multi-Layer Perceptron Network accomplished by the Backpropagation learning algorithm. We demonstrate that a neural predictor can achieve prediction rates better than conventional twolevel adaptive predictors and therefore suggest that neural predictors are a suitable vehicle for future branch prediction research. 1...|$|R
40|$|Processors are {{becoming}} increasingly complex with many instructions executing in parallel. In order to maintain {{a high rate of}} performance {{there needs to be a}} minimization of dependencies to exploit Instruction Level Parallelism (ILP). However, there are many hazards that a processor must overcome in order to increase its execution rate. In order to overcome this difficulty, data value prediction has been developed. In data value <b>prediction,</b> the <b>processor</b> will try and predict what the outcome of an instruction is before the instruction completes execution. Then the instruction that needs that value can proceed. This is similar to branch prediction, however, in branch prediction there can only be two choices, taken or not taken. When trying to predict data values, there can be many possible outcomes. DVP is best suited for instructions that are repeatable creating constant data values that are computed many times. Most programs are written in this repeatable fashion. This paper will discuss several methods of data value prediction and the experiment will compare these different methods. The experiment uses several SPEC 2000 benchmarks to evaluate the performance of the predictors. The methods discussed in this paper are the last value predictor, the stride predictor, the 2 level predictor, and hybrid predictors. In further study the paper will dive into current research and advanced methods of data prediction. 1...|$|R
40|$|Abstract- Existing power {{management}} techniques operate by reducing performance capacity (frequency, voltage, resource size) when performance demand is low, {{such as at}} idle or similar low activity phases. In the case of multi-core systems, the performance and power demand is the aggregate demand of all cores in the system. Monitoring aggregate demand makes detection of phase changes difficult (active-toidle, idle-to-active, etc.) since aggregate phase behavior obscures the underlying phases generated by the workloads on individual cores. This causes sub-optimal {{power management}} and over-provisioning of power resources. In this paper, we address these problems through core-level, activity prediction. The core-level view makes detection of phase changes more accurate, yielding more opportunities for efficient power management. Due to the difficulty in anticipating activity level changes, existing operating system power management strategies rely on reaction rather than prediction. This causes sub-optimal power and performance since changes in performance capacity by the power manager lag changes in performance demand. To address this problem we propose the Periodic Power Phase Predictor (PPPP). This activity level predictor decreases SYSMark 2007 client/desktop processor power consumption by 5. 4 % and increases performance by 3. 8 % compared to the reactive scheme used in Windows Vista operating system. Applying the predictor to the <b>prediction</b> of <b>processor</b> power, we improve accuracy by 4. 8 % compared to a reactive scheme. Index Terms – Dynamic power management, prediction, multi-core, power modeling...|$|R
40|$|Over the years, grid {{computing}} {{has emerged}} as one of the most viable and scalable alternatives to high performance supercomputing, tapping into computing power of the order of Gigaflops. However, the inherent dynamicity in grid computing has made it extremely difficult to come up with near-optimal solutions to efficiently schedule tasks in grids. The present paper proposes a novel grid-scheduling heuristic that adaptively and dynamically schedules tasks without requiring any prior information on the workload of incoming tasks. The approach models the grid system in the form of a state-transition diagram, employing a prioritized round-robin algorithm with task replication to optimally schedule tasks, using <b>prediction</b> information on <b>processor</b> utilization of individual nodes. Simulations, comparing the proposed approach with the roundrobin heuristic, have shown the given heuristic to be more effective in scheduling tasks as compared to the latter...|$|R
40|$|This report {{describes}} a software suite which {{was developed for}} modeling and performance <b>prediction</b> of advanced <b>processor</b> builds for the submarine acoustic superiority initiative. The software implements mathematically-based models and analysis techniques that facilitate a more thorough understanding of signal processing algorithm functionality, commonality, and performance relative to anticipated threat and noise environments. The entire software suite has been implemented in the MATLAB programming language and includes several graphical user interfaces for selection of signal processing functions and parameter entry. A description of each program is provided, emphasizing user requirements while deferring algorithm details to other associated MITRE reports. i Acknowledgement The {{author would like to}} acknowledge the contributions of Dr. Garry Jacyna and Dr. David Colella in the preparation of this report. ii 1. Introduction The modeling and prediction software described in this re [...] ...|$|R
40|$|Physical {{systems are}} {{inherently}} parallel. Intuition suggests that simulations {{of these systems}} may be amenable to parallel execution. The parallel execution of a discrete-event simulation requires careful synchronization of processes {{in order to ensure}} the execution's correctness; this synchronization can degrade performance. Largely negative results were recently reported in a study which used a well-known synchronization method on queueing network simulations. Discussed here is a synchronization method (appointments), which has proven itself to be effective on simulations of FCFS queueing networks. The key concept behind appointments is the provision of lookahead. Lookahead is a <b>prediction</b> on a <b>processor's</b> future behavior, based on an analysis of the processor's simulation state. It is shown how lookahead can be computed for FCFS queueing network simulations, give performance data that demonstrates the method's effectiveness under moderate to heavy loads, and discuss performance tradeoffs between the quality of lookahead, and the cost of computing lookahead...|$|R
40|$|Original article can {{be found}} at: [URL] Copyright Elsevier B. V. [Full text {{of this paper is}} not {{available}} in the UHRA]Dynamic branch <b>prediction</b> in high-performance <b>processors</b> is a specific instance of a general time series prediction problem that occurs in many areas of science. Most branch prediction research focuses on two-level adaptive branch prediction techniques, a very specific solution to the branch prediction problem. An alternative approach is to look to other application areas and fields for novel solutions to the problem. In this paper, we examine the application of neural networks to dynamic branch prediction. We retain the first level history register of conventional two-level predictors and replace the second level PHT with a neural network. Two neural networks are considered: a learning vector quantisation network and a backpropagation network. We demonstrate that a neural predictor can achieve misprediction rates comparable to conventional two-level adaptive predictors and suggest that neural predictors merit further investigation...|$|R
40|$|Current {{processors}} exploit {{out-of-order execution}} and branch prediction to improve instruction level parallelism. When a branch <b>prediction</b> is wrong, <b>processors</b> flush the pipeline and squash all the speculative work. However, control-flow independent instructions compute {{the same results}} when they re-enter the pipeline down the correct path. If these instructions are not squashed, branch misprediction penalty can significantly be reduced. In this paper we present a novel mechanism that detects control-flow independent instructions, executes them before the branch is resolved, and avoids their re-execution {{in the case of}} a branch misprediction. The mechanism can detect and exploit control-flow independence even for instructions that are far away from the corresponding branch and even out of the instruction window. Performance figures show that the proposed mechanism can exploit control-flow independence for nearly 50 % of the mispredicted branches, which results in a performance improvement that ranges from 14 % to 17, 8 % for realistic configurations of forthcoming microprocessors. 1...|$|R
40|$|Dynamic branch <b>prediction</b> in {{high-performance}} <b>processors</b> is {{a specific}} instance of a general time series prediction problem that occurs {{in many areas of}} science. Most branch prediction research focuses on two-level adaptive branch prediction techniques, a very specific solution to the branch prediction problem. An alternative approach is to look to other application areas and fields for novel solutions to the problem. In this paper, we examine the application of neural networks to dynamic branch prediction. We retain the first level history register of conventional two-level predictors and replace the second level PHT with a neural network. Two neural networks are considered: a learning vector quantisation network and a backpropagation network. We demonstrate that a neural predictor can achieve misprediction rates comparable to conventional two-level adaptive predictors and suggest that neural predictors merit further investigation. Ó 2003 Elsevier B. V. All rights reserved. Keywords: Neural branch prediction; Two-level adaptive branch prediction; Backpropagation network; Learning vector quantisation networ...|$|R
40|$|The {{importance}} of accurate branch <b>prediction</b> to future <b>processors</b> {{has been widely}} recognized. The correct prediction of conditional branch outcomes can help avoid pipeline bubbles and attendant loss in performance. In order to achieve high prediction accuracy, numerous dynamic branch prediction schemes that exploit branch correlation have recently been proposed. Several of the best predictors are the gshare, agree, filter, skewed and bi-mode predictors. However, despite the intensive research work to propose these new schemes, {{there is no direct}} and comprehensive performance comparison among them. Such comparison is essential to guiding the design of future microprocessors. Therefore, in this paper, we conduct extensive experiments to calibrate the performance of each prediction scheme. Furthermore, we discuss the design philosophy and underlying mechanism for these schemes, and contrast their relative advantages and disadvantages. Among the schemes examined, we find that the skewed predictor performs the best for small budgets, while the bi-mode predictor outperforms others for large budgets...|$|R
40|$|Out-of-order {{execution}} processors {{with aggressive}} branch prediction are {{the core of}} current-generation high-performance multiprocessor systems. Despite their relatively high branch <b>prediction</b> accuracies, these <b>processors</b> still execute many memory instructions on the mispredicted path. These wrong-path memory references pollute the caches and {{increase the amount of}} memory traffic, but may also prefetch useful data into the caches. For multiprocessor systems, wrong-path memory references also affect the number of writebacks, cache-to-cache transfers, and cache line state transitions. In this paper, we explore the effects that wrong-path memory references have in multiple chipmultiprocessor systems. Our results show that wrongpath memory references increase the amount of L 1 and L 2 cache traffic by 16 % and 35 %, respectively, and the traffic on the internal and external networks by 36 % and 30 %, respectively. These additional L 1 and L 2 cache replacements increase the number of additional writebacks by 70 %, L 1 copy invalidations up to 68 %, and extra cache line state transitions in the L 1 and L 2 caches by 4 % and 16 %, respectively. ...|$|R
40|$|In this paper, we {{describe}} an active networks architecture for dynamically constructing [clusters of] clusters {{in response to}} user requests for computational service. Active network nodes (i. e. routers) match requests for computational service to offers of such service using recent access and usage pattern information to construct on-demand, wide-area clusters. By combining appropriate offers of service with knowledge of current (as well as <b>predictions</b> of future) <b>processor</b> load and network conditions, the system dynamically constructs clusters to cost-effectively meet user needs. The active networks approach exploits the fact that network devices are in an ideal position to decide which resources should be combined to build clusters in response to particular requests. This approach offers advantages in terms of anonymity of service (systems are unaware of one anothers' existence), scalability (scheduling is distributed across many network processors), fault tolerance (service is distributed so failures are recoverable and/or localized) and automatic localization (clusters are created {{as close to the}} requestor as possible). Keywords Active Networks, Clusters, Computational Grids, High Performance Computing, Resource Discovery and Allocation. ...|$|R
40|$|The {{most common}} {{objective}} function of task scheduling problems is makespan. However, on a computational grid, the 2 nd optimal makespan {{may be much}} longer than the optimal makespan because the computing power of a grid varies over time. So, if the performance measure is makespan, there is no approximation algorithm in general for scheduling onto a grid. In contrast, recently the authors proposed the computing power consumed by a schedule as a criterion of the schedule and, for the criterion, gave (1 +) -approximation algorithm RR for scheduling n independent coarse-grained tasks with the same length onto a grid with m processors. RR does not use any prediction information on the underlying resources. RR is the first approximation algorithm for grid scheduling. However, so far any performance comparison among related heuristic algorithms is not given. This paper shows experimental results on the comparison of the consumed computing power of a schedule among RR and five related algorithms. It turns out that RR is next {{to the best of}} algorithms that need the <b>prediction</b> information on <b>processor</b> speeds and task lengths though RR does not require such information...|$|R
40|$|Recent work in {{the field}} of {{dynamical}} systems provides evidence that computer systems are nonlinear-deterministic dynamical systems. This implies the existence of a deterministic update rule, which, in turn, implies the existence of a deterministic forecasting rule for the state variables of a running computer. Even a short-term prediction of these quantities, if accurate, could be effective in tailoring system resources on-the-fly to the dynamics of a computing application. For example, a good <b>prediction</b> of <b>processor</b> load could allow a computer to increase its energy efficiency by dynamically turning off unused CPUs, and then turning them back on based on the programs predicted needs. To explore this, I use a custom measurement infrastructure, delay-coordinate embedding and nonlinear time-series analysis to forecast processor load and cache performance of a set of simple C programs running on an Intel Core 2 Duo. This proved to be quite effective. However, the use of traditional embedding techniques `on the fly 2 ̆ 7 is impractical due to the time required to correctly perform the processing and post-processing of the data. My alternative to this is to use arbitrary low-dimensional projections. While this is not consistent with the requirements in the current literature, recent work by Mischaikow suggests that this alternative might work. I verified this conjecture, showing that forecasts based on two-dimensional projections are largely as effective as strategies that use the full embedded dynamics. This is in contrast to the current view in the nonlinear dynamics community that a one-to-one delay map is sufficient for successful prediction using delay coordinate embedding. My results suggest that this may not be a necessary condition. The success of the projection-based forecasting schemes brings into questions the need for full topological conjugacy in forecasting schema. The results presented here suggest ways of improving computer design at a systems level; they also provide evidence to support the use of semi-conjugacies in forecasting schemes...|$|R
40|$|This paper {{discusses}} {{an approach}} to reducing memory latency in future systems. It focuses on systems where a single chip DRAM/processor will not be feasible even in 10 years, e. g. systems requiring a large memory and/or many CPU's. In such systems a solution needs to be found to DRAM latency and bandwidth {{as well as to}} inter-chip communication. Utilizing the projected advances in chip I/O bandwidth we propose to implement a decoupled access-execute processor where the access processor is placed in memory. A program is compiled to run as a computational process and several access processes with the latter executing in the DRAM processors. Instruction set extensions are discussed to support this paradigm. Using multi-level branch <b>prediction</b> the access <b>processor</b> stays ahead of the execute processor and keeps the latter supplied with data. The system reduces latency by moving address computation to memory and thus avoiding sending address to memory by the computational processor. This and the fetchahead capabilities of the access processor are combined with multiple DRAM "streaming" to improve performance. DRAM caching is assumed to be used to assist in this as well. This work {{was supported in part by}} the University of Illinois and National Science Foundation under Grant No. US NSF CCR- 9796315. 1...|$|R
