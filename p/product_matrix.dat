183|3374|Public
2500|$|... where [...] {{denotes the}} sum of the {{diagonal}} elements of the outer <b>product</b> <b>matrix,</b> known as its trace.|$|E
2500|$|Iterating {{the cross}} {{product on the}} right is {{equivalent}} to multiplying by the cross <b>product</b> <b>matrix</b> on the left, in particular ...|$|E
2500|$|... where [...] is {{the cross}} <b>product</b> <b>matrix</b> of , [...] is the tensor product and [...] is the {{identity}} matrix; or alternatively as ...|$|E
5000|$|... #Subtitle level 3: Properties of the <b>matrix</b> <b>product</b> (two <b>matrices)</b> ...|$|R
5000|$|... where c is the {{location}} of the center of mass expressed in the body-fixed frame,anddenote skew-symmetric cross <b>product</b> <b>matrices.</b>|$|R
40|$|Through Monte Carlo experiments, {{this paper}} compares the {{performances}} of different gradient optimization algorithms, when performing full information maximum likelihood (FIML) estimation of econometric models. Different matrices are used (Hessian, outer <b>products</b> <b>matrix,</b> GLS-type matrix, {{as well as a}} mixture of them). ...|$|R
2500|$|If u and v {{are both}} nonzero then the outer <b>product</b> <b>matrix</b> uvT always has matrix rank 1, {{as can be}} easily seen by {{multiplying}} it with a vector x: ...|$|E
2500|$|... where [...] is the {{identity}} matrix, {{so as to}} avoid confusion with the inertia matrix, and [...] is the outer <b>product</b> <b>matrix</b> formed from the unit vector [...] along the line.|$|E
2500|$|Build a vector {{the same}} length as R with 1 in each place where the {{corresponding}} number in R is in the outer <b>product</b> <b>matrix</b> (âˆˆ, set inclusion or element of or Epsilon operator), i.e. 0 0 1 0 1 ...|$|E
50|$|PCM Thermocules are finely printed onto flat fabric {{for next}} to skin <b>products.</b> <b>Matrix</b> {{infusion}} coating allows for an unchanged hand, drape, and wick.|$|R
50|$|The <b>matrix</b> <b>product</b> is not {{commutative}} in general, {{although it}} is associative and is distributive over matrix addition. The identity element of the <b>matrix</b> <b>product</b> is the identity matrix (analogous to multiplying numbers by 1), and a square matrix may have an inverse matrix (analogous to the multiplicative inverse of a number). Determinant multiplicativity applies to the <b>matrix</b> <b>product.</b> The <b>matrix</b> <b>product</b> is also important for matrix groups, and the theory of group representations and irreps.|$|R
40|$|Properties of <b>matrix</b> <b>product</b> codes over finite {{commutative}} Frobenius {{rings are}} investigated. The minimum distance of <b>matrix</b> <b>product</b> codes constructed with {{several types of}} matrices is bounded in different ways. The duals of <b>matrix</b> <b>product</b> codes are also explicitly {{described in terms of}} <b>matrix</b> <b>product</b> codes. Comment: Des. Codes Cryptogr. published online: 19 Jul 201...|$|R
2500|$|... where [...] is the {{identity}} matrix, {{so as to}} avoid confusion with the inertia matrix, and [...] is the outer <b>product</b> <b>matrix</b> formed from the unit vector [...] along the line. [...] Recall that skew-symmetric matrix [...] is constructed so that [...] [...] The matrix [...] in this equation subtracts the component of [...] that is parallel to [...]|$|E
5000|$|... {{which is}} the dyadic form the cross <b>product</b> <b>matrix</b> with a column vector.|$|E
50|$|The {{low-dimensional}} embedding {{is finally}} obtained by application of multidimensional scaling on the learned inner <b>product</b> <b>matrix.</b>|$|E
5000|$|It is also {{possible}} to express a <b>matrix</b> <b>product</b> in terms of concatenations of <b>products</b> of <b>matrices</b> and row or column vectors: ...|$|R
50|$|More {{operations}} on square matrices {{can be defined}} using the <b>matrix</b> <b>product,</b> such as powers and nth roots by repeated <b>matrix</b> <b>products,</b> the <b>matrix</b> exponential can be defined by a power series, the matrix logarithm is the inverse of matrix exponentiation, and so on.|$|R
40|$|In this paper, {{we present}} {{the problem of}} allocating {{processors}} for <b>matrix</b> <b>products.</b> First, we consider how many processors should be allocated for computing one <b>matrix</b> <b>product</b> on a parallel system. Then, we discuss how to allocate processors {{for a number of}} independent <b>matrix</b> <b>products</b> on the parallel system. In many cases, it is shown that the performance of parallel algorithms does not improve beyond a certain point even though more processors are allocated for more parallelism. The results from experiments on the Fujitsu AP 1000 parallel system for a <b>matrix</b> <b>product</b> show that allocating more processors is not always beneficial for overall system performance. Also, when evaluating a number of independent <b>matrix</b> <b>products,</b> the concurrent execution of multiple <b>matrix</b> <b>products</b> by partitioning the system is better than the independent evaluation of <b>matrix</b> <b>products</b> sequentially by parallelizing each <b>matrix</b> <b>product.</b> Finally, we conclude that such kind of result can be applicable to many proces [...] ...|$|R
5000|$|... where tr {{denotes the}} sum of the {{diagonal}} elements of the outer <b>product</b> <b>matrix,</b> known as its trace.|$|E
5000|$|... where [...] is the {{identity}} matrix and [...] is the dual 2-form of [...] or [...] cross <b>product</b> <b>matrix,</b> ...|$|E
50|$|Also notice, thatwhere tr {{denotes the}} sum of the {{diagonal}} elements of the outer <b>product</b> <b>matrix,</b> known as its trace.|$|E
40|$|The {{generalized}} multilinear {{model with}} the matrix-T error distribution is introduced in this paper. The sum of squares and <b>products</b> (SSP) <b>matrix,</b> as a counterpart of the Wishart matrix for the multinormal model, and the regression matrix for the errors and the observed {{as well as}} future responses are defined. The distributions of the regression matrix {{as well as the}} SSP matrix, and the prediction distribution of the future regression matrix and the future SSP matrix are derived. generalized multilinear model matrix-T distribution sum of squares and <b>products</b> <b>matrix</b> regression matrix prediction distribution Wishart distribution invariant differentials generalized beta and gamma distributions...|$|R
2500|$|The {{determinant}} of a <b>matrix</b> <b>product</b> of square <b>matrices</b> equals the <b>product</b> of their determinants: ...|$|R
5000|$|The <b>matrix</b> <b>product</b> of {{a column}} and a row vector gives the dyadic product of two vectors a and b, {{an example of}} the more general tensor <b>product.</b> The <b>matrix</b> <b>product</b> of the column vector {{representation}} of a and the row vector representation of b gives the components of their dyadic product, ...|$|R
5000|$|Iterating {{the cross}} {{product on the}} right is {{equivalent}} to multiplying by the cross <b>product</b> <b>matrix</b> on the left, in particular ...|$|E
5000|$|... where [...] is the -th {{column vector}} of the <b>product</b> <b>matrix</b> [...] and [...] is the -th column vector {{of the matrix}} [...]|$|E
5000|$|... where I is the {{identity}} matrix and S&thinsp;S is the outer <b>product</b> <b>matrix</b> formed from the unit vector S {{along the line}} L.|$|E
40|$|Transfer <b>matrices</b> and <b>matrix</b> <b>product</b> {{operators}} play a ubiquitous role in {{the field}} of many-body physics. This review gives an idiosyncratic overview of applications, exact results, and computational aspects of diagonalizing transfer <b>matrices</b> and <b>matrix</b> <b>product</b> operators. The results in this paper are a mixture of classic results, presented {{from the point of view}} of tensor networks, and new results. Topics discussed are exact solutions of transfer matrices in equilibrium and nonequilibrium statistical physics, tensor network states, <b>matrix</b> <b>product</b> operator algebras, and numerical <b>matrix</b> <b>product</b> state methods for finding extremal eigenvectors of <b>matrix</b> <b>product</b> operators...|$|R
5000|$|The {{spectral}} matrix, , can {{be written}} as the <b>matrix</b> <b>product</b> of a <b>matrix,</b> , and its transpose. That is, ...|$|R
50|$|Or {{more simply}} as a <b>product</b> of <b>matrices.</b>|$|R
5000|$|Here Ip denotes a p&times;p {{identity}} matrix. After multiplication {{with the}} matrix L the Schur complement {{appears in the}} upper p&times;p block. The <b>product</b> <b>matrix</b> is ...|$|E
5000|$|This can {{be written}} more {{concisely}} as where [...] is the cross <b>product</b> <b>matrix</b> of , [...] is the tensor product and [...] is the identity matrix; or alternatively as ...|$|E
5000|$|If u and v {{are both}} nonzero then the outer <b>product</b> <b>matrix</b> uvT always has matrix rank 1, {{as can be}} easily seen by {{multiplying}} it with a vector x: ...|$|E
40|$|A highly regular {{parallel}} multiplier architecture {{along with}} the novel low-power, high-performance CMOS implementation circuits is presented. The superiority is achieved through utilizing a unique scheme for recursive decomposition of partial <b>product</b> <b>matrices</b> and a recently proposed non-binary arithmetic logic {{as well as the}} complementary shift switch logic circuits...|$|R
40|$|Rates of {{increase}} in the number of parameters of a Fourier factor demand system that imply asymptotically normal elasticity estimates are characterized. The main technical problem in achieving this characterization is caused by the fact that the eigenvalues of the sample sum of squares and cross <b>products</b> <b>matrix</b> of the generalized least squares estimator are not bounded from below. This problem is addressed by establishing a uniform strong law with rate for the eigenvalues of this matrix so as to relate them to the eigenvalues of the expected sum of squares and cross <b>products</b> <b>matrix.</b> Because the minimum eigenvalue of the latter matrix considered as a function of the number of parameters decreases faster than any polynomial, the rate at which parameters may increase is slower than any fractional power of the sample size...|$|R
30|$|The {{operator}} âŠ— {{denotes the}} Kronecker <b>product</b> between <b>matrices.</b>|$|R
