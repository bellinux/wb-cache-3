18|1097|Public
50|$|Storing {{objects in}} a space-partitioning data {{structure}} (k-d tree or BSP tree for example) {{makes it easy}} and fast to perform certain kinds of geometry queries—for example in determining whether a ray intersects an object, space partitioning can {{reduce the number of}} intersection test to just a few per <b>primary</b> <b>ray,</b> yielding a logarithmic time complexity with respect to the number of polygons.|$|E
5000|$|The seven rays antific, philosophical, or practical. Students {{of these}} schools of thought think that {{the primary purpose of}} White Magic is the {{evolution}} of culture through the energy of Love-Wisdom (The second ray--the <b>primary</b> <b>ray</b> of which the other six rays are secondary expressions, because the second ray of Love-Wisdom is the ray of the Solar Logos, the governing deity of the Solar System.) ...|$|E
50|$|In the runoff, Nagin {{kept the}} base he had {{established}} in the primary, but expanded his support to win every majority-white precinct. Pennington maintained his base among lower and middle-class black voters, {{but was unable to}} pick up the support of many voters who had supported candidates defeated in the <b>primary.</b> <b>Ray</b> Nagin’s runoff victory sent him to city hall, where he has served as New Orleans’s mayor since May 2002.|$|E
5000|$|... a non-recursive {{ray tracing}} {{rendering}} algorithm that only casts <b>primary</b> <b>rays,</b> or ...|$|R
40|$|The high multiseriate <b>primary</b> <b>rays</b> {{that occur}} in young stems of Fagus silvatica L. undergo {{splitting}} during cambium development. Segments of the split <b>primary</b> <b>rays</b> remain {{close to each other}} and form characteristic strands. Within these strands, sections can be observed in which the rays are split in the same direction, either in configuration Z or S. The configuration of other cambial cellular events neighboring the <b>primary</b> <b>rays</b> under consideration is in agreement with the configuration of the splitting of these rays. The arrangement of the rays {{on the surface of the}} wood after debarking a young stem makes it possible to decipher the domain pattern on a surface of any given size. As the stem segment ages, the borders between domains migrate and new borders are formed, changing their course from longitudinal to transverse...|$|R
25|$|Cosmic <b>rays</b> {{originate}} as <b>primary</b> cosmic <b>rays,</b> {{which are}} those originally produced in various astrophysical processes. <b>Primary</b> cosmic <b>rays</b> are composed primarily of protons and alpha particles (99%), {{with a small}} amount of heavier nuclei (~1%) and an extremely minute proportion of positrons and antiprotons. Secondary cosmic rays, caused by a decay of <b>primary</b> cosmic <b>rays</b> as they impact an atmosphere, include neutrons, pions, positrons, and muons. Of these four, the latter three were first detected in cosmic rays.|$|R
40|$|FIGURE 3. Spicules of Doconesthes dustinchiversi n. sp. A. Diactine lateral prostalium with {{magnified}} shagreen surface. B. Dermalia {{of various}} forms with magnified ray tip of diactin. C. Atrialia with magnified pinular ray tip. D. Pentactine hypodermalia with very short and longer proximal ray, with magnified tangential ray tip. E. Diactine hypodermalium with magnified ray end. F. Choanosomal diactin with magnified ray tip. G. Strobiloplumicome with magnified <b>primary</b> <b>ray</b> tuft and secondary ray tips. H. Oxyhexactin. I. Oxypentactin. J. Hemioxyhexaster with magnified ray tips...|$|E
40|$|In {{this paper}} {{we present a}} new method for {{accelerating}} ray tracing of scenes containing NURBS (Non Uniform Rational B-Spline) surfaces by exploiting the GPU’s fast z-buffer rasterization for regular triangle meshes. In combination with a lightweight, memory efficient data organization this allows for fast calculation of <b>primary</b> <b>ray</b> intersections using a Newton Iteration based approach executed on the CPU. Since all employed shaders are kept simple the algorithm can profit from older graphics hardware as well. We investigate two different approaches, one initiating ray-surface intersections by referencing the surface through its child-triangles. The second approach references the surface directly and additionally delivers initial guesses, required for the Newton Iteration, using graphics hardware vector interpolation capabilities. Our approaches achieve a rendering acceleration of up to 95 % for primary rays compared to full CPU ray tracing without compromising image quality...|$|E
40|$|Systems {{of bright}} rays are {{exhibited}} by many fresh craters {{on the moon}} and Mercury. Diameter/density distributions suggest that lunar-rayed craters represent the Class 1 craters, and that Mercurian rayed craters represent post-Caloris craters. Photogeological analyses of lunar imagery indicate that the ray systems are composed of finely divided material from the primary crater along with locally derived ejecta from secondary and tertiary craters. The <b>primary</b> <b>ray</b> material probably occurs in moderately thick (0. 1 - 1 meter) deposits. The rate of darkening may depend more on the thickness of the ray material than on the rates of various darkening processes. Darkening rate may also be a function of crater size. It is observed that rays of craters more than 1 b. y. old remain bright, whereas those older than Class 1 generally fade to imperceptibility...|$|E
40|$|Ray tracing and {{rasterization}} {{have long}} been considered as two very different approaches to rendering images of 3 D scenes that – while computing the same results for <b>primary</b> <b>rays</b> – lie {{at opposite ends of}} a spectrum. While rasterization first projects every triangle onto the image plane and enumerates all covered pixels in 2 D, ray tracing operates in 3 D by generating rays through every pixel and then finding the first intersection with a triangle. In this paper we show that, by making a slight change that extends triangle edge functions to operate in 3 D instead of 2 D, the two approaches become almost identical with respect to <b>primary</b> <b>rays,</b> resulting in an efficient rasterization technique. We then use this similarity to transfer rendering concepts between the two domains. We generalize rasterization to arbitrary non-planar perspectives as known from ray tracing, while keeping all benefits from rasterization. In the reverse we transfer the concepts of rendering consistency, which have not been available for ray tracing thus far. We then demonstrate that the only remaining difference between rasterization and <b>ray</b> tracing of <b>primary</b> <b>rays</b> is scene traversal. We discuss a number of approaches from the continuum made accessible by 3 D rasterization...|$|R
3000|$|... {{for this}} TEW method {{do not contain}} any <b>primary</b> gamma <b>ray</b> counts. With a {{pixelated}} solid-state detector, however, {{a certain number of}} <b>primary</b> gamma <b>rays</b> are detected as lower energy gamma rays due to incomplete charge collection and inter-pixel scatter [4 – 6]. Therefore, E [...]...|$|R
40|$|We {{introduce}} a practical antialiasing approach for interactive ray tracing and path tracing. Our method {{is inspired by}} the Subpixel Reconstruction Antialiasing (SRAA) method which separates the shading from visibility and geometry sampling to produce antialiased images at reduced cost. While SRAA is designed for GPU-based deferred shading renderer, we extend the concept to ray-tracing based applications. We take a hybrid rendering approach in which we add a GPU rasterization step to produce the depth and normal buffers with subpixel resolution. By utilizing those extra buffers, {{we are able to}} produce antialiased ray traced images without incurring performance penalty of tracing additional <b>primary</b> <b>rays.</b> Furthermore, we go beyond the <b>primary</b> <b>rays</b> and achieve antialiasing for shadow rays and reflective rays as well. Keywords: antialiasing, ray tracing, path tracing. ...|$|R
40|$|We present {{fast ray}} tracing of dynamic scenes {{in this paper}} with primary and shadow rays. We present a GPUfriendly {{strategy}} to bring coherency to shadow rays, based on previous work on grids as acceleration structures. We introduce indirect mapping of threads to rays to improve the performance of ray tracing on the GPU for the traversal and intersection steps. We also construct a light frustum in a spherical space for shadow rays. A grid structureisconstructedeachframeforthelightfrustumandtraversedcoherently. Thisinvolvescarefulmappingof the <b>primary</b> <b>ray</b> information to the light space and balancing the work load of the threads. Using the finegrained parallelism of GPU, we reorder the shadow rays to make them coherent and process multiple thread blocks to each cell to balance the work load. Spherical mapping is key to handling light sources placed anywhere in the scenebyreducingthetrianglecountandimprovingperformanceinshadowchecking. Inadditionitalsoallowsus to introduce spotlights in raytracing. In practice, we attain interactive performance for moderately large models whichchange dynamically inthe scene...|$|E
40|$|We {{introduce}} {{a new approach to}} two important problems in ray tracing: antialiasing and distributed light sources. For antialiasing, adaptive supersampling in object space (ASOS) combines the quality of supersampling with the speed of adaptive supersampling. In adaptive supersampling, the decision to partition a ray is taken in image-space, which means that small or thin objects may be missed entirely. This is particularly problematic in an animation, where the intensity of such objects may appear to vary. ASOS is based on testing the proximity of a ray to the boundary of an object. If a <b>primary</b> <b>ray</b> is close to the boundary, it splits into 4 subrays, and the procedure continues recursively with each subray. This splitting continues until the estimated error in pixel intensity is sufficiently small. ASOS also computes shadows from distributed light sources to any required precision. Our implementation includes spheres, polygons, disks, boxes, cones and cylinders and does not preclude other primitives...|$|E
40|$|We {{describe}} {{a collection of}} algorithms, visualizations, and interactive operations that allow operators controlling the movement {{of a collection of}} sensors through an environment to monitor in real time the portion of the environment that can or cannot be seen by some subset of the sensors. The visualization allows preattentive detection of the number of sensors that can see a given location, and the coloring allows the exact identities of the sensors involved to be identified. Two <b>primary</b> <b>ray</b> tracing based algorithms are described. A GPU implementation using CUDA is used that allows all scene updates and sensor movements to be processed and reflected in the display in real time. Sensors and sensor placement {{play a central role in}} military as well as several other general security applications. Such sensors play a critical role in applications related to military situational awareness in combat zones and unfriendly urban areas. Other examples include security cameras for homes and businesses [Garaas 2011], motio...|$|E
40|$|FIGURE 12. Foa nivosa, BPBM 39658, 21 mm SL, cleared, stained for {{bone and}} counter stained for cartilage, semi-diagrammatic views. A. Last four dorsal spines and {{supporting}} elements. B. Caudal fin bone and cartilage elements, angled arrows show insertion of procurrent rays to straight arrow at insertion of <b>primary</b> <b>rays...</b>|$|R
40|$|The Parallel Virtual Machine (PVM) tool {{has been}} used for a {{distributed}} implementation of Greg Ward’s Radiance code. In order to generate exactly the same <b>primary</b> <b>rays</b> with both the sequential and the parallel codes, the quincunx sampling technique used in Radiance for the reduction of the number of <b>primary</b> <b>rays</b> by interpolation, must be left untouched in the parallel implementation. The octree of local ambient values used in Radiance for the indirect illumination has been shared among all the processors. Both static and dynamic image partitioning techniques which replicate the octree of the complete scene in all the processors and have load-balancing, have been developed for one frame rendering. Speedups larger than 7. 5 have been achieved in a network of 8 workstations. For animation sequences, a new dynamic partitioning distribution technique with superlinear speedups has also been developed...|$|R
25|$|Reaction {{products}} of <b>primary</b> cosmic <b>rays,</b> radioisotope half-lifetime, and production reaction.|$|R
40|$|An {{algorithm}} is presented {{that may be}} used to simulate energy flows within an environment. These flows are supposed to be represented by energy quanta that travel in straight lines within the environment. The Constructive Solid Geometry methodology is used to model the environment. Properties can be attached to any object in this environment, either to the inside or to the boundary. An application in the field of computer graphics is described. INTRODUCTION The ray tracing algorithm (Whitted 1980) is now a widely used algorithm in computer graphics. Basically, it works as follows: for each point in an screen (a pixel), it constructs a ray starting from the eye and passing through that point. This ray, called a <b>primary</b> <b>ray</b> is intersected with the objects in the environment and the nearest intersection found is retained. From that intersection point, others rays are traced to light sources to compute shadows: these rays are so called shadow rays. Other rays are traced in directions co [...] ...|$|E
40|$|Alto, CA) {{treatment}} planning system (TPS), {{to accurately}} {{account for the}} presence of inhomogeneities in simple geometries is examined. The goal of 2 % accuracy, as set out by the American Association of Physicists in Medicine Task Group 65, serves as a useful benchmark against which to evaluate the inhomogeneity correc-tion capabilities of this treatment planning algorithm. A planar geometry phantom consisting of upper and lower layers of Solid Water (Gammex rmi, Middleton, WI) separated by a heterogeneity region of variable thickness, is modeled within the Eclipse TPS. Results obtained with the AAA are compared with experimental measurements. Seven different materials, spanning the range from air to alumi-num, constitute the inhomogeneity layer. In general, the AAA overpredicts dose beyond low-density regions and underpredicts dose distal to volumes of high den-sity. In many cases, the deviation between the AAA and experimental results exceeds the Task Group 65 target of 2 %. The source of these deviations appears to arise from an inability of the AAA to correctly account for altered attenuation along <b>primary</b> <b>ray</b> paths. PACS number: 87. 53. T...|$|E
40|$|Compressional primary seismic nonzero offset {{reflections}} are {{the most}} essential wavefield attributes used in seismic parameter estimation and imaging. We show how the determination of angle-dependent reflection coefficients can be addressed from identifying such events for arbitrarily curved three-dimensional (3 -D) subsurface reflectors below a laterally inhomogeneous layered overburden. More explicitly, we show how the geometrical-spreading factor along a reflected <b>primary</b> <b>ray</b> with offset can be calculated from the identified (i. e., picked) traveltimes of offset primary reflections. Seismic traces in which all primary reflections are corrected with the geometrical-spreading factor are, as is well-known, referred to as true-amplitude traces. They can be constructed without any knowledge of the velocity distribution in the earth model. Apart from possibly finding a direct application in an amplitude-versus-offset (AVO) analysis, the theory developed here can be of use to derive true-amplitude time- and depth-migration methods for various seismic data acquisition configurations, which pursue the aim of performing the wavefield migration (based upon {{the use of a}} macro-velocity model) and the AVO analysis in one step...|$|E
5000|$|... #Subtitle level 2: The Isotopic Composition of <b>Primary</b> Cosmic <b>Rays</b> Experiment ...|$|R
30|$|FiveEW is a {{modified}} TEW (TEWDR)-based scatter and crosstalk correction method designed for simultaneous Tc- 99 m and I- 123 imaging. TEWDR uses the detector response for the <b>primary</b> gamma <b>rays</b> and estimates the counts {{corresponding to the}} <b>primary</b> gamma <b>rays</b> in sub-windows to avoid overcorrection. It increased the counts for the scatter-corrected main window over 10  % compared to TEW (Table  4). The effect of using detector responses for the <b>primary</b> gamma <b>rays</b> in the FiveEW method was better for the I- 123 images than the Tc- 99 m ones (Tables  3 and 5). This is because the mean detector responses for the <b>primary</b> gamma <b>rays</b> or <b>primary</b> counts in the sub-windows for I- 123 were greater than those for Tc- 99 m because the sub-windows for I- 123 were narrower and nearer to the photopeak than those for Tc- 99 m (Fig.  1).|$|R
5000|$|V-2 Cloud-Chamber Observation of a Multiply Charged <b>Primary</b> Cosmic <b>Ray,</b> Physical Review 75,524 (1949) http://prola.aps.org/abstract/PR/v75/i3/p524_1 ...|$|R
40|$|Ray tracing is a {{graphical}} {{technique that}} provides realistic simulation of light sources and complex lighting effects within three-dimensional scenes, {{but it is}} a time-consuming process that requires a tremendous amount of compute power. In order {{to reduce the number of}} calculations required to render an image, many different algorithms and techniques have been developed. One such development is the use of tree-like data structures to partition space for quick traversal when finding intersection points between rays and primitives. Even with this technique, ray-primitive intersection for large datasets is still the bottleneck for ray tracing. This thesis proposes the use of a specific spatial data structure, the K-D tree, for faster ray casting of primary rays and enables a ray-triangle culling technique that compliments view frustum and backface culling. The proposed method traverses the entire tree structure to mark nodes to be inactive if it is outside of the view frustum and skipped if the triangle is a backface. In addition, a ray frustum is calculated to test the spatial coherency of the <b>primary</b> <b>ray.</b> The combination of these optimizations reduces the average number of intersection tests per ray from 98 % to 99 %, depending on the data size...|$|E
40|$|Ray tracing is {{a method}} for {{producing}} photorealistic 3 D computer generated imagery by modeling the interaction of light rays with a scene. Because each <b>primary</b> <b>ray</b> is independent of other primary rays being modeled, ray tracing offers massive degrees of parallelism that is suitable to parallel architectures like GPUs, multicore CPUs, and distributed computing environments. Our goal is to implement a ray tracer running on multicore CPUs and NVIDIA GeForce 8800 GPUs in multiple systems in a cluster that exchange scene data and rendered images to distribute the computational load. A dynamic scheduling algorithm assigns work units to each node to provide high sustained performance that scales with additional processors and GPUs. 2 Implementation Methodology All elements of this project were written in ANSI/ISO C++ except the GPU renderer which was written in NVIDIA’s CUDA, a C-like language that exposes the parallel execution model of the GPU. 2. 1 Distributed Computing Our parallelization strategy is multi-tiered. When a scene is rendered, it is first decomposed into blocks of pixels that will be distributed among the compute nodes using MPI. Each node receives the block of the scene and begins rendering {{using a combination of}} host threads and GPU threads. When a nod...|$|E
40|$|It {{has become}} common practice, when {{considering}} a plane parallel structure for seismic modelling purposes, {{to treat the}} primary VPS and PP reflected arrivals in a similar fashion. For many of the aspects of these investigations the two arrivals may be studied using the same procedures. However, a major area of difference, more crucial in a model {{with a large number}} of layers, is the dynamics or amplitude properties of the VPS arrival. In a many layered medium there is only one ray in the PP case, having two P ray segments in each layer for a given source-receiver offset. In the VPS case there is the so called <b>primary</b> <b>ray</b> composed of P ray segments down to the reflector and VS ray segments up to the receiver. This arrival has only one conversion, the reflected conversion at the deepest interface. In addition, {{there are a number of}} other multiplied converted waves, each with one P and one SV ray segment per layer, which have identical kinematic properties (travel times) as the primary VPS arrival and may also have different conversion points on the reflecting interface. Due to the increase in the number of conversions in these other rays their amplitudes will be substantially less than that of the primary. However, as the number of layers increases, the number of these extra rays increases significantly. This effect is what will be considered here...|$|E
25|$|Satellite {{experiments}} have {{found evidence of}} positrons and a few antiprotons in <b>primary</b> cosmic <b>rays,</b> amounting to less than 1% of the particles in <b>primary</b> cosmic <b>rays.</b> These {{do not appear to}} be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe. Rather, they appear to consist of only these two elementary particles, newly made in energetic processes.|$|R
40|$|The {{determination}} of the <b>primary</b> cosmic <b>ray</b> mass composition from the characteristics of extensive air showers (EAS), obtained at an observation level in {{the lower half of}} the atmosphere, is still an open problem. In this work we propose a new method of Multiparametric Topological Analysis and show its applicability for the {{determination of}} the mass composition of the <b>primary</b> cosmic <b>rays</b> at the PeV energy region...|$|R
50|$|Satellite {{experiments}} have {{found evidence of}} positrons and a few antiprotons in <b>primary</b> cosmic <b>rays,</b> amounting to less than 1% of the particles in <b>primary</b> cosmic <b>rays.</b> These {{do not appear to}} be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe. Rather, they appear to consist of only these two elementary particles, newly made in energetic processes.|$|R
40|$|We {{present an}} {{efficient}} method {{to calculate the}} primary and scattered x-ray photon fluence component of a mammographic image. This {{can be used for}} a range of clinically important purposes, including estimation of breast density, personalized image display, and quantitative mammogram analysis. The method is based on models of: the x-ray tube; the digital detector; and a novel ray tracer which models the diverging beam emanating from the focal spot. The tube model includes consideration of the anode heel effect, and empirical corrections for wear and manufacturing tolerances. The detector model is empirical, being based on a family of transfer functions that cover the range of beam qualities and compressed breast thicknesses which are encountered clinically. The scatter estimation utilizes optimal information sampling and interpolation (to yield a clinical usable computation time) of scatter calculated using fundamental physics relations. A scatter kernel arising around each <b>primary</b> <b>ray</b> is calculated, and these are summed by superposition to form the scatter image. Beam quality, spatial position in the field (in particular that arising at the air-boundary due to the depletion of scatter contribution from the surroundings), and the possible presence of a grid, are considered, as is tissue composition using an iterative refinement procedure. We present numerous validation results that use a purpose designed tissue equivalent step wedge phantom. The average differences between actual acquisitions and modelled pixel intensities observed across the adipose to fibroglandular attenuation range vary between 5 % and 7 %, depending on beam quality and, for a single beam quality are 2. 09 % and 3. 36 % respectively with and without a grid...|$|E
40|$|In {{real-time}} computer graphics applications, {{at least three}} major forces are in conflict, namely, rendering speed, the level of realism, and interactivity. Therefore this thesis focuses on real-time algorithms that enhance the realism while maintaining high rendering speed, and on fast intersection test methods in order to speed up, for example, collision detection, picking processes and ray tracing. The first part on real-time algorithms consists of five papers, where the first introduces a new algorithm for enlarging and diminishing images. It can be used in, for example, real-time three dimensional games and interactive image processing programs. The remaining four papers use radiosity, soft shadows, planar reflections and environment mapping, respectively, {{in order to increase}} the level of realism in {{real-time computer}} graphics applications. The radiosity paper describes how to compute textures containing illumination, and how these together with hardware texture mapping can reconstruct the radiosity function, and thereby achieving real-time performance. Besides improving the realism of a rendered scene, shadows and reflections are used (by the viewer) as cues to determine spatial relationships between objects. Some techniques for gaining performance are described for an existing soft shadow algorithm, which is followed by a short note on how to implement real-time planar reflections. The first part is then ended with a report on a new environment mapping method, which provides a better approximation of reflections in curved surfaces. The second part on intersection test methods starts with a paper that derives a new method for speeding up the <b>primary</b> <b>ray</b> intersections in a ray tracing program. The second paper presents a fast method for computing the intersection between a ray and a triangle, which is followed by a paper on determining whether two triangles overlap. The methods of the last two papers have applications in ray tracing, global illumination, picking processes, and in collision detection. The majority of the algorithms and methods presented in this thesis have been implemented and used in commercial software. Categories and Subject Descriptors (as by ACM CR) : I. 3 [Computer Graphics]: Three-Dimensional Graphics and Realism - color, shading, shadowing, and texture; Computational Geometry and Object Modeling - Geometric algorithms, languages, and systems. General Terms: Algorithms. Additional Key Words and Phrases: ray tracing, radiosity, virtual reality...|$|E
40|$|The exam {{will cover}} {{material}} discussed {{in class and}} in the readings since the first exam. Topics will include 3 D viewing, lighting and shading, hidden surface removal, texture mapping, ray tracing, polygonal modeling, and modeling with curves and surfaces. Example short answer questions: 1. In the z-buffer algorithm, is the order of rendering for polygons important? Why or why not? 2. Define linear interpolation. In the z-buffer algorithm, is linear interpolation an exact reflection of reality or an approximation? Why? 3. Define back-face culling. Why is this not a general-purpose solution to the hidden surface problem? 4. What is ambient illumination? What does it approximate from physical reality? 5. Which term in the Phong illumination model does NOT use the material (color) properties of the object being illuminated? Why? 6. Why is texture mapping so important for real-time rendering? 7. Describe the process of determining a pixel’s color when rendering a polygon with a texture map. 8. Define the following terms from ray-tracing: <b>primary</b> <b>ray,</b> shadow ray, reflection ray. 9. Describe the algorithm used {{to determine whether a}} ray intersects a polygon. 10. Why are parametric equations often used to represent curves? 11. How are Bézier and Hermite curves related? What property do Bézier curves have due {{to the use of the}} constant 3 in this relationship? 12. Describe a basic algorithm for rendering parametric cubic curves. 13. Explain the concept of a blending function. How does one obtain the blending functions for a family of curves? 14. Why is the Q(t) = TMG form used instead of the more compact Q(t) = TC form for parametric curves? 15. Why is it possible to build a 3 D polygonal model without using the process of 3 D polygonal modeling? 16. Describe one method to manage the speed-realism tradeoff using level of detail (LOD). Example problems: 1. Given a View Plane Normal, VPN = [4, 0,- 7] and a View Up Vector, Vup = [2, 0, 0], calculate the u, v, and n vectors that define the x, y, and z axes of the Viewing Referenc...|$|E
40|$|International audienceDirect Cherenkov light {{emitted by}} <b>primary</b> cosmic <b>ray</b> {{particles}} {{prior to their}} first interaction in the atmosphere has been detected by the H. E. S. S. Imaging Atmospheric Cherenkov Telescopes. Thus, an energy spectrum for iron nuclei in the energy range 50 - 200 TeV has been derived. This paper discusses the advantage of combining this novel technique with a classic extensive air shower array {{in order to test}} the different criteria used for the identification of <b>primary</b> cosmic <b>ray</b> particles (muon and electron abundances, hadron content and lateral profiles). Such a hybrid experiment will offer a unique opportunity to approach the elemental composition around the knee of the <b>primary</b> cosmic <b>ray</b> energy spectrum when validated criteria are extended beyond the present direct Cherenkov energy window...|$|R
5000|$|<b>Primary</b> cosmic <b>ray</b> energy spectra and {{elemental}} composition (abundances of the elements) at energies 1015-1018eV (so called knee energy region) ...|$|R
40|$|In this 2001 {{status report}} of the MACRO experiment, results are {{presented}} on atmospheric neutrinos and neutrino oscillations, high energy neutrino astronomy, searches for WIMPs, search for low energy stellar gravitational collapse neutrinos, stringent upper limits on GUT magnetic monopoles, nuclearites and lightly ionizing particles, high energy downgoing muons, <b>primary</b> cosmic <b>ray</b> composition and shadowing of <b>primary</b> cosmic <b>rays</b> by the Moon and the Sun. Comment: 35 pages, 23 EPS figures included with epsfi...|$|R
