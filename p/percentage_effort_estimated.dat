0|1022|Public
50|$|There {{are many}} {{psychological}} factors potentially explaining the strong tendency towards over-optimistic <b>effort</b> <b>estimates</b> {{that need to}} be dealt with to increase accuracy of <b>effort</b> <b>estimates.</b> These factors are essential even when using formal estimation models, because much of the input to these models is judgment-based. Factors that have been demonstrated to be important are: Wishful thinking, anchoring, planning fallacy and cognitive dissonance. A discussion on these and other factors can be found in work by Jørgensen and Grimstad.|$|R
40|$|The review {{presented}} in this paper examines the evidence on use of expert judgment, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgment-based <b>effort</b> <b>estimates</b> was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgment-based <b>effort</b> <b>estimates</b> were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated <b>effort</b> <b>estimates</b> based on a combination of expert judgment and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other estimation methods. 1...|$|R
40|$|We {{introduce}} a portfolio management method that uses <b>effort</b> <b>estimates</b> to build sets of feasible deadlines for software projects at the bidding stage. <b>Effort</b> <b>estimates</b> can involve considerable error, and {{this must be}} taken into account when selecting deadlines. We show how a simple probability model can allow for possible errors. The model is built using a single <b>effort</b> <b>estimate</b> for each current project, together with historical data on <b>estimated</b> and actual <b>effort</b> for former projects. The probability model is used in two ways: firstly to find the probability of successfully meeting a set of proposed deadlines; and secondly to select deadlines that deliver a fixed probability of success. Rather than treating projects in isolation, we work with the full company portfolio, enabling setbacks in some projects to be balanced by gain in others. Our method is implemented with demonstrations in the tool PROJMAN. Key Words: effort estimation; project management; regression model; risk analysis; risk management...|$|R
50|$|Typically, <b>effort</b> <b>estimates</b> are over-optimistic {{and there}} is a strong over-confidence in their accuracy. The mean effort overrun seems to be about 30% and not {{decreasing}} over time. For a review of effort estimation error surveys, see. However, the measurement of estimation error is problematic, see Assessing the accuracy of estimates.The strong overconfidence in the accuracy of the <b>effort</b> <b>estimates</b> is illustrated by the finding that, on average, if a software professional is 90% confident or “almost sure” to include the actual effort in a minimum-maximum interval, the observed frequency of including the actual effort is only 60-70%.|$|R
40|$|Abstract — In this paper, {{we present}} an {{overview}} of existing size and <b>effort</b> <b>estimates</b> for software. All these estimates are described more or less on their own. Size & effort estimation is a very popular task. We also explain the fundamentals of size & effort estimation. We describe today’s approaches for size & effort estimation. From the broad variety of size & effort estimation models {{that have been developed}} we will compare the most important ones. Their strengths and weaknesses are also investigated. It turns out that the behavior of the size & <b>effort</b> <b>estimates</b> is much more similar as to be expected. Index Terms — COCOMO-II, Project planning and Software size & Estimation. I...|$|R
40|$|This paper {{presents}} {{a framework for}} the representation of uncertainty in the estimates for software design projects for use throughout the entire project lifecycle. The framework is flexible {{in order to accommodate}} uncertainty in the project and utilises Monte Carlo simulation to compute the propagation of uncertainty in <b>effort</b> <b>estimates</b> towards the total project uncertainty and therefore gives a project manager the means to make informed decisions throughout the project life. The framework also provides a mechanism for accumulating project knowledge {{through the use of a}} historical database, allowing <b>effort</b> <b>estimates</b> to be informed by, or indeed based upon, the outcome of previous projects. Initial results using simulated data are presented and avenues for further work are discussed...|$|R
40|$|Context: Early size {{predictions}} (ESP) {{can lead}} to errors in effort predictions for software projects. This problem is particular acute in parametric effort models that give extra weight to size factors (for example, the COCOMO model assumes that effort is exponentially proportional to project size). Objective: Are <b>effort</b> <b>estimates</b> crippled by bad ESP? Method: Document inaccuracies in early size estimates. Using those error sizes to determine the implications of those inaccuracies via an Monte Carlo perturbation analysis of effort models and {{an analysis of the}} equations used in those effort models. Results: While many projects have errors in ESP of up to +/- 100 %, those errors add very little to the overall <b>effort</b> <b>estimate</b> error. Specifically, we find no statistically significant difference in the estimation errors seen after increasing ESP errors from 0 to +/- 100 %. An analysis of effort estimation models explains why this is so: the net additional impact of ESP error is relatively small compared to the other sources of error associated within estimation models. Conclusion: ESP errors effect <b>effort</b> <b>estimates</b> by a relatively minor amount. As soon as a model uses a size estimate and other factors to predict project effort, then ESP errors are not crippling to the process of estimation. Comment: 15 pages. Submitted to IST journa...|$|R
40|$|This study investigates to {{what extent}} Web effort {{estimation}} models built using cross-company data sets can provide suitable <b>effort</b> <b>estimates</b> for Web projects belonging to another company, when compared to Web <b>effort</b> <b>estimates</b> obtained using that company’s own data on their past projects (single-company data set). It extends a previous study (S 3) where these same research questions were investigated using data on 67 Web projects from the Tukutuku database. Since S 3 was carried out, data on other 128 Web projects was added to Tukutuku; therefore this study uses the entire set of 195 projects from the Tukutuku database, which now also includes new data from other single-company data sets. Predictions between cross-company and single-company models are compared using Manual Stepwise Regression+Linear Regression and Case-Based Reasoning. In addition, we also investigated {{to what extent}} applying a filtering mechanism to cross-company datasets prior to building prediction models can affect {{the accuracy of the}} <b>effort</b> <b>estimates</b> they provide. The present study corroborates the conclusions of S 3 since the cross-company models provided much worse predictions than the single-company models. Moreover, the use of the filtering mechanism significantly improved the prediction accuracy of cross-company models when estimating single-company projects, making it comparable to that using single-company datasets...|$|R
50|$|As a group, {{empirical}} models work {{by collecting}} software project data (for example, effort and size)and fitting a curve to the data. Future <b>effort</b> <b>estimates</b> {{are made by}} providing size and calculating the associated effort using the equation which fit the original data (usually with some error).|$|R
5000|$|Post-Disaster Management Suite - A web-based {{software}} platform for planning, tracking and coordinating post-disaster recovery and reconstruction activities. It captures critical {{information such as}} affected locations and populations, emergency response <b>efforts,</b> <b>estimates</b> of damages, losses and needs and then matches this information with disaster recovery projects.|$|R
40|$|Abstract — Function Point Analysis {{is widely}} used, {{especially}} {{to quantify the}} size of applications {{in the early stages}} of development, when <b>effort</b> <b>estimates</b> are needed. However, the measurement process is often too long or too expensive, or it requires more knowledge than available when development <b>effort</b> <b>estimates</b> are due. To overcome these problems, simplified methods have been proposed to measure Function Points. We used simplified methods for sizing both “traditional ” and Real-Time applications, with the aim of evaluating the accuracy of the sizing with respect to full-fledged Function Point Analysis. To this end, a set of projects, which had already been measured by means of Function Point Analysis, have been measured using a few simplified processes, including those proposed by NESMA, the Early&Quick Function Points, the ISBSG average weights, and others; the resulting size measures were then compared. W...|$|R
25|$|The {{rinderpest}} eradication <b>effort</b> is <b>estimated</b> to {{have cost}} $5 billion.|$|R
50|$|Software {{development}} effort estimation {{is the process}} of predicting the most realistic amount of effort (expressed in terms of person-hours or money) required to develop or maintain software based on incomplete, uncertain and noisy input. <b>Effort</b> <b>estimates</b> may be used as input to project plans, iteration plans, budgets, investment analyses, pricing processes and bidding rounds.|$|R
40|$|We propose an {{approach}} for the ab initio calculation of materials with strong electronic correlations {{which is based}} on all local (fully irreducible) vertex corrections beyond the bare Coulomb interaction. It includes the so-called GW and dynamical mean field theory and important non-local correlations beyond, with a computational <b>effort</b> <b>estimated</b> to be still manageable. Comment: 8 pages, 6 figure...|$|R
40|$|Abstract: Previous {{studies show}} that {{software}} development projects strongly underestimate the uncertainty of their <b>effort</b> <b>estimates.</b> This overconfidence in estimation accuracy may lead to poor project planning and execution. In this paper, we investigate whether the use of estimation error information from previous projects improves the realism of uncertainty assessments. As far as we know, {{there have been no}} empirical software studies on this topic before. Nineteen realistically composed estimation teams provided minimummaximum effort intervals for the same software project. Ten of the teams (Group A) received no instructions about how to complete the uncertainty assessment process. The remaining nine teams (Group B) were instructed to apply a history-based uncertainty assessment process. The main results is that software professionals seem to willing to consider the error of previous <b>effort</b> <b>estimates</b> as relevant information when assessing the minimum effort of a new project, but not so much when assessing the maximum effort!...|$|R
40|$|Context: Software {{projects}} frequently incur {{schedule and}} budget overruns. Planning and estimation are particularlychallenging in large and globally distributed projects. While software engineering researchers have beeninvestigating effort estimation {{for many years}} to help practitioners to improve their estimation processes, there is littleresearch about effort estimation in large-scale distributed agile projects. Objective: The main objective of this paper is three-fold: i) to identify how effort estimation is carried out in largescaledistributed agile projects; ii) to analyze the accuracy of the effort estimation processes in large-scale distributedagile projects; and iii) to identify the factors that impact the accuracy of <b>effort</b> <b>estimates</b> in large-scale distributed agileprojects. Method: We performed an exploratory longitudinal case study. The data collection was operationalized througharchival research and semi-structured interviews. Results: The main findings of this study are: 1) underestimation is the dominant trend in the studied case, 2) reestimationat the analysis stage improves the accuracy of the <b>effort</b> <b>estimates,</b> 3) requirements with large size/scopeincur larger effort overruns, 4) immature teams incur larger effort overruns, 5) requirements developed in multi-sitesettings incur larger effort overruns as compared to requirements developed in a collocated setting, and 6) requirementspriorities impact the accuracy of the <b>effort</b> <b>estimates.</b> Conclusion: <b>Effort</b> estimation is carried out at quotation and analysis stages in the studied case. It is a challengingtask involving coordination amongst many different stakeholders. Furthermore, lack of details and changes in requirements,immaturity of the newly on-boarded teams and the challenges associated with the large-scale add complexitiesin the effort estimation process...|$|R
40|$|Should we {{be using}} {{relative}} estimation methods in software effort estimation? This thesis looks into {{an aspect of}} agile estimating by comparing the effects of using relative estimation methods with absolute estimation methods for software development <b>effort</b> <b>estimates.</b> The thesis describes a study conducted on a software development project where estimates in story points (relative) and ideal time (absolute) are provided using planning poker...|$|R
50|$|One {{of the key}} {{advantages}} to this model is the simplicity with which it is calibrated. Most software organizations, regardless of maturity level can easily collect size, effort and duration (time) for past projects. Process Productivity, being exponential in nature is typically converted to a linear productivity index an organization can use to track their own changes in productivity and apply in future <b>effort</b> <b>estimates.</b>|$|R
40|$|Context: Software eﬀort {{estimation}} (SEE) plays a {{key role}} in predicting the eﬀort needed to complete software development task. However, the conclusion instability across learners has aﬀected the implementation of SEE models. This instability can be attributed to the lack of an eﬀort classiﬁcation benchmark that software researchers and practitioners can use to facilitate and interpret prediction results. Objective: To ameliorate the conclusion instability challenge by introducing a classiﬁcation and self-guided interpretation scheme for SEE. Method: We ﬁrst used the density quantile function to discretise the eﬀort recorded in 14 datasets into three classes (high, low and moderate) and built regression models for these datasets. The results of the regression models were an <b>eﬀort</b> <b>estimate,</b> termed output 1, which was then classiﬁed into an eﬀort class, termed output 2. We refertothe models generated inthis study as duplex output models as they return twooutputs. Theintroduced duplex output models trained with the leave-one-out cross validation and evaluated with MAE, BMMRE and adjusted R 2, can be used to predict both the software eﬀort and the class of software <b>eﬀort</b> <b>estimate.</b> Robust statistical tests (Welch's t-test and Kruskal-Wallis H-test) were used to examine the statistical signiﬁcant differences in the models’ prediction performances. Results: Weobserved the following: (1) the duplex output models not only predicted the <b>eﬀort</b> <b>estimates,</b> they also oﬀeredaguidetointerpretingtheeﬀortexpended; (2) incorporatingthegeneticsearch algorithmintothe duplex output model allowed the sampling of relevant features for improved prediction accuracy; and (3) ElasticNet, a hybrid regression, provided superior prediction accuracy over the ATLM, the state-of-the-art baseline regression. Conclusion: The results show that the duplex output model provides a self-guided benchmark for interpreting <b>estimated</b> software <b>eﬀort.</b> ElasticNet can also serve as a baseline model for SEE...|$|R
25|$|In {{addition}} to Idaho mine owners, powerful and wealthy industrialists outside of Idaho were also tapped {{in an effort}} to destroy the Western Federation of Miners. Donations for the prosecutorial <b>effort</b> <b>estimated</b> in the range of $75,000 to $100,000 were simultaneously solicited and forwarded from the Colorado Mine Owners' Association and other wealthy Colorado donors. Mining interests in other states – Nevada and Utah, for example – were approached as well.|$|R
40|$|Estimation is an {{important}} part of software engineering projects, and the ability to produce accurate <b>effort</b> <b>estimates</b> has an impact on key economic processes, including budgeting and bid proposals and deciding the execution boundaries of the project. Work in this paper explores the interrelationship among different dimensions of software projects, namely, project size, effort, and effort influencing factors. The study aims at providing better <b>effort</b> <b>estimate</b> on the parameters of modified COCOMO along with the detailed use of binary genetic algorithm as a novel optimization algorithm. Significance of 15 cost drivers can be shown by their impact on MMRE of efforts on original 63 NASA datasets. Proposed method is producing tuned values of the cost drivers, which are effective enough to improve the productivity of the projects. Prediction at different levels of MRE for each project reflects the percentage of projects with desired accuracy. Furthermore, this model is validated on two different datasets which represents better estimation accuracy as compared to the COCOMO 81 based NASA 63 and NASA 93 datasets...|$|R
40|$|Abstract:- <b>Estimating</b> <b>effort</b> is an {{important}} component of planning software engineering tasks. Preventive incremental integration software testing is one of such tasks. In the moment when this testing begins software design already exists and can be used in <b>effort</b> <b>estimating.</b> Seven design metrics already proposed in the literature on software engineering have been selected in this paper in order to analyze their applicability to <b>estimating</b> <b>effort</b> of incremental integration testing. A number of programs has been developed to collect the data needed for this analysis. In addition, the conditions under which these data have been collected are shown. Based upon the data, the metrics have been analyzed by using the correlation between each of the metrics and the actual effort spent on this testing. The results point to best metrics to be used for the estimation purpose...|$|R
40|$|Two {{survey methods}} (postcard and interview) for {{generating}} marine fish catch and <b>effort</b> <b>estimates</b> for private boats were field tested at Oceanside, California during May and June of 1974. Sampling days were pre-assigned to weekday and weekend strata. The postcard survey {{was shown to}} produce biased estimates. Causes of the various biases are discussed. The interview survey provided background data to test for biases in the postcard survey and between marina an launch ramp interview areas. (31 pp. ...|$|R
50|$|LeanCMMI {{requires}} an enduring virtual organization approach that levels the effort across all stakeholders with a five percent re-direct <b>effort</b> <b>estimated</b> {{for the duration}} of the program. Organized into virtual teams called Special Interest Groups, or SIGs, these teams are Encapsulated Process Objects that own responsibility for the process, its data, maintenance, training, and communications. An enduring virtual Software Engineering Process Group, or SEPG, serves as the Process Owner, and provides oversight, direction, and authority for the program.|$|R
40|$|Function Point Analysis (FPA) {{is widely}} used, {{especially}} {{to quantify the}} size of applications {{in the early stages}} of development, when <b>effort</b> <b>estimates</b> are needed. However, the measurement process is often too long or too expensive, or it requires more knowledge than available when development <b>effort</b> <b>estimates</b> are due. To overcome these problems, early size estimation methods have been proposed, to get approximate estimates of Function Point (FP) measures. In general, early estimation methods (EEM's) adopt measurement processes that are simplified with respect to the standard process, in that one or more phases are skipped. EEM's are considered effective; however there is little evidence of the actual savings that they can guarantee. To this end, it is necessary to know the relative cost of each phase of the standard FP measurement process. This paper presents the results of a survey concerning the relative cost of the phases of the standard FP measurement process. It will be possible to use data provided in the paper to assess the expected savings that can be achieved by performing an early estimation of FP size, instead of properly measuring it...|$|R
40|$|AbstractWe {{analyzed}} {{data from}} 145 maintenance and development projects managed {{by a single}} outsourcing company, including <b>effort</b> and duration <b>estimates,</b> <b>effort</b> and duration actuals, and function points counts. The estimates were made {{as part of the}} company’s standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that <b>effort</b> <b>estimates</b> chosen as a basis for project budgets were, in general, reasonably good, with 63 % of the estimates being within 25 % of the actual value, and an average absolute error of 0. 26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed {{for the fact that the}} model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers...|$|R
40|$|Despite the {{predictive}} performance of Analogy-Based Estimation (ABE) in generating better <b>effort</b> <b>estimates,</b> {{there is no}} consensus on how to predict the best number of analogies, and which adjustment technique produces better estimates. This paper proposes a new adjusted ABE model based on optimizing and approximating complex relationships between features and reflects that approximation on the final estimate. The results show that {{the predictive}} performance of ABE has noticeably been improved, and the number of analogies was remarkably variable for each test project...|$|R
40|$|This study {{sought to}} improve the {{baseline}} knowledge of the fisheries of Lake Nasser and to make recommendations for the improved management of the fisheries, including stock assessment. The study included the review of key literature, visits to fisheries infrastructure and fishing camps, and individual consultations with the key stakeholders by means of semi-structured interviews, {{as well as a}} collective stakeholder consultation workshop. A preliminary stock assessment was also undertaken using the most recent time series of catch and <b>effort</b> <b>estimates...</b>|$|R
40|$|Abstract-Software {{development}} efforts estimation {{is the process}} of predicting the most realistic use of effort required to develop or maintain software based on incomplete, uncertain and/or noisy input. <b>Effort</b> <b>estimates</b> may be used as input to project plans, iteration plans, budgets, investment analyses, pricing processes and bidding rounds. In this paper, analysis are performed on data to show the parameters which has maximum influence on the productivity. The model presented in this paper can be applied in any organization to calculate the influence of parameters on productivity...|$|R
40|$|Abstract: Software {{development}} <b>effort</b> <b>estimates</b> {{are frequently}} too low, which {{may lead to}} poor project plans and project failures. One reason for this bias {{seems to be that}} the <b>effort</b> <b>estimates</b> produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire {{a better understanding of the}} underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: Self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i. e., with stronger emphasis connectedness, social context and relationships. We propose that this connection may be enabled by an activation of one’s self-construal when engaging in effort estimation, and, a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses. ...|$|R
30|$|The {{considerable}} number of delayed projects reported in literature indicates that practitioners have fallen short of providing accurate and reliable <b>effort</b> <b>estimates</b> in both collocated and globally distributed projects. To better understand these challenges, two studies related to effort estimation in GSE were carried out (Britto et al. 2014; Britto et al. 2015). These two studies among other findings confirmed the results reported by others, e.g. (Šmite et al. 2014), i.e. {{that there is a}} lack of a common terminology in GSE, which makes it hard to compare and synthesize results across studies.|$|R
40|$|Intelligent {{software}} estimation {{models are}} {{need of the}} time. With increased development of Bayesian networks for software project management, one requires an explicit Bayesian Network (BN) to provide <b>effort</b> <b>estimates</b> based on historical data. This paper proposes a simple BN, based on classification approach. However the classes of ranges of size value, are distributed with help of fuzzification to distribute the probability of crisp value The model is simple and smaller, thus can easily be connected to static as well as dynamic Bayesian Networks. General Terms Software effort estimation...|$|R
5000|$|The [...] "authorization imperative" [...] {{offers another}} {{possible}} explanation: much of project planning {{takes place in}} a context which requires financial approval to proceed with the project, and the planner often has a stake in getting the project approved. This dynamic may lead to a tendency {{on the part of the}} planner to deliberately underestimate the project effort required. It is easier to get forgiveness (for overruns) than permission (to commence the project if a realistic <b>effort</b> <b>estimate</b> were provided.) Such deliberate underestimation has been named by Jones and Euske [...] "strategic misrepresentation".|$|R
40|$|The {{aim of the}} {{research}} described in this thesis is to find out the reasons affecting the reliability of project estimates in Sri Lankan offshore software development organizations, and to propose corrective measures to mitigate those issues. A case study was conducted to identify the issues faced during the estimation process and the case study was based on one of the leading offshore software development organizations in Sri Lanka. The findings of this research demonstrate that there exists number of shortcomings in the current estimation process. The recommendations made {{as a result of this}} research study indicate a need for much emphasis on current estimation process for the Sri Lankan offshore software development organizations. Keywords: Software <b>effort</b> <b>estimates,</b> Expert estimates, Offshore Software Development, Estimation process, Factors affecting reliability of estimates, Estimation accuracy...|$|R
40|$|Accurate cost {{estimation}} of software projects isone {{of the most}} desired capabilities in softwaredevelopment Process. Accurate cost estimates not only helpthe customer make successful investments but also assistthe software project manager in coming up with appropriateplans for the project and making reasonable decisionsduring the project execution. Although there have beenreports that software maintenance accounts for themajority of the software total cost, the software estimationresearch has focused considerably on new development andmuch less on maintenance. Now if we talk about real timesoftware system(RTSS) development {{cost estimation}} andmaintenance cost estimation is not much differ from simplesoftware but some critical factor are considered for RTSSdevelopment and maintenance like response time ofsoftware for input and processing time to give correctoutput. As like simple software maintenance cost estimationexisting models (i. e. Modified COCOMO-II) can be usedbut after inclusion of some critical parameters related toRTSS. A Hypothetical Expert input and an industry data set ofeighty completed software maintenance projects were usedto build the model for RTSS maintenance cost. The fullmodel, which was derived through the Bayesian analysis,yields <b>effort</b> <b>estimates</b> within 30 % of the actual 51 % ofthe time,outperforming the original COCOMO II modelwhen {{it was used to}} estimate theseprojects by 34 %. Further performance improvement was obtained whencalibrating the full model to each individual program,generating <b>effort</b> <b>estimates</b> within 30 % of the actual 80 %of the time...|$|R
40|$|Context: Software {{development}} <b>effort</b> <b>estimates</b> {{are often}} inaccurate, and this inaccuracy cause {{problems for the}} clients {{as well as the}} providers. Consequently, we need more knowledge about the estimation processes, so that we can improve them. Objective: This study investigates how initial judgment-based estimation of work effort in software development affects subsequent, unrelated estimation work. Method: Fifty-six software professionals from the same company were allocated randomly to two groups. One group estimated the most likely effort required to complete a small software development task, while the other group <b>estimated</b> the <b>effort</b> required to complete a large task. After that, all the subjects <b>estimated</b> the <b>effort</b> required to complete the same medium-sized task. We replicated the experiment in another company (with 17 software professionals). Results: We found that sequence effects may have a strong impact on judgment-based <b>effort</b> <b>estimates.</b> Both in the first experiment and in the replication, the subsequent estimates were assimilated towards the subjects’ initial estimate, i. e., the group that began with a small task supplied, on average, lower estimates of the medium-sized task than the group that began with the large task. Conclusion: Our findings suggest that knowledge about sequence effects may be important in order to improve estimation processes. However, currently we have a quite incomplete understanding of how, when and how much sequence effects affect effort estimation. Consequently, further research is needed...|$|R
