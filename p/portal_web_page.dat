2|10000|Public
40|$|Abstract. The paper {{describes}} a methodological approach {{to design and}} develop a semantic portal by retrieving unstructured information from different Web sites on the Net. Such a method {{is applied to the}} International Space Station (ISS) knowledge domain, but it proves to be quite general and domain independent. The development of a domain specific ontology, on which the semantic portal is based, allows to annotate and classify the unstructured information available in heterogeneous formats (natural language texts, photos, videos,) on different Web sites, the annotated information becoming instances of the ontology’s classes and attributes. The ontology has been developed using Ontoedit. The Portal has a client-server architecture based on the middleware layer of Ontobroker. Queries from Web pages are formulated using FLORID, a Frame Logic syntax based language. PHP language is in charge to manage user requests from the <b>Portal</b> <b>Web</b> <b>page</b> and the replies of Ontobroker server. 1...|$|E
40|$|This study {{investigates the}} eye {{movement}} patterns of users viewing a <b>portal</b> <b>web</b> <b>page.</b> Previous {{research has shown}} that users scan a portal page in a top-to-bottom manner by row. In this study, the saliency of one of the portal channel titles was manipulated to see what impact this would have on the users ’ scan pattern. The saliency of the channel title was manipulated in two different page locations by modifying the color the text. Results indicate that channel location was found to be a stronger determinant of where users fixate and that the saliency of the channel title is a secondary determinant. Overall, users first fixated the portal page in the top, center channel regardless of the channel title color. Subsequent eye movements appear to be impacted by the salient title, but only when it was located {{on the left side of}} the page. Implications of these results to portal webpage design are discussed. The use of web-based portals has become more prevalent in recent years given the extensive use of websites like Yahoo. com and iGoogle. com and a shift toward centralizing enterprise information into corporate intranet sites. Generally, web portals are classified into two groups: public and private portals. Public portals like iGoogle offer services like e-mail, weather, news, games, etc. to all interne...|$|E
5000|$|... 3LHD's {{projects}} have been presented in reputable international and Croatian magazines and journals, such as Japanese A+U, British The Architectural Review, German DB, Italian The Plan, Croatian Oris; and also in many architectural <b>portals</b> and <b>web</b> <b>pages.</b>|$|R
40|$|The aim of {{this work}} is the design and {{construction}} a new <b>web</b> <b>pages</b> with administration interface for civic association Bá-Bi. It deals {{with the lack of}} internet advertising of this civic association and provides a proposal for a new <b>web</b> <b>pages</b> that should serve as an advertisement for association, but also as an information <b>portal</b> for <b>web</b> <b>pages</b> visitors...|$|R
40|$|In {{interface}} development, it {{is crucial}} to reflect the users' expectations and mental models. By meeting users' expectations, errors can be prevented and the efficiency of the interaction can be enhanced. Applying these guidelines to website development reveals the need to know where users expect to find the most common web objects like the search field, home button or the navigation. In a preliminary online study with 136 participants, the most common web objects were identified for three <b>web</b> <b>page</b> types: online shops, news <b>portals,</b> and company <b>web</b> <b>pages.</b> These objects were used for the main study, which was conducted with 516 participants. In an online application, prototypical websites had to be constructed by the participants. Data analysis showed that Internet users have distinct mental models for different <b>web</b> <b>page</b> types (online shop, news <b>portal,</b> and company <b>web</b> <b>page).</b> Users generally agree about the locations of many, but not all, web objects. These mental models are robust to demographic factors like gender and web expertise. This knowledge could be used to improve the perception and usability of website...|$|R
40|$|Nowadays <b>web</b> <b>portals</b> contain {{large amount}} of {{information}} that is meant for various visitors or groups of visitors. To effectively navigate within the content the website needs to “know ” its users {{in order to provide}} personalized content to them. We propose a method for automatic estimation of the user’s interest in a <b>web</b> <b>page</b> he visits. This estimation is used for the recommendation of <b>web</b> <b>portal</b> <b>pages</b> (through presenting adaptive links) that the user might like. We conducted series of experiments in the domain of our faculty <b>web</b> <b>portal</b> to evaluate proposed approach. Categories and Subject Descriptors H. 5. 4 [Information Interfaces and Presentation (e. g., HCI) ]...|$|R
40|$|This {{dissertation}} {{thesis is}} focused at opportunities {{in real estate}} industry in connection with using internet based portal for real estate agencies. The thesis is based on Čestmír Kadlec's dissertation thesis which presents a methodology for creating real estate agencies portal. The main aim {{of this paper is}} to specify a concept of real estate agencies <b>portal</b> <b>web</b> <b>pages</b> according to methodology developed by Kadlec and evaluate required investment into development and operation the real estate agencies internet based portal. Firstly, theoretical part deals with information society, real estate market in the Czech Republic and how the evolution of information society influenced Czech real estate market in the past. Also the thesis contains description of real estate agency portals in the Czech Republic, their typology and a selection of a <b>web</b> <b>portal</b> for further analysis. The analysis of Sreality. cz contains a description of key processes and also description and fees of provided services. Practical part is focused at specification of real estate agencies <b>web</b> <b>portal</b> according to Kadlec's methodology which can be found in his dissertation thesis. Firstly, there is a description of real estate agencies portal as a system. Then there is an assessment how the proposed solutions meets the criteria specified by Kadlec, which the "ideal" <b>web</b> <b>portal</b> must have. In conclusion, the business case was prepared to Compaq the costs of development and operation of the portal and potential revenues according to competition market analysis. The outcome of the whole thesis is a recommendation whether the investment in the portal makes business sense...|$|R
40|$|Information {{is the key}} asset of all {{organizations}} and can exist in many forms. It can be printed or written on paper, stored electronically, transmitted by mail or by electronic means, shown in films, or spoken in conversation. In today's competitive business environment, such information is constantly under threat from many sources, which can be internal, external, accidental, or malicious. Joomla is a very popular Content Management System (CMS) used for <b>web</b> <b>page</b> maintenance. This highly versatile software has found itself in both large corporate <b>web</b> <b>portals,</b> and simple <b>web</b> <b>pages</b> such as blogs. Such popularity increases its vulnerability to potential attacks and therefore needs an appropriate security management. ISO (the International Organization for Standardization) and IEC (the International Electrotechnical Commission) created the series of standards aimed at providing a model for establishing, implementing, operating, monitoring, reviewing, maintaining and improving an Information Security Management System (ISMS). This paper shows how principles set in ISO/IEC 27000 series of standards {{can be used to}} improve security of Joomla based <b>web</b> <b>portals...</b>|$|R
5000|$|A captive <b>portal</b> is a <b>web</b> <b>page</b> {{which is}} {{displayed}} to newly connected users {{before they are}} granted broader access to network resources. Captive portals are commonly used to present a landing or log-in page which may require authentication, payment, acceptance of EULA/accepted use policies, or other valid credentials that both the host and user agree to adhere by. Captive portals are used for {{a broad range of}} mobile and pedestrian broadband services - including cable and commercially provided Wi-Fi and home hotspots. A captive portal {{can also be used to}} provide access to enterprise or residential wired networks, such as: apartment houses, hotel rooms, and business centers ...|$|R
5000|$|Glenn Beck's {{viewpoint}} about early 20th century progressivism {{is greatly}} influenced by Ronald J. Pestritto, who holds a PhD. in Government from Claremont Graduate University, and currently teaches at Hillsdale College. R. J. Pestritto has been so influential in this respect, that the GlennBeck.com <b>web</b> <b>portal's</b> <b>page</b> for [...] "American Progressivism" [...] not only uses Pestritto's teachings, but links directly {{to one of his}} books. Professor Pestritto wrote an article on the Wall Street Journal detailing [...] "Glenn Beck, Progressives and Me". [...] As noted on The New York Times, when Glenn was on his Fox News show, Professor Pestritto was a regular guest.|$|R
40|$|The {{aim of this}} diploma {{thesis is}} to create derivative-focused <b>web</b> <b>pages</b> as a new part {{of a high school}} {{mathematics}} educational <b>web</b> <b>portal.</b> The <b>web</b> <b>portal</b> is operated by the Department of Mathematics Education, Faculty of Mathematics and Physics, Charles University in Prague. It is primarily intended as an additional learning resource for high school students. The diploma thesis consists of two main parts. The first part describes a search for and evaluation of similar <b>web</b> <b>pages</b> in Czech, Slovak, English and Hebrew, which deal with the topic of derivative. The second part details the creation of new <b>web</b> <b>pages.</b> In creating the <b>web</b> <b>pages</b> the author integrated interactive elements based on JavaScript, and interactive applets created with use of GeoGebra software. To display mathematical expressions on the web, the author used the technology MathJax, which works with languages of the typesetting systems TeX and LaTeX...|$|R
40|$|EDUX (EDucation with an aUthoring tool using XML) is an {{e-learning}} system {{using the}} intelligent hypertexts language; Extensible Markup Language (XML). EDUX {{enable students to}} follow courses at their own pace, based on their ability. EDUX is designed as an adaptable e-learning environment, which can {{meet the needs of}} the individual student. It has the intelligent agent features where for each and every student the system will generate different type of layout or presentation, based on their understanding on the subject or course. The learning process comprises of two main components: an instructor component and a student component. Instructors will be provided with a tool that has functionality for the creation of course. Students will also have a tool with three integrated functionalities for the collaborative curriculum. EDUX acts as a <b>portal</b> with customize <b>web</b> <b>page</b> for the instructors and students...|$|R
40|$|The use of {{dynamically}} generated Web {{content is}} gaining in popularity over traditional static HTML content. Dynamic Web content is generated {{on the fly}} according to the instructions embedded in HTML script files. Such instructions generate HTML by carrying out various forms of tasks such as session tracking, database queries and transactions, dynamic image generation, or even fetching information from remote servers. Popular <b>portal</b> and e-commerce <b>Web</b> <b>pages</b> contain a number of tasks that are executed in a serial manner by traditional multithreaded Web servers. In this ongoing work, we propose a parallel approach for dynamic content generation, and elaborate on how it affects the design and performance of Web servers. We have developed a prototype Web server that supports the parallel processing of tasks involved in the dynamic content generation with improved throughput and response time {{as compared to the}} classical (serial) approach...|$|R
40|$|Application {{integration}} via <b>web</b> <b>portals</b> is {{the most}} widely used and least expensive means for integrating enterprise applications and services. Component-based portals enable the composition of <b>web</b> <b>pages</b> from reusable <b>portal</b> components, where each component represents an independent application or service. This integration is often limited to components displayed on the same <b>web</b> <b>page,</b> to local deployed components, and to homogeneous environments. We describe a component model for enhanced integration of <b>portal</b> components in <b>web</b> <b>portals.</b> Our model supports not only the aggregation of components within one <b>web</b> <b>page</b> but also the composition of component navigation into a central navigation area, the communication between local and remote components, and the integration of heterogeneous environments. The approach is based on existing standards and uses XML for describing component navigation and communication capabilities. It is mostly declarative and may also be used for improving integration capabilities of already existing portal components. ...|$|R
40|$|Web server {{security}} {{is one of}} the most important security research problems today. In any network, most users communicate with a principal using <b>web</b> <b>portals.</b> Examples of a principal are large organizations, home businesses, government <b>portals,</b> personal <b>web</b> <b>pages,</b> search engines, online transaction systems and so on. Unlike other systems such as corporate data servers, mail servers, internal databases, etc which can be accessed only via corporate LAN or secure remote connection, web servers are always directly exposed to the internet. They are accessible to anyone in the world and hence probability of attacks is very high. Web sever intrusion can result in loss of important data, revenue loss, but more importantly in severe embarrassment of a reputed organization. There has been extensive research in this area and many security-related tightening systems available in the market. But internet hackers have always found a way to get around them. Most of the existing systems look for signatures in the web server requests to identify malicious attempts based on signatures already known to them. The limitation of this approach is that attack signatures can be changed in such a way to escape detection. In addition, new types of attacks cannot be detected as their signatures are not known. Another approach is to look for anomalies of requests instead. Advantage of this approach is that it is not influenced by attack signatures. It just distinguishes between normal and abnormal usage of existing web service, and so it is not vulnerable to altered signatures or new signatures. These systems are known as anomaly based systems. In this paper, we explore the implementation and evaluation of such a system. 1...|$|R
40|$|Health <b>portals</b> are {{dedicated}} <b>web</b> <b>pages</b> for medical practices to provide patients {{access to their}} electronic health records. The problem identified in this quality improvement project was that the health portal in the urgent care setting had not been available to staff nor patients. To provide leadership with information related to opening the portal, the first purpose {{of the project was}} to assess staff and patients 2 ̆ 7 perceived use, ease of use, attitude toward using, and intention to use the portal. The second purpose was to evaluate the portal education materials for the top 5 urgent care diagnoses: diabetes, hypertension, asthma, otitis media, and bronchitis for understandability and actionability using the Patient Education Material Assessment Tool, Simple Measures of Goobledygook, and the Up to Date application. The first purpose was framed within the technology acceptance model which used a 26 -item Likert scale ranging from - 3 (total disagreement) to + 3 (total agreement). The staff (n = 8) and patients (n = 75) perceived the portal as useful (62...|$|R
40|$|The "Guide of education, educative {{computing}} and e-learning electronic resources" is {{a directory}} of useful Internet-based academic free resources. This selected and anotated collection of e-resources includes watching <b>portals,</b> institutional <b>web</b> sites, personal <b>web</b> <b>pages,</b> serials, databases, repositories, virtual libraries, directories and search engines, {{related to these}} subjects. {{in spanish and english}} languages...|$|R
40|$|The {{development}} of a <b>web</b> 2. 0 <b>portal</b> using Ajax and jQuery techniques. This paper describes the {{development of}} a <b>web</b> <b>portal</b> using technologies like PHP, jQuery and Ajax. Regular <b>web</b> <b>portals</b> simply use PHP and MySQL, which is not enough to provide the interactivity the user needs from a <b>web</b> <b>portal.</b> jQuery technique is designed to change the way you write JavaScript, because it is very compact and easy to use and understand. jQuery is also very popular being used by Google, IBM, NBC, Amazon, Wordpress and many others. Ajax technique is used to increase responsiveness and interactivity of the <b>web</b> <b>pages</b> achieved by exchanging small amounts of data « behind the scenes » so that the entire <b>web</b> <b>pages</b> {{do not have to be}} reloaded each time there is a need to fetch data from the server...|$|R
40|$|Innovation {{in tourism}} {{industry}} such as products, services and technologies remain unclear {{with the exception}} of the internet through e-tourism. The internet has revolutionized the tourism industry in terms of distribution of information, communication and sales. Many new internet or web based technologies and innovations need to be developed to tap the vast potential market of this industry. One of the new technologies that has been acknowledged and recognized to give a great impact to this industry is virtual reality VR. VR offers a new kind of human computer interaction where the user of a VR system can immerse him/her self into a computer-generated, multi-sensory, intuitively operable virtual experience space. With VR technology, it is possible to visit tourist attraction sites without actually going to the site. This technology has been evaluated for practical use in many industrial fields such as manufacturing, medical, multimedia,show business and computer games. The rapid advancement within the computer technology is constantly generating new and improved VR applications. This paper discusses and presents two commonly used VR techniques i. e. Panoramic and 30 modeling in tourism industry. These techniques have been applied as online <b>portals</b> and <b>web</b> <b>pages,</b> online or standalone tourist information systems, 3 D or virtual exhibit, virtual museum and amusement parks worldwide. These systems are information-intensive, multimedia based, and provide spatial nature of tourism related services and processes which promote the concept of virtual tourism. The uses of VR techniques and systems have significantly contributed to the implementation of more VR based solutions for tourism industry...|$|R
5000|$|General country {{information}} on Iran was in May 2011 {{removed from the}} <b>web</b> <b>pages</b> of Landinfo, and replaced by {{a link to the}} Swedish information <b>web</b> <b>portal,</b> explained by lack of resources for giving a satisfactory continuous updating on the situation. http://www.landguiden.se/ ...|$|R
5000|$|Timway (...) is a <b>web</b> <b>portal</b> and {{directory}} primarily serving Hong Kong. Its “Timway Hong Kong Search Engine” {{is designed}} for searching web sites in Hong Kong. It supports web search query in English and Chinese, and indexes <b>web</b> <b>pages</b> in both languages.|$|R
50|$|Youdao (有道) is {{a search}} engine {{released}} by Chinese internet company NetEase (網易) in 2007. It is the featured search engine of its parent company's <b>web</b> <b>portal,</b> 163.com, and lets users search for <b>web</b> <b>pages,</b> images, news, music, blogs, Chinese-to-English dictionary entries, and more.|$|R
40|$|<b>Web</b> <b>portals</b> contain {{large amount}} of information. Users could really benefit from it if {{personalized}} presentation is used. For this to be accomplished the website needs to “know ” its users. When surfing the Web users leave digital footprints {{in the form of}} navigational paths and actions taken. We present a method for adaptive navigation support and link recommendation. The method is based on an analysis of the user’s navigational patterns and behavior on the <b>web</b> <b>pages</b> while browsing through a <b>web</b> <b>portal.</b> We extract interesting information from a <b>web</b> <b>portal</b> and then recommend it. Finally, we provide our experience with a recommender system deployed on our faculty’s website, which recommends events by means of personalized calendar...|$|R
40|$|Abstract. The Semantic Web {{will never}} be a reality unless the {{quantity}} of Semantic <b>Web</b> <b>pages</b> significantly increases. Our thesis is that enlargement of the Semantic Web content is possible and effective by acquisition of ontological structures from human users of community Semantic <b>Web</b> <b>portals.</b> An important aspect of the proposed approach is that ontology structure acquisition from humans takes place merely by the portal exploitation without significant effort from the user side. We describe the People’s portal environment that motivates and allows the People’s portal users to develop, populate and share ontologies, define the ways to manage the content on the community Semantic <b>Web</b> <b>portals,</b> and to reach dynamic consensus on the basis of heterogeneous ontologies. This work contributes to overcoming the limitations of the existing <b>web</b> <b>portals</b> by specifying and prototyping the People’s portal environment, i. e., a framework for user-driven community Semantic <b>Web</b> <b>portals.</b> ...|$|R
40|$|Abstract—The {{ever-increasing}} web is {{an important}} source for building large-scale corpora. However, dynamically generated <b>web</b> <b>pages</b> often contain much irrelevant and duplicated text, which impairs {{the quality of the}} corpus. To ensure the high quality of web-based corpora, a good boilerplate removal algorithm is needed to extract only the relevant content from <b>web</b> <b>pages.</b> In this article, we present an automatic text extraction procedure, GoldMiner, which by enhancing a previously published boilerplate removal algorithm, minimizes the occurrence of irrelevant duplicated content in corpora, and keeps the text more coherent than previous tools. The algorithm exploits similarities in the HTML structure of pages coming from the same domain. A new evaluation document set (CleanPortalEval) is also presented, which can demonstrate the power of boilerplate removal algorithms for <b>web</b> <b>portal</b> <b>pages.</b> Index Terms—corpus building, boilerplate removal, the web as corpus I. THE TASK When constructing corpora from web content, the extraction of relevant text from dynamically generated HTML pages is not a trivial task due to the great amount of irrelevant repeated text that needs to be identified and removed so that it does not compromise the quality of the corpus. This task, called boilerplate removal in the literature, consists of categorizing HTML content as valuable vs. irrelevant, filtering out menus, headers and footers, advertisements, and structure repeated on many pages. In this paper, we present a boilerplate removal algorithm that removes irrelevant content from crawled content more effectively than previous tools. The structure of our paper is as follows. First, we present some tools that we used as baselines when evaluating the performance of our system. The algorithm implemented in one of these tools, jusText, is also used as part of our enhanced boilerplate removal algorithm. This is followed by the presentation of the enhanced system, called GoldMiner, and the evaluation of the results...|$|R
5000|$|Publishing or {{pushing this}} content to other systems (enterprise <b>portals,</b> <b>web</b> sites) ...|$|R
5000|$|ATHENSi.com, a {{community}} <b>portal</b> <b>Web</b> site {{run by the}} E.W. Scripps School of Journalism ...|$|R
30|$|MBCC-HITS {{algorithm}} is proposed based on hyper-text induced topic selection (HITS) algorithm [16]. Authority score and hub score are defined as the measurements of <b>web</b> <b>page</b> importance in HITS algorithm. Authority score and hub score represent the contributions of a <b>web</b> <b>page</b> to information originality and transmission, respectively [17]. Internet can be abstracted as a directed and unweighted graph according to connection relations among <b>web</b> <b>pages.</b> Each <b>web</b> <b>page</b> corresponds to a node in the directed graph. The directions of edges are given according to the directions of corresponding hyperlinks. Authority score of a <b>web</b> <b>page</b> equals to the sum of hub scores of <b>web</b> <b>pages</b> (i.e. in-linked <b>web</b> <b>pages)</b> which cite this <b>web</b> <b>page.</b> It reflects {{the importance of a}} <b>web</b> <b>page</b> from the perspective of in-degree. Hub score of a <b>web</b> <b>page</b> equals to the sum of authority scores of <b>web</b> <b>pages</b> (i.e. out-linked <b>web</b> <b>pages)</b> which this <b>web</b> <b>page</b> cites. It reflects the importance of a <b>web</b> <b>page</b> from the perspective of out-degree.|$|R
40|$|It is {{attractive}} to extract parts of <b>Web</b> <b>pages</b> for the following two purposes. One is to clip parts of <b>Web</b> <b>pages</b> as we clip articles of newspapers. Another is to utilize information on <b>Web</b> <b>pages</b> by software. In this paper we define operations to extract parts of <b>Web</b> <b>pages,</b> namely path set operations. The operations are for both clipping of parts of <b>Web</b> <b>pages</b> and information extraction from <b>Web</b> <b>pages.</b> <b>Web</b> <b>page</b> clipping is extraction of parts of <b>Web</b> <b>pages</b> keeping their view information. Information extraction from <b>Web</b> <b>pages</b> is transformation of <b>Web</b> <b>pages</b> to tractable structures. We show that we can easily extract parts of <b>Web</b> <b>pages</b> for the two purposes by the operations, and also show that procedures based on the operations are somewhat robust against update of <b>Web</b> <b>pages.</b> 1...|$|R
40|$|The {{main goal}} of <b>web</b> <b>pages</b> ranking {{is to find}} the interrelated pages. In this paper, we {{introduce}} an algorithm called FPR-DLA. In the proposed method learning automata is assigned to each <b>web</b> <b>page</b> which its function is determining the weight of hyperlinks between <b>web</b> <b>pages.</b> Also for determining the weight of each <b>web</b> <b>page</b> parameters such as time duration on a <b>web</b> <b>page</b> and the importance of <b>web</b> <b>pages</b> are considered. Time duration on a <b>web</b> <b>page</b> and the importance of <b>web</b> <b>pages</b> are characterized as a fuzzy linguistic variable. The proposed algorithm calculates the rank of each <b>web</b> <b>page</b> as recursive according to the weights of each <b>web</b> <b>page</b> and hyperlinks between <b>web</b> <b>pages.</b> Experimental results show that the proposed method has a considerable efficiency in determining the rank of <b>web</b> <b>pages...</b>|$|R
40|$|Key Words: meta-tag, {{automatic}} classification, <b>web</b> <b>page</b> classification, weka {{naive bayes}} Recently, {{the amount of}} <b>web</b> <b>pages,</b> which include various information, has been drastically increased according to the explosive increase of WWW usage. Therefore, the need for <b>web</b> <b>page</b> classification arose {{in order to make}} it easier to access <b>web</b> <b>pages</b> and to make it possible to search the <b>web</b> <b>pages</b> through the grouping. <b>Web</b> <b>page</b> classification means the classification of various <b>web</b> <b>pages</b> that are scattered on the web according to the similarity of documents or the keywords contained in the documents. <b>Web</b> <b>page</b> classification method can be applied to various areas such as <b>web</b> <b>page</b> searching, group searching and e-mail filtering. However, it is impossible to handle the tremendous amount of <b>web</b> <b>pages</b> on the <b>web</b> by using the manual classification. Also, the automatic <b>web</b> <b>page</b> classification has the accuracy problem in that it fails to distinguish the different <b>web</b> <b>pages</b> written in different forms without classification errors. In this paper, we propose the automatic <b>web</b> <b>page</b> classification system using meta-tag that can be obtained from the <b>web</b> <b>pages</b> in order to solve the inaccurat...|$|R
40|$|Abstract — Today, {{the world}} is moving across the internet. Internet is {{collection}} of <b>Web</b> <b>Pages.</b> <b>Web</b> <b>Page</b> contains a bunch of information. In bunch of information to find or retrieve particular page or information is difficult task. It is difficult for the Search Engine to identify <b>web</b> <b>page.</b> So make this task easy there are different <b>web</b> <b>page</b> classification methods. <b>web</b> <b>page</b> classification is a web mining area. Using this method we can identify <b>web</b> <b>pages.</b> <b>Web</b> <b>page</b> Classification retrieves WebPages based on content and structure of <b>web</b> <b>page.</b> This paper shows results of different Classification methods and comparison of that Index Terms — <b>web</b> <b>page</b> classification, SVM, Naïve Bayes, Extraction, Feature I...|$|R
40|$|The {{growth of}} the Internet has {{generated}} <b>Web</b> <b>pages</b> that are rich in media and that incur significant rendering latency when accessed through slow communication channels. The technique of Web-object prefetching can potentially expedite the presentation of <b>Web</b> <b>pages</b> by utilizing the current <b>Web</b> <b>page’s</b> view time to acquire the Web objects of likely future <b>Web</b> <b>pages.</b> The performance of the Web object prefetcher is contingent on the predictability of future <b>Web</b> <b>pages</b> and quickly determining which Web objects to prefetch during the limited view time interval of the current <b>Web</b> <b>page.</b> The proposed Markov–Knapsack method uses an approach that combines a Multi-Markov Web-application centric prefetch model with a Knapsack Web object selector to enhance <b>Web</b> <b>page</b> rendering performance. The Markov <b>Web</b> <b>page</b> model ascertains the most likely next <b>Web</b> <b>page</b> set based on the current <b>Web</b> <b>page</b> and the <b>Web</b> object Knapsack selector determines the premium Web objects to request from these <b>Web</b> <b>pages.</b> The results presented in the paper show that the proposed methods can be effective in improving a Web browser cache-hit percentage while significantly lowering <b>Web</b> <b>page</b> rendering latency...|$|R
40|$|<b>Web</b> <b>Pages</b> {{which are}} meant for user {{registration}} or Comment in Blogs or Online Polls or logging in to a <b>web</b> <b>portal</b> or getting indexed in search engines {{are open to}} be accessed by scripts. It means it cannot be known whether the page being requested from a Human user or from a machine. There are different Websites who provides specific API {{to be used by}} scripts, for example: API for sending SMS through script. But there may be situations where a <b>web</b> <b>page</b> may need to be restricted from script access i. e. user must be a Human Being. Few popular techniques for restricting the <b>web</b> <b>page</b> from script access are “use of CAPTCHA ” or “use of OTP”. In this paper a different approach named as “RinG: Random id name Generator ” is proposed and then the popularly used approaches have been compared with different parameters...|$|R
40|$|This bachelor`s thesis {{deals with}} <b>web</b> <b>pages</b> design and {{structure}} of new <b>web</b> <b>pages</b> after previous analysis of actual <b>web</b> <b>pages.</b> It includes at analysis of negatives and positives of the actual website and new <b>web</b> <b>pages</b> design, functions, design of code structure and content structure of new <b>web</b> <b>pages</b> with possibility of later application...|$|R
40|$|Abstract. This {{paper will}} {{describe}} {{an approach to}} the design and implementa-tion of ontology driven dynamic web sites, a kind of architecture that provides the adoption of ontologies rather than more traditional forms of persistent data storage facilities such as relational databases. This approach provides a flexible support to the design and implementation of <b>web</b> <b>portals</b> in which navigation schemes are not entirely predetermined but are instead influences by actual rela-tionships among {{the contents of the}} ontology, that are used to generate <b>web</b> <b>pages</b> as well as hyperlinks. The application of this approach to the realization of a <b>web</b> <b>portal</b> devoted to the sharing of archaeological knowledge about the Silk Road will also be described. ...|$|R
