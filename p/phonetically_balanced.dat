124|5|Public
5000|$|The Harvard {{sentences}} are {{a collection}} of sample phrases that are used for standardized testing of Voice over IP, cellular, and other telephone systems. They are <b>phonetically</b> <b>balanced</b> sentences that use specific phonemes at the same frequency they appear in English.|$|E
5000|$|IEEE Recommended Practices for Speech Quality Measurements {{sets out}} seventy-two lists of ten phrases each, {{described}} as the [...] "1965 Revised List of <b>Phonetically</b> <b>Balanced</b> Sentences (Harvard Sentences)." [...] They are widely used in research on telecommunications, speech, and acoustics, where standardized and repeatable sequences of speech are needed. The Open Speech Repository provides some freely usable, prerecorded WAV files of Harvard Sentences in American and British English, in male and female voices.|$|E
5000|$|... "Spondee" [...] {{is a piece}} {{composed}} by intelligent dance music (IDM) duo Matmos. It is the third track on their 2001 album A Chance to Cut Is a Chance to Cure. The piece revolves around audio samples from a hearing test: a woman's voice recites two-syllable words, stressing both syllables. Audiologist Rebecca Highlander, CCC-A, reads a list of <b>phonetically</b> <b>balanced</b> words (spondees, with equal stress on {{the first and second}} syllable). Recorded in the hearing test booth at the Jean Weingarden Peninsula Oral School for the Deaf, beeps and tones were generated during the response curve testing of hearing aids.|$|E
40|$|The paper {{deals with}} the process of {{designing}} a phonetically and prosodically rich speech corpus for unit selection speech synthesis. The attention is given mainly to the recording and verification stage of the process. In order to ensure as high quality and consistency of the recordings as possible, a special recording environment consisting of a recording session management and “pluggable ” chain of checking modules was designed and utilised. Other stages, namely text collection (including) both <b>phonetically</b> and prosodically <b>balanced</b> sentence selection and a careful annotation on both orthographic and phonetic level are also mentioned. 1...|$|R
40|$|This paper {{presents}} a design {{and development of}} Speech Recognition System for Tamil language. This system is based on CMU Sphinx 4 open source speech recognition (ASR) engine developed by Carnegie Mellon University. This system should be adapted to speaker specific automatic, continuous speech. One of {{the main components of}} this system is a core Tamil speech recognition system that can be trained with field specific data. The target domain is the accent spoken by illiterate Tamil-speaker from Eastern area of Sri Lanka. The <b>phonetically</b> rich and <b>balanced</b> sentence text corpus were developed and recorded in conditional environment to set up speaker specific speech corpus. Using this speech corpus the system was trained and tested with speaker specific (testing with same word uttered by same person) and speaker independent data (testing with different word uttered by different person). The system currently gives a satisfactory peak performance of 39. 5 % Word Error Rate (WER) for speaker specific and unsatisfactory rate for speaker independent data, which is comparable with the best word error rates of most of the recognition systems for continuous speech available for any language...|$|R
40|$|Recently, Automatic Speech Recognition (ASR) {{systems were}} used to assist {{children}} in language acquisition as it {{has the ability to}} detect human speech signal. Despite the benefits offered by the ASR system for children, {{there is a lack of}} ASR systems for Malay-speaking children. One of the contributing factors for this is the lack of continuous speech database for Malay-speaking children. Though cross-lingual adaptation is a common solution for developing ASR systems for under-resourced language, it is not viable for children as there are very limited children's speech databases as a source model. In this research, we are proposing a two-stage adaptation using a very limited database of Malay-speaking children for the development of the children ASR system. The two stage adaptation comprises the cross-lingual adaptation (first stage) and cross-age adaptation. For the first stage, a well-known speech database that is <b>phonetically</b> rich and <b>balanced,</b> is adapted to the medium-sized Malay adults using supervised MLLR. The second stage adaptation uses the speech acoustic model generated from the first adaptation, and the target database is a small-sized database of Malay-speaking children. We have measured the performance of the proposed technique using word error rate, and then compare them with the conventional benchmark adaptation. The two stage adaptation proposed in this research has better recognition accuracy as compared to the benchmark adaptation in recognizing children's speech...|$|R
40|$|International audienceThe {{construction}} of a speech recognition system requires a recorded set of phrases to compute the pertinent acoustic models. This set of phrases must be phonetically rich and balanced {{in order to obtain}} a robust recognizer. By tradition, this set is defined manually implicating a great human effort. In this paper we propose an automated method for assembling a <b>phonetically</b> <b>balanced</b> corpus (set of phrases) from the Web. The proposed method was used to construct a <b>phonetically</b> <b>balanced</b> corpus for the Mexican Spanish language...|$|E
3000|$|The {{test set}} {{consisting}} of 16 individual speakers (8 male, 8 female), each speaking 10 <b>phonetically</b> <b>balanced</b> English sentences, is randomly chosen from the TIMIT training corpus [...]...|$|E
30|$|The {{proposed}} VAD algorithm {{was trained}} and tested using a speech database that is <b>phonetically</b> <b>balanced.</b> The system was evaluated using the error rate and accuracy rate metrics.|$|E
40|$|Abstract: This paper {{describes}} and proposes {{an efficient}} and effective framework for the design and development of a speaker-independent continuous automatic Arabic speech recognition system based on a <b>phonetically</b> rich and <b>balanced</b> speech corpus. The speech corpus contains a total of 415 sentences recorded by 40 (20 male and 20 female) Arabic native speakers from 11 different Arab countries representing the three major regions (Levant, Gulf, and Africa) in the Arab world. The proposed Arabic speech recognition system {{is based on the}} Carnegie Mellon University (CMU) Sphinx tools, and the Cambridge HTK tools were also used at some testing stages. The speech engine uses 3 -emitting state Hidden Markov Models (HMM) for tri-phone based acoustic models. Based on experimental analysis of about 7 hours of training speech data, the acoustic model is best using continuous observation’s probability model of 16 Gaussian mixture distributions and the state distributions were tied to 500 senones. The language model contains both bi-grams and tri-grams. For similar speakers with different sentences, the system obtained a word recognition accuracy of 92. 67 % and 93. 88 % and a Word Error Rate (WER) of 11. 27 % and 10. 07 % with and without diacritical marks, respectively. For different speakers with similar sentences, the system obtained a word recognition accuracy of 95. 92 % and 96. 29 %, and a WER of 5. 78 %, and 5. 45 % with and without diacritical marks, respectively. Whereas different speakers and different sentences, the system obtained a word recognition accuracy of 89. 08 % and 90. 23 %, and a WER of 15. 59 % and 14. 44 % with and without diacritical marks, respectively...|$|R
40|$|Ma W., "Connectionist vector {{quantization}} in automatic speech recognition", Proefschrift voorgedragen tot het behalen van het doctoraat in de toegepaste wetenschappen, K. U. Leuven, 162 pp., January 1999, Leuven, Belgium. In this thesis, we successfully apply connectionist approaches, particularly the Multi-Layer Perceptron (MLP), to tasks of speech recognition. We present {{in detail the}} Back Propagation theory and its implementation issues, including a modified weight adaptation algorithm. We provide a weight updating strategy {{to speed up the}} convergence during network training. The training data is <b>balanced</b> <b>phonetically</b> such that the network treats all phonemes equally. We introduce a random database generator to obtain a robust MLP network. We introduce the fuzzy MLP into speech recognition and use the overlapped Hamming window as the fuzzy membership function for the MLP output. We design and implement the Multi-Layer Perceptron {{to be used as a}} labeler for the Hidden Markov Model (HMM) system, which combines the good short-time classification properties of MLPs with the good integration and overall recognition capabilities of discrete HMMs. The standard {{vector quantization}} has been replaced by an MLP labeler giving phone-like labels in an MLP/HMM hybrid system. Compared with using MLPs as probability generators for HMMs, our system is more flexible in system design because it can use the word models instead of phonetic models. Moreover, as it does not need to be trained to reach a global minimum, the network can have fewer hidden units and therefore can be trained faster. Also, we do not need to retrain our MLPs with segmentations generated by a Viterbi alignment. Compared to Euclidean labeling, our method has the advantages of needing fewer HMM parameters per state and of obtaining higher recognition accuracy. We use histograms to illustrate the MLP output value for each phonetic class. From those MLP output histograms, we observe that the winner take-all MLPs ignore the relativity of different phonetic classes. We extend our base-line winner-take-all method to several Top-N methods. A series of MLP/HMM hybrid models are discussed to fully use the MLP output information and to improve the speech recognition performance. Those investigated models are: MLP multi-dimensional labeling, MLP multi-labeling, MLP fuzzy-labeling, multi-MLP multi-labeling and multi-MLP fuzzy labeling. status: publishe...|$|R
40|$|The {{construction}} of {{automatic speech recognition}} (ASR) systems is fundamentally dependent on the speech corpus used to train the acoustic models. The speech corpus should be <b>phonetically</b> <b>balanced</b> {{to assure that the}} acoustic models are properly trained. This paper presents the design and development of the first <b>phonetically</b> <b>balanced</b> Romanian speech corpus. It describes all the language processing steps taken in order to obtain a proper set of phrases, discusses some important aspects regarding Romanian phonetics and emphasizes the phrase selection mechanism. Index Terms—ASR, corpora acquisition, corpora processing, diacritics restoration 1...|$|E
40|$|We {{present a}} method for {{designing}} a <b>phonetically</b> <b>balanced</b> speech corpus. In this method, we used a phonotactic approach to design the phonetic content of VOXMEX: a <b>phonetically</b> <b>balanced</b> corpus for Mexican Spanish. The transcriptions of VOXMEX contain a complete coverage of phonemes and allophones of Mexican Spanish in every possible context. This corpus is designed for doing phonetic research and acoustic modeling in the speech recognition area. We are recording the readings of the designed text corpus to obtain the speech data of VOXMEX. Our main goal in this project is to construct a phonologically representative speech corpus for Mexican Spanish. 1...|$|E
30|$|The phoneme-level {{language}} {{models were}} trained using the phonetic transcriptions {{of the speech}} database that was used for training the acoustic models. The database contains 1132 <b>phonetically</b> <b>balanced</b> sentences, over 48000 phoneme instances.|$|E
3000|$|... [...]. We assume all {{the models}} {{resulting}} from a given codebook are equally likely. This assumption is valid, in general, if the codebook size is large and derived from a <b>phonetically</b> <b>balanced</b> large training set.|$|E
30|$|For English, we {{used the}} CSTR US KED Timit {{database}} [30] which contains 453 <b>phonetically</b> <b>balanced</b> utterances spoken by a US male speaker. The database was hand-labelled in phonemes, syllables and words, and carefully corrected. The syllables with lexical stress were also manually labelled.|$|E
40|$|This paper evaluates speech {{perception}} {{testing in}} pediatric cochlear implant users. Using pre-recorded stimulus presentation, the author replicated an earlier experiment comparing the Lexical Neighborhood List (LNT) test to the <b>Phonetically</b> <b>Balanced</b> Kindergarten (PB-K) test in estimating speech perception abilities in hearing impaired children...|$|E
30|$|The {{training}} set includes <b>phonetically</b> <b>balanced</b> English utterances of seven professional narrators. The utterances in this database are sampled at 16 kHz. The corpus includes sentences of JMK (Canadian male), BDL (American male), AWB (Scottish male), RMS (American male), KSP (Indian male), CLB (American female), and SLT (American female)[29].|$|E
30|$|The TMW is Tohoku University-Matsushita Isolated Word Database. It has <b>phonetically</b> <b>balanced</b> 212 {{words that}} are spoken by 60 people (30 males and 30 females). This {{database}} was divided into the training and testing set. The training set has 40 samples and the test set has 20 samples.|$|E
40|$|An {{amplitude}} control technique has been employed {{for use with}} analog voice communication systems, which improves low-level phoneme reception and eliminates the received noise between words and syllables. Tests were conducted on a narrow-band frequency-modulation simplex voice communication channel employing the {{amplitude control}} technique. Presented for both the modified rhyme word tests and the <b>phonetically</b> <b>balanced</b> word tests {{are a series of}} graphical plots of the tests' score distribution, mean, and standard deviation as a function of received carrier-to-noise power density ratio. At low received carrier-to-noise power density ratios, a significant improvement in the intelligibility was obtained. A voice intelligibility improvement of more than 2 dB was obtained for the modified rhyme test words, and a voice intelligibility improvement in excess of 4 dB was obtained for the <b>phonetically</b> <b>balanced</b> word tests...|$|E
40|$|The paper {{describes}} {{design and}} process of collection, annotation {{and evaluation of}} a new Slovak mobile-telephone speech database MobilDat-SK, which is a mobile-telephone extension to the SpeechDat-E SK. The MobilDat-SK database contains recordings of 1100 speakers and it is balanced according to the age, accent, and sex of the speakers. Every speaker pronounced 50 files (either prompted or spontaneous) containing numbers, names, dates, money amounts, embedded command words, geographical names, <b>phonetically</b> <b>balanced</b> words, <b>phonetically</b> <b>balanced</b> sentences, Yes/No answers and one longer non-mandatory spontaneous utterance. In the paper {{the structure of the}} database, the hardware and software solution of the automatic recording, the speaker recruitment strategy, the annotation process and evaluation process are described. The MobilDat-SK database has been developed for the “Intelligent Speech Communication Interface ” project in the frame of the State Research and Development Task “Buildin...|$|E
40|$|In {{this paper}} methods are {{proposed}} {{which can be}} used to select a set of <b>phonetically</b> <b>balanced</b> sentences. The principle of the methods is presented and some experimental results are given. In the end of the paper the use of the proposed methods for the Czech read-speech corpus design is described in detail and the structure of the corpus is explained...|$|E
40|$|This work {{introduces}} a <b>phonetically</b> <b>balanced</b> modi-fied rhyme test (MRT) for evaluating Catalan speech intelligibility. The proposal {{complies with the}} stan-dard MRT restrictions, besides yielding phonetic balanced word ensembles {{so as to avoid}} biasing the test to scarcely representative phonemes. Hence, it allows testing the intelligibility of any communica-tion system delivering Catalan speech by means of a unique phonetic meaningful comparison framework...|$|E
40|$|Phonemic {{content is}} one of many {{important}} criteria in a development of any kind of speech testing materials. In this paper, we explain a procedure and tool we created in the process of constructing phonetically-balanced (PB) sentence-length materials for Thai, as an assessment for speech reception thresholds. Our procedure includes establishing criteria, preselecting sentences, creating pool of replacement words, determining phonemic distribution, and constructing sentences. Importantly, a tool is created to determine whether set of words or sentences are <b>phonetically</b> <b>balanced.</b> Once the phoneme distribution and the set of words with transcription are specified, the tool efficiently computes phoneme occurrences among words or sentences (within a set) and can be used to manipulate words to achieve goal in <b>phonetically</b> <b>balanced</b> (PB). To show how this is accomplished, two sentence sets are constructed and evaluated by native speakers. The procedure and tool have characteristics that make them potentially useful in other applications and can be applied to other languages...|$|E
30|$|The Keele {{database}} The Keele Pitch Database {{was recorded}} at Keele University. Data were collected for five male and five female English speakers, {{each of them}} read a <b>phonetically</b> <b>balanced</b> text: the “north-wind story”. The speech and laryngograph signals were sampled at 20  kHz. The fundamental frequency was estimated by applying an autocorrelation on windows of 25.6  ms shifted by intervals of 10  ms.|$|E
30|$|The {{different}} CD and CI speech codebooks {{considered in}} the experiments are of large size (256) and are derived from {{a large number of}} <b>phonetically</b> <b>balanced</b> sentences from the WSJ database. Moreover, the LBG algorithm used to generate the speech codebooks computes cluster centroids in an optimal fashion. All these factors ensure the validity of the assumption about equal probability of models in Equation 16.|$|E
3000|$|... {{is formed}} by four {{components}} of vector α related to mouth movements. The first database (digits utterances) {{was used to}} evaluate the influence {{of the structure of the}} covariance matrices (full or diagonal) on the visual estimation. To evaluate the performance of the proposed conversion algorithm in a more complex and realistic scenario, AV-HMMs were trained using the second database (120 <b>phonetically</b> <b>balanced</b> sentences).|$|E
30|$|In {{order to}} built HMM-based voices, one needs a labeled corpus with {{transcribed}} speech. So, the speech training corpus {{used in this}} work consists of 1, 000 <b>phonetically</b> <b>balanced</b> phrases [34] recorded by a man speaker, corresponding to approximately 1.6 h of audio. This well-known set of sentences was obtained from CETEN-Folha through a genetic algorithm, seeking to minimize the number of speech synthesis units (triphones) not present in the collection.|$|E
40|$|The CMU Arctic {{databases}} {{designed for}} the purpose of speech synthesis research. These single speaker speech databases have been carefully recorded under studio conditions and consist of approximately 1200 <b>phonetically</b> <b>balanced</b> English utterances. In addition to wavefiles, the databases provide complete support for the Festival Speech Synthesis System, including pre-built voices that may be used as is. The entire package is distributed as free software, without restriction on commercial or non-commercial use. 1...|$|E
40|$|Conference PaperSecond-order {{statistical}} methods show very good results for automatic speaker identification in controlled recording conditions. These approaches are generally {{used on the}} entire speech material available. In this paper, we study {{the influence of the}} content of the test speech material on the performances of such methods, i. e. under a more analytical approach. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than <b>phonetically</b> <b>balanced</b> test utterances, even though the training is done, in both cases, with 15 seconds of <b>phonetically</b> <b>balanced</b> speech. Nevertheless, results with other phoneme classes are never dramatically poor. These results tend to show that the speaker-dependent information captured by long-term second-order statistics is consistently common to all phonetic classes, and that the homogeneity of the test material may improve the quality of the estimates...|$|E
40|$|Second-order {{statistical}} methods show very good results for automatic speaker identi cation in controlled recording conditions [2]. These approaches are generally {{used on the}} entire speech material available. In this paper, we study the in uence {{of the content of}} the test speech material on the performances of such methods, i. e. under a more analytical approach [3]. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker speci c: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identi cation results than <b>phonetically</b> <b>balanced</b> test utterances, even though the training is done, in both cases, with 15 seconds of <b>phonetically</b> <b>balanced</b> speech. Nevertheless, results with other phoneme classes are never dramatically poor. These results tend to show that the speaker-dependent information captured by long-term second-order statistics is consistently common to all phonetic classes, and that the homogeneity of the test material may improve the quality of the estimates. 1...|$|E
40|$|This paper {{describes}} {{the data collected}} in Wizard of Oz experiments in a spoken dialogue system, Waxholm, that provides information on boat traffic in the Stockholm archipelago. The data consist of utterance-length speech files, their corresponding transcriptions, and log files of the dialogue sessions. Apart from the spontaneous dialogue speech, the speech material also comprise recordings of <b>phonetically</b> <b>balanced</b> reference sentences uttered by all 66 subjects. The recording procedure is described {{as well as some}} characteristics of the speech data and the dialogue...|$|E
30|$|The {{recognition}} {{results are}} {{also influenced by}} the language models used in the recognition process. Phoneme bigram language model built on <b>phonetically</b> <b>balanced</b> speech data was found to increase {{the accuracy of the}} recognition with up to 13 % both for clean singing test cases and for vocal line extracted from polyphonic music. Word recognition in clean singing using a bigram language model built from lyrics text allows recognition of approximately one fifth of the sung words in clean singing. In polyphonic music, the results are lower.|$|E
30|$|To {{build the}} Spanish {{acoustic}} model {{we used the}} Albayzin [34] corpus. The corpus comprises two sub-corpora with 6, 800 utterances each: one based on texts extracted from novels and the other based on queries to a geography database. The utterances were recorded under good acoustic conditions (quiet offices, with {{the same set of}} professional microphones) and were pronounced by 304 speakers (152 female, 152 male), whose age varied from 18 to 55 years. The Albayzin corpus represents 12 h 52 min of annotated speech in 13, 600 gender and <b>phonetically</b> <b>balanced</b> sentences.|$|E
40|$|Abstract — This paper {{presents}} different steps {{followed to}} realize Algerian Speech Database (ALGASD) for standard Arabic language. The project concerns 300 Algerian native speakers whom are selected statistically from certain {{regions of the}} country and attempts to represent the principal variations of pronunciations denoted between the populations. Inspired by TIMIT protocol, ALGASD took into consideration the age and the education of each speaker. The basic text used to elaborate the database is constituted by 200 <b>phonetically</b> <b>balanced</b> sentences. Number of recorded sentences achieve to 1080 ones. Keywords—Database; Algerian; speec...|$|E
40|$|Audiology in general, and {{audiometric}} {{techniques in}} particular, have made commendable {{progress in the}} last few decades. The threshold for pure tones and speech can be accurately established and at subsequent examinations can be confirmed or compared, with a fair amount of precision. The articulation score for <b>phonetically</b> <b>balanced</b> words can be determined and so can the effect of amplification on such scores. The conductive and the cochlear component of hearing loss can be distinguished with a fair amount of reliability from what is called the "sensorineural" or the "retrocochlear component"...|$|E
