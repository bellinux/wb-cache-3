1|1|Public
40|$|We {{developed}} a distributed search {{system with the}} corresponding very large scale corpora from NTCIR- 5 WEB Task. And we arranged the scoring method {{which is based on}} link-structure of the Web documents to calculate lower cost. Our search system, which consists of 6 PCs could make indices for full texts size of about 1 TB. Additionally, we confirmed that our arranged scoring method made an improvement of mean average precision. Also we performed experiments with the <b>pseudodocument</b> vectors at every pseudo-relevance feedback. Meanwhile we made a pseudo-document vector at every relevance feedback. Therefore the results had slightly better precision than raw queries even though it had not been tuned yet...|$|E
40|$|Abstract: We {{describe}} our {{participation in}} the TREC 2006 Genomics track, in which our main focus was on query expansion. We hypothesized that applying query expansion techniques would help us both to identify and retrieve synonymous terms, and to cope with ambiguity. To this end, we developed several collection-specific as well as web-based strategies. We also performed post-submission experiments, in which we compare various retrieval engines, such as Lucene, Indri, and Lemur, using a simple baseline topicset. When indexing entire paragraphs as <b>pseudodocuments,</b> we find that Lemur is able to achieve the highest document-, passage-, and aspect-level scores, using the KL-divergence method and its default settings. Additionally, we index the collection at {{a lower level of}} granularity, by creating pseudo-documents comprising of individual sentences. When we search these instead of paragraphs in Lucene, the passage-level scores improve considerably. Finally we note that stemming improves overall scores by at least 10 %. ...|$|R

