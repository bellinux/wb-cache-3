11|15|Public
2500|$|PCBoard {{is still}} in use today by nostalgic BBS fans. There is a {{freeware}} FOSSIL driver called NetFoss which allows PCBoard to be accessible via telnet under Windows. There was also a DOS based PCBoard add-on [...] "PCB Internet Collection" [...] which allowed telnet access by installing a (DOS-only) <b>packet</b> <b>driver.</b>|$|E
5000|$|Crynwr Collection - {{alternative}} free <b>packet</b> <b>driver</b> collection ...|$|E
5000|$|API-level {{support for}} <b>Packet</b> <b>driver,</b> IPX, Berkeley sockets (dosnet).|$|E
50|$|<b>Packet</b> <b>drivers</b> can {{implement}} {{many different}} network interfaces, including Ethernet, Token ring, RS-232, Arcnet, and X.25.|$|R
50|$|WinPKT is {{a driver}} that enables use of <b>packet</b> <b>drivers</b> under Microsoft Windows that moves around {{applications}} willy nilly.|$|R
50|$|The PWS {{was capable}} of {{communicating}} with ICL departmental and mainframe services {{using a combination of}} Microlan2 and OSLAN (Open Systems Local Area Network) protocols. Microsoft and OSLAN network connectivity was supported via BICC OSLAN cards (OSLAN being ICL's implementation of OSI transport protocols over Ethernet). TCP/IP and DECnet support was implemented by third parties using <b>packet</b> <b>drivers,</b> as NDIS was not available at this time.|$|R
50|$|W3C507 is a DLL to <b>packet</b> <b>driver</b> for the Microsoft Windows environment.|$|E
50|$|Initially PC/TCP's {{protocol}} stacks {{and network}} interface drivers were linked into individual application executables, as with PC/IP. By 1990 all PC/TCP applications shared a TSR kernel, which itself (initially) used built-in network interface drivers. By 1991, John Romkey's TSR PC/TCP <b>Packet</b> <b>Driver</b> specification had largely replaced the built-in drivers.|$|E
5000|$|PCBoard {{is still}} in use today by nostalgic BBS fans. There is a {{freeware}} FOSSIL driver called NetFoss which allows PCBoard to be accessible via telnet under Windows. There was also a DOS based PCBoard add-on [...] "PCB Internet Collection" [...] which allowed telnet access by installing a (DOS-only) <b>packet</b> <b>driver.</b>|$|E
5000|$|Nelson wrote {{code for}} some programs: In 1983, he co-wrote a MacPaint clone, Painter's Apprentice, with Patrick Naughton. Nelson was the {{original}} developer of Freemacs (a variant of Emacs used by FreeDOS). While attending university, Nelson began developing the collection of drivers later commercially released as the [...] "Crynwr Collection". In 1991, Nelson founded Crynwr Software, a company located in Potsdam, New York, supporting deployment of large-scale e-mail systems, development of <b>packet</b> <b>drivers,</b> Linux kernel drivers, and reverse engineering of embedded systems.|$|R
40|$|We {{present the}} Harvard User-Level Metricom Radio driver (HUMR), a {{substrate}} {{for development of}} wireless datagram routing protocols, and Any-Path Routing without Loops (APRL), a distance-vector, wireless datagram routing protocol developed on HUMR that provably does not acquire routes that contain loops. Reachability between hosts changes constantly on a wireless, mobile network. HUMR, our radio IP implementation, uses link-level neighbor acquisition and loss reports from Metricom spreadspectrum radios to perform truly dynamic discovery of a host's radio neighbors and their IP-to-MAC address mappings, without requiring any statically configured state in hosts. Unlike almost all IP <b>packet</b> <b>drivers,</b> HUMR is implemented entirely at application level, and is therefore portable, simple, and extensible. APRL's loop- [...] ...|$|R
5000|$|Extra Security: Npcap can be {{restricted}} {{so that only}} Administrators can sniff packets. Non-Admin user will have to pass a User Account Control (UAC) dialog to utilize the driver. This is conceptually similar to UNIX, where root access is generally required to capture <b>packets.</b> The <b>driver</b> also has Windows ASLR and DEP security features enabled.|$|R
50|$|PC/TCP <b>Packet</b> <b>Driver</b> is a {{networking}} API for MS-DOS, PC DOS, {{and later}} x86 DOS implementations such as DR-DOS, FreeDOS, etc. It implements {{the lowest levels}} of a TCP/IP stack, where the remainder is typically implemented either by TSR drivers or as a library linked into an application program. It was invented in 1983 at MIT's Lab for Computer Science (CSR/CSC group under Jerry Saltzer and David D. Clark), and was commercialized in 1986 by FTP Software.|$|E
5000|$|A <b>packet</b> <b>driver</b> uses an x86 {{interrupt}} number (INT) between 60h [...]. 80h. The number used {{is detected}} at runtime, {{it is most}} commonly 60h but may be changed to avoid application programs which use fixed interrupts for internal communications. The interrupt vector {{is used as a}} pointer (4-bytes little endian) to the address of a possible interrupt handler. If the text string [...] "PKT DRVR" [...] is found within the first 12-bytes immediately following the entry point then a driver has been located.|$|E
40|$|Most {{implementations}} of IP {{security are}} deeply entwined in {{the source of}} the protocol stack. However, such source code is not readily available for MS-DOS systems. We implemented a version using the <b>packet</b> <b>driver</b> interface. Our module sits between the generic Ethernet driver and the hardware driver; it emulates each to the other. Most of the code is straightforward; in a few places, though, we were forced to compensate for inadequate interface definitions...|$|E
40|$|International audienceVirtualization {{is a key}} {{technology}} to enable cloud computing. Driver domain based model for network virtualization offers isolation {{and high levels of}} flexibility. However, it suffers from poor performance and lacks scalability. In this paper, we evaluate networking performance of virtual machines within Xen. The I/O channel transferring <b>packets</b> between the <b>driver</b> domain and the virtual machines is shown to be the bottleneck. To overcome this limitation, we proposed a packet aggregation based mechanism to transfer <b>packets</b> from the <b>driver</b> domain to the virtual machines. Packet aggregation, combined with an efficient core allocation, allows virtual machines throughput to scale up by 700 %, while minimizing both memory and CPU consumption. Besides, aggregation impact on packets delay and jitter remains acceptable. Hence, the proposed I/O virtualization model satisfies infrastructure providers to offer Cloud computing services...|$|R
40|$|Abstract. Driver is a communicated program {{between the}} {{operating}} system and hardware. It {{is the responsibility of}} handling I/O request <b>packet.</b> Keyboard filter <b>driver</b> which used Windows NT driver model is completed in DDK developed environment. This driver realized filtering and recording key information. It’s convenient for users to view information of keyboard history. The design of this driver can also be studied for other Windows drivers...|$|R
5000|$|Packet loss can {{be caused}} by a number of other factors that can corrupt or lose packets in transit, such as radio signals that are too weak due to {{distance}} or multi-path fading (in radio transmission), faulty networking hardware, or faulty network <b>drivers.</b> <b>Packets</b> are also intentionally dropped by normal routing routines (such as Dynamic Source Routing in ad hoc networks, [...] ) and through network dissuasion technique for operational management purposes.|$|R
40|$|Graduation date: 1990 This thesis {{discusses}} {{an approach}} whereby Microsoft's MS OS/ 2 {{is provided with}} a means of running the Department of Defense's Transmission Control Protocol/Internet Protocol (TCP/IP). This is done by developing a Packet Protocol Device Driver. This device driver complies with the <b>Packet</b> <b>Driver</b> Specification from FTP Software Inc. and with the Network Driver Interface Specification (NDIS) from 3 Com and Microsoft Corporations. This packet protocol device driver co-resides with other protocol device drivers and shares one medium access control (MAC) device driver as defined in the NDIS. With the successful implementation of the packet protocol device driver, an existing Microsoft MS-DOS version of a TCP/IP package was ported and with minor modifications recompiled to run under MS OS/ 2. This method allows users to retain utility {{and use of the}} OS/ 2 LAN Manager, a networking strategy provided within MS OS/ 2...|$|E
40|$|Most {{implementations}} of IP {{security are}} deeply entwined in {{the source of}} of the protocol stack. However, such source code is not readily available for MS-DOS systems. We implemented a version using the <b>packet</b> <b>driver</b> interface. Our module sits between the generic Ethernet driver and the hardware driver; it emulates each to the other. Most of the code is straightforward; in a few places, though, {{we were forced to}} compensate for inadequate interface definitions. 1 Introduction The Internet Engineering Task Force (IETF) {{is in the process of}} adopting standards for IP-layer encryption and authentication (IPSEC) [3, 1, 2]. Not surprisingly, most of the original implementations are being developed for various flavors of the Unix 1 operating system. While this is good [...] -indeed, we are primarily Unix system users ourselves [...] -much of the world feels differently, and prefers MS-DOS 2 systems. For many reasons, it seemed desirable to develop an IPSEC implementation for a MS-DOS system. Our [...] ...|$|E
40|$|International audienceRouter {{virtualization}} {{seems as}} the obvious next step to system virtualization {{and the key}} to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between <b>packets</b> in the <b>driver</b> domain before switching them to the target virtual router...|$|R
40|$|International audienceNetwork {{virtualization}} is {{a promising}} technology that offers {{high levels of}} flexibility, isolation, extensibility and cost-effectiveness. In this paper, we focus on router virtualization. We evaluate the forwarding performance of virtual routers when the data plane runs in the guests. This scenario offers {{a high level of}} isolation and flexibility, however, it suffers from performance limitations due to the virtualization overhead. We show that the he I/O communication between the driver domain and the guests is the bottleneck. To overcome this limitation, we propose a new packets aggregation mechanism that transfers groups of <b>packets</b> between the <b>driver</b> domain and the guests. This enhancement makes the forwarding performance of one guest scale up to 1600 Kp/s. Furthermore, we propose a dimensioning tool {{in order to determine the}} maximum achievable throughput with regard to the container size...|$|R
30|$|A closer {{scrutiny}} further {{suggests that}} this performance disparity between the helper stations and source stations in a CoopMAC network is an artifact of our present implementation approach, {{and is expected to}} disappear once the access to firmware becomes available. More specifically, as explained in Section 5, the cooperative MAC protocol is currently realized at the driver level, which forces the helper stations to pass the received foreign <b>packets</b> into the <b>driver</b> space and queue them together with the native traffic in the same buffer. When the local load at the helpers grows high enough, the arrival rate of the indigenous packets at the buffer far surpasses that of the packets received from the source stations. Therefore, {{the rate at which the}} packets can be received at the helpers places a bottleneck on the end-to-end throughput of the forwarded traffic, which essentially gives local helper traffic preferential treatment.|$|R
40|$|Virtualization {{is a key}} {{technology}} for cloud based data centers to implement the vision of {{infrastructure as a service}} (IaaS) and to promote effective server consolidation and application consolidation. A new method is implemented in virtual machine monitor and representative workloads set method in cloud-based data centers does not provide sufficient performance isolation to guarantee the effectiveness of resource sharing, particularly during run time for a program on multiple virtual machines of the same physical machine are competing for computing and communication resources. In this project, we present our performance measurement study of network I/O applications in virtualized cloud. The proposed packet aggregation based mechanism is to transfer <b>packets</b> from the <b>driver</b> domain to the virtual machines. By observational the performance is calculated and documented that our proposal allows the virtual machines throughput to scale up at line rates. The proposed model allowed us to dynamically tune the aggregation mechanism in order to achieve the best tradeoff between the packets delay and throughput. This I/O virtualization model henceforth satisfies the infrastructure providers to offer Cloud computing services...|$|R
40|$|Abstract:Cloud {{computing}} is a {{latest technology}} that gives platform and software as a service. A cloud platform {{can be either}} virtualized or not. the cloud platform increases the resources availability and the flexibility of their management (allocation, migration, etc.). It also reduces the cost through hardware multiplexing and helps energy saving. Virtualization is then a more and interesting technology of cloud computing. System virtualization refers to the software and hardware techniques that allow partitioning one physical machine into multiple virtual instances that run concurrently and share the underlying physical resources and devices. Virtualization is a key technology to enable cloud computing. It enhances resource availability and offers high flexibility and cost effectiveness. In our proposed scheme allows us to dynamically tune the aggregation mechanism to achieve the best tradeoff between the packets delay and throughput. The proposed I/O virtualization model hence forth satisfies the infrastructure providers to offer cloud computing services. Here I/O virtualization model based on packet aggregation to transfer <b>packets</b> between the <b>driver</b> domain and the VMs. It will Improve networking performance of VM where the access to the network device is shared through the driver domain...|$|R
40|$|Replaying traces is a time-honored {{method for}} benchmarking, stress-testing, and {{debugging}} systems—and more recently—forensic analysis. One benefit to replaying traces is the reproducibility {{of the exact}} set of operations that were captured during a specific workload. Existing trace capture and replay systems operate at different levels: network <b>packets,</b> disk device <b>drivers,</b> network file systems, or system calls. System call replayers miss memory-mapped operations and cannot replay I/Ointensive workloads at original speeds. Traces captured at other levels miss vital information that is available only at the file system level. We designed and implemented Replayfs, the first system for replaying file system traces at the VFS level. The VFS is the most appropriate level for replaying file system traces because all operations are reproduced {{in a manner that}} is most relevant to file-system developers. Thanks to the uniform VFS API, traces can be replayed transparently onto any existing file system, even a different one than the one originally traced, without modifying existing file systems. Replayfs’s user-level compiler prepares a trace to be replayed efficiently in the kernel where multiple kernel threads prefetch and schedule the replay of file system operations precisely and efficiently. These techniques allow us to replay I/O-intensive traces at different speeds, and even accelerate them on the same hardware that the trace was captured on originally. ...|$|R

