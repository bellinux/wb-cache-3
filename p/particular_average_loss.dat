0|3732|Public
40|$|Abstract. We {{consider}} {{a model of}} correlated defaults in which the default times of multiple entities depend not only on a common and specific factors, {{but also on the}} extent of past defaults in the market, via the <b>average</b> <b>loss</b> process, including the average number of defaults as a special case. The paper characterizes the <b>average</b> <b>loss</b> process when the number of entities becomes large, showing that under some monotonicity conditions the limiting <b>average</b> <b>loss</b> process can be determined by a fixed point problem. We also show that the Law of Large Numbers holds under certain compatibility conditions...|$|R
5000|$|The <b>average</b> <b>loss</b> of life-expectancy, LLE, in the {{population}} is defined by: ...|$|R
5000|$|Thus, the ETL can be {{interpreted}} as the <b>average</b> <b>loss</b> beyond VaR: ...|$|R
50|$|The overall <b>average</b> <b>loss</b> of {{the gears}} is about 1&thinsp;%-5&thinsp;%, {{comparable}} to a derailleur.|$|R
50|$|One study {{commissioned}} in the UK {{estimated the}} <b>average</b> <b>loss</b> rate of libraries to theft at 5.3%.|$|R
5000|$|Expectancy = (Trading system Winning {{probability}} * Average Win) - (Trading system losing probability * <b>Average</b> <b>Loss)</b> ...|$|R
50|$|In January, 6 U-boats were {{destroyed}} in the theatre; this also became the <b>average</b> <b>loss</b> for the year.|$|R
5000|$|Choose the {{decision}} rule {{with the lowest}} <b>average</b> <b>loss</b> (i.e. minimize the expected value of the loss function): ...|$|R
5000|$|<b>Average</b> <b>loss</b> {{of energy}} of an atom moving through a dense medium gives idea on {{stopping}} cross section and capability of depth perception.|$|R
40|$|The {{selection}} of controlled variables (CVs) from available measurements through exhaustive search is computationally forbidding for large-scale processes. We have recently proposed novel bidirectional {{branch and bound}} (B- 3) approaches for CV selection using the minimum singular value (MSV) rule and the local worst- case loss criterion {{in the framework of}} self-optimizing control. However, the MSV rule is approximate and worst-case scenario may not occur frequently in practice. Thus, CV selection by minimizing local <b>average</b> <b>loss</b> can be deemed as most reliable. In this work, the B- 3 approach is extended to CV selection based on local <b>average</b> <b>loss</b> metric. Lower bounds on local <b>average</b> <b>loss</b> and, fast pruning and branching algorithms are derived for the efficient B- 3 algorithm. Random matrices and binary distillation column case study are used to demonstrate the computational efficiency of the proposed method...|$|R
30|$|Thirdly, even {{profitable}} {{strategies such as}} MACD and STOCH-D {{could not}} reliably predict subsequent market directions as their profitable trades are usually less than fifty percent of total number of trades. They make money from having a higher average profit from profitable trades than an <b>average</b> <b>loss</b> from unprofitable ones. Interestingly, optimized parameters do not improve the odds of profitable trades. Our results {{support the idea that}} profitable strategies make money not from outguessing market directions, but from maximizing average profits and minimizing <b>average</b> <b>losses.</b>|$|R
40|$|This paper {{presents}} a comparative analysis of various interior permanent magnet (IPM) machines and a surface permanent magnet (SPM) machine {{designed for a}} commuter train. The different IPM machines are characterized by a different rotor geometry and volume of permanent magnets. The traction requirements fix the maximum machine volume and the back electromotive force at uncontrolled generator operations. The comparison is extended considering the drive performance. Therefore, considering a standard load cycle the different machines and drives are compared in terms of torque capability at reduced voltage, machine <b>average</b> <b>losses</b> and power converter <b>average</b> <b>losses...</b>|$|R
40|$|This paper {{discusses}} {{the problem of}} designing a new variable sampling plan. Suppose that the lot quality characteristic obeys an exponential distribution. Adopting Taguchi's loss function, {{the objective is to}} design a plan under which the producer's risk of rejecting a lot that has a specified <b>average</b> <b>loss</b> per item is no greater than alpha, and the consumer's risk of accepting a lot that has a specified <b>average</b> <b>loss</b> per item is no greater than beta. The method of designing this plan is an extension of the method used by Derman and Ross. ...|$|R
30|$|Ireland {{is one of}} the {{countries}} most severely affected by the Great Recession. National income fell by more than 10 per cent between 2007 and 2012, {{as a result of the}} bursting of a remarkable property bubble, an exceptionally severe banking crisis, and deep fiscal adjustment. This paper examines the income distribution consequences of the recession, and identifies the impact of a broad range of austerity policies on the income distribution. The overall fall in income was just under 8 per cent between 2008 and 2011, but the greatest losses were strongly concentrated on the bottom and top deciles. Tax, welfare and public sector pay changes over the 2008 to 2011 period gave rise to lower than <b>average</b> <b>losses</b> for the bottom decile. Thus, the larger than <b>average</b> <b>losses</b> observed overall are not due to these policy changes; instead, the main driving factors are the direct effects of the recession itself. Policy changes do contribute to the larger than <b>average</b> <b>losses</b> at high income levels.|$|R
40|$|This paper {{reports the}} results of {{experiments}} carried out to establish losses that occur at the different stages in the processing of cassava into gari at both local and improved technology centres. An <b>average</b> <b>loss</b> of 6. 1 % occurred during cassava processing into gari at the peeling stage for local and 4. 7 % at improved technology centres. The grating process recorded <b>average</b> <b>losses</b> of 5. 9 % at both local and improved technology centers. The losses at the dewatering stage ranges from 3. 0 % for local and 5. 6 % for improved technology centres. The sifting process recorded losses of 4. 3 and 5. 4 % for local and improved technology centres respectively. The total <b>average</b> <b>loss</b> in cassava processing into gari for local centres was 19. 3 % while that for improved technology centres 21. 5 %. A garification rate of 0. 32 and 0. 33 was established for local and improved technology centres respectively...|$|R
30|$|<b>Average</b> packet <b>loss</b> ratio. The <b>average</b> packet <b>loss</b> {{ratio is}} the number of packets {{received}} unsuccessfully by all receivers versus the total number of packets sent out by all senders.|$|R
50|$|According to the National Fire Protection Association (NFPA), {{fires in}} hotels with {{sprinklers}} averaged 78% less damage than fires in hotels without them (1983-1987). The NFPA says the <b>average</b> <b>loss</b> per fire in buildings with sprinklers was $2,300, {{compared to an}} <b>average</b> <b>loss</b> of $10,300 in unsprinklered buildings. The NFPA adds {{that there is no}} record of a fatality in a fully sprinklered building outside the point of fire origin. However, in a purely economic comparison, this is not a complete picture; the total costs of fitting, and the costs arising from non-fire triggered release must be factored.|$|R
5000|$|Since Δ commutes with G, any left {{translate}} of fs {{is also an}} eigenfunction {{with the}} same eigenvalue. In <b>particular,</b> <b>averaging</b> over K, the function ...|$|R
3000|$|..., can be obtained. This <b>average</b> path <b>loss</b> {{reflects}} {{the distance between}} the FBS and the MUE. The <b>average</b> DL path <b>loss,</b> PL [...]...|$|R
40|$|This paper {{focuses on}} a version of {{sequential}} mastery testing (i. e., classifying students as a master/nonmaster or continuing testing and administering another item or testlet) in which response behavior is modeled by a multidimensional item response theory (IRT) model. First, a general theoretical framework is outlined {{that is based on}} a combination of Bayesian sequential decision theory and multidimensional IRT. Then how multidimensional IRT-based sequential master testing can be generalized to adaptive item- and testlet-selection rules is discussed for the case where the choice of the next item or testlet to be administered is optimized using the information from previous responses. Both compensatory and conjunctive loss structures are considered. Simulation studies are used to evaluate: (1) the performance, in terms of <b>average</b> <b>loss,</b> of multidimensional IRT-based sequential mastery testing {{as a function of the}} number of items administered per testing stage; (2) the effects on <b>average</b> <b>loss</b> when turning the sequential procedure into an adaptive sequential procedure; and (3) the impact on <b>average</b> <b>loss</b> when the multidimensional structure is ignored and a unidimensional IRT model is used in the decision procedure...|$|R
40|$|This paper {{presents}} QoS control enhanced architecture for VoIP networks. In this architecture we {{use both}} the probe flow delay and <b>average</b> <b>loss</b> rate measurement systems. First we apply the probability-based EMBAC scheme on our delay system. Then we propose a new probability-based EMBAC {{with a severe}} congestion consideration scheme to improve the admission control scheme in both measurement systems. We compare {{the performance of the}} enhanced systems in terms of blocking probability under the same condition of achieving <b>average</b> packet <b>loss</b> rate no greater than the certain target by setting an appropriate admission threshold in each system under each scenario. In this study, it is shown through simulations that the enhanced systems proposed in this paper can be a powerful and reliable EMBAC tool for VoIP networks with minimum blocking probability and minimum <b>average</b> <b>loss</b> rates. I...|$|R
50|$|Example 1. If {{we believe}} our <b>average</b> <b>loss</b> on the worst 5% of the {{possible}} outcomes for our portfolio is EUR 1000, then we could say our expected shortfall is EUR 1000 for the 5% tail.|$|R
30|$|This {{number is}} {{the ratio of}} an average profit from {{profitable}} trades over an <b>average</b> <b>loss</b> from unprofitable ones. A good trading system would let the profit run while cutting losses quickly, resulting in a high ratio.|$|R
50|$|The EE {{represents}} the estimated <b>average</b> <b>loss</b> {{at a specific}} future point of time that a lender would suffer from if the borrower (counterparty) fully defaults on his debt (i.e. if the Loss Given Default (LGD) was 100%).|$|R
40|$|The {{purpose of}} this {{investigation}} was to interpret the bitumen-aggregate adhesion based on the dielectric spectroscopic response of individual material components utilizing their dielectric constants, refractive indices and average tangent of the dielectric <b>loss</b> angle (<b>average</b> <b>loss</b> tangent). Dielectric spectroscopy of bitumen binders at room temperature was performed in the frequency range of 0. 01 – 1000 Hz. Dielectric spectroscopy is an experimental method for characterizing the dielectric permittivity of a material as a function of frequency. Adhesion data has been determined using the Rolling bottle method. The results show that the magnitude of the average tangent of the dielectric <b>loss</b> angle (<b>average</b> <b>loss</b> tangent) depends on bitumen type. The <b>average</b> <b>loss</b> tangent in the frequency range 0. 01 – 1 Hz is introduced as a potential indicator for predicting polarizability and, thereby, adhesion potential of bitumen binders to quartz aggregates when using Portland cement. In order to obtain acceptable adhesion of 70 / 100 penetration grade bitumen binders and quartz aggregates when using Portland cement, it is suggested that the binder have an average tan δ > 0. 035 in the frequency range 0. 01 – 1 Hz...|$|R
30|$|Annually, each herder lost {{on average}} {{approximately}} 8.5 % of their annual household cash income due to depredation. The <b>average</b> <b>loss</b> {{calculated for the}} current study is higher than the <b>average</b> depredation <b>loss</b> in JSWNP (Wang and Macdonald 2006) and <b>average</b> <b>loss</b> in Tsarap valley in north India (Spearing 2000). The maximum losses attributed to SL and TW were calculated as 0.1 % and 3.5 % of the herder’s total annual income respectively. The difference in the depredation loss between the two prey species could be due to the incidences of the depredation differences on young and adult livestock. TW depredated more on adult yaks, which are valued significantly higher than the young yaks and the sheep. The losses in the study area are four times higher than those in the Kibber Wildlife Sanctuary (Mishra 1997). According to herders, the rate of depredation cases has been increasing and such losses affect their livelihood.|$|R
3000|$|... 59 {{fractures}} {{were analyzed}} {{with mean age}} of 56  years and male to female ratio of 1 : 2. <b>Average</b> <b>loss</b> of radial tilt was 2.6 °, loss of palmar tilt was 2.6 ° and loss of ulnar variance was 1.3  mm.|$|R
50|$|Each {{connection}} made adds about 0.6 dB of <b>average</b> <b>loss,</b> {{and each}} joint (splice) adds about 0.1 dB. Depending on the transmitter {{power and the}} sensitivity of the receiver, if the total loss is too large the link will not function reliably.|$|R
50|$|With an <b>average</b> <b>loss</b> of 80,000 acres {{each year}} in the U.S. passage of the Wetlands Loan Act would ensure that land owners would have a {{partnership}} with the United States Fish and Wildlife Service to voluntarily conserve the land through conservation easements.|$|R
3000|$|Because this {{transformation}} matches the Lagrangian form, the theoretical results {{shown in the}} previous section can be applied. Specifically, this means that we can optimize the system for a <b>particular</b> <b>average</b> power [...]...|$|R
5000|$|On November 5, 2013, Advantage Rent A Car {{announced}} to file bankruptcy. As of October 25, 2013 [...] "Simply Wheelz" [...] sold 5,295 vehicles through auctions with an <b>average</b> <b>loss</b> of about $1,633 per vehicle, {{which resulted in}} a total loss of about $8.6 million.|$|R
30|$|In {{only one}} {{study was the}} {{clinical}} presentation and physical examination described [17]. Pain and limited range of motion was shown in 12 of the 14 patients evaluated in this study (86  %). Varus alignment was noted in five patients (range 3 – 8 °), valgus in six patients (range 6 – 18 °) and neutral in three patients. The <b>average</b> <b>loss</b> of extension is 12 ° (range 0 – 40 °) and the <b>average</b> flexion <b>loss</b> is 11 ° (range 0 – 40 °).|$|R
40|$|In {{standard}} online learning, {{the goal}} of the learner is to maintain an <b>average</b> <b>loss</b> close to the loss of the best-performing function in a fixed class. Classic results show that simple algorithms can achieve an <b>average</b> <b>loss</b> arbitrarily close to that of the best function in retrospect, even when input and output pairs are chosen by an adversary. However, in many real-world applications such as spam prediction and classification of news articles, the best target function may be drifting over time. We introduce a novel model of concept drift in which an adversary is given control of both the distribution over input at each time step and the corresponding labels. The goal of the learner is to maintain an <b>average</b> <b>loss</b> close to the 0 / 1 loss of the best slowly changing sequence of functions with no more than K large shifts. We provide regret bounds for learning in this model using an (inefficient) reduction to the standard no-regret setting. We then go on to provide and analyze an efficient algorithm for learning d-dimensional hyperplanes with drift. We conclude with some simulations illustrating the circumstances under which this algorithm outperforms other commonly studied algorithms when the target hyperplane is drifting. ...|$|R
40|$|Three {{types of}} {{packages}} containing enough sanitary supplies for one menstrual period were weighed and distributed to 100 subjects to determine adequacy of supplies and compliance to protocol. Returned packages were then reweighed to determine total menstrual loss. No subject used all the supplies in her package; therefore, packages contents were deemed ample. 7 subjects added their own purchased products to their packages; since added products were specified by name, weight corrections were easily made. 2 subjects lost unused supplies which were specified, and weight corrections were made accordingly. Evaporative loss from sealed and stored and frequently opened packages was measured. 100 g distilled water {{were added to}} contents inside 60 packages which were sealed and set aside 7 days. <b>Average</b> <b>loss</b> from these packages was 0. 65 ± 0. 57 g. 5 g water were added to 60 packages 4 times daily for 5 days (100 g total). Then packages were set aside for 2 days and weighed on day 7. <b>Average</b> <b>loss</b> from these packages was 1. 25 ± 0. 85 g. The combination of storage plus frequent opening resulted in an <b>average</b> <b>loss</b> of 1. 90 ± 0. 87 g. Thus, the direct-weight method permits recovery of 97 – 98 % of sample...|$|R
30|$|Budgetary {{policy in}} the most recent years has {{involved}} somewhat greater losses for low income groups than those at the top. However, over the full 2008 to 2013 period it remains true that the highest losses are in the top quintile, with lower than <b>average</b> <b>losses</b> for other deciles.|$|R
30|$|Interestingly, {{even for}} {{profitable}} trading {{strategies such as}} STOCH-D or MACD long-only trading strategies, the percent of profitable trades over total number of trades is still usually less than 50  %. This means that profitable strategies make money not so much from correctly predicting directions of the market, but from letting the profits to run in profitable trades while minimizing loss in unprofitable ones. This fact is reflected in larger than one ratios of average profit over <b>average</b> <b>loss.</b> In sharp contrast, unprofitable long-only strategies like RSI are profitable more than 50  % of the times and sometime up to 80  %, yet it still gives minuscule annualized returns or even negative ones. The average profits over <b>average</b> <b>losses</b> of RSI are much lower than one. No wonder, they are beaten by a BH.|$|R
