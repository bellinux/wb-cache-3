26|417|Public
2500|$|Taking the log of this {{representation}} of a double <b>precision</b> <b>number</b> and simplifying results in the following: ...|$|E
50|$|A {{traditional}} {{element in}} each Holiday on Ice {{show is the}} <b>precision</b> <b>number</b> with its famous spinning wheel, in which the skaters link arms with each other, one by one, lengthening the two spokes which spin around a center point. For many years, the traditional kickline, the light finale with illuminated costumes and the firework fountains {{at the end of}} a show were much expected elements as well.|$|E
5000|$|As of July 2017 {{the family}} of microarchitectures {{implementing}} the identically called instruction set [...] "Graphics Core Next" [...] has seen five iterations. The differences in the instruction set are rather minimal and do not differentiate too much from one another. An exception is the fifth generation GCN architecture, which heavily modified the stream processors to improve performance and support the simultaneous processing of two lower precision numbers {{in place of a}} single higher <b>precision</b> <b>number.</b>|$|E
2500|$|FORTRAN (at {{least since}} FORTRAN IV as of 1961) also uses [...] "D" [...] to signify double <b>precision</b> <b>numbers.</b>|$|R
2500|$|The {{number of}} bits needed for the {{exponent}} of the extended precision format follows from the requirement that the product of two double <b>precision</b> <b>numbers</b> should not overflow when computed using the extended format. [...] The largest possible exponent of a double precision value is 1023 so the exponent of the largest possible product of two double <b>precision</b> <b>numbers</b> is 2047 (an 11-bit value). [...] Adding in a bias to account for negative exponents means that the exponent field {{must be at least}} 12 bits wide.|$|R
50|$|A cell {{can return}} any Python object, which allows {{calculations}} with vectors, matrices, fractions, arbitrary <b>precision</b> <b>numbers</b> and symbols.Therefore, pyspread follows {{an approach that}} is similar to the spreadsheet SIAG from Siag Office.|$|R
50|$|AMD began {{releasing}} {{details of}} their next generation of GCN Architecture, termed the 'Next-Generation Compute Unit', in January 2017. The new design {{is expected to increase}} instructions per clock, higher clock speeds, support for HBM2, a larger memory address space, and the High Bandwidth Cache Controller. Additionally, the new chips are expected to include improvements in the Rasterisation and Render output units. The stream processors are heavily modified from the previous generations to support packed math Rapid Pack Math technology for 8-bit, 16-bit, and 32-bit numbers. With this there is a significant performance advantage when lower precision is acceptable (for example: processing two half-precision numbers {{at the same rate as}} a single single <b>precision</b> <b>number).</b>|$|E
50|$|SSE is a SIMD {{instruction}} set that works only on floating point values, like 3DNow!. However, unlike 3DNow! it severs all legacy {{connection to the}} FPU stack. Because it has larger registers than 3DNow!, SSE can pack {{twice the number of}} single precision floats into its registers. The original SSE was limited to only single-precision numbers, like 3DNow!. The SSE2 introduced the capability to pack double precision numbers too, which 3DNow! had no possibility of doing since a double <b>precision</b> <b>number</b> is 64-bit in size which would be the full size of a single 3DNow! MMn register. At 128 bits, the SSE XMMn registers could pack two double precision floats into one register. Thus SSE2 is much more suitable for scientific calculations than either SSE1 or 3DNow!, which were limited to only single precision. SSE3 does not introduce any additional registers.|$|E
40|$|This paper {{describes}} {{a collection of}} Fortran routines for multiple precision complex arithmetic and elementary functions. The package provides good exception handling, flexible input and output, trace features, and results that are almost always correctly rounded. For best e#ciency on di#erent machines, the user can change the arithmetic type used to represent the multiple <b>precision</b> <b>number...</b>|$|E
25|$|In conclusion, {{the exact}} number of bits of {{precision}} needed in the significand of the intermediate result is somewhat data dependent but 64 bits is sufficient to avoid precision loss {{in the vast majority of}} exponentiation computations involving double <b>precision</b> <b>numbers.</b>|$|R
50|$|The core {{command set}} and syntax {{is the same}} in all {{implementations}} of Microsoft BASIC and generally speaking, a program can be run on any version {{if it does not}} use hardware-specific features or double <b>precision</b> <b>numbers</b> (not supported in some implementations).|$|R
25|$|MIPS has 32 floating-point {{registers}}. Two registers are paired for double <b>precision</b> <b>numbers.</b> Odd numbered registers {{cannot be}} used for arithmetic or branching, just {{as part of a}} double precision register pair, resulting in 16 usable registers for most instructions (moves/copies and loads/stores were not affected).|$|R
40|$|Abstract — Digital {{calibration}} {{techniques are}} widely utilized to linearize pipelined A/D converters (ADCs). However, their power dissipation can be prohibitively high, especially when high-order gain calibration is needed. For high-order gain calibration, this paper proposes a design methodology {{to optimize the}} data <b>precision</b> (<b>number</b> of bits) within the digital calibration unit. Thus, the power dissipation of the calibration unit can be minimized, without affecting the linearity of the pipelined ADC. A 90 -nm FPGA synthesis of a 2 nd-order digital gain-calibration unit shows that the proposed optimization methodology results in a 59 % reduction in power dissipation. I...|$|E
40|$|On a {{suitably}} {{large and}} stable telescope, direct coronagraphic imaging can make numerous important measurements of terrestrial exoplanets. Astrometric time-series observations {{will be of}} primary importance: Only when the planet orbital elements are known can 1) {{the effects of the}} planet's illumination phase and asterocentric distance be accounted for in the observed planetary fluxes, and 2) the planet's location relative to the habitable zone be established. Planetary colors and spectra will allow characterization of the planet's atmosphere and possibly even its surface, while time-variable fluxes may indicate surface contrast features or seasonal changes. Imaging will also reveal the context of other planets and dust belts in the system, both of which can affect habitability. Requirements on astrometric <b>precision,</b> <b>number</b> of visits, and telescope aperture will be discussed...|$|E
40|$|In {{elicitation}} tasks, {{people are}} asked to make estimates under conditions of uncertainty but elicitors then interpret these estimates as if the estimator were certain of them. An analysis of people’s patterns of responding during the elicitation of uncertainty, indicates that there are markers of confidence incorporated into these estimates {{that can be used}} to predict the person’s true level of confidence. One such marker is the <b>precision</b> (<b>number</b> of significant figures) of the estimate. Analyses of elicited data show the expected positive relationships between accuracy, precision and explicit confidence and, further, that precision offers information beyond that of explicit confidence ratings. We then demonstrate the importance of incorporating this information on an overconfidence task, showing that it can account for a 9 % difference in calibration. Matthew B. Welsh, Daniel J. Navarro and Steve H. Beg...|$|E
50|$|Note {{that these}} <b>precision</b> <b>numbers</b> {{are for the}} {{interpolated}} values relative original tabulated coordinates. The overall precision and accuracy of interpolated values for describing the actual motions of the planets will {{be a function of}} both the precision of the ephemeris tabulated coordinates and the precision of the interpolation.|$|R
40|$|A quad-double {{number is}} an unevaluated sum of four IEEE double <b>precision</b> <b>numbers,</b> ca-pable of {{representing}} at least 212 bits of signicand. We present the algorithms for various arithmetic operations (including the four basic operations and various algebraic and transcen-dental operations) on quad-double numbers. The {{performance of the}} algorithms, implemented in C++, is also presented. ...|$|R
40|$|A quad-double {{number is}} an unevaluated sum of four IEEE double <b>precision</b> <b>numbers,</b> capable of {{representing}} at least 212 bits of signi cand. Algorithms for various arithmetic operations (including the four basic operations and various algebraic and transcendental operations) are presented. A C++ implementation of these algorithms is also described, {{as well as}} an application of this quad-double library...|$|R
40|$|We {{describe}} {{the design and}} implementation of a high dynamic range (HDR) imaging system capable of capturing RGB color images with a dynamic range of 10, 000, 000 : 1 at 25 frames per second. We use a highly programmable camera unit with high throughput A/D conversion, data processing and data output. HDR acquisition is performed by multiple exposures in a continuous rolling shutter progression over the sensor. All the different exposures for one particular row of pixels are acquired head to tail within the frame time, {{which means that the}} time disparity between exposures is minimal, the entire frame time can be used for light integration and the longest expo- sure is almost the entire frame time. The system is highly configurable, and trade-offs are possible between dynamic range, <b>precision,</b> <b>number</b> of exposures, image resolution and frame rate...|$|E
40|$|In {{the context}} of methodologies {{intended}} to confer robustness to geometric algorithms, we elaborate on the exact computation paradigm and formalize the notion of degree of a geometric algorithm, aa a worst-case quantification of the <b>precision</b> (<b>number</b> of bits) to which arithmetic calculation have to be executed in order to guarantee topological correctness. We aleo propose a formalism for the expeditious evaluation of algorithmic degree. As an application of this paradigm and an illustration of our general approach, we consider the important classical problem of proximity queries in 2 and 3 dimensions, and develop a new technique for the efficient and robust execution of such queries baaed on an implicit representation of Voronoi diagrams. Our new technique gives both low degree and fast query time, and for 2 D queries is optimal with respect to both cost meixmres of the paradigm, asymptotic number of operations md arithmetic degree...|$|E
40|$|We {{show that}} {{rational}} data of bounded input length are uniformly distributed {{with respect to}} condition numbers of numerical analysis. We deal both with condition numbers of Linear Algebra and with condition numbers for systems of multivariate polynomial equations. For instance, we show that for any w> 1 and for any n× n rational matrix M of bit length O(n^ 4 n) + w, the condition number k(M) satisfies k(M) ≤ w n^ 5 / 2 with probability at least 1 - 2 w^- 1. Similar estimates are shown for the condition number μ_norm of M. Shub and S. Smale when applied to systems of multivariate homogeneous polynomial equations of bounded input length. Finally we apply these techniques to show the probability distribution of the <b>precision</b> (<b>number</b> of bits of the denominator) required to write down approximate zeros of affine systems of multivariate polynomial equations of bounded input length. Comment: 54 page...|$|E
40|$|A quad-double {{number is}} an unevaluated sum of four IEEE double <b>precision</b> <b>numbers,</b> capable of {{representing}} at least 212 bits of significand. We present the algorithms for various arithmetic operations (including the four basic operations and various algebraic and transcendental operations) on quad-double numbers. The {{performance of the}} algorithms, implemented in C++, is also presented. 1...|$|R
40|$|This paper {{presents}} a memory-efficient auxiliary data structure for ray tracing called a lightweight bounding volume hierarchy, or LBVH. The new data structure reduces memory requirements in three ways: using implicit indexing, limited <b>precision</b> <b>numbers,</b> {{and a high}} branching factor. We show that LBVHs can be nearly as effective as standard bounding volumes in terms of speed while using significantly less memory. ...|$|R
30|$|<b>Precision</b> (P): <b>number</b> of {{correctly}} fixed errors {{divided by}} the total number of errors detected.|$|R
40|$|Abstract. In {{the context}} of methodologies {{intended}} to confer robustness to geometric algorithms, we elaborate on the exact-computation paradigm and formalize the notion of degree of a geometric algorithm as a worst-case quantification of the <b>precision</b> (<b>number</b> of bits) to which arithmetic calculation have to be executed in order to guarantee topological correctness. We also propose a formalism for the expeditious evaluation of algorithmic degree. As an application of this paradigm and an illustration of our general approach where algorithm design is driven also by the degree, we consider the important classical problem of proximity queries in two and three dimensions and develop a new technique for the efficient and robust execution of such queries based on an implicit representation of Voronoi diagrams. Our new technique offers both low degree and fast query time and for 2 D queries is optimal with respect to both cost measures of the paradigm, asymptotic number of operations, and arithmetic degree...|$|E
30|$|An {{electrocardiogram}} (ECG) signal, {{which is}} a graphical display of the electrical activity of the heart, {{is one of the}} essential biological signals for the monitoring and diagnosis of heart diseases. ECG signals recorded by the digital equipments are most widely used in the applications such as monitoring, cardiac diagnosis, real-time transmission over telephone networks, patient databases and long-term recording. Some key parameters such as the sampling rate, sampling <b>precision,</b> <b>number</b> of leads and recording time {{play an important role in}} the increase of the amount of data collected from an ECG signal. Evidently, when continuously generating the huge amount of ECG data, in order to be able to process these data, we need the proper equipments that have the high storage capacity. On the other hand, when the equipments are used in the remote monitoring activities, they must have the wide transmission band. Therefore, in order to achieve removing the redundant information from the ECG signal with retaining all clinically significant features including P-wave, QRS complex and T -wave [1, 2], we need to employ an effective ECG compression algorithm.|$|E
40|$|Performing {{computations}} with a low-bit number representation {{results in}} a faster implementation that uses less silicon, and hence allows an algorithm to be implemented in smaller and cheaper processors without loss of performance. We propose a novel formulation to efficiently exploit the low (or non-standard) <b>precision</b> <b>number</b> representation of some computer architectures when computing the solution to constrained LQR problems, {{such as those that}} arise in predictive control. The main idea is to include suitably-defined decision variables in the quadratic program, in addition to the states and the inputs, to allow for smaller roundoff errors in the solver. This enables one to trade off the number of bits used for data representation against speed and/or hardware resources, so that smaller numerical errors can be achieved for the same number of bits (same silicon area). Because of data dependencies, the algorithm complexity, in terms of computation time and hardware resources, does not necessarily increase despite the larger number of decision variables. Examples show that a 10 -fold reduction in hardware resources is possible compared to using double precision floating point, without loss of closed-loop performance...|$|E
5000|$|Holographic {{principle}} and Bekenstein bound, which prohibit unlimited <b>precision</b> real <b>numbers</b> {{in the physical}} universe ...|$|R
5000|$|Note that [...] is {{a string}} and [...] {{is a single}} <b>precision</b> floating-point (<b>number).</b> They are {{separate}} variables.|$|R
40|$|Later, they [5] provide another method, named static-analysis techniques, {{to reduce}} the cost of exact integer arithmetic. Another {{research}} direction is to investigate how {{to reduce the}} error when using fixedlength floating-point arithmetic. Priest [15] presents algorithms to compensate errors in the floating-point operations and applies them to computing the intersection of a line and a line segment. This method is reliable, but in some cases the time cost is very expensive. Recently, Gavrilova and Rokne [6] give a method that converts the problem of line segment intersection testing into calculating signs of several dot product summations. If those signs could be computed exactly, then the testing result is reliable. In their method, signs are obtained by ESSA (Exact Sign of Sum Algorithm) [18] method, which can exactly calculate the sign of a sum of n floating-point numbers. When the coordinates of the ending points are given with single <b>precision</b> floating-point <b>numbers,</b> their method uses a double <b>precision</b> floating-point <b>number</b> to exactly calculate the product of two single <b>precision</b> <b>numbers.</b> However, the double precision is the highest precision floating-point arithmetic available in typica...|$|R
40|$|AbstractEmbedded CPUs {{typically}} use {{much less}} power than desktop or server CPUs but provide limited or no support for floating-point arithmetic. Hybrid reconfigurable CPUs combine fixed and reconfigurable computing fabrics to balance better execution performance and power consumption. We show how a Stretch S 6 hybrid reconfigurable CPU (S 6) {{can be extended}} to natively support double precision floating-point arithmetic. For lower <b>precision</b> <b>number</b> formats, multiple parallel arithmetic units can be implemented. We evaluate if the superlinear performance improvement of floating-point multiplication on reconfigurable fabrics can be exploited {{in the framework of}} a hybrid reconfigurable CPU. We provide an in-depth investigation of data paths to and from the S 6 reconfigurable fabric and present peak and sustained throughput as a function of wide registers used and total operand size. We demonstrate the effect of the given interface when using a floating-point fused multiply-accumulate (FMA) SIMD unit to accelerate the LINPACK benchmark. We identify a mismatch between the size of the S 6 s reconfigurable fabric and the available interface bandwidth as the major bottleneck limiting performance which makes it a poor choice for scientific workloads relying on native support for floating-point arithmetic...|$|E
40|$|In {{the work}} {{reported}} here, we describe a phonetic spell-checking algorithm, Phonetex which integrates aspects of Soundex and its extension Phonix. It {{is designed to}} provide a phonetic component for an existing typographic spell checker. We increase the number of letter codes compared to Soundex and Phonix. We also integrate phonetic rules but use far less than Phonix which was designed for South African name matching or Rogers and Willett's Phonix extension which was designed for 17 th century spellings as these includes many rules that are redundant in a contemporary word-based domain. We evaluate our algorithm by comparing it to phonetic spell checkers, Soundex and Editex and four benchmark spell checkers (Agrep, MS Word 97 & 2000 and UNIX `ispell') using a list of phonetic spelling errors. We nd that our approach has superior recall (accuracy) to the alternative approaches although the higher recall is at the expense of <b>precision</b> (<b>number</b> of possible matches retrieved). We intend to integrate it into an existing spell checker so the precision will be improved by integration thus high recall is the aim for our approach in this paper. Keywords: Data Cleaning, Phonetic Spell Checker, Phonetic Code Generation. ...|$|E
40|$|We {{are proposing}} an {{algorithm}} for tracing polylines that are oriented by a direction field defined on a triangle mesh. The {{challenge is to}} ensure that two such polylines cannot cross or merge. This property is fundamental for mesh segmentation and is impossible to enforce with existing algorithms. The core of our contribution is to determine how polylines cross each triangle. Our solution is inspired by EdgeMaps where each triangle boundary is decomposed into inflow and outflow intervals such that each inflow interval is mapped onto an outflow interval. To cross a triangle, we find the inflow interval that contains the entry point, and link it to the corresponding outflow interval, with the same barycentric coordinate. To ensure that polylines cannot merge or cross, we introduce a new direction field representation, we resolve the inflow/outflow interval pairing with a guaranteed combinatorial algorithm, and propagate the barycentric positions with arbitrary <b>precision</b> <b>number</b> representation. Using these techniques, two streamlines crossing the same triangle cannot merge or cross, but only locally overlap when all streamline extremities are located on the same edge. Cross-free and merge-free polylines can be traced on the mesh by iteratively crossing triangles. Vector field singularities and polyline/vertex crossing are characterized and consistently handled...|$|E
50|$|The MI25 is a Vega based card, {{utilizing}} HBM2 memory. The MI25 {{performance is}} expected to be 12.3 TFLOPS using FP32 numbers. In contrast to the MI6 and MI8, the MI25 is able to increase performance when using lower <b>precision</b> <b>numbers,</b> and accordingly {{is expected to}} reach 24.6 TFLOPS when using FP16 numbers. The MI25 is rated at <300W TDP with passive cooling. The MI25 also provides 768 GFLOPS peak double precision (FP64) at 1/16th rate.|$|R
40|$|International audienceWe {{describe}} {{algorithms used}} {{to optimize the}} GNU MPFR library when the operands fit into one or two words. On modern processors, a correctly rounded addition of two quadruple <b>precision</b> <b>numbers</b> is now performed in 22 cycles, a subtraction in 24 cycles, a multiplication in 32 cycles, a division in 64 cycles, and a square root in 69 cycles. We also introduce a new faithful rounding mode, which enables even faster computations. Those optimizations {{will be available in}} version 4 of MPFR...|$|R
40|$|The {{development}} of processors {{has given rise}} to problems that need more than double precision arithmetic. Some of them are known to require very long multiple <b>precision</b> <b>numbers,</b> but for some others, doubling the available precision to reach about 100 bits is sufficient. We propose an insight on the {{development of}} a library for the exponential function. Since the hardware is able to perform all the arithmetic operations on 53 bits, our exponential has to be based on a polynomial or a rational approximation. Our routine...|$|R
