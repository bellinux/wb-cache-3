47|84|Public
2500|$|Any {{physically}} meaningful equation (and likewise any {{inequality and}} inequation) {{will have the}} same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a <b>plausibility</b> <b>check</b> on derived equations and computations. [...] It also serves as a guide and constraint in deriving equations that may describe a physical system {{in the absence of a}} more rigorous derivation.|$|E
50|$|CubETH is a {{technology}} demonstration mission. The first {{goal is to}} prove that low cost receivers {{can be used for}} positioning in space. The orbit as well as the attitude of the spacecraft will be computed on-board the satellite and send to a station on earth. The post-processing of the data will allow a first <b>plausibility</b> <b>check.</b> In order to fulfil an external validation, CubETH will be equipped with satellite laser ranging reflectors for range measurements from ground stations.|$|E
40|$|The {{evaluated}} {{concept of}} echocardiographic dyssynchrony investigation with 100 subjects consisting of combined TDI and TSI with their <b>plausibility</b> <b>check</b> {{has been successfully}} applied in patients with LBBB. The condition is given to test this method in a multicenter study to determine scientifically whether our approach really improves dyssynchrony diagnostics...|$|E
5000|$|Data exchanges {{are carried}} out via the {{communications}} link and <b>plausibility</b> <b>checks.</b>|$|R
40|$|Latest {{technological}} developments have enabled {{the evolution of}} new vehicle systems which allow the implementation of new functions or the realisation of known functions with alternative actuation systems. The rising complexity of such systems seriously affects the safety demands on these systems. The following contribution describes a <b>plausibility</b> <b>checking</b> system and a method to define the requirements to <b>plausibility</b> <b>checking</b> systems {{in order to achieve}} required safety demands for new vehicle systems...|$|R
40|$|Software fault {{tolerance}} demands additional tasks like error detection and recovery through executable assertions, exception handling, diversity and redundancy based mechanisms. These mechanisms {{do not come}} for free, rather they introduce additional complexity to the core functionality. This paper presents light weight error detection and recovery mechanisms based on {{the rate of change}} in signal or data values. Maximum instantaneous and mean rates are used as <b>plausibility</b> <b>checks</b> to detect erroneous states and recover. These <b>plausibility</b> <b>checks</b> are exercised in a novel aspect oriented software fault tolerant design framework that reduces the additional logical complexity. A Lego �XT Robot based case study has been completed to demonstrate the effectiveness of the proposed design framework...|$|R
30|$|If {{there is}} {{disagreement}} {{or lack of}} conclusiveness, e.g., about the probability of harm occurring, a <b>plausibility</b> <b>check</b> of the available data should be performed. The plausibility should be decided {{on the basis of}} scientific criteria that are recognised by the scientific community. Theories or hypotheses must explain a specific phenomenon and be testable, fulfil coherence requirements, and satisfy the principle of organised scepticism (for instance, through peer review). To conduct this check in accordance with scientific criteria, it is necessary to have complete access to the information that has led to the formulation of the scientific theory. The data must be presented in an understandable manner and include any information that does not support the scientific theory. To ensure that the <b>plausibility</b> <b>check</b> is carried out in an unbiased manner and according to scientific criteria, the scientific institutions must be independent.|$|E
40|$|This {{document}} {{describes an}} {{implementation of a}} program which does an extensive consistency and <b>plausibility</b> <b>check</b> {{on a set of}} C program files. This may lead to warnings which help the programmer to debug the program, to remove useless code and to improve his style. The program has been used to test itself and has found bugs in sources of some heavily used code...|$|E
30|$|Another {{important}} {{issue that has}} not been addressed in this work is to obtain a confidence measure giving indication about the reliability of the inferred crowd density. It may be that due to a small percentage of users compared to the total number of attendees, the inferred crowd density may even become null. Hereby, a <b>plausibility</b> <b>check</b> e.g. by comparing the active number of users to a roughly estimated number of attendees by the security personnel could give confidence about the inferred crowd density.|$|E
50|$|More than {{a decade}} later spammers started to abuse this flaw in post-1123 SMTP, today most mails are spam and most reverse paths are forged. Note that spammers {{typically}} forge working reverse paths, many MTAs reject mail if callback verification or other <b>plausibility</b> <b>checks</b> fail for the reverse path.|$|R
30|$|All {{data were}} {{collected}} through a web based data acquisition system previously used in other observational studies (Cutlip et al. 2007; Wöhrle et al. 2012; Zeymer et al. 2014). National principal investigators {{were responsible for the}} accuracy of their datasets and performed source data verification when the routinely performed web based <b>plausibility</b> <b>checks</b> indicated discrepancies.|$|R
40|$|This paper {{estimates}} {{annual data}} on educational attainment for 3, 076 mainland U. S. counties 1991 - 2005. Being estimated {{without resorting to}} ancillary information, this data is suited particular well for panel regression analyses. Several <b>plausibility</b> <b>checks</b> indicate that the data is fairly reliable and yields plausible parameter estimates in a panel regression...|$|R
40|$|This paper {{introduces}} a time {{series of the}} Dutch capital stock for 1900 - 1995. The estimates were derived using the perpetual inventory method. To enhance international comparability, we followed Maddison's standardized methodology. To ensure transparency a thorough description of sources and methods is given. A <b>plausibility</b> <b>check</b> is performed by comparing our results with the stylized facts of Dutch {{economic growth in the}} twentieth century. It is concluded that the new data fit other available evidence for Dutch macroeconomic development better than previously used. ...|$|E
40|$|Visualisations can highly {{contribute}} to the importance and authority of new ideas, concepts, and knowledge claims. Among the many visualisations, few become well-known and influential in environmental governance. Whilst these have been objects of specific research, this study questions what constitutes and underpins their influence. For this, the paper codifies influential visualisations and defines criteria for studying their visual characteristics. The criteria are applied to two case studies, the “traffic light” and the “planetary-boundaries” diagrams. To increase {{the validity of the}} findings, the study also introduces two “failure cases” as <b>plausibility</b> <b>check...</b>|$|E
30|$|Before {{utilizing}} these data, we subjected {{them to a}} <b>plausibility</b> <b>check.</b> For example, {{the data}} processing performed {{as part of the}} ranking was checked {{on the basis of the}} raw data provided. In addition to minor corrections, in particular of the number of personnel, above all an error in the internationally visible publications was corrected. 269 international publications were erroneously assigned to LMU Munich. However, only 38 publications could actually be found in the Web of Science database for the period under consideration. We therefore only made use of these corrected data sets in our analyses.|$|E
30|$|The MYPTAS object {{describes}} a pTAS distribution with parameterization P_P(α,γ,θ)= (0.5, 1, 1.5). The PTAS function automatically performs some <b>plausibility</b> <b>checks</b> on the given parameters and translates the given parameterization (i.e. according to Palmer et al. (2008) in this case) {{to the other}} ones. In addition, distribution figures such as mean, variance, skewness and kurtosis are calculated.|$|R
40|$|An ad hoc {{network is}} a {{collection}} of mobile nodes that dynamically form a temporary network and are infrastructure less. Black hole attack is an attack in network layer which degrades the network performance by dropping packets. This paper identifies black hole attack against Optimized Link State Routing (OLSR) protocols, one of the four standard routing protocols for MANETs. We used Topology Graph Based Anomaly Detection (TOGBAD) a new centralized approach using topology graphs to identify nodes attempting to create a black hole. It is used to gain knowledge about the network topology and use this knowledge to perform <b>plausibility</b> <b>checks</b> of the routing information propagated by the nodes in the network. When a node generates fake (malicious) routing information or when <b>plausibility</b> <b>checks</b> fail, an alarm is triggered. This paper gives the corresponding simulation results in NS 2. It also demonstrates detection process when the attempt to create a black hole before the actual impact occurs...|$|R
30|$|Thin strut cobalt {{chromium}} BMS implantation in a priori pre-defined subgroups {{was investigated in}} a non-randomized, international, multi-center ‘all-comers’ observational study. Primary end-point was the 9 -month clinically driven target lesion revascularization (TLR) rate. Secondary end-points included the 9 -month major adverse cardiac event (MACE) and procedural success rates. Data collection was done using an established electronic data acquisition form with built-in <b>plausibility</b> <b>checks.</b>|$|R
40|$|Abstract—The {{new idea}} of this {{research}} is application of a new fault detection and isolation (FDI) technique for supervision of sensor networks in transportation system. In measurement systems, it is necessary to detect all types of faults and failures, based on predefined algorithm. Last improvements in artificial neural network studies (ANN) led to using them for some FDI purposes. In this paper, application of new probabilistic neural network features for data approximation and data classification are considered for <b>plausibility</b> <b>check</b> in temperature measurement. For this purpose, two-phase FDI mechanism was considered for residual generation and evaluation. Keywords—Fault detection and Isolation, Neural network, Temperature measurement, measurement approximation and classification. I...|$|E
40|$|This paper {{presents}} the {{hardware and software}} design of a battery monitoring circuit developed {{to be used in}} aviation applications employing lithium-ion batteries in their electrified powertrain. The considered aircraft is a manned sailplane able to take-off and climb by using its electric propulsion system supplied by NCA/Graphite lithium-ion batteries. The battery monitoring electronics was developed with the highest considerations in terms of fail-safe and fail-operational requirements. The electronic design of the battery monitoring circuit integrates the battery busbars and uses new passive balancing components with an innovative busbar cooling solution, thus increasing the reliability and the robustness of the whole battery system. The software also contributes to the high safety level by employing crosscheck and <b>plausibility</b> <b>check</b> mechanisms...|$|E
40|$|This comment {{describes}} the actions {{taken on a}} report of a potential error in the units of indicative values of BCR- 723. As raw data were no longer available, we confirmed the correctness of this claim by a <b>plausibility</b> <b>check.</b> We investigated the possible {{extent of the problem}} and informed customers who bought the material. Finally, we highlight differences between the organization of production of certified reference materials in the Community Bureau of Reference (BCR) and Standards, Measurements and Testing (SMT) Programs and the current system at the European Commission’s Institute for Reference Materials and Measurements (IRMM), and that should minimize the risk of re-occurrence of such problems. JRC. DG. D. 2 -Reference material...|$|E
50|$|Prior {{attempts}} at integrating static and dynamic typing {{tried to make}} the dynamic type be both {{the top and bottom of}} the subtype hierarchy. However, because subtyping is transitive, that results in every type becoming related to every other type, and so subtyping would no longer rule out any static type errors. The addition of a second phase of <b>plausibility</b> <b>checking</b> to the type system did not completely solve this problem.|$|R
40|$|Due to the {{complexity}} of studies with peripheral artery disease in the past, data clear-ing for such studies lasted intolerably long. This couldpartly be improved by increas-ing on-site monitoring frequency and duration. In order to improve data quality and reduce time for data clearing further, a computer-assistedplausibility check program was establishedprior to initiating a multicenter, multinational study concept (PART-NER concept) (I). In four completed studies (total N = 512 in 36 centers all over Europe) the num-ber of correction lists and inquiries due to open questions after completion of the recruitment period could be remarkably lowered in comparison to previous studies where no computer-assistedplausibility check was employed. The number of missing values was reduced to a minimum (100 % of the data with regard to both primary endpoints were available). The number of correction sheets that had to be filled in by the investigator was 3. 4 sheets per patient in the study with early <b>plausibility</b> <b>checks,</b> whereas the number was 5. I sheets per patient in the study where <b>plausibility</b> <b>checks</b> were started later (the average of items to be corrected was I 0 in the study with earl...|$|R
5000|$|EEX {{operates}} [...] "Transparency in Energy Markets”, {{the neutral}} platform for energy market data which fulfils the statutory publication requirements and implements the market participants’ voluntary commitments. The platform {{was established by}} EEX and the four German transmission system operators and launched in October 2009. In 2011, the Austrian transmission system operator Austrian Power Grid AG joined the cooperation. EEX {{is in charge of}} the operation of the platform, which comprises <b>plausibility</b> <b>checking,</b> anonymisation, aggregation and publication of the data reported.|$|R
40|$|Today’s {{power grids}} must utilize {{more energy than}} they were {{originally}} planned for. Hence, they are slowly reaching their stability limits and that causes {{an increased risk of}} blackouts in electrical networks. To avoid such critical situations in the control room the system dynamic state must be assessed as quickly as possible and its personnel have to react to it early enough. This kind of evaluation could be done by the so-called Dynamic Security Assessment system which is able to assess the dynamic state of the electrical network online. This paper presents completely automatic methods of the dynamic security assessment system integration into a power grid, functionality testing of the Over Excitation Limiter (OXL) - which is implemented in the Automatic Voltage Regulator (AVR), and the electrical grid <b>plausibility</b> <b>check...</b>|$|E
40|$|An {{important}} goal of smart home automation {{is to improve}} the user’s comfort and security along with a reduced overall energy consumption. A core element of this strategy is an automated decentralised indoor climate control system. The subjective perception of indoor air quality and thermal comfort in rooms is influenced by a large number of different physical parameters, the most important being the mean air room temperature, the relative air humidity, the mean air velocity, and the CO 2 concentration. For a comprehensive indoor climate monitoring and control system, multi-gas sensor arrays and person detection sensors are required. In the paper, the features of a flexible KNX based distributed sensor network, including stationary multi-gas sensor modules and wearable wireless devices, is described. Important issues for smart sensor design like self-monitoring, <b>plausibility</b> <b>check</b> and model-based self-calibration abilities have been especially devoted to. An outlook for a perspective sensor miniaturization is given...|$|E
40|$|The {{analysis}} of pyrogenic carbon (PyC) in environmental samples {{is of great}} interest, e. g. for carbon cycle assessment, (bio-) char characterization and palaeo-environmental or archeological reconstruction. Here, an HPLC method (HPLC) is presented that reproducibly quantifies benzene polycarboxylic acids (BPCA) as molecular markers for PyC in various kinds of environmental samples. It operates at low pH without requiring an organic modifier and was thoroughly tested with PyC reference materials and a peatland core {{that served as a}} feasibility and <b>plausibility</b> <b>check.</b> Compared to the established gas chromatography (GC) method, the HPLC method results in higher BPCA quantification reproducibility by showing a significantly smaller coefficient of variation (HPLC: 5 %, GC: 16 – 23 %). It works well with small sample amounts, as for instance from sediment cores and aerosol collectors, and requires less sample preparation work than the GC method. Moreover, the here presented HPLC method facilitates 13 C and 14 C analyses on PyC from environmental samples...|$|E
30|$|LDA {{models were}} firstly applied on the Europe-wide dataset of moss {{sampling}} sites {{with information on}} land use density around the sampling sites. Model-specific error rates (%) were calculated by means of confusion matrix values (actual vs. predicted values). Charts for the linear discriminant functions were used for <b>plausibility</b> <b>checks.</b> Logistic regression models were built using the same predictors from the LDA models. Confusion matrices and error rates (%) specified for each LR model were calculated and compared with the statistical characteristics of the LDA models.|$|R
40|$|Abstract. We {{present a}} general vision-based method for reconstructing {{multiple}} unknown objects (e. g. humans) within a known environment (e. g. tables, racks, robots) which usually has occlusions. These occlusions {{have to be}} explicitly considered since parts of the unknown objects might be hidden in some or even all camera views. In order to avoid cluttered reconstructions, <b>plausibility</b> <b>checks</b> are used to eliminate reconstruction artifacts which actually do not contain any unknown object. One application is a supervision/surveillance system for safe human/robot-coexistence and – cooperation. Experiments for a voxel-based implementation are given. ...|$|R
50|$|The European Severe Weather Database (ESWD) {{collects}} and verifies {{reports on}} dust, sand- or steam devils, tornado sightings, gustnados, large hail, heavy rain and snowfall, severe wind gusts,damaging lightning strikes and avalanches all over Europe {{and around the}} Mediterranean. The ESWD {{is the most important}} database for such events in Europe. Everybody is welcome to report extreme weather observations. Each report undergoes a quality control and each event is flagged either as received (QC0), <b>plausibility</b> <b>checked</b> (QC0+), report confirmed by other observer (QC1) or as fully verified by trusted source (QC2).|$|R
40|$|Abstract — Position {{estimation}} {{is one of}} {{the major}} challenges of sensor nodes in wireless sensor networks. By utilizing the information of messages of some pre-deployed location aware nodes, called beacons, and signal strength based distance estimation, a location unaware node is able to estimate its position. In the recent years, several localization methodologies have been developed. Due to imprecise distance estimations via received signal strength utilization, the localization accuracies of these algorithms differ, depending on the scenario conditions. In the proposed work, we developed an algorithmic approach to improve the localization accuracy of a least squares approach via two strategies. On the one hand, we tackle the imprecise distance measurements with a preceding <b>plausibility</b> <b>check.</b> On the other hand, we evaluate the achieved accuracy and improve the result by weaving the result of adaptive weighted centroid localization into a hybrid localization. The achieved localization accuracy beats the performance of the preceding approaches in all investigated scenarios, particularly for low beacon densities. Keywords- Wireless Sensor Networks, Localization, Log-normal Fading. I...|$|E
40|$|In Vehicular Ad-Hoc Networks (VANETs), the {{exchange}} of location data (i. e. absolute position, heading, time) for traffic safety applications plays an important role. The trustworthiness of this information is crucial as false data affects applications heavily and might endanger human lives. Beside cryptographic solutions that ensure sender authenticity and message integrity, the data <b>plausibility</b> <b>check</b> is an important mechanism to ensure positional reliability. In this paper, we show that a particle filter is an appropriate instrument to perform plausibility checks {{in order to assess}} the trustworthiness of neighbor nodes. Our approach allows the aggregation of information from different data sources directly in one particle filter per neighbor. Thus, dependencies and relationships between individual sources can be fully accounted for and the framework is easily extensible and scales well. The concept is implemented as a Java-OSGi bundle for a field operational test framework and evaluated using both manually generated traces and recorded data from real vehicle trips. We show that the detection of several types of location-based attacks is possible under consideration of errors and system inherent deviations in sensor data...|$|E
40|$|Abstract — This work {{is part of}} a {{research}} activity aiming to improve and to optimize environmental parameters monitoring system. It is essential in order to preserve the quality, safety and shelf life of perishable products. The present study reports on the investigation a way to both <b>plausibility</b> <b>check</b> and energy management in a wireless sensor network established in a closed space container. It introduces a new technique to decrease the total power consumption due to measuring and transmitting data in a few desired sensor nodes (DSNs). They are either failed or inactive (Sleeping) sensor nodes. They can be deactivated by some of surrounding key sensor nodes (KSNs) due to reduce battery-consumption. A new technique of the model making to estimate temperature, relative humidity, and air flow as important environmental parameters (EPs) instead of the direct measurement and then assessment the validity of the proposed model using some experiments will be investigated. Introduced estimators use linear models between the KSNs and a DSN. These models can be extended for possible use in different applications such as EP-controllers in air conditioning systems as well as the estimator in fault recognition procedures. We can...|$|E
40|$|This {{contribution}} {{emerged as}} part of the collaborative project “Data network for better European organic market information” carried out in the 7 th Framework Programme of the EU. Up to now, organic market data collection has been inconsistent throughout European countries; data from different organisations and/or countries is hard to compare, because very different sampling methods, product categories, and nomenclatures have been used. Interpretations based on incomplete and inconsistent data might lead to wrong decisions and misinvestments of companies or policy divisions. The objective of this contribution is the identification of inconsistencies in organic market data which is currently available throughout Europe. Therefore <b>plausibility</b> <b>checks</b> were applied to data collected through a standardized survey in 39 countries. The inconsistencies that could be revealed were grouped according to data type and also according to data origin. The number of inconsistencies highly depends on the availability of data, which in turn depends on the country the data stems from. These inconsistencies often occur due to heterogeneous nomenclature and varying definitions of product categories. Inconsistencies in data from two subsequent years often occur because data from different sources was used. Further steps resulting from the outcome of the data <b>plausibility</b> <b>checks</b> are the compilation of a guideline for organic market data collectors, the exchange of opinions and experiences within the organic data network, and the revision of current organic market data reports...|$|R
40|$|To {{accommodate}} {{the growing demand}} for complexity in the automotive industry, an approach for a cross-process information model is introduced: the infrastructure presented aims at storing and relating key development data from the various software engineering phases. The model can be harmonized with proprietary development methodologies by means of metamodeling and serves {{as the starting point}} for the traceability of requirements, global <b>plausibility</b> <b>checks,</b> and (semi-) automatic transformation of model elements. Based on the medini tool chain, the model infrastructure is open with respect to the models and tools used thanks to the consistent deployment of OMG standards and the benefit of metamodeling...|$|R
40|$|This paper {{presents}} {{an approach to}} safe navigation of autonomous mobile systems within partially known or unknown dynamic environments. In this context, faulty, maladjusted and otherwise influenced sensors must be recognized (error detection), and adequate measures for failure correction (error recovery) must be taken. As a general basis for monitoring the state of environmental sensors, a so called "error detection model" was created, which consists of sub-models for data from laser range finders and ultrasonic sensors. With {{the aid of the}} created models, different kinds of redundancy can be utilized and consistency and <b>plausibility</b> <b>checks</b> can be carried out. The error detection model serves for failure recognition based on environment modeling and on hypotheses for expected sensor readings...|$|R
