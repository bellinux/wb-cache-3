3|10000|Public
5000|$|... list = 2, 3, 4, 5, 6, 7, 8, 9/* [...] * Non-zero {{numbers are}} coerced to true, {{so when it}} % 2 == 0 (even), it is false. * The type of the {{implicit}} [...] "it" [...] <b>parameter</b> <b>can</b> <b>be</b> <b>inferred</b> as an Integer by the IDE. * It could also be written as: * list.findAll { Integer i -> i % 2 } * list.findAll { i -> i % 2 } */def odds = list.findAll { it % 2 }assert odds == 3, 5, 7, 9 ...|$|E
40|$|The {{standard}} power {{utility function}} {{is widely used}} to explain asset prices. It assumes that the coefficient of relative risk aversion is the inverse of the elasticity of substitution. Here I use the Kihlstrom and Mirman (1974) expected utility approach to relax this assumption. I use time consistent preferences that lead to time consistent plans. In our examples, the past does not matter much for current portfolio decisions. The risk aversion <b>parameter</b> <b>can</b> <b>be</b> <b>inferred</b> from experiments and introspections about bets in terms of permanent consumption (wealth). Evidence about {{the change in the}} attitude towards bets over the life cycle may also restrict the value of the risk aversion parameter. Monotonic transformations of the standard power utility function do not change the predictions about asset prices by much. Both the elasticity of substitution and risk aversion play a role in determining the equity premium. Consumption smoothing, intertemporal elasticity of substitution, risk aversion, asset prices, equity premium...|$|E
40|$|In {{monitoring}} active volcanoes, the magma overpressure {{is one of}} the key parameters used in forecasting volcanic eruptions. This <b>parameter</b> <b>can</b> <b>be</b> <b>inferred</b> {{from the}} ground displacements measured on the Earth's surface by applying inversion techniques. However, in most studies, the huge amount of information about the behavior of the volcano contained in the temporal evolution of the deformation signal is not fully exploited by inversion. Our work focuses on developing a strategy in order to better forecast the magma overpressure using data assimilation. We take advantage of the increasing amount of geodetic data [i. e., Interferometric Synthetic Aperture Radar (InSAR) and Global Navigation Satellite System (GNSS) ] recorded on volcanoes nowadays together with the wide-range availability of dynamical models that can provide better understanding about the volcano plumbing system. Here, we particularly built our strategy on the basis of the Ensemble Kalman Filter (EnKF). We forecast the temporal behaviors of the magma overpressures and surface deformations by adopting a simple and generic two-magma chamber model and by using synthetic GNSS and/or InSAR data. We prove the ability of EnKF to both estimate the magma pressure evolution and constrain the characteristics of the deep volcanic system (i. e., reservoir size as well as basal magma inflow). High temporal frequency of observation is required to ensure the success of EnKF and the quality of assimilation is also improved by increasing the spatial density of observations in the near-field. We thus show that better results are obtained by combining a few GNSS temporal series of high temporal resolution with InSAR images characterized by a good spatial coverage. We also show that EnKF provides similar results to sophisticated Bayesian-based inversion while using the same dynamical model with the advantage of EnKF to potentially account for the temporal evolution of the uncertain model parameters. Our results show that EnKF works well with the synthetic cases and there is a great potential in using the method for real-time monitoring of volcanic unrest...|$|E
30|$|HV is {{ranging from}} 0.92 to 0.95. From the {{distribution}} of these polarization <b>parameters,</b> it <b>can</b> <b>be</b> <b>inferred</b> that these regions belong to the freezing level; and it {{is consistent with the}} identification results of the system.|$|R
30|$|HV is {{ranging from}} 0.95 to 1. From the {{distribution}} of these polarization <b>parameters,</b> it <b>can</b> <b>be</b> <b>inferred</b> that the precipitation particles in this region should be raindrops, and it {{is consistent with the}} classification results of the system.|$|R
40|$|We {{update the}} best {{constraints}} on {{fluctuations in the}} solar medium deep within the solar Radiative Zone to include the new SNO-salt solar neutrino measurements. We find that these new measurements are now sufficiently precise that neutrino oscillation <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> independently of any assumptions about fluctuation properties. Constraints on fluctuations are also improved, with amplitudes of 5 % now excluded at the 99 % confidence level for correlation lengths {{in the range of}} several hundred km. Because they are sensitive to correlation lengths which are so short, these solar neutrino results are complementary to constraints coming from helioseismology. Comment: 4 pages, LaTeX file using RevTEX 4, 6 figures include...|$|R
40|$|This paper {{presents}} a path planning technique for ground vehicles {{that accounts for}} {{the dynamics of the}} vehicle, the topography of the terrain and the wheel/ground interaction properties such as friction. The first two properties <b>can</b> <b>be</b> estimated using well known sensors and techniques, but the third is not often estimated even though it has a significant effect on the motion of a high-speed vehicle. We introduce a technique which allows the estimation of wheel slip from which frictional <b>parameters</b> <b>can</b> <b>be</b> <b>inferred.</b> We present simulation results which show the importance of modelling topography and ground properties and experimental results which show how ground properties <b>can</b> <b>be</b> estimated along a 350 m outdoor traverse...|$|R
40|$|Some {{statistical}} models are specified via a data generating process {{for which the}} likelihood function cannot be computed in closed form. Standard likelihood-based inference is then not feasible but the model <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> by finding the values which yield simulated data that resemble the observed data. This approach faces at least two major difficulties: The first difficulty is {{the choice of the}} discrepancy measure which is used to judge whether the simulated data resemble the observed data. The second difficulty is the computationally efficient identification of regions in the parameter space where the discrepancy is low. We give here an introduction to our recent work where we tackle the two difficulties through classification and Bayesian optimization...|$|R
30|$|The results above {{show that}} <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> and the correct model <b>can</b> <b>be</b> {{determined}} {{for the specific}} parameter choices used in the simulations. Here we explore the model selection accuracy for varying parameter values including the weight, stimulus dissimilarity, stimulus strength and number of spike trains. In the following analysis, we use the bursting response kernel, a mixture of two stochastic stimuli and the Fokker–Planck CDF method. To introduce a stimulus dissimilarity, a sinusoidal perturbation is added {{to one of two}} identical OU processes, S̃(t) = S(t) + asin(10 t), where t is measured in seconds and a is the perturbation size. To change the stimulus strength, the OU processes are linearly scaled using S̃(t) = bS(t) where b denotes the scaling size.|$|R
40|$|The Cosmic Microwave Background Radiation (CMBR) holds {{information}} about {{almost all the}} fundamental cosmological parameters, and by performing a likelihood analysis of high precision CMBR fluctuation data, these <b>parameters</b> <b>can</b> <b>be</b> <b>inferred.</b> However, this analysis relies on assumptions about the initial power spectrum, which is usually taken to be a featureless power-law, P(k) ~ k^{n_s- 1 }. Many inflationary models predict power spectra with non-power law features. We discuss the possibility for detecting such features by describing the power spectrum as bins in k-space. This method for power spectrum reconstruction is demonstrated in practise by performing likelihood optimization on synthetic spectra, and the difficulties arising from reconstructing smooth features using discontinuous bins are discussed in detail. Comment: 10 pages, 6 figs, mathes version to appear in PR...|$|R
40|$|Recently nonparametric {{functional}} {{model with}} functional responses {{has been proposed}} within the functional reproducing kernel Hilbert spaces (fRKHS) framework. Motivated by its superior performance and also its limitations, we propose a Gaussian process model whose posterior mode coincide with the fRKHS estimator. The Bayesian approach has several advantages compared to its predecessor. Firstly, the multiple unknown <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> together with the regression function in a unified framework. Secondly, as a Bayesian method, the statistical inferences are straightforward through the posterior distributions. We also use the predictive process models adapted from the spatial statistics literature to overcome the computational limitations, thus extending the applicability of this popular technique to a new problem. Modifications of predictive process models are nevertheless critical in our context to obtain valid inferences. The numerical results presented demonstrate {{the effectiveness of the}} modifications...|$|R
40|$|This paper {{presents}} {{a new approach}} for determining the effectiveness of preventive generation rescheduling and shunt/series compensation in improving transient stability using trajectory sensitivity (TS) method. It is shown that the effectiveness of changes in <b>parameter</b> values <b>can</b> <b>be</b> judged by evaluating the TS of certain variables with respect to clearing time. Therefore, the need to evaluate a host of sensitivities with respect to many parameters is obviated. This heuristic is motivated {{by the fact that}} for simple models and small changes, controllability of trajectories using certain <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from observability in certain output signals. An example of such a parameter and signal pair is shunt susceptance at a bus and the square of voltage at that bus, respectively. Case studies are carried out on a 10 -machine 39 -bus system with detailed models and 17 -machine system to validate this method. IEE...|$|R
40|$|We discuss {{two views}} on {{extending}} existing methods for complex network modeling which we dub the communities {{first and the}} networks first view, respectively. Inspired by the networks first view that we attribute to White, Boorman, and Breiger (1976) [1], we formulate the multiple-networks stochastic blockmodel (MNSBM), which seeks to separate the observed network into subnetworks of different types and where the problem of inferring structure in each subnetwork becomes easier. We show how this model is specified in a generative Bayesian framework where <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> efficiently using Gibbs sampling. The result is an effective multiple-membership model without the drawbacks of introducing complex definitions of "groups" and how they interact. We demonstrate results on the recovery of planted structure in synthetic networks and show very encouraging results on link prediction performances using multiple-networks models {{on a number of}} real-world network data sets...|$|R
40|$|Omnidirectional video enables direct {{surround}} immersive {{viewing of}} a scene by warping the original image into the correct perspective given a viewing direction. However, novel views from viewpoints off the camera path <b>can</b> only <b>be</b> obtained if we solve the 3 D motion and calibration problem. In this paper we address {{the case of a}} parabolic catadioptric camera – a paraboloidal mirror in front of an orthographic lens – and we introduce a new representation, called the circle space, for points and lines in such images. In this circle space, we formulate an epipolar constraint involving a 4 x 4 fundamental matrix. We prove that the intrinsic <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> in closed form from the 2 D subspace of the new fundamental matrix from two views if they are constant or from three views if they vary. Three dimensional motion and structure <b>can</b> then <b>be</b> estimated from the decomposition of the fundamental matrix...|$|R
40|$|A {{theory is}} {{presented}} for {{the mechanics of}} the left ventricle. A linear continuum description of the myocardium is developed, which incorporates anisotropic elastic effects due to the fiber direction field. The relation between fiber tension and fiber strain contains a time-dependent activation function that drives the ventricle around its cycle. The theory is applied to a simplified geometry consisting of a thick-walled finite cylinder in which fibers spiral on helical paths and terminate on planar end surfaces. The helix pitch angle varies continuously through the wall. The ventricular cycle is analyzed by specifying the pressures at which the aortic and mitral valves open and close. Key quantities are tabulated which permit a simple determination of the properties of the model under changes of wall thickness, fiber angles, muscle parameters, preload, afterload, etc. It is shown how the active muscle <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from a measurement of the end systolic pressure-volume line...|$|R
40|$|Parameter {{estimation}} is {{a challenging}} problem for biological systems modelling since {{the model is}} normally of high dimension, the measurement data are sparse and noisy {{and the cost of}} experiments high. Accurate recovery of parameters depends on {{the quality and quantity of}} measurement data. It is therefore important to know which measurements to be taken when and how through optimal experimental design (OED). In this paper a method was proposed to determine the most informative measurement set for parameter estimation of dynamic systems, in particular biochemical reaction systems, such that the unknown <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> with the best possible statistical quality using the data collected from the designed experiments. System analysis using matrix theory was used to examine the number of necessary measurement variables. The priority of each measurement variable was determined by optimal experimental design based on Fisher information matrix (FIM). The applicability and advantages of the proposed method were shown through an example of signal pathway model...|$|R
40|$|The {{long-term}} goals {{of this research}} are to combine state-of-the-art remote sensing and in situ measurements with advanced numerical modeling (a) to characterize coherent structures in river and estuarine flows and (b) {{to determine the extent}} to which their remotely sensed signatures <b>can</b> <b>be</b> used to initialize and guide predictive models. OBJECTIVES Coherent structures are generated by the interaction of the flow with bathymetric and coastline features. These coherent structures produce surface signatures that <b>can</b> <b>be</b> detected and quantified using remote sensing techniques. Furthermore, a number of relationships between coherent structures and flow characteristics have been suggested that have the potential to allow flow parameters (e. g. mean velocity, bottom roughness, shear, and turbidity) to <b>be</b> <b>inferred</b> from remote measurements. The objectives are to test the following four hypotheses: 1. Flow <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from remotely sensed signatures of coherent structures. 2. Numerical models <b>can</b> <b>be</b> constrained with these inferred parameters. 1 3. The effect of stratification on the strength of coherent structures <b>can</b> <b>be</b> used to detect th...|$|R
5000|$|Linear {{regression}} <b>can</b> also <b>be</b> used to numerically assess {{goodness of}} fit and estimate {{the parameters of the}} Weibull distribution. The gradient informs one directly about the shape parameter [...] and the scale <b>parameter</b> [...] <b>can</b> also <b>be</b> <b>inferred.</b>|$|R
40|$|The {{function}} of the resource allocation sub-model within the IIASA Health Care System model is to simulate how the HCS allocates limited supplies of resources between competing demands. The principal outputs of the sub-model are the numbers of patients treated, in different categories, and the modes and quotas of treatment they receive. The Mark 2 version of the sub-model described in this paper simulates the allocation of many resources within one mode of treatment. It uses the same main assumption as used in the Mark 1 version previously reported; namely that in allocating its resources the HCS attempts to optimise a utility function whose <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from data on past allocations. Depending upon the type of data that is availabledifferent procedures for parameter estimation are required. This paper analyses estimation procedures which use historical allocation data directly. Both these procedures and the solution algorithm have been realized in a small computer program which <b>can</b> <b>be</b> readily installed on most scientific computer installations. The use of the sub-model is illustrated by three hypothetical applications using hospital data...|$|R
40|$|The {{vertical}} directionality of {{ambient noise}} is {{strongly influenced by}} seabed reflections. Therefore, potentially, geoacoustic <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> by inversion of the noise. In this approach, using vertical array measurements, the reflection loss is found directly by comparing the upward- with the downward-going noise. Theory suggests that this simple ratio is, in fact, the power reflection coefficient—potentially a function of angle and frequency. Modeling and parameter searching are minimized, and the method {{does not require a}} detailed knowledge of the noise source distribution. The approach can handle stratified environments and is believed to tolerate range dependence. Experimental data from five sites, four in the Mediterranean, one on the New Jersey Shelf, are described. Most of the Mediterranean sites had temporally varying noise directionality, yet yielded the same reflection properties, as one would hope. One site was visited in conditions of very low surface noise. This paper concentrates on an experimental demonstration of the feasibility of the method and data quality issues rather than automatic search techniques for geoacoustic parameter...|$|R
40|$|The TOPEX/Poseidon {{spacecraft}} {{was launched}} on August 10, 1992 {{to study the}} Earth's oceans. To achieve maximum benefit from the altimetric data collected, mission requirements dictate that TOPEX/Poseidon's orbit must be computed at an unprecedented level of accuracy. In order to satisfy these requirements, a model which accounts for the satellite's complex geometry, attitude, and surface properties has been developed. This `box-wing' representation treats the spacecraft as the combination of flat plates arranged {{in the shape of}} a box and a connecetd solar array. The nonconservative forces acting on each of the eight surfaces are computed independently, yielding vector accelerations which are summed to compute the total aggregate effect on the satellite center-of-mass. Parameters associated with each flat plate were derived from a finite element analysis of the spacecraft. Certain <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from tracking data and have been adjusted to obtain a better representation of the satellite acceleration history. Changes in the nominal mission profile and the presence of an `anomalistic' force have complicated this tuning process. Model performance, parameter sensitivities, and the `anomalistic' force will be discussed...|$|R
40|$|International audienceModels of isolation-by-distance formalize {{the effects}} of genetic drift and gene flow in a spatial context where gene {{dispersal}} is spatially limited. These models {{have been used to}} show that, at an appropriate spatial scale, dispersal <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from the regression of genetic differentiation against geographic distance between sampling locations. This approach is compelling because it is relatively simple and robust and has rather low sampling requirements. In continuous populations, dispersal <b>can</b> <b>be</b> <b>inferred</b> from isolation-by-distance patterns using either individuals or groups as sampling units. Intrigued by empirical findings where individual samples seemed to provide more power, we used simulations to compare the performances of the two methods in a range of situations with different dispersal distributions. We found that sampling individuals provide more power in a range of dispersal conditions that is narrow but fits many realistic situations. These situations were characterized not only by the general steepness of isolation-by-distance but also by the intrinsic shape of the dispersal kernel. The performances of the two approaches are otherwise similar, suggesting that the choice of a sampling unit is globally less important than other settings such as a study's spatial scale...|$|R
40|$|Statistics of the {{mechanical}} and failure properties on the grain scale are often assumed to follow the Weibull distribution in numerical simulations of failure and damage development. To investigate the microstructural basis for such a statistical model of compressive failure in a brittle rock, we consider the development of instability in a wing crack model and establish a methodology whereby the Weibull <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from microstructural data on microcrack density and length statistics for input into finite element simulations. Application of this methodology to six Yuen Long marble samples provides important insights into how different attributes of the microstructure may influence the progressive development of rock failure. The finite element simulations underscore the significant influence of microcrack length statistics, {{which has not been}} emphasized in continuum damage mechanics models that usually emphasize the roles of average crack size and crack density. The microstructural data indicate that strength heterogeneity increases with increasing grain size, and this {{plays a key role in}} lowering the uniaxial compressive strength, which contributes to the overall decrease of strength with increasing grain size. Department of Civil and Environmental Engineerin...|$|R
40|$|Background: Reaction-diffusion {{systems are}} {{frequently}} used in systems biology to model developmental and signalling processes. In many applications, count {{numbers of the}} diffusing molecular species are very low, leading {{to the need to}} explicitly model the inherent variability using stochastic methods. Despite their importance and frequent use, parameter estimation for both deterministic and stochastic reaction-diffusion systems is still a challenging problem. Results: We present a Bayesian inference approach to solve both the parameter and state estimation problem for stochastic reaction-diffusion systems. This allows a determination of the full posterior distribution of the parameters (expected values and uncertainty). We benchmark the method by illustrating it on a simple synthetic experiment. We then test the method on real data about the diffusion of the morphogen Bicoid in Drosophila melanogaster. The results show how the precision with which <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> varies dramatically, indicating that the ability to infer full posterior distributions on the <b>parameters</b> <b>can</b> have important experimental design consequences. Conclusions: The results obtained demonstrate the feasibility and potential advantages of applying a Bayesian approach to parameter estimation in stochastic reaction-diffusion systems. In particular, the ability to estimate credibility intervals associated with <b>parameter</b> estimates <b>can</b> <b>be</b> precious for experimental design. Further work, however, will be needed to ensure the method can scale up to larger problems. ...|$|R
40|$|Within {{the context}} of the IIASA Health Care System model the {{function}} of the resource allocation sub-model is to simulate how the HCS allocates limited supplies of resources between competing demands. The principal outputs of the sub-model should be the numbers of patients treated,in different categories, and the modes and standards of treatments they receive. The Mark 1 version of the sub-model is described in this paper. It simulates the allocation of one resource within one mode of treatment but {{it should be possible to}} use the approach to develop further versions to cover more general cases. The main assumption of the model is that in allocating its resources the HCS attempts to optimise a utility function whose <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from data on past allocations. Depending upon the type of data that is available different procedures for parameter estimation are required. The procedures for <b>parameter</b> estimation <b>can</b> <b>be</b> incorporated with the algorithm for solving the model into a computer programme whose main inputs consist solely of empirical data. The programme is fairly small and <b>can</b> readily <b>be</b> installed on most scientific computer installations. The use of the sub-model is illustrated by a hypothetical application using hospital data from England...|$|R
40|$|Gravitational-wave {{observations}} of compact binaries {{have the potential}} to uncover the distribution of masses and angular momenta of black holes and neutron stars in the universe. The binary components' physical <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from their effect on the phasing of the gravitational-wave signal, but a partial degeneracy between the components' mass ratio and their angular momenta limits our ability to measure the individual component masses. At the typical signal amplitudes expected by the Advanced Laser Interferometer Gravitational-wave Observatory (signal-to-noise ratios between 10 and 20), we show that it will in many cases be difficult to distinguish whether the components are neutron stars or black holes. We identify when the masses of the binary components could be unambiguously measured outside the range of current observations: a system with a chirp mass M 2. 786 must contain a black hole. However, additional information would be needed to distinguish between a binary containing two 1. 35 M_ neutron stars and an exotic neutron-star [...] black-hole binary. We also identify those configurations that could be unambiguously identified as black-hole binaries, and show how the observation of an electromagnetic counterpart to a neutron-star [...] black-hole binary could be used to constrain the black-hole spin. Comment: 5 pages, 4 figures. Final version to be published in Ap. J. Let...|$|R
40|$|The {{knowledge}} of subsurface heterogeneity {{is a prerequisite}} to describe flow and transport in porous media. Of particular interest are the variance and the correlation scale of hydraulic conductivity. In this study, we present how these aquifer <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> using empirical steady state pumping test data. We refer to a previously developed analytical solution of “effective well flow” and examine its applicability to pumping test data as under field conditions. It is examined how the accuracy and confidence of parameter estimates of variance and correlation length depend on the number and location of head measurements. Simulations of steady state pumping tests in a confined virtual aquifer are used to systematically reduce sampling size while determining the rating of the estimates at each level of data density. The method was then applied to estimate the statistical parameters of a fluvial heterogeneous aquifer at the test site Horkheimer Insel, Germany. We conclude that the “effective well flow” solution is a simple alternative to laboratory investigations to estimate the statistical heterogeneity parameter using steady state pumping tests. However, the accuracy and uncertainty of the estimates depend {{on the design of}} the field study. In this regard, our results can help to improve the conceptual design of pumping tests with regard to the parameter of interest...|$|R
40|$|SCIAMACHY data {{processing}} generates products on various levels {{as required by}} the user community. These products are either produced in the ENVISAT Payload Data Segment as operational products or by science institutes as scientific products or value-added products. Operational processing occurs in two steps – level 0 - 1 b and level 1 b- 2. In both steps different timescales may apply – near-realtime, fast delivery or offline. Level 0 - 1 b processing generates geolocated and calibrated radiances from the raw atmospheric measurements, as well as from measurements for calibration and instrument monitoring. The algorithms convert measured signals into calibrated radiances. Therefore a sequence of calibration steps has to be applied starting with correcting the memory effect and non-linearity and ending with applying the radiometric instrument response. Particular attention has to be given to the correction of polarisation and degradation. The goal of level 1 b- 2 processing is to provide geophysical parameters such as column densities and profiles from trace gas species as well as cloud and aerosol parameters. Nadir measurements permit retrieving total column densities or cloud and aerosol parameters. From limb observations height resolved profiles of atmospheric <b>parameters</b> <b>can</b> <b>be</b> <b>inferred.</b> Together with the scientific and value-added products the SCIAMACHY data can serve a wide range of applications...|$|R
40|$|Abstract Background Reaction-diffusion {{systems are}} {{frequently}} used in systems biology to model developmental and signalling processes. In many applications, count {{numbers of the}} diffusing molecular species are very low, leading {{to the need to}} explicitly model the inherent variability using stochastic methods. Despite their importance and frequent use, parameter estimation for both deterministic and stochastic reaction-diffusion systems is still a challenging problem. Results We present a Bayesian inference approach to solve both the parameter and state estimation problem for stochastic reaction-diffusion systems. This allows a determination of the full posterior distribution of the parameters (expected values and uncertainty). We benchmark the method by illustrating it on a simple synthetic experiment. We then test the method on real data about the diffusion of the morphogen Bicoid in Drosophila melanogaster. The results show how the precision with which <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> varies dramatically, indicating that the ability to infer full posterior distributions on the <b>parameters</b> <b>can</b> have important experimental design consequences. Conclusions The results obtained demonstrate the feasibility and potential advantages of applying a Bayesian approach to parameter estimation in stochastic reaction-diffusion systems. In particular, the ability to estimate credibility intervals associated with <b>parameter</b> estimates <b>can</b> <b>be</b> precious for experimental design. Further work, however, will be needed to ensure the method can scale up to larger problems. </p...|$|R
40|$|Malachite green {{adsorption}} from an {{aqueous solution}} onto activated Enteromorpha carbon {{has been studied}} experimentally using batch adsorption method. Adsorption kinetics and equilibrium were investigated {{as a function of}} initial dye concentration, pH, contact time and adsorbent dosage. Kinetics studies indicated that the adsorption followed pseudo second order reaction. Equilibrium data was analyzed using Langmuir and Freundlich isotherm models. The adsorption capacity of Enteromorpha was found to be 94. 74 %. On the basis of experimental results and the model <b>parameters,</b> it <b>can</b> <b>be</b> <b>inferred</b> that the carbonaceous Enteromorpha is effective for the removal of malachite green from aqueous solution...|$|R
40|$|Wave {{function}} collapse models postulate {{a fundamental}} {{breakdown of the}} quantum superposition principle at the macroscale. Therefore, experimental tests of collapse models are also fundamental tests of quantum mechanics. Here, we compute the upper bounds on the collapse <b>parameters,</b> which <b>can</b> <b>be</b> <b>inferred</b> by the gravitational wave detectors LIGO, LISA Pathfinder, and AURIGA. We consider {{the most widely used}} collapse model, the continuous spontaneous localization (CSL) model. We show that these experiments exclude a huge portion of the CSL parameter space, the strongest bound being set by the recently launched space mission LISA Pathfinder. We also rule out a proposal for quantum-gravity-induced decoherence...|$|R
40|$|The {{constrained}} {{local model}} (CLM) proposes a paradigm that {{the locations of}} a set of local landmark detectors are constrained to lie in a subspace, spanned by a shape point distribution model (PDM). Fitting the model to an object involves two steps. A response map, which represents the likelihood of the location of a landmark, is first computed for each landmark using local-texture detectors. Then, an optimal PDM is determined by jointly maximizing all the response maps simultaneously, with a global shape constraint. This global optimization <b>can</b> <b>be</b> considered as a Bayesian inference problem, where the posterior distribution of the shape parameters, as well as the pose <b>parameters,</b> <b>can</b> <b>be</b> <b>inferred</b> using maximum a posteriori (MAP). In this paper, we present a cascaded face-alignment approach, which employs random-forest regressors to estimate the positions of each landmark, as a likelihood term, efficiently in the CLM model. Interpretation from CLM framework, this algorithm is named as an efficient likelihood Bayesian constrained local model (elBCLM). Furthermore, in each stage of the regressors, the PDM non-rigid parameters of previous stage can work as shape clues for training each stage regressors. Experimental results on benchmarks show our approach achieve about 3 to 5 times speed-up compared with CLM models and improve around 10 % on fitting quality compare with the same setting regression models. Comment: 6 pages, for submitting to ICME- 201...|$|R
40|$|Pulsar timing arrays (PTAs) are {{presently}} {{the only means}} {{to search for the}} gravitational wave stochastic background from supermassive black hole binary populations, considered to be within the grasp of current or near future observations. However, the stringent upperlimit set by the Parkes PTA (Shannon et al. 2013, 2015) has been interpreted as excluding at > 90 % confidence the current paradigm of binary assembly through galaxy mergers and hardening via stellar interactions, suggesting evolution is accelerated (by stars and/or gas) or stalled. Using Bayesian hierarchical modelling, we consider implications of this upperlimit for a comprehensive range of astrophysical scenarios, without invoking stalling nor more exotic physical processes. We find they are fully consistent with the upperlimit, but (weak) bounds on population <b>parameters</b> <b>can</b> <b>be</b> <b>inferred.</b> Bayes factors between models vary between ≈ 1. 03 [...] 5. 81 and Kullback-Leibler divergences between characteristic amplitude prior and posterior lie between 0. 37 [...] 0. 85. Considering prior astrophysical information on galaxy merger rates, recent upwards revisions of the black hole-galaxy bulge mass relation (Kormendy & Ho 2013) are disfavoured at 1. 6 σ against lighter models (e. g. Shankar et al. 2016). We also show, if no detection is achieved once sensitivity improves by an order of magnitude, the most optimistic scenario is disfavoured at 3. 9 σ. Comment: 20 pages, 7 figure...|$|R
40|$|We {{present an}} {{application}} of the stereoscopic self-similar-expansion model (SSSEM) to Solar Terrestrial Relations Observatory (STEREO) /Sun-Earth Connection Coronal and Heliospheric Investigation (SECCHI) observations of the 03 April 2010 CME and its associated shock. The aim is to verify whether CME-driven shock <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> from the analysis of j-maps. For this purpose we use the SSSEM to derive the CME and the shock kinematics. Arrival times and speeds, inferred assuming either propagation at constant speed or with uniform deceleration, show good agreement with Advanced Composition Explorer (ACE) measurements. The shock standoff distance [Δ], the density compression [ρ_d/ρ_u] and the Mach number [M] are calculated combining the results obtained for the CME and shock kinematics with models for the shock location. Their values are extrapolated to L_ 1 and compared to in-situ data. The in-situ standoff distance is obtained from ACE solar-wind measurements, and the Mach number and compression ratio are provided by the Harvard-Smithsonian Center for Astrophysics interplanetary shock database. They are ρ_d/ρ_u = 2. 84 and M = 2. 2. The best fit to observations is obtained when the SSSEM half width λ = 40 and the CME and shock propagate with uniform deceleration. In this case we find Δ = 23 R_, ρ_d/ρ_u = 2. 61, and M = 2. 93. The study shows that CME-driven shock <b>parameters</b> <b>can</b> <b>be</b> estimated from the analysis of time-elongation plots and <b>can</b> <b>be</b> used to predict their in-situ values...|$|R
40|$|High {{dynamic range}} images require tone {{reproduction}} {{to match the}} range of values to {{the capabilities of the}} display. For computational reasons as well as absence of fully calibrated im-agery, rudimentary color reproduction is often added as a post-processing step rather than integrated into the tone reproduction algorithm. However, in the general case this currently requires manual parameter tuning, although for some global tone repro-duction operators, <b>parameter</b> settings <b>can</b> <b>be</b> <b>inferred</b> from the tone curve. We present a novel and fully automatic saturation correction technique, suitable for any tone reproduction opera-tor, which exhibits better color reproduction than the state-of-the-art and we validate its comparative effectiveness through psy-chophysical experimentation...|$|R
40|$|We {{present a}} small {{statistical}} data set, where we investigate energy conversion at the magnetopause using Cluster measurements of magnetopause crossings. The Cluster observations of magnetic field, plasma velocity, current density and magnetopause orientation <b>are</b> needed to <b>infer</b> the energy conversion at the magnetopause. These <b>parameters</b> <b>can</b> <b>be</b> <b>inferred</b> either from accurate multispacecraft methods, or by using single-spacecraft methods. Our final aim {{is a large}} statistical study, for which only single-spacecraft methods <b>can</b> <b>be</b> applied. The Cluster mission provides an opportunity to examine and validate single-spacecraft methods against the multispacecraft methods. For single-spacecraft methods, we use the Generic Residue Analysis (GRA) and a standard one-dimensional current density method using magnetic field measurements. For multispacecraft methods, we use triangulation (Constant Velocity Approach - CVA) and the curlometer technique. We find {{that in some cases}} the single-spacecraft methods yield a different sign for the energy conversion than compared to the multispacecraft methods. These sign ambiguities arise from the orientation of the magnetopause, choosing the interval to be analyzed, large normal current and time offset of the current density inferred from the two methods. By using the Finnish Meteorological Institute global MHD simulation GUMICS- 4, we are able to determine which sign is likely to be correct, introducing an opportunity to correct the ambiguous energy conversion values. After correcting the few ambiguous cases, we find that the energy conversion estimated from single-spacecraft methods is generally lower by 70 % compared to the multispacecraft methods. Peer reviewe...|$|R
