38|29|Public
50|$|The Oberheim OB-X is {{an analog}} <b>polyphonic</b> <b>sound</b> synthesizer.|$|E
50|$|Sacred music (on liturgical texts, in Latin), melodically rich, with <b>polyphonic</b> <b>sound,</b> in {{particular}} using brass instruments (trumpets, trombones), strings and organ. Vocal parts {{are based on}} the timbre contrast between solo vocals and choir.|$|E
5000|$|Tone {{vibrations}} made {{visible in}} liquids, light rays, and flames. Generate visible models in fluids by {{the influence of}} specific sound vibrations (play a <b>polyphonic</b> <b>sound</b> structure into two or three containers) and project them on a screen.|$|E
40|$|The {{increasing}} {{needs of}} content-based automatic indexing for large musical repositories {{have led to}} extensive investigation in musical sound pattern recognition. Numerous acoustical sound features {{have been developed to}} describe the characteristics of a sound piece. Many of these features have been successfully applied to monophonic sound timbre recognition. However, most of those features failed to describe enough characteristics of <b>polyphonic</b> <b>sounds</b> for the purpose of classification, where sound patterns from different sources are overlapping with each other. Thus, sound separation technique is needed to process <b>polyphonic</b> <b>sounds</b> into monophonic sounds before feature extraction. In this paper, we proposed a novel sound source separation and estimation system to isolate sound sources by maximum likelihood fundamental frequency estimation and pattern matching of a harmonic sequence in our feature database. 1...|$|R
40|$|In this paper, we {{introduce}} a new partial tracking method suit-able for the sinusoidal modeling of mixtures of instrumental sounds with pseudo-stationary frequencies. This method, based on the lin-ear prediction of the frequency evolutions of the partials, enables us to track these partials more accurately at the analysis stage, even in complex sound mixtures. This allows our spectral model to bet-ter handle <b>polyphonic</b> <b>sounds.</b> 1...|$|R
40|$|Abstract: Recently, {{numerous}} successful {{approaches have}} been developed for instrument recognition in monophonic sounds. Unfortunately, none of them can be successfully applied to <b>polyphonic</b> <b>sounds.</b> Identification of music instruments in <b>polyphonic</b> <b>sounds</b> is still difficult and challenging. This has stimulated a number of research projects on music sound separation and new features development for content-based automatic music information retrieval. The paper introduces several temporal features based on pitch to improve automatic music instrument recognition. The results from experiments show that these new features, with the pitch information removed from them, tend to provide less distraction for timber estimation. Sometime, the addition of new features to the database of music instruments does not help and related classifiers still do not perform well. One possibility to handle this problem is to build classifiers which learn not only the descriptions of music instruments but also their generalizations on different granularity levels. We show that by introducing several optional hierarchical classifications of musical instruments and constructing related classifiers, we increase a chance to build a system of good performance in terms of successful indexing of music by instruments and their types...|$|R
50|$|Bedhead's {{music was}} {{generally}} subdued, with a <b>polyphonic</b> <b>sound</b> {{based on the}} interlocking of single-line melodies played by three electric guitars and one electric bass guitar (often played with a capo), nearly always using clean (undistorted) tones, prompting comparisons to the Velvet Underground.|$|E
50|$|In 2002 the Nokia 3510i {{introduced}} <b>polyphonic</b> <b>sound,</b> mostly MIDI-based. This Nokia Tune {{has also}} been used in much later Nokia models (even after the newer ones were introduced), the final Nokia phone using this Nokia Tune was the Nokia 7070 Prism from 2008.|$|E
50|$|Yoiking {{with the}} Winged Ones is a <b>polyphonic</b> <b>sound</b> project by Ánde Somby. In the recording, Somby {{performs}} yoiking, the ancient chanting practise of the Sámi People of northern Europe, {{in concert with}} the sounds of nature. The album edition of the project was released by Ash International as a vinyl record in January 2016.|$|E
5000|$|More {{sophisticated}} approaches compare {{segments of}} the signal with other segments offset by a trial period to find a match. AMDF (average magnitude difference function), ASMDF (Average Squared Mean Difference Function), and other similar autocorrelation algorithms work this way. These algorithms can give quite accurate results for highly periodic signals. However, they have false detection problems (often [...] "octave errors"), can sometimes cope badly with noisy signals (depending on the implementation), and - in their basic implementations - do not deal well with <b>polyphonic</b> <b>sounds</b> (which involve multiple musical notes of different pitches).|$|R
40|$|Abstract. With {{the fast}} booming of online music repositories, {{there is a}} need for {{content-based}} automatic indexing which will help users to find their favorite music objects in real time. Recently, numerous successful approaches on musical data feature extraction and selection have been proposed for instrument recognition in monophonic sounds. Unfortunately, none of these methods can be successfully applied to <b>polyphonic</b> <b>sounds.</b> Identification of music instruments in <b>polyphonic</b> <b>sounds</b> is still difficult and challenging, especially when harmonic partials are overlapping with each other. This has stimulated the research on music sound separation and new features development for content-based automatic music information retrieval. Our goal is to build a cooperative query answering system (QAS), for a musical database, retrieving from it all objects satisfying queries like ”find all musical pieces in pentatonic scale with a viola and piano where viola is playing for minimum 20 seconds and piano for minimum 10 seconds”. We use the database of musical sounds, containing almost 4000 sounds taken from the MUMs (McGill University Master Samples), as a vehicle to construct several classifiers for automatic instrument recognition. Classifiers showing the best performance are adopted for automatic indexing of musical pieces by instruments. Our musical database has an FS-tree (Frame Segment Tree) structure representation. The cooperativeness of QAS is driven by several hierarchical structures used for classifying musical instruments. ...|$|R
50|$|By {{overlaying}} {{voices in}} different planes, the compositional {{style of the}} seventeenth century was enriched with <b>polyphonic</b> <b>sounds,</b> expanding itself both to the low as well as the high pitch. The prevailing three or four voices of {{the latter half of the}} fourteenth and the first half of the fifteenth centuries, which are almost frequently intertwined between them, already in the latter half of the sixteenth century were preferred by four or five, or even more voices, by the addition of a quintus, also called vagans, and a sextus playing the part of a second cantus, normally in the soprano or mezzo-soprano range.|$|R
5000|$|The {{definitive}} Dixieland {{sound is}} created when one instrument (usually the trumpet) plays the melody or a recognizable paraphrase or variation on it, {{and the other}} instruments of the [...] "front line" [...] improvise around that melody. This creates a more <b>polyphonic</b> <b>sound</b> than the arranged ensemble playing of the big band sound or the straight [...] "head" [...] melodies of bebop.|$|E
50|$|Graphical {{sound or}} drawn sound (Fr. son dessiné, Ger. graphische Tonerzeugung,; It. suono disegnato) {{is a sound}} {{recording}} created from images drawn directly onto film or paper that were then played back using a sound system. There are several different techniques depending on the technology employed, but all are {{a consequence of the}} sound-on-film technology and based on the creation of artificial optical <b>polyphonic</b> <b>sound</b> tracks on transparent film.|$|E
5000|$|U.S. Chaos are an American {{punk rock}} band from Paterson, New Jersey, formed in 1981 from {{remnants}} of first wave punk outfits The Front Line in 1978 and The Radicals in 1979. They {{were one of}} the first American bands to play in an Oi!/street punk style. The band was originally based in Passaic and Bergen counties. The band's approach was to play music with lyrics that had an ironic, as well as satirical, overzealous pro-American stance. They often played recordings of military marches, air raid sirens and <b>polyphonic</b> <b>sound</b> assaults before going on stage.|$|E
5000|$|Maskandi is {{typically}} played on cheap, portable instruments, or modern instruments tuned or produced {{to imitate the}} <b>polyphonic</b> <b>sounds</b> of the old instruments. Traditionally, a Maskandi musician had one song, a long one that evolved as {{the story of the}} musician's life grew. Now albums may contain the usual 10-14 tracks, which though they are still way over the three-minute mark, are easier for non-"world music" [...] audiences to digest. Although there are several variations of Maskandi, the instrumental ensemble typically remains the same in all variations. This is deliberately done to keep the unique [...] "sound". When listening to Maskandi, these are the typical/expected instruments to be heard: ...|$|R
40|$|Guitar audio {{transcription}} is {{the process}} of generating a human-interpretable musical score from guitar audio. The musical score is presented as guitar tablature, which indicates what notes are played and where they are played on the guitar fretboard. Automatic transcription remains a challenge with <b>polyphonic</b> <b>sounds</b> as those generated by a guitar. The guitar adds more ambiguity to the transcription problem because the same note can be played in many ways. On the other hand, the guitar offers the potential to constrain the polyphonic pitch detection problem because it has few strings and the fin-gering is usually performed by a single hand. In this paper, a real-time guitar transcription scheme is presented. Accuracy is improved by considering physical limitations of the guitar and the human player...|$|R
5000|$|Four {{additional}} analogue pedals, {{three for}} distortion (as, says Leonard, digital distortion is [...] "pretty hideous" [...] {{if not a}} special effect), ranging from subtle to extreme (a Voodoo Labs Sparkle Drive, an Ibanez Tube Screamer (TS 9), an EHX Little Big Muff; an EHX Pog octave pedal for <b>polyphonic</b> octave <b>sounds</b> ...|$|R
50|$|The AirPiano is {{a musical}} {{computer}} interface (from 2007) which allows playing and controlling software instruments simply by moving {{hands in the}} air over the device, connected by USB cable. Above the AirPiano is an invisible matrix of virtual keys plus faders, each pre-assigned with MIDI messages and waiting to be triggered, via infrared sensors. The length of each triggered note is determined by the time the hand is placed over the corresponding virtual key. Beyond the feedback of the sounds changing, hand placement is also confirmed by LED feedback. The first version of AirPiano generated <b>polyphonic</b> <b>sound,</b> with 24 keys using 8 faders.|$|E
5000|$|New Orleans music {{combined}} earlier {{brass band}} marches, French quadrilles, biguine, ragtime, and blues with collective, polyphonic improvisation. The [...] "standard" [...] band {{consists of a}} [...] "front line" [...] of trumpet (or cornet), trombone, and clarinet, with a [...] "rhythm section" [...] {{of at least two}} of the following instruments: guitar or banjo, string bass or tuba, piano, and drums. The Dixieland sound is created when one instrument (usually the trumpet) plays the melody or a variation on it, and the other instruments improvise around that melody. This creates a more <b>polyphonic</b> <b>sound</b> than the heavily arranged big band sound of the 1930s or the straight melodies (with or without harmonizing) of bebop in the 1940s.|$|E
50|$|Since {{the release}} of its first album back in 1991, Taraf de Haïdouks has been {{considered}} the epitome of Romany music's vitality. Their <b>polyphonic</b> <b>sound</b> incorporates instruments such as the violin, double drum, accordion, flute, cimbalom, double bass and some wind instruments. The group has toured worldwide, released acclaimed albums and a DVD (see below), and counts among its fans the late Yehudi Menuhin, the Kronos Quartet (with whom it has recorded and performed), actor Johnny Depp (alongside whom the group appeared in the film The Man Who Cried), fashion designer Yohji Yamamoto (who invited the band to be models-cum-musicians for his Paris and Tokyo shows), and many more. Meanwhile, the band members {{seem to have been}} relatively unaffected by all this, maintaining their way of life (they still reside in Clejani, in the Valachian countryside).|$|E
3000|$|We {{provide a}} new {{solution}} to the problem of feature variations caused by the overlapping of sounds in instrument identification in polyphonic music. When multiple instruments simultaneously play, partials (harmonic components) of their sounds overlap and interfere, which makes the acoustic features different from those of monophonic sounds. To cope with this, we weight features based on how much they are affected by overlapping. First, we quantitatively evaluate the influence of overlapping on each feature as the ratio of the within-class variance to the between-class variance in the distribution of training data obtained from <b>polyphonic</b> <b>sounds.</b> Then, we generate feature axes using a weighted mixture that minimizes the influence via linear discriminant analysis. In addition, we improve instrument identification using musical context. Experimental results showed that the recognition rates using both feature weighting and musical context were 84.1 [...]...|$|R
50|$|ZynAddSubFX is an {{open-source}} software synthesizer for Linux, macOS and Windows. It {{can generate}} <b>polyphonic,</b> multitimbral, microtonal <b>sounds</b> in realtime.It {{is a free}} program, licensed under version 2 of the GNU General Public License.|$|R
40|$|The author {{explores the}} Christian {{doctrine}} of the Trinity {{to shed light on}} the nature of the pastoral ministry. Using the trinitarian term, "polyphony" (David Cunningham) for this purpose, he explicates unity and difference as key polyphonic categories in the {{doctrine of the}} Trinity. The author suggests that the <b>polyphonic</b> notes <b>sounded</b> by pastoral caregivers are toughness and tenderness, woundedness and health, wisdom and folly, and communion, nearness and distance...|$|R
5000|$|The {{album is}} {{considered}} an early example of proto-synthesizer-pop. It sparked a [...] "revolution in synthesizer programming" [...] which it was responsible for taking to new heights. The album's contributions to electronic music included an ambience resembling a symphony orchestra, the use of reverberation, the use of phasing and flanging to create a spatial audio effect with stereo speakers, electronic surround sound using four speakers, realistic string simulations, portamento whistles, and abstract bell-like sounds created using ring modulation. A particularly significant achievement was its <b>polyphonic</b> <b>sound,</b> which was created {{without the use of}} any polyphonic synthesizers (which were not yet commercially released). Tomita created the album's polyphonic sounds by recording selections one part at a time, taking 14 months to produce the album. The modular human whistle sounds used would also be copied in the presets of later electronic instruments.|$|E
50|$|Tomita {{then started}} {{arranging}} Claude Debussy's impressionist pieces for synthesizer and, in 1974, released the album Snowflakes are Dancing; {{it became a}} worldwide success and was responsible for popularizing several aspects of synthesizer programming. The album's contents included ambience, realistic string simulations, an early attempt to synthesize {{the sound of a}} symphony orchestra, whistles, and abstract bell-like sounds, {{as well as a number}} of processing effects including reverberation, phase shifting, flanging, and ring modulation. Quadrophonic versions of the album provided a spatial audio effect using four speakers. A particularly significant achievement was its <b>polyphonic</b> <b>sound,</b> created prior to the era of polyphonic synthesizers. Tomita created the album's polyphony as Carlos had done before him, with the use of multitrack recording, recording each voice of a piece one at a time, on a separate tape track, and then mixing the result to stereo or quad. It took 14 months to produce the album.|$|E
5000|$|In {{the late}} 1980s, {{due to the}} growth in {{popularity}} of so-called World Music - especially {{the success of the}} Bulgarian polyphonic choral recordings of the album Le Mystère des Voix Bulgares - Corsican artists such as Voce di Corsica began to record music for the international market. Attempts to appeal to this market had a major effect on Corsican music. Ethnomusicologist Caroline Bithell describes some of these changes, saying that paghjella recordings began to shift from a more <b>polyphonic</b> <b>sound</b> to a more [...] "homophonic sound where the emphasis is on the effect created by the sum of the voices," [...] as opposed to earlier exampled of paghjella, [...] "where the individual voices and melodic lines are far more clearly differentiated and behave more independently." [...] Additionally, younger singers may have a tendency to exaggerate use of elements considered by outside music consumers to be [...] "typically Corsican," [...] such as heavy use of melisma. Despite the changes initiated by interaction with the world music market, Bithell argues that Corsica can also be seen as [...] "one of the success stories, in musical terms, of an era wherethere are tales aplenty of once unique and flourishing musical cultures threatening to disappear forever," [...] and that {{partly as a result of}} commercial recordings, [...] "indigenous music has been pulled from the brink of the grave and grown to take its place as a national emblem." ...|$|E
2500|$|In 1906, Lee de Forest {{invented the}} triode {{electronic}} valve. In 1915 he invented the first vacuum tube instrument, the audio piano. Then, until {{the invention of}} the transistor, the vacuum tube was an essential component in electric instruments. In 1935, the Hammond organ was introduced, exploiting previous limited production efforts like the [...] from 1923. It was capable of producing <b>polyphonic</b> <b>sounds</b> by virtue of a spinning shaft with many magnetic 'lobes' which would cycle past an electromagnetic pickup at a rate that would produce each desired tone. [...] "Tone Wheel Organ" [...] is the general name for this technology. The Hammond organ was connected to a power amplifier and a speaker cabinet. In 1929, the electric piano was invented. In 1939, Hammond introduced the Novachord which used about 170 vacuum tubes, coils, capacitors and resistors largely to create an upper octave of notes and then divide them in half using 'flip flop' circuits to create successively lower octaves from each note. But the instrument also has many features like envelopes for filter and amplifiers so that sounds can be contoured at the user's discretion, making it the first production analog synthesizer.|$|R
40|$|Music {{transcription}} {{refers to}} extraction {{of a human}} readable and interpretable description from a recording of a music performance. Automatic music transcription remains, nowadays, a challenging research problem when dealing with <b>polyphonic</b> <b>sounds</b> or when removing certain constraints. Some instruments like guitars and violins add ambiguity to the problem as the same note can be played at different positions. When dealing with guitar music tablature are, often, preferred to the usual music score, as they present information in a more accessible way. Here, we address this issue with a system which uses the visual modality to support traditional audio transcription techniques. The system is composed of four modules which have been implemented and evaluated: a system which tracks {{the position of the}} fretboard on a video stream, a system which automatically detects the position of the guitar on the first fret to initialize the first system, a system which detects the position of the hand on the guitar, and finally a system which fuses the visual and audio information to extract a tablature. Results show that this kind of multimodal approach can easily disambiguate 89 % of notes in a deterministic way. 1...|$|R
5000|$|In 1906, Lee de Forest {{invented the}} triode {{electronic}} valve. In 1915 he invented the first vacuum tube instrument, the audio piano. Then, until {{the invention of}} the transistor, the vacuum tube was an essential component in electric instruments. In 1935, the Hammond organ was introduced, exploiting previous limited production efforts like the Robb Wave Organ from 1923. It was capable of producing <b>polyphonic</b> <b>sounds</b> by virtue of a spinning shaft with many magnetic 'lobes' which would cycle past an electromagnetic pickup at a rate that would produce each desired tone. [...] "Tone Wheel Organ" [...] is the general name for this technology. The Hammond organ was connected to a power amplifier and a speaker cabinet. In 1929, the electric piano was invented. In 1939, Hammond introduced the Novachord which used about 170 vacuum tubes, coils, capacitors and resistors largely to create an upper octave of notes and then divide them in half using 'flip flop' circuits to create successively lower octaves from each note. But the instrument also has many features like envelopes for filter and amplifiers so that sounds can be contoured at the user's discretion, making it the first production analog synthesizer.|$|R
40|$|This paper {{describes}} our submission for {{the audio}} melody extraction {{task of the}} Music Information Retrieval Evalu-ation eXchange (MIREX 2014). Our algorithm first sep-arates the vocal spectra from <b>polyphonic</b> <b>sound</b> spectra. Melody extraction and vocal activity detection are applied to the separated spectra. 1...|$|E
40|$|International audienceIn this paper, we {{introduce}} a new partial tracking method suitable for the sinusoidal modeling of mixtures of instrumental sounds with pseudo stationary frequencies. This method, based on the linear prediction of the frequency evolutions of the partials, enables us to track these partials more accurately at the analysis stage, even in complex sound mixtures. This allows our spectral model to better handle <b>polyphonic</b> <b>sound...</b>|$|E
40|$|Language Identification, {{being an}} {{important}} aspect of Automatic Speaker Recognition has had many changes and new approaches to ameliorate performance over the last decade. We compare the performance of using audio spectrum in the log scale and using <b>Polyphonic</b> <b>sound</b> sequences from raw audio samples to train the neural network and to classify speech as either English or Spanish. To achieve this, we use the novel approach of using a Convolutional Recurrent Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit (GRU) for forward propagation of the neural network. Our hypothesis is that the performance of using <b>polyphonic</b> <b>sound</b> sequence as features and both LSTM and GRU as the gating mechanisms for the neural network outperform the traditional MFCC features using a unidirectional Deep Neural Network. Comment: Further experiments were performed on the model using LibriVox speech dataset and it was found that a Time Distributed CRNN model performed better and represented our initial ideas about the speaker recognition task better. The dataset contains speech in three languages - English, Spanish and Czech. A report on our findings along with experimental results will be published soo...|$|E
40|$|We {{provide a}} new {{solution}} to the problem of feature variations caused by the overlapping of sounds in instrument identification in polyphonic music. When multiple instruments simultaneously play, partials (harmonic components) of their sounds overlap and interfere, which makes the acoustic features different from those of monophonic sounds. To cope with this, we weight features based on how much they are affected by overlapping. First, we quantitatively evaluate the influence of overlapping on each feature as the ratio of the within-class variance to the between-class variance in the distribution of training data obtained from <b>polyphonic</b> <b>sounds.</b> Then, we generate feature axes using a weighted mixture that minimizes the influence via linear discriminant analysis. In addition, we improve instrument identification using musical context. Experimental results showed that the recognition rates using both feature weighting and musical context were 84. 1 % for duo, 77. 6 % for trio, and 72. 3 % for quartet; those without using either were 53. 4, 49. 6, and 46. 5 %, respectively. Copyright © 2007 Tetsuro Kitahara et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|R
50|$|Using {{a feature}} called multi-sampling, the Mirage was also capable of {{assigning}} multiple samples to different keys across its keyboard. Using this technique, the Mirage essentially {{turned into a}} <b>polyphonic</b> mult-timbral MIDI <b>sound</b> module complete with a velocity-sensitive keyboard {{that could be used}} to drive other MIDI sound modules as well its own sound engine.|$|R
40|$|Hypothesis: It was {{hypothesized}} that cochlear implant (CI) subjects {{would be able to}} correctly identify one, two and three simultaneous pitches through direct electrical stimulation. We further hypothesized that the location on the implant array and the fundamental frequency of the pitches would {{have an impact on the}} performance. Background: "They gave me back speech but not music. " is a sentence commonly heard by CI subjects. One of the reasons is that in music, multiple streams are frequently played at the same time which is an essential feature of harmony. Current CI speech processors do not allow CI users to perceive such complex <b>polyphonic</b> <b>sounds.</b> Methods: In the present study the authors assessed the ability of CI subjects to perceive simultaneous modulation frequencies based on direct electrical stimulation. Ten CI subjects were asked to identify one, two and three simultaneous pitches applied on different electrodes using sinusoidal amplitude modulation. All stimuli were loudness balanced before the actual identification task. Results: Subjects were able to identify one, two and three simultaneous pitches. The further the distance between the two electrodes, the better was the performance in the two-pitch condition. The distance between the modulation frequencies had a significant effect on the performance in the two and three pitch condition. Conclusion: Subjects are able to identify complex polyphonic stimuli based on the number of active electrodes. The additional polyphonic rate pitch cue improves performance in some conditions...|$|R
