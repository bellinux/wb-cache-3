1910|1090|Public
25|$|Primary stress {{may fall}} {{on any of}} the three final syllables of a word, but mostly on the last two. There is a <b>partial</b> <b>correlation</b> between the {{position}} of the stress and the final vowel; for example, the final syllable is usually stressed when it contains a nasal phoneme, a diphthong, or a close vowel. The orthography of Portuguese takes advantage of this correlation to minimize the number of diacritics.|$|E
25|$|Most {{sources of}} {{electromagnetic}} radiation contain {{a large number}} of atoms or molecules that emit light. The orientation of the electric fields produced by these emitters may not be correlated, in which case the light is said to be unpolarised. If there is <b>partial</b> <b>correlation</b> between the emitters, the light is partially polarised. If the polarization is consistent across the spectrum of the source, partially polarised light can be described as a superposition of a completely unpolarised component, and a completely polarised one. One may then describe the light in terms of the degree of polarization, and the parameters of the polarization ellipse.|$|E
5000|$|To test if {{a sample}} <b>partial</b> <b>correlation</b> [...] implies a true {{population}} <b>partial</b> <b>correlation</b> of 0, Fisher's z-transform of the <b>partial</b> <b>correlation</b> can be used: ...|$|E
40|$|<b>Partial</b> <b>correlations</b> are {{the natural}} {{interaction}} terms {{to be associated}} with the edges of the independence graph of a multivariate normal distribution. Two edges of the graph can thus be compared by comparing the corresponding <b>partial</b> <b>correlations.</b> The comparison of "dependent" correlations is a well known problem in statistics but, when the variables satisfy some conditional independence relations, the maximum likelihood estimates of <b>partial</b> <b>correlations</b> are different from the sample <b>partial</b> <b>correlations,</b> so that classical results no longer bold. In this paper we analyze to what extent the classical test statistics can be applied. We show that maximum likelihood estimates of <b>partial</b> <b>correlations</b> are more efficient than sample <b>partial</b> <b>correlations.</b> Furthermore we show that the conditional independence structure of the model can be used to turn the comparison of "dependent" correlations into the comparison of "independent" parameters...|$|R
40|$|Learning of {{large-scale}} networks of interactions from microarray data {{is an important}} and challenging problem in bioinformatics. A widely used approach is {{to assume that the}} available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order <b>partial</b> <b>correlations</b> which are <b>partial</b> <b>correlations</b> between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order <b>partial</b> <b>correlations</b> does not exist. In this paper we consider limited-order <b>partial</b> <b>correlations,</b> these are <b>partial</b> <b>correlations</b> computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order <b>partial</b> <b>correlations,</b> that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data...|$|R
40|$|When {{dealing with}} {{graphical}} Gaussian models for gene regulatory networks, {{the major problem}} is to compute the matrix of <b>partial</b> <b>correlations.</b> Based on the close connection between <b>partial</b> <b>correlations</b> and least squares regression, we suggest estimation of high-dimensional gene networks in terms of partial least squares (PLS) regression and the adaptive Lasso, respectively. In a simulation study, we compare {{the performance of the}} proposed methods in terms of their ability to estimate <b>partial</b> <b>correlations</b> and to derive the underlying network structure. ...|$|R
50|$|It can be {{computationally}} {{expensive to}} solve the linear regression problems. Actually, the nth-order <b>partial</b> <b>correlation</b> (i.e., with |Z| = n) can be easily computed from three (n - 1)th-order partial correlations. The zeroth-order <b>partial</b> <b>correlation</b> ρXY·Ø is defined to be the regular correlation coefficient ρXY.|$|E
50|$|In fact, if we {{compute the}} Pearson {{correlation}} coefficient between variables X and Y, {{the result is}} approximately 0.836, while if we compute the <b>partial</b> <b>correlation</b> between X and Y, using the formula given below, we find a <b>partial</b> <b>correlation</b> of 0.919, which is stronger than the full correlation.|$|E
5000|$|Next {{we use the}} {{resulting}} node correlations to compute the partial correlations. The first order <b>partial</b> <b>correlation</b> coefficient is a statistical measure indicating how a third variable affects the correlation between two other variables. The <b>partial</b> <b>correlation</b> between nodes i and k {{with respect to a}} third node [...] is defined as: ...|$|E
5000|$|... #Caption: The {{transition}} from normal (healthy) market behavior into abnormal seizure-like behavior {{at the end}} of 2001 (the vertical dashed line). Top is the S&P Index from March 7, 2000 until March 22, 2011. The third panel shows the stock correlations and the second panel shows the <b>partial</b> <b>correlations</b> (the correlations after subtraction of the index effect). The decrease in the <b>partial</b> <b>correlations</b> manifests the abnormal dominance of the Index. This effect is further pronounced when looking at the Index Cohesive Force - the ratio between the stock <b>correlations</b> and the <b>partial</b> <b>correlations,</b> shown at the bottom panel.|$|R
40|$|Connections between {{graphical}} Gaussian {{models and}} classical single-factor models are obtained by parameterizing the single-factor model as a graphical Gaussian model. Models {{are represented by}} independence graphs, and associations between each manifest variable and the latent factor are measured by factor <b>partial</b> <b>correlations.</b> Power calculations for the single-factor graphical Gaussian model are facilitated by expressing the manifest <b>partial</b> <b>correlations</b> as functions of the factor <b>partial</b> <b>correlations.</b> The power of selecting a graphical Gaussian model with an association structure between manifest variables compatible with a single-factor model is investigated. The results are illustrated using 2 examples: the 1 st is a hypothetical factor model with parallel measures. The 2 nd uses data from the British Household Panel Survey on job satisfaction...|$|R
30|$|Of further {{relevance}} is {{the following}} result, which states that conditional correlations are constant and equal to <b>partial</b> <b>correlations</b> for the linear circular copula.|$|R
5000|$|The semipartial (or part) {{correlation}} statistic {{is similar}} to the <b>partial</b> <b>correlation</b> statistic. Both compare variations of two variables after certain factors are controlled for, but to calculate the semipartial correlation one holds the third variable constant for either X or Y but not both, whereas for the <b>partial</b> <b>correlation</b> one holds the third variable constant for both. The semipartial correlation compares the unique variation of one variable (having removed variation associated with the Z variable(s)), with the unfiltered variation of the other, while the <b>partial</b> <b>correlation</b> compares the unique variation of one variable to the unique variation of the other.|$|E
50|$|The {{distribution}} of the sample <b>partial</b> <b>correlation</b> was described by Fisher.|$|E
5000|$|... where [...] is the <b>partial</b> <b>correlation</b> between x and y given set Z.|$|E
40|$|Frequency hopping {{sequences}} (FHSs) with favorable <b>partial</b> Hamming <b>correlation</b> properties {{have important}} applications in many synchronization and multiple-access systems. In this paper, we investigate constructions of FHSs and FHS sets with optimal <b>partial</b> Hamming <b>correlation.</b> We first establish a correspondence between FHS sets with optimal <b>partial</b> Hamming <b>correlation</b> and multiple partition-type balanced nested cyclic difference packings {{with a special}} property. By virtue of this correspondence, some FHSs and FHS sets with optimal <b>partial</b> Hamming <b>correlation</b> are constructed from various combinatorial structures such as cyclic difference packings, and cyclic relative difference families. We also describe a direct construction and two recursive constructions for FHS sets with optimal <b>partial</b> Hamming <b>correlation.</b> As a consequence, our constructions yield new FHSs and FHS sets with optimal <b>partial</b> Hamming <b>correlation.</b> Comment: 16 pages. arXiv admin note: text overlap with arXiv: 1506. 0737...|$|R
40|$|AbstractWe extend {{and improve}} two {{existing}} methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S. G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276 – 294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on <b>partial</b> <b>correlations,</b> Journal of Multivariate Analysis 97 (2006) 2177 – 2189] based on <b>partial</b> <b>correlations.</b> The latter {{is based on}} the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple <b>correlation</b> and <b>partial</b> <b>correlations</b> on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions...|$|R
40|$|Higgins et al. (2006) report several {{statistically}} significant partial correlates with U. S. {{per capita income}} growth. However, Levine and Renelt (1992) demonstrate that such correlations are hardly ever robust to changing the combination of conditioning variables included. We ask whether {{the same is true}} for the variables identified as important by Higgins et al. Using the extreme bounds analysis of Levine and Renelt, we find that the majority of the <b>partial</b> <b>correlations</b> can be accepted as robust. The variables associated with those <b>partial</b> <b>correlations</b> stand solidly as variables of interest for future studies of U. S. growth. ...|$|R
5000|$|Farrar-Glauber test: If the {{variables}} {{are found to}} be orthogonal, there is no multicollinearity; if {{the variables}} are not orthogonal, then at least some degree of multicollinearity is present. C. Robert Wichers has argued that Farrar-Glauber <b>partial</b> <b>correlation</b> test is ineffective in that a given <b>partial</b> <b>correlation</b> may be compatible with different multicollinearity patterns. The Farrar-Glauber test has also been criticized by other researchers.|$|E
5000|$|... where Φ(·) is the {{cumulative}} distribution function of a Gaussian distribution with zero mean and unit standard deviation, and N is the sample size. This z-transform is approximate and that the actual distribution of the sample (<b>partial)</b> <b>correlation</b> coefficient is not straightforward. However, an exact t-test based {{on a combination of}} the partial regression coefficient, the <b>partial</b> <b>correlation</b> coefficient and the partial variances is available.|$|E
5000|$|... #Caption: Geometrical {{interpretation}} of <b>partial</b> <b>correlation</b> {{for the case}} of N=3 observations and thus a 2-dimensional hyperplane ...|$|E
50|$|To be more specific, the <b>partial</b> <b>{{correlations}}</b> of the pair, given j is {{the correlations}} between them after proper subtraction of {{the correlations between}} i and j and between k and j. Defined this way, {{the difference between the}} <b>correlations</b> and the <b>partial</b> <b>correlations</b> provides a measure of the influence of node j on the correlation. Therefore, we define the influence of node j on node i, or the dependency of node i on node j- D(i,j), to be the sum of the influence of node j on the correlations of node i with all other nodes.|$|R
30|$|The Kaiser-Meyer-Olkin {{measure of}} {{sampling}} adequacy tests whether the <b>partial</b> <b>correlations</b> among variables are small. Bartlett’s test of sphericity tests whether the correlation matrix is an identity matrix, which {{would indicate that}} the factor model is inappropriate.|$|R
40|$|Key Words: global {{navigation}} satellite system, binary offset carrier (BOC), signal tracking, side-peak, local signals, tracking error standard deviation (TESD) In this paper, we design local signals to remove side-peaks in the binary offset carrier (BOC) autocorrelation. Specifically, we first investigate why local signals {{of the conventional}} schemes are applicable to either sine or cosine-phased BOC signals, and then, design local signals applicable to both sine and cosine-phased BOC signals. Finally, we obtain two <b>partial</b> <b>correlations</b> and propose a correlation function with no side-peak via {{a combination of the}} <b>partial</b> <b>correlations.</b> From numerical results, we demonstrate that the designed local signals are applicable to both sine and cosine-phased BOC signals and can remove side-peaks completely...|$|R
5000|$|Correlations: Pearson {{product-moment}} correlation coefficient, Spearman's rank correlation coefficient, Kendall tau rank correlation coefficient, <b>Partial</b> <b>correlation,</b> Intraclass correlation ...|$|E
5000|$|... and {{the sample}} <b>partial</b> <b>{{correlation}}</b> is then {{given by the}} usual formula for sample correlation , but between these new derived values: ...|$|E
50|$|Other {{methods for}} characterizing resting-state {{networks}} include <b>partial</b> <b>correlation,</b> coherence and partial coherence, phase relationships, dynamic time warping distance, clustering, and graph theory.|$|E
40|$|We {{consider}} electroencephalograms (EEGs) {{of healthy}} individuals {{and compare the}} properties of the brain functional networks found through two methods: unpartialized and partialized cross-correlations. The networks obtained by <b>partial</b> <b>correlations</b> are fundamentally different from those constructed through unpartial correlations in terms of graph metrics. In particular, they have completely different connection efficiency, clustering coefficient, assortativity, degree variability, and synchronization properties. Unpartial correlations are simple to compute and they can be easily applied to large-scale systems, yet they cannot prevent the prediction of non-direct edges. In contrast, <b>partial</b> <b>correlations,</b> which are often expensive to compute, reduce predicting such edges. We suggest combining these alternative methods in order to have complementary information on brain functional networks...|$|R
40|$|We {{extend the}} notion of a tree graph to {{sequences}} of prime graphs which are cycles and edges and name these non-chordal graphs hollow trees. These structures are especially attractive for palindromic Ising models, which mimic a symmetry of joint Gaussian distributions. We show that for an Ising model all defining independences are captured by zero <b>partial</b> <b>correlations</b> and conditional <b>correlations</b> agree with <b>partial</b> <b>correlations</b> within each prime graph if and only if the model is palindromic and has a hollow-tree structure. This implies that the strength of dependences can be assessed locally. We use the results to find a well-fitting general Ising model with hollow-tree structure for a set of longitudinal data. Comment: 27 pages, 9 figure...|$|R
30|$|As {{would be}} expected, admiration, adoration, and awe {{overlapped}} substantially. Therefore, I determined whether {{some of the}} reported associations were unique to admiration, adoration, or awe by computing <b>partial</b> <b>correlations</b> controlling for the other two emotions (e.g., <b>partial</b> <b>correlations</b> of admiration controlling for adoration and awe). As {{can be seen in}} Table  1 (above the diagonal), differences between admiration and adoration became more evident when looking at their unique associations. After controlling for adoration and awe, admiration showed a unique positive association with envy (Hypothesis 6). Adoration was negatively related to envy once admiration and awe were partialled out. Admiration demonstrated a unique positive association with personal growth (Hypothesis 7). Adoration demonstrated a unique positive association with purpose in life (Hypothesis 8).|$|R
5000|$|Bedford and Cooke [...] {{show that}} any {{assignment}} of {{values in the}} open interval (−1, 1) to the edges in any <b>partial</b> <b>correlation</b> vine is consistent, the assignments are algebraically independent, {{and there is a}} one-to-one relation between all such assignments and the set of correlation matrices. In other words, <b>partial</b> <b>correlation</b> vines provide an algebraically independent parametrization of the set of correlation matrices, whose terms have an intuitive interpretation. Moreover, the determinant of the correlation matrix is the product over the edges of (1 &minus; &rho;2ik;D(ik)) where &rho;ik;D(ik) is the <b>partial</b> <b>correlation</b> assigned to the edge with conditioned variables i,k and conditioning variables D(ik). A similar decomposition characterizes the mutual information, which generalizes the determinant of the correlation matrix. [...] These features have been used in constrained sampling of correlation matrices, building non-parametric continuous Bayesian networks [...] and addressing the problem of extending partially specified matrices to positive definite matrices ...|$|E
50|$|The <b>partial</b> <b>{{correlation}}</b> based Dependency Networks is {{a revolutionary}} {{new class of}} correlation based networks, which is capable of uncovering hidden relationships between the nodes of the network.|$|E
50|$|Standardized Coefficients: Each predictor’s unique {{contribution}} to each function, therefore {{this is a}} <b>partial</b> <b>correlation.</b> Indicates {{the relative importance of}} each predictor in predicting group assignment from each function.|$|E
3000|$|... {{decrease}} and a given variable, while {{controlling the}} effect of the other variables, Pearson <b>partial</b> <b>correlations</b> were also calculated. The level of significance was set at P[*]<[*] 0.05. Finally, aiming to compare the difference in main variables, between the subject who exhibited a[*]>[*] 5 % decrease in [...]...|$|R
40|$|Vocabulary Test (PPVT) were {{conducted}} {{with a group of}} 36 DS children {{with a mean age of}} 106. 28 months, a group of 30 normally-developing children matched for mental age (MA) and a group of 40 normally-developing children matched for chronological age (CA). Mean scores of social adjustment were compared between the three groups, and <b>partial</b> <b>correlations</b> and stepwise multiple regression models were used to further explore related factors. Results: There was no difference between the DS group and the MA group in terms of communication skills. However, the DS group scored much better than the MA group in self-dependence, locomotion, work skills, socialization and self-management. Children in the CA group achieved significantly higher scores in all aspects of social adjustment than the DS children. <b>Partial</b> <b>correlations</b> indicate a relationship between social adjustment and th...|$|R
40|$|The {{identification}} of a VAR requires differentiating between correlation and causation. This paper presents a method {{to deal with this}} problem. Graphical models, which provide a rigorous language to analyze the statistical and logical properties of causal relations, associate a particular set of vanishing <b>partial</b> <b>correlations</b> to every possible causal structure. The structural form is described by a directed graph and from the analysis of the <b>partial</b> <b>correlations</b> of the residuals the set of acceptable causal structures is derived. This procedure is applied to an updated version of the King et al. (American Economic Review, 81, (1991), 819) data set and it yields an orthogonalization of the residuals consistent with the causal structure among contemporaneous variables and alternative to the standard one, based on a Choleski factorization of the covariance matrix of the residuals...|$|R
