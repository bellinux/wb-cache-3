15|0|Public
5000|$|<b>Pytorch</b> - Python based {{implementation}} of Torch API, allows {{for dynamic graph construction}} ...|$|E
5000|$|<b>Pytorch</b> - Tensors and Dynamic neural {{networks}} in Python with GPUs. The Python version of Torch, associated with Facebook.|$|E
40|$|We explore {{different}} approaches to integrating a simple convolutional neural network (CNN) with the Lucene search engine in a multi-stage ranking architecture. Our models are trained using the <b>PyTorch</b> deep learning toolkit, which is implemented in C/C++ with a Python frontend. One obvious integration strategy is to expose the neural network directly as a service. For this, we use Apache Thrift, a software framework for building scalable cross-language services. In exploring alternative architectures, we observe that once trained, the feedforward evaluation of neural networks is quite straightforward. Therefore, we can extract the parameters of a trained CNN from <b>PyTorch</b> and import the model into Java, {{taking advantage of the}} Java Deeplearning 4 J library for feedforward evaluation. This has the advantage that the entire end-to-end system can be implemented in Java. As a third approach, we can extract the neural network from <b>PyTorch</b> and "compile" it into a C++ program that exposes a Thrift service. We evaluate these alternatives in terms of performance (latency and throughput) as well as ease of integration. Experiments show that feedforward evaluation of the convolutional neural network is significantly slower in Java, while the performance of the compiled C++ network does not consistently beat the <b>PyTorch</b> implementation. Comment: SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR' 17), August 7 - 11, 2017, Shinjuku, Tokyo, Japa...|$|E
40|$|Every {{internet}} user today {{is exposed to}} countless article headlines. These can range from informative, to sensationalist, to downright misleading. These snippets of information can have tremendous impacts on those exposed and can shape ones views on a subject before even reading the associated article. For these reasons and more, {{it is important that}} the Natural Language Processing community turn its attention towards this critical part of everyday life by improving current abstractive text summarization techniques. To aid in that endeavor, this project explores various methods of teacher forcing, a technique used during model training for sequence-to-sequence recurrent reural network architectures. A relatively new deep learning library called <b>PyTorch</b> has made experimentation with teacher forcing accessible for the first time and is utilized for this purpose in the project. Additionally, to the author’s best knowledge this is the first implementation of abstrac¬tive headline summarization in <b>PyTorch.</b> Seven different teacher forcing techniques were designed and experimented with: (1) Constant levels of 0...|$|E
40|$|We {{describe}} {{a variant of}} Child-Sum Tree-LSTM deep neural network (Tai et al, 2015) fine-tuned for working with dependency trees and morphologically rich languages using the example of Polish. Fine-tuning included applying a custom regularization technique (zoneout, described by (Krueger et al., 2016), and further adapted for Tree-LSTMs) as well as using pre-trained word embeddings enhanced with sub-word information (Bojanowski et al., 2016). The system was implemented in <b>PyTorch</b> and evaluated on phrase-level sentiment labeling task {{as part of the}} PolEval competition...|$|E
40|$|We {{describe}} Honk, an open-source <b>PyTorch</b> reimplementation of convolutional {{neural networks}} for keyword spotting that are included as examples in TensorFlow. These models {{are useful for}} recognizing "command triggers" in speech-based interfaces (e. g., "Hey Siri"), which serve as explicit cues for audio recordings of utterances that are sent to the cloud for full speech recognition. Evaluation on Google's recently released Speech Commands Dataset shows that our reimplementation is comparable in accuracy and provides {{a starting point for}} future work on the keyword spotting task. Comment: 3 pages, 2 figure...|$|E
30|$|We {{implement}} the multi-task deblurring network on the <b>Pytorch</b> platform and train the network on NVIDIA Titan X GPU. We set the batch size of 16 and learning rate to 1 e− 4. To guarantee {{the convergence of}} the multi-task framework, we firstly train the multi-task deblurring network with a content loss for about 5 days. we then add the perceptual loss and adversarial loss individually for joint training. Specifically, first, we train this multi-task deblurring network using the loss (2) for 100, 000 iterations. Second, we embed the perceptual loss (3) for 50, 000 iterations. Finally, we add the adversarial loss (4) and jointly train this network for 50, 000 iterations.|$|E
40|$|Common {{recurrent}} {{neural network}} architectures scale poorly {{due to the}} intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is {{as fast as a}} convolutional layer and 5 - 10 x faster than an optimized LSTM implementation. We study SRUs {{on a wide range of}} applications, including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. We open source our implementation in <b>PyTorch</b> and CNTK. Comment: submission versio...|$|E
40|$|Machine {{learning}} libraries such as TensorFlow and <b>PyTorch</b> simplify model implementation. However, {{researchers are}} still {{required to perform}} a non-trivial amount of manual tasks such as GPU allocation, training status tracking, and comparison of models with different hyperparameter settings. We propose a system to handle these tasks and help researchers focus on models. We present {{the requirements of the}} system based on a collection of discussions from an online study group comprising 25 k members. These include automatic GPU allocation, learning status visualization, handling model parameter snapshots as well as hyperparameter modification during learning, and comparison of performance metrics between models via a leaderboard. We describe the system architecture that fulfills these requirements and present a proof-of-concept implementation, NAVER Smart Machine Learning (NSML). We test the system and confirm substantial efficiency improvements for model development. Comment: 8 pages, 4 figure...|$|E
40|$|Despite {{the rapid}} {{progress}} in style transfer, existing approaches using feed-forward generative network for multi-style or arbitrary-style transfer are usually compromised of image quality and model flexibility. We find it is fundamentally {{difficult to achieve}} comprehensive style modeling using 1 -dimensional style embedding. Motivated by this, we introduce CoMatch Layer that learns to match the second order feature statistics with the target styles. With the CoMatch Layer, we build a Multi-style Generative Network (MSG-Net), which achieves real-time performance. We also employ an specific strategy of upsampled convolution which avoids checkerboard artifacts caused by fractionally-strided convolution. Our method has achieved superior image quality comparing to state-of-the-art approaches. The proposed MSG-Net as a general approach for real-time style transfer is compatible with most existing techniques including content-style interpolation, color-preserving, spatial control and brush stroke size control. MSG-Net {{is the first to}} achieve real-time brush-size control in a purely feed-forward manner for style transfer. Our implementations and pre-trained models for Torch, <b>PyTorch</b> and MXNet frameworks will be publicly available...|$|E
40|$|Even todays most {{advanced}} machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox {{is a new}} Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around {{the idea that the}} most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as <b>PyTorch,</b> Keras, TensorFlow, Theano and MXNet, provides a straight forward way to add support for other frameworks and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at [URL]. Comment: Code and examples available at [URL] and documentation available at [URL]...|$|E
40|$|Dynamic {{neural network}} toolkits such as <b>PyTorch,</b> DyNet, and Chainer offer more {{flexibility}} for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e. g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - {{require that the}} developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually...|$|E
30|$|We {{evaluate}} {{our approach}} using the UK-DALE data set (Kelly & Knottenbelt, 2015 b) {{which consists of}} electric meter recordings of up to 1.8  years duration from 5 households, sampled at 1 / 6  Hz. We use the same pre-processing, artificial data augmentation approach, and data partitioning into train, validation and test data folds as described in (Kelly & Knottenbelt, 2015 a). Based on Kelly’s own re-write of his denoising autoencoder, 1 we re-implemented the neural networks using <b>PyTorch.</b> 2 Our first GAN implementation {{is based on the}} Deep Convolutional GAN topology (DC-GAN) by Radford et al. (Radford et al., 2015). The generator and discriminator networks contain five convolutional layers and one fully-connected layer each. The generator uses transposed convolutional layers, which reflects the convolutions of the discriminator. For the disaggregator’s topology, we replaced the last layer of Kelly’s autoencoder (Kelly & Knottenbelt, 2015 a) in order to map to the latent space ℝz. The loss function is binary cross entropy for the discriminator and mean squared error for the disaggregator. We use the Adam optimizer (Kingma & Ba, 2014) when training the generator and discriminator. For the disaggregator, we use Stochastic Gradient Descent with Nesterov Momentum.|$|E
40|$|We {{present a}} deep, fully {{convolutional}} neural network that learns to route a circuit layout net with appropriate choice of metal tracks and wire class combinations. Inputs to the network are the encoded layouts containing spatial location of pins to be routed. After 15 fully convolutional stages followed by a score comparator, the network outputs 8 layout layers (corresponding to 4 route layers, 3 via layers and an identity-mapped pin layer) which are then decoded to obtain the routed layouts. We formulate this as a binary segmentation problem on a per-pixel per-layer basis, where the network is trained to correctly classify pixels in each layout layer to be 'on' or 'off'. To demonstrate learnability of layout design rules, we train the network on a dataset of 50, 000 train and 10, 000 validation samples that we generate based on certain pre-defined layout constraints. Precision, recall and F_ 1 score metrics are used to track the training progress. Our network achieves F_ 1 ≈ 97 % on the train set and F_ 1 ≈ 92 % on the validation set. We use <b>PyTorch</b> for implementing our model. Code is made publicly available at [URL]. Comment: Code released. 8 pages, 6 figure...|$|E
40|$|Direct visual {{localization}} {{has recently}} enjoyed a resurgence in popularity {{with the increasing}} availability of cheap mobile computing power. The competitive accuracy and robustness of these algorithms compared to state-of-the-art feature-based methods, {{as well as their}} natural ability to yield dense maps, makes them an appealing choice for a variety of mobile robotics applications. However, direct methods remain brittle in the face of appearance change due to their underlying assumption of photometric consistency, which is commonly violated in practice. In this paper, we propose to mitigate this problem by training deep convolutional encoder-decoder models to transform images of a scene such that they correspond to a previously-seen canonical appearance. We validate our method in multiple environments and illumination conditions using high-fidelity synthetic RGB-D datasets, and integrate the trained models into a direct visual localization pipeline, yielding improvements in visual odometry (VO) accuracy through time-varying illumination conditions, as well as improved metric relocalization performance under illumination change, where conventional methods normally fail. We further provide a preliminary investigation of transfer learning from synthetic to real environments in a localization context. An open-source implementation of our method using <b>PyTorch</b> is available at [URL] Accepted for publication in the IEEE Robotics and Automation Letters (RA-L) and for presentation at the IEEE International Conference on Robotics and Automation (ICRA 2018...|$|E

