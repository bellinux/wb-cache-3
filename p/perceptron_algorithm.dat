274|193|Public
50|$|The kernel <b>perceptron</b> <b>algorithm</b> {{was already}} {{introduced}} in 1964 by Aizerman et al. Margin bounds guarantees were {{given for the}} <b>Perceptron</b> <b>algorithm</b> in the general non-separable case first by Freund and Schapire (1998), and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new L1 bounds.|$|E
50|$|This allows {{capturing}} {{latent structure}} between the observations and labels. While LDCRFs {{can be trained}} using quasi-Newton methods, a specialized version of the <b>perceptron</b> <b>algorithm</b> called the latent-variable perceptron has been developed for them as well, based on Collins' structured <b>perceptron</b> <b>algorithm.</b> These models find applications in computer vision, specifically gesture recognition from video streams and shallow parsing.|$|E
50|$|Plugging {{these two}} {{equations}} into the training loop {{turn it into}} the dual <b>perceptron</b> <b>algorithm.</b>|$|E
25|$|Yang, J., Parekh, R. & Honavar, V. (2000). Comparison of Performance of Variants of Single-Layer <b>Perceptron</b> <b>Algorithms</b> on Non-Separable Data. Neural, Parallel, and Scientific Computation. Vol. 8. pp.415–438.|$|R
40|$|A {{simple way}} is {{introduced}} {{to determine the}} connecting of a multilinear neural network, in which only matrix division is used. The character of the convergence theorem of the multilinear back-propagation and <b>perceptron</b> <b>algorithms</b> is discussed. For the simplest case, the signum function of the neuron is not needed...|$|R
40|$|We {{propose a}} novel {{approach}} to the identification of biomedical terms in research publications using the <b>Perceptron</b> HMM <b>algorithm.</b> Each important term is identified and classified into a biomedical concept class. Our proposed system achieves a 68. 6 % F-measure based on 2, 000 training Medline abstracts and 404 unseen testing Medline abstracts. The system achieves performance that {{is close to the}} state-of-the-art using only a small feature set. The <b>Perceptron</b> HMM <b>algorithm</b> provides an easy way to incorporate many potentially interdependent features. ...|$|R
50|$|The voted-perceptron {{algorithm}} is a margin maximizing algorithm {{based on an}} iterative application of the classic <b>perceptron</b> <b>algorithm.</b>|$|E
50|$|The voted <b>perceptron</b> <b>algorithm</b> of Freund and Schapire also {{extends to}} the kernelized case, giving {{generalization}} bounds comparable to the kernel SVM.|$|E
50|$|The <b>perceptron</b> <b>algorithm</b> {{dates back}} to the late 1950s; its first implementation, in custom hardware, {{was one of the first}} {{artificial}} neural networks to be produced.|$|E
40|$|The paper {{develops}} {{a connection between}} traditional <b>perceptron</b> <b>algorithms</b> and recently introduced herding algorithms. It is shown that both algorithms {{can be viewed as}} an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM...|$|R
40|$|AbstractIn matrix computations, {{such as in}} {{factoring}} matrices, Hermitian and, preferably, positive definite {{elements are}} occasionally required. Related problems can often be cast as those of existence of respective elements in a matrix subspace. For two dimensional matrix subspaces, first results in this regard are due to Finsler. To assess positive definiteness in larger dimensional cases, the task becomes computational geometric for the joint numerical range in a natural way. The Hermitian element of the Frobenius norm one with the maximal least eigenvalue is found. To this end, extreme eigenvalue computations are combined with ellipsoid and <b>perceptron</b> <b>algorithms...</b>|$|R
40|$|Abstract. The nearest {{neighbor}} and the <b>perceptron</b> <b>algorithms</b> are intuitively {{motivated by the}} aims to exploit the “cluster ” and “linear separation” structure of the data to be classified, respectively. We develop a new online perceptron-like algorithm, Pounce, to exploit both types of structure. We refine the usual margin-based analysis of a perceptron-like algorithm to now additionally reflect the cluster-structure of the input space. We apply our methods to study the problem of predicting the labeling of a graph. We find that when both the quantity and extent of the clusters are small we may improve arbitrarily over a purely margin-based analysis. ...|$|R
5000|$|The <b>perceptron</b> <b>algorithm</b> {{is an old}} but popular online {{learning}} algorithm that operates by a principle called [...] "error-driven learning". It iteratively improves a model by running it on training samples, then updating the model whenever it finds it has made an incorrect classification {{with respect to a}} supervised signal. The model learned by the standard <b>perceptron</b> <b>algorithm</b> is a linear binary classifier: a vector of weights [...] (and optionally an intercept term , omitted here for simplicity) that is used to classify a sample vector [...] as class [...] "one" [...] or class [...] "minus one" [...] according to ...|$|E
5000|$|... #Caption: Two {{classes of}} points, {{and two of}} the {{infinitely}} many linear boundaries that separate them. Even though the boundaries are at nearly right angles to one another, the <b>perceptron</b> <b>algorithm</b> has no way of choosing between them.|$|E
50|$|Unlike other linear {{classification}} algorithms such as logistic regression, {{there is}} no need for a learning rate in the <b>perceptron</b> <b>algorithm.</b> This is because multiplying the update by any constant simply rescales the weights but never changes the sign of the prediction.|$|E
3000|$|... ≤ 1 are {{reported}} in [35], special results for least squares (LS) estimators including Kalman filters appeared in [36]. The real-valued <b>Perceptron</b> learning <b>algorithm</b> (PLA), see Algorithm 7, was shown to be l [...]...|$|R
25|$|Rosenblatt (1958) {{created the}} <b>perceptron,</b> an <b>algorithm</b> for pattern recognition. With {{mathematical}} notation, Rosenblatt described circuitry {{not in the}} basic perceptron, such as the exclusive-or circuit {{that could not be}} processed by neural networks at the time.|$|R
40|$|Artificial {{radiance}} sets, generated {{with the}} use of a biooptical and radiative transfer model Hydrolight, corresponding to Case 1 and Case 2 waters, are used as inputs to Multi-layer <b>Perceptron</b> and K-NN <b>algorithms</b> to study the algorithm’s retrieval capabilities for optically active constituents in the water. The radiative transfer model Hydrolight has been used to produce 18, 000 artificial reflectance spectra representing various Case 1 and Case 2 water conditions. The remote sensing reflectances were generated at the MERIS wavebands 412, 442, 490, 510, 560, 620, 665 and 682 nm from randomly generated triplet combinations of phytoplankton, non-chlorophyllous particles and CDOM concentrations. These reflectances were then used to assess the performance of the KNearest Neighbour and the Multilayer <b>Perceptron</b> <b>algorithms,</b> which were compared to some more traditional band ratio regression algorithms that had been a popular choice for CZCS and SeaWiFS imagery. The objective of the work was to establish the best kind of algorithm for this type of application...|$|R
5000|$|To derive a kernelized {{version of}} the <b>perceptron</b> <b>algorithm,</b> we must first {{formulate}} it in dual form, starting from the observation that the weight vector [...] can be expressed as a linear combination of the [...] training samples. The equation for the weight vector is ...|$|E
50|$|The winnow {{algorithm}} {{is a technique}} from machine learning for learning a linear classifier from labeled examples. It {{is very similar to}} the <b>perceptron</b> <b>algorithm.</b> However, the <b>perceptron</b> <b>algorithm</b> uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.|$|E
50|$|While the <b>perceptron</b> <b>algorithm</b> is {{guaranteed}} to converge on some solution {{in the case of}} a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality. The perceptron of optimal stability, nowadays better known as the linear support vector machine, was designed to solve this problem.|$|E
40|$|This paper {{proposes a}} new discriminative {{training}} method, called minimum sample risk (MSR), of estimating parameters of language models for text input. While most existing discriminative training methods use a loss function {{that can be}} optimized easily but approaches only approximately to the objective of minimum error rate, MSR minimizes the training error directly using a heuristic training procedure. Evaluations {{on the task of}} Japanese text input show that MSR can handle a large number of features and training samples; it significantly outperforms a regular trigram model trained using maximum likelihood estimation, and it also outperforms the two widely applied discriminative methods, the boosting and the <b>perceptron</b> <b>algorithms,</b> by a small but statistically significant margin. ...|$|R
40|$|We {{present a}} {{modified}} version of the <b>perceptron</b> learning <b>algorithm</b> (PLA) which solves semidefinite programs (SDPs) in polynomial time. The algorithm is based on the following three observations: (i) Semidefinite programs are linear programs with infinitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the <b>perceptron</b> learning <b>algorithm</b> solves a constraint satisfaction problem with linear constraints in finitely many updates. Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time. We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods. ...|$|R
40|$|Abstract:- This paper {{presents}} a {{comparative study of}} the hybrid models, neural networks and nonparametric regression models in time series forecasting. The components of these hybrid models are consisting of the nonparametric regression and artificial neural networks models. Smoothing spline, regression spline and additive regression models are considered as the nonparametric regression components. Furthermore, various multilayer <b>perceptron</b> <b>algorithms</b> and radial basis function network model are regarded as the artificial neural networks components. The performances of these models are compared by forecasting the series of number of produced Cars and Domestic product per capita (GDP) data occurred in Turkey. This comparisons show that hybrid models proposed in this paper have denoted much more excellent performance than the hybrid models in literature...|$|R
50|$|Another {{problem with}} the kernel {{perceptron}} {{is that it does}} not regularize, making it vulnerable to overfitting. The NORMA online kernel learning algorithm can be regarded as a generalization of the kernel <b>perceptron</b> <b>algorithm</b> with regularization. The sequential minimal optimization (SMO) algorithm used to learn support vector machines can also be regarded as a generalization of the kernel perceptron.|$|E
50|$|In {{the context}} of neural networks, a {{perceptron}} is an artificial neuron using the Heaviside step function as the activation function. The <b>perceptron</b> <b>algorithm</b> is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.|$|E
5000|$|... #Caption: The Mark I Perceptron {{machine was}} the first {{implementation}} of the <b>perceptron</b> <b>algorithm.</b> The machine was connected to a camera that used 20×20 cadmium sulfide photocells to produce a 400-pixel image. The main visible feature is a patchboard that allowed experimentation with different combinations of input features. To the right of that are arrays of potentiometers that implemented the adaptive weights.|$|E
40|$|We {{present a}} {{theoretical}} analysis of online parameter tuning in {{statistical machine translation}} (SMT) from a coactive learn-ing view. This perspective allows us to give regret and generalization bounds for latent <b>perceptron</b> <b>algorithms</b> that are com-mon in SMT, but fall outside of the stan-dard convex optimization scenario. Coac-tive learning also introduces the concept of weak feedback, which we apply in a proof-of-concept experiment to SMT, showing that learning from feedback that consists of slight improvements over predictions leads to convergence in regret and transla-tion error rate. This suggests that coactive learning might be a viable framework for interactive machine translation. Further-more, we find that surrogate translations replacing references that are unreachable in the decoder search space can be inter-preted as weak feedback and lead to con-vergence in learning, if they admit an un-derlying linear model. ...|$|R
40|$|A {{number of}} neural network techniques, namely {{multi-layer}} <b>perceptron,</b> k-means <b>algorithm</b> and the self-organizing map {{are applied to}} the detection of technical trading patterns within stock markets. We do not find exploitable information content and it is concluded {{that there are no}} significant patterns in any of the data analysed. ...|$|R
50|$|In machine learning, the kernel {{perceptron}} is {{a variant}} of the popular <b>perceptron</b> learning <b>algorithm</b> that can learn kernel machines, i.e. non-linear classifiers that employ a kernel function to compute the similarity of unseen samples to training samples. The algorithm was invented in 1964, making it the first kernel classification learner.|$|R
5000|$|... where [...] is {{the number}} of times [...] was misclassified, forcing an update [...] Using this result, we can {{formulate}} the dual <b>perceptron</b> <b>algorithm,</b> which loops through the samples as before, making predictions, but instead of storing and updating a weight vector , it updates a [...] "mistake counter" [...] vector [...]We must also rewrite the prediction formula to get rid of : ...|$|E
5000|$|In machine learning, {{alternatives}} to the latent-variable models of ordinal regression have been proposed. An early result was PRank, {{a variant of the}} <b>perceptron</b> <b>algorithm</b> that found multiple parallel hyperplanes separating the various ranks; its output is a weight vector [...] and a sorted vector of [...] thresholds , as in the ordered logit/probit models. The prediction rule for this model is to output the smallest rank [...] such that [...]|$|E
5000|$|The <b>{{perceptron}}</b> <b>algorithm</b> {{was invented}} in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt, funded by the United States Office of Naval Research.The perceptron {{was intended to be}} a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the [...] "Mark 1 perceptron". This machine was designed for image recognition: it had an array of 400 photocells, randomly connected to the [...] "neurons". Weights were encoded in potentiometers, and weight updates during learning were performed by electric motors.|$|E
40|$|Combination of {{proposed}} modular, multilayer <b>perceptron</b> and <b>algorithm</b> for its operation recognizes new objects after relatively brief retraining sessions. (Perceptron is multilayer, feedforward {{artificial neural network}} fully connected and trained via back-propagation learning algorithm.) Knowledge pertaining to each object to be recognized resides in subnetwork of full network, therefore not necessary to retrain full network to recognize each new object...|$|R
40|$|Describes {{the concept}} of neural {{engineering}} which supports decision-making in the systematic construction of neural networks. We gave some guidelines on building neural networks tailored to a specific applications, such as: what is the optimal number of hidden nodes, how to prune the input nodes, segregation and transformation of data etc. Experiments were carried on cost estimation using a multi-layer <b>perceptron</b> (backpropagation <b>algorithm...</b>|$|R
40|$|The kernel Perceptron is an {{appealing}} online learning algorithm {{that has a}} drawback: whenever it makes an error it must increase its support set, which slows training and testing {{if the number of}} errors is large. The Forgetron and the Randomized Budget <b>Perceptron</b> <b>algorithms</b> overcome this problem by restricting the number of support vectors the Perceptron is allowed to have. These algorithms have regret bounds whose proofs are dissimilar. In this paper we propose a unified analysis of both of these algorithms by observing that {{the way in which they}} remove support vectors can be seen as types of L 2 -regularization. By casting these algorithms as instances of online convex optimization problems and applying a variant of Zinkevich’s theorem for noisy and incorrect gradient, we can bound the regret of these algorithms more easily than before. Our bounds are similar to the existing ones, but the proofs are less technical. 1...|$|R
