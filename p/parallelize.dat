2423|6778|Public
5|$|Applications {{are often}} {{classified}} {{according to how}} often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism {{if they do not}} communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to <b>parallelize.</b>|$|E
25|$|Samplesort {{can be used}} to <b>parallelize</b> any of the non-comparison sorts, by {{efficiently}} distributing {{data into}} several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.|$|E
25|$|The {{high demand}} for {{low-cost}} sequencing has driven {{the development of}} high-throughput sequencing technologies that <b>parallelize</b> the sequencing process, producing thousands or millions of sequences at once. High-throughput sequencing is intended to {{lower the cost of}} DNA sequencing beyond what is possible with standard dye-terminator methods. In ultra-high-throughput sequencing, as many as 500,000 sequencing-by-synthesis operations may be run in parallel.|$|E
40|$|Environment) that {{integrates}} {{the functions}} of an interactive <b>parallelizing</b> compiler and some interactive performance analyzers. By applying these tools in ParaPIE, we have successfully <b>parallelized</b> three difficult sequential programs in SPECfp 95 benchmarks {{that can not be}} <b>parallelized</b> by automatic <b>parallelizing</b> compilers...|$|R
40|$|<b>Parallelizing</b> compilers are {{essential}} tools for <b>parallelizing</b> old but sophisticated sequential programs. Program restructuring techniques, like loop transformations together with dependence analysis {{are applied to}} transform automatically sequential programs into parallel code. User interaction with the <b>parallelizing</b> process is very useful because, on massive parallel systems, small mistakes may cause large degradation on performance. In this paper we propose an interactive compiling environment, named Graphic <b>Parallelizing</b> Environment (GPE) 1, equipped with visualization tools to join user knowledge and compiler techniques to efficiently tune the program parallelization process. GPE is oriented to the advanced users of parallel computers. The Parafrase- 2 <b>parallelizing</b> compiler represents {{the core of the}} environment. Tcl/Tk is used as middleware integration language, mainly used to implement the environment components dispatcher, and the graphical components. Keywords Compilers, G [...] ...|$|R
40|$|This paper {{addresses}} {{the issue of}} <b>parallelizing</b> non-perfect nested loops with nonuniform dependences. This kind of loop normally can't be <b>parallelized</b> by existing <b>parallelizing</b> compilers and transformations. Even when <b>parallelized</b> in rare instances, the performance is very poor. Our approach is based on Unique Sets theory [1]. The advantage of our technique is the simpleness of the transformed loops which can be taken over in lower level optimization, as well as low synchronization overhead. Experiments were carried out using several types of loops including those from math library Eispack and Chen and Yew's paper [2]. Experiments were done on different multiprocessor platforms and performance was compared with Chen and Yew's technique, Cray's autotasking, and Stanford's SUIF compiler. Our technique's performance was consistently better. 1 Introduction Loop parallelization {{is very important for}} <b>parallelizing</b> compiler. Cross-iteration dependences are the major reason that forces the l [...] ...|$|R
25|$|Quicksort {{has some}} {{disadvantages}} {{when compared to}} alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, {{it is difficult to}} <b>parallelize</b> the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.|$|E
25|$|It is {{relatively}} straightforward to <b>parallelize</b> {{a number of}} steps in ABC algorithms based on rejection sampling and sequential Monte Carlo methods. It has also been demonstrated that parallel algorithms may yield significant speedups for MCMC-based inference in phylogenetics, which may be a tractable approach also for ABC-based methods. Yet an adequate model for a complex system is very likely to require intensive computation irrespectively of the chosen method of inference, and {{it is up to}} the user to select a method that is suitable for the particular application in question.|$|E
25|$|The {{algorithm}} {{attempts to}} set up a congruence of squares modulo n (the integer to be factorized), which often leads to a factorization of n. The algorithm works in two phases: the data collection phase, where it collects information that may lead to a congruence of squares; and the data processing phase, where it puts all the data it has collected into a matrix and solves it to obtain a congruence of squares. The data collection phase can be easily parallelized to many processors, but the data processing phase requires large amounts of memory, and is difficult to <b>parallelize</b> efficiently over many nodes or if the processing nodes do not each have enough memory to store the whole matrix. The block Wiedemann algorithm can be used {{in the case of a}} few systems each capable of holding the matrix.|$|E
40|$|Microreaction Technology IMRET 9 : Proceedings of the Ninth International Conference on Microreaction Technology - IMRET 9 Special IssueOne of the {{critical}} operational issues of micro chemical plants with external numbering-up structure is to keep a uniform flow distribution among <b>parallelized</b> microdevices even when blockage occurs {{in one or more}} microdevice. Since it is not practical to install flow controllers in all the microdevices, a simple and effective operation policy against blockage occurrence needs to be developed. In this research, micro chemical plants having four or eight <b>parallelized</b> microdevices are constructed to analyze the influence of blockage on the flow distribution among the <b>parallelized</b> microdevices. The numerical and experimental results show that pressure drop control is superior to total flow control. In addition, two control structures based on pressure drop control, pumping pressure control and pressure drop control over the <b>parallelized</b> section, are investigated. It is clarified that the latter control structure enables us to successfully keep the flow rate in each unblocked microdevice at a normal level when blockage occurs. Pressure drop control over the <b>parallelized</b> section is applied to a micro chemical plant having four <b>parallelized</b> micro heat exchangers, and its validity is demonstrated...|$|R
40|$|Multi-grain <b>parallelizing</b> {{scheme is}} one of {{effective}} <b>parallelizing</b> schemes which exploits various level parallelism: coarse-grain(macro-dataflow), medium-grain(loop level <b>parallelizing)</b> and near-fine-grain(statements <b>parallelizing)</b> from a sequential program. A multi-processor ASCA is designed for efficient execution of multi-grain <b>parallelizing</b> program. A processing element called MAPLE are mainly designed for near-fine-grain parallelism, and has two modules called MAPLE core and DTC. The MAPLE core is a simple RISC processor which executes every operation in a fixed time and realize direct register to register transfer. The DTC realize a software controlled cache by instructions which are generated by the compiler. With a static scheduling, near-fine-grain parallel processing is efficiently performed using a communication mechanism with receive registers, and non-synchronization operation mechanism. Through implementation of the prototype chip and clock level simulation, {{it appears that the}} performance of a single chip multi-processor with 4 MAPLEs is close to those of modern super-scaler processors in spite of small hardware and low clock frequency...|$|R
40|$|This paper {{describes}} {{several issues}} of <b>parallelizing</b> an image processing application. The parallelization is performed both automatically and user controlled at the application code level. The sequential image processing library is not modi ed. A solution for <b>parallelizing</b> an image processing library is also proposed...|$|R
2500|$|The maximal {{independent}} set {{problem was}} originally {{thought to be}} non-trivial to <b>parallelize</b> {{due to the fact}} that the lexicographical maximal independent set proved to be P-Complete; however, it has been shown that a deterministic parallel solution could be given by an [...] reduction from either the maximum set packing or the maximal matching problem or by an [...] reduction from the 2-satisfiability problem. Typically, the structure of the algorithm given follows other parallel graph algorithms - that is they subdivide the graph into smaller local problems that are solvable in parallel by running an identical algorithm.|$|E
5000|$|Transparently <b>parallelize</b> complex {{computations}} using semantics {{based on}} data dependencies ...|$|E
50|$|Dependence {{analysis}} {{determines whether}} {{it is safe to}} reorder or <b>parallelize</b> statements.|$|E
40|$|The paper {{presents}} a data and task parallel environment for <b>parallelizing</b> low-level image processing applications on distributed memory systems. Image processing operators are <b>parallelized</b> by data decomposition using algorithmic skeletons. At the application level we use task decomposition, {{based on the}} Image Application Task Graph...|$|R
40|$|This paper proposes an {{execution}} order control method of coarse grain distributed processes for <b>parallelizing</b> compiler on distributed computer environment. Currently proposed execution order control algorithms {{are designed for}} the basic-block-oriented distributed processes. However, we adopted an process generation algorithm based on function-oriented distributed processes and therefore propose new execution order control algorithm. Our algorithm gives more effective and simple executable conditions than the currently investigated ones since it is designed for function-oriented distributed processes especially. 1 Introduction Recently, <b>parallelizing</b> compilers are drawing attentions for software support of parallel processing. The <b>parallelizing</b> compiler decomposes a given sequential programs into program fragments and organizes them as parallel executable distributed processes [1 [...] 8]. We already proposed <b>parallelizing</b> compiler with coarse grain distributed processes on distributed com [...] ...|$|R
40|$|We exhibit {{some simple}} gadgets useful in {{designing}} shallow parallel circuits for quantum algorithms. We prove that any quantum circuit composed entirely of controlled-not gates or of diagonal gates can be <b>parallelized</b> to logarithmic depth, while circuits composed of both cannot. Finally, while we note the Quantum Fourier Transform can be <b>parallelized</b> to linear depth, we exhibit a simple quantum circuit related {{to it that}} we believe cannot be <b>parallelized</b> to less than linear depth, and therefore {{might be used to}} prove that QNC < QP...|$|R
5000|$|Is {{it safe to}} <b>parallelize</b> the loop? Answering this {{question}} needs accurate dependence analysis and alias analysis ...|$|E
50|$|Scale-out, in-memory {{technology}} allows smart caching {{for small}} or huge datasets using more computers to <b>parallelize</b> the efforts, as needed.|$|E
5000|$|The {{conditional}} {{inside this}} loop {{makes it difficult}} to safely <b>parallelize</b> this loop. When we unswitch the loop, this becomes: ...|$|E
5000|$|Multi-threaded execution, <b>parallelized</b> {{version for}} {{computer}} clusters ...|$|R
30|$|Since {{the weight}} solver was iterative, the {{iterations}} cannot be <b>parallelized.</b> But inside the iterations, parallelization was achievable. For example, the L matrix was formed column by column {{and only one}} column in one iteration, but the element calculations within each column of L could be <b>parallelized.</b>|$|R
40|$|Several large {{applications}} {{have been}} <b>paralleli,zed</b> on Nectar, a network-based multicomputer recently developed by Carnegie Mellon. These applications were previously either too large or too complex {{to be easily}} implemented on distributed memory parallel systems. <b>Parallelizing</b> these applications {{was made possible by}} the cooperative use of many existing general-purpose computers over high-speed networks, and by an implementation methodology based on a clean separation between applicatiion-specific and system-specific code. We illustrate these points using our experience with <b>parallelizing</b> three real-world applications. The success in these applications clearly points out a new direction in parallel processing. 1...|$|R
5000|$|Likewise, {{there is}} a formula for {{combining}} the covariances of two sets {{that can be used}} to <b>parallelize</b> the computation: ...|$|E
50|$|Past {{techniques}} provided {{solutions for}} languages like FORTRAN and C; however, {{these are not}} enough. These techniques dealt with parallelization sections with specific system in mind like loop or particular section of code. Identifying opportunities for parallelization is a critical step while generating multithreaded application. This need to <b>parallelize</b> applications is partially addressed by tools that analyze code to exploit parallelism. These tools use either compile time techniques or run-time techniques. These techniques are built-in in some parallelizing compilers but user needs to identify <b>parallelize</b> code and mark the code with special language constructs. The compiler identifies these language constructs and analyzes the marked code for parallelization. Some tools <b>parallelize</b> only special form of code like loops. Hence a fully automatic tool for converting sequential code to parallel code is required.|$|E
5000|$|... if: This {{will cause}} the threads to <b>parallelize</b> the task only if a {{condition}} is met. Otherwise the code block executes serially.|$|E
40|$|Abstract. Recent {{innovations}} in automatic <b>parallelizing</b> compilers are showing impressive speedups on multicore processors using shared memory with asynchronous channels. We have formulated an operational semantics and proved sound a concurrent separation logic to reason about multithreaded programs that communicate asynchronously through channels and share memory. Our logic supports shared channel endpoints (multiple producers and consumers) and introduces histories to overcome limitations with local reasoning. We demonstrate how to transform a sequential proof into a <b>parallelized</b> proof that targets {{the output of}} the <b>parallelizing</b> optimization DSWP (Decoupled Software Pipelining). ...|$|R
40|$|A {{state-of-the-art}} {{parallel programming}} environment called UPPER (User-interactive Parallel Programming EnviRonment) {{is presented in}} this paper. Parallel machines which execute programs concurrently on {{hundreds or thousands of}} processors provide far more computational power than does a uniprocessor. However, designing parallel programs on parallel machines manually is very difficult and error-prone. Due to these problems, many tools which help programmers translate sequential programs into <b>parallelized</b> programs or even help them design parallel programs have been developed. The proposed environment also has the same purpose. The major components of this environment include a <b>parallelizing</b> compiler system and simulators of the given target machines. The <b>parallelizing</b> compiler system introduces new and existing techniques for compiler-time analysis, and the simulator can simulate execution of the translated <b>parallelized</b> program on the target machine and show the simulated performance rep [...] ...|$|R
40|$|This paper {{presents}} {{a model to}} evaluate the performance and overhead of <b>parallelizing</b> sequential code using compiler directives for multiprocessing on distributed shared memory (DSM) systems. With increasing popularity of shared address space architectures, {{it is essential to}} understand their performance impact on programs that benefit from shared memory multiprocessing. We present a simple model to characterize the performance of programs that are <b>parallelized</b> using compiler directives for shared memory multiprocessing. We <b>parallelized</b> the sequential implementation of NAS benchmarks using native Fortran 77 compiler directives for an Origin 2000, which is a DSM system based on a cache-coherent Non Uniform Memory Access (ccNUMA) architecture. We report measurement based performance of these <b>parallelized</b> benchmarks from four perspectives: efficacy of parallelization process; scalability; parallelization overhead; and comparison with hand-parallelized and -optimized version of the same benchmarks. Our results indicate that sequential programs can conveniently be <b>parallelized</b> for DSM systems using compiler directives but realizing performance gains as predicted by the performance model depends primarily on minimizing architecture-specific data locality overhead...|$|R
5000|$|Is it {{worthwhile}} to <b>parallelize</b> it? This answer requires a reliable estimation (modeling) {{of the program}} workload and {{the capacity of the}} parallel system.|$|E
50|$|Cosmology@Home uses an {{innovative}} way of using machine learning to effectively <b>parallelize</b> a large computational task that involves many inherently sequential calculations over an extremely {{large number of}} distributed computers.|$|E
5000|$|To {{compare the}} constraint-based {{polyhedral}} model to prior approaches such as individual loop transformations and the unimodular approach, consider {{the question of}} whether we can <b>parallelize</b> (execute simultaneously) the iterations of following contrived but simple loop: ...|$|E
50|$|There are {{a variety}} of methodologies for <b>parallelizing</b> loops.|$|R
5000|$|... max(M &minus; 2, N) <b>parallelized</b> {{interactions}} for intermediate stage.|$|R
40|$|Programmer doesn’t define how {{computation}} is <b>parallelized</b> Compiler <b>parallelizes</b> {{the execution}} automatically Language’s constructs are inherently parallel Often purely functional languages (single-assignment) Parallelization not programmed ⇒ no parallelization bugs in code, easier to program Explicit Parallelism [8] Is explicitly {{defined by the}} programmer Can be difficult to program, debugging is har...|$|R
