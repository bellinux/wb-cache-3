5|6|Public
40|$|We {{introduce}} the Semantic Depth of Field (SDOF) technique, {{which is an}} alternative focus+context method for information visualisation. SDOF blurs objects which are currently out of focus, i. e., not interesting for the user. In the experimental study described in this paper, we found that SDOF supports the <b>preattentive</b> <b>perception</b> of sharp targets and the numerical estimation {{of the amount of}} data being in focus. We investigated the influence of distracting encodings (color and orientation), as well as threshold values for human perception. We also tested applications of SDOF (a textviewer and a scatterplot) with regard to their effectiveness in guiding the user's attention and with highlighting of data items...|$|E
40|$|The {{present study}} was {{intended}} to make electrophysiological investigations into the <b>preattentive</b> <b>perception</b> of native and non-native speech sounds. We recorded the mismatch negativity, elicited by single syllable change of both native and non-native speech-sound contrasts in tonal languages. EEGs were recorded and low-resolution brain electromagnetic tomography (LORETA) was utilized to explore the neural electrical activity. Our results suggested that the left hemisphere was predominant {{in the perception of}} native speech sounds, whereas the non-native speech sound was perceived predominantly by the right hemisphere, which may be explained by the specialization in processing the prosodic and emotional components of speech formed in this hemisphere...|$|E
40|$|This paper generalizes {{the basic}} idea of blobs in <b>preattentive</b> <b>perception</b> using color information. This is used as {{the base of a}} basic {{classification}} of low resolution pictures taken with mobile phones. This classification, the blob-like representation of the image and other information in user’s context, such as GPS information, can be used in the presented framework as the basis of a new graphical interface for HCI (Human Computer Interaction). Similar systems whether they work with global properties of the image, which leads to inaccurate results, or with complex segmentation process that fails to capture expected objects in the scene. Most of those systems do not pay attention on other information involved {{in the creation of the}} image, such as time or location. We describe a system which uses geographical information associated with a picture in a mobile phone terminal, and with a fast segmentation based on color categorization. 1...|$|E
40|$|We {{present a}} model of human <b>preattentive</b> texture <b>perception.</b> This model {{consists}} of three stages: (1) convolution of the image with a bank of even-symmetric linear filters followed by half-wave rectification to give a set of responses modeling outputs of V 1 simple cells, (2) inhibition, localized in space, within and among the neural-response profiles that results in the suppression of weak responses when there are strong responses at the same or nearby locations, and (3) texture-boundary detection by using wide odd-symmetric mechanisms. Our model can predict the salience of texture boundaries in any arbitrary gray-scale image. A computer implementation of this model has been tested {{on many of the}} classic stimuli from psychophysical literature. Quantitative predictions of the degree of discriminability of different texture pairs match well with experimental measurements of discriminability in human observers...|$|R
40|$|Abstract. Textons {{refer to}} {{fundamental}} micro-structures in generic natural images and thus constitute {{the basic elements}} in early (<b>preattentive)</b> visual <b>perception.</b> However, the word “texton ” remains a vague concept {{in the literature of}} computer vision and visual perception, and a precise mathematical definition has yet to be found. In this article, we argue that the definition of texton should be governed by a sound mathematical model of images, and the set of textons must be learned from, or best tuned to, an image ensemble. We adopt a generative image model that an image is a superposition of bases from an over-complete dictionary, then a texton is defined as a mini-template that consists of a varying number of image bases with some geometric and photometric configurations. By analogy to physics, if image bases are like protons, neutrons and electrons, then textons are like atoms. Then a small number of textons can be learned from training images as repeating micro-structures. We report four experiments for comparison. The first experiment computes clusters in feature space of filter responses. The second use transformed component analysis in both feature space and image patches. The third adopts a two-layer generative model where an image is generated by image bases and image bases are generated by textons. The fourth experiment shows textons from motion image sequences, which we call movetons. ...|$|R
40|$|AbstractTexture {{segmentation}} of ‘target’ Gabors from {{an array}} of ‘background’ Gabors was {{measured in terms of}} the difference in orientation between the two regions, as well as the difference in orientation within each region. Segmentation was shown to occur on the basis of local orientation differences at the boundary between the target and background regions (Nothdurft, H. C. (1992). Feature analysis and the role of similarity in <b>preattentive</b> vision. <b>Perception</b> and Psychophysics, 52, 355 – 375.). We obtained similar results for both the amblyopic and non-amblyopic eye of three strabismic amblyopes, and showed also that the effects of texture undersampling and positional jitter were similar for the two eyes. This pattern of results is consistent with intact mechanisms of texture perception in amblyopic cortex, and suggests also that any amblyopic deficits in first-order cortical units (undersampling and/or positional uncertainty) do not limit higher-order texture segmentation processes. Therefore, first- and second-order processes involved in perceptual grouping of oriented elements (that appear to be abnormal in amblyopic cortex; Kovács, I., Polat, U., Norcia, A. M. (1996). Breakdown of binding mechanisms in amblyopia. Association for Research in Vision and Ophthalmology Abstracts; Mussap, A. J., Levi, D. M. (1995). Amblyopic deficits in perception of second-order orientation. Investigative Ophthalmology and Visual Science (Supplement), 36, S 634; Mussap, A. J., Levi, D. M. (1998). Amblyopic deficits in perceptual grouping. Vision Research, submitted) do not contribute to texture perception based on orientation contrast...|$|R
40|$|<b>Preattentive</b> <b>perception</b> of {{occasional}} deviating stimuli in {{the stream}} of standard stimuli can be recorded with cognitive event-related potential (ERP) mismatch negativity (MMN). The earlier detection of stimuli at the auditory cortex can be examined with N 1 and P 2 ERPs. The MMN recording does not require co-operation, it correlates with perceptual threshold, and even complex sounds {{can be used as}} stimuli. The aim {{of this study was to}} examine different aspects that should be considered when measuring discrimination of hearing with ERPs. The MMN was found to be stimulusintensity- dependent. As the intensity of sine wave stimuli was increased from 40 to 80 dB HL, MMN mean amplitudes increased. The effect of stimulus frequency on the MMN was studied so that the pitch difference would be equal in each stimulus block according to the psychophysiological mel scale or the difference limen of frequency (DLF). However, the blocks differed from each other. The contralateral white noise masking (50 dB EML) was found to attenuate the MMN amplitude when the right ear was stimulated. The N 1 amplitude was attenuated and, in contrast, P 2 amplitude was not affected by contralateral white noise masking. The perception and production of vowels by four postlingually deafened patients with a cochlear implant were studied. The MMN response could be elicited in the patient with the best vowel perception abilities. The results of the studies show that concerning the MMN recordings, the stimulus parameters and recording procedure design have a great influence on the results...|$|E
40|$|When {{a person}} glimpses {{the face of}} a famous actor, sniffs a {{favorite}} food or hears the voice of a friend, recognition is instant. Within {{a fraction of a second}} after the eyes, nose, ears, tongue or skin is stimulated, one knows the object is familiar and whether it is desirable or dangerous. How does such recognition, which psychologists call <b>preattentive</b> <b>perception,</b> happen so accurately and quickly, even when the stimuli are complex and the context in which they arise varies? Much is known about the way the cerebral cortex, the outer rind of the brain, initially analyzes sensory messages. Yet investigations are only now beginning to suggest how the brain moves beyond the mere extraction of features-how it combines sensory messages with past experience and with expectation to identify both the stimulus and its particular meaning to the individual. My own group's studies, carried out over more than 30 years at the University of California at Berkeley, suggest that perception cannot be understood solely by examining properties of individual neurons, a microscopic approach that currently dominates neuroscience research. We have found that perception depends on the simultaneous, cooperative activity of millions of neurons spread throughout expanses of the cortex. Such global activity can be identified, measured and explained only if one adopts a macroscopic view alongside the microscopic one. There is an analogy to this approach in music. To grasp the beauty in a choral piece, it is not enough to listen to the individual singers sequentially. One must hear the performers together, as they modulate their voices and timing in response to one another. Our studies have led us as well to the discovery in the brain of chaos- complex behavior that seems random but actually has some hidden order. The chaos is evident in the tendency of vast collections of neurons to shift abruptly and simultaneously from one complex activity pattern to another in response to the smallest of inputs. This changeability is a prime characteristic of many chaotic systems. It is not harmful in the brain. In fact, we propose it is the very property that makes perception possible. We also speculate that chaos underlies the ability of the brain to respond flexibly to the outside world and to generate novel activity patterns, including those that are experienced as fresh ideas. ...|$|E
40|$|Abstract—An {{elementary}} {{characterization of}} the map underlying Harris corners, also known as Harris interest points or key points, is provided. Two principal and basic assumptions made are: 1) Local image structure is captured in an uncommitted way, simply using weighted raw image values around every image location to describe the local image information, and 2) the lower the probability of observing the image structure present in a particular point, the more salient, or interesting, this position is, i. e., saliency is related to how uncommon it is to see a certain image structure, how surprising it is. Through the latter assumption, the axiomatization proposed makes a sound link between image saliency in computer vision {{on the one hand}} and, on the other, computational models of <b>preattentive</b> human visual <b>perception,</b> where exactly the same definition of saliency has been proposed. Because of this link, the characterization provides a compelling case in favor of Harris interest points over other approaches...|$|R
40|$|An {{elementary}} {{characterization of}} the map underlying Harris corners, also known as Harris interest points or key points, is provided. Two principal and basic assumptions made are: 1) Local image structure is captured in an uncommitted way, simply using weighted raw image values around every image location to describe the local image information, and 2) the lower the probability of observing the image structure present in a particular point, the more salient, or interesting, this position is, i. e., saliency is related to how uncommon it is to see a certain image structure, how surprising it is. Through the latter assumption, the axiomatization proposed makes a sound link between image saliency in computer vision {{on the one hand}} and, on the other, computational models of <b>preattentive</b> human visual <b>perception,</b> where exactly the same definition of saliency has been proposed. Because of this link, the characterization provides a compelling case in favor of Harris interest points over other approaches. MediamaticsElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Our {{large-scale}} {{computational model}} of the primary visual cortex that incorporates orientation-specific, long-range couplings with slow NMDA conductances operates in a fluctuating dynamic state of intermittent desuppression (IDS), which captures the behavior of coherent spontaneous cortical activity, as revealed by in vivo optical imaging based on voltage-sensitive dyes. Here, we address the functional significance of the IDS cortical operating points by investigating our model cortex response to the Hikosaka line-motion illusion (LMI) stimulus—a cue of a quickly flashed stationary square followed a few milliseconds later by a stationary bar. As revealed by voltage-sensitive dye imaging, there is an intriguing similarity between the cortical spatiotemporal activity in response to (i) the Hikosaka LMI stimulus and (ii) a small moving square. This similarity {{is believed to be}} associated with the <b>preattentive</b> illusory motion <b>perception.</b> Our numerical cortex produces similar spatiotemporal patterns in response to the two stimuli above, which are both in very good agreement with experimental results. The essential network mechanisms underpinning the LMI phenomenon in our model are (i) the spatiotemporal structure of the LMI input as sculpted by the lateral geniculate nucleus, (ii) a priming effect of the long-range NMDA-type cortical coupling, and (iii) the NMDA conductance–voltage correlation manifested in the IDS state. This mechanism in our model cortex, in turn, suggests a physiological underpinning for the LMI-associated patterns in the visual cortex of anaesthetized cat...|$|R

