2641|3835|Public
5|$|Parallel {{computers}} can be roughly classified {{according to the}} level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized <b>parallel</b> <b>computer</b> architectures are sometimes used alongside traditional processors, for accelerating specific tasks.|$|E
5|$|Main {{memory in}} a <b>parallel</b> <b>computer</b> is either shared memory (shared between all {{processing}} {{elements in a}} single address space), or distributed memory (in which each processing element has its own local address space). Distributed memory refers {{to the fact that}} the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.|$|E
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). <b>Parallel</b> <b>computer</b> systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.|$|E
40|$|Low-cost <b>parallel</b> <b>computers</b> such as PC {{clusters}} {{are becoming}} available, and many previously unsolvable {{problems can be}} solved using such computers. However, designing algorithms that perform well on <b>parallel</b> <b>computers</b> is often challenging. The focus of this course is on learning how to design algorithms for <b>parallel</b> <b>computers</b> and how to evaluate them...|$|R
40|$|Simple Skeleton Particle-in-Cell codes {{designed}} for massively <b>parallel</b> <b>computers</b> are described. These codes {{are used to}} develop new algorithms and evaluate new <b>parallel</b> <b>computers.</b> Benchmark results {{from a number of}} MIMD <b>parallel</b> <b>computers</b> are presented. One, two and three dimensional skeleton particle-in-cell codes have been developed for <b>parallel</b> <b>computers</b> to provide a testbed where new algorithms can be developed and tested and new computer architectures can be evaluated. These codes have been deliberately kept to a minimum, but they include all the essentia...|$|R
40|$|CFD or Computational Fluid Dynamics {{is one of}} the {{scientific}} disciplines that has always posed new challenges to the capabilities of the modern, ultra-fast supercomputers, and now to the even faster <b>parallel</b> <b>computers.</b> For applications where number crunching is of primary importance, there is perhaps no escaping <b>parallel</b> <b>computers</b> since sequential computers can only be (as projected) as fast as a few gigaflops and no more, unless, of course, some altogether new technology appears in future. For <b>parallel</b> <b>computers,</b> on the other hand, there is no such limit since any number of processors can be made to work in parallel. Computationally demanding CFD codes and <b>parallel</b> <b>computers</b> are therefore soul-mates, and will remain so for all foreseeable future. So much so that there is a separate and fast-emerging discipline that tackles problems specific to CFD as applied to <b>parallel</b> <b>computers.</b> For some years now, there is an international conference on parallel CFD. So, one can indeed say that parallel CFD has arrived. To understand how CFD codes are parallelized, one must understand a little about how <b>parallel</b> <b>computers</b> function. Therefore, in what follows we will first deal with <b>parallel</b> <b>computers,</b> how a typical CFD code (if there is one such) looks like, and then the strategies of parallelization...|$|R
5|$|In {{the early}} 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started {{developing}} {{what came to}} be known as the Society of Mind theory which views biological brain as massively <b>parallel</b> <b>computer.</b> In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.|$|E
25|$|In {{addition}} {{to use on}} desktops for personal scientific computing, Octave is used in academia and industry. For example, Octave was used on a massive <b>parallel</b> <b>computer</b> at Pittsburgh supercomputing center to find vulnerabilities related to guessing social security numbers.|$|E
25|$|<b>Parallel</b> <b>computer</b> {{programs}} {{are more difficult}} to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.|$|E
40|$|This paper {{describes}} the attempts for fast volume visualization using current commercial and research <b>parallel</b> <b>computers.</b> Several <b>parallel</b> volume rendering algorithms are described {{together with their}} implementations. Brief introductions to both, volume rendering techniques and <b>parallel</b> <b>computers</b> are included...|$|R
50|$|Neural & Massively <b>Parallel</b> <b>Computers,</b> 1988.|$|R
40|$|The {{purpose of}} this study is to analyze linearized domain {{decomposition}} approaches for different nonlinear boundary value problems (BVPs). Nonlinear BVPs frequently form a large system of equations when they are discretized and require <b>parallel</b> <b>computers</b> to solve this system. Domain decomposition approaches are useful to utilize the advantages of <b>parallel</b> <b>computers</b> in order to solve the differential equations. Cherpion’s single domain linearized iterative technique is quite useful to solve the nonlinear BVPs that have the form u′′ = f(ξ, u, u′). However with this iterative scheme we are not able to solve the BVP using <b>parallel</b> <b>computers.</b> Therefore we extend this iterative scheme to the domain decomposition context so that we can solve the nonlinear BVP on <b>parallel</b> <b>computers.</b> Theoretical and numerical results are given...|$|R
500|$|Similar models (which also view {{biological}} {{brain as}} massively <b>parallel</b> <b>computer,</b> i.e. {{the brain is}} made up of a constellation of independent or semi-independent agents) were also described by: ...|$|E
500|$|Subtasks in a {{parallel}} program are often called threads. Some <b>parallel</b> <b>computer</b> architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, [...] "threads" [...] is generally accepted as a generic term for subtasks. Threads will often need to update some variable that is shared between them. The instructions between the two programs may be interleaved in any order. For example, consider the following program: ...|$|E
500|$|SIMD {{parallel}} computers {{can be traced}} back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a massively <b>parallel</b> <b>computer</b> for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called [...] "the most infamous of supercomputers", because the project was only one fourth completed, but took 11years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.|$|E
50|$|Most <b>parallel</b> <b>computers,</b> as of 2013, are MIMD systems.|$|R
40|$|<b>Parallel</b> <b>computers</b> are {{systems of}} {{multiple}} processors capable of coordinated computa-tion. They vary widely in their scalability, size, cost, application, architecture, construction, {{and complexity of}} design. This thesis focuses on <b>parallel</b> <b>computers</b> that scale to hundreds of nodes, are inexpensive to design and build, and are applicable {{to a range of}} applications...|$|R
40|$|The Bulk Synchronous Parallel model costs {{programs}} {{using three}} parameters, the processor speed (s), the network permeability (g), and the superstep overhead (l). This simple model is accurate {{over a wide}} variety of applications and <b>parallel</b> <b>computers.</b> However, all real <b>parallel</b> <b>computers</b> exhibit behaviour that is not captured by the cost model...|$|R
2500|$|As a polymath, {{he wrote}} nearly two hundred {{professional}} papers, gaining renown in fields from computer science and artificial intelligence to epistemology, and researched high-speed electronics and electro-optics switching devices as a physicist, and in biophysics, {{the study of}} memory and knowledge. He worked on cognition based on neurophysiology, mathematics, and philosophy and was called [...] "one of the most consequential thinkers {{in the history of}} cybernetics". He came to the United States, and stayed after meeting with Warren Sturgis McCulloch, where he received funding from The Pentagon to established the Biological Computer Laboratory, which built the first <b>parallel</b> <b>computer,</b> the Numa-Rete. Working with William Ross Ashby, one of the original Ratio Club members, and together with Warren McCulloch, Norbert Wiener, John von Neumann and Lawrence J. Fogel, Heinz von Foerster was an architect of cybernetics and {{one of the members of}} the Macy conferences, eventually becoming editor of its early proceedings alongside Hans-Lukas Teuber and Margaret Mead.|$|E
5000|$|... #Subtitle level 2: Biological {{brain as}} massively <b>parallel</b> <b>computer</b> ...|$|E
5000|$|... #Caption: Intel iPSC/2 16-node <b>parallel</b> <b>computer.</b> August 22, 1995.|$|E
40|$|Random number {{generators}} {{are used}} in many applications, from slot machines to simulations of nuclear reactors. For many computational science applications, such as Monte Carlo simulation, {{it is crucial that}} the generators have good randomness properties. This is particularly true for large-scale simulations done on high-performance <b>parallel</b> <b>computers.</b> Good random number generators are hard to find, and many widely-used techniques {{have been shown to be}} inadequate. Finding high-quality, efficient algorithms for random number generation on <b>parallel</b> <b>computers</b> is even more difficult. Here we present a review of the most commonly-used random number generators for <b>parallel</b> <b>computers,</b> and evaluate each generator based on theoretical knowledge and empirical tests. In conclusion, we provide recommendations for using random number generators on <b>parallel</b> <b>computers.</b> Outline This review is organized as follows: A brief summary of the findings of this review is first presented, giving an overview of th...|$|R
40|$|We are {{developing}} a compiler that translates ordinary MATLAB scripts into code suitable for compilation and execution on <b>parallel</b> <b>computers</b> supporting C and the MPI message-passing library. In this paper we report the speedup achieved for several MATLAB scripts on three diverse parallel architectures: a distributed-memory multicomputer (Meiko CS- 2), a symmetric multiprocessor (Sun Enterprise Server 4000), and a cluster of symmetric multiprocessors (Sun SPARCserver 20 s). By generating code suitable for execution on <b>parallel</b> <b>computers,</b> our system multiplies the gains achievable by compiling, rather than interpreting, MATLAB scripts. Generating parallel code has an additional advantage: the amount of primary memory available on most <b>parallel</b> <b>computers</b> {{makes it possible to}} solve problems too large to solve on a single workstation. 1. Introduction Many problems that scientists and engineers want to solve are computationally intensive. Since <b>parallel</b> <b>computers</b> offer the potential for high c [...] ...|$|R
5000|$|The Royal Institute of Technologies - Center for <b>Parallel</b> <b>Computers,</b> (KTH), Stockholm, Sweden ...|$|R
5000|$|Leonard Uhr (Ed.) <b>Parallel</b> <b>Computer</b> Vision. Boston: Academic Press. 1987.|$|E
5000|$|World’s First <b>Parallel</b> <b>Computer</b> Based on Biomolecular Motors (DOI: 10.1073/pnas.1510825113) ...|$|E
5000|$|IEEE Fellow, 2008: Awarded for his {{contributions}} in <b>Parallel</b> <b>Computer</b> Architectures and Compilers.|$|E
40|$|We {{present a}} new {{approach}} for reconfigurable massively <b>parallel</b> <b>computers.</b> The approach uses FPGA as reconfig-urable device to build <b>parallel</b> <b>computers</b> which can adapt their physical topology to match the virtual topology used to model the parallel computation paradigm of a given appli-cation. We use {{a case study in}} which a virtual ring topology is first simulated on a tree topology and then directly im-plemented in an FPGA configuration. Preliminary results show that we can increase the performance of the <b>parallel</b> <b>computers</b> which make use of message passing interface by a factor of up to 20 % if a reconfigurable topology approach is used. ...|$|R
40|$|ISBN: 0769503713 One way {{to improve}} {{reliability}} in <b>parallel</b> <b>computers</b> consists of adding supplementary processors and interconnections to the functional structure in order to replace faulty processors {{with respect to the}} network structure. This approach is named structural fault tolerance (SFT). Very integrated <b>parallel</b> <b>computers</b> are one way to implement a parallel structure. The material structure is then composed of many elementary blocks, such as ASICs or multi-chip modules (MCMs), each containing many processors. We show that former SFT methods fail in combining the different features, constraints and requirements of such structures. Thus, this paper introduces a new reconfiguration approach that is dedicated to very integrated <b>parallel</b> <b>computers...</b>|$|R
50|$|Some system {{designers}} building <b>parallel</b> <b>computers</b> pick CPUs {{based on}} the speed per dollar.|$|R
5000|$|... #Caption: Intel iPSC/860 32-node <b>parallel</b> <b>computer</b> {{running a}} Tachyon {{performance}} test. August 22, 1995.|$|E
50|$|Hypertrees are {{a choice}} for <b>parallel</b> <b>computer</b> architecture, used, e.g., in the {{connection}} machine CM-5.|$|E
50|$|The first {{computer}} {{that was not}} serial (the first <b>parallel</b> <b>computer)</b> was the Whirlwind — 1951.|$|E
5000|$|... 2003 David Kent IV, New Quantum Monte Carlo Algorithms to Efficiently Utilize Massively <b>Parallel</b> <b>Computers</b> ...|$|R
40|$|A {{parallel}} {{ray tracing}} library is presented for rendering high detail images of three dimensional geometry and computational fields. The library {{has been developed}} for use on distributed memory and shared memory <b>parallel</b> <b>computers</b> and can also run on sequential computers. Parallelism is achieved {{through the use of}} message passing and threads. It is shown that the library achieves almost linear scalability when run on large distributed memory <b>parallel</b> <b>computers</b> as well as large shared memory <b>parallel</b> <b>computers.</b> Several applications of parallel rendering are explored including rendering of CAD models, animation, magnetic resonance imaging, and visualization of volumetric flow fields. Ray tracing offers many advantages over polygon rendering techniques, in its innate parallelism, and quality of output...|$|R
5000|$|... "VR-CUBE" [...] at Center for <b>Parallel</b> <b>Computers</b> (PDC) at the Royal Institute of Technology (KTH) in Stockholm, Sweden.|$|R
