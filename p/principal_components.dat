10000|10000|Public
5|$|In January and February 2015, 1450 {{malnourished}} or sick {{sea lion}} pups {{have been found}} {{on the coast of}} California, and estimations give a higher number of dead pups. National Oceanic and Atmospheric Administration has pointed the cause to unprecedentedly warm PDO in the North American west coast {{as a result of a}} failed attempt at El Niño, which has reduced the abundance of anchovies, sardines and mackerel, <b>principal</b> <b>components</b> of the sea lion diet just in the pups nursery season. Pups are leaving the rookeries in search of food long before they are capable of hunting fish. The outcome is pups really malnourished washed out on the shores. These conditions lead some of these pups to death if not rescued in time.|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called <b>principal</b> <b>components</b> analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to <b>principal</b> <b>components</b> and the eigenvalues to the variance explained by the <b>principal</b> <b>components.</b> Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the <b>principal</b> <b>components</b> that are associated with most of the covariability among a number of observed data.|$|E
25|$|Dimensional reduction: Analysts often {{reduce the}} number of {{dimensions}} (genes) prior to data analysis. This may involve linear approaches such as <b>principal</b> <b>components</b> analysis (PCA), or non-linear manifold learning (distance metric learning) using kernel PCA, diffusion maps, Laplacian eigenmaps, local linear embedding, locally preserving projections, and Sammon's mapping.|$|E
5000|$|<b>Principal</b> <b>component</b> {{analysis}} (PCA) and <b>principal</b> <b>component</b> regression ...|$|R
30|$|Step 2 Calculate the <b>principal</b> <b>component</b> loading matrix by <b>principal</b> <b>component</b> analysis.|$|R
40|$|Probabilistic <b>Principal</b> <b>Component</b> Analysis is a {{reformulation}} of {{the common}} multivariate analysis technique known as <b>Principal</b> <b>Component</b> Analysis. It employs a latent variable model framework similar to factor analysis allowing to establish a maximum likelihood solution for the parameters that comprise the model. One of the main assumptions of Probabilistic <b>Principal</b> <b>Component</b> Analysis is that observed data is independent and identically distributed. This assumption is inadequate for many applications, in particular, for modeling sequential data. In this paper, the authors introduce a temporal version of Probabilistic <b>Principal</b> <b>Component</b> Analysis by using a hidden Markov model {{in order to obtain}} optimized representations of observed data through time. Combining Probabilistic <b>Principal</b> <b>Component</b> Analyzers with a hidden Markov model, it is possible to enhance the capabilities of transformation and reduction of time series vectors. In order to find automatically the dimensionality of the principal subspace associated with these Probabilistic <b>Principal</b> <b>Component</b> Analyzers through time, a Bayesian treatment of the <b>Principal</b> <b>Component</b> model is introduced as well. Keywords: Hidden Markov models, <b>principal</b> <b>component</b> analysis, bayesian <b>principal</b> <b>component</b> analysis, EM algorithm, model selection 1...|$|R
25|$|In solid mechanics, {{the stress}} tensor is {{symmetric}} {{and so can}} be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the <b>principal</b> <b>components.</b>|$|E
25|$|Finns show {{very little}} if any Mediterranean and African genes {{but on the other}} hand almost 10% of Finnish genes seem to be shared with Siberian populations. Nevertheless, more than 80% of Finnish genes are from a single ancient Northeastern European population, while most Europeans are a mixture of 3 or more <b>principal</b> <b>components.</b>|$|E
25|$|Other <b>principal</b> <b>components</b> are the crown, {{band and}} insignia, {{typically}} a cap badge and embroidery {{in proportion to}} rank. Piping is also often found, typically {{in contrast to the}} crown colour, which is usually white for navy, blue for air force and green for army. The band is typically a dark, contrasting colour, often black, but may be patterned or striped.|$|E
40|$|This paper {{proposes a}} novel methodology, {{based on the}} Common <b>Principal</b> <b>Component</b> analysis, {{allowing}} one to estimate the factors driving the term structure of interest rates, {{in the presence of}} time-varying covariance structure. The advantages of this method are first, that, unlike classical <b>principal</b> <b>component</b> analysis, common factors can be estimated without assuming that the volatility of the factors is constant; and second, that the factor structure can be decomposed into permanent and transitory common factors. We conclude that only permanent factors are relevant for modeling the dynamics of interest rates, and that the common <b>principal</b> <b>component</b> approach appears to be more accurate than the classical <b>principal</b> <b>component</b> one to estimate the risk factor structure. Term Structure of Interest Rates, <b>Principal</b> <b>Component</b> Analy-sis, Common <b>Principal</b> <b>Component</b> Analysis...|$|R
50|$|Careful {{comparison}} of the random initiation approach to <b>principal</b> <b>component</b> initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of <b>principal</b> <b>component</b> SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. <b>Principal</b> <b>component</b> initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first <b>principal</b> <b>component</b> (quasilinear sets). For nonlinear datasets, however, random initiation performs better.|$|R
40|$|International audienceGiven a {{stationary}} multidimensional {{process and the}} process which is deduced after centering, we wish to study possible links between the <b>principal</b> <b>component</b> analyses, in the frequency domain, of these two processes. It is well-known that there is, a priori, no obvious relationship between the centered and non-centered <b>principal</b> <b>component</b> analyses in the temporal domain. Furthermore, {{we also know that}} <b>principal</b> <b>component</b> analysis in the frequency domain is reduced to <b>principal</b> <b>component</b> analysis of each spectral component. In this paper, we show the remarkable result that the centered and non-centered <b>principal</b> <b>component</b> analyses in the frequency domain are equal except for a given frequency...|$|R
25|$|With the {{development}} of statistical techniques and numerical taxonomy in the 1960s, mathematical methods (including Cluster analysis, <b>Principal</b> <b>components</b> analysis, correspondence analysis and Factor analysis) {{have been used to}} build typologies. These techniques provide a qualitative way to articulate the degrees of consistency among particular attributes. Correlation coefficients created by these methods help archaeologists discern between meaningful and useless similarities between artefacts. During the 1990s archaeologists began to use phylogenetic methods borrowed from Cladistics.|$|E
25|$|There are tools on {{the site}} {{for a wide range}} of {{functions}} that range from simple graphical displays of variation in gene expression or other phenotypes, scatter plots of pairs of traits (Pearson or rank order), construction of both simple and complex network graphs, analysis of <b>principal</b> <b>components</b> and synthetic traits, QTL mapping using marker regression, interval mapping, and pair scans for epistatic interactions. Most functions work with up to 100 traits and several functions work with an entire transcriptome.|$|E
25|$|Russell {{claimed that}} he was more {{convinced}} of his method of doing philosophy than of his philosophical conclusions. Science was one of the <b>principal</b> <b>components</b> of analysis. Russell was a believer in the scientific method, that science reaches only tentative answers, that scientific progress is piecemeal, and attempts to find organic unities were largely futile. He believed the same was true of philosophy. Russell held that the ultimate objective of both science and philosophy was to understand reality, not simply to make predictions.|$|E
40|$|Classical <b>principal</b> <b>component</b> {{analysis}} on manifolds, e. g. on Kendall’s shape spaces, {{is carried out}} in the tangent space of a Euclidean mean equipped with a Euclidean metric. We propose a method of <b>principal</b> <b>component</b> analysis for Riemannian manifolds based on geodesics of the intrinsic metric and provide for a numerical implementation in case of spheres. This method allows e. g. to compare <b>principal</b> <b>component</b> geodesics of different data samples. In order to determine <b>principal</b> <b>component</b> geodesics, we show that in general, due to curvature, the <b>principal</b> <b>component</b> geodesics do not pass through the intrinsic mean. As a consequence other means, different from the intrinsic mean, enter the setting allowing for several choices of a definition for geodesic variance. In conclusion we apply our method to the space of planar triangular shapes and compare our findings with standard Euclidean <b>principal</b> <b>component</b> analysis...|$|R
40|$|I {{examine the}} self-consistency of a <b>principal</b> <b>component</b> axis; that is, when a {{distribution}} is centered about a <b>principal</b> <b>component</b> axis. A <b>principal</b> <b>component</b> axis of a random vector X is self-consistent if each {{point on the}} axis corresponds to the mean of X given that X projects orthogonally onto that point. A large class of symmetric multivariate distributions are examined in terms of self-consistency of <b>principal</b> <b>component</b> subspaces. Elliptical distributions are characterized by the preservation of self-consistency of <b>principal</b> <b>component</b> axes after arbitrary linear transformations. A 2 ̆ 2 lack-of-fit 2 ̆ 2 test is proposed that tests for self-consistency of a principal axis. The test is applied to two real datasets...|$|R
40|$|FIGURE 3. Scatterplots of {{the first}} versus the second <b>principal</b> <b>component</b> scores of a <b>Principal</b> <b>Component</b> Analysis of the morphometric {{variables}} measured from male specimens of Syndactyla dimidiata (see Table 2). Given the small sample size, we did not perform a PCA analysis for females. Factor loadings are presented as {{a table in the}} upper left corner of the figure. The first <b>principal</b> <b>component</b> (PC 1) accounted for 49. 7 % of the variation, and the second <b>principal</b> <b>component</b> (PC 2) explained 24. 9 %...|$|R
25|$|A study {{conducted}} by the HUGO Pan-Asian SNP Consortium in 2009 used <b>principal</b> <b>components</b> analysis, which makes no prior population assumptions, on genetic data sampled from a large number of points across Asia. They said that East Asian and South-East Asian populations clustered together, and suggested a common origin for these populations. At the same time they observed a broad discontinuity between this cluster and South Asia, commenting most of the Indian populations showed evidence of shared ancestry with European populations. The study said that genetic ancestry is strongly correlated with linguistic affiliations as well as geography.|$|E
25|$|Principal {{component}} regression (PCR) is {{used when}} the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal component analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant <b>principal</b> <b>components</b> of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|E
25|$|Abduction, deduction, and {{induction}} make incomplete {{sense in}} isolation from one another but comprise a cycle understandable as a whole insofar as they collaborate toward the common end of inquiry. In the pragmatic {{way of thinking about}} conceivable practical implications, every thing has a purpose, and, as possible, its purpose should first be denoted. Abduction hypothesizes an explanation for deduction to clarify into implications to be tested so that induction can evaluate the hypothesis, in the struggle to move from troublesome uncertainty to more secure belief. No matter how traditional and needful it is to study the modes of inference in abstraction from one another, the integrity of inquiry strongly limits the effective modularity of its <b>principal</b> <b>components.</b>|$|E
40|$|<b>Principal</b> <b>component</b> {{analysis}} {{is one of}} the most important and powerful methods in chemometrics as well as in a wealth of other areas. This paper provides a description of how to understand, use, and interpret <b>principal</b> <b>component</b> analysis. The paper focuses on the use of <b>principal</b> <b>component</b> analysis in typical chemometric areas but the results are generally applicable...|$|R
40|$|Face {{recognition}} {{has been}} a very valuable research to pattern recognition and face recognition systems. These years, two-dimensional <b>principal</b> <b>component</b> analysis and kernel <b>principal</b> <b>component</b> analysis have been successfully applied in face recognition systems. However, there is still some space for us to make it better. This study has proposed a novel approach based on Two-dimensional <b>Principal</b> <b>Component</b> Analysis (2 DPCA) and Kernel <b>Principal</b> <b>Component</b> Analysis (KPCA) for face recognition. The proposed approach first performs two-dimensional <b>principal</b> <b>component</b> analysis process to project the faces onto the feature pace and then performs kernel <b>principal</b> <b>component</b> analysis on the projected data. And finally, one nearest neighbor classifier based on Euclidean distance is used for recognizing faces. The experiments on ORL face database, Yale face database and FERET face database show that the proposed approach gives a high recognition rate of 100 % and outperforms state-of-the-art approaches and demonstrates promising applications...|$|R
50|$|Robust <b>principal</b> <b>component</b> {{analysis}} (RPCA) is {{a modification}} of the widely used statistical procedure <b>principal</b> <b>component</b> analysis (PCA) which works well with respect to grossly corrupted observations.|$|R
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated <b>principal</b> <b>components</b> analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
25|$|The {{ecological}} {{restoration of}} islands, or island restoration, is {{the application of}} the principles of ecological restoration to islands and island groups. Islands, due to their isolation, are home to many of the world's endemic species, as well as important breeding grounds for seabirds and some marine mammals. Their ecosystems are also very vulnerable to human disturbance and particularly to introduced species, due to their small size. Island groups such as New Zealand and Hawaii have undergone substantial extinctions and losses of habitat. Since the 1950s several organisations and government agencies around the world have worked to restore islands to their original states; New Zealand has used them to hold natural populations of species that would otherwise be unable to survive in the wild. The <b>principal</b> <b>components</b> of island restoration are the removal of introduced species and the reintroduction of native species.|$|E
2500|$|C38 	Classification Methods • Cluster Analysis • <b>Principal</b> <b>Components</b> • Factor Models ...|$|E
40|$|The GeoPCA {{package is}} the first tool {{developed}} for multivariate analysis of dihedral angles based on <b>principal</b> <b>component</b> geodesics. <b>Principal</b> <b>component</b> geodesic analysis provides a natural generalization of <b>principal</b> <b>component</b> analysis for data distributed in non-Euclidean space, {{as in the case}} of angular data. GeoPCA presents projection of angular data on a sphere composed of the first two <b>principal</b> <b>component</b> geodesics, allowing clustering based on dihedral angles as opposed to Cartesian coordinates. It also provides a measure of the similarity between input structures based on only dihedral angles, in analogy to the root-mean-square deviation of atoms based on Cartesian coordinates. The <b>principal</b> <b>component</b> geodesic approach is shown herein to reproduce clusters of nucleotides observed in an η–θ plot. GeoPCA can be accessed via [URL]...|$|R
40|$|In this paper, {{we study}} {{the problem of}} sparse <b>Principal</b> <b>Component</b> Analysis (PCA) in the high-dimensional setting with missing observations. Our goal is to {{estimate}} the first <b>principal</b> <b>component</b> when we only have access to partial observations. Existing estimation techniques are usually derived for fully observed data sets and require a prior knowledge of the sparsity of the first <b>principal</b> <b>component</b> {{in order to achieve}} good statistical guarantees. Our contributions is threefold. First, we establish the first information-theoretic lower bound for the sparse PCA problem with missing observations. Second, we propose a simple procedure that does not require any prior knowledge on the sparsity of the unknown first <b>principal</b> <b>component</b> or any imputation of the missing observations, adapts to the unknown sparsity of the first <b>principal</b> <b>component</b> and achieves the optimal rate of estimation up to a logarithmic factor. Third, if the covariance matrix of interest admits a sparse first <b>principal</b> <b>component</b> and is in addition approximately low-rank, then we can derive a completely data-driven procedure computationally tractable in high-dimension, adaptive to the unknown sparsity of the first <b>principal</b> <b>component</b> and statistically optimal (up to a logarithmic factor). Comment: 30 page...|$|R
40|$|We {{propose a}} nonparametric method to {{explicitly}} model {{and represent the}} derivatives of smooth underlying trajectories for longitudinal data. This representation {{is based on a}} direct Karhunen [...] Loève expansion of the unobserved derivatives and leads to the notion of derivative <b>principal</b> <b>component</b> analysis, which complements functional <b>principal</b> <b>component</b> analysis, {{one of the most popular}} tools of functional data analysis. The proposed derivative <b>principal</b> <b>component</b> scores can be obtained for irregularly spaced and sparsely observed longitudinal data, as typically encountered in biomedical studies, as well as for functional data which are densely measured. Novel consistency results and asymptotic convergence rates for the proposed estimates of the derivative <b>principal</b> <b>component</b> scores and other components of the model are derived under a unified scheme for sparse or dense observations and mild conditions. We compare the proposed representations for derivatives with alternative approaches in simulation settings and also in a wallaby growth curve application. It emerges that representations using the proposed derivative <b>principal</b> <b>component</b> analysis recover the underlying derivatives more accurately compared to <b>principal</b> <b>component</b> analysis-based approaches especially in settings where the functional data are represented with only a very small number of components or are densely sampled. In a second wheat spectra classification example, derivative <b>principal</b> <b>component</b> scores were found to be more predictive for the protein content of wheat than the conventional functional <b>principal</b> <b>component</b> scores...|$|R
2500|$|... data {{transformation}} (incorporating aspects such as data normalization and data analysis) – for example <b>principal</b> <b>components</b> analysis dimensionality reduction, mean calculation ...|$|E
2500|$|The Quebec Act of 1774 (...) , {{formally}} {{known as}} the British North America (Quebec) Act 1774, {{was an act of}} the Parliament of Great Britain (citation 14 Geo. III c. 83) setting procedures of governance in the Province of Quebec. [...] The Act's <b>principal</b> <b>components</b> were: ...|$|E
2500|$|The Saint-Maurice fortifications are {{disposed}} in mutually supporting groups, {{often with}} pairs of forts that provide mutual support by direct fire {{as well as}} supporting more distant parts of the ensemble by means of indirect fire. From north to south, the <b>principal</b> <b>components</b> are as follows: ...|$|E
30|$|The last {{column in}} Table  5 shows the {{correlation}} of the different factors of the TIA measure with the standardized first <b>principal</b> <b>component.</b> Most factors are strongly correlated with the <b>principal</b> <b>component.</b> The first <b>principal</b> <b>component</b> of teacher behavior in math classes mainly captures call on, praise, and remediation; in other subjects, it mainly captures call on, time spent, and remediation. For {{the two types of}} actions, we plot the distribution for girls and boys separately for math and language arts/other subjects. We report the p value of the test of equality of the <b>principal</b> <b>component</b> across gender.|$|R
40|$|FIGURE 3. Bivariate plot of (A) <b>principal</b> <b>component</b> 1 (eigenvalue = 0. 168) versus <b>principal</b> <b>component</b> 2 (eigenvalue = 0. 012), and (B) <b>principal</b> <b>component</b> 2 versus <b>principal</b> <b>component</b> 3 (eigenvalue = 0. 008) {{for eight}} type {{specimens}} of Cophixalus aimbensis (closed circles), ten other specimens of C. verrucosus from Sudest Island (open circles), ten specimens of C. verrucosus from Mt. Victoria and Moroka, the localities {{from which the}} type series was obtained (diamonds), and 27 other specimens of C. verrucosus from Milne Bay Province in southeastern Papua New Guinea (squares). Component loadings presented in Table 2...|$|R
40|$|To {{overcome}} {{the shortcomings of}} traditional dimensionality reduction algorithms, incremental tensor <b>principal</b> <b>component</b> analysis (ITPCA) based on updated-SVD technique algorithm is proposed in this paper. This paper proves the relationship between PCA, 2 DPCA, MPCA, and the graph embedding framework theoretically and derives the incremental learning procedure to add single sample and multiple samples in detail. The experiments on handwritten digit recognition have demonstrated that ITPCA has achieved better recognition performance than that of vector-based <b>principal</b> <b>component</b> analysis (PCA), incremental <b>principal</b> <b>component</b> analysis (IPCA), and multilinear <b>principal</b> <b>component</b> analysis (MPCA) algorithms. At the same time, ITPCA also has lower time and space complexity...|$|R
