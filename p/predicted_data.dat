731|3637|Public
5000|$|... Can bioinformatically <b>predicted</b> <b>data</b> {{be shared}} through Human Proteinpedia? ...|$|E
5000|$|... which {{represents}} the L-2 norm of the misfit between the observed data and the <b>predicted</b> <b>data</b> from the model. We use the L-2 norm here as a generic measurement of {{the distance between the}} <b>predicted</b> <b>data</b> and the observed data, but other norms are possible for use. The goal of the objective function is to minimize the difference between the predicted and observed data.|$|E
50|$|Adjusted and {{predicted}} fire {{are not mutually}} exclusive, the former may use <b>predicted</b> <b>data</b> and the later may need adjusting in some circumstances.|$|E
30|$|In this section, a {{new method}} {{incorporating}} data prediction {{not restricted to}} only causal pixels is designed for the proposed NIPE technique. This noncausal predictive method can <b>predict</b> <b>data</b> in a single pass.|$|R
50|$|Romonet claims {{they have}} the only cloud-based, big data {{software}} platform that models, simulates, and <b>predicts</b> <b>data</b> center investment and operational performance. They say their analytics platform enables energy, financial, investment, and profitability objectives.|$|R
50|$|Decision tree {{learning}} algorithms can {{be applied}} to learn to predict a dependent variable from data. Although the original CART formulation applied only to <b>predicting</b> univariate <b>data,</b> the framework can be used to <b>predict</b> multivariate <b>data</b> including time series.|$|R
50|$|Defining a metric {{to measure}} {{distances}} between observed and <b>predicted</b> <b>data</b> {{is a useful}} tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.|$|E
50|$|The third {{task is to}} {{formulate}} a function which has the parameters {{that are expected to}} be design variables, and which represents the distance between the measured data and the finite element model <b>predicted</b> <b>data.</b>|$|E
50|$|In 2011 some <b>predicted</b> <b>data</b> center {{management}} domains would converge {{across the}} logical and physical layers. This type of converged management environment will allow enterprises to use fewer resources, eliminate stranded capacity, {{and manage the}} coordinated operations of these otherwise independent components.|$|E
5000|$|Spatial (picture size) scalability: {{video is}} coded at {{multiple}} spatial resolutions. The data and decoded samples of lower resolutions {{can be used}} to <b>predict</b> <b>data</b> or samples of higher resolutions {{in order to reduce the}} bit rate to code the higher resolutions.|$|R
5000|$|SNR/Quality/Fidelity scalability: {{video is}} coded {{at a single}} spatial {{resolution}} but at different qualities. The data and decoded samples of lower qualities {{can be used to}} <b>predict</b> <b>data</b> or samples of higher qualities {{in order to reduce the}} bit rate to code the higher qualities.|$|R
40|$|Singular Value Decomposition can be {{considered}} as an effective method for Signal Processing/especially data compression. In this short paper we investigate the application of SVD to <b>predict</b> <b>data</b> equation from data. The method is similar to nonlinear ARMA method for fitting a nonlinear equation to the data...|$|R
5000|$|In lossy {{predictive}} codecs, previous and/or subsequent decoded data is used {{to predict}} the current sound sample or image frame. The error between the <b>predicted</b> <b>data</b> and the real data, together with any extra information needed to reproduce the prediction, is then quantized and coded.|$|E
50|$|Databases are {{essential}} for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), <b>predicted</b> <b>data</b> (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.|$|E
5000|$|It is {{important}} to examine the [...] "fit" [...] of an estimated model to determine how well it models the data. This is a basic task in SEM modeling: forming the basis for accepting or rejecting models and, more usually, accepting one competing model over another. The output of SEM programs includes matrices of the estimated relationships between variables in the model. Assessment of fit essentially calculates how similar the <b>predicted</b> <b>data</b> are to matrices containing the relationships in the actual data.|$|E
40|$|Fast-and-frugal trees (FFTs) {{are simple}} {{algorithms}} that facilitate efficient and accurate {{decisions based on}} limited information. But despite their successful use in many applied domains, there is no widely available toolbox that allows anyone to easily create, visualize, and evaluate FFTs. We fill this gap by introducing the R package FFTrees. In this paper, we explain how FFTs work, introduce {{a new class of}} algorithms called fan for constructing FFTs, and provide a tutorial for using the FFTrees package. We then conduct a simulation across ten real-world datasets to test how well FFTs created by FFTrees can <b>predict</b> <b>data.</b> Simulation results show that FFTs created by FFTrees can <b>predict</b> <b>data</b> as well as popular classification algorithms such as regression and random forests, while remaining simple enough for anyone to understand and use...|$|R
50|$|Conte {{realized}} in the early 90’s that Flynn’s prediction of the fetch bandwidth being the limit to increasing instruction level parallelism was coming true. His oft-cited International Symposium on Computer Architecture paper and subsequent work on instruction fetch mechanisms have influenced industry and spawned much follow-on research. More recently, Conte and his Ph.D. students invented a technique to <b>predict</b> <b>data</b> values with very high (~90%) accuracy and showed how <b>predicting</b> <b>data</b> values {{can be used to}} scale the memory wall by enabling aggressive prefetching. The work is of great interest to industry design teams who are struggling with performance limitations imposed by the speed gap between microprocessors and memory systems. Conte and his students have also developed a very small yet highly effective prefetcher termed the Spectral Prefetcher. This was published in the ACM Transactions on Computer Systems.|$|R
40|$|Spatial {{interpolation}} {{is performed}} to <b>predict</b> <b>data</b> values of unseen locations {{based on the}} distribution of known samples. In the field of geostatistics, the technique of unbiased linear interpolation, known as kriging, is used to <b>predict</b> <b>data</b> at unsampled locations. When working with large data sets or a large domain of interest, standard kriging methods such as ordinary and universal kriging can become computationally slow or require the domain to be partitioned with different models fit to different partitions. In this paper, we review common kriging methods as well as an extension known as fixed rank kriging that circumvents these problems. We apply the method of fixed rank kriging to a dataset of 2016 California groundwater, evaluating prediction accuracy under various model setups. We also compare fixed rank kriging to ordinary and universal kriging based on prediction accuracy and time taken to build the model and make predictions...|$|R
50|$|The {{first report}} of {{discovery}} of element 105 {{came from the}} Joint Institute for Nuclear Research (JINR) in Dubna, Moscow Oblast, Russian SFSR, Soviet Union, in 1968. A target of 243Am was bombarded by a beam of 22Ne ions. The scientists at Dubna reported 9.4 MeV (with the reported half-life of 0.1-3 seconds) and 9.7 MeV (t1/2 > 0.05 s) alpha activities followed by alpha activities {{similar to those of}} 256103 and 257103. The two activity lines were, based on <b>predicted</b> <b>data,</b> assigned to 261105 and 260105, accordingly.|$|E
5000|$|Because {{we cannot}} {{directly}} invert the observation matrix, we use methods from optimization {{to solve the}} inverse problem. To do so, we define a goal, also known as an objective function, for the inverse problem. The goal is a functional that measures how close the <b>predicted</b> <b>data</b> from the recovered model fits the observed data. In the case where we have perfect data (i.e. no noise) and perfect physical understanding (i.e. we know the physics) then the recovered model should fit the observed data perfectly. The standard objective function, , is usually of the form: ...|$|E
50|$|In many applications, such as {{time series}} analysis, it is {{possible}} to estimate the models that generate the observations. If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and <b>predicted</b> <b>data</b> estimates can be calculated. If the underlying generating models are linear then a minimum-variance Kalman filter and a minimum-variance smoother may be used to recover data of interest from noisy measurements. These techniques rely on one-step-ahead predictors (which minimise the variance of the prediction error). When the generating models are nonlinear then stepwise linearizations may be applied within Extended Kalman Filter and smoother recursions. However, in nonlinear cases, optimum minimum-variance performance guarantees no longer apply.|$|E
40|$|A new {{statistical}} model of stress-induced leakage current (SILC) is implemented {{and used to}} <b>predict</b> <b>data</b> retention and program disturbs of state-of-the-art flash memories, and to correlate oxide characterization outputs (density, cross section, energy level of defects) to flash memory reliability. Physical mechanisms inducing the largest threshold voltage (VT) degradation are explained, and tunnel oxide scaling effects on flash reliability are predicted...|$|R
50|$|Filter type methods select {{variables}} {{regardless of}} the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables {{will be part of}} a classification or a regression model used to classify or to <b>predict</b> <b>data.</b> These methods are particularly effective in computation time and robust to overfitting.|$|R
40|$|During this {{reporting}} period all preliminary tasks were completed (such as {{the creation of}} a flexible project database) and construction of the actual broadband transform function was begun. Analysis of intermediate results performed during the {{reporting period}} has proven that the neural networks being used can accurately <b>predict</b> <b>data</b> elements using surface seismic or crosswell seismic data and attributes as input...|$|R
50|$|The Three-detector {{problem is}} a problem in traffic flow theory. Given is a {{homogeneous}} freeway and the vehicle counts at two detector stations. We seek the vehicle counts at some intermediate location. The method can be applied to incident detection and diagnosis by comparing the observed and <b>predicted</b> <b>data,</b> so a realistic solution to this problem is important. Newell G.F. proposed a simple method to solve this problem. In Newell's method, one gets the cumulative count curve (N-curve) of any intermediate location just by shifting the N-curves of the upstream and downstream detectors. Newell's method was developed before the variational theory of traffic flow was proposed to deal systematically with vehicle counts. This article shows how Newell's method fits in the context of variational theory.|$|E
50|$|Transportation {{forecasting}} is {{the attempt}} of estimating {{the number of}} vehicles or people that will use a specific transportation facility in the future. For instance, a forecast may estimate the number of vehicles on a planned road or bridge, the ridership on a railway line, the number of passengers visiting an airport, {{or the number of}} ships calling on a seaport. Traffic forecasting begins with the collection of data on current traffic. This traffic data is combined with other known data, such as population, employment, trip rates, travel costs, etc., to develop a traffic demand model for the current situation. Feeding it with <b>predicted</b> <b>data</b> for population, employment, etc. results in estimates of future traffic, typically estimated for each segment of the transportation infrastructure in question, e.g., for each roadway segment or railway station.|$|E
50|$|Newer methodologies {{taking into}} account data {{relational}} structure, forecast traffic density in time relying on linked data from multiple spatial positions at different moments in time, event future already <b>predicted</b> <b>data.</b> Studies using data relational structures have mainly used STARIMA models (space-time ARIMA), Kalman filters and Structural Time Series model. The use of a Statistical Relational Learning (SRL) framework is very effective to improve predictive accuracy of relational structured data. Statistical Relational Learning matches very well this field of research by its ability to describe dependencies and relations and include background knowledge in the model, as in a transportation network. Models generated with a Statistical Relational Learning method can represent a wide set of location thanks to its ability to perform concurrent grouping and regressions on its multiple sources and information levels. This {{makes it possible to}} such model to predict traffic conditions out of a network in a single interference process.|$|E
5000|$|It {{can measure}} the {{efficiency}} of the parameterized model in terms of <b>predicting</b> the <b>data.</b>|$|R
40|$|This {{research}} {{concerns about}} application of artificial neural networks (ANN) for <b>predicting</b> time series <b>data.</b> By modifying perceptron’s activation function with linear function, we got linear networks. In order to <b>predict</b> time series <b>data,</b> these linear networks combine with adaptive LMS algorithm. And {{then we had}} completed this model with time delay function to accommodate past data in time series. The data that used in the test had varied in frequency and sampling time. Results of the test had shown that the networks work properly to <b>predict</b> the <b>data</b> series...|$|R
30|$|The {{limitation}} {{with this}} approach would be that i) {{we do not know}} how much of the missing data in the population we do not predict since we do not know the population size, and ii) we cannot <b>predict</b> <b>data</b> for which there is no information to base a model on. This problem, however, also needs to be addressed for wage assessments that do not include the black market segment.|$|R
5000|$|Addendum:Delbrück {{scattering}} is the coherent {{elastic scattering}} of photons in the Coulomb field of heavy nuclei. It {{is one of}} the two nonlinear effects of quantum electrodynamics (QED) in the Coulomb field investigated experimentally. The other is the splitting of a photon into two photons. Delbrück scattering was introduced by Max Delbrück in order to explain discrepancies between experimental and <b>predicted</b> <b>data</b> in a Compton scattering experiment on heavy atoms carried out by Meitner and Köster. Delbrück’s arguments were based on the relativistic quantum mechanics of Dirac according to which the QED vacuum is filled with electrons of negative energy or - in modern terms - with electron-positron pairs. These electrons of negative energy should be capable of producing coherent-elastic photon scattering because the recoil momentum during absorption and emission of the photon is transferred to the total atom while the electrons remain in their state of negative energy. This process is the analog of atomic Rayleigh scattering with the only difference that in the latter case the electrons are bound in the electron cloud of the atom. The experiment of Meitner and Köster was {{the first in a series}} of experiments where the discrepancy between experimental and predicted differential cross sections for elastic scattering by heavy atoms were interpreted in terms of Delbrück scattering. From the present point of view these early results are not trustworthy. Reliable investigations were possible only after modern QED techniques based on Feynman diagrams were available for quantitative predictions, and on the experimental side photon detectors with high energy resolution and high detection efficiency had been developed. This was the case at the beginning of the 1970s when also computers with high computing capacity were in operation which delivered numerical results for Delbrück scattering amplitudes with sufficient precision. A first observation of Delbrück scattering was achieved in a high-energy, small-angle photon scattering experiment carried out at DESY (Germany) in 1973, where only the imaginary part of the scattering amplitude is of importance. Agreement was obtained with predictions of Cheng Wu [...] which later were verified by Milstein and Strakhovenko. These latter authors make use of the quasi-classical approximation being very different from the one of Cheng and Wu. It could however be shown that both approximations are equivalent and lead to the same numerical results.The essential breakthrough came with the Göttingen (Germany) experiment in 1975 carried out at an energy of 2.754 MeV.In the Göttingen experiment Delbrück scattering was observed as the dominant contribution to the coherent-elastic scattering process, in addition to minor contributions stemming from atomic Rayleigh scattering and nuclear Rayleigh scattering. This experiment was the first where exact predictions based on Feynman diagrams, were confirmed with high precision and, therefore, has to be considered as the first definite observation of Delbrück scattering. For a comprehensive description of the present status of Delbrück scattering see.Nowadays, the most accurate measurements of high-energy Delbrück scattering are performed at the Budker Institute of Nuclear Physics (BINP) in Novosibirsk (Russia). The experiment where photon splitting was really observed for the first time was also performed at the BINP.|$|E
3000|$|Table 6 {{contains}} {{information about}} actual and <b>predicted</b> <b>data</b> distribution by a clustering algorithm. Each column of the Table (C [...]...|$|E
3000|$|... where g is {{a column}} vector of the <b>predicted</b> <b>data</b> {{calculated}} along the profile, using formula (1), from some approximative solution m, g=[g [...]...|$|E
40|$|MapReduce {{one of the}} {{distributed}} computing techniques is integrated with decision tree classifier C 4. 5 in the distributed environment and ensemble learning with its classifier. This paper proposes an algorithm to classify, <b>predict</b> <b>data</b> using MapReduce for DC 4. 5 with ensemble learning. Proposed algorithm increases the accuracy and scalability of the data. Noise handling in the decision trees {{with respect to the}} distributed data is also handled here...|$|R
40|$|The {{problem of}} torsion of a {{circular}} cylinder is solved {{with the aid}} of a new strain energy density function. The results obtained are used to <b>predict</b> <b>data</b> of Rivlin and Saunders on natural rubber in simple torsion and torsion with extension. It is shown that the new s train energy function correctly describes the torsional couple and the normal load data from parameters obtained in simple tension...|$|R
30|$|The {{purpose of}} this {{research}} is to derive the formula to <b>predict</b> royalty-related <b>data,</b> such as running royalty rate (back-end payments) and up-front payment (up-front fee[*]+[*]milestones), using the attrition rate for the corresponding development phase of the drug candidate within a specific drug class, such as anticancer (antineoplastics) or cardiovascular, and the revenue data of the license buyer (licensee) using regression analysis. Another purpose is to find the relationship between the formula to <b>predict</b> royalty-related <b>data</b> and eNPV.|$|R
