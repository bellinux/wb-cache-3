5|20|Public
30|$|Pre-paging: It {{requests}} to the destination server for future access page, {{that helps to}} avoid or mitigate page fault rate. For this, it uses the hint of page access pattern on destination VM. So we can avoid the future page faults in advance and accept the better page <b>pushing</b> <b>sequence</b> to access the patterns.|$|E
40|$|Copyright © 2014 T. D. Bhatt and S. P. Singh. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Abstract This paper presents {{the use of the}} Linear Congruence Modified <b>Pushing</b> <b>Sequence</b> (LC-MPS) waveforms application as target tracking, for Ground Control Intercept (GCI) based Fire Control Radar (FCR). The distinct features of LC-MPS are combined with Linear Frequency Modulation (LFM), and a new approach has been developed for the radar waveform design on an adaptive basis, that is suitable for fire control radar systems and Track before Detect (TBD) applications...|$|E
40|$|Eighteen {{patients}} {{suffering from}} Parkinson's disease and nineteen control subjects, who were matched for age and intelligence, were compared in tests measuring "shifting aptitude" at cognitive and motor levels (word production, sorting blocks or animals, and finger pushing sequences). It {{was found that}} Parkinson patients produced fewer different names of animals and professions in one minute than control subjects, needed more trials for detecting a shift in a sorting criterion, and produced fewer finger responses in a change of <b>pushing</b> <b>sequence</b> than control subjects. These results are interpreted as reflecting a central programming deficit that manifests itself in verbal, figural and motor modalities, that is, a diminished "shifting aptitude" characteristic of patients with dysfunctioning basal ganglia. The results are discussed in relation to changes of behaviour organisations in animals with dysfunctioning basal ganglia...|$|E
30|$|Most {{of these}} {{sequence}} alignment tools were very efficient and accurate until {{the introduction of}} Next-Generation Sequencing (NGS). NGS {{has led to a}} huge amount of sequencing data which has rendered many existing sequence alignment tools obsolete. For example, NGS technologies like Illumina HiSeqX can easily generate nearly 6 billion sequence reads per run. After sequencing, mapping these read sequences onto a reference genome is the most important task in a sequence analysis work-flow. Alignment of such large volumes of read data onto a reference genome is a very time consuming task. Many state-of-the-art sequence aligners are developed to handle this huge amount of data efficiently but NGS platforms are evolving so rapidly that they <b>push</b> <b>sequencing</b> capacity to unprecedented levels. Therefore, sequence alignment still remains a bottleneck for bioinformaticians.|$|R
40|$|We {{investigate}} {{manipulation and}} active sensingby a pushing control system using only tactile feedback. The {{equations of motion}} of a pushed object are derived using {{a model of the}} object's limit surface, and we design a control system to translate and orient objects. The effectiveness of the proposed controller is confirmed through simulation and experiments. Active sensing of the object's center of mass is described. I. INTRODUCTION 1992 International Conference on Intelligent Robots and Systems Pushing is a useful robotic capability for positioning and orienting parts. Several researchers have demonstrated the utility of pushing operations by planning open-loop <b>pushing</b> <b>sequences</b> to position and orient polygonal objects despite the presence of uncertainty in the initial state [1, 2, 3, 4, 5, 6, 7]. These operations typically plan for a known object shape and center of mass (CM) and a flat pushing fence or specially designed pusher geometry to exploit the mechanics of pushing. Others ha [...] ...|$|R
25|$|By 2004 / 2005, {{pyrosequencing}} {{had been}} brought to commercial viability by 454 Life Sciences. This new sequencing method generated reads much shorter than those of Sanger sequencing: initially about 100 bases, now 400-500 bases. Its much higher throughput and lower cost (compared to Sanger <b>sequencing)</b> <b>pushed</b> the adoption of this technology by genome centers, which in turn <b>pushed</b> development of <b>sequence</b> assemblers that could efficiently handle the read sets. The sheer amount of data coupled with technology-specific error patterns in the reads delayed development of assemblers; at the beginning in 2004 only the Newbler assembler from 454 was available. Released in mid-2007, the hybrid version of the MIRA assembler by Chevreux et al. was the first freely available assembler that could assemble 454 reads as well as mixtures of 454 reads and Sanger reads. Assembling sequences from different sequencing technologies was subsequently coined hybrid assembly.|$|R
40|$|Each year {{million of}} {{babies are born}} pre-term, some of these pre-term births occur due to the motherhaving a too soft cervix which can not {{withstand}} the forces the baby exposes it to. The aim of thisstudy was to implement and evaluate a programmable shear wave elastography ultrasound system forcervical applications and investigate the optimal settings of shear wave elastography push voltage andshear wave elastography push focus depth. Shear wave elastography is an ultrasound based imagingmodality aiming to evaluate the tissue elasticity by using acoustic radiation forces to induce shear waves. The propagation of the shear waves through the tissue is then tracked in order to calculate the shearwave velocity which {{is related to the}} tissue elasticity. B-mode imaging, <b>pushing</b> <b>sequence</b> and planewave imaging have been implemented and measurements have been conducted on four cervical polyvinylalcohol phantoms. The acquired data has been post-processed using Loupas 2 D-autocorrector to gainthe axial displacement and enabling tracking of the shear waves to allow evaluation and optimizationof the implemented method. The implemented shear wave technique showed to be able to distinguishcervical phantoms of dierent elasticity and a high pushing voltage and shallow focus push depth havebeen found to produce the most reliable results...|$|E
40|$|This thesis {{consists}} of three topics related to the improvement of delay-Doppler resolution in radar measurement systems. ^ In the first topic, we propose techniques {{for the construction of}} frequency coding sequences that give rise to frequency coded waveforms having ambiguity functions with a clear area—containing no sidelobes—in a connected region surrounding the main lobe. These constructed sequences are called pushing sequences. Next, two important properties of pushing sequences are investigated: the group D 4 dihedral symmetry property and the frequency omission property. Using the group D 4 dihedral symmetry property, we show how to construct additional pushing sequences from a given <b>pushing</b> <b>sequence.</b> Using the frequency omission property, we show how to construct pushing sequences of any length N and design proper frequency coded waveforms that meet specific constraints in the frequency domain. We also note that Costas sequences also has the former property while extended sonar sequences have both properties and the implication of this are investigated. Finally, we show how to construct pushing sequences with any desired power using the Lempel T 4 construction and Lee codewords. Because these arbitrary-power pushing sequences constructed using Lee codewords do not have the Costas property, we derive expressions for the pattern of hits in the geometric array. Based on this, the general form of the positions and levels of all the sidelobe peaks are derived. ^ In the second topic, we propose the frequency division multiplexing technique, the clean algorithm and time frequency division multiplexing technique to achieve a delay-Doppler response that approximates the composite ambiguity function. First, the channel estimate based on maximum likelihood estimation (MLE) for each subband of frequency division multiplexing system is provided. Also, the signal to noise power ratio (SNR) needed to achieve the specific variance requirement of the MLE is derived. Next, using the Doppler transformation, we show how to combine the output response of each subband suffering a different Doppler shift effect induced by each different carrier frequency. Then the composite ambiguity function approximation of each target is derived based on the clean algorithm. Finally, the composite ambiguity function approximation of all the targets is obtained and provides a way to resolve initially undetected targets using the recursive procedure. ^ In the third topic, we propose an approach to detect masked targets. First, in terms of computation, the proposed approach implements the CLEAN process efficiently because only a few selective samples are used, instead of all the samples of matched filter response. In some situations that the signal strength and phase need to be estimated, we propose a slightly modified data model so that these parameters and image residues can be estimated simultaneously. Hence the computation complexity is further reduced. Secondly, we provide a hypothesis-test based detector and a quantitative way to select detection samples such that the detection performance is optimized. The selection of residue samples and transmitted waveforms are also investigated. ...|$|E
50|$|By <b>pushing</b> {{a certain}} <b>sequence</b> of {{buttons on the}} Super NES version, a screen can be {{accessed}} where the variables for offense (pitchers/runners) and defense (fielding) can be edited. Another test mode can be found by virtue of using either Game Genie or Pro Action Replay on the Super NES version; which acts more like a traditional cheat menu then the debug menu.|$|R
40|$|Have been {{designed}} a Single Channel Analyzer (SCA) for the spectrometer of the nuclear. This Single Channel Analyzer (SCA) function as Pulse High Analyzer (PHA) with maximum range of high pulse {{that can be}} analyze is 10 Volt. Single Channel Analyzer (SCA) which have been made consisted of four part that is Discriminator sequence, Anti Coinsidence, Pulse width shape and <b>Push</b> pull. Discriminator <b>sequence</b> consisted of the Upper Level and Lower Level. This LL and UL function to choosen high pulse which enter to the sequence, pulse passing LL and or UL will be got away to next sequence. Anti Coinsidence sequence process pulse output from UL and LL, this sequence only overcoming pulse passing LL but still undersize to pass the UL and hold up the incoming pulse from LL and UL concurrently. Pulse width shape sequence function to determine pulse width. While <b>Push</b> pull <b>sequence</b> result {{the positive and negative}} pulse. Result of research by using generator pulse as pulse source, indicate that Single Channel Analyzer (SCA) can work for discriminate of pulse. From research result SCA in the spectrometer of the nuclear with radiation source of Co 60 obtained energy (1162. 12 ± 29. 422) keV and (1309. 28 ± 36. 778) keV...|$|R
500|$|After {{learning}} to program BASIC on a Tandy 1000 and becoming interested in computer and video games, {{he applied for}} {{a job as a}} game tester at Westwood studios. He submitted his demo tape—described as [...] "an acoustic guitar song with electric guitar leads and keyboard strings, and raining sound effects"—to the company's audio director. The growing company enlisted him as a composer for the NES port of DragonStrike and the computer game Eye of the Beholder II. He later composed with MIDI sequencing for several other Dungeons & Dragons games. In 1992, he helmed the audio of Dune II, attempting to complement the music of the original Dune. He later noted that he <b>pushed</b> the <b>sequencing</b> program on his Amiga to the limit while scoring the game. While working on Disney's The Lion King in 1994, he and the Westwood team were shown sketches of the unfinished feature film. Film composer Hans Zimmer later praised Klepacki for reworking his scores. After finishing the third entry into The Legend of Kyrandia series; Malcolm's Revenge, Frank Klepacki met with Westwood leaders to discuss the upcoming game Command & Conquer—the first in a series which would bring him wider fame and critical acclaim.|$|R
40|$|This work {{addresses}} the instability in asynchronous data parallel optimization. It does so by introducing a novel distributed optimizer which {{is able to}} efficiently optimize a centralized model under communication constraints. The optimizer achieves this by <b>pushing</b> a normalized <b>sequence</b> of first-order gradients to a parameter server. This implies that the magnitude of a worker delta is smaller compared to an accumulated gradient, and provides a better direction towards a minimum compared to first-order gradients, which in turn also forces possible implicit momentum fluctuations to be more aligned since we make the assumption that all workers contribute towards a single minima. As a result, our approach mitigates the parameter staleness problem more effectively since staleness in asynchrony induces (implicit) momentum, and achieves a better convergence rate compared to other optimizers such as asynchronous EASGD and DynSGD, which we show empirically. Comment: 16 pages, 12 figures, ACML 201...|$|R
40|$|At present, {{the risk}} of a woman {{developing}} invasive breast cancer during her life is about 1 in 8. This makes breast cancer the most prevalent type of cancer in women worldwide. As {{the risk of}} dying from breast cancer for a woman is about 1 in 36, early breast cancer detection and effective treatment are paramount in decreasing this risk. From preclinical studies and clinical studies in large tumors it is known that phospholipid- and energy metabolism are altered in cancer as compared to healthy fibro­glandular tissue. Phosphorus magnetic reson­ance spectroscopy (31 P MRS) offers the possi­bility to measure a number of key metabolites involved in these processes in vivo. Unfortunately, the 31 P MRS method is not very sensitive and requires that the magnetic field strength, the coil efficiency, the B 1 -field, and the pulse <b>sequences</b> used, are <b>pushed</b> to the limit, all to maximize the signal to noise ratio. The main theme of this thesis is <b>pushing</b> pulse <b>sequences</b> to the limit, to obtain the best signal to noise ratio possible in detecting phosphorus metabolites. To this end, direct detection, polarization transfer, multi-echo acquisitions and combinations of these methods are utilized. The work described in this thesis can possibly contribute to increasing the specificity of breast cancer detection and monitoring the efficacy of neoadjuvant therapy. A feasibility study of using 31 P MRS in monitoring tumor metabolism during neoadjuvant chemotherapy forms part of this thesis...|$|R
40|$|Voyne Ray Cox {{settled into}} the {{radiation}} machine for the eighth routine treatment of his largely cured cancer. The operator went to the control room and pushed some buttons. Soon, the machine went into action and the treatment began. A soft whir and then an intense searing pain made him yell for help and jump from the machine. The doctors assured him {{there was nothing to}} worry about. What they didn't know was that the operator had inadvertently <b>pushed</b> an unusual <b>sequence</b> of controls that activated a defective part of the software controlling the machine. He didn't die for six months but he had received a lethal dose of radiation. This software defect actually killed two patients and severely injured several others. The final decisions in the resulting lawsuits have not been made public. Software defects are rarely lethal and the number of injuries and deaths is now very small. Software, however, is now the principal controlling element in many industrial and consumer products. It is [...] ...|$|R
40|$|Project (M. S., Civil Engineering) [...] California State University, Sacramento, 2012. This report, 3 -D Finite Element Modeling of Reinforced Concrete Beam-Column Connections ??? Development and Comparison to NCHRP 12 - 74, {{investigates the}} use of finite element {{modeling}} (FEM) to predict the structural response of the cast-in-place (CIP) reinforced concrete bent cap-column test specimen reported in NCHRP Report 681 ??? Development of a Precast Bent Cap System for Seismic Regions. 	Analysis was performed using LS-DYNA as the finite element processor. The Karagozian & Case Damaged Concrete model, material MAT_ 072, {{was used as the}} constitutive model for all concrete elements and material MAT_ 003, a plastic kinematic model, was used as the constitutive model for the reinforcing steel. Strain-hardening effects of steel were neglected for this analysis. Boundary conditions on the FE model were identical to the vertical and horizontal restraints used on the CIP specimen during testing. The FE model only considered a monotonic <b>push</b> loading <b>sequence,</b> whereas the CIP specimen was subjected to reverse cyclic loading. To account for the difference in loading, the FE model results were compared to the hysteretic envelope from the CIP specimen. 	The lateral load-lateral displacement response of the FE model (Model 1) compared reasonably well to the actual and theoretically predicted response of the CIP specimen. For lateral displacements less than that corresponding to a displacement ductility of 4. 1, the FE model showed a larger stiffness than the actual CIP response. The model stiffness degraded as a greater number of concrete elements in the column plastic hinging region accumulated damage. The degradation and lateral load-displacement response matched the predicted response within 5...|$|R
40|$|DelaylDisruption {{tolerant}} networks (DTNs) are wireless networks where {{a complete}} path from source to destination {{is not in}} existence most of the time, and even when it does exist, it is highly unstable and unpredicted. This together with limited computing and storage capacity, heterogeneity and high error rate amongst others violate most of the internet assumptions. This necessitated the design of DTN architecture to relax some of the Internet assumptions and provide interoperabilijy across heterogeneous networks with different network characteristics. The identified security threats in these networks this work is designed to address are masquerading, modification, replay and unauthorized access/use of resources. This work proposes a novel access control scheme {{that is based on}} both secret-key and public-key cryptography. The scheme is designed to be independent of server availability and recipient network connectivity during post trust establishment communication. The main contributions in this thesis are: Propose and implement a lightweight asymmetric based Authorization Pass (APass) as an alternative to digital certificate; Design and implement a PKI-based trust management scheme that facilitates secure exchange of public keys without binding it to credential, access control implementation and flexible trust termination process; Propose and implement a trust based authentication scheme that employs Hash-based Message Authentication Code (HMAC) for message authentication and integrity, and APass for source authentication; Investigate and establish the applicability of the <b>push</b> messaging <b>sequence</b> of the generic AAA (Authentication, Authorization and Accounting) architecture with modification, and extend the proposed authentication scheme to implement policy; Propose and implement generic AAA architecture concepts based access control decision making process using DTN Bundle Node. The proposed solutions are extensively discussed with their efficiency and effectiveness as well as comparative advantage demonstrated through simulations. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
25|$|Most {{of these}} centers {{developed}} to their apogees in the Preclassic period before declining or disappearing. In {{addition to these}} large sites, many Early Preclassic communities, found mostly along the Pacific Coast, {{bear witness to the}} seminal character of the Southern area; notably these include La Victoria, a site studied by Michael Coe that yielded the first secure ceramic sequence from early on in Preclassic times. Since Coe’s work, John E. Clark and other scholars from the New World Archaeological Foundation have found, at Paso de la Amada and other sites, ceramics that refine and deepen in time Coe’s <b>sequence,</b> <b>pushing</b> back to ca. 2000 BC the earliest nuclear centers, fine pottery, figurines, and other manifestations of the beginnings of complex society and culture in Mesoamerica. The earliest pristine ballcourt and evidence of a ranked society (a rich child's burial), indicative of emerging social hierarchization, were found at Paso de la Amada and nearby. At La Blanca, archaeologists discovered a quatrefoil made of baked clay buried near Mound 1, {{one of the largest and}} earliest temple mounds in Mesoamerica, indicating an early fount of what later became core Maya ideology.|$|R
40|$|The {{purpose of}} this {{overview}} is to outline the metering points that will be monitored by the Energy Systems Laboratory (ESL). At three of the campuses, Alamo Community College District (ACCD) has a central energy management system (EMS) installed that has the capability to record energy data. It is ESL's intent to utilize this system to collect data for energy monitoring and commissioning purposes. Since ACCD does not monitor gas consumption, ESL would like to incorporate gas metering into the current EMS. ESL may recommend upgrading {{some or all of}} the existing Energy Management System's software and client PC workstations (these hardware/software upgrades are not included in this budget), hi order for ESL to link into the existing EMS, it will be necessary to install additional software (PCAnywhere) or equipment (mainly another PC workstation) near the existing EMS computer terminals. ESL would also like Ethernet lines connected to the PC workstation so the system could be accessed via Internet. Software development will be needed to extract and format the existing EMS data <b>sequence,</b> <b>push</b> the data to ESL's network and load it into ESL's databases...|$|R
50|$|Most {{of these}} centers {{developed}} to their apogees in the Preclassic period before declining or disappearing. In {{addition to these}} large sites, many Early Preclassic communities, found mostly along the Pacific Coast, {{bear witness to the}} seminal character of the Southern area; notably these include La Victoria, a site studied by Michael Coe that yielded the first secure ceramic sequence from early on in Preclassic times. Since Coe’s work, John E. Clark and other scholars from the New World Archaeological Foundation have found, at Paso de la Amada and other sites, ceramics that refine and deepen in time Coe’s <b>sequence,</b> <b>pushing</b> back to ca. 2000 BC the earliest nuclear centers, fine pottery, figurines, and other manifestations of the beginnings of complex society and culture in Mesoamerica. The earliest pristine ballcourt and evidence of a ranked society (a rich child's burial), indicative of emerging social hierarchization, were found at Paso de la Amada and nearby. At La Blanca, archaeologists discovered a quatrefoil made of baked clay buried near Mound 1, {{one of the largest and}} earliest temple mounds in Mesoamerica, indicating an early fount of what later became core Maya ideology.|$|R
40|$|Proceedings of: Second International Workshop on Sustainable Ultrascale Computing Systems (NESUS 2015). Krakow (Poland), September 10 - 11, 2015. Ultrascale {{computing}} and bioinformatics are two {{rapidly growing}} fields {{with a big}} impact right now {{and even more so}} in the future. The introduction of next generation <b>sequencing</b> <b>pushes</b> current bioinformatics tools and workflows to their limits in terms of performance. This forces the tools to become increasingly performant {{to keep up with the}} growing speed at which sequencing data is created. Ultrascale computing can greatly benefit bioinformatics in the challenges it faces today, especially in terms of scalability, data management and reliability. But before this is possible, the algorithms and software used in the field of bioinformatics need to be prepared to be used in a heterogeneous distributed environment. For this paper we choose to look at sequence alignment, which has been an active topic of research to speed up next generation sequence analysis, as it is ideally suited for parallel processing. We present a multilevel stream based parallel architecture to transparently distribute sequence alignment over multiple cores of the same machine, multiple machines and cloud resources. The same concepts are used to achieve multithreaded and distributed parallelism, making the architecture simple to extend and adapt to new situations. A prototype of the architecture has been implemented using an existing commercial sequence aligner. We demonstrate the flexibility of the implementation by running it on different configurations, combining local and cloud computing resources...|$|R
40|$|We {{treat the}} problem of {{particle}} pushing by growing ice as a free diffusion near a wall that moves with discrete steps. When the particle diffuse away from the surface the surface can grow, blocking the particle from going back. Elementary calculations of the model reproduce established results for the critical velocity v_c for particle engulfment: v_c∼ 1 /r for large particles and v_c∼ Const for small particles, r being the particle's radius. Using our model we calculate the dragging distance of the particle by treating the <b>pushing</b> as a <b>sequence</b> of growing steps by the surface, each enabled by the particle's diffusion away. Eventually the particle is engulfed by ice growing around it when a rare event of long diffusion {{time away from the}} surface occurs. By calculating numerically the statistics of the diffusion times from the surface and therefore the probability for a such a rare event we calculate the total dragging time and distance L of the particle by the ice front to be L∼[1 /(vr) ] where v is the freezing velocity. This relation for L is confirmed by ours and others experiments. The distance L provides a length scale for pattern formation during phase transition in colloidal suspensions, such as ice lenses and lamellae structures by freeze casting. Data from the literature for ice lenses thickness and lamellae spacing during freeze casting agree with our prediction for the relation of the distance L. These results lead us to conjecture that lamellae formation is dominated by their lateral growth which pushes and concentrates the particles between them...|$|R
40|$|Visiting places one {{has never}} seen before, which in my case Cairo at that time, renders the {{experience}} of it into {{a limited number of}} isolated moments that are <b>pushed</b> into a <b>sequence</b> of narration. A city experience is reduced to a number of words, images or stills that together create a mental context that collapses reality into a collage of singular biased perspectives. This episodic view of space is transformed and reshaped, reviewing the existing spatial field. A single view subsequently creates several new narratives. Every new perspective retraces previous steps. Within the limitations of personal perspective, space is sensorially presented from singular points at certain moments in time. From the collection of views, smells, sounds etc. we remember certain fragments and ascribe them with a personal value within a spatial context. Portrayed in memory these shreds of space leaves one switching back and forth between perceiving and projecting space. A Coptic Christian minority in Moqattam (garbage city) are Cairo’s informal waste collectors. Because there is no religious objection, they keep pigs that in turn are key in waste processing. Although this social and economic system is counteracted by the city to sanitise the area, these waste streams comprise a high potential. Apart from their financial value they have spatial implications on both an architectural and urban scale, considering the process from dumping to reselling. Similar to a bottle of wine recalling a specific event, the personal value of waste deteriorates when it’s discarded. Being part of a waste stream it ‘decomposes’ into smaller generic pieces, leaving only traces of memory. Spatially analogous to the waste process, these traces elaborate on past events to the point where it’s original shape has become unrecognizable. Retracing their origin allows for personal and speculative interpretations. This design for a waste processing plant in Cairo offers space for the Zabballeen to maintain the current waste system, also taking benefit from it on spatial level. While currently waste is stored within the domestic area, this plant allows families to dump, process, store and sell it again elsewhere. Border conditionsPublic BuildingArchitectur...|$|R
40|$|International audienceMany {{firms have}} {{implemented}} a customer-value based segmentation {{to improve the}} efficiency of MARCOM campaigns {{as part of their}} long term customer relationship strategies (Kumar 2010, Thomas et al. 2005). If distribution channel addition may increase the intrinsic customer value (Kumar 2005, Rangaswamy 2005) in a US context as well as in a French context (Vanheems 2009), few studies have been conducted about the impact of adding a new communication channel during the same buying journey. As the number of devices enabling to communicate with consumers increases, the goal of this research is to understand whether combining web and mobile communication channels for a same journey is more efficient than using a single web channel, monitoring the promotional pressure. Due to consumer reactance at higher levels of communication, Godfrey et al. (2011) have shown that channel interaction effects could lead to diminishing returns. The issue {{of this paper is to}} evaluate if the same result can be observed in case of omni-channel strategy involving a mobile channel (Verhoef et al. 2015). More precisely the major contribution of our paper is to extend current knowledge by measuring the impact of combining a web channel (email) with a mobile one (SMS). The measure will be made on a large population of 304, 410 individuals from a French click & mortar retailer’s customer database and with an unprecedented richness of data thanks to individual variables regarding customer purchases, browsing behavior, customer historical reaction to brand communications and customer distance to closest store. Different questions can be raised:(1) Can combining web and mobile channels within a communication strategy result in an increase in customer count and spend both offline and online?(2) How can we explain this impact in terms of segment specificity such as: i) FRAT segmentation, ii) Browsing data: mobile versus desktop readers, iii) Customer historical reaction to brand emails, iv) Customer distance to closest store?The aim of this research is then:(1) to evaluate, at equal effective pressure, the impact of adding a mobile communication channel on customer buying behavior. (2) to find out whether given customer segments reveal higher value thanks to multichannel targeting. (3) to explore and advance factors explaining the differential impact of multichannel versus single channel communication on purchase behavior. To comply with our focus on customer value unlocking, we measure increments provided by multichannel targeting compared to single channel targeting. As recommended by Dinner, van Heerde, Neslin (2014), the four following outcomes of interest have been retained: i) Customer count offline, ii) Customer spend offline, iii) Customer count online and iv) Customer spend onlineA field experiment has been carried out to measure and understand the differential impact of <b>push</b> multichannel communication <b>sequences.</b> Five populations, split randomly, have been exposed to a specific channel strategy. The first one was assimilated to a control group as exposed to single channel targeting by email only. Four multichannel test groups have been implemented; three of them were targeted with three messages but with different sequences mixing email and SMS. The fourth group was intentionally slightly over-pressured with four messages. The analysis methodology consists in calculating, for each population these four dependent variables and measure a “multichannel lift”. This lift analysis will be done with the customer-level data exposed above. Very first results advance significant insights. No impact has been observed on brands’ heaviest customers, which is counterintuitive in a FRAT-based targeting framework. Combining mobile and web channels results in a 40 % lift in customer count on non-reactive to email customers, highlighting that inactive customers on one channel can still be active on a second channel. Finally, desktop email readers are more likely to purchase when targeted via mobile than mobile readers are. This research enables a new understanding for academia in the field of consumer response to multimedia communications as well as guidelines for practitioners about differentiated segments sensitivity, to refine their multichannel targeting strategies. Due to space limitations, underlying explanations are not presented here but theoretical propositions will be discussed in a full paper...|$|R

