20|283|Public
50|$|Kinew {{has been}} a {{reporter}} and host for the CBC's radio and television operations, including the weekly arts magazine show The 204 in Winnipeg and the national documentary series 8th Fire in 2012. He is also a host of the documentary <b>program</b> <b>Fault</b> Lines on Al Jazeera America.|$|E
40|$|AbstractThe paper {{describes}} {{a study that}} explored the relationship of program slicing to (1) code understanding gained while debugging, and to (2) a debugger's ability to localize the <b>program</b> <b>fault</b> area. The study included two experiments. The first experiment compared the program understanding abilities of two classes of debuggers: those who slice while debugging {{and those who do}} not. For debugging purposes, a slice {{can be thought of as}} a minimal subprogram of the original code that contains the program faults. Those who only examine statements within a slice for correctness are considered slicers; all others are considered non-slicers. Using accuracy of subprogram construction as a measure of understanding, it was determined that slicers have a better understanding of the code after debugging. The second experiment compared debugger fault localization abilities before and after a training session on how to use slicing in debugging. Using time as a measure of ability, it was shown that slicing while debugging improves a debugger's ability to localize the <b>program</b> <b>fault</b> area...|$|E
40|$|AbstractIn {{their most}} general form, program {{specifications}} {{can be represented}} as binary relations. The study of binary relations {{for the purpose of}} discussing program construction, <b>program</b> <b>fault</b> tolerance and program exception handling have led us to discover an interesting property of relations: regularity. The interest of this property is twofold: first it is very general, i. e. it is verified by several specifications we encounter; second, it is very strong, i. e. it allows us to simplify our formal computations rather dramatically...|$|E
40|$|This paper {{discusses}} {{sensitivity analysis}} {{and its relationship}} to random black box testing. Sensitivity analysis estimates the impact that a <b>programming</b> <b>fault</b> at a particular location would have on the program's input/output behavior. Locations that are relatively "insensitive" to faults can render random black box testing unlikely to uncover <b>programming</b> <b>faults.</b> Therefore, sensitivity analysis gives new insight when interpreting random black box testing results. Although sensitivity analysis is computationally intensive, it requires no oracle and no human intervention...|$|R
50|$|Tasks {{can also}} {{terminate}} due to <b>program</b> <b>faults,</b> marked as F-DS or P-DS, for faults such as invalid index, numeric overflow, etc. Completed entries can be listed {{by the operator}} with the 'C' command.|$|R
50|$|In public declarations, glitch is used {{to suggest}} a minor fault which will soon be {{rectified}} and is therefore used as a euphemism for a bug, which is a factual statement that a <b>programming</b> <b>fault</b> {{is to blame for}} a system failure.|$|R
40|$|Abstract. In this demo {{paper we}} present the {{prototype}} of our fault injection testbed generator. Our tool empowers engineers to generate emulated SOA environments and to <b>program</b> <b>fault</b> injection behavior on diverse levels: at the network layer, at the execution level, and at the message layer. Engineers can specify the structure of testbeds via scripts, attach fault models, and generate running testbed instances from that. As a result, our tool facilitates the setup of customizable testbeds for evaluating fault-handling mechanisms of SOA-based systems. ...|$|E
40|$|Program slicing is {{a program}} {{analysis}} and understanding of technology. Sequence fault localization refers {{to the use of}} specific methods for faults in the program. Currently, the research <b>program</b> <b>fault</b> positioning is more and more people's attention and gets some results which is the more mainstream software fault localization method. Program slicing technique currently used to locate the fault procedures, which primarily to take advantage of dynamic slicing technique. Based on the full analysis of the advantages and disadvantages on the basis of previous work, we propose a flexible slicing rule and give a new method based on the slicing rule...|$|E
40|$|Abstract In non-strict {{functional}} programming languages such as Haskell, it happens often that {{some parts of}} a program are not evaluated because their values are not demanded. In practice, those unevaluated parts are often replaced by a placeholder (e. g. _) {{in order to keep}} the trace size smaller. In the process of algorithmic debugging, one needs to answer several questions in order to locate a <b>program</b> <b>fault.</b> Replacing unevaluated parts makes these questions shorter and semantically clearer. In this paper, we present a formal model of tracing in which unevaluated parts are replaced by the symbol _. The most important property, the correctness of algorithmic debugging, is proved. ...|$|E
50|$|As {{complexity}} and potential <b>programming</b> <b>faults</b> of the set-top box increase, software such as MythTV, Select-TV and Microsoft's Media Center have developed features {{comparable to those}} of set-top boxes, ranging from basic DVR-like functionality to DVD copying, home automation, and housewide music or video playback.|$|R
50|$|Weak typing {{allows a}} value of one type {{to be treated as}} another, for example {{treating}} a string as a number. This can occasionally be useful, but it can also allow some kinds of <b>program</b> <b>faults</b> to go undetected at compile time and even at run time.|$|R
40|$|We {{investigate}} {{the implementation of}} standard algorithms for three classes of Distributed Agreement problems in Erlang, an industry-strength language for <b>programming</b> <b>fault</b> tolerant distributed systems. We develop a framework {{to bridge the gap}} between the assumptions of these standard algorithm and the network abstraction provided by Erlang, and structure our implementations as reusable behaviours within this framework. peer-reviewe...|$|R
40|$|A {{prototype}} {{expert system}} {{was developed for}} maintaining autonomous operation of a Mars oxygen production system. Normal operation conditions and failure modes according to certain desired criteria are tested and identified. Several schemes for failure detection and isolation using forward chaining, backward chaining, knowledge-based and rule-based are devised to perform several housekeeping functions. These functions include self-health checkout, an emergency shut down <b>program,</b> <b>fault</b> detection and conventional control activities. An {{effort was made to}} derive the dynamic model of the system using Bond-Graph technique in order to develop the model-based failure detection and isolation scheme by estimation method. Finally, computer simulations and experimental results demonstrated the feasibility of the expert system and a preliminary reliability analysis for the oxygen production system is also provided...|$|E
40|$|Problem: To locate faults that {{reside in}} branch statements. Approach: To perform fault {{localization}} using program state history. A {{common type of}} <b>program</b> <b>fault</b> is related to branch statements, which {{is referred to as}} branch fault. In an object-oriented (OO) programming paradigm, branch statements are typically used to implement state-dependent behavior of classes or clusters of classes. Thus, one effective way to locate the branch fault in an OO program is to identify a condition on the object or object cluster state that activates the fault. In this paper, we briefly present our recent work regarding this basic idea. The presented approach combines static and dynamic analysis. It first generates a set of suspicious candidates by extracting relevant predicates from the source code, and then filters out irrelevant candidates by analyzing program state history which is recorded incrementally in the executions of successl and failure-inducing test cases...|$|E
40|$|Automatic program repair {{has been}} a longstanding goal in {{software}} engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a <b>program</b> <b>fault</b> is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs totaling 63, 000 lines in under 200 seconds, on average. ...|$|E
50|$|In the 2009 election, a <b>programming</b> <b>fault</b> in the {{software}} calculating the allocation prognosis for one county made their leveling seat go to another party. That changed the outcome in two other counties, and it took over {{a week and a}} recount until the distribution of leveling seats was finally decided. Mette Hanekamhaug got the final seat.|$|R
40|$|An unshared object can be {{accessed}} without regard to possible conflicts with other parts of a system, whether concurrent or single-threaded. A unique variable (sometimes known as a "free" or "linear" variable) is one that either is null or else refers to an unshared object. Being able to declare and check which variables are unique improves a programmer 's ability to avoid <b>program</b> <b>faults...</b>|$|R
50|$|The ALGOL {{used on the}} B5000 is an {{extended}} ALGOL subset. It includes powerful string manipulation instructions but excludes certain ALGOL constructs, notably unspecified formal parameters. A DEFINE mechanism serves a similar purpose to the #defines found in C, but is fully integrated into the language rather than being a preprocessor. The EVENT data type facilitates coordination between processes, and ON FAULT blocks enable handling <b>program</b> <b>faults.</b>|$|R
40|$|Automatic {{repair of}} {{programs}} {{has been a}} longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a <b>program</b> <b>fault</b> is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report results from an initial set of experiments demonstrating that it can successfully repair ten different C programs totaling 63, 000 lines in under 200 seconds, on average. ...|$|E
30|$|Ammar et al. ([2000]) {{propose a}} survey of the {{different}} aspects of system fault tolerance and discuss some issues that arise in hardware fault tolerance and software fault tolerance. In this context, the authors distinguish information, spatial and temporal redundancy; present the three fundamental concepts of fault tolerance (i.e. failure, error, fault); describe the four steps of fault tolerance (i.e. error detection, damage assessment, error recovery, and fault removal) and relate these to the differences of redundant techniques for handling hardware as well as software faults. According to the authors, since redundancy may be used under a variety of forms to achieve fault tolerance, the design of a fault-tolerant system involves a set of trade-offs between redundancy requirements (imposed by the need for fault tolerance) and requirements of economy (economy of the process, and the product) (Ammar et al.[2000]). The authors also emphasize that <b>program</b> <b>fault</b> tolerance is no panacea, like almost everything in software engineering. We refer to their work for an interesting discussion on reasons to support this claim.|$|E
40|$|A {{transient}} hardware fault {{occurs when}} an energetic particle strikes a transistor, {{causing it to}} change state. Although transient faults do not permanently damage the hardware, they may corrupt computations by altering stored values and signal transfers. In this paper, we propose a new scheme for provably safe and reliable computing {{in the presence of}} transient hardware faults. In our scheme, software computations are replicated to provide redundancy while special instructions compare the independently computed results to detect errors before writing critical data. In stark contrast to any previous efforts in this area, we have analyzed our fault tolerance scheme from a formal, theoretical perspective. To be specific, first, we provide an operational semantics for our assembly language, which includes a precise formal definition of our fault model. Second, we develop an assembly-level type system designed to detect reliability problems in compiled code. Third, we provide a formal specification for <b>program</b> <b>fault</b> tolerance under the given fault model and prove that all well-typed programs are indeed fault tolerant. In addition to the formal analysis, we evaluate our detection scheme and show that it only takes 34 % longer to execute than the unreliable version. ...|$|E
50|$|American champions Marissa Castelli / Simon Shnapir had {{an error}} ridden <b>program</b> <b>faulting</b> on their triple twist, {{side by side}} spins and throw triple salchow to finish in third with 53.06, {{trailing}} second by 13.27 points. There were no Japanese teams in this event. China's highest ranking team was Peng Cheng / Zhang Hao in fifth place with 52.46 points, after doubling a side-by-side triple toe and making an error on the throw triple loop.|$|R
40|$|Abstract-This paper {{presents}} four optimization {{models to}} demonstrate that the optimization of software reliability within the available resources can be accomplished. The models help us find the optimal software system structure while considering basic information on reliability and cost of modules. Each model is applicable to a distinct situation. All four models maximize reliability while ensuring that expenditures remain within budget. Index Terms- Dynamic <b>programming,</b> <b>fault</b> tolerance, integer pro-gramming, modularization, optimization, software reliability. I...|$|R
50|$|A {{compiler}} {{is likely}} to perform many {{or all of the}} following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. <b>Program</b> <b>faults</b> caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.|$|R
40|$|We {{introduce}} {{the concept of}} “residual investigation ” for program analysis. A residual investigation is a dynamic check installed {{as a result of}} running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted <b>program</b> <b>fault</b> is likely to be realizable and relevant. The key feature of a residual investigation is {{that it has to be}} much more precise (i. e., with fewer false warnings) than the static analysis alone, yet significantly more general (i. e., reporting more errors) than the dynamic tests in the program’s test suite pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when taken in conjunction with the static error report, increase confidence in the existence of an error, as well as its severity, without needing to directly observe a fault resulting from the error. We enhance the static analyzer FindBugs with several residual investigations, appropriately tuned to the static error patterns in FindBugs, and apply it to 7 large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (“false positives”) while reporting several actual errors that would not have been detected by mere execution of a program’s test suite...|$|E
40|$|This paper {{reports on}} an {{empirical}} {{evaluation of the}} fault-detecting ability of two white-box soft-ware testing techniques: decision coverage (branch testing) and the all-uses data flow testing cri-terion. Each subject program was tested using {{a very large number}} of randomly generated test sets. For each test set, the extent to which it satisfied the given testing criterion was measured and it was determined whether or not the test set detected a <b>program</b> <b>fault.</b> These data were used to explore the relationship between the coverage achieved by test sets and the likelihood that they will detect a fault. Previous experiments of this nature have used relatively small subject programs and/or have used programs with seeded faults. In contrast, the subjects used here were eight versions of an antenna configuration program written for the European Space Agency, each consisting of over 10, 000 lines of C code. For each of the subject programs studied, the likelihood of detecting a fault increased sharply as very high coverage levels were reached. Thus, this data supports the belief that these testing techniques can be more effective than random testing. However, the magnitudes of the increases were rather inconsistent and it was difficult to achieve high coverage levels. ...|$|E
40|$|This paper {{introduces}} a dynamic metric {{that is based}} on the estimated ability of a program to withstand the effects of injected "semantic mutants" during execution by computing the same function as if the semantic mutants had not been injected. Semantic mutants include: (1) syntactic mutants injected into an executing program and (2) randomly selected values injected into an executing program's internal states. The metric is a function of a program, the method used for injecting these two types of mutants, and the program's input distribution; this metric is found through dynamic executions of the program. A program's ability to withstand the effects of injected semantic mutants by computing the same function when executed is then used as a tool for predicting the difficulty that will be incurred during random testing to reveal the existence of faults, i. e., the metric suggests the likelihood that a program will expose the existence of faults during random testing assuming faults were to exist. If the metric is applied to a module rather than to a program, the metric can be used to guide the allocation of testing resources among a program's modules. In this manner the metric acts as a white-box testing tool for determining where to concentrate testing resources. Index Terms: Revealing ability, random testing, input distribution, <b>program,</b> <b>fault,</b> failure...|$|E
50|$|PSP0 has 3 phases: planning, {{development}} (design, code, compile, test) and a post mortem. A baseline {{is established}} of current process measuring: {{time spent on}} <b>programming,</b> <b>faults</b> injected/removed, size of a program.In a post mortem, the engineer ensures all data for the projects has been properly recorded and analysed. PSP0.1 advances the process by adding a coding standard, a size measurement {{and the development of}} a personal process improvement plan (PIP). In the PIP, the engineer records ideas for improving his own process.|$|R
40|$|Historically, {{relatively}} {{less emphasis}} {{has been placed}} on software testing in comparison with other activities, such as systems analysis and design, of the software life cycle in an undergraduate computer science or software engineering curriculum. Testing, however, is a common and important technique used to detect <b>program</b> <b>faults.</b> Thus, testing must be taught rigorously to the students. This paper reports our experience of teaching the classification-tree method as a black-box testing technique at The University of Melbourne and Swinburne University of Technology. Keywords Black-box testing, category-partition method, classification-tree method, specification-base...|$|R
40|$|We {{describe}} {{a technique to}} make application <b>programs</b> <b>fault</b> tolerant. This techADnique {{is based on the}} concept of checkpointing from an active program to one ormore passive backup copies which serve as an abstraction of stable memory. Ifthe primary copy fails, one of the backup copies takes over and resumes processADing service requests. After each failure a new backup copy is created in order torestore the replication degree of the service. All mechanisms necessary to achieveand maintain fault tolerance can be added automatically to the code of a nonADfaulttolerant server, thus making fault tolerance completely transparent for the applicaADtion programmer...|$|R
40|$|Background: Complications of {{national}} program vaccination have great importance, because {{widespread use of}} vaccines and expanded age range of vaccinees increases the percentage of side effects. Present study was performed for evaluating the complications {{of national}} measles and rubella vaccination in Hamedan province. Materials and methods: This cross-sectional descriptive study was done from December sixth to January fifth 2003 and two months after termination of vaccination program. The study was performed by filling complication forms that were distributed in the vaccination centers. The data was analyzed by SPSS software. Results: A total of 827468 persons had been vaccinated in Hamedan, 260 out of them showed different complications. They had occurred mostly in 10 to 14 year old age group and had reported mainly by vaccination teams. 61. 41 % of complications had been occurred in the first 24 hour after vaccination. 86 % of them had reactions to vaccination. 91 % of cases were outpatient. The complications with incidence rate more than were urticaria, fever, head ache, vomiting, lymphadenopathy, cough, skin rash, myalgia, pharyngitis, arthritis, rhinitis, restlessness and occular complications in decreasing order. The complications with incidence rate less than were sever dyspnea, seizure, diarrhea, paresthesia, anaphylaxis, encephalitis, thrombocytopenia, and Gillan – Barre syndrome. Conclusion: Mild complications were more frequent than severe ones. Most of the complications were related to reaction to vaccine, we didn’t have any <b>program</b> <b>fault.</b> Therefore we suggest a potent and flexible care system for reacting to side effects...|$|E
40|$|In {{debugging}} distributed programs {{a distinction}} is made between an observed error and the <b>program</b> <b>fault,</b> or bug, {{that caused the}} error. Testing reveals an error; debugging {{is the process of}} tracing the error through time and space to the bug that caused it. A program is considered to be in error when some state of computation violates a safety requirement of the program. Expressing safety requirements {{in such a way that}} a computation can be monitored for safe behavior is thus a basic preliminary step in the testing-debugging cycle. Safety requirements are usually expressed as predicates. When a state of the computation violates such a safety predicate, that state can be said to be in error. A predicate logic is proposed that permits the specification of relationships between distributed predicates. This increases the scope and precision of situation-specific conditions that can be specified and detected. It also permits the specification of safety primitives such as P unless Q using distributed predicates. Thus a distributed program can be directly monitored for satisfaction and violation of safety requirements. Breakpoint conditions and predicates expressing safety may hold over a number of states of a program. A breakpoint state is meaningful if the causal relationships of events included in the breakpoint are unambiguous. At least two such states exist for each condition: the minimal and the maximal prefix of the computation at which the predicate holds. These states are specifiable as part of a breakpoint definition in the logic presented...|$|E
40|$|An optimistic {{computation}} is a computation {{that makes}} guesses about its future behavior, then proceeds with execution {{based on these}} guesses {{before they can be}} verified. Optimistic computations guess data values before they are produced and guess the control flow of computations before it is known. The performance of optimistic computations is determined by the number of idle resources available for optimistic execution, the percentage of guesses that are correct, the bookkeeping costs of managing optimistic execution, and the overhead of preventing optimistic computations from interfering with their execution environments until the guesses that they are based on are verified. We model computations by their program dependence graphs, then present a series of application-independent transformations on the dependence graphs that convert pessimistic computations into semantically equivalent optimistic computations. We demonstrate the viability of this approach by applying our transformations to the make <b>program,</b> <b>fault</b> tolerance based on message logging and checkpointing, distributed simulation, database concurrency control, and bulk data transfer. Our derived optimistic computations are similar to those presented in the literature, but typically require additional application-dependent transformations to produce viable implementations. We investigate, in detail, the implementation and performance of optimistic make, an optimistic variant of conventional pessimistic make. Optimistic make executes the commands necessary to bring makefile targets up-to-date prior to the time the user types a make request. The side effects produced by these optimistically executed commands are masked using encapsulations until the commands are known to be needed, at which time the side effects are committed to the external world. The side effects of unneeded commands and commands executed with inputs that have changed are discarded. Measured and simulated results from our implementation of optimistic make show a median response time improvement of 1. 72 and a mean improvement of 8. 28 over pessimistic make...|$|E
40|$|Estimating {{the fault}} rate {{function}} Paging activity {{can be a}} major factor in determining whether a software workload will run on a given computer system. A program’s paging behavior is difficult to predict because it depends not only on the workload processed by the program, but also on the level of storage contention of the processor. A <b>program’s</b> <b>fault</b> rate function relates storage allocation to the page fault rate experienced while processing a given workload. Thus, with the workload defined, the fault rate function can be used to see how the program’s storage allocation is affected by varying levels of storage contention, represente...|$|R
50|$|One {{problem with}} the {{handling}} logic behind the ON statement {{was that it would}} only be invoked for <b>program</b> <b>faults,</b> not for <b>program</b> terminations having other causes. Over time, the need for guaranteed handling of abnormal terminations grew. In particular, a mechanism was needed to allow programs to invoke plug-ins written by customers or third parties without any risk should the plug-in behave badly. In addition to general plug-in mechanisms, the new form of dynamic library linkage (Connection Libraries) allows programs to import and export functions and data, and hence one program runs code supplied by another.|$|R
5000|$|Developer of Poly3D, a {{boundary}} element computer <b>program</b> to analyse <b>faults</b> and fractures.|$|R
