314|10000|Public
5|$|Applying ROT13 to a <b>piece</b> <b>of</b> <b>text</b> merely {{requires}} examining its alphabetic {{characters and}} replacing each {{one by the}} letter 13 places further along in the alphabet, wrapping {{back to the beginning}} if necessary.|$|E
5|$|Due to the Famicom's {{restricted}} hardware capabilities, the remake {{is entirely}} two-dimensional. Special compensation {{was made for}} some of the in-battle sprites such as Cloud Strife's, combining two 16x24 pixel sprites side-by-side instead of the usual single sprite to account for weapons such as Cloud's sword or Barret's gun. While most Japanese games use only 8x8-pixel hiragana or katakana fonts, and most Chinese games use 4-color 16x16-pixel tiles stored in dedicated CHR ROM pages, this game uses its own several-hundred 16x16-pixel monochrome font instead. The script itself is strewn in chunks across the code; {{at the beginning of each}} <b>piece</b> <b>of</b> <b>text</b> for dialog boxes is a three digit number preceded by an @-symbol, signifying which character portrait to display.|$|E
5|$|The {{basic unit}} of Maya hieroglyphic text is the glyph block, which transcribes {{a word or}} phrase. The block is {{composed}} {{of one or more}} individual glyphs attached to each other to form the glyph block, with individual glyph blocks generally being separated by a space. Glyph blocks are usually arranged in a grid pattern. For ease of reference, epigraphers refer to glyph blocks from left to right alphabetically, and top to bottom numerically. Thus, any glyph block in a <b>piece</b> <b>of</b> <b>text</b> can be identified: C4 would be third block counting from the left, and the fourth block counting downwards. If a monument or artefact has more than one inscription, column labels are not repeated, rather they continue in the alphabetic series; if there are more than 26 columns, the labelling continues as A', B', etc. Numeric row labels restart from 1 for each discrete unit of text.|$|E
40|$|Many {{applications}} of computational linguistics are {{greatly influenced by}} the quality of corpora available and as automatically generated corpora continue to play an increasingly common role, it is essential that we not overlook the importance of well-constructed and homogeneous corpora. This paper describes an automatic approach to improving the homogeneity of corpora using an unsupervised method of statistical outlier detection to find documents and segments that do not belong in a corpus. We consider collections of corpora that are homogeneous with respect to topic (i. e. about the same subject), or genre (written for the same audience or from the same source) and use a combination of stylistic and lexical features <b>of</b> the <b>texts</b> to automatically identify <b>pieces</b> <b>of</b> <b>text</b> in these collections that break the homogeneity. These <b>pieces</b> <b>of</b> <b>text</b> that are significantly {{different from the rest of}} the corpus are likely to be errors that are out of place and should be removed from the corpus before it is used for other tasks. We evaluate our techniques by running extensive experiments over large artificially constructed corpora that each contain single <b>pieces</b> <b>of</b> <b>text</b> from a different topic, author, or genre than the rest of the collection and measure the accuracy <b>of</b> identifying these <b>pieces</b> <b>of</b> <b>text</b> without the use of training data. We show that when these <b>pieces</b> <b>of</b> <b>text</b> are reasonably large (1, 000 words) we can reliably identify them in a corpus. 1...|$|R
5000|$|The {{memorial}} {{is built}} from two <b>pieces</b> <b>of</b> black granite and was constructed by Zanon Memorials. It depicts {{an image of}} a family and two <b>pieces</b> <b>of</b> <b>text.</b>|$|R
50|$|The {{brand is}} often {{characterized}} by bold, abstract patterns, unusual graphics and short <b>pieces</b> <b>of</b> <b>text</b> which all allude {{to a similar}} theme.|$|R
25|$|In {{algorithmic}} {{information theory}} (a subfield of computer science and mathematics), the Kolmogorov complexity of an object, {{such as a}} <b>piece</b> <b>of</b> <b>text,</b> is {{the length of the}} shortest computer program (in a predetermined programming language) that produces the object as output.|$|E
500|$|Shen Kuo {{wrote that}} during the Qingli reign period (1041–1048), under Emperor Renzong of Song (1022–1063), an obscure {{commoner}} and artisan known as Bi Sheng (990–1051) invented ceramic movable type printing. Although the use of assembling individual characters to compose a <b>piece</b> <b>of</b> <b>text</b> had its origins in antiquity, Bi Sheng's methodical innovation was something completely revolutionary for his time. Shen Kuo noted that the process was tedious if one only wanted to print a few copies of a book, but if one desired to make {{hundreds or thousands of}} copies, the process was incredibly fast and efficient. Beyond Shen Kuo's writing, however, nothing is known of Bi Sheng's life or the influence of movable type in his lifetime. Although the details of Bi Sheng's life were scarcely known, Shen Kuo wrote: ...|$|E
2500|$|Many wiki implementations, such as MediaWiki, allow {{users to}} supply an edit summary when they edit a page. This {{is a short}} <b>piece</b> <b>of</b> <b>text</b> {{summarizing}} the changes they have made (e.g., [...] "Corrected grammar," [...] or [...] "Fixed formatting in table."). It is not inserted into the article's main text, but is stored along with that revision of the page, allowing users to explain {{what has been done}} and why, similar to a log message when making changes in a revision-control system. This enables other users to see which changes have been made by whom and why, often in a list of summaries, dates and other short, relevant content, a list which is called a [...] "log" [...] or [...] "history." ...|$|E
40|$|The　object　of　this　paper　is　to　consider　what　sort　of　composition　are　made　for different　speaker　who　lived　in　different　region．　There　are　regional　differences between　two　<b>pieces</b>　<b>of</b>　<b>text</b>　which　is　spoken　in　different　dialect． 　Iattempt　to　show　the　contract　<b>of</b>　the　two　<b>pieces</b>　<b>of</b>　<b>text</b>　<b>of</b>　discourse　which　is spoken　in　TOTTORI，　TOKYO，　OSAKA　and　TOHOKU　dialect．　I　try　analyzing　the two　<b>pieces</b>　<b>of</b>　<b>text</b>　<b>of</b>　TOTTORI　dialect　by　content　and　what　information　comes　in what　order　in　each． 　　 1 n　TOTTORI　dialect　the　following　patterns　of　composition　are　frequently observed：the　Same　information　is　repeatedly　presented：argument　are　given　in the　inductive　manner：argument　are　given　in　the　deductive　manner．　The　main information　is　not　presented　in　the　middle　of　discourse．　As　for　this　composition， TOHOKU　dialect　is　similar　to　TOTTORI　dialect．　In　TOKYO　dialect　there　are patterns　of　composition　that　the　main　information　is　in　the　middle　of　discourse．　In OSAKA　dialect　the　main　information　in　．itself　is　not　in　the　each　position　of　discourse． 　The　arrangement　<b>of</b>　<b>pieces</b>　<b>of</b>　information，　what　shou且d　be　the　main　information seem　to　be　set　by　the　character　of　the　two　different　speaker...|$|R
30|$|This {{approach}} {{supports the}} detection of wiki contributions that simply copy and paste large <b>pieces</b> <b>of</b> <b>text</b> without actually improving the wiki page, which otherwise may have received an undeservedly good quantitative assessment.|$|R
5000|$|A {{cut-and-paste}} job or cut {{and paste}} approach is a pejorative reference to various kinds of work produced by [...] "{{cut and paste}}", i.e., a quick combination <b>of</b> <b>pieces</b> <b>of</b> <b>text</b> collected from various sources, a [...]|$|R
5000|$|Convert a given <b>piece</b> <b>of</b> <b>text</b> into a spoken {{wave file}} (.wav).|$|E
5000|$|All text {{has been}} merged {{to form a}} single <b>piece</b> <b>of</b> <b>text</b> and they are from these cited sources: ...|$|E
5000|$|In a {{hypertext}} or hypermedia database, {{any word}} or a <b>piece</b> <b>of</b> <b>text</b> representing an object, e.g., another <b>piece</b> <b>of</b> <b>text,</b> an article, a picture, or a film, can be hyperlinked to that object. Hypertext databases are particularly useful for organizing {{large amounts of}} disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database.|$|E
5000|$|Siouxsie and the Banshees' set, however, was {{completely}} improvisational. They didn't know or play any songs, and their act {{had a very}} [...] "performance art" [...] quality. Siouxsie, for instance, recited The Lord's Prayer and similar memorised <b>pieces</b> <b>of</b> <b>text.</b>|$|R
5000|$|More recently, Randall Trigg's TextNet and NoteCards systems further {{explored}} this idea. TextNet {{revolved around}} [...] "primitive <b>pieces</b> <b>of</b> <b>text</b> connected with typed links {{to form a}} network similar in many ways to a semantic network". Though text-centric, {{it was clear that}} Trigg's goal was to model the associations between primitive ideas and hence to reflect the mind's understanding. [...] "By using...structure, meaning can be extracted from the relationships between chunks (small <b>pieces</b> <b>of</b> <b>text)</b> rather than from the words making them up." [...] The subsequent NoteCards effort was similarly designed to [...] "formulate, structure, compare, and manage ideas". It was useful for [...] "analyzing information, constructing models, formulating arguments, designing artifacts, and generally processing ideas".|$|R
40|$|The {{task for}} this year consist in {{retrieve}} snippets or <b>pieces</b> <b>of</b> <b>text</b> from web documents about several topics. The extraction of such snippets can be approached in several ways, {{as well as the}} selection of most usefull of them. We describe the segementation process adopted, and the selection of snippets carried out...|$|R
5000|$|Unsummarize - A service plugin {{that takes}} a <b>piece</b> <b>of</b> <b>text</b> and makes it longer, {{seemingly}} doing the reverse of the system's [...] "summarize" [...] service.|$|E
50|$|TSF {{enables a}} text service to store {{metadata}} with a document, a <b>piece</b> <b>of</b> <b>text,</b> or an object within the document. For example, a speech input text service can store sound information {{associated with a}} block of text.|$|E
50|$|For a {{more recent}} method, see Řehůřek and Kolkus (2009). This method can detect {{multiple}} languages in an unstructured <b>piece</b> <b>of</b> <b>text</b> and works robustly on short texts {{of only a few}} words: something that the n-gram approaches struggle with.|$|E
50|$|An SGML {{document}} may {{be composed}} from many entities (discrete <b>pieces</b> <b>of</b> <b>text).</b> In SGML, the entities and element types {{used in the}} document may be specified with a DTD, the different character sets, features, delimiter sets, and keywords are specified in the SGML Declaration to create the concrete syntax of the document.|$|R
5000|$|Some {{place names}} are also spelled {{differently}} {{in order to}} emphasize some political view. For instance, Brasil (the Portuguese spelling of [...] "Brazil"), is sometimes misconstrued as a typo for Brazil in English texts. Alternatively, the English spelling Brazil is used in Portuguese <b>pieces</b> <b>of</b> <b>text</b> {{as a way to}} denote anti-Americanism or anti-globalization sentiment.|$|R
50|$|In {{order for}} search-engine spiders {{to be able}} to rate the {{significance}} <b>of</b> <b>pieces</b> <b>of</b> <b>text</b> they find in HTML documents, and also for those creating mashups and other hybrids as well as for more automated agents as they are developed, the semantic structures that exist in HTML need to be widely and uniformly applied to bring out the meaning <b>of</b> published <b>text.</b>|$|R
5000|$|... ____ Revisited - [...] "conceived" [...] by Max Brandel {{according}} to his credit, these photographic pieces would take a long-established <b>piece</b> <b>of</b> <b>text,</b> such as the Preamble to the U.S. Constitution, or the Ten Commandments, and systematically illustrate the text with ironically-chosen photo images.|$|E
50|$|The {{mappings}} {{of words}} to concepts are often ambiguous. Typically each {{word in a}} given language will relate to several possible concepts. Humans use context to disambiguate the various meanings of a given <b>piece</b> <b>of</b> <b>text,</b> where available machine translation systems cannot easily infer context.|$|E
50|$|Gnomes {{states that}} American Born Chinese {{is a great}} {{resource}} to help academically struggling students (particularly struggling readers) and students with social-cognitive disabilities to find motivation to learn, to relate a <b>piece</b> <b>of</b> <b>text</b> to their lives, {{and to use the}} graphics to help them understand/relate to the words.|$|E
50|$|The unique {{method used}} by RSVP {{allows for the}} reading <b>of</b> {{unlimited}} <b>text</b> in a limited position. Researchers from Carnegie Mellon have found {{that as many as}} 12 words per second are readable in controlled situations (720wpm).Further research has shown that for shorter <b>pieces</b> <b>of</b> <b>text,</b> RSVP formats increased reading speed by 33% with no significant differences in comprehension or task load.|$|R
25|$|Dear Dead Days (1959) {{is not a}} {{collection}} of his cartoons (although it reprints a few from previous collections); it is a scrapbook-like compendium of vintage images (and occasional <b>pieces</b> <b>of</b> <b>text)</b> that appealed to Addams' sense of the grotesque, including Victorian woodcuts, vintage medicine-show advertisements, and a boyhood photograph of Francesco Lentini, who had three legs.|$|R
50|$|Another {{use that}} was typical before {{the era of}} {{personal}} computers (which brought about word processing and computer graphics) was to hold <b>pieces</b> <b>of</b> <b>text</b> and art that had been clipped with scissors from one sheet of paper to be pasted with paste onto another sheet. This use provided the analogy and terminology for computer clipboards.|$|R
50|$|Type {{color of}} text is {{affected}} by the amount and weight of the strokes on a page. Similar to when writing with a pen on paper, the more layers of strokes, the darker the text. At smaller sizes, darker colored text {{does not necessarily mean that}} text will be more legible. The boldness and weight at this smaller size can actually make a <b>piece</b> <b>of</b> <b>text</b> more difficult to read. For this reason, the fonts that are commonly used to type large blocks of text are not overly dark or heavy on the page. They create a good balance of text color and white space. This balance is important for an attractive and legible <b>piece</b> <b>of</b> <b>text.</b> There are differences between acceptable and legible levels of type color between body text and headings or titles.|$|E
50|$|The Alias System {{feature of}} Nxt {{essentially}} allows one <b>piece</b> <b>of</b> <b>text</b> to {{be substituted for}} another, so that keywords {{can be used to}} represent other things - names, telephone numbers, physical addresses, web sites, account numbers or emails. This would for example allow for a decentralized DNS system similar to Namecoin.|$|E
5000|$|An {{interaction}} task is [...] "the unit of {{an entry}} of information by the user", such as entering a <b>piece</b> <b>of</b> <b>text,</b> issuing a command, or specifying a 2D position. A similar concept {{is that of}} domain object, which {{is a piece of}} application data that can be manipulated by the user.|$|E
50|$|Intertextuality is the {{combining}} of past writings into original, new <b>pieces</b> <b>of</b> <b>text.</b> The term intertextuality {{was coined}} in 1966 by Julia Kristeva. All texts are necessarily related to prior texts through {{a network of}} links, writers (often unwittingly) make use of what has previously been written and thus some degree of borrowing is inevitable. This generally occurs within a specific discourse community.|$|R
50|$|Trac is a web {{application}} for issue tracking in software development. It has a wiki engine {{which is used}} for all text and documentation in the system. This includes not only wiki pages but also tickets and check-in log messages. These <b>pieces</b> <b>of</b> <b>text</b> have AutoLinks created, for example the text ticket:1 links to ticket 1. This {{is an example of}} server-side AutoLinking.|$|R
30|$|So as to {{increase}} the validity and reliability, all the research instruments were piloted before the main study. Furthermore, double-coding was employed in order to check and increase {{the reliability of the}} content analysis. Two methods for double-coding are intra-coder and inter-coder. For the intra-coding, the researcher chose three <b>pieces</b> <b>of</b> <b>text</b> from the interview items which had been already coded to recode them. The researcher checked the reliability which was set over 65 %. Concerning the inter-coding, the researcher had two scholars as inter-coders to recode three <b>pieces</b> <b>of</b> <b>text</b> from the open-ended items. Then, the researcher compared his three coded texts with those from two inter-coders. The two inter-coders and the researcher reached to an agreement level (83 %) of reliability. As the interview transcriptions were in participants’ mother tongue, the researcher had to translate all the transcriptions into English. The researcher then asked one teacher of English to double-check the accuracy of the translated version.|$|R
