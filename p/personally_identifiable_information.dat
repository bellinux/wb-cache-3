505|361|Public
25|$|Software and {{technology}} services (20%), {{which do not}} include the distribution of <b>personally</b> <b>identifiable</b> <b>information.</b>|$|E
25|$|In June 2016, Julia Angwin of ProPublica {{wrote about}} Google's updated privacy policy, which deleted a clause that had stated Google would not combine DoubleClick web {{browsing}} cookie information with <b>personally</b> <b>identifiable</b> <b>information</b> from its other services. This change has allowed Google to merge users’ <b>personally</b> <b>identifiable</b> <b>information</b> from different Google services {{to create one}} unified ad profile for each user. After publication of the article, Google reached out to ProPublica {{to say that the}} merge would not include Gmail keywords in ad targeting.|$|E
25|$|Information {{security}} concerns surrounding PHRs extend beyond technological issues. There are also ethical issues affecting {{the transfer of}} <b>personally</b> <b>identifiable</b> <b>information</b> in the treatment process. Only gradually are architectural requirements and information-use policies becoming available such as the Privacy Rule under the U.S. Health Insurance Portability and Accountability Act (HIPAA).|$|E
50|$|In 2010, {{the company}} was criticzied, along with other social networks, for passing user <b>personally</b> <b>identifiable</b> profile <b>information</b> to {{advertisers}} when members clicked on ads.|$|R
40|$|Secretary {{shall not}} approve {{a request for}} an {{inter-agency}} transfer of <b>personally</b> <b>identifiable</b> employee medical <b>information,</b> {{which has not been}} consented to by the affected employees, unless the request is by a public health agency which: (i) Needs the requested <b>information</b> in a <b>personally</b> <b>identifiable</b> form for a substantial public health purpose, (ii) Will not use the requested information to make individual determinations concerning affected employees which could be to their detriment, (iii) Has regulations or established written procedures providing protection for <b>personally</b> <b>identifiable</b> medical <b>information</b> substantially equivalent to that of this section, and (iv) Satisfies an exemption to the Privacy Act {{to the extent that the}} Privacy Act applies to the requested information (See, 5 U. S. C. 552 a(b); 29 CFR 70 a. 3). (3) Upon the approval of the Assistant Secretary, <b>personally</b> <b>identifiable</b> employee medical <b>information</b> may be transferred to...|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO reviewed four areas related to the Health Care Financing Administration's (HCFA) use of <b>personally</b> <b>identifiable</b> health <b>information,</b> focusing on: (1) HCFA's need for <b>personally</b> <b>identifiable</b> health <b>information</b> to manage the Medicare program and accomplish other purposes; (2) HCFA's policies and practices regarding disclosure of information on Medicare beneficiaries; (3) the adequacy of HCFA's safeguards for protecting the confidentiality of electronic information and HCFA's monitoring of others' protection of beneficiary information; and (4) the effect on HCFA of state restrictions on the disclosure of confidential health information. ...|$|R
25|$|FieldShield is a CoSort {{spin-off}} {{designed to}} protect data privacy. The software protects <b>personally</b> <b>identifiable</b> <b>information</b> and other private data at the field or record level within database tables, files and other sources subject to data spill. Privacy functions include AES encryption, data masking, and pseudonymization. Job details can be audited from a log file in XML format.|$|E
500|$|Twitter {{messages}} are public, but users can also send private messages. Information about who {{has chosen to}} follow an account and who a user has chosen to follow is also public, though accounts can be changed to [...] "protected" [...] which limits this information (and all tweets) to approved followers. Twitter collects <b>personally</b> <b>identifiable</b> <b>information</b> about its users and shares it with third parties as specified in its privacy policy. The service also reserves the right to sell this information as an asset if the company changes hands. While Twitter displays no advertising, advertisers can target users based on their history of tweets and may quote tweets in ads directed specifically to the user.|$|E
500|$|According to LinkNYC, it {{does not}} monitor its kiosks' Wi-Fi, nor does it give {{information}} to third parties. However, data {{will be given to}} law enforcement officials in situations where LinkNYC is legally obliged. Its privacy policy states that it can collect <b>personally</b> <b>identifiable</b> <b>information</b> (PII) from users to give to [...] "service providers, and sub-contractors to the extent reasonably necessary to enable us provide the Services; a third party that acquires CityBridge or a majority of its assets a third party with whom we must legally share information about you; you, upon your request; [...] other third parties with your express consent to do so." [...] Non-personally identifiable information can be shared with service providers and advertisers. The privacy policy also states that [...] "in the event that we receive a request from a governmental entity to provide it with your [...] Information, we will take reasonable attempts to notify you of such request, to the extent possible." ...|$|E
40|$|Testimony {{issued by}} the General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO discussed how the Health Care Financing Administration (HCFA) protects <b>personally</b> <b>identifiable</b> health <b>information</b> on Medicare beneficiaries, focusing on: (1) HCFA's need for <b>personally</b> <b>identifiable</b> health <b>information</b> to manage the Medicare program; (2) HCFA's policies and practices regarding disclosure of information on Medicare beneficiaries to other organizations; (3) the adequacy of HCFA's safeguards for protecting the confidentiality of electronic information and its monitoring of other organizations that obtain information on Medicare beneficiaries; and (4) the effect on HCFA of state restrictions on the disclosure of confidential health information. ...|$|R
40|$|The California Financial Information Privacy Act, 1 {{enacted on}} August 28, 2003, and {{effective}} on July 1, 2004, governs {{the rights of}} California residents {{with respect to the}} dissemination of nonpublic personal information by financial institutions. In some respects, it diverges from two federal laws that impose restrictions on the dissemination of nonpublic <b>personally</b> <b>identifiable</b> customer <b>information</b> by financial information...|$|R
40|$|The {{widespread}} use of automated systems to collect, store, and retrieve data in the public, private, and academic sectors {{has given rise to}} a large number of databases. Many of these databases contain private, <b>personally</b> <b>identifiable,</b> or sensitive <b>information.</b> In the financial sector, databases store information abou...|$|R
500|$|CityBridge {{emphasized}} {{that it takes}} security and privacy seriously [...] "and will never sell any <b>personally</b> <b>identifiable</b> <b>information</b> or share with third parties for their own use." [...] Aside from the unsecured network that devices can directly connect to, the Links provide an encrypted network that shields communications from eavesdropping within the network. There {{are two types of}} networks: a private (secured WPA/WPA2) network called [...] "LinkNYC Private," [...] which is available to iOS devices with iOS 7 and above; and a public network called [...] "LinkNYC Free Public Wi-Fi," [...] which is available to all devices but is only protected by the device's browser. Private network users will have to accept a network key in order to log onto the LinkNYC Wi-Fi. This would make New York City one of the first American municipalities to have a free, encrypted Wi-Fi network, as well as North America's largest. LinkNYC would also be the fastest citywide ISP in the world, with download and upload speeds between 15 and 32 times faster than on free networks at Starbucks, in LaGuardia Airport, and within New York City hotels.|$|E
2500|$|The {{inadvertent}} {{revelation of}} <b>personally</b> <b>identifiable</b> <b>information</b> {{leading to the}} provider violates Fair Information Practices. [...] This indiscretion can cause financial, ...|$|E
2500|$|Major {{components}} {{put into}} place to govern the collection, disclosure, and protection of consumers' nonpublic personal information; or <b>personally</b> <b>identifiable</b> <b>information</b> include: ...|$|E
50|$|The TPM {{specification}} offers {{features and}} suggested implementations that {{are meant to}} address the anonymity requirement. By using a third-party Privacy Certification Authority (PCA), the information that identifies the computer could be held by a trusted third party. Additionally, the use of direct anonymous attestation (DAA), introduced in TPM v1.2, allows a client to perform attestation while not revealing any <b>personally</b> <b>identifiable</b> or machine <b>information.</b>|$|R
40|$|Public {{awareness}} {{of the potential for}} violation of personal privacy in clinical information systems is increasing. Much of this increase {{can be attributed to the}} popularity and publicity of the World Wide Web. Nightly news reports of intruder break-ins and flaws in Internet software security have stimulated public interest in the security of clinical information systems available over the web. As part of the development of systems designed to provide clinical narratives to physicians over the Internet, we are exploring designs that provide additional protection and security to these systems. Specifically, we are developing and testing automated access control measures based on provider-patient relationships for controlling access to <b>personally</b> <b>identifiable</b> patient <b>information...</b>|$|R
5000|$|Definition: A [...] "customer" [...] is a {{consumer}} {{who has a}} [...] "customer relationship" [...] with a financial institution. A [...] "customer relationship" [...] is a continuing relationship with {{a consumer}}.Examples of establishing a customer relationship:* Opening a credit card account with a financial institution* Entering into an automobile lease (on a non-operating basis for an initial lease term of at least 90 days) with an automobile dealer* Providing <b>personally</b> <b>identifiable</b> financial <b>information</b> to a broker {{in order to obtain}} a mortgage loan* Obtaining a loan from a mortgage lender* Agreeing to obtain tax preparation or credit counseling services"Special Rule" [...] for Loans: The customer relationship travels with ownership of the servicing rights.|$|R
2500|$|Marketing {{services}} (9%), none {{of which}} include the distribution of <b>personally</b> <b>identifiable</b> <b>information,</b> but are regulated by state and federal [...] "Do Not Mail" [...] and [...] "Do Not Call" [...] legislation.|$|E
2500|$|In Firefox {{versions}} {{prior to}} 7.0, an information bar {{appears on the}} browser's first start asking users whether {{they would like to}} send performance statistics, or [...] "telemetry", to Mozilla. It is enabled by default in development versions of Firefox, but not in release versions. According to Mozilla's privacy policy, these statistics are stored only in aggregate format, and the only <b>personally</b> <b>identifiable</b> <b>information</b> transmitted is the user's IP address.|$|E
2500|$|Commentators {{both inside}} and outside the video game {{industry}} condemned the attacks against Quinn. The attacks included doxing (researching and broadcasting <b>personally</b> <b>identifiable</b> <b>information</b> about an individual) and hacking of her Tumblr, Dropbox, and Skype accounts; she was also subjected to rape and death threats. The release of personal information forced Quinn to flee her home; she explained that [...] "I can't go home because they have been posting around my home address, often with threats attached to it".|$|E
40|$|Sharing {{data that}} {{contains}} <b>personally</b> <b>identifiable</b> or sensitive <b>information,</b> such as medical records, always has privacy and security implications. The issues can become rather complex when {{the methods of}} access can vary, and accurate individual data needs to be provided whilst mass data release for specific purposes (for example for medical research) also has to be catered for. Although various solutions have been proposed to address the different aspects individually, a comprehensive approach is highly desirable. This paper presents a solution for maintaining the privacy of data released en masse in a controlled manner, and for providing secure access to the original data for authorized users. The {{results show that the}} solution is provably secure and maintains privacy in a more efficient manner than previous solutions...|$|R
40|$|The {{purpose of}} this study is to shed light on the problem of medical data loss—how it is disclosed, who is causing it and what can be done to combat it. This is a {{far-reaching}} problem that impacts not only organizations that are victims of these breaches, but also doctor-patient relationships. And it can have consequences that spread more broadly than just those directly affected by the incidents. For the purposes of this study, protected health information (PHI) is defined as <b>personally</b> <b>identifiable</b> health <b>information</b> collected from an individual, and covered under one of the state, federal or international data breach disclosure laws. PHI may be collected or created by a healthcare provider, health plan, employer, healthcare clearinghouse or other entity. The main criteria is whether there is a reasonable basis to believe the information could be used to identify an individual. In the U. S., the disclosure of this type of information would trigger a duty to report the breach under the Health Insurance Portability and Accountability Act (HIPAA), the Health Information Technology for Economic and Clinical Health Act (HITECH) and one or more of the state laws...|$|R
40|$|Mobile health {{applications}} (or mHealth apps, as {{they are}} commonly known) are increasingly popular with both individual end users and user groups such as physicians. Due {{to their ability to}} access, store and transmit <b>personally</b> <b>identifiable</b> and sensitive <b>information</b> (e. g. geolocation information and personal details), they are potentially an important source of evidentiary materials in digital investigations. In this paper, we examine 40 popular Android mHealth apps. Based on our findings, we propose a taxonomy incorporating artefacts of forensic interest to facilitate the timely collection and analysis of evidentiary materials from mobile devices involving the use of such apps. Artefacts of forensic interest recovered include user details and email addresses, chronology of user locations and food habits. We are also able to recover user credentials (e. g. user password and four-digit app login PIN number), locate user profile pictures and identify timestamp associated with the location of a user. Comment: Proceedings of 21 st Americas Conference on Information Systems (AMCIS 2015...|$|R
2500|$|Limited {{uses for}} <b>personally</b> <b>identifiable</b> <b>information</b> (PII), {{including}} name, address, phone, company name, size and business type, [...] "we {{do not share}} PII with third parties" [...] except for situations listed in the Privacy Policy, which include that they [...] "may disclose PII to government ... and private parties ... to satisfy... regulation ... subpoenas... to protect ... the public in general; ... to prevent or stop activity we consider to be illegal or unethical." [...] They also disclose enough information to provide [...] "advertising based upon your browsing activities and interests." ...|$|E
2500|$|The Jester, {{a hacker}} who {{generally}} {{went by the}} leetspeak handle th3j35t3r, vowed to find and expose members of LulzSec. Claiming to perform hacks {{out of a sense}} of American patriotism, he attempted to obtain and publish the real world <b>personally</b> <b>identifiable</b> <b>information</b> of key members, whom he described as [...] "childish". On 24 June 2011, he claimed to have revealed the identity of LulzSec leader Sabu as an information technology consultant possibly from New York City. On 24 June 2011, a hacker allegedly going by the name Oneiroi briefly took down the LulzSec website in what he labelled [...] "Operation Supernova". The Twitter page for the group also briefly became unavailable.|$|E
2500|$|Privacy advocates raised {{concerns}} that the deployment {{could be used to}} track individuals by generating radar geo-location data and correlating it with [...] other technology, including cellphone metadata and traffic cameras. [...] A spokesperson for the Army stated that [...] "absolutely, 100 percent" [...] that JLENS will not have video cameras, nor will it collect <b>personally</b> <b>identifiable</b> <b>information.</b> [...] “The primary mission...is to track airborne objects,” the Army said. “Its secondary mission is to track surface moving objects such as vehicles or boats. [...] The capability to track surface objects does not extend to individual people." [...] Experts cite the extreme angles from overhead as precluding even advanced surveillance systems from being able to identify faces or other features such as license plates, though anonymized geolocation data has been found to easily identify specific individuals.|$|E
40|$|The {{idea behind}} the usage of online and smart environments has been researched. Smart environments is a {{universal}} computer that help the people to do their daily activities invisibly and let them feel relax from daily tedious task. For {{the idea of making}} life much easier and independent of tedious work, ubiquitous computing gave birth to smart environments that will help us in our daily lives. As the technology is evolving day by day so it is become important for people to know the context behind every technology. In present study, a survey has been conducted to investigate the privacy and context awareness while working in an Ubicomp environment in different age groups. For this purpose, three different age groups (less than 30 years, between 30 to 40 years and above 40) has been selected. An online and physically participants are examined by different questions involving collection of <b>personally</b> and <b>identifiable</b> <b>information.</b> Results indicate that majority of online and smartphone users are aware of ubiquitous computing, smart environments and context awareness. At the same time the sampling data shows that the subjective are highly concerned about their privacy and want to know the context behind their actions. Further analysis of the data suggest on the basis of distinct groups participants with age between 30 - 40 Years are more concerned about privacy and knows the context behind their each action...|$|R
40|$|Effective bioterrorism planning, prevention, and {{response}} require information sharing between various entities, ranging from public health authorities and health-care workers {{to national security}} and law enforcement officials. While the source of much information exchanged may be nonidentifiable, many entities legitimately need access to <b>personally</b> <b>identifiable</b> health <b>information</b> (or 2 ̆ 2 protected health information 2 ̆ 2 [PHI]) in planning for and responding to a bioterrorism event. The HIPAA Privacy Rule allows for essential exchanges of health data during a public health emergency while protecting against unnecessary disclosures of PHI. In {{the event of a}} bioterrorist attack, the Privacy Rule allows covered entities to disclose PHI without individual authorization in the following instances: (1) for treatment by health-care providers, (2) to avert a serious threat to health or safety, (3) to public health authorities for public health purposes, (4) to protect national security, (5) to law enforcement under certain conditions, and (6) for judicial or administrative proceedings. Despite these favorable disclosure provisions, some privacy challenges remain. The flow of PHI may be slowed by misunderstandings of the Privacy Rule 2 ̆ 7 s accounting requirement. In addition, in a bioterrorism scenario, nontraditional entities may find themselves acting as health-care providers, triggering Privacy Rule provisions. Finally, the potential for de facto disclosures of individuals 2 ̆ 7 disease or exposure status increases where conspicuous treatment methods, isolation, or quarantine are implemented without additional measures to protect privacy. Understanding the Privacy Rule 2 ̆ 7 s impact on bioterrorism planning {{and response}} ensures that various entities can conduct their activities with needed information while still protecting individual privacy...|$|R
40|$|Smart cities rely on {{dynamic and}} {{real-time}} data to enable smart urban {{applications such as}} intelligent transport and epidemics detection. However, the streaming of big data from IoT devices, especially from mobile platforms like pedestrians and cars, raises significant privacy concerns. Future autonomous vehicles will generate, collect and consume significant volumes of data to be utilized in delivering safe and efficient transportation solutions. The sensed data will, inherently, contain <b>personally</b> <b>identifiable</b> and attributable <b>information</b> - both external (other vehicles, environmental) and internal (driver, passengers, devices). The autonomous vehicles {{are connected to the}} infrastructure cloud (e. g., Amazon), the edge cloud, and also the mobile cloud (vehicle to vehicle). Clearly these different entities must co-operate and interoperate in a timely fashion when routing and transferring the highly dynamic data. In order to maximise the availability and utility of the sensed data, stakeholders must have confidence that the data they transmit, receive, aggregate and reason on is appropriately secured and protected throughout. There are many different metaphors for providing end-to-end security for data exchanges, but they commonly require a management and control sidechannel. This work proposes a scalable smart city privacy-preserving architecture named Authorized Analytics that enables each node (e. g. vehicle) to divulge (contextually) local privatised data. Authorized Analytics is shown to scale gracefully to IoT scope deployments...|$|R
2500|$|Viewers {{expressed}} their anger on Twitter, Tumblr, {{and other social}} media sites, {{with a number of}} them threatening to dox (reveal <b>personally</b> <b>identifiable</b> <b>information</b> about) the writers, others making death threats, and some stating they were suicidal after watching the episode; people associated with the show immediately responded and tried to ease their thoughts, and defended the series by stating characters die on the show all the time. A number of fans compared Clarke and Lexa's final moments together to the death scene involving Willow Rosenberg and Tara Maclay from Buffy the Vampire Slayer, finding both moments unjust. The [...] "bury your gays" [...] trope rose to a national debate, and the international fan-led initiatives [...] "Lexa Deserved Better" [...] and [...] "LGBT Fans Deserve Better" [...] emerged, initially dominating Twitter. The Trevor Project was also borne out of the backlash, and had [...] "raised more than $30,000 (£21,000) in just a few hours." ...|$|E
2500|$|In {{ethics and}} politics, Deleuze again echoes Spinoza, albeit in a sharply Nietzschean key. In a {{classical}} liberal model of society, morality begins from individuals, who bear abstract natural rights or duties set by themselves or a God. Following his rejection of any metaphysics based on identity, Deleuze criticizes {{the notion of}} an individual as an arresting or halting of differentiation (as the etymology of the word [...] "individual" [...] suggests). Guided by the naturalistic ethics of Spinoza and Nietzsche, Deleuze instead seeks to understand individuals and their moralities as products of the organization of pre-individual desires and powers. In the two volumes of Capitalism and Schizophrenia, Deleuze and Guattari describe history as a congealing and regimentation of [...] "desiring-production" [...] (a concept combining features of Freudian drives and Marxist labor) into the modern individual (typically neurotic and repressed), the nation-state (a society of continuous control), and capitalism (an anarchy domesticated into infantilizing commodification). Deleuze, following Karl Marx, welcomes capitalism's destruction of traditional social hierarchies as liberating, but inveighs against its homogenization of all values to the aims of the market. [...] In a 1990 Postscript on the Societies of Control, Deleuze claims that institutions and technologies introduced since World War II have moved social coercion and discipline from only physical enclosures (such as schools, factories, prisons, office buildings, etc.) into the lives of individuals considered as [...] "masses, samples, data, markets, or 'banks'." [...] The mechanisms of modern [...] "societies of control" [...] will be continuous, following and tracking individuals throughout their existence via transaction records, mobile location tracking, and other <b>personally</b> <b>identifiable</b> <b>information.</b>|$|E
5000|$|In the <b>personally</b> <b>identifiable</b> <b>information</b> {{systems for}} {{maintenance}} of security level 1 included (Decree of the Federal Office for Technical and Export Control dated 18.02.2013 № 21 «List and {{contents of the}} organizational and technical means to provide information security of <b>personally</b> <b>identifiable</b> <b>information</b> in the <b>personally</b> <b>identifiable</b> <b>information</b> systems»).|$|E
40|$|Errors and {{anomalies}} {{during the}} capture and processing of health data {{have the potential}} to place <b>personally</b> <b>identifiable</b> values into attributes of a dataset that are expected to contain non-identifiable values. Anonymisation focuses on those attributes that have been judged to enable identification of individuals. Attributes that are judged to contain non-identifiable values are not considered, but may be included in datasets that are shared by organisations. Consequently, organisations are at risk of sharing datasets that unintendedly disclose <b>personally</b> <b>identifiable</b> values through these attributes. This would have ethical and legal implications for organisations and privacy implications for individuals whose <b>personally</b> <b>identifiable</b> values are disclosed. In this paper, we formulate the problem of unintended disclosure following anonymisation, describe the necessary steps to address this problem, and discuss some key challenges to applying these steps in practice...|$|R
30|$|Data was {{collected}} via interviews {{which could be}} <b>personally</b> <b>identifiable.</b> Sharing of collected data was not approved by the Institutional Review Board.|$|R
5000|$|Through {{the use of}} data anonymization or pseudo-anonymization, every data {{processing}} system should achieve the goal to use no (or as little as possible) <b>personally</b> <b>identifiable</b> data.|$|R
