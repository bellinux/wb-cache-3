2|13|Public
40|$|The {{theory of}} <b>polyphase</b> <b>sort</b> has {{simplified}} the mathematical derivations of Wilson and Shortt's (1980) algorithm, and offered an intuitive {{explanation of why}} Gries and Levin's (1980), and Urbanek's (1980) algorithms work. The computation of order-k Fibonacci numbers is equivalent to moving a window of matrix upwards {{in a series of}} ideal distribution...|$|E
40|$|Fibonacci {{sequences}} {{have been}} studied from many points of view. We shall be concerned with sequences of integers which satisfy difference equations of the form (1) £ /- LM + Li-k-i. For various values of k, we obtain generalized Fibonacci sequences as studied by Daykin [1] and Hoggatt [4], In [4], many interesting properties of these sequences are derived by using generating functions and generalized diagonals of Pascal's Triangle. For our purposes, however, we need a formula which allows the direct calculation of any particular term {{in any one of}} the sequences determined by (1). The techniques are standard (see Miles [6] or Flores [2]) but will be developed here for completeness. The advantages of closed-form formulas are important to many applications of Fibonacci sequences. In particular, the solution of (1) is useful to computer scientists in their study of algorithms. The <b>polyphase</b> <b>sort</b> algorithm, for instance, requires the use of Fibonacci numbers, and Fibonacci numbers arise naturally in the analysis of the algorithm to compute the greatest common divisor of two numbers. The application we investigate concerns the way Fibonacci numbers can be used to manage computer memory. Consider the objective of keeping as many jobs in memory as possible. To implement this, the system must keep extensive tables of areas in memory and the size of each area. As jobs finish, the memory area becomes checkerboarde...|$|E
50|$|A more {{sophisticated}} merge sort that optimizes tape (and disk) drive usage is the <b>polyphase</b> merge <b>sort.</b>|$|R
50|$|A <b>polyphase</b> merge <b>sort</b> is an {{algorithm}} which {{decreases the}} number of runs at every iteration of the main loop by merging runs into larger runs. It is used for external sorting.|$|R
50|$|In general, <b>polyphase</b> merge <b>sort</b> {{is better}} than {{ordinary}} merge sort when there are less than 8 files, while ordinary merge sort starts to become better at around 8 or more files.|$|R
50|$|Cascade {{merge sort}} {{is similar to}} the <b>{{polyphase}}</b> merge <b>sort</b> but uses a simpler distribution. The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.|$|R
40|$|The thesis {{presents}} {{the field of}} external sorting. In the thesis we describe and compare multiple sorting algorithms for external sorting based on their behavior, their advantages and disadvantages. The algorithms we compare are the straight multiway merge sort, balanced multiway merge sort, natural multiway merge <b>sort,</b> <b>polyphase</b> merge <b>sort,</b> cascade sort, distribution sort, funnel sort and two pre-sorting algorithms. The purpose of the thesis is to describe and present how the algorithms work in theory and in practice. We implemented the algorithms in the C programming language and then experimentally compared them on a personal computer with one external storage device...|$|R
25|$|Fibonacci {{numbers are}} used in a {{polyphase}} version of the merge sort algorithm in which an unsorted list {{is divided into two}} lists whose lengths correspond to sequential Fibonacci numbers– by dividing the list so that the two parts have lengths in the approximate proportion φ. A tape-drive implementation of the <b>polyphase</b> merge <b>sort</b> was described in The Art of Computer Programming.|$|R
5000|$|In practice, {{the input}} file {{will not have}} the exact number of runs needed for a perfect distribution. The <b>polyphase</b> merge <b>sort</b> fixes this {{discrepancy}} by padding the actual distribution with imaginary [...] "dummy runs" [...] to simulate an ideal run distribution. A dummy run behaves like a run with no records in it. Merging one or more dummy runs with one or more real runs just merges the real runs.|$|R
50|$|For N {{less than}} 8 working files, a <b>polyphase</b> merge <b>sort</b> {{achieves}} a higher effective run count reduction factor by unevenly distributing sorted runs between N-1 working files (explained in next section). Each iteration merges runs from N-1 working files onto a single output working file. When {{the end of}} one of the N-1 working files is reached, then it becomes the new output file and what was the output file becomes one of the N-1 working input files, starting a new iteration of <b>polyphase</b> merge <b>sort.</b> Each iteration merges {{only a fraction of the}} dataset (about 1/2 to 3/4), except for the last iteration which merges all of the dataset into a single sorted run. The initial distribution is set up so that only one input working file is emptied at a time, except for the final merge iteration which merges N-1 single runs (of varying size, this is explained next) from the N-1 input working files to the single output file, resulting in a single sorted run, the sorted dataset.|$|R
50|$|For {{everything}} {{to work out}} optimally, the last merge phase should have exactly one run on each input file. If any input file {{has more than one}} run, then another phase would be required. Consequently, the <b>polyphase</b> merge <b>sort</b> needs to be clever about the initial distribution of the input data's runs to the initial output files. For example, an input file with 13 runs would write 5 runs to file 1 and 8 runs to file 2.|$|R
5000|$|Using the run move count {{equation}} for {{the above}} examples: ordinary merge sort -> 16 x log2(16) = 64, <b>polyphase</b> merge <b>sort</b> -> 17 x log2.73(17) = 48. Here is {{a table of}} effective reduction factors for polyphase and ordinary merge sort listed by number of files, based on actual sorts of a few million records. This table roughly corresponds to the reduction factor per dataset moved tables shown in fig 3 and fig 4 of polyphase merge sort.pdf ...|$|R
50|$|After {{the initial}} distribution, an {{ordinary}} merge sort using 4 files will sort 16 single record runs in 4 iterations {{of the entire}} dataset, moving a total of 64 records in order to sort the dataset after the initial distribution. A <b>polyphase</b> merge <b>sort</b> using 4 files will sort 17 single record runs in 4 iterations, but since each iteration but the last iteration only moves {{a fraction of the}} dataset, it only moves a total of 48 records in order to sort the dataset after the initial distribution. In this case, ordinary merge sort factor is 2.0, while polyphase overall factor is ~2.73.|$|R
50|$|For each {{polyphase}} iteration, {{the total}} number of runs follows a pattern similar to a reversed Fibonacci numbers of higher order sequence. With 4 files, and a dataset consisting of 57 runs, the total run count on each iteration would be 57, 31, 17, 9, 5, 3, 1. Note that except for the last iteration, the run count reduction factor is a bit less than 2, 57/31, 31/17, 17/9, 9/5, 5/3, 3/1, about 1.84 for a 4 file case, but each iteration except the last reduced the run count while processing about 65% of the dataset, so the run count reduction factor per dataset processed during the intermediate iterations is about 1.84 / 0.65 = 2.83. For a dataset consisting of 57 runs of 1 record each, then after the initial distribution, <b>polyphase</b> merge <b>sort</b> moves 232 records during the 6 iterations it takes to sort the dataset, for an overall reduction factor of 2.70 (this is explained in more detail later).|$|R

