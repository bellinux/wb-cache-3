15|712|Public
40|$|The {{research}} of Gene Predicting Algorithms {{is a key}} section in bioinformatics. After brief introduction to bioinformatics, we introduce a new and improved Gene <b>Predicting</b> <b>Algorithm,</b> together with a model to give an analytical explanation of its meaning. DNA sequences are formed by patches or domains of different nucleotide composition, for which the Jensen Shannon divergence method is often employed to find the boundaries for these compositional domains. By introducing one new parameter α into Jensen Shannon divergence, we find numerically the optimal value to obtain the best accuracy of border finding. We explain this result mathematically, and give the exact expression for this parameter. We then apply this improved Jensen Shannon divergence to artificial sequences and some real DNA sequences. The results demonstrate that this parameter is useful for segmentation of genomic sequences into compositionally homogeneous segments...|$|E
40|$|This course {{project for}} CPSC 540 ”Probabilistic Machine Learning ” {{is based on}} the idea of {{predicting}} folder locations on a computer for files which are down-loaded with an Internet browser. This idea was originally developed for a course project in CPSC 532 C ”User Adaptive Interaction”. While we evaluated the do-main of browser downloads, designed an user interface, developed an interac-tion design and realized a client-server software for the 532 C project, this project solely focuses on the implementation and evaluation of classification algorithms for the task of folder prediction. Therefore, a dataset containing the data of 158 downloaded files was created. The classification algorithms K-Nearest-Neighbor and Logistic Regression were implemented, optimized and compared in several variants in order to find the best folder <b>predicting</b> <b>algorithm.</b> We evaluated the al-gorithms by running leave-one-out cross-validation on the dataset and analyzing the online performance on random sequences of data items. ...|$|E
30|$|As the {{temporal}} profiles of distribution {{networks in the}} future are needed in the proposed temporal scheduling strategy, the outputs of DGs, the demand of baseload, and RTP should be predicted day-ahead, and the <b>predicting</b> <b>algorithm</b> is available in [21]. Moreover, {{the temporal}} PEV normal charging demand {{in the future is}} also needed to be estimated, which can be obtained from the modeling approach in [19, 20] Once the normal charging behaviors in the current time slot of all PEVs are determined by the strategy, the SoC distribution (i.e. the expected charging duration) of PEVs with normal charging demand in the next time slot can be updated, and the real-time output of DGs, the baseload profiles and the RTP can also be detected to replace the predicted data. Thus, the temporal scheduling strategy will repeat in each time slot, which can ensure the real-time performance and the accuracy of the scheduling strategy.|$|E
40|$|We {{present an}} {{empirical}} hardness model for nurse rostering by explicitly building on previous developed models for SAT problems. The resulting model allows to <b>predict</b> <b>algorithm</b> performance based on {{features of the}} problem instances in a specific distribution for specific performance criteria. In doing so we demonstrate that the performance of an algorithm on an instance of a real world problem can be estimated from {{a limited number of}} indicators. We speculate that a similar approach may be valid in other application domains. status: publishe...|$|R
5000|$|... #Subtitle level 3: Regression <b>algorithms</b> (<b>predicting</b> real-valued labels) ...|$|R
5000|$|... #Subtitle level 3: Classification <b>algorithms</b> (supervised <b>algorithms</b> <b>predicting</b> {{categorical}} labels) ...|$|R
40|$|International audienceThe fractal {{behaviour}} of the seismicity in the Southern Iberian Peninsula is analysed {{by considering}} two different series of data: {{the distance and}} the elapsed time between consecutive seismic events recorded by the seismic network of the Andalusian Institute of Geophysics (AIG). The fractal analyses have been repeated by considering four threshold magnitudes of 2. 5, 3. 0, 3. 5 and 4. 0. The re-scaled analysis lets {{to determine if the}} seismicity shows strong randomness or if it is characterised by time-persistence and the cluster dimension indicates the degree of time and spatial clustering of the seismicity. Another analysis, based on the reconstruction theorem, permits to evaluate the minimum number of nonlinear equations describing the dynamical mechanism of the seismicity, its "loss of memory", its chaotic character and the instability of a possible <b>predicting</b> <b>algorithm.</b> The results obtained depict some differences depending on distances or elapsed times and the different threshold levels of magnitude also lead to slightly different results. Additionally, only a part of the fractal tools, the re-scaled analysis, have been applied to five seismic crises in the same area...|$|E
40|$|Dynamic {{selection}} and dynamic binding and rebinding at runtime are new characters of composite services. The traditional static reliability prediction models are unsuitable to dynamic composite services. A new reliability <b>predicting</b> <b>algorithm</b> for composite services is proposed in this paper. Firstly, a composite service is decomposed into composition unites (executing path, composite module and atomic service) {{according to their}} constituents. Consequently, a hierarchical graph of all composite units is constructed. Lastly, a new dynamic reliability prediction algorithm is presented. Comparing with the traditional reliability model, the new dynamic reliability approach is more flexible, which does not recompute reliability for all composite units and only computes {{the reliability of the}} effected composite units. In addition, an example to show how to measure the reliability based on our algorithm is designed. The experimental results show our proposed methods can give an accurate estimation of reliability. Furthermore, a more flexible sensitivity analysis is performed to determine which service component has the most significant impact on the improvement of composite service reliability...|$|E
40|$|This paper {{examines}} {{important factors}} for link prediction in networks {{and provides a}} general, high-performance framework for the prediction task. Link prediction in sparse networks presents a significant challenge due to the inherent disproportion of links that can form to links that do form. Previous research has typically approached this as an unsupervised problem. While {{this is not the}} first work to explore supervised learning, many factors significant in influencing and guiding classification remain unexplored. In this paper, we consider these factors by first motivating the use of a supervised framework through a careful investigation of issues such as network observationalperiod, generality of existing methods, variance reduction, topological causes and degrees of imbalance, and sampling approaches. We also present an effective flow-based <b>predicting</b> <b>algorithm,</b> offer formal bounds on imbalance in sparse network link prediction, and employ an evaluation method appropriate for the observed imbalance. Our careful consideration of the above issues ultimately leads to a completely general framework that outperforms unsupervised link prediction methods by more than 30 % AUC...|$|E
5000|$|... #Subtitle level 3: Clustering <b>algorithms</b> (unsupervised <b>algorithms</b> <b>predicting</b> {{categorical}} labels) ...|$|R
40|$|The paper {{considers}} causal smoothing of {{the real}} sequences, i. e.,discrete time processes in a deterministic setting. A family of causal linear time-invariant filters is suggested. These filters approximate the gain decay for some non-causal smoothing filters with transfer functions vanishing at a point of the unit circle and such that they transfer processes into predictable ones. In this sense, the suggested filters are near-ideal; a faster gain decay {{would lead to the}} loss of causality. Applications to <b>predicting</b> <b>algorithms</b> are discussed and illustrated by experiments with forecasting of autoregressions with the coefficients that are deemed to be untraceable...|$|R
5000|$|... #Subtitle level 3: Sequence {{labeling}} <b>algorithms</b> (<b>predicting</b> {{sequences of}} categorical labels) ...|$|R
40|$|The fractal {{behaviour}} of the seismicity in the Southern Iberian Peninsula is analysed {{by considering}} two different series of data: {{the distance and}} the elapsed time between consecutive seismic events recorded by the seismic network of the Andalusian Institute of Geophysics (AIG). The fractal analyses have been repeated by considering four threshold magnitudes of 2. 5, 3. 0, 3. 5 and 4. 0. The re-scaled analysis lets {{to determine if the}} seismicity shows strong randomness or if it is characterised by time-persistence and the cluster dimension indicates the degree of time and spatial clustering of the seismicity. Another analysis, based on the reconstruction theorem, permits to evaluate the minimum number of nonlinear equations describing the dynamical mechanism of the seismicity, its 'loss of memory', its chaotic character and the instability of a possible <b>predicting</b> <b>algorithm.</b> The results obtained depict some differences depending on distances or elapsed times and the different threshold levels of magnitude also lead to slightly different results. Additionally, only a part of the fractal tools, the re-scaled analysis, have been applied to five seismic crises in the same area...|$|E
40|$|We {{aimed to}} compare the Finapres system, which is {{designed}} for accurate intra-arterial amplitude measurement, to the Caretaker system, which is designed for temporal accuracy of intra-arterial measurement, in regard to measurement of pulse transit time (PTT) at baseline and following an endurance exercise session. Pulse transit time was evaluated between the R-wave of the ECG and {{the foot of the}} arterial waveform using either the Finapres (fpPTT) or Caretaker (ctPTT). 23 participants were measured before and after completion of endurance exercise. When comparing PTT values before and after an exercise intervention within devices, ctPTT was significantly different following exercise (P= 0. 03); however, the Finapres obtained values did not differ significantly. Before exercise, there was no significant relationship between devices, however, after exercise a significant moderate correlation was observed (r= 0. 45, P= 0. 02). Significant differences existed between ctPTT and fpPTT (P< 0. 001). The Caretaker system appears to be more accurate at detecting changes in PTT occurring {{as a result of a}} single aerobic exercise session. This may be due to the servo-controller feedback loop in the waveform contour <b>predicting</b> <b>algorithm</b> within the Finapres system, which is not present in the Caretaker unit. The Finapres system also appears to have an inherent delay in pulse contour reporting...|$|E
40|$|The {{research}} of Gene Predicting Algorithms {{is a key}} section in bioinformatics. After brief introduction to bioinformatics, we introduce a new and improved Gene <b>Predicting</b> <b>Algorithm,</b> together with a model to give a physical explanation of its meaning. We first use our Improved Entropic Segmentation Algorithm to locate the borders between the Coding and Noncoding regions in DNA sequence. Then, I introduce a method for parameter determination by the continuous condition on random sequence. To give a physical explanation of this algorithm, I built an Ising model of DNA sequence and deduced the exact solution of this model, and as the extension of this model, I gave the exact solution of Potts Model of DNA sequence. To gain more insight into this model, I performed a Monte Carlo simulation of these DNA sequence, and compared the Shannon-Jenson divergence using physical entropy and that using information entropy. The results are very similar. We therefore conclude {{that they may be}} used interchangeably. It shows that our model can give a preliminary explanation of this algorithm. As the other part of my research, I have advanced two algorithms of Point Recognition of DNA microarray figure, with satisfactory results. This is cooperation with Dr. Tam, a professor of biochemistry department of Hong Kong University...|$|E
5000|$|... #Subtitle level 3: Real-valued {{sequence}} labeling <b>algorithms</b> (<b>predicting</b> {{sequences of}} real-valued labels) ...|$|R
5000|$|... #Subtitle level 3: Multilinear {{subspace}} learning <b>algorithms</b> (<b>predicting</b> labels of multidimensional data using tensor representations) ...|$|R
40|$|There are {{a number}} of {{similarities}} and differences between FutureLearn MOOCs and those offered by other platforms, such as edX. In this research we compare the results of applying machine learning <b>algorithms</b> to <b>predict</b> course attrition for two case studies using datasets from a selected FutureLearn MOOC and an edX MOOC of comparable structure and themes. For each we have computed a number of attributes in a pre-processing stage from the raw data available in each course. Following this, we applied several machine learning algorithms on the pre-processed data to predict attrition levels for each course. The analysis suggests that the attribute selection varies in each scenario, which also impacts on the behaviour of the <b>predicting</b> <b>algorithms...</b>|$|R
40|$|Perhaps surprisingly, it is {{possible}} to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm’s runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and— perhaps most importantly—a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for <b>predicting</b> <b>algorithm</b> runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing {{to a wide range of}} runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously...|$|E
40|$|Abstract—Noise {{models are}} crucial for {{designing}} image restoration algorithms, generating synthetic training data, and <b>predicting</b> <b>algorithm</b> performance. There are two related but distinct estimation scenarios. The first is model calibration, {{where it is}} assumed that the input ideal bitmap and the output of the degradation process are both known. The second is the general estimation problem, where only the image from the output of the degradation process is given. While researchers have addressed the problem of calibration of models, issues with the general estimation problems have not been addressed in the literature. In this paper, we describe a parameter estimation algorithm for a morphological, binary, page-level image degradation model. The inputs to the estimation algorithm are 1) the degraded image and 2) information regarding the font type (italic, bold, serif, sans serif). We simulate degraded images using our model and search for the optimal parameter by looking for a parameter value for which the local neighborhood pattern distributions in the simulated image and the given degraded image are most similar. The parameter space is searched using a direct search optimization algorithm. We use the p-value of the Kolmogorov-Smirnov test as the measure of similarity between the two neighborhood pattern distributions. We show results of our algorithm on degraded document images. Index Terms—Degradation models, parameter estimation, direct search algorithms, neighborhood pattern distributions. ...|$|E
40|$|Recent {{increases}} in CPU performance have outpaced in-creases in hard drive performance. As a result, disk op-erations {{have become more}} expensive in terms of CPU cycles spent waiting for disk operations to complete. File prediction can mitigate this problem by prefetching les into cache before they are accessed. However, incorrect prediction is {{to a certain degree}} both unavoidable and costly. Battery is a valuable resource in a mobile com-puting environment. The utility of mobile computers is greatly affected the battery life. Incorrect prediction not only wastes cache space, also consumes battery energy. Consequently, incorrect prediction is more expensive to mobile computers than its counterpart to desktop comput-ers. Last Successor (LS) is a commonly-used <b>predicting</b> <b>algorithm</b> in practice. We present the Program-based Last Successor (PLS) le prediction model that identies rela-tionships between les through the names of the programs accessing them. Our simulation results show that PLS makes 21 % fewer incorrect predictions and roughly the same number of correct predictions as the last-successor model. Hence, the amount of battery energy wasted on incorrect prediction could be greatly reduced. We nally examine the cache hit ratio of applying PLS to the Least Recently Used (LRU) caching algorithm and show that a cache using PLS and LRU together can perform better than a cache up to 40 times larger using LRU alone. This shows that saving battery energy and increasing perfor-mance can be reached at the same time. ...|$|E
40|$|Abstract. Steady state {{contingency}} analysis aims at {{the assessment}} of the risk certain contingencies may pose to an electrical network. This is a particularly important task of network operators, especially as network stability issues become of prime importance in the current era of electricity deregulation. The article focuses on the analysis of experimental data that are produced through operating point simulation, contingency application, machine- learning cross validation (based on pre-contingency network index selection algorithms) to point out the “nature ” of given contingencies. Experimental statistical results of contingency prediction and selected network state indicators are translated to electric network data in an effort to further interpret the “nature ” of each contingency and produce effective <b>predicting</b> <b>algorithms</b> that support operators...|$|R
40|$|Abstract. Black-box {{optimization}} {{problems are}} of practical importance throughout science and engineering. Hundreds of algorithms and heuris-tics {{have been developed}} to solve them. However, none of them outper-forms any other on all problems. The success of a particular heuristic is always relative to a class of problems. So far, these problem classes are elusive and it is not known what algorithm to use on a given prob-lem. Here we describe the use of Formal Concept Analysis (FCA) to extract implications about problem classes and algorithm performance from databases of empirical benchmarks. We explain the idea in a small example and show that FCA produces meaningful implications. We fur-ther outline the use of attribute exploration to identify problem features that <b>predict</b> <b>algorithm</b> performance. ...|$|R
40|$|Endometriosis is {{determined}} by genetic factors, and the prevalence of genetic polymorphisms varies greatly depending on the ethnic group studied. The objective {{of this study was}} to investigate the relationship between single nucleotide polymorphisms (SNPs) of 9 genes involved in estrogen biosynthesis and metabolism and the risks of endometriosis. Three hundred patients with endometriosis and 337 non-endometriotic controls were recruited. Thirty four non-synonymous SNPs, which change amino acid residues, were analyzed using matrix-assisted laser desorption-ionization time-of-flight mass spectrometry (MALDI-TOF MS). The functions of SNP-resulted amino acid changes were analyzed using multiple web-accessible databases and phosphorylation <b>predicting</b> <b>algorithms.</b> Among the 34 NCBI-listed SNPs, 22 did not exhibit polymorphism in this study of more than 600 Taiwanese Chinese women. However, homozygous and heterozygou...|$|R
40|$|Despite growing interest, basic {{information}} on methods and models for mathematically analyzing algorithms {{has rarely been}} directly accessible to practitioners, researchers, or students. An Introduction to the Analysis of Algorithms, Second Edition, organizes and presents that knowledge, fully introducing primary techniques and results in the field. Robert Sedgewick and the late Philippe Flajolet have drawn from both classical mathematics and computer science, integrating discrete mathematics, elementary real analysis, combinatorics, algorithms, and data structures. They emphasize the mathematics needed to support scientific studies that can {{serve as the basis}} for <b>predicting</b> <b>algorithm</b> performance and for comparing different algorithms on the basis of performance. Techniques covered in the first half of the book include recurrences, generating functions, asymptotics, and analytic combinatorics. Structures studied in the second half of the book include permutations, trees, strings, tries, and mappings. Numerous examples are included throughout to illustrate applications to the analysis of algorithms that are playing a critical role in the evolution of our modern computational infrastructure. Improvements and additions in this new edition include * Upgraded figures and code * An all-new chapter introducing analytic combinatorics * Simplified derivations via analytic combinatorics throughout The book's thorough, self-contained coverage will help readers appreciate the field's challenges, prepare them for advanced results-covered in their monograph Analytic Combinatorics and in Donald Knuth's The Art of Computer Programming books-and provide the background they need to keep abreast of new research...|$|E
40|$|Noise {{models are}} crucial for {{designing}} image restoration algorithms, generating synthetic training data, and <b>predicting</b> <b>algorithm</b> performance. However, to accomplish {{any of these}} tasks, {{an estimate of the}} degradation model parameters is essential. In this paper we describe a parameter estimation algorithm for a morphological, binary image degradation model. The inputs to the estimation algorithm are i) the degraded image, and ii) information regarding the font type (italic, bold, serif, sans serif). We simulate degraded images and search for the optimal parameter by looking for a parameter value for which the neighborhood pattern distributions in the simulated image and the given degraded image are most similar. The parameter space is searched using the Nelder-Mead downhill simplex algorithm. We use the p-value of the Kolmogorov-Smirnov test for the measure of similarity between the two neighborhood pattern distributions. We show results of our algorithm on degraded document images. This research was funded in part by the Science Application International Corporation under Contract 4400019848, the Defense Advanced Research Projects Agency under Contract N 660010028910, and the National Science Foundation under Grant IIS 9987944. LAMP-TR- 066 CAR-TR- 963 CS-TR- 4219 4400019848 N 660010028910 /IIS 9987944 February 2001 A Downhill Simplex Algorithm for Estimating Morphological Degradation Model Parameters Tapas Kanungo and Qigong Zheng A Downhill Simplex Algorithm for Estimating Morphological Degradation Model Parameters Tapas Kanungo and Qigong Zheng Center for Automation Research University of Maryland College Park, MD 20742 - 3275 fkanungo,qzhengg@cfar. umd. edu Abstract Noise models are crucial for designing image restoration algorithms, generating synthetic tra [...] ...|$|E
5000|$|DPCM encodes the PCM {{values as}} {{differences}} between the current and the <b>predicted</b> value. An <b>algorithm</b> <b>predicts</b> the next sample based on the previous samples, and the encoder stores only the difference between this prediction and the actual value. If the prediction is reasonable, fewer bits {{can be used to}} represent the same information. For audio, this type of encoding reduces the number of bits required per sample by about 25% compared to PCM.|$|R
40|$|This paper tackles the {{difficult}} but important task of objective algorithm performance assessment for optimization. Rather than reporting average performance of algorithms across {{a set of}} chosen instances, which may bias conclusions, we propose a methodology to enable {{the strengths and weaknesses}} of different optimization algorithms to be compared across a broader instance space. The results reported in a recent Computers and Operations Research paper comparing the performance of graph coloring heuristics are revisited with this new methodology to demonstrate (i) how pockets of the instance space can be found where algorithm performance varies significantly from the average performance of an algorithm; (ii) how the properties of the instances can be used to <b>predict</b> <b>algorithm</b> performance on previously unseen instances with high accuracy; and (iii) how the relative strengths and weaknesses of each algorithm can be visualized and measured objectively...|$|R
40|$|In this {{abstract}} we give {{an overview}} of the work described in [15]. Belief networks provide a graphical representation of causal relationships together with a mechanism for probabilistic inference, allowing belief updating based on incomplete and dynamic information. We present a survey of Belief Network belief updating algorithms and propose a domain characterisation system which is used as a basis for algorithm comparison. We give experimental comparative results of algorithm performance using the proposed framework. We show how domain characterisation may be used to <b>predict</b> <b>algorithm</b> performance. Introduction Belief networks are directed acyclic graphs, where nodes correspond to random variables, which are usually assumed to take discrete values. The relationship between any set of state variables can be specified by a joint probability distribution. The nodes in the network are connected by directed arcs, which may be thought of as causal or influence links. The connections also [...] ...|$|R
40|$|Recently, Massive Open Online Courses (MOOCs) have {{experienced}} rapid development. However, {{one of the}} major issues of online education is the high dropout rates of participants. Many studies have attempted to explore this issue, using quantitative and qualitative methods for student attrition analysis. Nevertheless, {{there is a lack of}} studies which 1) predict the actual moment of dropout, providing opportunities to enhance MOOCs’ student retention by offering timely interventions; and 2) compare the performance of such <b>predicting</b> <b>algorithms.</b> In this paper, we aim to predict student drop out in MOOCs using process and sequence mining techniques, and provide a comparative analysis of these techniques. We perform a case study based on the data from KU Leuven online course in e-Psychology", available on the edX platform. The results reveal, that while process mining is better capable to perform descriptive analysis, sequence mining techniques provide better features for predictive purposes. status: publishe...|$|R
50|$|C3orf62 is {{predicted}} to be localized in the nucleus. The k-nearest neighbors <b>algorithm</b> <b>predicts</b> C3orf62 to be classified as follows: k=9/23; 69.6% nuclear, 13.0% mitochondrial, 13.0% cytoskeletal, 4.3% cytoplasmic.|$|R
50|$|The protein {{secondary}} structure can be <b>predicted</b> using <b>algorithms</b> to <b>predict</b> {{the occurrence of}} alpha helices and beta sheets within the protein. An analysis of the protein structure was conducted using the CHOFAS, GOR4, and PELE algorithms in the SDSC Biology Workbench. The analyses were combined and included in the adjacent diagram. Only structures that appeared {{in more than one}} output were included.|$|R
40|$|We {{present a}} new {{class-based}} prediction algorithm for time series. Given time series produced by di#erent underlying generating processes, the <b>algorithm</b> <b>predicts</b> future time series values based on past time series values for each generator. Unlike many <b>algorithms,</b> this <b>algorithm</b> <b>predicts</b> a distribution over future values. This prediction {{forms the basis}} for labelling part of a time series with the underlying generator that created it given some labelled examples. The algorithm is robust {{to a wide variety}} of possible types of changes in signals including mean shifts, amplitude changes, noise changes, period changes, and changes in signal shape...|$|R
40|$|Like its linear counterpart, the Kernel Least Mean Square (KLMS) {{algorithm}} is also becoming popular in nonlinear adaptive filtering {{due to its}} simplicity and robustness. The “kernelization ” of the linear adaptive filters modifies the statistics of the input signals, which now depends on {{the parameters of the}} used kernel. A Gaussian KLMS has two design parameters; the step size and the kernel bandwidth. Thus, new analytical models are required to <b>predict</b> the kernel-based <b>algorithm</b> behavior {{as a function of the}} design parameters. This paper studies the stochastic behavior of the Gaussian KLMS algorithm for white Gaussian input signals. The resulting model accurately <b>predicts</b> the <b>algorithm</b> behavior and can be used for choosing the algorithm parameters in order to achieve a prescribed performance. Index Terms — Adaptive filtering, KLMS, convergence analysis, nonlinear system, reproducing kerne...|$|R
40|$|A {{series of}} {{synthetic}} receptors capable of binding to the calmodulin-binding domain of calcineurin (CN 393 – 414) was designed, synthesized and characterized. The design {{was accomplished by}} docking CN 393 – 414 against a two-helix receptor, using an idealized three-stranded coiled coil as a starting geometry. The sequence of the receptor was chosen using a side-chain re-packing program, which employed a genetic algorithm to select potential binders from a total of 7. 5 × 106 possible sequences. A total of 25 receptors were prepared, representing 13 sequences <b>predicted</b> by the <b>algorithm</b> as well as 12 related sequences that were not predicted. The receptors were characterized by CD spectroscopy, analytical ultracentrifugation, and binding assays. The receptors <b>predicted</b> by the <b>algorithm</b> bound CN 393 – 414 with apparent dissociation constants ranging from 0. 2 μM to > 50 μM. Many of the receptors that were not <b>predicted</b> by the <b>algorithm</b> also bound to CN 393 – 414. Methods to circumvent this problem and to improve the automated design of functional proteins are discussed...|$|R
40|$|This {{work has}} been funded by the Web Science Institute Pump-priming 2015 / 16 Project “The MOOC Observatory Dashboard: Management, {{analysis}} and visualisation of MOOC data ”. Additionally, Ruth Cobos’ contribution has been partially funded by the Madrid Regional Government with grant No. S 2013 /ICE - 2715, the Spanish Ministry of Economy and Competitiveness project "Flexor" (TIN 2014 - 52129 - R). There {{are a number of}} similarities and differences between FutureLearn MOOCs and those offered by other platforms, such as edX. In this research we compare the results of applying machine learning <b>algorithms</b> to <b>predict</b> course attrition for two case studies using datasets from a selected FutureLearn MOOC and an edX MOOC of comparable structure and themes. For each we have computed a number of attributes in a pre-processing stage from the raw data available in each course. Following this, we applied several machine learning algorithms on the pre-processed data to predict attrition levels for each course. The analysis suggests that the attribute selection varies in each scenario, which also impacts on the behaviour of the <b>predicting</b> <b>algorithms.</b> PostprintPeer reviewe...|$|R
