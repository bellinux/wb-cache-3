55|107|Public
5000|$|Székely, G.J. and Rizzo, M.L. (2014) <b>Partial</b> <b>distance</b> {{correlation}} with methods for dissimilarities, The Annals of Statistics, 42/6, 2382-2412.|$|E
5000|$|Without any {{restrictions}} {{on the number of}} degrees of freedom in the completed matrix this problem is underdetermined since the hidden entries could be assigned arbitrary values. Thus matrix completion often seeks to find the lowest rank matrix or, if the rank of the completed matrix is known, a matrix of rank [...] that matches the known entries. The illustration shows that a partially revealed rank-1 matrix (on the left) can be completed with zero-error (on the right) since all the rows with missing entries should be the same as the third row. In the case of the Netflix problem the ratings matrix is expected to be low-rank since user preferences can often be described by a few factors, such as the movie genre and time of release. Other applications include computer vision, where missing pixels in images need to be reconstructed, detecting the global positioning of sensors in a network from <b>partial</b> <b>distance</b> information, and multiclass learning. The matrix completion problem is in general NP-hard, but there are tractable algorithms that achieve exact reconstruction with high probability.|$|E
40|$|Abstract. Distance {{covariance}} {{and distance}} correlation are scalar coefficients that characterize independence of random vectors in arbitrary dimension. Prop-erties, extensions, and applications of distance correlation {{have been discussed}} in the recent literature, but the problem of defining the <b>partial</b> <b>distance</b> cor-relation has remained an open question of considerable interest. The problem of <b>partial</b> <b>distance</b> correlation {{is more complex than}} partial correlation partly because the squared distance covariance is not an inner product in the usual linear space. For the definition of <b>partial</b> <b>distance</b> correlation we introduce a new Hilbert space where the squared distance covariance is the inner prod-uct. We define the <b>partial</b> <b>distance</b> correlation statistics with the help of this Hilbert space, and develop and implement a test for zero <b>partial</b> <b>distance</b> cor-relation. Our intermediate results provide an unbiased estimator of squared distance covariance, and a neat {{solution to the problem of}} distance correlation for dissimilarities rather than distances...|$|E
30|$|When all {{the threads}} within a thread block {{complete}} the calculating of <b>partial</b> <b>distances</b> and weights, the subsequent wave of points’ coordinates {{is transferred from}} the global memory to the shared memory again. This new piece of data is employed to compute the current wave of <b>partial</b> <b>distances</b> and corresponding weights.|$|R
5000|$|Bivariate statistics: Means, t-test, ANOVA, Correlation (bivariate, <b>partial,</b> <b>distances),</b> Nonparametric tests ...|$|R
40|$|This paper {{presents}} a novel distance concept, Vector-Distance (VD) for high dimensional data. VD extends traditional scalar-distance to a vector-like fashion by collecting multi <b>partial</b> <b>distances</b> from diverse angles. These <b>partial</b> <b>distances</b> {{are derived from}} random projections, and they preserve individual features of dimensions as much as possible. Based on VD definition, a method family for neighborhood development is proposed, where methods consist of some norm definitions and certain constrains specified for various purposes. Experiments on real datasets verify the quality of neighborhoods produced by the proposed method family better or competitive with the neighborhood produced {{by the state of}} the art. </p...|$|R
40|$|<b>Partial</b> <b>distance</b> {{correlation}} measures {{association between}} two random vectors {{with respect to}} a third random vector, analogous to, but more general than (linear) partial correlation. Distance correlation characterizes independence of random vectors in arbitrary dimension. Motivation for the definition is discussed. We introduce a Hilbert space of U-centered distance matrices in which squared distance covariance is the inner product. Simple computation of the sample <b>partial</b> <b>distance</b> correlation and definitions of the population coefficients are presented. Power of the test for zero <b>partial</b> <b>distance</b> correlation is compared with power of the partial correlation test and the partial Mantel test. © Springer International Publishing Switzerland 2016...|$|E
3000|$|... (line 8 in Algorithm 3). The <b>partial</b> <b>distance</b> τ 1 is {{calculated}} (line 9 in Algorithm 3). The search is terminated in two cases: (a) if this <b>partial</b> <b>distance</b> {{is greater than}} the current minimum branch distance (τ 1 >p 1) and (b) if the overall distance is beyond the current radius of the sphere decoder in the tree search phase ((τ 1 +d 2 +d 3 +d 4 +d)>radius).|$|E
40|$|In this paper, {{we present}} a class of {{algorithms}} that use a <b>partial</b> <b>distance</b> metric to speedup the motion estimation process. The <b>partial</b> <b>distance</b> metric is used within the motion search to eliminate unlikely candidates through a thresholding process that enables computation scalability. We also propose a multiresolution variant which further reduces the complexity. Our results are comparable to {{state of the art}} approaches present a regular structure and are computationally scalable...|$|E
30|$|More specifically, each thread invoked {{to first}} load the {{coordinates}} of one data point from global memory to shared memory, and then compute the distances and inverse weights to those data points stored in current shared memory. After {{the completion of}} calculating these <b>partial</b> <b>distances</b> and weights, next “tile” of data residing in global memory is newly loaded to shared memory and then accepted to compute current round of <b>partial</b> <b>distances</b> and weights. Each thread accumulates the result of all partial weights and all weighted values into two registers. Finally, the desired interpolation value of each prediction point can be obtained according to the sums of all partial weights and weighted values.|$|R
30|$|When {{finishing}} {{the calculation of}} all waves of <b>partial</b> <b>distances</b> and weights, each thread is invoked to accumulate all the weights and weighted values into two registers. At last, the desired prediction value of an unknown point, i.e., the weighted average, is computed and then written to the global memory.|$|R
40|$|In this paper, we have {{present the}} face {{recognition}} method based on <b>partial</b> Hausdorff <b>distance.</b> Normally face recognition algorithm gives poor results against pose and illumination variation. But the algorithm we have presented is robust to those conditions. We have applied transformation on face image which is robust todifferent face pose and illumination variations. Then the <b>partial</b> Hausdorff <b>distance</b> is calculated for matching {{after that the}} performance of face recognition is evaluated on different database...|$|R
40|$|This {{dissertation}} {{is dedicated}} to study the modeling of drift control in foreign exchange reserves management and design the fast algorithm of statistical inference with its application in high dimensional data analysis. The thesis has two parts. The first topic involves the modeling of foreign exchange reserve management as an drift control problem. We show that, under certain conditions, the control band policies are optimal for the discounted cost drift control problem and develop an algorithm to calculate the optimal thresholds of the optimal control band policy. The second topic involves the fast computing algorithm of <b>partial</b> <b>distance</b> covariance statistics with its application in feature screening in high dimensional data. We show that an O(n log n) algorithm for {{a version of the}} <b>partial</b> <b>distance</b> covariance exists, compared with the O(n^ 2) algorithm implemented directly accordingly to its definition. We further propose an iterative feature screening procedure in high dimensional data based on the <b>partial</b> <b>distance</b> covariance. This procedure enjoys two advantages over the correlation learning. First, an important predictor that is marginally uncorrelated but jointly correlated with the response can be picked by our procedure and thus entering the estimation model. Second, our procedure is robust to model mis- specification. Ph. D...|$|E
40|$|Motion {{search is}} by far the most complex {{operation}} to be performed in a video encoder. This complexity stems from the need to compute a matching metric for a potentially large number of candidate motion vectors, with the objective being to nd the candidate with the smallest metric. Most work on fast motion estimation algorithms has focused on reducing the number of candidate motion vectors that have to be tested. Instead, this paper proposes a novel fast matching algorithm to help speed-up the computation of the matching (distance) metric used in the search, e. g. the sum of absolute dierence (SAD). Based on a <b>partial</b> <b>distance</b> technique, our algorithm reduces complexity by terminating the SAD calculation early (i. e. when the SAD has only been partially computed) once it becomes clear that, given the partial SAD, it is likely that the total SAD will exceed that of the best candidate encountered so far in the search. The key idea is to introduce models to describe the probability distribution of the total distance given a measured <b>partial</b> <b>distance.</b> These models enable us to evaluate the risk involved in " a distance estimate obtained from a <b>partial</b> <b>distance.</b> By varying the amount of risk we are willing to take, we can increase the speed, but we may also eliminate some good candidates to...|$|E
3000|$|... [...]. For the node under checking, the <b>partial</b> <b>distance</b> {{resulted}} by {{the current}} path is compared with the radius. If the <b>partial</b> <b>distance</b> is smaller than the radius, the search moves on to the children nodes on the next level. Otherwise, the search jumps to another sibling node on the current level. When all the nodes of the level have already been checked, the search {{goes back to the}} upper level. The radius is initially set to infinity and is adaptively decreased according to the best solution already found in the search. Specifically, the radius is updated, taking into account the best combination of [c,d], and [a,b] (line 16 of Algorithm 2). The latter is obtained from the parallel decisions phase. The tree search is terminated when all the nodes within the hypersphere have been checked. The best solution is the ML solution.|$|E
40|$|Curve {{matching}} is {{a fundamental}} problem which occurs in many applications. In this paper, we extend the Fréchet <b>distance</b> to measure <b>partial</b> curve similarity, study its properties, and develop e cient algorithms to compute it. In particular, under the L 1 or L ∞ metrics, we present an algorithm to compute, in O(m 2) time, the <b>partial</b> Fréchet <b>distance</b> between a segment and a polygonal curve of size m. We then develop an approximation algorithm that, in O((n + m) 3 /ε 2) time, computes a (1 − ε) approximation to the optimal <b>partial</b> Fréchet <b>distance</b> between two polygonal curves of size n and m, respectively. Finally, we propose a third algorithm that, in near quadratic time, computes a (constant) double-sided error approximation to the optimal <b>partial</b> Fréchet <b>distance...</b>|$|R
3000|$|The {{sequence}} {{in which the}} sibling nodes are visited is determined according to the their <b>partial</b> <b>distances</b> in an ascending order. This is to guarantee that the promising candidates are visited first {{in order to reduce}} the search complexity. This ordering process is referred to as the Schnorr-Euchner enumeration [18, 27, 28]. It can simply be implemented by a lookup table [29, 30] (line 4 in Algorithm 1), and its complexity is merely the computation of the linear estimation [...]...|$|R
40|$|A {{new method}} of {{determining}} <b>partial</b> interatomic <b>distances</b> in multicomponent systems from extended X-ray absorption fine structure data is presented. The method {{is based upon}} the regular procedure of solving a Fredholm integral equation of the first kind. The effectiveness of the method has been tested on mode 1 examples of two closely spaced coordination spheres and crystalline CuZr 2. The <b>partial</b> interatomic <b>distances</b> of amorphous Cu 33 Zr 67 have been investigated by EXAFS using the synchrotron radiation of a VEPP- 4 storage ring. The structural results are compared with previous experimental and theoretical investigations...|$|R
40|$|Abstract — Although the “neural-gas ” network {{proposed}} by Martinetz et al. in 1993 {{has been proven}} for its optimality in vector quantizer design and has been demonstrated to have good performance in time-series prediction, its high computational complexity (N logN) makes it a slow sequential algorithm. In this letter, we suggest two ideas to speedup its sequential realization: 1) using a truncated exponential function as its neighborhood function and 2) applying a new extension of the <b>partial</b> <b>distance</b> elimination method (PDE). This fast realization is compared with the original version of the neural-gas network for codebook design in image vector quantization. The comparison indicates that a speedup of five times is possible, while {{the quality of the}} re-sulting codebook is almost {{the same as that of}} the straightforward realization. Index Terms — Neural-gas network, <b>partial</b> <b>distance</b> elimina-tion, vector quantization. I...|$|E
40|$|Finding fast {{algorithms}} {{to detect}} outliers (as unusual objects) by their distance to neighboring objects {{is a big}} desire. Two algorithms were proposed to detect outliers quickly. The first {{was based on the}} <b>Partial</b> <b>Distance</b> (PD) algorithm and the second was an improved version of the PD algorithm. It was found that the proposed algorithms reduced the number of distance calculations compared to the nested-loop method...|$|E
40|$|Although fractal image {{compression}} can achieve high compression ratio theoretically, {{it needs a}} lot of encoding time to encode an image so that it has not been widely applied as other coding schemes in the field of {{image compression}}. In this paper, an algorithm is devised to improve this drawback. The algorithm uses three major processes including classification, PDS (<b>Partial</b> <b>distance</b> search) and simplification of eight transformations to decrease the encoding time. Classification is the major factor to reduce the majority of encoding time. PDS decreases computation time of MSE (Mean Square Error). Simplification of eight transformations diminishes unnecessary computation. The experimental result shows that our proposed method makes the encoder much faster than the conventional fractal compression method and the quality is imperceptible to the conventional fractalencoding algorithm. Compared to the published fast fractal encoding algorithms, the proposed method outperforms them. This paper contributes to the performance of source signal compression before the communication and raises the effectiveness of multimedia system. Key words: fast fractal encoding; image compression; <b>partial</b> <b>distance</b> search; classification; simplification; 1...|$|E
40|$|The {{asymptotic}} {{performance of}} a polar code under successive cancellation decoding {{is determined by the}} exponent of its polarizing matrix. We first prove that the <b>partial</b> <b>distances</b> of a polarizing matrix constructed from the Kronecker product are simply expressed as a product of those of its component matrices. We then show that the exponent of the polarizing matrix is shown to be a weighted sum of the exponents of its component matrices. These results may be employed in the design of a large polarizing matrix with high exponent. Comment: 7 pages, 1 figure, submitted to IEEE Transactions on Information Theory (July 27, 2011...|$|R
40|$|Nowadays {{sequences}} of symbols {{are becoming more}} important, as they are the standard format for representing information in a large variety of domains such as ontologies, sequential patterns or non numerical attributes in databases. Therefore, {{the development of new}} distances for this kind of data is a crucial need. Recently, many similarity functions have been proposed for managing {{sequences of}} symbols; however, such functions do not always hold the triangular inequality. This property is a mandatory requirement in many data mining algorithms like clustering or k-nearest neighbors algorithms, where the presence of a metric space is a must. In this paper, we propose a new distance for sequences of (non-repeated) symbols based on the <b>partial</b> <b>distances</b> between the positions of the common symbols. We prove that this <b>Partial</b> Symbol Ordering <b>distance</b> satisfies the triangular inequality property, and we finally describe a set of experiments supporting that the new distance outperforms the Edit distance in those ecenarios where sequence similarity is related to the positions occupied by the symbols. Postprint (published version...|$|R
40|$|The {{complexity}} of depth-first sphere decoders (SDs) {{is determined by}} the employed tree search and pruning strategies. Proposed is a new SD approach for maximum-likelihood (ML) detection of spatially multiplexed, high-order, QAM symbols. In contrast to typical ML approaches, the proposed tree traversal skips the computationally intensive requirement of visiting the nodes in ascending order of their <b>partial</b> <b>distances</b> (PDs). Then, a new pruning method efficiently narrows the search space and preserves the ML performance despite the non-ordered tree traversal. This proposed approach results in substantially reduced PD calculations when compared to typical ML SDs and, for high SNRs, the necessary calculations can be reduced down to the number of transmit antennas. © 2012 The Institution of Engineering and Technology...|$|R
40|$|In {{this paper}} {{a number of}} {{improvements}} are suggested {{that can be applied}} to most k-medoids-based algorithms. These can be divided into two categories - conceptual / algorithmic improvements, and implementational improvements. These include the revisiting of the accepted cases for swap comparison and the application of <b>partial</b> <b>distance</b> searching and previous medoid indexing to clustering. We propose extensions to the problem of nearest neighbor search, by combining the previous medoid index with triangular inequality elimination and <b>partial</b> <b>distance</b> searching. An improved k-medoids algorithm using simulated annealing, CLASA, is also discussed, as is a novel mechanism for managing memory usage. Various hybrids of these search approaches are then applied to a number of k-medoids-based algorithms and we show that the method is generally applicable. For example, experimental results based on various datasets, including both artificial and real datasets, demonstrate that when applied to CLARANS the number of distance calculations can be reduced by up to 98 % with similar average distance per object. Importantly, these search approaches can also be applied to nearest neighbor searching and other clustering algorithms...|$|E
40|$|One of {{the most}} popular {{clustering}} techniques is the k-means clustering algorithm. However, the utilization of the k-means is severely limited by its high computational complexity. In this study, we propose a new strategy to accelerate the k-means clustering algorithm through the <b>Partial</b> <b>Distance</b> (PD) logic. The proposed strategy avoids many unnecessary distance calculations by applying efficient PD strategy. Experiments show the efficiency of the proposed strategy when applied to different data sets...|$|E
40|$|In {{this paper}} we propose a new method of single imputation, reconstruction, and {{estimation}} of non-reported, incorrect or excluded values both in the target and in the auxiliary variables where the first is on ratio or interval scale and the last are heterogeneous in measurement scale. Our technique is a variation of the popular nearest neighbor hot deck imputation (NNHDI) where "nearest" is defined in terms of a global distance obtained as a convex combination of the <b>partial</b> <b>distance</b> matrices computed for the various types of variables. In particular, we address the problem of proper weighting the <b>partial</b> <b>distance</b> matrices in order to reflect their significance, reliability and statistical adequacy. Performance of several weighting schemes is compared under a variety of settings in coordination with imputation of the least power mean. We have demonstrated, through analysis of simulated and actual data sets, the appropriateness of this approach. Our main contribution has been to show that mixed data may optimally be combined to allow accurate reconstruction of missing values in the target variable {{even in the absence of}} some data in the other fields of the record. hot-deck imputation, nearest neighbor, general distance coefficient, least power mean...|$|E
5000|$|... where [...] is the <b>partial</b> {{pressure}} at <b>distance</b> x and [...] is the partial {{pressure at}} infinite {{distance from the}} surface.|$|R
40|$|Nuclear Magnetic Resonance (NMR) Spectroscopy is {{a widely}} used {{technique}} to predict the native structure of proteins. However, NMR machines are only able to report approximate and <b>partial</b> <b>distances</b> between pair of atoms. To build the protein structure one has to solve the Euclidean distance geometry problem given the incomplete interval distance data produced by NMR machines. In this paper, we propose a new genetic algorithm for solving the Euclidean distance geometry problem for protein structure prediction given sparse NMR data. Our genetic algorithm uses a greedy mutation operator to intensify the search, a twin removal technique for diversification in the population and a random restart method to recover stagnation. On a standard set of benchmark dataset, our algorithm significantly outperforms standard genetic algorithms. Comment: Accepted for publication in the 8 th International Conference on Electrical and Computer Engineering (ICECE 2014...|$|R
40|$|Abstract. The recent {{interest}} in function of various RNA structures, re-flected {{in the growth}} of solved RNA structures in PDB, calls for methods for effective and efficient similarity search in RNA structural databases. Here, we propose a method called SETTER (RNA SEcondary sTructure-based TERtiary structure similarity) based on partitioning of RNA structures into so-called generalized secondary structure units (GSSU). We introduce a fast similarity method exploiting RMSD-based algorithm allowing to assess distance to a pair of GSSU, and a method for aggregat-ing these <b>partial</b> <b>distances</b> into a final distance corresponding to struc-tural similarity of the examined RNA structures. Our algorithm yields not only the distance but also a superposition allowing to visualize the structural similarity. Comparative experiments show that our proposed method is competitive with the best existing solutions, both in terms of effectiveness and efficiency...|$|R
40|$|In this paper, {{we provide}} an {{overview}} of fast nearest-neighbor search algorithms based on an &approxima- tion}elimination' framework under a class of elimination rules, namely, <b>partial</b> <b>distance</b> elimination, hypercube elimination and absolute-error-inequality elimination derived from approximations of Euclidean distance. Previous algorithms based on these elimination rules are reviewed in the context of approximation}elimination search. The main emphasis in this paper is a comparative study of these elimination constraints with reference to their approximation}elimination e$ciency set within di!erent approximation schemes. # 2000 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved...|$|E
40|$|The k-means {{is one of}} {{the most}} popular and widely used {{clustering}} algorithm; however, it is limited to numerical data only. The k-prototypes algorithm is an algorithm famous for dealing with both numerical and categorical data. However, there have been no studies to accelerate it. In this paper, we propose a new, fast k-prototypes algorithm that provides the same answers as those of the original k-prototypes algorithm. The proposed algorithm avoids distance computations using <b>partial</b> <b>distance</b> computation. Our k-prototypes algorithm finds minimum distance without distance computations of all attributes between an object and a cluster center, which allows it to reduce time complexity. A <b>partial</b> <b>distance</b> computation uses a fact that a value of the maximum difference between two categorical attributes is 1 during distance computations. If data objects have m categorical attributes, the maximum difference of categorical attributes between an object and a cluster center is m. Our algorithm first computes distance with numerical attributes only. If a difference of the minimum distance and the second smallest with numerical attributes is higher than m, we can find the minimum distance between an object and a cluster center without distance computations of categorical attributes. The experimental results show that the computational performance of the proposed k-prototypes algorithm is superior to the original k-prototypes algorithm in our dataset...|$|E
40|$|Abstract — <b>Partial</b> <b>Distance</b> Search (PDS) is {{a method}} of {{reducing}} the amount of computation required for vector quantization encoding. The method is simple and general enough to be incorporated into many fast encoding algorithms. This paper describes a simple improvement to PDS, based on principal components analysis,that rotates the codebook without altering the interpoint distances. Like PDS,this new method can be used to improve many fast encoding algorithms. The algorithm decreases the decoding time of PDS by as much as 44 % and decreases the decoding time of k-d trees by as much as 66 % on common vector quantization benchmarks...|$|E
40|$|This paper {{details the}} project "Edge Matching Using the Hausdorff Distance" for CIS 6930, Computational Geometry. This project {{implements}} variants {{of the image}} matching algorithm described in [3] and [7]. The algorithm considered here is a <b>partial</b> Hausdorff <b>distance</b> matching algorithm with and without edge orientation information. The algorithm is implemented and testing against a series of infrared military images...|$|R
40|$|An {{important}} research {{issue in}} multimedia databases is the retrieval of similar objects. For most applications in multimedia databases, an exact search is not meaningful. Thus, much {{effort has been}} devoted to develop efficient and effective similarity search techniques. A recent approach, that has been shown to improve the effectiveness of similarity search in multimedia databases, resorts to the usage of combinations of metrics where the desirable contribution (weight) of each metric is chosen at query time. This paper presents the Multi-Metric M-tree (M 3 -tree), a metric access method that supports similarity queries with dynamic combinations of metric functions. The M 3 -tree, an extension of the Mtree, stores <b>partial</b> <b>distances</b> to better estimate the weighed distances between routing/ground entries and each query, where a single distance function is used to build the whole index. An experimental evaluation shows that the M 3 -tree may be as efficient as having multiple M-trees (one for each combination of metrics) ...|$|R
40|$|Persistent {{homology}} provides shapes descriptors called persistence diagrams. We use persistence diagrams {{to address}} the problem of shape comparison based on partial similarity. We show that two shapes having a common sub-part in general present a common persistence sub-diagram. Hence, the <b>partial</b> Hausdorff <b>distance</b> between persistence diagrams measures partial similarity between shapes. The approach is supported by experiments on 2 D and 3 D data sets...|$|R
