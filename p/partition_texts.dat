0|43|Public
40|$|This paper {{presents}} {{a new idea}} for lexical analysis: lolo (language-oriented lexer objects) is strictly based on the object orientation paradigm. We introduce the idea behind the system, describe the implementation, and {{compare it to the}} conventional approach using lex[1] or flex[2]. lolo[3] extracts symbols from a sequence of input characters belonging to the ASCII or Unicode sets. lolo scanners can be extended without access to the source code: symbol recognizers can be derived by inheritance and an executing scanner can be reconfigured for different contexts. Recognizer actions are represented by objects which may be replaced at any time. Recognizers need not be based on finite state automata; therefore, lolo can recognize symbols that systems like lex cannot recognize directly. The idea Conventional tools for lexical analysis, such as lex, <b>partition</b> <b>text</b> based on patterns, i. e., on regular expressions. Patterns are associated with program statements to b...|$|R
40|$|This paper {{introduces}} a new statistical approach to automatically <b>partitioning</b> <b>text</b> into coherent segments. The approach {{is based on}} a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitat [...] ...|$|R
40|$|This paper {{introduces}} a new statistical approach to <b>partitioning</b> <b>text</b> automatically into coherent segments. Our approach enlists both short-range and long-range language models {{to help it}} sni# out likely sites of topic changes in text. To aid its search, the system consults a set of simple lexical hints it has learned to associate {{with the presence of}} boundaries through inspection of a large corpus of annotated data. We also propose a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms. Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the e#ectiveness of our approachintwovery di#erent domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts. 1 Introduction The task we address in this paper might se [...] ...|$|R
40|$|This paper {{describes}} TextTiling, an algorithm for <b>partitioning</b> expository <b>texts</b> into coherent multi-paragraph discourse units which re ect the subtopic {{structure of}} the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts...|$|R
40|$|Abstract. This paper {{introduces}} a new statistical approach to automatically <b>partitioning</b> <b>text</b> into coherent segments. The approach {{is based on}} a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms. 1...|$|R
40|$|This paper {{presents}} {{a new idea}} for lexical analysis: oolex (object-oriented lexer) is strictly based on the object orientation paradigm. We introduce the idea behind the system, describe the implementation, and {{compare it to the}} conventional approach using flex or lex. oolex extracts symbols from a sequence of Unicode input characters. It can be extended without access to the source code: symbol recognizers can be derived by inheritance and an executing scanner can be reconfigured for different contexts. Recognizer actions are represented by objects which may be replaced at any time. oolex need not be based on finite state automata; therefore, it can recognize symbols that systems like flex cannot recognize directly. oolex can be used for rapid prototyping: most of the existing recognizers can represent themselves as regular expressions for the Java based JLex. The idea Conventional tools for lexical analysis, such as flex or lex, <b>partition</b> <b>text</b> based on patterns, i. e., on regular expressions. Patterns are associated with program statements to be executed upon recognition. Pattern syntax is rather cryptic and difficult for novices to understand. To understand a complicated older pattern (or even someone else’s) usually requires considerable experience. As an example, her...|$|R
40|$|This paper {{presents}} TextTiling, {{a method}} for <b>partitioning</b> full-length <b>text</b> documents into coherent multiparagraph units. The layout of text tiles is meant to reflect the pattern of subtopics contained in an expository text. The approach uses lexical analyses based on tf. idf, an information retrieval measurement, {{to determine the extent}} of the tiles, incorporating thesaural information via a statistical disambiguation algorithm. The tiles have been found to correspond well to human judgements of the major subtopic boundaries of science magazine articles...|$|R
40|$|International audienceWe {{consider}} a compact text index based on evenly spaced sparse suffix trees {{of a text}} [9]. Such a tree is defined by <b>partitioning</b> the <b>text</b> into blocks of equal size and constructing the suffix tree only for those suffixes that start at block boundaries. We propose a new pattern matching algorithm on this structure. The algorithm {{is based on a}} notion of suffix links different from that of [9] and on the packing of several letters into one computer wor...|$|R
40|$|FIGURE 4. Bayesian tree {{obtained}} {{with a simple}} codon <b>partition</b> (see <b>text).</b> Bars on branches represent levels of support from analyses under the following inference methods (in order) : Bayesian with codon partition, Bayesian with single partition, Maximum likelihood with codon partition, ML with single partition, Parsimony. Black = jack-knife or bootstrap support> 70, PP> 0. 95, Grey = clade present but with lower supports, White = clade not recovered. Haplotype networks corresponding to Cestocampa iberica sp. n. drawn beside their corresponding clades...|$|R
40|$|A <b>text</b> <b>partition</b> {{model is}} {{proposed}} {{to determine the}} boundaries of discourse structures. It is based on association of noun-noun relations and noun-verb relations defined on discourse level and sentence level. Three factors are considered: 1) repetition of words, 2) importance of words, and 3) collocational semantics. Ten texts serve as experimental objects. The applications of the results to sentence alignment, topic identification, topic shift and topic abstraction are discussed. Word Count: 2746 Topic Areas: Discourse Analysis, <b>Text</b> <b>Partition.</b> 1. INTRODUCTION In general, a text is not just juxtaposition of a sequence of sentences, but is well-organized for reading. Discourse analysis investigates how texts are organized and realizes their underlying structure. The structure which spans a few sentences of texts is often called a discourse segment. Many researches present various relations among and within these segments. Discourse representation theory (DRT) (Kamp, 1981) proposes di [...] ...|$|R
40|$|Usage of term “envelope” {{related to}} {{architecture}} is quite recent and comes as an {{evolution of the}} concept of closure which identifies, as different units, exterior vertical and horizontal <b>partitions.</b> This <b>text</b> present the envelope, considered as an architecture interface, a border sensible to innovation and not only frontier between interior and exterior. Indeed, in contemporary architecture, the envelope has to become a place of meeting among different qualities (spatial, environmental, functional, technological, constructive, formal). These qualities have not just to be measurable and verifiable on the basis of performance parameters but they have {{to be part of a}} tectonic, holistic and architectural concept...|$|R
40|$|This paper {{presents}} a new enhanced text extraction algorithm from degraded document {{images on the}} basis of the probabilistic models. The observed document image is considered as a mixture of Gaussian densities which represents the foreground and background document image components. The EM algorithm is introduced in order to estimate and improve the parameters of the mixtures of densities recursively. The initial parameters of the EM algorithm are estimated by the k-means clustering method. After the parameter estimation, the document image is <b>partitioned</b> into <b>text</b> and background classes by the means of ML approach. The performance of the proposed approach is evaluated on a variety of degraded documents comes from the collections of the National library of Tunisia...|$|R
40|$|The {{focus of}} this essay is gendered {{collective}} memory of the partition of the Indian sub-continent in 1947, {{at the time of}} Independence from British rule. The essay addresses the question of whether there are similarities between trauma studies that developed within a Western Freudian psychoanalytic framework and the anti-colonial theory practiced by decolonizing nations. Taking two women's texts, the essay examines how gender manifests itself within the framework of trauma and how it is played out in collective memory of <b>partition.</b> The <b>texts</b> chosen raise interesting questions about gender, trauma and the nation and provide an alternative non-Western framework through which the trauma of partition can be read. The essay points to how the category of memory and its meanings vary in their national, cultural and historical specificity...|$|R
40|$|Most {{existing}} semi-supervised document clustering {{approaches are}} model-based clustering {{and can be}} treated as parametric model taking an assumption that the underlying clusters follow a certain pre-defined distribution. In our semi-supervised document clustering, each cluster is represented by a non-parametric probability distribution. Two approaches are designed for incorporating pairwise constraints in the document clustering approach. The first approach, term-to-term relationship approach (TR), uses pairwise constraints for capturing term-to-term dependence relationships. The second approach, linear combination approach (LC), combines the clustering objective function with the user-provided constraints linearly. Extensive experimental results show that our proposed framework is effective. This thesis presents a new framework for automatically <b>partitioning</b> <b>text</b> documents taking into consideration of constraints given by users. Semi-supervised document clustering is developed based on pairwise constraints. Different from traditional semi-supervised document clustering approaches which assume pairwise constraints to be prepared by user beforehand, we develop a novel framework for automatically discovering pairwise constraints revealing the user grouping preference. Active learning approach for choosing informative document pairs is designed by measuring {{the amount of information}} that can be obtained by revealing judgments of document pairs. For this purpose, three models, namely, uncertainty model, generation error model, and term-to-term relationship model, are designed for measuring the informativeness of document pairs from different perspectives. Dependent active learning approach is developed by extending the active learning approach to avoid redundant document pair selection. Two models are investigated for estimating the likelihood that a document pair is redundant to previously selected document pairs, namely, KL divergence model and symmetric model. Huang, Ruizhang. Adviser: Wai Lam. Source: Dissertation Abstracts International, Volume: 70 - 06, Section: B, page: 3600. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2008. Includes bibliographical references (leaves 117 - 123). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstracts in English and Chinese. School code: 1307...|$|R
40|$|We {{present a}} method for <b>partitioning</b> expository <b>texts</b> into {{coherent}} multi-paragraph units which reflect the subtopic structure of the texts. Using Chafe's Flow Model of discourse, we observe that subtopics are often expressed by the interaction of multiple simultaneous themes. We describe two fully-implemented algorithms that use only term repetition information to determine the extents of the subtopics. We show that the segments correspond well to human judgements of the major subtopic boundaries of thirteen lengthy texts, and suggest {{the use of such}} segments in information retrieval applications. 1 Introduction: Multi-paragraph Segmentation The structure of expository texts can be characterized as a sequence of subtopical discussions that occur {{in the context of a}} few main topic discussions. For example, a text called Stargazers, whose main topic is the existence of life on earth and other planets, could be judged to consist of the following subdiscussions (numbers indicate paragra [...] ...|$|R
40|$|This paper {{describes}} TextTiling, an algorithm for <b>partitioning</b> expository <b>texts</b> into coherent multi-paragraph discourse units which {{reflect the}} subtopic {{structure of the}} texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. INTRODUCTION The structure of expository texts {{can be characterized as}} a sequence of subtopical discussions that occur {{in the context of a}} few main topic discussions. For example, a popular science text called Stargazers, whose main topic is the existence of life on earth and other planets, can be described as consisting of the following subdiscussions (numbers indicate paragraph numbers) : 1 - 3 Intro [...] the search for life in space 4 - 5 The moon's chemical composition 6 - 8 [...] ...|$|R
40|$|With Zipf’s law being {{originally}} {{and most}} famously observed for word frequency, it is surprisingly {{limited in its}} applicability to human language, holding over {{no more than three}} to four orders of magnitude before hitting a clear break in scaling. Here, building on the simple observation that phrases of one or more words comprise the most coherent units of meaning in language, we show empirically that Zipf’s law for phrases extends over as many as nine orders of rank magnitude. In doing so, we develop a principled and scalable statistical mechanical method of random <b>text</b> <b>partitioning,</b> which opens up a rich frontier of rigorous text analysis via a rank ordering of mixed length phrases...|$|R
40|$|Abstract. Successful {{applications}} of digital libraries require structured access to sources of information. This paper presents {{an approach to}} extract the logical structure of text documents. The extracted structure is explicated by means of SGML (Standard Generalized Markup Language). Consequently, the extraction is achieved {{on the basis of}} grammars that extend SGML with recognition rules. From these grammars parsing automata are generated. These automata are used to <b>partition</b> a at <b>text</b> document into its elements, to discard formatting information, and to insert SGML markups. Complex document structures and fallback rules needed for error tolerant parsing make such automata highly ambiguous. Anovel parsing strategy has been developed that ranks and prunes ambiguous parsing paths. Key words: document analysis, logical structure, automaton parsing, fault toleranc...|$|R
40|$|A text is any {{sequence}} of symbols (or characters) drawn from an alphabet. A {{large portion of the}} information available worldwide in electronic form is actually in text form (other popular forms are structured and multimedia information). Some examples are: natural language text (for example books, journals, newspapers, jurisprudence databases, corporate information, and the Web), biological sequences (ADN and protein sequences), continuous signals (such as audio, video sequence descriptions and time functions), and so on. Recently, due to the increasing complexity of applications, text and structured data are being merged into so-called semistructured data, which is expressed and manipulated in formats such as XML (out of the scope of this review). As more text data is available, the challenge to search them for queries of interest becomes more demanding. A text database is a system that maintains a (usually large) text collection and provides fast and accurate access to it. These two goals are relatively orthogonal, and both are critical to profit from the text collection. Traditional database technologies are not well suited to handle text databases. Relational databases structure the data in fixed-length records whose values are atomic and are searched for by equality or ranges. There is no general way to <b>partition</b> a <b>text</b> into atomic records, an...|$|R
40|$|AbstractThis work {{presents}} a syntax-directed, modular approach to temporal logic model checking of sequential programs. In contrast to hardware designs, {{the models of}} software systems might be too large to fit into memory even when they consist of a single sequential unit. Furthermore, even when the model can be held in memory, model checking might exceed the memory capacity of the computer. To avoid the high space requirements for software we therefore suggest to <b>partition</b> the <b>text</b> of a sequential program into sequentially composed sub-programs. Based on this partition, we present a model-checking algorithm for sequential programs that arrives at its conclusion by examining each sub-program in separation. The novelty of our approach is that it uses a decomposition {{of the program in}} which the interconnection between parts is sequential and not parallel. We handle each part separately, while keeping all other parts on an external memory (files). Consequently, our approach reduces space requirements and enables verification of larger systems. We implemented the ideas described in this paper in a prototype tool called SoftVer and applied it to a few small examples. We have achieved reduction in both space and time requirements. We consider this work as a step towards making temporal logic model checking useful for verification of sequential programs...|$|R
40|$|Abstract: This paper {{presents}} a text block extraction algorithm that takes as its input {{a set of}} text lines of a given document, and <b>partitions</b> the <b>text</b> lines into a set of text blocks, where each text block {{is associated with a}} set of homogeneous formatting attributes, e. g. text-alignment, indentation. The text block extraction algorithm described in this paper is probability based. We adopt an engineering approach to systematically characterising the text block structures based on a large document image database, and develop statistical methods to extract the text block structures from the image. All the probabilities are estimated from an extensive training set of various kinds of measurements among the text lines, and among the text blocks in the training data set. The off-line probabilities estimated in the training then drive all decisions in the on-line text block extraction. An iterative, relaxation-like method is used to find the partitioning solution that maximizes the joint probability. To evaluate the performance of our text block extraction algorithm, we used a three-fold validation method and developed a quantitative performance measure. The algorithm was evaluated on the UW-III database of some 1600 scanned document image pages. The text block extraction algorithm identifies and segments 91 % of text blocks correctly...|$|R
40|$|An {{important}} step to understand text {{is to build}} the discourse structure through cohesion and coherence. However, to build the discourse structure in turn depends on the full understanding of texts, so that many efforts on this line are not automatic and not successful. A corpus-based model based on 1) repetition of words, 2) importance of words, and 3) collocational semantics for texts is proposed in this paper. It focuses on association norms of noun-noun relations and noun-verb relations defined on discourse level and sentence level, respectively. According to this model, a <b>text</b> <b>partition</b> algorithm is proposed to determine the boundaries of discourse structures and a topic identification algorithm is also presented. The results {{of a series of}} experiments show that the proposed model is Promising. [Article content in Chinese...|$|R
40|$|The {{aim of this}} {{research}} is to evaluate the extent to which Thesaurus allow us to modify a researcher's knowledge frame. A well calculated Thesaurus has the power to overwrite existent knowledge frames, or habitual "heuristics " for the humanities, if word occurrence data are rigorously manipulated by the algorithms of statistical linguistics. However, even if all targets and means of data gathering and analyzing are readily available, there remains behind various interpretations of subjects a sort of Frame question of how we <b>partition</b> off the <b>texts</b> and documents to avoid arbitrary text segmentation. Frames are needed before we can gather and interpret the data for a word occurrence computation. If the problem of text segmentation remains unresolved, any experiment in quantitative text analysis will be still fa...|$|R
40|$|Abstract—Text {{information}} in natural scene images serves as important clues for many image-based {{applications such as}} scene understanding, content-based image retrieval, assistive navigation, and automatic geocoding. However, locating text from complex background with multiple colors is a challenging task. In this paper, we explore a new framework to detect text strings with arbitrary orientations in complex natural scene images. Our proposed framework of text string detection consists of two steps: 1) Image <b>partition</b> to find <b>text</b> character candidates based on local gradient features and color uniformity of character components. 2) Character candidate grouping to detect text strings based on joint structural features of text characters in each text string such as character size differences, distances between neighboring characters, and character alignment. By assuming that a text string has at least three characters, we propose two algorithms o...|$|R
40|$|Successful {{applications}} of digital libraries require structured access to sources of information. This paper presents {{an approach to}} extract the logical structure of text documents. The extracted structure is explicated by means of SGML (Standard Generalized Markup Language). Consequently, the extraction is achieved {{on the basis of}} grammars that extend SGML with recognition rules. From these grammars parsing automata are generated. These automata are used to <b>partition</b> a flat <b>text</b> document into its elements, to discard formatting information, and to insert SGML markups. Complex document structures and fallback rules needed for error tolerant parsing make such automata highly ambiguous. A novel parsing strategy has been developed that ranks and prunes ambiguous parsing paths. 1 Introduction Digital Libraries is a field mainly concerned with technologies for information acquisition, processing, distribution, and access in highly distributed, heterogeneous environments. Successful appli [...] ...|$|R
40|$|We {{present a}} novel {{approach}} to visualize and explore unstructured text. The underlying technology, called TOPIC-O-GRAPHY TM, applies wavelet transforms to a custom digital signal constructed from words within a document. The resultant multiresolution wavelet energy is {{used to analyze the}} characteristics of the narrative flow in the frequency domain, such as theme changes, which is then related to the overall thematic content of the text document using statistical methods. The thematic characteristics of a document can be analyzed at varying degrees of detail, ranging from section-sized <b>text</b> <b>partitions</b> to partitions consisting of a few words. Using this technology, we are developing a visualization system prototype known as TOPIC ISLANDS TM to browse a document, generate fuzzy document outlines, summarize text by levels of detail and according to user interests, define meaningful subdocuments, query text content, and provide summaries of topic evolution. Keywords: text visualizati [...] ...|$|R
40|$|This work {{presents}} a fine-grained, text-chunking algorithm {{designed for the}} task of multiword expressions (MWEs) segmentation. As a lexical class, MWEs include {{a wide variety of}} idioms, whose automatic identification are a necessity for the handling of colloquial language. This algorithm's core novelty is its use of non-word tokens, i. e., boundaries, in a bottom-up strategy. Leveraging boundaries refines token-level information, forging high-level performance from relatively basic data. The generality of this model's feature space allows for its application across languages and domains. Experiments spanning 19 different languages exhibit a broadly-applicable, state-of-the-art model. Evaluation against recent shared-task data places <b>text</b> <b>partitioning</b> as the overall, best performing MWE segmentation algorithm, covering all MWE classes and multiple English domains (including user-generated text). This performance, coupled with a non-combinatorial, fast-running design, produces an ideal combination for implementations at scale, which are facilitated through the release of open-source software...|$|R
40|$|We {{argue that}} the advent of large volumes of fulllength text, as opposed to short texts like abstracts and newswire, should be {{accompanied}} by corresponding new approaches to information access. Toward this end, we discuss the merits of imposing structure on fulllength text documents � that is, a <b>partition</b> of the <b>text</b> into coherent multi-paragraph units that represent the pattern of subtopics that comprise the text. Using this structure, we canmake {{a distinction between the}} main topics, which occur throughout the length of the text, and the subtopics, which are of only limited extent. We discuss why recognition of subtopic structure is important and how, to some degree of accuracy, itcanbe found. We describe a new way of specifying queries on full-length documents and then describe an experiment in which making use of the recognition of local structure achieves better results on a typical information retrieval task than does a standard IR measure. ...|$|R
40|$|This work {{presents}} a modular approach to temporal logic model checking of software. Model checking {{is a method}} that automatically determines whether a finite state system satisfies a temporal logic specification. Model checking algorithms have been successfully used to verify complex systems. However, their use {{is limited by the}} high space requirements needed to represent the verified system. When hardware designs are considered, a typical solution is to partition the design into units running in parallel, and handle each unit separately. For software systems such a solution is not always feasible. This is because a software system might be too large to fit into memory even when it consists of a single sequential unit. To avoid the high space requirements for software we suggest to <b>partition</b> the program <b>text</b> into sequentially composed subprograms. Based on this partition, we present a model checking algorithm for software that arrives at its conclusion by examining each subprogram in s [...] ...|$|R
40|$|The {{hypertext}} system "HyperMan" {{provides for}} an automatic conversion of linear machine [...] readable documents into hypertexts by applying <b>text</b> <b>partitioning</b> and link generation methods. After {{completion of the}} generation process, the graphical user interface of the system enables users to browse through the converted documents very easily. To determine whether user actions allow conclusions to be drawn about a generated hypertext, a special component that records user actions has been integrated into the system. In this way sequences of actions can be identified that provide hints of relationships between two document passages or between two terms {{that occur in the}} text. Then, the relationships can be stored as links or thesaurus entries, respectively, in the system's data base and can be made available to subsequent users. In addition to acquiring relationships, the user observation component also provides for hints about the acceptance of system components. These hints can serve as a bas [...] ...|$|R
40|$|Static code {{analysis}} is a methodology of detecting errors in program code {{based on the}} programmer's reviewing the code in areas within the program text where errors {{are likely to be}} found and since the process considers all syntactic program paths; there is the need for a model-based approach with slicing. This paper presented a model of high-level abstraction of code structure analysis for a large component based software system. The work leveraged on the most important advantage of static code structure analysis in re-tooling software maintenance for a developing economy. A program slicing technique was defined and deployed to <b>partition</b> the source <b>text</b> to manageable fragments to aid in the analysis and statecharts were deployed as visual formalism for viewing the dynamic slices. The resulting model was a high-tech static analysis process aimed at determining and confirming the expected behaviour of a software system using slices of the source text presented in the statechart...|$|R
40|$|AbstractWe {{attempt to}} extract {{characteristic}} expressions from literary works. That is, given two collections of literary works, {{one of which}} is written by a particular author (positive examples) and the other by a different author (negative examples), the problem is to find expressions that appear frequently in the positive examples but which are seldom found in the negative examples. This is considered as a special case of the optimal pattern discovery from textual data, in which only the substring patterns are considered. One approach would be to create a list of text substrings sorted according to goodness, and to scrutinize {{the first part of the}} list by human efforts. Since there is no word boundary in Japanese texts, a substring is often a fragment of a word or phrase. A method to assist domain experts who are involved in this task is a key problem. In this paper, we propose <b>partitioning</b> the <b>text</b> substrings into equivalence classes under an equivalence relation on strings, originally defined by Blumer et al. (J. ACM 34 (3) (1987) 578). The equivalence relation has the desirable property that all members of each equivalence class necessarily have a unique goodness value. This idea effectively reduces the inefficiency of the task of evaluating mined patterns. We also present a method for browsing possible superstrings of a focused string as well as its context. We report successful results with two pairs of anthologies of classical Japanese poems. We expect that the extracted expressions may lead to discovering overlooked aspects of individual poets...|$|R
40|$|Abstract. Large backup {{and restore}} systems may have a {{petabyte}} or more data in their repository. Such systems are often compressed by means of deduplication tech-niques, that <b>partition</b> the input <b>text</b> into chunks and store recurring chunks only once. One of the approaches is to use hashing methods to store fingerprints for each data chunk, detecting identical chunks with very low probability for collisions. As alterna-tive, {{it has been suggested}} to use similarity instead of identity based searches, which allows the definition of much larger chunks. This implies that the data structure needed to store the fingerprints is much smaller, so that such a system may be more scalable than systems built on the first approach. This paper deals with an extension of the second approach to systems in which it is still preferred to use small chunks. We describe the design choices made during the development of what we call an approximate hash function, serving as the basic tool of the new suggested deduplication system and report on extensive tests performed on an variety of large input files...|$|R
40|$|Public {{concern over}} the spread of {{infectious}} diseases such as avian H 5 N 1 influenza and swine flu (H 1 N 1) influenza A has underscored the importance of health surveillancesystems for the speedy and precise detection of disease outbreaks. However, two key barriers faced by the current web-based health surveillance systems are their inability to (a) understand complex geo-temporal attributes of events and (b) to obtain the levels of geo-temporal recognition. In this thesis, I develop a novel framework {{as a means to}} overcome these limitations. This framework is called spatiotemporal zoning. 　The objective of the spatiotemporal zoning scheme is to enable language echnology software to <b>partition</b> <b>text</b> into segments based on the spatiotemporal characteristics of its content. Each segment, which is called a text zone, contains a set of events that occurred at the same geographical location in the same time frame. The capability of associating events reported in each text segment with the most specific spatial and temporal information available in news reports enables simple techniques to be employed for detecting specific outbreak locations. These techniques could be, for example, text classification to detect text segments that indicate outbreak situations. At the same time, false alarms about past outbreaks can be avoided by taking the temporal information about the events into consideration. 　I created a representative corpus in order to demonstrate that spatiotemporal zoning can be automatically and manually applied to unrestricted text. The corpus consisted of 100 news articles from multiple news agencies reporting on various disease outbreaks {{in different parts of the}} world. 　To study the reliability of spatiotemporal zoning, an experiment was conducted in which three annotators were recruited to annotate the same set of documents according to the annotation guidelines and the agreement between these annotators was then analyzed. Several statistical measures, namely kappa, Krippendorff’s alpha (α), and the percentage agreement, were used for quantitatively measuring the agreement. The results showed that the level of agreement kappa was more than 0. 9 on average for event type and temporal attribute annotations, and it was only a slight lower for annotating spatial attributes. 　The task of spatiotemporal zoning can be separated into 3 main steps. (1) Document pre-processing: This step provides the basic elements for zone attribute analysis and was done automatically using natural language processing software. (2) Zone attribute annotation: Each event-predicate is analyzed to recognize its class, spatial and temporal attributes. (3) Zone boundary generation: This step is done based on the attribute values of each event-predicate. For spatiotemporal zone annotation, the study of automatic zone attribute annotation was done for each group of zone attributes, i. e., event type recognition, temporal attributes recognition, and spatial attribute recognition. 　To automatically classify event expressions, i. e. zone type recognition, Conditional Random Fields (CRFs) was used to incorporate various sets of text features into a classifier. 　To recognize spatial information, several approaches, ranging from simple techniques such as the commonly used heuristic-based approach to the more sophisticated achine learning approach, were experimented. I also explored various feature sets and feature encoding strategies in order to determine the best ones for recognizing spatial attributes. 　For temporal attribute recognition, I took a rule-based approach to recognizing an event 2 ̆ 7 s temporal information. However, one of the problems is that in many cases the same event is repeatedly mentioned whereas the time of its occurrence is stated only once. To improve the system 2 ̆ 7 s ability to recognize the temporal information, I employed a simple heuristic that helps to identify linguistic expressions referring to the same events. 　The above studies that I undertook prove that spatiotemporal zoning is reliable. Moreover, the results from automatic zone attribute recognition show that this scheme can be done automatically with a reliable level of performance...|$|R
40|$|An {{approach}} to automatic translation is outlined that utilizes technklues of statistical inl'ormatiml extraction from large data bases. The method {{is based on}} the availability of pairs of large corresponding texts that are translations of each other. In our case, the iexts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) <b>Partition</b> the source <b>text</b> into a set of fixed locutioris. (2) Use the glossary plus coutextual information to select im corresponding set of fixed Ioctttions into a sequen{e forming the target sentence. (3) Arrange the words of the talget fixed locutions into a sequence forming the target sentence. We have developed stalistical techniques facilitating both tile autonlatic reation of the glossary, and the performance of tile three translation steps, all on the basis of an aliglnncllt of corresponding sentences in tile two texts. While wc are not yet able to provide examples of French / English tcanslation, we present some encouraging intermediate results concerning lossary creation and the arrangement of target WOl'd seq l ie) lees. 1...|$|R
40|$|International audienceIn {{her famous}} {{autobiography}} Azadon ki Chaon men (Urdu, 1974), Anis Kidwai opens her {{depiction of the}} violence and chaos that followed India's Partition {{with the story of}} the tragic death of her husband. This valuable historical testimony that intertwines intimate narrations of pain, anxiety and loneliness clearly shows the interdependence of historical accounts and personal narratives. In Amrita Pritam’s Pinjar (Punjabi, 1950) and Jyotirmoyee Devi’s Epar Ganga Opar Ganga (Bengali, 1967), pseudo-autobiographies though written in the third person, similarly highlight the predominance of personal experience in the narration of the <b>Partition.</b> These three <b>texts</b> undoubtedly show that women’s narratives are inextricably linked to a history of pain and, more widely, of violence; but they also demonstrate that social history as conveyed by women (and more generally by subalterns) is elaborated through the narration of intimacy, as exemplified by Azadi ki Chaon men which nonetheless adopts a historical perspective. This paper aims at showing that whereas this feature of female Partition narratives can be understood as a specificity of subaltern historical narratives, it also reveals the way in which women’s voices internalize the metaphors of the Nation which transforms them into suffering bodies at the service of the Motherland...|$|R
