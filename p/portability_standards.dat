6|36|Public
50|$|The Centers for Medicare & Medicaid Services (CMS), {{previously}} {{known as}} the Health Care Financing Administration (HCFA), is a federal agency within the United States Department of Health and Human Services (HHS) that administers the Medicare program and works in partnership with state governments to administer Medicaid, the State Children's Health Insurance Program (SCHIP), and health insurance <b>portability</b> <b>standards.</b> In addition to these programs, CMS has other responsibilities, including the administrative simplification standards from the Health Insurance Portability and Accountability Act of 1996 (HIPAA), quality standards in long-term care facilities (more {{commonly referred to as}} nursing homes) through its survey and certification process, clinical laboratory quality standards under the Clinical Laboratory Improvement Amendments, and oversight of HealthCare.gov.|$|E
40|$|Component-based {{development}} architectures (CBDA) {{are increasingly}} being adopted by software engineers. The Object Management Group (OMG) has focused on paving the way to provide CBDAs with, among other benefits, interoperability and <b>portability</b> <b>standards</b> through the Model Driven Architecture (MDA). The main idea of MDA is to use UML to specify both the static interfaces and the dynamic behavior of the components in platform-independent models(PIM). Additionally, it defines rules so that PIMs can be mapped {{into a number of}} platform-specific models(PSM). Although MDA promises to overcome important unsolved problems in software engineering, it has not specified ways to represent software dependability yet. Particularly, there are no current means to design and assess system dependability, specially reliability, using the modeldrive...|$|E
40|$|A central premise for the {{creation}} of Electronic Health Records (EHR) is ensuring the portability of patient health records across various clinical, insurance, and regulatory entities. From <b>portability</b> <b>standards</b> such as International Classification of Diseases (ICD) to data sharing across institutions, a lack of portability of health data can jeopardize optimal care and reduce meaningful use. This research empirically investigates the relationship between health records availability and portability. Using data collected from 168 medical providers and patients, we confirm the positive relationship between user perceptions of expected satisfaction with EHR availability and the expected satisfaction with portability. Our findings contribute to more informed practice by understanding how ensuring the availability of patient data by virtue of enhanced data sharing standards, device independence, and better EHR data integration can subsequently drive perceptions of portability across a multitude of stakeholders...|$|E
50|$|Braintree {{initiated}} {{the credit card}} data <b>portability</b> <b>standard</b> in 2010, which was accepted as an official action group of the DataPortability project. Credit card data portability is supported by an opt-in community of electronic payment processing providers that agree to provide credit card data and associated transaction information to an existing merchant upon request in a PCI compliant manner.|$|R
5000|$|ESBs {{implement}} standardized interfaces for communication, connectivity, transformation, security, and <b>portability.</b> Supported <b>standards</b> include: ...|$|R
50|$|The Simulation Model <b>Portability</b> is a <b>standard</b> for {{simulation}} models developed by ESA together with various stakeholders in the European Space Industry.|$|R
40|$|Social {{networking}} graphs {{are often}} secluded and encapsulated in one social network, unable to interact and connect to other social graphs. This phenomenon {{is often referred}} to as “walled gardens”, and that’s exactly what most popular social networks are. This master thesis shows an overview over the most important data <b>portability</b> <b>standards</b> and what should be taken care of to enhance the accessibility of the data and information, eventually connecting the social graphs together. These standards and concepts are analyzed and outlined critically, highlighting their privacy, authenticity and security implications. The thesis contains two sections, a theoretical part about data portability and social networks, and a practical part about enhancements to the Origo [1] Platform. Acknowledgment I thank the entire Origo team for all their help and commitment to the project. Special thanks go to my supervisor Till Bay and to Dominique Schneider and Denni...|$|E
40|$|The {{development}} of micro-power generators for centrifugal microfluidic discs enhances the platform as a green point-of-care diagnostic system and {{eliminates the need}} for attaching external peripherals to the disc. In this work, we present micro-power generators that har-vest energy from the disc’s rotational movement to power biomedical applications on the disc. To implement these ideas, we developed two types of micro-power generators using piezoelectric films and an electromagnetic induction system. The piezoelectric-based gen-erator takes advantage of the film’s vibration during the disc’s rotational motion, whereas the electromagnetic induction-based generator operates on the principle of current genera-tion in stacks of coil exposed to varying magnetic flux. We have successfully demonstrated that at the spinning speed of 800 revolutions per minute (RPM) the piezoelectric film-based generator is able to produce up to 24 microwatts using 6 sets of films and the magnetic induction-based generator is capable of producing up to 125 milliwatts using 6 stacks of coil. As a proof of concept, a custom made localized heating system was constructed to test the capability of the magnetic induction-based generator. The heating system was able to achieve a temperature of 58. 62 °C at 2200 RPM. This {{development of}} lab-on-a-disc micro power generators preserves the <b>portability</b> <b>standards</b> and enhances the future biomedical applications of centrifugal microfluidic platforms...|$|E
40|$|Every useful {{application}} outlives {{the platform}} {{on which it}} was developed and deployed. Application source code portability {{is one of the}} cornerstones of most open systems definitions. The intention is that if an application is written to a particular model of source-code portability, it can port relatively easily to any platform that supports the portability model. This model is often based on source code <b>portability</b> <b>standards</b> such as the ISO/IEEE family of POSIX standards [1, 2] and ISO/ANSI C[3], and specifications that include these standards such as the Open Group’s Single UNIX Specification[4]. The strength of this model is that the investment in the application’s development is not lost in redeployment to new architectures. Re-writing the application to new platforms every five years or so is not an option. There are already too many new applications to be written in the queue without adding the burden of costly re-writes every few years to address newer faster hardware platforms. Currently businesses are faced with the decision of moving to Microsoft’s Windows NT operating system. Windows NT provides a robust environment to solve many business application problems through a host of software development tools, as well as a platform to run the many Win 32 -based applications that businesses use today. It does this in a price competitive manner with respect to the hardware platforms on which it runs. The problem becomes protecting the huge investment in applications development over the past decade or more in UNIX applications. How does one leverage and protec...|$|E
40|$|Aerospace {{engineers}} {{often use}} specific off-the-shelf model development environments (e. g. MATLAB/Simulink, Scilab, EcosimPro, Modelica, 20 -sim) as their design environment for simulation models and control algorithms. At {{the same time}} many projects require that these models and algorithms are executed in real-time, e. g. with hardware and/or human-in the-loop using a specific real-time simulation environment. Furthermore, for portability and reuse, simulation models can be required {{to comply with the}} Simulation Model <b>Portability</b> <b>standard</b> (SMP 2 and ECSS E 40 - 07). Porting the simulation models from one environment to the other and/or making them compliant to a standard by hand can be a cumbersome task. In order to reduce development costs and time it is widely acknowledged that automatic model transfer from the development tools to the real-time simulation environments and the simulation standards is essential. NLR has developed a tool calle...|$|R
50|$|Complementary to the Open System Environment is the Application <b>Portability</b> Profile <b>{{standard}}.</b> This standard can {{covers a}} broad range of application software domains of interest to many US federal agencies, but it does not include every domain within the U.S. Government’s application inventory. The individual standards and specifications in the APP define data formats, interfaces, protocols, or a mix of these elements.|$|R
50|$|According to the <b>standard,</b> <b>portability</b> {{problems}} are very limited as GNU programs {{are designed to}} be compiled with one compiler, the GNU C Compiler, and only run on one system, which is the GNU system.|$|R
40|$|Enabling cloud {{infrastructures}} {{to evolve}} into a transparent platform raises interoperability issues. Interoperability requires standard data models and communication technologies compatible with the existing Internet infrastructure. To reduce vendor lock-in situations, cloud computing must implement common strategies regarding <b>standards,</b> interoperability and <b>portability.</b> Open <b>standards</b> are of critical importance {{and need to be}} embedded into interoperability solutions. Interoperability is determined at the data level as well as the service level. Relevant modelling standards and integration solutions shall be analysed in the context of clouds...|$|R
40|$|Abstract—Enabling cloud {{infrastructures}} {{to evolve}} into a transparent platform while preserving integrity raises interop-erability issues. How components are connected needs to be addressed. Interoperability requires standard data models and communication encoding technologies compatible with the exist-ing Internet infrastructure. To reduce vendor lock-in situations, cloud computing must implement universal strategies regarding <b>standards,</b> interoperability and <b>portability.</b> Open <b>standards</b> are of critical importance {{and need to be}} embedded into interoperability solutions. Interoperability is determined at the data level as well as the service level. Corresponding modelling standards and integration solutions shall be analysed...|$|R
40|$|Product-line {{architectures}} (PLAs) are {{an emerging}} paradigm for developing software families for distributed real-time and embedded (DRE) systems by customizing reusable artifacts, rather than handcrafting software from scratch. To reduce {{the effort of}} developing software PLAs and product variants for DRE systems, developers are applying general-purpose – ideally standard – middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, {{due to lack of}} optimizations tailored to meet the specific QoS requirements of different product variants in a PLA. This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ∼ 65 %, average- and worst-case end-toend latency measures by ∼ 43 % and ∼ 45 %, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on <b>portability,</b> <b>standard</b> middleware APIs, or application software implementations, and interoperability...|$|R
40|$|Object plays a very {{important}} role in comparing softwareentities. But the role of object when it comes to simulationfield is still undefined. Several standards have been createdwith the purpose to provide a design solution for the projectof Simulators. The design solution reported in this paperaggregates the principles of both software and simulatorarchitectures. The objective is to invert the top-downstrategy of model-driven development with the SimulationModel <b>Portability</b> (SMP) <b>standard</b> into a bottom-updevelopment process of a SMP framework. The paper alsoprovides solution for two development lines of two differentframeworks: the first is the SMP framework that changesaccording to the design models, and the second is aframework designed to support the development ofreusable behavior implementation...|$|R
40|$|We {{investigate}} the <b>portability</b> of <b>standard</b> norm-conserving pseudopotentials outside the density functional theory-local density approximation (DFT-LDA) framework, i. e., their use and interpretation as electron-ion effective potentials in valence-only diffusion Monte Carlo simulations. While first-principles many-body pseudopotentials {{are not available}} in the literature yet, the use of approximate pseudopotentials in quantum Monte Carlo simulations is becoming widespread. Here we attempt a systematic analysis of the portability of norm-conserving pseudopotentials generated within DFT-LDA, focusing on a model many-body system, the two-electron valence-only ion. Our {{results indicate that the}} portability is good in most cases, hence the use of pseudopotentials in quantum Monte Carlo simulations is in general a reasonable approximation but suggest that in some cases this approximation may be relevant...|$|R
40|$|MATH 77, Release 3. 17, is {{library of}} 412 FORTRAN 77 subprograms {{for use in}} {{numerical}} computation. Subprograms providing machine and system characteristic parameters make library operational on any computer system supporting full FORTRAN 77 <b>standard.</b> <b>Portability</b> and high quality of subprograms and user's manual make MATH 77 extremely versatile and valuable tool for all numerical computation applications. Written in FORTRAN 77. Program and documentation copyrighted products of California Institute of Technology...|$|R
40|$|In this paper, {{some of the}} {{weaknesses}} of the current software DSM systems are identified. Among these weaknesses are the lack of user friendly network management functions, limited performance on high performance networks, limited <b>portability,</b> and no <b>standard</b> APIs. OpenMP is proposed as a standard API for future software DSM systems and the network management subsystem and network layer of the Balder system is described {{as an example of}} an infrastructure of a future DSM system...|$|R
40|$|PCTE, Portable Common Tool Environment, is an {{interface}} standard. The interface {{is designed}} to support program <b>portability</b> Specification (<b>Standard</b> ECMA- 149), are designed particularly to provide an infrastructure for programs which {{may be part of}} environments supporting systems engineering projects. Such programs, which are used as aids to systems development, are often referred to as tools. CDIF, CASE Data Interchange Format, is a standard of the Electronic Industries Association. CDIF was defined primarily as a standard for exchanging models between CASE tools. Since it is necessary for parties that exchange models to have a common understanding of them, CDIF is not just a standard for a transfer format, but also for an integrated meta-model (schema) of the data and process models that can be exchanged. This harmonises the concepts of different methods and viewpoints, making CDIF independent of particular methods and tools...|$|R
40|$|One of {{the most}} {{important}} topics in parallel application development is <b>portability.</b> No <b>standard</b> parallel computational model exists, whereas the multitude of different parallel architectures, programming paradigms and methodologies introduces significant difficulty in developing multi-system parallel applications. In this paper we present a platform-based architecture for developing portable parallel applications through the use of the platform approach. A platform is built on top of the native operating systems offering a common application programming interface (API). The API consists of a set of primitives through which portable applications can be developed (since only the platform - not the applications - requires re-programming for porting to different architectures). Furthermore, we define a set of key primitives specifically designed for implementing database applications. Our work towards this direction has been triggered by our recent implementation experience on parallel platforms and our interest in developing a parallel database system...|$|R
40|$|Reuse {{of record}} except for {{individual}} research requires license from Congressional Information Service, Inc. Prepared {{for the use}} of the Committee on Ways and Means, U. S. House of Representatives. At head of title: Committee print. CIS Microfiche Accession Numbers: CIS 73 H 782 - 45 (pt. 1), CIS 73 H 782 - 49 (pt. 2) Pt. 1. Participation, vesting, funding, <b>portability,</b> insurance, fiduciary <b>standards,</b> reporting and disclosure, and enforcement. Pt. 2 : Individual retirement accounts, limitations on contributions, and lump sum distributions. Microfiche. Mode of access: Internet...|$|R
40|$|Abstract [...] This paper {{reviews the}} {{concepts}} of a theory of modelling and simulation {{that relate to the}} validation enterprise. The theory provides a vocabulary, concepts, and mathematically rigorous tools with which to tackle problems in simulation model validation. We have implemented a hierarchical, distributed, object-oriented, and knowledge-based modelling and simulation environment in Ada. The environment, DEVS-Ada, provides <b>portability,</b> a <b>standard</b> model specification language, the means to manage a model repository, the ability to reuse models, and distributed simulation. We show how DEVS-Ada can exploit the parallelism intrinsic in the multiple execution of simultaneous experiments required for model validation. Faster execution of such a computationally intensive process reduces the bottleneck that validation imposes in the model development process and enables greater confidence to be achieved in the results. We also discuss the desirability of a more global view of validation which requires parallel symbolic analysis of a newly created model relative to existing models in a model base...|$|R
40|$|In {{this paper}} we first outline {{and discuss the}} issues of {{currently}} accepted computational models for hybrid CPU/FPGA systems. Then, we discuss the need for researchers to develop new high-level programming models, and not just focus on extensions to programming languages, for enabling accessibility and <b>portability</b> of <b>standard</b> high level applications across the CPU/FPGA boundary. We then present hthreads, a unifying programming model for specifying application threads running within a hybrid CPU/FPGA system. Threads are specified from a single pthreads (POSIX threads) multithreaded application program and compiled to run on the CPU or synthesized to run on the FPGA. The hthreads system, in general, is unique within the reconfigurable computing community as it abstracts the CPU/FPGA components into a unified custom threaded multiprocessor architecture platform. A hardware thread interface (HWTI) component has been developed that provides an abstract, platform-independent compilation target. Thus, the HWTI {{enables the use of}} standard thread communication and synchronization operations across the software/hardware boundary. ...|$|R
40|$|In large (space) {{programmes}} {{the road}} from feasibility study to operations and/or training may be long and demanding. During a typical programme many application models will be developed and usually different development environments are used during different phases of a programme. The Model-Oriented Software Automatic Interface Converter (MOSAIC) automates transfer of simulation models from MATLAB ® /Simulink ® /Stateflow ® to both EuroSim and to ESA’s Simulation Model <b>Portability</b> (SMP) <b>standard.</b> Updates of Commercial Off-The-Shelf (COTS) tools and model standard definitions result in an upgrade of MOSAIC. Developments of MOSAIC are driven by customer requests. The MOSAIC product is developed and maintained by the National Aerospace Laboratory NLR. MOSAIC already is an important ingredient of various space development projects like ATV, SMART-I and GSSF. It has been experienced {{that the use of}} MOSAIC reduces costs, time, and effort in simulator development. In this paper the wide applicability and the use of MOSAIC is discussed...|$|R
40|$|Abstract: Communication is an {{important}} but difficult aspect of parallel programming. This paper describes a parallel communication infrastructure, based on remote method invocation, to simplify parallel programming by abstracting lowlevel shared-memory or message passing details while maintaining high performance and <b>portability.</b> STAPL, the <b>Standard</b> Template Adaptive Parallel Library, builds upon this infrastructure to make communication transparent to the user. The basic design is discussed, {{as well as the}} mechanisms used in the current Pthreads and MPI implementations. Performance comparisons between STAPL and explicit Pthreads or MPI are given on a variety of machines, including an HP-V 2200, Origin 3800 and a Linux Cluster. ...|$|R
40|$|Cloud {{computing}} {{offers an}} innovative business model for organizations to adopt IT services at a reduced cost with increased reliability and scalability. However organizations are slow in adopting the cloud model {{due to the}} prevalent vendor lock-in issue and challenges associated with it. While the existing cloud solutions for public and private companies are vendor locked-in by design, their existence is subject to limited possibility to interoperate with other cloud systems. In this paper we have presented a critical review of pertinent business, technical and legal issues associated with vendor lock-in, and how it impacts on the widespread adoption of cloud computing. The paper attempts {{to reflect on the}} issues associated with interoperability and portability, but with a focus on vendor lock-in. Moreover, the paper demonstrates the importance of interoperability, <b>portability</b> and <b>standards</b> applicable to cloud computing environments along with highlighting other corporate concerns due to the lock-in problem. The outcome of this paper provides a foundation for future analysis and review regarding the impact of vendor neutrality for corporate cloud computing application and services...|$|R
40|$|A Territorial Framing Plan (TFP) is an {{open and}} {{cooperative}} system, outlining a reference model that allows describing social, economical and cultural phenomena coming into the plan and their actual manifestations. In the light of current research insights and available technology, it appears that a TFP may be adequately modeled through a Federated Information System approach. In this paper we will discuss {{the implementation of a}} platform based on software agents for federated information systems. The platform has been developed trying to fulfil the FIPA specifications with special emphasis on: <b>portability,</b> use of <b>standard</b> protocols both for intra and extra platform communication, autonomy and dinamicity, support of cooperation among agents...|$|R
30|$|Vendor lock-in {{is a major}} {{barrier to}} the {{adoption}} of cloud computing, {{due to the lack of}} standardization. Current solutions and efforts tackling the vendor lock-in problem are predominantly technology-oriented. Limited studies exist to analyse and highlight the complexity of vendor lock-in problem in the cloud environment. Consequently, most customers are unaware of proprietary standards which inhibit interoperability and portability of applications when taking services from vendors. This paper provides a critical analysis of the vendor lock-in problem, from a business perspective. A survey based on qualitative and quantitative approaches conducted in this study has identified the main risk factors that give rise to lock-in situations. The analysis of our survey of 114 participants shows that, as computing resources migrate from on-premise to the cloud, the vendor lock-in problem is exacerbated. Furthermore, the findings exemplify the importance of interoperability, <b>portability</b> and <b>standards</b> in cloud computing. A number of strategies are proposed on how to avoid and mitigate lock-in risks when migrating to cloud computing. The strategies relate to contracts, selection of vendors that support standardised formats and protocols regarding standard data structures and APIs, developing awareness of commonalities and dependencies among cloud-based solutions. We strongly believe that the implementation of these strategies has a great potential to reduce the risks of vendor lock-in.|$|R
30|$|Another problem emerged {{when the}} {{software}} was evaluated by using several criteria (including functionality, reliability, usability, <b>portability,</b> e-learning <b>standards</b> support, security and privacy, vendor criteria, and learner’s communication environment). Each piece of software has several attributes, and each decision maker has different weights for these attributes. Thus, selecting the suitable software to use is difficult. On one hand, users who aim {{to use one}} kind of software may prioritize functionality, usability, and user support rather than other features, whereas users who intend to develop this software in actual education environments would probably target different attributes. On the other hand, LMS package selection (in particular, OSS) is an MADM/MCDM problem where each type of software is considered an available alternative for the decision maker. In other words, the MADM/MCDM problem refers to making preference decisions over the available alternatives that are characterized by multiple and usually conflicting attributes (Zaidan et al. 2014). The process of selecting the OSS-LMS packages involves the simultaneous consideration of multiple attributes to rank the available alternatives and select the best one. Thus, the selection process of the OSS-LMS packages {{can be considered a}} multi-criteria decision-making problem. Additional details of the fundamental terms of software selection based on multi-criteria analysis will be provided in the following section.|$|R
40|$|Vendor lock-in {{is a major}} {{barrier to}} the {{adoption}} of cloud computing, {{due to the lack of}} standardization. Current solutions and efforts tackling the vendor lock-in problem are predominantly technology-oriented. Limited studies exist to analyse and highlight the complexity of vendor lock-in problem in the cloud environment. Consequently, most customers are unaware of proprietary standards which inhibit interoperability and portability of applications when taking services from vendors. This paper provides a critical analysis of the vendor lock-in problem, from a business perspective. A survey based on qualitative and quantitative approaches conducted in this study has identified the main risk factors that give rise to lock-in situations. The analysis of our survey of 114 participants shows that, as computing resources migrate from on-premise to the cloud, the vendor lock-in problem is exacerbated. Furthermore, the findings exemplify the importance of interoperability, <b>portability</b> and <b>standards</b> in cloud computing. A number of strategies are proposed on how to avoid and mitigate lock-in risks when migrating to cloud computing. The strategies relate to contracts, selection of vendors that support standardised formats and protocols regarding standard data structures and APIs, developing awareness of commonalities and dependencies among cloud-based solutions. We strongly believe that the implementation of these strategies has a great potential to reduce the risks of vendor lock-in...|$|R
50|$|Portability was {{a problem}} in the early days because there was no agreed {{standard}} not even IBM's reference manual and computer companies vied to differentiate their offerings from others by providing incompatible features. <b>Standards</b> have improved <b>portability.</b> The 1966 <b>standard</b> provided a reference syntax and semantics, but vendors continued to provide incompatible extensions. Although careful programmers were coming to realize that use of incompatible extensions caused expensive portability problems, and were therefore using programs such as The PFORT Verifier, it was not until after the 1977 standard, when the National Bureau of Standards (now NIST) published FIPS PUB 69, that processors purchased by the U.S. Government were required to diagnose extensions of the standard. Rather than offer two processors, essentially every compiler eventually had at least an option to diagnose extensions.|$|R
40|$|The <b>portability</b> of the <b>Standard</b> Shiftwork Index (SSI) and {{the impact}} of {{shiftwork}} was assessed on a sample of television production employees in Auckland, New Zealand. Sixty three respondents completed the SSI and reported a moderate impact of shiftwork on physical and psychological health and moderately high sleep disturbance. Social and domestic life yielded the greatest detrimental impact. Gender related coping strategies was the only significant difference within the sample. Chronic fatigue, somatic anxiety, general job satisfaction and disengagement were significantly related to intention to leave the organisation. Statistical analysis of effect size indicated equivalent levels of power in both the U. K and the present sample. Overall, the results for the present sample were comparable to the U. K sample, indicating the portability of the SSI to the present sample. Organisational restructuring was considered a potential moderator of the overall moderate impact of shiftwork on the sample...|$|R
40|$|As a {{technical}} demonstration {{project for the}} NASA Advanced Communications Technology Satellite (ACTS), we have implemented remote observing on the 10 -meter Keck II telescope on Mauna Kea in Hawaii from the California Institute of Technology campus in Pasadena. The data connection consists of optical fiber networks in Hawaii and California, connecting the end-points to high data rate (HDR) ACTS satellite antennae at JPL in Pasadena and at the Tripler Army Medical Center in Honolulu. The terrestrial fiber networks run the asynchronous transfer mode (ATM) protocol at DS- 3 (45 Mbit/sec) speeds, providing ample bandwidth to enable remote observing with a software environment identical to that used for on-site observing in Hawaii. This experiment has explored the data requirements of remote observing with a modern research telescope and large-format detector arrays. While the maximum burst data rates are lower than those required for many other applications (e. g., HDTV), the network reliability and data integrity requirements are critical. As we show in this report, the former issue particularly may be the greatest challenge for satellite networks for this class of application. We have also experimented with the <b>portability</b> of <b>standard</b> TCP/IP applications to satellite networks, demonstrating the need for alternative TCP congestion algorithms and minimization of bit error rates (BER). Reliability issues aside, we have demonstrated that true remote observing over high-speed networks provides several important advantages over standard observing paradigms. Technical advantages of the high-speed network access include more rapid download of data to a user's home institution and the opportunity for alternative communication facilities between members of an observing team, such as audio- and videoconferencing...|$|R
40|$|The {{requirement}} for more dependable embedded supercomputing systems is usually {{dealt with by}} resorting {{to one of the}} following two different approaches: either by fault tolerance (FT) measures taken at operating system level (with mediocre, however results due to the general - purpose mechanisms applied), or by fault tolerance measures integrated at application level (with considerable development effort incurred and, furthermore, resulting in non - reusable solutions). In this dissertation a middle way approach is examined: the development of an adaptable, scalable, portable, maintainable and cost-effective software framework to integrate fault tolerance in embedded supercomputer applications, which lies between the application and the operating system (as middleware). This way the application developer is free to select the FT level that he prefers and tailor the FT mechanisms to his application. In the presented approach standard constituent phases of fault tolerance can be indentified: error detection, error diagnosis and isolation, system reconfiguration and error recovery. With extendibility and maintainability in mind, the framework follows a layered software architecture approach, while for probability reasons the framework is built as a core block with well - defined interfaces and some adaption layers. The layered architecture is mantained throughout the framework core, as well. As a result, the framework consists {{of a wide variety of}} independent reusable FT modules acting at different levels and coupled to a central module (the intelligent backbone), while all modules cooperate closely for the realization of fault tolerance in favor of the application level above. Since this framework has been ported to heterogeneous systems, all platform dependent aspects are well hidden from the user-developer and <b>portability</b> to <b>standard</b> platforms and kernels is guaranteed. ...|$|R
40|$|VISART is a {{standardized}} format for postprocessor files with data calculated by fluid-dynamics and similar codes over a multi-dimensional geometry and time. VISART files primarily {{provide for the}} graphical {{evaluation of the results}} in a separate run, but they do not include graphics control information. The report states the geometrical and other assumptions for which the standard was developped and defines the standardized structure of the postprocessor files in the VISART format. The standard provides for regular and irregular, both full or defective, mesh geometries. The calculated quantities can be stored over all mesh cells, at selected cells or points or as space-independent quantities, and this at arbitrarily specified problem times. The VISART file contents, i. e. the total number, choice, kind etc. of the stored quantities, is not determined by the standard but by the code generating the file. Thus, by its flexibility and <b>portability</b> the VISART <b>standard</b> is suitable {{for a wide range of}} applications. (orig.) Available from TIB Hannover: ZA 5141 (5996) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
