17|6|Public
25|$|The <b>line</b> <b>finder</b> {{was also}} {{improved}} during the system's lifetime. Originally, the <b>line</b> <b>finder</b> frame had {{a capacity of}} 300 lines each, and used 15 brushes (vertical hunting segments) on each rod. This was intended to reduce hunting time as there were more brushes hunting over a shorter distance. As these line finders went into service, however, it became evident that 15 brushes on each vertical selector rod were quite heavy, and needed springs and pulleys {{at the top of}} the frame to compensate for their mass. Later line finders used 10 brushes and rearranged the layout to accommodate 400 lines per <b>line</b> <b>finder</b> frame. This increased capacity while eliminating the need for compensating equipment.|$|E
25|$|Many of {{the urban}} and {{commercial}} areas where Panel was first used had message rate service rather than flat rate calling. For this reason the <b>line</b> <b>finder</b> had, besides the tip and ring leads for talking and the sleeve lead for control, a fourth wire for the District circuit to send metering pulses to control the message register. The introduction of direct distance dialing (DDD) in the 1950s required the addition of automatic number identification equipment for centralized automatic message accounting.|$|E
5000|$|... #Caption: Western Electric {{friction}} drive <b>Line</b> <b>Finder</b> (No. 7001 type) ...|$|E
50|$|In Christchurch, New Zealand at the Ferrymead Heritage Park, the Ferrymead Post & Telegraph Historical Society has {{a working}} exhibit of the 7A Rotary Switching system. The display {{includes}} a bay of 7A1 <b>Line</b> <b>Finders</b> and a bay of 7A1 Registers.|$|R
50|$|Initial {{deployments}} {{were based}} on step-by-step equipment such as the Western Electric 350A (first deployed on May 27th, 1928 in Arcadia, California) and its successor models 360A, 355A and 356A. These switches had some design similarities to No. 1 step-by-step systems already in use in large offices (10000 subscribers or more) but were of a simpler design (<b>line</b> <b>finders</b> would only need to scan hundreds of lines instead of the thousands in use in the cities) and not designed for expandability.|$|R
40|$|A new {{algorithm}} {{for finding}} lines in images under a bounded error noise model is described. The algorithm {{is based on}} a hierarchical and adaptive subdivision of the space of line parameters, but, unlike previous adaptive or hierarchical <b>line</b> <b>finders</b> based on the Hough transform, measures errors in image space and thereby guarantees that no solution satisfying the given error bounds will be lost. In addition, the algorithm can find interpretations of all the lines in the image that satisfy the constraint that each image feature supports at most one line hypothesis [...] a constraint that is often useful to impose in practice. The algorithm can be extended to compute the probabilistic Hough transform and the generalized Hough transform a variety of statistical error models efficiently...|$|R
50|$|The <b>line</b> <b>finder</b> {{was also}} {{improved}} during the system's lifetime. Originally, the <b>line</b> <b>finder</b> frame had {{a capacity of}} 300 lines each, and used 15 brushes (vertical hunting segments) on each rod. This was intended to reduce hunting time as there were more brushes hunting over a shorter distance. As these line finders went into service, however, it became evident that 15 brushes on each vertical selector rod were quite heavy, and needed springs and pulleys {{at the top of}} the frame to compensate for their mass. Later line finders used 10 brushes and rearranged the layout to accommodate 400 lines per <b>line</b> <b>finder</b> frame. This increased capacity while eliminating the need for compensating equipment.|$|E
5000|$|... #Caption: Western Electric 7A Rotary, {{friction}} drive (Bird-cage), No. 7001 <b>Line</b> <b>Finder.</b> Note the driven bevel gear {{on the right-hand}} side; this type has a steady rotary motion and does not employ an electromagnet for stepping.|$|E
5000|$|... #Caption: An {{example of}} a Western Electric 7A Rotary (Bird-cage) <b>Line</b> <b>Finder</b> assembly. The {{horizontal}} shaft is driven by a gear and when the Line Finder's electromagnet is energized, a flexible disc {{at the base of}} the Line Finder's brush carriage is engaged through friction to the horizontal shaft's driving disc, causing the brush carriage to rotate.|$|E
50|$|In 2012 she {{published}} {{her fifth}} novel, Menáge (Other Press), a comedy of manners satirizing {{the rich and}} the literary life; and a collection of essays, A Marriage Agreement and Other Essays: Four Decades of Feminist Writing (Open Road). In addition, she has written two books on anarchist-feminist Emma Goldman: the biography To The Barricades (T.Y.Crowell, 1971) and Red Emma Speaks: An Emma Goldman Reader (Random House, 1972). Except for her three children's books (Bosley on the Number <b>Line</b> 1970, <b>Finders</b> Keepers Press, 1971, Awake or Asleep Wesley, 1971), all her titles are available as ebooks.|$|R
40|$|ERAF: A 3. P 34 These Instructions {{are issued}} by the Ministry of Transport for the {{guidance}} of their Marine Surveyors in surveying passenger ships under {{the provisions of the}} Merchant Shipping Acts. They also indicate to shipowners, shipbuilders and others concerned the procedure which the Department adopts for the survey of passenger ships, and the conditions under which Passenger and Safety Certificates and Passenger Certificates are issued. These instructions should be read in conjunction with the other volumes of the Ministry’s Instructions to their Marine Surveyors on the subject of life-saving appliances, fire appliances, load <b>line,</b> radio, direction <b>finders,</b> lights, shapes and sound Signals, and pilot ladders. Separate publications are also available covering the statutory provisions for tonnage measurement, crew accommodation, musters and drills and the marking, stowage and carriage of dangerous goods...|$|R
40|$|This paper {{presents}} an improvement for an automatic extrinsic parameter calculation between a monocular camera and single <b>line</b> laser range <b>finder.</b> The {{focus of this}} work is a further reduction of calibration errors compared to other existing methods, and the unique and automatic identification and detection of a calibration object under usage of the remission measurements of the laser scanner data. The use of the remission measurements leads to a reduction of the edge effect, which leads to error measurements {{at the edges of}} a measured object, if distance measurements are used exclusively. Further, the usage of the remission measurements enables the unique identification of a new calibration object with a significant remission signature. The result is an automatic extrinsic parameter calculation which works reliably with a high accuracy. The performance of the proposed method was verified in simulation and experiments. In order to classify the results of simulation and experiments, they were compared to other existing methods...|$|R
50|$|Many of {{the urban}} and {{commercial}} areas where Panel was first used had message rate service rather than flat rate calling. For this reason the <b>line</b> <b>finder</b> had, besides the tip and ring leads for talking and the sleeve lead for control, a fourth wire for the District circuit to send metering pulses to control the message register. The introduction of direct distance dialing (DDD) in the 1950s required the addition of automatic number identification equipment for centralized automatic message accounting.|$|E
50|$|For {{outgoing}} calls, the Line Link Frame {{acted like}} the <b>line</b> <b>finder</b> {{of the panel}} switch, autonomously connecting the line to a junctor, which corresponded to the cord circuit of the old cord telephone switchboard. As in the panel switch, the sender then found the chosen junctor and supplied dial tone. Like the panel switch, the 1XB common control {{was based on a}} complex, versatile sender circuit. The sender decoded dial pulses and was retrofitted for dual-tone multi-frequency signaling (DTMF) in the 1960s. A large number of senders used a common translator circuit to detect a call going to a nearby area code to be stored in abbreviated form. It called in an auxiliary sender when necessary to implement Direct Distance Dialing (DDD). Like the panel switch, two or more offices with separate incoming sections could share an outgoing section for more efficient trunking. Unlike the panel switch, it was rare to combine more than two this way.|$|E
40|$|International audienceL 1 Chimeric Transcripts (LCTs) are {{initiated}} by repeated LINE- 1 element antisense promoters and include the L 1 5 'UTR sequence in antisense orientation {{followed by the}} adjacent genomic region. LCTs have been characterized mainly using bioinformatics approaches to query dbEST. To take advantage of NGS data to unravel the transcriptome composition, we developed Chimeric <b>LIne</b> <b>Finder</b> (CLIFinder), a new bioinformatics tool. Using oriented paired-end RNA-seq data, we demonstrated that CLIFinder can identify genome-wide transcribed chimera sequences corresponding to potential LCTs. Moreover, CLIFinder {{can be adapted to}} study transcription from other repeat types...|$|E
40|$|A novel {{approach}} to junction detection using an explicit <b>line</b> <b>finder</b> model and contextual rules is presented. Contextual rules expressing properties of 3 D-edges (surface orientation discontinuities) {{limit the number}} of line intersections interpreted as junctions. Probabilistic relaxation labelling scheme is used to combine the a priori world knowledge represented by contextual rules and the information contained in observed lines. Junctions corresponding to a vertex (V-junctions) and an occlusion (T-junctions) of a 3 D object are detected and stored in a junction graph. The information in the junction graph is used to extract higher level features. Results of the most promising method, the polyhedral object face recovery, are briefly discussed. The performance of the junction detection process is demonstrated on images from indoor, outdoor, and industrial environments. ...|$|E
40|$|Road {{information}} {{is essential for}} automatic GIS (geographical information system) data acquisition, transportation and urban planning. Automatic road (network) detection from high resolution satellite imagery will hold great potential for significant reduction of database development/updating cost and turnaround time. From so called low level feature detection to high level context supported grouping, so many algorithms and methodologies have been presented for this purpose. There is not any practical system that can fully automatically extract road network from space imagery {{for the purpose of}} automatic mapping. This paper presents the methodology of automatic main road detection from high resolution satellite IKONOS imagery. The strategies include multiresolution or image pyramid method, Gaussian blurring and the <b>line</b> <b>finder</b> using 1 -dimemsional template correlation filter, line segment grouping and multi-layer result integration. Multi-layer or multi-resolution method for road extraction is a very effective strategy to save processing time and improve robustness. To realize the strategy, the original IKONOS image is compressed into different corresponding image resolution so that an image pyramid is generated; after that the <b>line</b> <b>finder</b> of 1 -dimemsional template correlation filter after Gaussian blurring filtering is applied to detect the road centerline. Extracted centerline segments belong to or do not belong to roads. There are two ways to identify the attributes of the segments, the one is using segment grouping to form longer line segments and assign a possibility to the segment depending on the length and other geometric and photometric attribute of the segment, for example the longer segment means bigger possibility of being road. Perceptual-grouping based method is used for road segment linking by a possibility model that takes multi-information into account; here the clues existing in the gaps are considered. Another way to identify the segments is feature detection back-to-higher resolution layer from the image pyramid...|$|E
40|$|This report {{describes}} {{the implementation of}} convex line segment grouper proposed by David W. Jacobs [9]. The inputs of this algorithm are line segments obtained by edge detector and straight <b>line</b> <b>finder.</b> Definition of mutually convex lines(MCL) given by Daniel P. Huttenlocher and Peter C. Wayer[10] was adopted. Mathematical expressions of these definitions will be give. All convex groups satisfying the length-gap ratio and the angle constraints will be found. Keywords [...] - Perceptual grouping, convexity, proximity, non-accidental properties. I. Introduction P ERCEPTUAL grouping is an important method for abstracting meaningful organizations from observed visual data. It clusters low level image features to higher level geometric primitives and further to models of objects. The theoretical basis of perceptual grouping {{can be traced back}} to Gestalt psychologists' work[1][2][3]. They stated that it's not the separate parts of the image but their relative relationships that dramatically [...] ...|$|E
40|$|In recent years, airport runway {{extraction}} {{has become}} increasingly important for various engineering applications. Existing approaches for airport runway extraction primarily focus on locating the airport roughly, i. e., determining whether an airport is present or not, but not delineating the airport runway accurately. This study develops a novel method for semiautomatic airport runway extraction from Google earth images by integrating a long straight <b>line</b> <b>finder</b> and a region-based level set evolution (LSE). Specifically, we start by detecting the long straight lines that most likely represent airport runway boundaries in the original images. Then, based on the extracted lines, we propose a method for semiautomatic generation of initial level curves for the LSE. Furthermore, for accurate extraction of the entire airport runways, a fast region-based LSE is used to evolve the initial level curves toward the desired boundaries. Experiments validate that the proposed method is capable of semiautomatically extracting objects with complex geometrical shapes and topological structures from challenging backgrounds. Compared with other state-of-the-art approaches, the proposed method has much fewer parameters and is more computationally efficient while achieving object extraction accuracy comparable to other approaches. Department of Land Surveying and Geo-Informatic...|$|E
40|$|This paper {{describes}} an approach for {{the recognition of}} invalidation lines and its use to summarize the valid information in structured text. This approach is applied to an interactive user interface to assist the capture of data sets from scanned commercial registers into a data base system. Invalidation lines are hand drawn lines below the invalid words or text lines (originally in red, but black in scanned binary images). In a first step segmentation of the text objects (block, line, word, character) is performed based on a fast connected component analysis using sub-components, which is robust against touching lines. In a second step horizontal lines are localized with a run-length-based <b>line</b> <b>finder.</b> Invalidation is performed on a word or text line level based on the neighborhood relation between text objects and invalidation lines. In several hundred pages of commercial registers the invalidations are recognized with about 90 % accuracy at about 10 % rejection threshold. The error rate (i. e. invalidation of a valid word) is less than 0. 5 %. For most data sets it is sufficient to eliminate the invalidated text from further OCR and syntactical analysis, so the summarization is already finished on the layout level. For other data sets the valid data fields have to be augmented by data fields from earlier invalidated entries, which is performed after OCR using syntactical analysis and string matching. 1...|$|E
40|$|Airglow {{emission}} lines, which {{dominate the}} optical-to-near-IR sky radiation, show strong, line-dependent variability on various time scales. Therefore, the subtraction {{of the sky}} background in the affected wavelength regime becomes a problem if plain sky spectra {{have to be taken}} at a different time as the astronomical data. A solution of this issue is the physically motivated scaling of the airglow lines in the plain sky data to fit the sky lines in the object spectrum. We have developed a corresponding instrument-independent approach based on one-dimensional spectra. Our code skycorr separates sky lines and sky/object continuum by an iterative approach involving a <b>line</b> <b>finder</b> and airglow line data. The sky lines are grouped according to their expected variability. The line groups in the sky data are then scaled to fit the sky in the science data. Required pixel-specific weights for overlapping groups are taken from a comprehensive airglow model. Deviations in the wavelength calibration are corrected by fitting Chebyshev polynomials and rebinning via asymmetric damped sinc kernels. The scaled sky lines and the sky continuum are subtracted separately. VLT X-Shooter data covering time intervals from two minutes to about one year were selected to illustrate the performance. Except for short time intervals of a few minutes, the sky line residuals were several times weaker than for sky subtraction without fitting. Further tests show that skycorr performs consistently better than the method of Davies (2007) developed for VLT SINFONI data. Comment: 17 pages, 18 figures, accepted for publication in A&...|$|E
40|$|International audienceWe present SELFI, the Source Emission <b>Line</b> <b>FInder,</b> a new Bayesian method {{optimized}} for detection of faint galaxies in Multi Unit Spectroscopic Explorer (MUSE) deep fields. MUSE {{is the new}} panoramic integral field spectrograph at the Very Large Telescope (VLT) that has unique capabilities for spectroscopic investigation of the deep sky. It has provided data cubes with 324 million voxels over a single 1 arcmin 2 field of view. To address the challenge of faint-galaxy detection in these large data cubes, we developed a new method that processes 3 D data either for modeling or for estimation and extraction of source configurations. This object-based approach yields a natural sparse representation of the sources in massive data fields, such as MUSE data cubes. In the Bayesian framework, the parameters that describe the observed sources are considered random variables. The Bayesian model leads to a general and robust algorithm where the parameters are estimated in a fully data-driven way. This detection algorithm {{was applied to the}} MUSE observation of Hubble Deep Field-South. With 27 h total integration time, these observations provide a catalog of 189 sources of various categories and with secured redshift. The algorithm retrieved 91 % of the galaxies with only 9 % false detection. This method also allowed the discovery of three new Lyα emitters and one [OII] emitter, all without any Hubble Space Telescope counterpart. We analyzed the reasons for failure for some targets, and found that the most important limitation of the method is when faint sources are located in the vicinity of bright spatially resolved galaxies that cannot be approximated by the Sérsic elliptical profile...|$|E
40|$|PolyU Library Call No. : [THS] LG 51. H 577 P LSGI 2016 Lixxv, 235 pages :color illustrationsExtracting {{geographic}} objects {{has long}} been an essential research topic in geographical sciences and remote sensing. Past research has been primarily focused on tackling specific types of objects by using particular features, and little {{attention has been paid to}} the following three aspects: 1) developing more operational methods that can be applied to handle multiple types of objects by exploring more generic features; 2) improving the computational efficiency of existing methods; and 3) increasing the degree of automation of object extraction to reduce the load on users. Thus, object extraction from remote sensing images remains challenging. In recent years, level set evolution (LSE) has proven effective at object extraction. It can handle topological changes automatically while achieving high accuracy. However, the application of the state-of-the-art LSE methods is compromised by laborious parameter tuning and expensive computation. For these reasons, two fast LSE methods are proposed to extract anthropogenic objects from high spatial resolution remote sensing images. The significant advantages of the proposed LSE methods are as follows: 1) two novel data terms (one is an edge-based and the other is a region-based) are proposed based on the conventional LSE methods and they are more practical and efficient than the existing approaches; 2) the traditionally used mean curvature-based regularization term is replaced by a Gaussian kernel in the proposed methods, which makes it possible to use a larger time step in the numerical scheme, thus expediting the proposed methods considerably; 3) for computational efficiency, the level set function in the proposed methods is initialized as a binary function rather than the traditionally used signed distance function; and 4) a long straight <b>line</b> <b>finder</b> and a change detection technique are introduced into the proposed LSE methods, automating the proposed methods substantially. Although LSE methods have shown promising performance in homogeneous object extraction, they generally have difficulty in handling heterogeneous objects as they only take advantage of the intensity (spectral) information. To develop a more general and reliable object extraction system, an enhanced binary Markov random field (MRF) is thus proposed, which takes advantage of both the spectral and spatial contextual information of objects of interest simultaneously. The principal novelties of the proposed method are as follows. First, a new mixture model (MM) is proposed for spectral learning. Compared to the existing Gaussian mixture model (GMM), the proposed MM is more robust to objects that do not follow the trained Gaussian distributions. Then, for computational efficiency, the parameters in MM are estimated by using tree-structured vector quantizer (TSVQ) instead of the expectation-maximization (EM) algorithm widely used in GMM. Next, a morphology-based post-processing mechanism is particularly devised to improve its performance. Finally, the proposed enhanced MRF is automated by using a multi-threshold method and is then applied to automatic burned area mapping from Landsat 8 images. The efficiency and accuracy of the proposed methods (both LSE methods and the enhanced binary MRF) are finally corroborated by a wide range of experiments. Compared with other state-of-the-art approaches, they have following significant advantages: 1) they are capable of dealing with multiple types of objects (including both anthropogenic and natural objects) effectively, 2) they can achieve much better performance, 3) they are computationally much more efficient, and 4) they are operational and reliable in real applications due to less parameter tuning and appropriate manual interaction. Department of Land Surveying and Geo-InformaticsPh. D., Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, 2016 Doctorat...|$|E

