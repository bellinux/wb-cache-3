20|4463|Public
40|$|Activity-based cost management: A {{tool for}} {{survival}} The measurement of processes and costs {{is necessary to}} demonstrate quality to the purchasers of home health services. An activity-based cost management (ABCM) system will provide the vehicle to <b>link</b> <b>process</b> cost to payers, product lines, clinical pathways, and agency processes...|$|E
40|$|This paper {{describes}} on-going {{work being}} carried out under the Rapid Knowledge Formation (RKF) program. The representation of process and plan knowledge within the Cyc ontology is the central focus of this paper. Within the RKF program, we are also concerned with the advanced knowledge elicitation techniques that are main theme of the program as a whole. We begin by discussing {{the inclusion of the}} Process Specification Language, being developed by NIST, in the Cyc ontology[4, 5], then briefly introduce an alternative approach to process representation based on Scripts. Finally, we <b>link</b> <b>process</b> representation to planning...|$|E
40|$|A grain {{comminution}} {{process is}} based on a gradual size reduction approach, through repeated milling and sieving units that increase the flour yield. In this study we use latent variable modeling techniques to <b>link</b> <b>process</b> parameters and grain properties to the final product quality. Using experimental data, it is shown how the use of models in their direct form allows one to improve process understanding and to predict the product quality from the process settings and grain properties. Additionally, it is shown that, by inverting the latent variable models, the optimal combination of process parameters and grain properties leading to a desired product quality can be determined...|$|E
50|$|A GOP {{involves}} {{two or more}} <b>linked</b> <b>process</b> elements.|$|R
5000|$|Control link, {{which is}} a {{procedural}} (transforming or enabling) link with a control modifier—the letter e (for event) or c (for condition), which adds semantics of a control element. The letter e signifies an event for triggering the <b>linked</b> <b>process,</b> while the letter c signifies a condition for execution of the <b>linked</b> <b>process,</b> or connection of two processes denoting invocation, or exception.|$|R
40|$|The <b>linking</b> <b>process</b> of {{information}} to ICF is a common task in different strategies used in rehabilitation practise but is a time consuming process mainly due to reliability issues. This work aims to developed additional rules to those already published {{in order to improve}} reliability of the <b>linking</b> <b>process</b> to ICF. The results are encouraging and these work could help to develop in formation technologies tools for facilitate this process...|$|R
40|$|In this paper, we {{demonstrate}} a methodology to <b>link</b> <b>process</b> parameters to BSIM model parameters. Here, we have combined well-known statistical methods like {{principal component analysis}} (PCA), design of experiments (DOE), and response surface methodology (RSM) to bridge the missing link between process parameters and model parameters. The proposed methodology uses {{the concept of a}} correlation matrix, which transforms the process level information to the device and circuit level information through the BSIM model parameters. The proposed methodology has been successfully implemented on an advanced CMOS process. Our results show a strong linear correlation for the data obtained from two techniques namely TCAD technique and the standard HSPICE simulation technique. In both cases the process conditions were kept identical for comparison...|$|E
40|$|The {{electrical}} breakdown performance, either unaged or after ageing (laboratory or service), {{is often used}} as the basis for qualification of a device, design or material. Many of the features that affect these performance levels have been discussed in other documents; contaminants, propensity for water treeing, insulating and semiconducting materials. However the size of cable tested is rarely discussed. This is somewhat surprising as it has been long recognized that electrical failure is an Extreme Value (the Weibull distribution is a member of this family) or weakest <b>link</b> <b>process.</b> In Extreme Value processes the performance of the whole device is determined by the single “weakest link”. Thus when more “weak links ” are present the chance of failure is consequently higher: the measured performanc...|$|E
40|$|International audienceSurface {{topography}} is a {{key element}} between process parameters and manufactured part performances. Within the context of 3 D printed parts, one difficulty is to consider the total 3 D surface topography including internal porosity. In this paper, an original method is proposed for the characterization of the surface topography, both internal and external. Starting from volumetric data obtained by Computed Tomography measurements, a method of surface extraction is performed that identifies skin voxels corresponding to the internal and external part surface,and which are the support to the calculation of the Specific Surface Area (SSA). A multi-scale analysis is thus proposed to characterize the total surface, (SSA), obtained at different scales. The interest of the multi-scale analysis is illustrated through various examples that attempt to <b>link</b> <b>process</b> parameters to part properties...|$|E
50|$|Serial {{extraction}} is a multi-decisional, time <b>linked</b> <b>process.</b> Annual records such as panoramic radiographs, {{photographs and}} study models are essential.|$|R
5000|$|... #Caption: An {{illustration}} of the <b>linking</b> <b>process.</b> Object files and static libraries are assembled into a new library or executable ...|$|R
40|$|Abstract—Speed is a {{great concern}} in the {{recursive}} shortest spanning tree (RSST) algorithm as its applications are focused on image segmentation and video coding, in which {{a large amount of}} data is processed. Several efficient RSST algorithms have been proposed in the literature, but the linking properties are not properly addressed and used in these algorithms and they are intended to produce a truncated RSST. This paper categorizes the <b>linking</b> <b>process</b> into three classes based on link weights. These <b>linking</b> <b>processes</b> are defined as the <b>linking</b> <b>process</b> for <b>link</b> weight equal to zero (LPLW-Z), the <b>linking</b> <b>process</b> for <b>link</b> weight equal to one (LPLW-O), and the <b>linking</b> <b>process</b> for <b>link</b> weight equal to real number (LPLW-R). We study these linking properties and apply them to an efficient RSST algorithm. The proposed efficient RSST algorithm is novel, as it makes use of linking prop-erties, and its resulting shortest spanning tree is truly identical to that produced by the conventional algorithm. Our experimental results show that the percentages of links for the three classes are 17 %, 27 %, and 58 %, respectively. This paper proposes a prediction method for LPLW-O, as a result of which the vertex weight of the next region can be determined by comparing sizes of the merging regions. It is also demonstrated that the pro-posed LPLW-O with prediction approach is applicable to the multiple-stage merging. Our experimental results show that the proposed algorithm has a substantial improvement over the con-ventional RSST algorithm. Index Terms—Edge detection, graph theory, image segmenta-tion, linking property, recursive shortest spanning tree (RSST). I...|$|R
40|$|International audienceLand-use/cover change, climate change, {{sea level}} {{evolution}} {{are examples of}} application {{that are associated with}} change detection. Actually, we use satellite image time series to monitor the change where entities are often dynamic along time. Moreover, knowledge associated to these spatio-temporal objects can evolve when changes occur. Thus, for modeling this kind of knowledge it is necessary to deal with four aspects: spectral, spatial, temporal and semantic. Such approach can be modeled by ontologies in many levels. Thereby, a shared ontology can be an ontology or a combination of some ontologies based on some mechanisms of linking. Such <b>link</b> <b>process</b> should maintain consistency between represented knowledge. In this paper, we propose a multi-level ontological approach for monitoring dynamics in remote sensing images. The proposed methodology aims to link our domain ontology to an upper level ontology thus enabling to represent existing change processes...|$|E
40|$|Abstract—Although process-aware {{information}} {{systems have been}} adopted in enterprises for many years, they still do not properly link the business processes they implement with re-lated enterprise process information (e. g., guidelines, checklists, templates, and e-mails). On one hand, process management technology is used to design, implement, enact, and monitor processes. On the other, enterprise process information is spread over various sources like shared drives, databases, and enterprise {{information systems}}. As a consequence, users often manually <b>link</b> <b>process</b> information with particular process objects (e. g., using process portals). What is needed instead, however is an integrated access to both processes and related enterprise process information. This paper establishes such a link by introducing an integrated navigation space for process model collections and related enterprise process information. In particular, this navigation space allows process participants to flexibly navigate within process model collections, single process models, and related process information. In turn, this enables advanced end-user support for process repositories. I...|$|E
40|$|As {{airports}} are being {{identified as the}} bottleneck of the future ATM system, optimised airport processes have a significant influence on the overall ATM system. Comprehensive {{research in the field}} of the surface movement and scheduling of aircraft has led to technology and support systems that are in place at various airports worldwide. In contrast the coordination and optimisation of service vehicles on the aprons were investigated only insignificantly. Within two projects at the Airport Research Facility Hamburg a concept for managing the process of the luggage transport to and from the aircraft was developed. The concept is based on experiences from A-SMGCS research and implements a complete data <b>link</b> <b>process</b> as a replacement of the actual voice solution. Furthermore a first prototype of a support system was developed, implemented and technically tested at the airport. Live connection to operational systems like A-SMGCS and Airport database enabled the use of real data and first shadow mode trials...|$|E
40|$|Speed is a {{great concern}} in the {{recursive}} shortest spanning tree (RSST) algorithm as its applications are focused on image segmentation and video coding, in which {{a large amount of}} data is processed. Several efficient RSST algorithms have been proposed in the literature, but the linking properties are not properly addressed and used in these algorithms and they are intended to produce a truncated RSST. This paper categorizes the <b>linking</b> <b>process</b> into three classes based on link weights. These <b>linking</b> <b>processes</b> are defined as the <b>linking</b> <b>process</b> for <b>link</b> weight equal to zero (LPLW-Z), the <b>linking</b> <b>process</b> for <b>link</b> weight equal to one (LPLW-O), and the <b>linking</b> <b>process</b> for <b>link</b> weight equal to real number (LPLW-R). We study these linking properties and apply them to an efficient RSST algorithm. The proposed efficient RSST algorithm is novel, as it makes use of linking properties, and its resulting shortest spanning tree is truly identical to that produced by the conventional algorithm. Our experimental results show that the percentages of links for the three classes are 17 %, 27 %, and 58 %, respectively. This paper proposes a prediction method for LPLW-O, as a result of which the vertex weight of the next region can be determined by comparing sizes of the merging regions. It is also demonstrated that the proposed LPLW-O with prediction approach is applicable to the multiple-stage merging. Our experimental results show that the proposed algorithm has a substantial improvement over the conventional RSST algorithm. Department of Electronic and Information Engineerin...|$|R
40|$|Idea {{association}} {{involves a}} dynamic <b>linking</b> <b>process</b> among ideas, cases and the links themselves. Based {{on the knowledge}} representation issue-concept-form proposed by Oxman (1994), design ideas, cases and links are elucidated. Furthermore, various and dynamic linking plays are involved in two steps: divergent, in which alternative idea entities are linked, and convergent, in which these idea entities are selected. These linking plays provide a computational mechanism for indexing prior design cases dynamically. Finally, an index prototype for supporting the <b>linking</b> <b>process</b> of idea association, called Mapmaker, is proposed in this paper...|$|R
5000|$|Weick proposes three {{dynamics}} {{which are}} closely relevant to innovation: [...] "that the knowledge is both tacit and articulated, that <b>linking</b> <b>processes</b> cut across several levels of organizational action, and that <b>linking</b> <b>processes</b> embody several tensions". A study applies these dynamics and organizational sensemaking to analyze how people {{make sense of}} market and technology knowledge for product innovation. Innovative organizations have a system of sensemaking that allows actors to [...] "construct, bracket, interpret, and rethink the right kinds of market and technology knowledge {{in the right way}} for innovation". However, actors in non-innovative organizations make sense of knowledge in a separate way.|$|R
40|$|AbstractA {{wireless}} electrocardiogram (ECG) {{monitoring system}} is developed which integrates Wi-Fi technology. This Wi-Fi based system {{is comprised of}} a single-chip ECG signal acquisition module on Concerto MCU, a SimpleLink CC 3000 Wi-Fi module and a smart-phone. The Concerto is a multi-core system-on-chip microcontroller (MCU) with independent communication and real-time control subsystems. The data rate {{of the system is}} 11 Mbps at least and 54 Mbps at most. The Wi-Fi module uses infrastructure mode network of IEEE 802. 11 g. Apple's iPhone 4 S is selected as the mobile device platform, which embedded with Wi-Fi and iOS. In this paper, the monitoring system is able to acquire ECG signals through 2 -lead electrocardiogram (ECG) sensor, transmit the ECG data via the wireless <b>link,</b> <b>process</b> and display the ECG waveform in a smart-phone. The results show that implementation of Wi-Fi technology in the existing ECG monitoring system not only eliminates the physical constraints imposed by hard-wired link but also highly reduces the power consumption of the long-term monitoring system...|$|E
40|$|AbstractCurrently, {{the main}} field of {{application}} of additive manufacturing processes is shifting from research laboratories to production facilities. Simulation models can foster this transition by providing support in process development and design. This paper introduces approaches to modelling the beam-material interaction in laser beam melting on a level of detail that allows the simulation of the whole build-up process of parts, not only of single laser tracks. Thus both the achievable result accuracy and the needed calculation time are discussed. For this purpose, fundamental correlations to <b>link</b> <b>process</b> characteristics with model parameters are explained. Subsequently, four modelling approaches are analysed. After an introduction of the well-known method of applying a uniform load on a whole layer compound (e.  g. [1]), the developed methods are discussed which allow modelling the beam-material interaction on a more detailed level. Thereby, the focus lies {{on the ability to}} model load gradients perpendicular to the build direction. This article is completed with a discussion of simulated temperature curves for selected monitoring points using two different modelling approaches...|$|E
40|$|As {{software}} projects grow in {{size and}} complexity, many individuals take over the responsibilities for one project, creating a potential for new errors in the development process. Software version inconsistency, unfamiliarity with the tools used, and software tool restrictions are {{but some of the}} problems encountered in a multi-programmer environment. These problems are not always self-evident to the programmer and may require a dedicated software support representative or experienced programmers to assist. These problems can be reduced through the development of a multi-user software environment diagnostic expert system, AMUSED (A Multi-User Software Environment Diagnostic). The AMUSED expert system is designed for use by programmers responsible for creating the executable software releases on a standard copier / duplicator project. Project source code is transported to a common workstation, and linked together with other programmers 2 ̆ 7 code through a linking tool. AMUSED 2 ̆ 7 s diagnostic help assists (a) the <b>link</b> <b>process</b> that will be used to create the executable code from the source files, (b) the retrieval of source files from remote sites to the link workstation, and (c) the use of any interfacing connections between the source modules...|$|E
50|$|Since DLLs are {{essentially}} the same as EXEs, the choice of which to produce as part of the <b>linking</b> <b>process</b> is for clarity, since it is possible to export functions and data from either.|$|R
50|$|LIBPATH is {{a system}} {{variable}} on a few computer operating systems, that has a meaning {{in the context of}} the runtime <b>linking</b> <b>process,</b> where it influences the search order for shared libraries at alternate locations.|$|R
50|$|At {{about this}} time FORTRAN IV had started to become an {{important}} educational tool and implementations such as the University of Waterloo's WATFOR and WATFIV were created to simplify the complex compile and <b>link</b> <b>processes</b> of earlier compilers.|$|R
40|$|The {{ultimate}} aim {{of casting}} simulation is to recommend process parameter values {{that result in}} the best possible casting quality. However, commercial casting simulation is currently used as a trial-and-error tool, mostly comparing between possible scenarios with no guarantee of realism. Three techniques are presented to overcome these discrepancies for solid investment casting of jewellery as a test bed. The first technique consists in recording temperature versus time profiles by thermocouples embedded in the casting. These profiles are used as calibration reference regarding the overall heat transfer coefficient at the melt-mould interface in simulation. The second technique consists in metallographic validation of simulation findings in terms of correlating microstructure and defects of the casting with simulation results concerning temperature distribution and porosity. The third technique consists in introducing genetic algorithms as an optimization tool. A small number of simulation sample runs <b>link</b> <b>process</b> parameters of interest, such as mould temperature and melt temperature, with results characterizing casting quality, e. g. porosity. These samples are used to train a neural network, thereby creating a generalized meta-model of the process, which is directly used as the fitness function of the genetic algorithm. Combining the three techniques described above caters for realistic and practical casting process planning using commercially available casting simulation software as a tool. © IMechE 2007...|$|E
40|$|Background. Intervention {{fidelity}} is {{an increasingly}} important methodological concept in process evaluations. In this article, the authors investigated the intervention fidelity in a randomized controlled trial on excessive weight gain prevention in pregnancy. Method. A sample of 109 audiotaped counseling sessions, linked to 65 women in the intervention group of the New Life(style) trial, was drawn. The following criteria were quantitatively evaluated using a fidelity checklist: (a) reach, (b) dose, (c) adherence to study objectives, (d) adherence to underlying problem-solving treatment (PST) theory, and (e) counselor competence. Results. A total of 60. 4 % received all counseling sessions. The dose of intervention components was generally moderate (50. 9 % to 60. 4 %), and the dose of PST components was low (17. 3 %). Adherence to study objectives was moderate (64. 2 %) and adherence to PST theory was low (43. 2 %). The counselors sufficiently stimulated the participant to optimize lifestyle (54. 2 % of the sessions), provided positive feedback (50. 5 %), and left the initiative regarding problem solving to the participant (71 %). One of the two counselors performed significantly better on all measured criteria (p<. 001). Conclusions. Intervention fidelity in the New Life(style) trial was generally low to moderate. In future interventions, it is recommended to put more emphasis on counselor recruitment, training, and intervention protocol contents. Fellow researchers are encouraged to embed a process evaluation into all study stages, taking into account all essential process elements, and to <b>link</b> <b>process</b> outcomes to more distal, health outcomes. © 2012 Society for Public Health Education...|$|E
40|$|Abstract—We {{show that}} the {{probability}} of source routing success in dynamic networks, where the link up-down dynamics is governed by a time-varying stochastic process, exhibit critical phase-transition (percolation) phenomena {{as a function of}} the end-to-end message latency per unit path length. We evaluate the probability of routing success on dynamic network (1 D and 2 D) lattices with links going up and down as per an arbitrary binary-valued stationary random process (such as a Markov process), in a source-routing framework. We find percolation thresholds on the time deadline for high-probability of routing success in terms of the first and second order moments of the link state process and we also demonstrate percolation thresholds on the parameters characterizing the <b>link</b> <b>process</b> for a fixed time deadline for a 1 D Markov network as an example. This work happens to generalize results reported in two articles from the 80 ’s that appeared in the Physical Review Letters on directed percolation theory. We analyzed the performance of a stateless single-copy opportunistic forwarding algorithm on a 2 D probabilistic grid – it does not demonstrate a non-trivial percolation threshold in link-up probability as does a flooding based approach. However, interestingly, when we add a time dimension, i. e., let the network evolve as per a potentially timecorrelated link dynamics, the opportunistic “store and forward” routing algorithm also exhibits a critical threshold behavior. In this 2 D grid network case (as in the 1 D network case), the normalized messaging latency (ratio of routing latency to path length) exhibits a critical phase transition, where we evaluate the critical latency to path-length ratio {{as a function of the}} moments of the link up-down process. I...|$|E
50|$|Like {{the basic}} gene {{expression}} algorithm, the GEP-RNC algorithm is also multigenic and its chromosomes are decoded as usual by expressing one gene after another and then linking {{them all together}} by {{the same kind of}} <b>linking</b> <b>process.</b>|$|R
25|$|One {{important}} {{property of}} carbon {{is that it}} readily forms chains, or networks, that are linked by carbon-carbon (carbon-to-carbon) bonds. The <b>linking</b> <b>process</b> is called polymerization, while the chains, or networks, are called polymers. The source compound is called a monomer.|$|R
40|$|A. NET {{application}} {{is a set}} of assemblies developed or reused by programmers, and tested together for correctness and performance. Each assembly’s references to other assemblies are type-checked at compile-time and embedded into the executable image, from where they guide the dynamic <b>linking</b> <b>process.</b> We propose that an application can potentially consist of multiple sets of assemblies, all known to the application’s programmers. Each set implements the application’s functionality in some special way, e. g. using only patent-free algorithms or being optimised for 64 -bit processors. Depending on the assemblies available on a user’s machine, the dynamic <b>linking</b> <b>process</b> will select a suitable set and load assemblies from it. We describe how, in our scheme, an {{application is}} written to use a default set of assemblies but carries nominal and structural specifications about permissible sets of alternative assemblies. We implement the scheme on Rotor, a. NET virtual machine, by modifying its linking infrastructure to efficiently find assemblies on the user’s machine that satisfy the application’s specifications. Specifications can be applied to individual classes and methods, so that only code wishing to use alternative assemblies has to undergo the modified <b>linking</b> <b>process.</b> ...|$|R
40|$|A {{power system}} {{protection}} system consists, at least, of an instrument trans- former, a protective device (relay), and a circuit breaker. Conventional instrument transformers bring currents and voltages from power network levels to much lower scaled-down replicas {{that serve as}} input signals to protective relays. The relay's function is to measure input signals (or a relationship among them in some cases) and compare them to defined operating characteristic thresholds (relay settings) to quickly decide whether to operate associated circuit breaker(s). Existing protection systems within a substation {{are based on a}} hardwired interface between instrument transformers and protective relays. Recent development of electronic instrument transformers and the spread of digital relays allow the development of an all-digital protection system, in which the traditional analog interface has been replaced with a digital signal connected to digital relays through a digital communication <b>link</b> (<b>process</b> bus). Due to their design, conventional instrument transformers introduce distortions to the current and voltage signal replicas. These distortions may cause protective relays to misoperate. On the other hand, non-conventional instrument transformers promise distortion-free replicas, which, in turn, should translate into better relay performance. Replacing hardwired signals with a communication bus also reduces the significant cost associated with copper wiring. An all-digital system should provide compatibility and interoperability so that different electronic instrument transformers can be connected to different digital relays (under a multi-vendor connection) Since the novel all-digital system has never been implemented and/or tested in practice so far, its superior performance needs to be evaluated. This thesis proposes a methodology for performance and compatibility evaluation of an all-digital protection system through application testing. The approach defines the performance indices and compatibility indices as well as the evaluation methodology...|$|E
40|$|This {{paper was}} {{presented}} at IEEE International Symposium on Electrical Insulation, June 2006. © 2006 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must {{be obtained from the}} IEEE. Digital Object Identifier: 10. 1109 /ELINSL. 2006. 1665249 The electrical breakdown performance, either unaged or after ageing (laboratory or service), is often used as the basis for qualification of a device, design or material. Many of the features that affect these performance levels have been discussed in other documents; contaminants, propensity for water treeing, insulating and semiconducting materials. However the size of cable tested is rarely discussed. This is somewhat surprising as it has been long recognized that electrical failure is an extreme value (the Weibull distribution is a member of this family) or weakest <b>link</b> <b>process.</b> In extreme value processes the performance of the whole device is determined by the single "weakest link". Thus when more "weak links" are present the chance of failure is consequently higher: the measured performance depends on weak link concentration or size of the device. Additionally at some dimensions the thickness of the dielectric can influence the breakdown mechanism itself; especially if the thermal influences are present. This paper will attempt to discuss a number of these size related issues for both AC & impulse conditions; these will include: 1) the effect of the dielectric volume actual mechanism of failure, 2) prediction of performance on service length cables from short length laboratory tests. This has practical relevance on the selection of appropriate qualification levels which will have direct relevance to service performance, 3) the requirements for cable quality when increasing the size (thickness or length) installed...|$|E
40|$|Fsculty of Science School of Animal,Plant And Enviromental Sciences Msc 9201098 j tsherwill@yahoo. comSouth Africa’s {{new water}} policy and law have {{introduced}} {{the requirement for}} public participation {{in all aspects of}} resource management and decision-making. This policy change is in recognition of the potential benefits of participation in generating more informed, acceptable, equitable and sustainable management of the nation’s water resources. However the new water law does not prescribe the form this participation should take, or offer criteria for evaluating the success of participatory processes. The term ‘public participation’, in its contemporary usage worldwide, is poorly or broadly defined and may thus encompass a range of processes, which differ in the roles and influence afforded to their stakeholder participants, and in their ability to deliver desired outcomes and benefits to government or the public. This study aimed to investigate the influence of this procedural variation on public and stakeholder participation in the implementation of the National Water Act (Act no. 36 of 1998) in South Africa, and thereby recommend a preferred approach to conducting and facilitating these processes in the future. Use was made of a qualitative and primarily inductive research approach. This was designed to gather perspectives of the various role-players (stakeholders, specialists and government) for a desired process and outcome of participation, and to <b>link</b> <b>process</b> and outcome by means of two case studies of the current implementation of participatory processes for water resource management decision-making. Both case studies illustrated the over-riding negative influence of a product-oriented and ‘specialist-centred’ approach to participation, focused on the development of specific statutory products or decisions required by the National Water Act. This approach in turn is being driven by the current fragmentation of participation around these different products and stages of the overall resource management process. A recommended alternative is a more process-oriented, ‘stakeholder-centred’ approach to participatory events, within an integrative, ongoing participatory process. This must be based on mutual learning by all role-players, and an emphasis on inter-sectoral interaction and relationships. A key constraint identified to the achievement of more integrative participatory processes that offer influence to, and generate ownership by, stakeholder participants, is the lack of clarity within government and the South African water sector as to the intent of participation within the new water policy, and thus the process by which this participation should take place. In particular, the role of stakeholders, and how much influence or power they should be afforded in decision-making processes, is subject to individual interpretation. The recommendation from this research is that, given the ideals of equity, sustainability and citizen empowerment aspired to by the Constitution and the new water policy, government should strive to fully engage stakeholders in processes that both offer influence and inspire action. Ideally, linkages should be created to extend this influence to the broader political process...|$|E
40|$|This report {{presents}} a critical rethinking of the Java security architecture {{from the perspective}} of a software engineer. In existing commercial implementations of the Java Virtual Machine, there is a tight coupling between the dynamic <b>linking</b> <b>process</b> and the bytecode verifier. This leads to delocalized and interleaving program plans, making the verifier difficult to maintain and comprehend. A modular mobile code verification architecture, called Proof Linking, is proposed. By establishing explicit verification interfaces in the form of proof obligations and commitments, and by careful scheduling of linking events, Proof Linking supports the construction of bytecode verifier as a separate engineering component, fully decoupled from Java's dynamic <b>linking</b> <b>process.</b> This turns out to have two additional benefits: (1) Modularization enables distributed verification protocols, in which part of the verification burden can be safely offloaded to remote sites; (2) Alternative static analyses can now be integrated into Java's dynamic <b>linking</b> <b>process</b> with ease, thereby making it convenient to extend the protection mechanism of Java. These benefits make Proof Linking a competitive verification architecture for mobile code systems. Progress to date is reported, and proposals for additional work to evaluate the benefits and feasibility of the Proof Linking architecture are detailed...|$|R
30|$|The {{increasing}} {{complexity of}} knowledge development, transfer and adoption, and the multiple factors affecting the <b>linking</b> <b>processes</b> of both researchers and farmers, and among farmers, suggest {{the need for}} changes in science, technology and innovation policy, from the linear approach to a systemic approach.|$|R
30|$|After the {{formation}} of CLEA, the excess glutaraldehyde was removed by repeatedly washing in water to avoid any modification of protein by the free glutaraldehyde. However, the already reacted glutaraldehyde may continue the cross <b>linking</b> <b>process</b> with the protein even after the washing steps.|$|R
