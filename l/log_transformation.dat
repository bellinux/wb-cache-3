295|133|Public
5000|$|... #Subtitle level 3: <b>Log</b> <b>transformation</b> for {{approximating}} {{a normal}} distribution ...|$|E
50|$|Possibly, the {{transformation}} {{could be improved}} by adding a shift parameter to the <b>log</b> <b>transformation.</b> Panel (c) of the figure shows the log-likelihood. In this case, the maximum of the likelihood is close to zero suggesting that a shift parameter is not needed. The final panel shows the transformed data with a superimposed regression line.|$|E
5000|$|For {{comparing}} the dissimilarities {{between the two}} sets of samples independently from their mean values, it is more appropriate to look at the ratio of the pairs of measurements. <b>Log</b> <b>transformation</b> (base 2) of the measurements before the analysis will enable the standard approach to be used; hence the plot will be given by the following equation: ...|$|E
3000|$|... (as {{opposed to}} an {{additive}} error term in Eq.  4). The <b>log</b> <b>transformations</b> lead to some more transformations of model parameters, e.g. α′ = exp(α) in Eq.  11.|$|R
50|$|These {{variables}} can {{be measured}} for the population of a given area and combined (via a series of calculations involving <b>log</b> <b>transformations</b> and standardisations) to give a “Townsend score” for that area.A greater Townsend index score implies {{a greater degree of}} deprivation. Areas may be “ranked” according to their Townsend score as a means of expressing relative deprivation.|$|R
40|$|Abstract Background The {{usefulness}} of <b>log</b> 2 <b>transformation</b> for cDNA microarray data {{has led to}} its widespread application to Affymetrix data. For Affymetrix data, where absolute intensities are indicative of number of transcripts, there is a systematic relationship between variance and magnitude of measurements. Application of the <b>log</b> 2 <b>transformation</b> expands the scale of genes with low intensities while compressing the scale of genes with higher intensities thus reversing the mean by variance relationship. The {{usefulness of}} these transformations needs to be examined. Results Using an Affymetrix GeneChip ® dataset, problems associated with applying the <b>log</b> 2 <b>transformation</b> to absolute intensity data are demonstrated. Use of the spread-versus-level plot to identify an appropriate variance stabilizing transformation is presented. For the data presented, the spread-versus-level plot identified a power transformation that successfully stabilized the variance of probe set summaries. Conclusion The spread-versus-level plot is helpful to identify transformations for variance stabilization. This is robust against outliers and avoids assumption of models and maximizations. </p...|$|R
5000|$|... where [...] is the {{relative}} potency of [...] This {{is the fundamental}} assumption of similarity of dose-response curves which is necessary for a meaningful and unambiguous definition of {{the relative}} potency. In many cases it is convenient to apply a power transformation [...] with [...] or a logarithmic transformation [...] The latter can {{be shown to be}} a limit case of [...] so if [...] is written for the <b>log</b> <b>transformation</b> the above equation can be redefined as ...|$|E
5000|$|... where [...] is {{the sample}} {{standard}} deviation of the data after a natural <b>log</b> <b>transformation.</b> (In the event that measurements are recorded using any other logarithmic base, b, their standard deviation [...] is converted to base e using , and the formula for [...] remains the same.) This estimate is {{sometimes referred to as}} the [...] "geometric CV" [...] in order to distinguish it from the simple estimate above. However, [...] "geometric coefficient of variation" [...] has also been defined by Kirkwood as: ...|$|E
5000|$|A common way {{to analyze}} data {{such as those}} {{collected}} in scaling is to use log-transformation. There are two reasons for <b>log</b> <b>transformation</b> - a biological reason and a statistical reason. Biologically, log-log transformation places numbers into a geometric domain so that proportional deviations are represented consistently, independent of the scale and units of measurement. In biology this is appropriate because many biological phenomena (e.g. growth, reproduction, metabolism, sensation) are fundamentally multiplicative. [...] Statistically, it is beneficial to transform both axes using logarithms and then perform a linear regression. This will normalize the data set and {{make it easier to}} analyze trends using the slope of the line. Before analyzing data though, {{it is important to have}} a predicted slope of the line to compare the analysis to.|$|E
40|$|Motivation: Standard {{statistical}} techniques often {{assume that}} data are normally distributed, with constant variance not {{depending on the}} mean of the data. Data that violate these assumptions can often be brought in line with the assumptions by application of a transformation. Gene-expression microarray data have a complicated error structure, with a variance that changes with the mean in a non-linear fashion. <b>Log</b> <b>transformations,</b> which are often applied to microarray data, can inflate the variance of observations near background. Results: We introduce a transformation that stabilizes the variance of microarray data across the full range of expression. Simulation studies also suggest that this transformation approximately symmetrizes microarray data...|$|R
40|$|Triceps and subscapular {{skinfold}} thicknesses {{were measured}} in 222 pairs of like-sex twins (78 monozygotic and 144 dizygotic) aged 3 - 15 years. <b>Log</b> <b>transformations</b> of the measurements were standardized for {{age and sex}} and the results used to estimate heritability [...] that is, the proportion of total variation determined by genetic factors. The overall contribution of non-genetic familial effects was small. There were appreciable differences in heritability between limb and trunk fat and between the sexes and at different ages. Over the age of 10 heritability was high for both sites in boys and girls. In younger children environmental factors contributed more to the variation...|$|R
40|$|Abstract Background The {{standard}} approach for preprocessing spotted microarray data is to subtract the local background intensity from the spot foreground intensity, {{to perform a}} <b>log</b> 2 <b>transformation</b> and to normalize the data with a global median or a lowess normalization. Although well motivated, {{standard approach}}es for background correction and for transformation have been widely criticized because they produce high variance at low intensities. Whereas various alternatives to the standard background correction methods and to <b>log</b> 2 <b>transformation</b> were proposed, impacts of both successive preprocessing steps were not compared in an objective way. Results In this study, we assessed the impact of eight preprocessing methods combining four background correction methods and two <b>transformations</b> (the <b>log</b> 2 and the glog), by {{using data from the}} MAQC study. The current results indicate that most preprocessing methods produce fold-change compression at low intensities. Fold-change compression was minimized using the Standard and the Edwards background correction methods coupled with a <b>log</b> 2 <b>transformation.</b> The drawback of both methods is a high variance at low intensities which consequently produced poor estimations of the p-values. On the other hand, effective stabilization of the variance as well as better estimations of the p-values were observed after the glog transformation. Conclusion As both fold-change magnitudes and p-values are important in the context of microarray class comparison studies, we therefore recommend to combine the Edwards correction with a hybrid transformation method that uses the <b>log</b> 2 <b>transformation</b> to estimate fold-change magnitudes and the glog transformation to estimate p-values. </p...|$|R
40|$|The {{logarithmic}} (<b>log)</b> <b>transformation</b> is {{a simple}} yet controversial step {{in the analysis of}} positive continuous data measured on an interval scale. Situations where a <b>log</b> <b>transformation</b> is indicated will be reviewed. This paper contends that the <b>log</b> <b>transformation</b> should not be classed with other transformations as it has particular advantages. Problems with using the data themselves {{to decide whether or not}} to transform will be discussed. It is recommended that log transformed analyses should frequently be preferred to untrans-formed analyses and that careful consideration should be given to use of a <b>log</b> <b>transformation</b> at the protocol design stage. 1...|$|E
3000|$|... f Since we use Spearman’s rank {{correlation}} coefficient (which {{is sensitive to}} outliers), the variables were transformed as described above before computing the specific values. Note {{that we do not}} use Pearson’s coefficient as many of the independent variables are not normally distributed. Furthermore, since <b>log</b> <b>transformation</b> is order preserving, it does not impact Spearman. However, <b>log</b> <b>transformation</b> allows us to have a data set without outliers.|$|E
30|$|Due to the {{instance}} of zero values for reported monthly income, we added {{one into the}} original values before taking the <b>log</b> <b>transformation.</b>|$|E
40|$|To {{improve the}} {{effectiveness}} of a previous regression-based approach for the assessment of the agreement between different analytical methods, two modifications/integrations to the original scheme by means of <b>log</b> 10 <b>transformation</b> of data and implementation of inherent combined imprecision are presented in this study...|$|R
30|$|Data are {{presented}} as means[*]±[*]SEM or medians (interquartile ranges). Two-way repeated-measures analysis of variance (ANOVA) followed by Student’s t test with Bonferroni correction was performed to compare the BLV and ULV groups. Histological scores were analyzed with Kruskal–Wallis test followed by Dunn’s multiple comparison test. One-way ANOVA followed by Dunnett’s test was performed to compare the ULV, NTV 60 %, and NTV 0 % groups. Student’s t test was performed to compare cell-culture findings. Two-way ANOVA followed by Student’s t test with Bonferroni correction was performed for multiple comparisons of the cell-culture experiments. Cytokine concentrations were analyzed after performing <b>log</b> <b>transformations</b> to adjust the standard deviations. GraphPad Prism 6 (GraphPad Software, La Jolla, CA) was used for all statistical analyses. Statistical significance was set at p[*]<[*] 0.05.|$|R
40|$|In this paper, {{we apply}} the ARFIMA-GARCH {{model to the}} {{realized}} volatility and the continuous sample path variations constructed from high-frequency Nikkei 225 data. While the homoskedastic ARFIMA model performs excellently in predicting the Nikkei 225 realized volatility time series and their square-root and <b>log</b> <b>transformations,</b> the residuals of the model suggest presence of strong conditional heteroskedasticity similar to the finding of Corsi et al. (2007) for the realized S&P 500 futures volatility. An ARFIMA model augmented by a GARCH(1, 1) specification for the error term largely captures this and substantially improves the fit to the data. In a multi-day forecasting setting, we also find some evidence of predictable time variation in the volatility of the Nikkei 225 volatility captured by the ARFIMA-GARCH model. ...|$|R
30|$|A <b>log</b> <b>transformation</b> of the speedup was performed, where y∗ = log (y). For the log-transformed speedup model, the {{residual}} versus predicted value plot {{is shown in}} Figure  2 b.|$|E
3000|$|..., {{it can be}} {{converted}} into a concave function through an appropriate <b>log</b> <b>transformation,</b> leading to a critical convexity property that establishes global optimality (see “PCC management for a two-tier femtocell/macrocell system” section).|$|E
3000|$|Variance {{homogeneity}} of residuals across the different centers {{was tested using}} a likelihood ratio test which indicated that this assumption was reasonable (p = 0.77). In all cases, residual analysis revealed that a <b>log</b> <b>transformation</b> of the DAT measurements was favorable (all results based on the <b>log</b> <b>transformation</b> and hence parameters {{can be interpreted as}} log relative differences). However, quite similar conclusions were reached based on modeling of the data on their original scale. Linearity of the continuous predictor age was assessed using a linear mixed additive model and by inclusion of polynomial terms in the model. R version 2.15 was used for all analyses [39], [URL] [...]...|$|E
40|$|The {{development}} of an artificial visual system consists of several algorithms based on the biological human eye sight, particularly about the retina. It involves algorithms such as <b>log</b> polar <b>transformation,</b> Gaussian filters, Gabor Wavelets, etc. Overall, the system is implemented into recognition and able to perform some tracking abilities...|$|R
40|$|An {{automated}} fluorometric method, {{rather than}} the Guthrie test, {{has been used in}} North Carolina for neonatal screening for phenylketonuria (PKU). Although there is no testing law, 97 % of newborn infants are screened. Twelve children with PkU, not referred for dietary management, were born before the screening program was established, were born elsewhere, or were successfully identified at birth but not referred for treatment. None was missed because of laboratory error or {{because of the lack of}} a testing law. Positive skewing was noted among initial blood phenylalanine levels of 49 infants with PKU and severe hyperphenylalaninemia. <b>Log</b> <b>transformations</b> caused the values to be normally distributed and permitted the calculation of tolerance and confidence limits. These provided estimates of the percentage of phenylketonuric infants whose initial blood levels might fall below any given cutoff value...|$|R
40|$|Analysis {{of coral}} {{population}} size structure {{is difficult because}} corals vary enormously in size. The great differences in colony size are caused by the coral colony growth process. We studied size distributions in coral species and found that analyzing coral size frequencies using <b>log</b> <b>transformations</b> of colony size {{has a number of}} advantages. Coral size distributions become comparable within and between species across space and through time. Such analyses that (1) small colony size and positively skewed size-frequency distributions characterize populations of brooding species. (2) Spawning species show large colony size and negatively skewed size-frequency distributions. (3) Populations in marginal habitats are characterized by a coefficient of variation higher than 0. 50. The described method of log transforming size frequency data of coral populations provides both scientists and wildlife managers with a new tool to monitor the dynamics of coral populations...|$|R
40|$|We explore {{coupling}} to a configurable subsurface {{reactive transport}} code as a flexible and extensible approach to biogeochemistry in land surface models. A reaction network with the Community Land Model carbon–nitrogen (CLM-CN) decomposition, nitrification, denitrification, and plant uptake {{is used as}} an example. We implement the reactions in the open-source PFLOTRAN (massively parallel subsurface flow and reactive transport) code and couple it with the CLM. To make the rate formulae designed for use in explicit time stepping in CLMs compatible with the implicit time stepping used in PFLOTRAN, the Monod substrate rate-limiting function with a residual concentration is used to represent the limitation of nitrogen availability on plant uptake and immobilization. We demonstrate that CLM–PFLOTRAN predictions (without invoking PFLOTRAN transport) are consistent with CLM 4. 5 for Arctic, temperate, and tropical sites. Switching from explicit to implicit method increases rigor but introduces numerical challenges. Care {{needs to be taken}} to use scaling, clipping, or <b>log</b> <b>transformation</b> to avoid negative concentrations during the Newton iterations. With a tight relative update tolerance (STOL) to avoid false convergence, an accurate solution can be achieved with about 50  % more computing time than CLM in point mode site simulations using either the scaling or clipping methods. The <b>log</b> <b>transformation</b> method takes 60 – 100  % more computing time than CLM. The computing time increases slightly for clipping and scaling; it increases substantially for <b>log</b> <b>transformation</b> for half saturation decrease from 10 − 3 to 10 − 9  mol m − 3, which normally results in decreasing nitrogen concentrations. The frequent occurrence of very low concentrations (e. g. below nanomolar) can increase the computing time for clipping or scaling by about 20  %, double for <b>log</b> <b>transformation.</b> Overall, the <b>log</b> <b>transformation</b> method is accurate and robust, and the clipping and scaling methods are efficient. When the reaction network is highly nonlinear or the half saturation or residual concentration is very low, the allowable time-step cuts may need to be increased for robustness for the <b>log</b> <b>transformation</b> method, or STOL may need to be tightened for the clipping and scaling methods to avoid false convergence. As some biogeochemical processes (e. g., methane and nitrous oxide reactions) involve very low half saturation and thresholds, this work provides insights for addressing nonphysical negativity issues and facilitates the representation of a mechanistic biogeochemical description in Earth system models to reduce climate prediction uncertainty...|$|E
40|$|The Palmprint is an {{efficient}} physiological biometric trait {{to identify a}} person. In this paper we propose Palmprint Identification using <b>Log</b> <b>Transformation</b> of Transform Domain Features. The Region of Interest (ROI) of palmprint image is extracted using preprocessing. The KWT and DWT are applied on preprocessed image to generate features. The KWT and DWT features of test image and database images are compared using Euclidian distance to compute EER and TSR values. The EER and TSR values of KWT and DWT are fused using <b>Log</b> <b>Transformation</b> to get better performance parameters. It is observed that the values of performance parameters are better {{in the case of}} proposed algorithm compared to existing algorithms...|$|E
30|$|The {{collected}} data was analyzed {{by using the}} statistical program SPSS version 19.0. The differences of the four intravenous injection methods were measured by Kruskal–Wallis test and χ 2 -test after <b>log</b> <b>transformation</b> of the number {{and the length of}} glass particles.|$|E
40|$|This version: January 14, 2009; First draft: October 24, 2008 In this paper, {{we apply}} the ARFIMA-GARCH {{model to the}} {{realized}} volatility and the continuous sample path variations constructed from high-frequency Nikkei 225 data. While the homoskedastic ARFIMA model performs excellently in predicting the Nikkei 225 realized volatility time series and their square-root and <b>log</b> <b>transformations,</b> the residuals of the model suggest presence of strong conditional heteroskedasticity similar to the finding of Corsi et al. (2007) for the realized S&P 500 futures volatility. An ARFIMA model augmented by a GARCH(1, 1) specification for the error term largely captures this and substantially improves the fit to the data. In a multi-day forecasting setting, we also find some evidence of predictable time variation in the volatility of the Nikkei 225 volatility captured by the ARFIMA-GARCH model. グローバルCOEプログラム = Global COE Program[Revised version...|$|R
40|$|Cloud based web-applications {{are quickly}} {{becoming}} common in modern society. A {{new class of}} such applications, collaborative cloud applications, are gaining in popularity as they greatly improve remote collaboration. Most of these applications use a log structure as a coordination mechanism for shared application state. Such structures typically store the entire application state as well as deltas (changes sets) while the application runs. In this {{paper we propose a}} monadic, dependency-aware, self-cleaning log structure for collaborative cloud applications, which we refer to as a monadic log. This structure provides a rich set of analytical tools to support a variety of <b>log</b> <b>transformations</b> and rewrites. For example, the garbage collection mechanisms already present in any managed language will automatically bound the memory footprint of a monadic log. Moreover, a monadic log substantially eases the computational and bandwidth burdens of a server infrastructure when compared with traditional log structures. 1...|$|R
40|$|When gross {{deviations}} from parametric assumptions are observed, conventional data transformations are often applied {{with little regard}} for substantive theoreti-cal implications. One such transformation involves using the logarithm of posi-tively skewed dependent variables. <b>Log</b> <b>transformations</b> were shown to severely decrease estimates of true moderator effects using moderated regression proce-dures in a Monte Carlo simulation. Estimates of moderator effect sizes were sub-stantially better estimates of the true latent moderator effect (i. e., larger by a mul-tiple of 2. 6 to 534) when estimated using a simple percentile bootstrap procedure in the original, positively skewed data. Conclusions with regard to {{the presence or absence of}} a true moderator effect using a simple bootstrap procedure were unaf-fected by the violation of parametric assumptions in the original, positively skewed data. In contrast, moderated regression analysis performed on a log-transformed dependent variable severely increased Type-II error. Implications are drawn for applied psychological and management research. At one time or another, almost all investigators in applied psychological and manage...|$|R
3000|$|Treatment: Via a <b>log</b> <b>transformation</b> [29, 30], we convert (P 2) into an {{equivalent}} convex optimization problem (P 3) (see “Pursuing convexity (second problem transformation)” section). This enables the proposed PCC algorithm {{with the following}} desirable properties: (a) global convergence to optimality [...]...|$|E
30|$|A plot of BVol% against height {{indicated}} that BVol% decreased non-linearly with height. Height in regression models was, therefore, transformed using a natural <b>log</b> <b>transformation</b> after first adding one to each height to adequately deal with bark volume measurements at the butt (height[*]=[*] 0).|$|E
40|$|Introducción: Validar la aplicación retrospectiva de los Adjusted Clinical Groups (ACG) en varios centros de atención primaria y especializada en la población española. Métodos: Estudio restrospectivo-multicéntrico, realizado a partir de los registros de sujetos atendidos en 5 equipos de atención primaria (AP) y dos hospitalarios, durante el año 2005. Las principales mediciones fueron dependientes (visitas, episodios, coste en AP y coste total) y de casuística/morbilidad con el ACG Case-Mix-System. Cálculo del poder explicativo: cociente de determinación, p Purpose: To {{validate}} the Johns Hopkins ACG case-mix system used in various primary and specialized care centers attending a defined population in Spain. Methods: A retrospective, multicenter study {{was carried out}} by applying the ACG case-mix system to the clinical records of patients attending five primary care teams and two hospitals over a 1 -year period in 2005. The main measurements were dependent variables (visits, episodes, primary care costs, and total costs), and morbidity. The determination coefficient (R²; p< 0. 05) was used to measure the explained variability. Results: A total of 81, 873 patients were included with a mean (standard deviation) number of 4. 8 (3. 5) episodes and 8. 0 (8. 1) visits/patient/year. The explained variance (R²) of ACG classification was 73. 1 % (75. 5 % <b>log</b> <b>transformation)</b> for episodes, 43. 2 % (54. 0 % <b>log</b> <b>transformation)</b> for visits, 19. 6 % (54. 8 % <b>log</b> <b>transformation)</b> for primary care costs, and 22. 7 % (48. 3 % <b>log</b> <b>transformation)</b> for total costs (p< 0. 001). Conclusion: The ACG system classified a defined population on the basis of morbidity and individual resource consumption. Moreover, the ACG system was useful to assess the clinical (comorbidity) and economical information of each center...|$|E
40|$|Preliminary and {{incomplete}} In this paper, we apply the ARFIMA-GARCH {{model to the}} realized volatility and the continuous sample path variations constructed from high-frequency Nikkei 225 data. While the homoskedastic ARFIMAmodel performs excellently in predicting the Nikkei 225 realized volatility time series and their square-root and <b>log</b> <b>transformations,</b> the residuals of the model suggest presence of strong conditional heteroskedasticity similar to the nding of Corsi et al. (2007) for the realized S&P 500 futures volatility. An ARFIMA model augmented by a GARCH(1, 1) specication for the error term largely captures this and substantially improves the t to the data. In a multi-day forecasting setting, we also nd evidence of predictable time variation in the volatility of the Nikkei 225 volatility captured by the ARFIMA-GARCH model. A battery of specication tests including the BDS, CCK, and Hong-Li tests for detecting higher-order dependence are run. The results of these tests reveal various forms of misspecication remaining in the ARFIMA-GARCH model, which suggest the model can be further improved upon...|$|R
50|$|Plectrohyla pycnochila, {{also known}} as the thicklip spikethumb frog or thick-lipped spikethumb frog, is a species of frog in the family Hylidae. It is endemic to Mexico and occurs in the Meseta Central of Chiapas. Its natural {{habitats}} are pine-oak forests. It is threatened by habitat loss caused by <b>logging</b> and <b>transformation</b> of the forest to agricultural land. Chytridiomycosis might also be a threat.|$|R
2500|$|Most ESB {{implementations}} {{contain a}} facility called [...] "mediation". For example, mediation flows {{are part of}} the WebSphere enterprise service bus intercept. Mule also supports mediation flows. Mediation flows modify messages that are passed between existing services and clients that use those services. A mediation flow mediates or intervenes to provide functions, such as message <b>logging,</b> data <b>transformation,</b> and routing, typically the functions can be implemented using the Interception Design Pattern.|$|R
