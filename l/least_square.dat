10000|10000|Public
25|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the method of <b>least</b> <b>square</b> and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the Gauss–Markov theorem.|$|E
2500|$|... we {{can then}} see that in that case the <b>least</b> <b>square</b> {{estimate}} (or estimator, {{in the context of}} a random sample), [...] is given by ...|$|E
2500|$|... {{where the}} sum {{is over the}} number {{different}} sizes of particle, indexed by i, and [...] gives the weighting, which {{is related to the}} quantum yield and concentration of each type. This introduces new parameters, which makes the fitting more difficult as a higher-dimensional space must be searched. [...] Nonlinear <b>least</b> <b>square</b> fitting typically becomes unstable with even a small number of s. [...] A more robust fitting scheme, especially useful for polydisperse samples, is the Maximum Entropy Method.|$|E
40|$|Abstract:Through {{theoretical}} derivation, some {{properties of}} the total <b>least</b> <b>squares</b> estimation are found. The total <b>least</b> <b>squares</b> estimation is the linear transformation of the <b>least</b> <b>squares</b> estimation, and the total <b>least</b> <b>squares</b> estimation is unbiased. The condition number of the total <b>least</b> <b>squares</b> estimation {{is greater than the}} <b>least</b> <b>squares</b> estimation, so the total <b>least</b> <b>squares</b> estimation is easier to be affected by the data error than the <b>least</b> <b>squares</b> estimation. Then through the further derivation, the relationships of solutions, residuals and unit weight variance estimations between the total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> are given...|$|R
40|$|AbstractIn this note, {{we present}} two {{results on the}} scaled total <b>least</b> <b>squares</b> problem. First, we discuss the {{relation}} between the scaled total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> problems. We derive an upper bound for the difference between the scaled total <b>least</b> <b>squares</b> solution and the <b>least</b> <b>squares</b> solution and establish a quantitative relation between the scaled total <b>least</b> <b>squares</b> residual and the <b>least</b> <b>squares</b> residual. Second, we give a perturbation analysis of the scaled total <b>least</b> <b>squares</b> problem. Numerical experiments in comparing our results with existing results are demonstrated...|$|R
30|$|In this section, we {{investigate}} widely used fuzzy regression methods of Fuzzy <b>Least</b> <b>Squares</b> (FLS), General Fuzzy <b>Least</b> <b>Squares</b> (GFLS), Sakawa–Yano (SY), Hojati–Bector–Smimou (HBS), Approximate-Distance Fuzzy <b>Least</b> <b>Squares</b> (ADFLS) and Interval-Distance Fuzzy <b>Least</b> <b>Squares</b> (IDFLS).|$|R
5000|$|In {{this form}} the above {{expression}} {{can be easily}} compared with weighed <b>least</b> <b>square</b> and Gauss-Markov estimate. In particular, when , corresponding to infinite variance of the apriori information concerning , the result [...] {{is identical to the}} weighed linear <b>least</b> <b>square</b> estimate with [...] as the weight matrix. Moreover, if the components of [...] are uncorrelated and have equal variance such that [...] where [...] is an identity matrix, then [...] is identical to the ordinary <b>least</b> <b>square</b> estimate.|$|E
50|$|This is {{originated}} from the alternating <b>least</b> <b>square</b> method for multi-way data analysis.|$|E
5000|$|... and [...] are {{the only}} non-trivial (i.e. at <b>least</b> <b>square)</b> {{consecutive}} powers of positive integers (Catalan's conjecture).|$|E
40|$|Abstract: This Paper {{describes}} {{an application of}} Structured Total <b>Least</b> <b>Squares</b> method to the interpolation of terrain data. We briefly review the ideas of <b>Least</b> <b>Squares,</b> Total <b>Least</b> <b>Squares,</b> and Structured Total <b>Least</b> <b>Squares.</b> We illustrate the use of Structured Total <b>Least</b> <b>Squares</b> in the approximation of terrain surfaces using a novel discrete surface, the Triangular Regular Network. The Structured Total <b>Least</b> <b>Squares</b> algorithm allows us to deal with data corrupted by noise in every coordinate (x; y; z) ...|$|R
50|$|In {{mathematical}} statistics, polynomial <b>least</b> <b>squares</b> {{refers to}} {{a broad range of}} statistical methods for estimating an underlying polynomial that describes observations. These methods include polynomial regression, curve fitting, linear regression, <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> simple linear regression, linear <b>least</b> <b>squares,</b> approximation theory and method of moments. Polynomial <b>least</b> <b>squares</b> has applications in radar trackers, estimation theory, signal processing, statistics, and econometrics.|$|R
40|$|In {{this paper}} we {{investigate}} the finite sample risk performance of feasible generalised <b>least</b> <b>squares</b> estimators applied in models with serially correlated error terms. The risk {{functions of the}} ordinary <b>least</b> <b>squares,</b> generalised <b>least</b> <b>squares</b> and feasible generalised <b>least</b> <b>squares</b> estimators are derived under the asymmetric Linear-Exponential loss function. A numerical evaluation using simulation is {{used to compare the}} risk functions. Our numerical results show that the relative risk gains of the feasible generalised <b>least</b> <b>squares</b> estimators over the ordinary <b>least</b> <b>squares</b> estimator increases with higher loss asymmetry, particularly for larger serial correlation coefficients. [URL]...|$|R
50|$|<b>Least</b> <b>Square</b> Monte Carlo {{is used in}} valuing American options. The {{technique}} {{works in}} a two step procedure.|$|E
5000|$|In {{order to}} obtain the <b>least</b> <b>square</b> cost function, {{it is assumed that}} the {{probability}} of a data point is proportional to: ...|$|E
50|$|Alternatively, the IASI Level 1 {{data can}} be {{processed}} by <b>least</b> <b>square</b> fit algorithms. Again, the expected error {{must be taken into}} consideration.|$|E
40|$|We {{show that}} the {{generalized}} total <b>least</b> <b>squares</b> (GTLS) problem with a singular noise covariance matrix {{is equivalent to the}} restricted total <b>least</b> <b>squares</b> (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in <b>least</b> <b>squares</b> (LS), data <b>least</b> <b>squares</b> (DLS), total <b>least</b> <b>squares</b> (TLS), and RTLS noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms. Index Terms total <b>least</b> <b>squares</b> (TLS), generalized total <b>least</b> <b>squares</b> (GTLS), restricted total <b>least</b> <b>squares</b> (RTLS), recursive estimation, subspace tracking, system identification. I...|$|R
40|$|The {{behavior}} of the t test in small samples for coefficient significance in time-series regressions is examined after using the Prais-Winsten (PW) and Cochrane-Orcutt (CO) corrections for autocorrelation. Results are compared to ordinary <b>least</b> <b>squares</b> and generalized <b>least</b> <b>squares.</b> Key words: First-order autocorrelation generalized <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> Prais-Winsten...|$|R
40|$|This article studies weighted, generalized, <b>least</b> <b>squares</b> estimators {{in simple}} linear {{regression}} with serially correlated errors. Closed-form expressions of weighted <b>least</b> <b>squares</b> estimators and variances are presented under some common stationary autocorrelation settings, a first-order autoregression and a first-order moving-average. These explicit expressions also have appealing applications, including an efficient weighted <b>least</b> <b>squares</b> computation method and a new sufficient and necessary condition on the equality of weighted <b>least</b> <b>squares</b> estimators and ordinary <b>least</b> <b>squares</b> estimators...|$|R
5000|$|The <b>least</b> <b>square</b> method (LSM) is a {{slightly}} complicated algorithm for odor localization. The LSM {{version of the}} odor tracking model is given by: ...|$|E
5000|$|... we {{can then}} see that in that case the <b>least</b> <b>square</b> {{estimate}} (or estimator, {{in the context of}} a random sample), [...] is given by ...|$|E
50|$|Basis {{function}} centers can be randomly sampled {{among the}} input instances or obtained by Orthogonal <b>Least</b> <b>Square</b> Learning Algorithm or found by clustering the samples and choosing the cluster means as the centers.|$|E
5000|$|Two-stage <b>least</b> <b>squares,</b> three-stage <b>least</b> <b>squares,</b> and {{seemingly}} unrelated regressions.|$|R
5000|$|Finding {{the tree}} and branch lengths {{minimizing}} the <b>least</b> <b>squares</b> residual is an NP-complete problem. However, for a given tree, the optimal branch lengths can be determined in [...] time for ordinary <b>least</b> <b>squares,</b> [...] time for weighted <b>least</b> <b>squares,</b> and [...] time for generalised <b>least</b> <b>squares</b> (given the inverse of the covariance matrix).|$|R
50|$|For details {{concerning}} nonlinear {{data modeling}} see <b>least</b> <b>squares</b> and non-linear <b>least</b> <b>squares.</b>|$|R
5000|$|First we run an {{ordinary}} <b>least</b> <b>square</b> regression that has Xi {{as a function}} of all the other explanatory variables in the first equation. If i = 1, for example, the equation would be ...|$|E
5000|$|Minimization aims {{to reduce}} the {{differences}} between the computed and observed slip directions of fault planes by choosing a function to proceed the <b>least</b> <b>square</b> minimization. Here are a few examples of the functions: ...|$|E
50|$|The MPCA {{solution}} {{follows the}} alternating <b>least</b> <b>square</b> (ALS) approach. It is iterative in nature.As in PCA, MPCA works on centered data. Centering {{is a little}} more complicated for tensors, and it is problem dependent.|$|E
40|$|Preface Examples of the General Linear Model Introduction One-Sample Problem Simple Linear Regression Multiple Regression One-Way ANOVA First Discussion The Two-Way Nested Model Two-Way Crossed Model Analysis of Covariance Autoregression Discussion The Linear <b>Least</b> <b>Squares</b> Problem The Normal Equations The Geometry of <b>Least</b> <b>Squares</b> Reparameterization Gram-Schmidt Orthonormalization Estimability and <b>Least</b> <b>Squares</b> Estimators Assumptions for the Linear Mean Model Confounding, Identifiability, and Estimability Estimability and <b>Least</b> <b>Squares</b> Estimators...|$|R
40|$|AbstractThis article {{surveys the}} history, development, and {{applications}} of <b>least</b> <b>squares,</b> including ordinary, constrained, weighted, and total <b>least</b> <b>squares.</b> The presentation includes proofs {{of the basic}} theory, in particular, unitary factorizations and singular-value decompositions of matrices. Numerical examples with real data demonstrate {{how to set up}} and solve several types of problems of <b>least</b> <b>squares.</b> The bibliography lists comprehensive sources for more specialized aspects of <b>least</b> <b>squares...</b>|$|R
40|$|<b>Least</b> <b>squares</b> estimations {{have been}} used {{extensively}} in many applications system identification and signal prediction. These applications, the <b>least</b> <b>squares</b> estimators can usually be found by solving Toeplitz <b>least</b> <b>squares</b> problems. We present fast algorithms for solving the Toeplitz <b>least</b> <b>squares</b> problems. The algorithm is derived by using the displacement representation of the normal equations matrix. Numerical experiments show that these algorithms are efficient. published_or_final_versio...|$|R
5000|$|To see why {{the initial}} {{observation}} assumption stated by Prais-Winsten (1954) is reasonable, considering the mechanics of generalized <b>least</b> <b>square</b> estimation procedure sketched above is helpful. The inverse of [...] can be decomposed as [...] with ...|$|E
5000|$|We {{consider}} {{an example of}} which the receiver is endowed with N antennas. In this case, the received vector [...] iswhere [...] is noise vector [...] Following the ML detection criterion the detection procedure may be written aswhere [...] is the <b>least</b> <b>square</b> solution to the above model.The <b>least</b> <b>square</b> solution {{in this case is}} also known as maximum-ratio-combining (MRC). In the case of N antennas the LS can be written aswhich means that the signal from each antenna is rotated and weighted according to the phase and strength of the channel, such that the signals from all antennas are combined to yield the maximum ratio between signal and noise terms.|$|E
5000|$|The main {{difference}} between LSM algorithm and the direct triangulation method is the noise. In LSM, noise is considered and the odor source location is being estimated by minimizing the squared error. The nonlinear <b>least</b> <b>square</b> problem is given by: ...|$|E
40|$|The {{performance}} of several algorithms for positioning a single sensor node based on distance estimates to {{it from a}} number of nodes at known positions (anchor nodes) is compared when the distance estimates are obtained from a measurement campaign. The distance estimates are based on timeof-arrival measurements done by ultrawideband devices in an indoor office environment. The compared algorithms are based on nonlinear <b>least</b> <b>squares,</b> <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing, and projection methods. No algorithm is uniformly best; however, <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing show higher mean squared errors in almost all cases, while the nonlinear <b>least</b> <b>squares</b> and projection methods have similar performance; projection methods performs slightly better. 1...|$|R
50|$|The {{discrete}} <b>least</b> <b>squares</b> meshless (DLSM) {{method is}} a newly introduced meshless method {{based on the}} <b>least</b> <b>squares</b> concept. The method {{is based on the}} minimization of a <b>least</b> <b>squares</b> functional defined as the weighted summation of the squared residual of the governing differential equation and its boundary conditions at nodal points used to discretize the domain and its boundaries. While most of the existing meshless methods need background cells for numerical integration, DLSM did not require numerical integration procedure due to the use of discrete <b>least</b> <b>squares</b> method to discretize the governing differential equation. A Moving <b>least</b> <b>squares</b> (MLS) approximation method is used to construct the shape function making the approach a fully <b>least</b> <b>squares</b> based approach.|$|R
40|$|In this correspondence, we {{introduce}} a minimax regret criteria to the <b>least</b> <b>squares</b> problems with bounded data uncertainties and solve it using semi-definite programming. We investigate a robust minimax <b>least</b> <b>squares</b> approach that minimizes a worst case difference regret. The regret {{is defined as}} the difference between a squared data error and the smallest attainable squared data error of a <b>least</b> <b>squares</b> estimator. We then propose a robust regularized <b>least</b> <b>squares</b> approach to the regularized <b>least</b> <b>squares</b> problem under data uncertainties by using a similar framework. We show that both unstructured and structured robust <b>least</b> <b>squares</b> problems and robust regularized <b>least</b> <b>squares</b> problem can be put in certain semi-definite programming forms. Through several simulations, we demonstrate the merits of the proposed algorithms with respect to the the well-known alternatives in the literature. Comment: Submitted to the IEEE Transactions on Signal Processin...|$|R
