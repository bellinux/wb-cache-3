18|41|Public
5000|$|Changing the {{physical}} block size or <b>logical</b> <b>record</b> {{length of a}} sequential data set.|$|E
50|$|Data on CMS is {{structured}} in logical records {{rather than a}} stream of bytes. For textual data a line of text corresponds to a <b>logical</b> <b>record.</b> In CMS Pipelines the data is passed between the stages as logical records.|$|E
50|$|The IEBCOMPR utility is used {{to compare}} two {{sequential}} or partitioned datasets. This data set comparison is performed at the <b>logical</b> <b>record</b> level. Therefore, IEBCOMPR is commonly used to verify that a backup copy of a data set is correct (exact match to the original).|$|E
50|$|IBM {{refers to}} the data records programmers work with as <b>logical</b> <b>records,</b> and the format on disc as blocks or {{physical}} records. One block might contain several <b>logical</b> (or user) <b>records</b> or, in some schemes, partial <b>logical</b> <b>records.</b>|$|R
50|$|QSAM is—as {{its name}} says—queued, in this {{specific}} context meaning buffered with deblocking of reads and blocking of writes. It allows programs {{to read and}} write <b>logical</b> <b>records</b> within physical blocks of data, as opposed to the less advanced basic sequential access method (BSAM) which allows programs to access physical blocks of data, but provides no support for accessing <b>logical</b> <b>records</b> within blocks.|$|R
50|$|The access {{methods are}} {{responsible}} for blocking and deblocking <b>logical</b> <b>records</b> as they are written to or read from external media.|$|R
5000|$|If the dataset is unblocked, that is, the <b>logical</b> <b>record</b> length (LRECL) {{is equal}} to the {{physical}} block size (BLKSIZE), BSAM may be utilized to simulate a directly accessed dataset using [...] and [...] on any supported direct access device type (DEVD=DA), and some primitive applications were designed in this way.|$|E
50|$|DLGs are {{available}} in two different formats: optional format, a simple-to-use format that incorporates an 80-byte <b>logical</b> <b>record</b> length, the UTM ground coordinate system, and topology linkages contained in line, node and area elements; and Spatial Data Transfer Standard (SDTS) format, a format that facilitates transferring of spatial data between different computer systems.|$|E
50|$|Most {{programs}} will require services from the operating system, and the OS provides standard macros for requesting those services. These are analogous to Unix system calls. For instance, in MVS (later z/OS), STORAGE (with the OBTAIN parameter) dynamically allocates {{a block of}} memory, and GET retrieves the next <b>logical</b> <b>record</b> from a file.|$|E
5000|$|The {{amount of}} <b>logical</b> <b>records</b> {{that can be}} stored in the PAT is {{determined}} by its page size. Each page pointer in versions 6.x and 7.x of the MKDE takes up 4 bytes of space, and the PAT header takes up 8 bytes, so the amount of logical pages in the PAT becomes: ...|$|R
50|$|Records {{can exist}} in any storage medium, {{including}} main memory and mass storage {{devices such as}} magnetic tapes or hard disks. Records are a fundamental component of most data structures, especially linked data structures. Many computer files are organized as arrays of <b>logical</b> <b>records,</b> often grouped into larger physical records or blocks for efficiency.|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. Main Topics : Variables The County and City Data Book Consolidated File: City Data, 1944 - 1977, {{is a collection of}} data gathered from both governmental and private sources. The data cover such diverse areas as the following: population, employment, vital statistics, school enrolment, health, income, public assistance and social security, banking, housing, government employment and finance, elections, crime, manufacturing, retail and wholesale trade, selected services, mineral industries, farm population, agriculture, and weather. The data are drawn from County and City Data Books published from 1944 - 1977 for all U. S. cities (both incorporated and unincorporated) with a population greater than 25, 000. No data are available, however, for individual cities in years when the city had a population of less than 25, 000. The data in this collection are contained in one file of 1, 014 <b>logical</b> <b>records.</b> The <b>logical</b> <b>records</b> are each 9, 985 characters in length and are arranged in hierarchical order by city within state...|$|R
50|$|Cullinet {{attempted}} to continue competing against IBM's DB2 and other relational databases {{by developing a}} relational front-end {{and a range of}} productivity tools. These included Automatic System Facility (ASF), which made use of a pre-existing IDMS feature called LRF (<b>Logical</b> <b>Record</b> Facility). ASF was a fill-in-the-blanks database generator that would also develop a mini-application to maintain the tables.|$|E
5000|$|The {{album was}} {{reviewed}} by Scott Yanow at Allmusic {{who wrote that}} [...] "For this very <b>logical</b> <b>record,</b> altoist Paul Desmond....makes the group a quintet and his interplay with Mulligan is consistently delightful. ...In addition, Desmond is showcased on [...] "Koto Song" [...] and as an encore Brubeck plays a lighthearted if brief [...] "Sweet Georgia Brown." [...]" ...|$|E
50|$|An {{example is}} the BLKSIZE= variable, which may be (and usually is) {{specified}} in the DCB as zero. In the DD statement, the BLKSIZE is specified as a non-zero value and this, then, results in a program-specified LRECL (<b>logical</b> <b>record</b> length) and a JCL-specified BLKSIZE (physical block size), with the merge of the two becoming the permanent definition of the dataset.|$|E
50|$|Computers in the 1950s and 1960s {{typically}} {{dealt with}} data that were organized into records {{either by the}} nature of the media, e.g., lines of print, or by application requirements. IOCS was intended to allow Assembler language programmers to read and write records without having to worry about the details of the various devices or the blocking of <b>logical</b> <b>records</b> into physical records. IOCS provided the run time I/O support for several compilers.|$|R
40|$|A list of 1186 ultraviolet-excess objects (designated KUV) was {{compiled}} {{as a result}} of a search conducted with the 105 -cm Schmidt telescope of the Kiso station of the Tokyo Astronomical Observatory. This document describes the machine readable version of the KUV survey list and presents a sample listing showing the <b>logical</b> <b>records</b> as they are recorded in the machine readable catalog. The KUV data include equatorial coordinates, magnitudes, color indices, and identifications for previously cataloged objects...|$|R
40|$|The {{machine-readable}} {{version of}} the catalog, as it is currently being distributed from the Astronomical Data Center, is presented. The complete catalog is contained in the magnetic tape file, and corrections published in all corrigenda {{were made to the}} data. The machine version contains 613959 records, but only 613953 stars (six stars were later deleted, but their <b>logical</b> <b>records</b> are retained in the file so that the zone counts are not different from the published catalog) ...|$|R
5000|$|Data {{sets are}} not {{unstructured}} streams of bytes, but rather are organized in various <b>logical</b> <b>record</b> and block structures {{determined by the}} [...] (data set organization), [...] (record format), and other parameters. These parameters are specified {{at the time of}} the data set allocation (creation), for example with Job Control Language [...] statements. Inside a job they are stored in the Data Control Block (DCB), which is a data structure used to access data sets, for example using access methods.|$|E
5000|$|In {{direct access}} (BDAM) files, the {{application}} program has {{to specify the}} relative block number, the relative track and record (TTR) or the actual physical location (MBBCCHHR) in a Direct-access storage device (DASD) of the data it wanted to access, or {{the starting point for}} a search by key. BDAM programming was not easy and most organizations never used it themselves; but it was the fastest way to access data on disks and many software companies used it in their products, especially database management systems such as ADABAS, IDMS and IBM's DL/I. It is also available from OS/360 Fortran. BDAM datasets are unblocked, with one <b>logical</b> <b>record</b> per physical record.|$|E
5000|$|IDT 'HELLO' TITL 'HELLO - {{hello world}} program' * DXOP SVC,15 Define SVC TMLUNO EQU 0 Terminal LUNO * R0 EQU 0 R1 EQU 1 R2 EQU 2 R3 EQU 3 R4 EQU 4 R5 EQU 5 R6 EQU 6 R7 EQU 7 R8 EQU 8 R9 EQU 9 R10 EQU 10 R11 EQU 11 R12 EQU 12 R13 EQU 13 R14 EQU 14 R15 EQU 15 * DATA WP,ENTRY,0 * [...] * Workspace (On the 990 we can [...] "preload" [...] registers) * WP DATA 0 R0 DATA 0 R1 [...] DATA >1600 R2 - End of program SVC DATA >0000 R3 - Open I/O opcode DATA >0B00 R4 - Write I/O opcode DATA >0100 R5 - Close I/O opcode DATA STRING R6 - Message address DATA STRLEN R7 - Message length DATA 0 R8 DATA 0 R9 DATA 0 R10 DATA 0 R11 DATA 0 R12 DATA 0 R13 DATA 0 R14 DATA 0 R15 * [...] * Terminal SVC block * TRMSCB BYTE 0 SVC op code (0 = I/O) TRMERR BYTE 0 Error code TRMOPC BYTE 0 I/O OP CODE TRMLUN BYTE TMLUNO LUNO TRMFLG DATA 0 Flags TRMBUF DATA $-$ Buffer address [...] TRMLRL DATA $-$ <b>Logical</b> <b>record</b> length TRMCHC DATA $-$ Character count * [...] * Message * STRING TEXT 'Hello world!' BYTE >D,>A STRLEN EQU $-STRING EVEN PAGE * [...] * Main program entry * ENTRY MOVB R3,@TRMOPC Set open opcode in SCB SVC @TRMSCB Open {{terminal}} MOVB @TRMERR,R0 Check for error JNE EXIT MOVB R4,@TRMOPC Set write opcode MOV R6,@TRMBUF Set buffer address [...] MOV R7,@TRMLRL Set <b>logical</b> <b>record</b> length MOV R7,@TRMCHC and character count SVC @TRMSCB Write message MOVB @TRMERR,R0 Check for error JNE CLOSE CLOSE MOVB R5,@TRMOPC Set close opcode SVC @TRMSCB Close terminal EXIT SVC R2 Exit program * END ...|$|E
50|$|BSAM allows {{programs}} {{to read and}} write physical blocks of data, as opposed to the more powerful but less flexible Queued Sequential Access Method (QSAM) which allows {{programs to}} access <b>logical</b> <b>records</b> within physical blocks of data.The BSAM user must be aware of the possibility of encountering short (truncated) blocks (blocks within a dataset which are shorter than the BLKSIZE of the dataset), particularly {{at the end of a}} dataset, but also in many cases within a dataset. QSAM has none of these limitations.|$|R
5000|$|Abstract Principles Taken to Their <b>Logical</b> Extremes (Unisound <b>Records,</b> 1995) ...|$|R
40|$|The {{machine-readable}} {{version of}} the Atlas as it is currently being distributed from the Astronomical Data Center is described. The data were obtained with the Oke multichannel scanner on the 5 -meter Hale reflector for purposes of synthesizing galaxy spectra, and the digitized Atlas contains normalized spectral energy distributions, computed colors, scan line and continuum indices for 175 selected stars covering the complete ranges of spectral type and luminosity class. The documentation includes a byte-by-byte format description, a table of the indigenous characteristics of the magnetic tape file, and a sample listing of <b>logical</b> <b>records</b> exactly as they are recorded on the tape...|$|R
5000|$|Regardless of organization, the {{physical}} structure of each record {{is essentially the}} same, and is uniform throughout the data set. This is specified in the DCB [...] parameter. [...] means that the records are of fixed length, specified via the [...] parameter, and [...] specifies a variable-length record. V records when stored on media are prefixed by a Record Descriptor Word (RDW) containing the integer length of the record in bytes. With [...] and , multiple logical records are grouped together into a single physical block on tape or disk. FB and VB are , and , respectively. The [...] parameter specifies the maximum length of the block. [...] could be also specified, meaning , meaning all the blocks except the last one were required to be in full [...] length. , or , means a <b>logical</b> <b>record</b> could be spanned across two or more blocks, with flags in the RDW indicating whether a record segment is continued into the next block and/or was continued from the previous one.|$|E
40|$|Detailed {{descriptions}} of the three files of the machine-readable catalog are given. The files of the original tape have been restructured and the data records reformatted to produce a uniform data file having a single <b>logical</b> <b>record</b> per star and homogeneous data fields. The characteristics of the tape version as it is presently being distributed from the Astronomical Data Center are given and the changes to the original tape supplied are described...|$|E
3000|$|Access {{services}} {{is in charge}} of the physical data representations of data records and provides access path structures like B-trees. It provides more complex access paths, mappings, particular extensions for special data models, that are represented in the Storage Services Layer. Moreover, it is responsible for sorting record sets, navigating through <b>logical</b> <b>record</b> structures, making joins, and similar higher-level operations. This layer represents a key factor to database performance. The Access Services Layer has functions that are comparable to those in the third and fourth layer as presented by Härder and Reuter [62, 65].|$|E
5000|$|... 2001: Cycle <b>Logical</b> (Cadenc Jazz <b>Records),</b> with Don Messina and Bill Chattin ...|$|R
40|$|A {{detailed}} description of the machine-readable version of the atlas as it is currently beng distributed from the Astronomical Data Center is given. The data were obtained with the Oke multichannel scanner on the 5 -meter Hale reflector for purposes of synthesizing galaxy spectra, and the digitized atlas contains normalized spectral energy distributions, computed colors, scan line and continuum indices for 175 selected stars covering the complete ranges of spectral type and luminosity class. The documentation includes a byte-by-byte format description, a table of the indigenous characteristics of the magnetic tape file, and a sample listing of <b>logical</b> <b>records</b> exactly as they are recorded on the tape...|$|R
40|$|The machine {{readable}} library as it {{is currently}} being distributed from the Astronomical Data Center is described. The library contains digital spectral for 161 stars of spectral classes O through M and luminosity classes 1, 3 and 5 in the wavelength range 3510 A to 7427 A. The resolution is approximately 4. 5 A, while the typical photometric uncertainty of each resolution element is approximately 1 percent and broadband variations are 3 percent. The documentation includes a format description, a table of the indigenous characteristics of the magnetic tape file, and a sample listing of <b>logical</b> <b>records</b> exactly as they are recorded on the tape...|$|R
40|$|This paper {{presents}} and discusses {{a radically different}} approach to multi-dimensional indexing based {{on the concept of}} the spacefilling curve. It reports the novel algorithms which had to be developed to create the first actual implementation of a system based on this approach, on some comparative performance tests, and on its actual use within the TriStarp Group at Birkbeck to provide a Triple Store repository. An important result that goes beyond this requirement, however, is that the performance improvement over the Grid File is greater the higher the dimension. 1 Introduction Underlying any dbms is some form of repository management system or data store. The classic and dominant model for such repositories is that of some form of <b>logical</b> <b>record</b> or data aggregate type with a collection of instances conforming to that type usually termed a file. Such file systems are, of course, also used directly in many applications. The data model of a dbms may be radically different f [...] ...|$|E
40|$|This {{document}} {{describes a}} database containing daily measurements of snow depth at 195 National Weather Service (NWS) first-order climatological {{stations in the}} United States. The data have been assembled and made available by the National Climatic Data Center (NCDC) in Asheville, North Carolina. The 195 stations encompass 388 unique sampling locations in 48 of the 50 states; no observations from Delaware or Hawaii {{are included in the}} database. Station selection criteria emphasized the quality and length of station records while seeking to provide a network with good geographic coverage. Snow depth at the 388 locations was measured once per day on ground open to the sky. The daily snow depth is the total depth of the snow on the ground at measurement time. The time period covered by the database is 1893 [...] 1992; however, not all station records encompass the complete period. While a station record ideally should contain daily data for at least the seven winter months (January through April and October through December), not all stations have complete records. Each <b>logical</b> <b>record</b> in the snow depth database contains one station`s daily data values for a period of one month, including data source, measurement, and quality flags...|$|E
40|$|Graduation date: 1973 It is {{necessary}} to evaluate and compare the characteristics of various methods of accessing data-files in order to utilize economically both the hardware and the software (Space and Time) supported by the digital on-line system. The {{purpose of this paper}} is to describe and evaluate the structure and use of four conventional methods of file organization: Sequential File, Indexed Sequential File, Partitioned File, and Direct File. Special attention is given to the Direct File, which possesses the fastest accessing time. Five selected Hash Coding Techniques, each associated with three methods of handling redundant keys, are simulated and examined with the use of a selected data model of 1024 random United States names, and the resulting "average number of search per record retrieval" are compared with their corresponding theoretical values. As Hash 1 has offered the best results, it has been used to evaluate the organization of the Direct File, and to compare this organization with that of the other files. The CDC- 3300 system hardware parameter, control cycle time, the internal core storage, and the auxiliary storage parameters are introduced. From these values and the average number of searches per record retrieval, an expression of <b>logical</b> <b>record</b> file size, or loading factor is developed. The file size, or loading factor varies for different methods of file structure and accessing, (based upon the selected testing program). The system characteristics consisting of the average throughput per record retrieval, achievable-throughput-rate capability and user operating cost per call (unit cost) are evaluated and compared. The file system uses the full name of the record and a fixed length numerical key. Two common internal searches, Linear search and Binary search, are evaluated and compared as the preliminary work of this investigation, as shown in Appendix B...|$|E
40|$|The {{database}} means {{a collection}} of many types of occurrences of <b>logical</b> <b>records</b> containing relationships between records and data elementary aggregates. Management System database (DBMS) - a set of programs for creating and operation of a database. Theoretically, any relational DBMS {{can be used to}} store data needed by a Web server. Basically, it was observed that the simple DBMS such as Fox Pro or Access is not suitable for Web sites that are used intensively. For large-scale Web applications need high performance DBMS's able to run multiple applications simultaneously. Hyper Text Markup Language (HTML) is used to create hypertext documents for web pages. The purpose of HTML is rather the presentation of information – paragraphs, fonts, tables, than semantics description document. internet, informations, web, dates...|$|R
40|$|Increasingly, le {{systems for}} multiprocessors are {{designed}} with parallel access to multiple disks, to keep I/O from becoming a serious bottleneck for parallel applications. Although le system software can transparently provide high-performance access to parallel disks, a new le system interface is needed to facilitate parallel access to a le from a parallel application. We describe the di culties faced when using the conventional (Unix-like) interface in parallel applications, and then outline ways to extend the conventional interface to provide convenient access to the le for parallel programs, while retaining the traditional interface for programs that have no need for explicitly parallel le access. Our interface includes a single naming scheme, a multiopen operation, local and global le pointers, mapped le pointers, <b>logical</b> <b>records,</b> multi les, and logical coercion for backward compatibility. ...|$|R
40|$|Increasingly, file {{systems for}} multiprocessors are {{designed}} with parallel access to multiple disks, to keep I/O from becoming a serious bottleneck for parallel applications. Although file system software can transparently provide high-performance access to parallel disks, a new file system interface is needed to facilitate parallel access to a file from a parallel application. We describe the difficulties faced when using the conventional (Unix-like) interface in parallel applications, and then outline ways to extend the conventional interface to provide convenient access to the file for parallel programs, while retaining the traditional interface for programs that have no need for explicitly parallel file access. Our interface includes a single naming scheme, a multiopen operation, local and global file pointers, mapped file pointers, <b>logical</b> <b>records,</b> multifiles, and <b>logical</b> coercion for backward compatibility. 1 Introduction Multiprocessors have increased in computational power to [...] ...|$|R
