165|97|Public
5000|$|Motion estimation: {{measures}} temporal {{difference between}} frames on the <b>luminance</b> <b>component</b> ...|$|E
5000|$|In these ANTIC modes COLPF2 {{is output}} as the [...] "background" [...] of the Playfield and COLBK is output as the border around the Playfield. The {{graphics}} or glyph pixels are output using only the <b>luminance</b> <b>component</b> of COLPF1 {{mixed with the}} color component of the background (usually COLPF2).|$|E
5000|$|The <b>luminance</b> <b>component</b> of a {{composite}} video signal varies between 0 V and approximately 0.7 V above the [...] "black" [...] level. In the NTSC system, there is a blanking signal level used during the front porch and back porch, and a black signal level 75 mV above it; in PAL and SECAM these are identical.|$|E
30|$|Fourth, the {{influence}} of different luminance filters (see Section 3.2) on the overall performance is evaluated in Section 5.4. We expect that luminance filtering improves the robustness of the approach, since it removes colors with particularly low and high <b>luminance</b> <b>components</b> which are often unreliably classified.|$|R
40|$|When a {{color image}} is {{converted}} into a monochrome one, <b>luminance</b> <b>components</b> of the pixels {{have been used as}} gray-levels for the representation of the monochrome image in HDTV standard. However, saliencies of the image embedded only in the chrominance components are disappeared in the monochrome image converted by using <b>luminance</b> <b>components.</b> To cope with this problem, A. A. Gooch et al. have proposed the salience-preserving color removal method called “Color 2 Gray. ” The monochrome image well reected the impression of an input color image can be yielded by Color 2 Gray. However, the calculation cost of that algorithm is tremendous, and its utility is not so much. In this paper, fast Color 2 Gray algorithms are proposed. The effectiveness of the proposed method is illustrated through the experiments...|$|R
50|$|Dot crawl is {{the popular}} {{name for a}} visual defect of color analog video {{standards}} when signals are transmitted as composite video, as in terrestrial broadcast television. It consists of animated checkerboard patterns which appear along horizontal color transitions (vertical edges). It results from intermodulation or crosstalk between chrominance and <b>luminance</b> <b>components</b> of the signal, which are imperfectly multiplexed in the frequency domain.|$|R
50|$|Analogue color video signals {{comprise}} two components: chrominance and luminance. The <b>luminance</b> <b>component</b> {{describes the}} brightness of {{each part of the}} picture, while the chrominance component describes the color tone. When displayed on a black-and-white monitor, the luminance signal produces a normal black-and-white image, while the chrominance signal manifests as a fine pattern of dots of varying size and intensity overlaid over the black-and-white picture. A related phenomenon is dot crawl, which can produce visual artifacts in color pictures.|$|E
5000|$|The {{priority}} {{relationship between}} Players/Missiles, and COLPF2 work {{according to the}} priority chart below. Player/Missile pixels with higher priorities will replace COLPF2 as the [...] "background" [...] color. COLPF1 always has the highest priority and cannot be obscured by Players or Missiles. The glyph/graphics pixels use the color component of highest priority color (Playfield, Player, or Missile), and the <b>luminance</b> <b>component</b> of COLPF1. Note that this behavior is also consistent where Player/Missile priority conflicts result in true black for the [...] "background". In effect, the color value CTIA/GTIA finally uses for the [...] "background" [...] color [...] "tints" [...] the COLPF1 foreground glyph/graphics pixels.|$|E
5000|$|ANTIC Text modes 2 and 3, and Map mode F behave {{differently}} with Player/Missile graphics {{from the}} other modes. COLPF1 used for the glyph or graphics pixels always has the highest priority and cannot be obscured by Players or Missiles. The color of COLPF1 always comes from the [...] "background" [...] which is ordinarily COLPF2. Therefore, where Players/Missiles and Fifth Player have priority over COLPF2 the COLPF1 glyph/graphics pixels use the color component of the highest priority color (Player or Missile), and the <b>luminance</b> <b>component</b> of COLPF1. This behavior is consistent where Player/Missile priority conflicts result in true black for the [...] "background". In summary, the color CTIA/GTIA finally determines to use [...] "behind" [...] the high-res pixel is then used to [...] "tint" [...] the COLPF1 foreground glyph/graphics pixels.|$|E
40|$|Abstract. In this paper, {{a robust}} and {{efficient}} face recognition {{system based on}} luminance distribution by using maximum likelihood estimation is proposed. The distribution of <b>luminance</b> <b>components</b> of the face region is acquired and applied to maximum likelihood test for face matching. The experimental {{results showed that the}} proposed method has a high recognition rate and requires less computation time...|$|R
40|$|We present {{techniques}} for the amplification of small contrast of bounded signals; one {{is based on}} gamma correction and another is of an unsharp-masking type; {{the one of the}} unsharp-masking type is suitably modified for its application on circular signals as well. We enhance the saturation and <b>luminance</b> <b>components</b> of high dynamic range images {{on the basis of a}} segmentation of the image into light and dark regions...|$|R
40|$|In this paper, {{we propose}} a novel {{algorithm}} for image enhancement in compressed (DCT) domain. Despite, few algorithms {{have been reported}} to enhance images in DCT domain proposed algorithm differs from previous algorithms {{in such a way that}} it enhances both dark and bright regions of an image equally well. In addition, it outperforms in enhancing the chromatic components as well as <b>luminance</b> <b>components.</b> Since the algorithm works in DCT domain, computational complexity is reduced reasonably...|$|R
5000|$|Ignoring color, all {{television}} systems work in {{essentially the}} same manner. The monochrome image seen by a camera (later, the <b>luminance</b> <b>component</b> of a color image) is divided into horizontal scan lines, some number of which make up a single image or frame. A monochrome image is theoretically continuous, and thus unlimited in horizontal resolution, but to make television practical, a limit had {{to be placed on}} the bandwidth of the television signal, which puts an ultimate limit on the horizontal resolution possible. When color was introduced, this necessity of limit became fixed. All analog television systems are interlaced: alternate rows of the frame are transmitted in sequence, followed by the remaining rows in their sequence. Each half of the frame is called a video field, and the rate at which fields are transmitted is one of the fundamental parameters of a video system. It is related to the utility frequency at which the electricity distribution system operates, to avoid flicker resulting from the beat between the television screen deflection system and nearby mains generated magnetic fields. All digital, or [...] "fixed pixel," [...] displays have progressive scanning and must deinterlace an interlaced source. Use of inexpensive deinterlacing hardware is a typical difference between lower- vs. higher-priced flat panel displays (Plasma display, LCD, etc.).|$|E
40|$|The {{invention}} {{concerns a}} device and {{a method for}} creating a saliency map of an image. It comprises the steps of: Projection of said image according to the <b>luminance</b> <b>component</b> and if said image is a color image, according to the <b>luminance</b> <b>component</b> {{and according to the}} chrominance components, Perceptual sub-bands decomposition of said components according to the visibility threshold of a human eye, Extraction of the salient elements of the sub-bands related to the <b>luminance</b> <b>component,</b> Contour enhancement of said salient elements in each sub-band related to the <b>luminance</b> <b>component,</b> Calculation of a saliency map from the contour enhancement, for each sub-band related to the <b>luminance</b> <b>component.</b> Creation of the saliency map {{as a function of the}} saliency maps obtained for each sub-band...|$|E
40|$|We {{report a}} {{calculation}} reduction method for color computer-generated holograms (CGHs) using color space conversion. Color CGHs are generally calculated on RGB space. In this paper, we calculate color CGHs in other color spaces: for example, YCbCr color space. In YCbCr color space, a RGB image {{is converted to}} the <b>luminance</b> <b>component</b> (Y), blue-difference chroma (Cb) and red-difference chroma (Cr) components. In terms of the human eye, although the negligible difference of the <b>luminance</b> <b>component</b> is well-recognized, the difference of the other components is not. In this method, the <b>luminance</b> <b>component</b> is normal sampled and the chroma components are down-sampled. The down-sampling allows us to accelerate the calculation of the color CGHs. We compute diffraction calculations from the components, and then we convert the diffracted results in YCbCr color space to RGB color space...|$|E
5000|$|Finally the <b>luminance</b> and {{chrominance}} <b>components</b> are scaled to 8-bit {{values by}} the following equations: ...|$|R
50|$|However, {{just as the}} {{modulation}} and demodulation of RF loses quality, {{the mixing}} of the various signals into the original composite signal does the same, causing a checkerboard video artifact known as dot crawl. Dot crawl is a defect that results from crosstalk due to the intermodulation of the chrominance and <b>luminance</b> <b>components</b> of the signal. This is usually seen when chrominance is transmitted with a high bandwidth, and its spectrum reaches into the band of the luminance frequencies. This {{has led to a}} proliferation of systems such as S-Video and component video to maintain the signals separately. Comb filters are also commonly used to separate signals, and eliminate artifacts, from composite sources.|$|R
40|$|This paper {{presents}} {{the enhancement of}} color images in the frequency domain with interpolation. The novelty in our approach is {{the treatment of the}} chromatic components, while previous techniques treated only the <b>luminance</b> <b>components</b> and the enhanced image is interpolated with bicubic interpolation. After interpolation, the interpolated surface is smoother than corresponding surface obtained by the enhancement. The treatment of chromatic components improves the visual quality of the images to a great extent. The proposed technique, is more efficient than the spatial domain method. This paper investigate how we can increase the number of pixels and there by smoothening the enhanced surface. General Terms Image processing, Digital camera application, Scaling coefficients, Algorithms...|$|R
40|$|A noise {{reduction}} system that divides the color video signal into its luminance and chrominance components is reported. The <b>luminance</b> <b>component</b> {{of a given}} frame is summed with the <b>luminance</b> <b>component</b> {{of at least one}} preceding frame which was stored on a disc recorder. The summation is carried out so as to achieve a signal amplitude equivalent to that of the original signal. The averaged luminance signal is then recombined with the chrominance signal to achieve a noise-reduced television signal...|$|E
40|$|This paper puts {{forward a}} new color multi-focus image fusion {{algorithm}} based on fuzzy theory and dual-tree complex wavelet transform {{for the purpose}} of removing uncertainty when choosing sub-band coefficients in the smooth regions. <b>Luminance</b> <b>component</b> is the weighted average of the three color channels in the IHS color space and it is not sensitive to noise. According to the characteristics, <b>luminance</b> <b>component</b> was chosen as the measurement to calculate the focus degree. After separating the <b>luminance</b> <b>component</b> and spectrum component, Fisher classification and fuzzy theory were chosen as the fusion rules to conduct the choice of the coefficients after the dual-tree complex wavelet transform. So fusion color image could keep the natural color information as much as possible. This method could solve the problem of color distortion in the traditional algorithms. According to the simulation results, the proposed algorithm obtained better visual effects and objective quantitative indicators...|$|E
30|$|Herein, {{the model}} {{designed}} for gray image is also applied to chrominance components since the human visual perception is {{more sensitive to}} <b>luminance</b> <b>component</b> than to chrominance compo-nents.|$|E
30|$|Image {{segmentation}} is {{an important}} preprocessing operation in image recognition and computer vision. This paper proposes an adaptive K-means image segmentation method, which generates accurate segmentation results with simple operation and avoids the interactive input of K value. This method transforms the color space of images into LAB color space firstly. And the value of <b>luminance</b> <b>components</b> is set to a particular value, {{in order to reduce}} the effect of light on image segmentation. Then, the equivalent relation between K values and the number of connected domains after setting threshold is used to segment the image adaptively. After morphological processing, maximum connected domain extraction and matching with the original image, the final segmentation results are obtained. Experiments proof that the method proposed in this paper is not only simple but also accurate and effective.|$|R
5000|$|In {{real-world}} photographs, {{the highest}} spatial-frequency detail consists mostly of variations in brightness ("luminance detail") rather than variations in hue ("chroma detail"). Since any noise reduction algorithm {{should attempt to}} remove noise without sacrificing real detail from the scene photographed, one risks a greater loss of detail from luminance noise reduction than chroma noise reduction simply because most scenes have little high frequency chroma detail to begin with. In addition, most people find chroma noise in images more objectionable than luminance noise; the colored blobs are considered [...] "digital-looking" [...] and unnatural, compared to the grainy appearance of luminance noise that some compare to film grain. For these two reasons, most photographic noise reduction algorithms split the image detail into chroma and <b>luminance</b> <b>components</b> and apply more noise reduction to the former.|$|R
40|$|As one {{of methods}} {{to improve the}} image quality, there is a method called {{multiscale}} retinex (MSR) which has been proposed by D. J. Jobson et al. In MSR, the reection components of an image are extracted and emphasized, and then the image with improved quality is obtained. This method is very useful and powerful especially for the visibility improvement of dark regions of the image. However, the resulting image tends {{to give us the}} unnatural impression because <b>luminance</b> <b>components</b> are removed, and the global contrast of the image is decreased in the processing. In this paper, a new MSR with a variable offset, which changes dependently on the local luminance information of the image, is proposed in order to overcome the disadvantage of the conventional MSR, and to further improve the image quality. Through the experiments, the effectiveness of the proposed method is illustrated...|$|R
40|$|Abstract — In this {{proposed}} reconstructed scheme the high frequency subband images are constructed by taking LWT of <b>Luminance</b> <b>component</b> of LR image. The edges are enhanced by introducing an intermediate stage by using stationary wavelet transform (SWT). LWT is applied {{in order to}} decompose an input image into different subbands. Then the high frequency subbands are interpolated. The estimated high frequency subbands are being modified by using high frequency subband obtained through SWT. Then to preserve the more information sparse representation is applied to <b>Luminance</b> <b>component</b> of LR image. Then modified high frequency subband images and sparesed image are combined to generate a <b>Luminance</b> <b>component</b> of high resolution image by using inverse SWT (ISWT). Then the Cb and Cr component of LR image is interpolated using bicubic interpolation. And at last the YCbCr to RGB conversion is done and High Resolution Colour Image can be generated. The quantitative and visual results are showing {{the superiority of the}} proposed technique over the conventional and state-of-art image resolution enhancement techniques...|$|E
40|$|This paper {{presented}} a video watermarking algorithm based on wavelet chaotic neural network. First, to enhance binary image’s security, the algorithm encrypted it with double chaotic based on Arnold and Logistic map, Then, the host video {{was divided into}} some equal frames and distilled the key frame through chaotic sequence which generated by Logistic. Meanwhile, we distilled the low frequency coefficients of <b>luminance</b> <b>component</b> and self-adaptively embedded the processed image watermark into the low frequency coefficients of the wavelet transformed <b>luminance</b> <b>component</b> with the wavelet neural network. The experimental result suggested that the presented algorithm has better invisibility and robustness against noise, Gaussian filter, rotation, frame loss and other attacks...|$|E
30|$|It {{is worth}} {{stressing}} that sepia images are {{the input of}} the proposed algorithm. For this reason, only their <b>luminance</b> <b>component</b> has been processed and is shown; the two chrominance components can be kept unchanged if desired.|$|E
50|$|The CIE 1960 UCS {{does not}} define a <b>luminance</b> or {{lightness}} <b>component,</b> but the Y tristimulus {{value of the}} XYZ color space or a lightness index similar to W* of the CIE 1964 color space are sometimes used.|$|R
40|$|AbstractThere {{appear to}} be two modes of {{stereoscopic}} processing: a conventional linear operation that is dependent on correspondence between local <b>luminance</b> <b>components</b> in the two eyes’ views, and a non-linear or second-order processing mode. This second mode may use disparity information provided by particular ‘non-Fourier’ features of the stimulus such as the contrast envelope. Preliminary results suggest that people who fail standard clinical stereotests are able to extract non-linear disparity information from Gabor stimuli [McColl & Mitchell, 1998. Vision Research, 38, 1889 – 1900]. Here we evaluate {{the status of the}} non-linear mechanism in such individuals by using two types of contrast enveloped stimuli, namely random line and Gabor micropatterns, in a task that requires near/far depth judgements [Ziegler & Hess, 1999. Vision Research, 39, 1491 – 1507]. Although our sample was small, three of our four subjects who had performed poorly on at least one standard clinical test of stereopsis could perform the task, as well as one ‘stereoblind’ subject who had failed all four standard clinical tests. The overall results suggest that individuals with stereoanomalies show a diversity of deficits, but some nevertheless can see depth using ‘non-linear’ mechanisms...|$|R
40|$|Abstract—We {{develop a}} new biologically {{motivated}} algorithm for representing natural images using successive projections into complementary subspaces. An image is first projected into an edge subspace spanned using an ICA basis adapted to natural images which captures the sharp features of an image like edges and curves. The residual image obtained after extraction of the sharp image features is approximated using a mixture of probabilistic principal component analyzers (MPPCA) model. The model is consistent with cellular, functional, information theoretic, and learning paradigms in visual pathway modeling. We demonstrate the efficiency of our model for representing different attributes of natural images like color and luminance. We compare the performance of our model in terms of quality of representation against commonly used basis, like the discrete cosine transform (DCT), independent component analysis (ICA), and principalcomponents analysis (PCA), basedon their entropies. Chrominance and <b>luminance</b> <b>components</b> of images are represented using codes having lower entropy than DCT, ICA, or PCA for similar visual quality. The model attains considerable simplification for learning from images by using a sparse independent code for representing edges and explicitly evaluating probabilities in the residual subspace. Index Terms—Computer vision, feature representation, statistical models, clustering algorithms, machine learning, color. ...|$|R
30|$|Bit {{rate and}} PSNR. As usual for WZ video coding, only the <b>luminance</b> <b>component</b> of each frame {{is used to}} compute the overall bit rate and PSNR which always {{considers}} both the key frames and WZ frames.|$|E
40|$|A {{method for}} {{generating}} color video signals representative of color {{images of a}} scene includes the following steps: focusing light from the scene on an electronic image sensor via a filter having a tri-color filter pattern; producing, from outputs of the sensor, first and second relatively low resolution luminance signals; producing, from outputs of the sensor, a relatively high resolution luminance signal; producing, from a ratio of the relatively high resolution luminance signal to the first relatively low resolution luminance signal, a high band <b>luminance</b> <b>component</b> signal; producing, from outputs of the sensor, relatively low resolution color component signals; and combining each of the relatively low resolution color component signals with the high band <b>luminance</b> <b>component</b> signal to obtain relatively high resolution color component signals...|$|E
40|$|The {{brightness}} adjustment method for the night-vision image enhancement is considered in this paper. The color RGB night-vision image {{is transformed into}} an uncorrelated color space [...] - the YUV space. According to {{the characteristics of the}} night-vision image, we develop the modified Retinex algorithm based on the S curve firstly, by which the <b>luminance</b> <b>component</b> is enhanced and the brightness of the night-vision image is effectively improved. Then the <b>luminance</b> <b>component</b> of source image is enhanced by the selective and nonlinear gray mapping to retain the essential sunlight and shade information. Based on the two enhancement images, the night-vision image with enough bright and necessary sunlight and shade information is combined by the weighted parameter. According to experimental results, the night-vision image obtained is very fit for the visual observation...|$|E
40|$|Humans {{perceive}} the content (gist) {{of a scene}} very rapidly within about 40 ms [Castelhano and Henderson, 2008 Journal of Experimental Psychology Human Perception and Performance 43 (3) 660 - 675]. It has also been demonstrated that colours contribute to {{the perception of the}} gist of a scene if the colours are diagnostic for the distinction of scenes (Oliva and Schyns, 2000 Cognitive Psychology 41 176 - 210). We presented 320 coloured photographs of 2 diagnostic (mountains and coasts) and 2 nondiagnostic colour scenes (cities and rooms), 80 per category, in a masking paradigm. The mask consisted of randomly distributed colour patches. SOA was varied between 20 and 80 ms, in steps of 20 ms and subjects had to indicate the gist of the scene (4 AFC). A control condition without masking was also included. In line with previous results we have found that the gist of nondiagnostic coloured scenes is extracted within 40 ms. However, if colour comes into play, the extraction of the scene gist is prolonged by about 20 ms. A possible reason for this outcome might be that nondiagnostic colour scenes are identified by their <b>luminance</b> <b>components</b> which are processed faster than the colour information, which in turn mediates the identification of diagnostic colour scene...|$|R
40|$|This thesis {{presents}} color hand gesture segmentation for static {{images with}} complex background along with tracking and detection of hand gesture from video sequence. This thesis {{consists of two}} works: 1) Static. 2) Dynamic. In the first part, aim is to automatically segment the hand gesture from a given image under different luminance conditions and complex backgrounds. The luminance value affects the color component of an image which leads to increase the noise level in the segmented image. This paper proposes a combined model of two color spaces i. e., HSI, YCbCr and morphological operations with labeling to improve the segmentation performance of color hand gesture from complex backgrounds in terms of completeness and correctness. The proposed color model separates the chrominance and <b>luminance</b> <b>components</b> of the image. The performance of the proposed method is demonstrated through simulation and the experime ntal results reveal that proposed method provides better performance accuracy compared to the HSI and YCbCr methods individually in terms of correctness and completeness. In the second part, aim is to automatic detection and tracking of hand gesture from v ideo sequence under different backgrounds. It involves three steps: 1). Hand tracking 2). Hand detection 3). Hand identification. Here all the simulations are done in MATLAB 10 environment...|$|R
40|$|When multi-view {{sequences}} {{are recorded}} using a setup of several cameras, significant discrepancies between the <b>luminance</b> and chrominance <b>components</b> {{of the different}} camera views can often be observed. These variations may well impair {{the performance of a}} multi-view coder or a renderer. In many cases, it is therefore desirabl...|$|R
