18|92|Public
5000|$|Brown, Driver, Briggs and Gesenius. Hebrew <b>Lexicon</b> <b>entry</b> for Dabaq. The Old Testament Hebrew Lexicon.|$|E
5000|$|<b>Lexicon</b> <b>entry</b> in Svenskt biografiskt handlexikon of 1906 (Swedish) Palbitzki, Mattias. In: Nordisk familjebok. Volume 20, Second Edition. Stockholm 1904-1926, p 1269 f. (Swedish) ...|$|E
5000|$|... where [...] ,and [...] Let {{there be}} an axiom for every symbol ,an axiom [...] for every {{production}} rule ,a <b>lexicon</b> <b>entry</b> [...] for every terminal symbol ,and Cut for the only rule.This categorial grammar generates the same language as the given CFG.|$|E
40|$|This paper {{discusses}} {{the problems that}} arise in NLP lexicons if German particle verbs are analyzed as words. I suggest analyzing them as phrasal constructions consisting of an adverb, an adjective, or a preposition and a verb. It is then not necessary to have separate <b>lexicon</b> <b>entries</b> for compositional particle verbs. Noncompositional particle verbs also do not need separate <b>lexicon</b> <b>entries</b> for tagging and parsing. They need multi-word <b>lexicon</b> <b>entries</b> for all those applications that operate on semantic representations...|$|R
40|$|Abstract. This project report {{presents}} the results of an approach in which synsets for Slovene wordnet were induced automatically from parallel corpora and already existing wordnets. First, multilingual lexicons were obtained from word-aligned corpora and compared to the wordnets in various languages in order to disambiguate <b>lexicon</b> <b>entries.</b> Then appropriate synset ids were attached to Slovene <b>entries</b> from the <b>lexicon.</b> In the end, Slovene <b>lexicon</b> <b>entries</b> sharing the same synset id were organized into a synset. The results were evaluated against a goldstandard and checked by hand...|$|R
40|$|We {{introduce}} an Icelandic {{corpus of}} more than 250 million running words and de-scribe the methodology to build it. The re-source is available for use free of charge. We provide automatically generated mono-lingual <b>lexicon</b> <b>entries,</b> comprising fre-quency statistics, samples of usage, co-occurring words and a graphical represen-tation of the word’s semantic neighbour-hood. ...|$|R
40|$|Today 2 ̆ 7 s {{international}} community faces challenging but not insurmountable obstacles, as do {{you in the}} bipartite quiz that follows. To begin, utilize each of the definitions below to access an appropriate <b>lexicon</b> <b>entry</b> of indicated length. Then supplement your response with the given letter of the alphabet and anagrammatize the outcome into {{a member of the}} United Nations...|$|E
40|$|Word sense {{discrimination}} {{is the process}} of distinguishing the number of unique senses of a target word in a given corpus. This thesis approaches the task of word sense discrimination as an unsupervised clustering problem and uses semantic and syntactic features not seen in the current literature. Using the features from the clusters, this thesis constructs a new <b>lexicon</b> <b>entry</b> for the target word...|$|E
40|$|Abstract: There {{are many}} known Arabic lexicons {{organized}} on different ways, {{each of them}} has a different number of Arabic words according to its organization way. This paper has used mathematical relations to count a number of Arabic words, which proofs the number of Arabic words presented by Al Farahidy. The paper also presents new way to build an electronic Arabic lexicon by using a hash function that converts each word (as input) to correspond a unique integer number (as output), these integer numbers {{will be used as}} an index to a <b>lexicon</b> <b>entry...</b>|$|E
40|$|The paper {{presents}} {{an experiment in}} which synsets for Slovene wordnet were induced automatically from several multilingual resources. Our research {{is based on the}} assumption that translations are a plausible source of semantically relevant information. More specifically, we argue that the translational relation on the one hand reduces ambiguity of a source word and on the other conveys semantic relatedness of a set of target words. We tried to identify sense distinctions of polysemous words and obtain sets of synonyms by first extracting multilingual lexicons from a word-aligned JRC-Acquis parallel corpus and then comparing them with the already existing wordnets in various languages. At this stage, <b>lexicon</b> <b>entries</b> were disambiguated and appropriate synset ids were assigned to their Slovene translation equivalents. Finally, the Slovene <b>lexicon</b> <b>entries</b> sharing the same assigned synset id were organized into a synset...|$|R
40|$|In {{this paper}} {{we present a}} {{proposal}} to help bypass the bottleneck of knowledge-based sys-tems working {{under the assumption that}} the knowledge sources are complete. We show how to create, on the fly, new <b>lexicon</b> <b>entries</b> us-ing lexico-semantic rules and how to create new concepts for unknown words, investigating a new linguistically-motivated model to trigger concepts in context. ...|$|R
40|$|Recognition {{using only}} visual {{evidence}} cannot always be successful due to limitations {{of information and}} resources available during training. Considering relation among <b>lexicon</b> <b>entries</b> is sometimes useful for decision making. In this paper, we present a method to capture lexical similarity of a lexicon and reliability of a character recognizer which serve to capture the dynamism of the environment. A parameter, lexical similarity, is defined by measuring these two factors as edit distance between <b>lexicon</b> <b>entries</b> and separability of each character's recognition results. Our experiments show that a utility function considering lexical similarity in a decision stage can enhance {{the performance of a}} conventional word recognizer. 1 Introduction Shapes of characters in handwriting vary widely. Some of the variation in the shpae of handwriting are categorized and methods to normalize the variations have been developed in previous literature [1]. However, still the variation of character sha [...] ...|$|R
40|$|Lexicon schemas {{and their}} use are {{discussed}} in this paper {{from the perspective of}} lexicographers and field linguists. A variety of lex-icon schemas have been developed, with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and com-pared to each other in terms of conversion and usability for this particular user group, using a common <b>lexicon</b> <b>entry</b> and providing examples for each schema under consideration. The formats are assessed and the final recommendation is given for the potential users, namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards, lexicographers and field linguists. 1...|$|E
40|$|Various {{techniques}} {{have been developed}} to automatically induce semantic dictionaries from text corpora and from the Web. Our research combines corpus-based semantic lexicon induction with statistics acquired from the Web to improve the accuracy of automatically acquired domain-specific dictionaries. We use a weakly supervised bootstrapping algorithm to induce a semantic lexicon from a text corpus, and then issue Web queries to generate co-occurrence statistics between each <b>lexicon</b> <b>entry</b> and semantically related terms. The Web statistics provide a source of independent evidence to confirm, or disconfirm, that a word belongs to the intended semantic category. We evaluate this approach on 7 semantic categories representing two domains. Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and {{can also be used to}} filter incorrect entries. ...|$|E
40|$|Abstract—A fast {{method of}} {{handwritten}} word recognition suitable for real time applications {{is presented in}} this paper. Preprocessing, segmentation and feature extraction are implemented using a chain code representation of the word contour. Dynamic matching between characters of a <b>lexicon</b> <b>entry</b> and segment(s) of the input word image is used to rank the lexicon entries in order of best match. Variable duration for each character is defined and used during the matching. Experimental results prove that our approach using the variable duration outperforms the method using fixed duration {{in terms of both}} accuracy and speed. Speed of the entire recognition process is about 200 msec on a single SPARC- 10 platform and the recognition accuracy is 96. 8 percent are achieved for lexicon size of 10, on a database of postal words captured at 212 dpi. Index Terms—Handwritten word recognition, segmentation algorithm, variable duration, chain code representation, dynami...|$|E
40|$|We {{deal with}} the {{automated}} acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon. Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed <b>lexicon</b> <b>entries</b> by heuristic cognate mapping. We validated the emergent lexical translation hypotheses by determining the similarity of fixed-window context vectors {{on the basis of}} Portuguese and Spanish text corpora. ...|$|R
40|$|While {{working on}} valency lexicons for Czech and English, it was {{necessary}} to define treatment of multiword entities (MWEs) with the verb as the central lexical unit. Morphological, syntactic and semantic properties of such MWEs had to be formally specified in order to create <b>lexicon</b> <b>entries</b> and use them in treebank annotation. Such a formal specification has also been used for automated quality control of the annotation vs. the <b>lexicon</b> <b>entries.</b> We present a corpus-based study, concentrating on multilayer specification of verbal MWEs, their properties in Czech and English, and a comparison between the two languages using the parallel Czech-English Dependency Treebank (PCEDT). This comparison revealed interesting differences in the use of verbal MWEs in translation (discovering that such MWEs are actually rarely translated as MWEs, at least between Czech and English) as well as some inconsistencies in their annotation. Adding MWE-based checks should thus result in better quality control of future treebank/lexicon annotation. Since Czech and English are typologically different languages, we believe that our findings will also contribute {{to a better understanding of}} verbal MWEs and possibly their more unified treatment across languages. This work has been supported by the Grant No...|$|R
40|$|Here we {{describe}} an electronic lexical resource for German {{and the structure}} of its <b>lexicon</b> <b>entries,</b> notably the structure of verbal single-word and multi-word entries. The verb as the center of the sentence structure, as held by dependency models, is also a basic principle of the JAKOB narrative analysis application, for which the dictionary is the background. Different linguistic layers are combined for construing <b>lexicon</b> <b>entries</b> with a rich set of syntactic and semantic properties, suited to represent the syntactic and semantic behavior of verbal expressions (verb patterns), extracted from transcripts of real discourse, thereby lexicalizing the specific meaning of a specific verb pattern in a specific context. Verb patterns are built by the lexicographer by using a parser analyzing the input of a test clause and generating a machine-readable property string with syntactic characteristics and propositions for semantic characteristics grounded in an ontology. As an example, the German idiomatic expression an den Karren fahren (to come down hard on somebody) demonstrates the overall structure of a dictionary entry. The goal is to build unique dictionary entries (verb patterns) with reference to the whole of their properties...|$|R
40|$|Treatment {{of meaning}} in NLP is greatly {{facilitated}} if semantic analysis and generation systems rely on a language-neutral, independently motivated world model, or ontology. However, {{the benefits of the}} ontology are somewhat offset in practice by the difficulty of its acquisition. This is why a number of computational linguists make a conscious choice to bypass ontology in their semantic deliberations. This decision is often justified by questioning the principles underlying ontologies and by challenging the ontology-based semantic enterprise on the grounds of its ostensible irreproducibility. In this paper we illustrate, on the example of the <b>lexicon</b> <b>entry</b> for the Spanish verb dejar, the expressive power of lexical-semantic descriptions based on the ontology used in the Mikrokosmos machine translation project. This expressive power is compared with that of some of the non-ontological approaches to lexical semantics. We argue that these approaches in reality rely on ontologies in ever [...] ...|$|E
40|$|Abstract—This paper {{describes}} how a supervised learning method {{is used for}} assigning inflectional paradigms to organizational named entities as the main prerequisite for generating a morphological lexicon of these entities. An inflectional paradigm consists {{of a set of}} rules for generating all forms of a <b>lexicon</b> <b>entry.</b> A morphological lexicon consists of lexicon entries and their corresponding forms. This type of language resource is crucial in tasks such as natural language generation (generating natural language business news from database data and news templates) and named entity identification (necessary step in data mining and business intelligence). The basic resource used in this research is a list of 106. 530 named entities of organizations given in basic form (nominative) and ranked by relevance. On the first 5. 000 manually tagged named entities 59 inflectional paradigm classes are defined. Using linear successive abstraction, a suffix model is trained, validated and tested on this tagged dataset. Morphological lexica of general language, personal names and settlements are used as additional resources in the decision process. The achieved accuracy on the test set is 98, 70 %. I...|$|E
40|$|We {{present an}} {{integrated}} approach to corpus and lexicon development, {{both for the}} language archive and a repository of materials for local community. We assume that the target audiences of the archive and the repository have different interests in the same underlying body of data, and we seek to construct that body of data {{in such a way}} that both sets of interests can be addressed. This involves integration of three pieces of software: ELAN, for work with digital video/audio and its transcription; FLEx, for grammatical analysis and lexicon development; and MannX, a browser-based video player for language learning. Integrating corpus and the lexicon means the following functionality: • Each <b>lexicon</b> <b>entry</b> has links to its tokens in the corpus, which are in turn linked, via time alignment, to the media segments in which the tokens occur. • Each word in the corpus has a link to its <b>lexicon</b> <b>entry.</b> To achieve this, the corpus and the lexicon must be integrated throughout their development: • The lexicon maintains the list of all lexical and grammatical morphemes. • When a morpheme that is already in the lexicon is encountered in the corpus, interlinear glosses are filled in from the lexicon. • When a new morpheme is encountered, a new entry is created. • A change in the lexical entry is automatically propagated through the corpus. We seek to achieve this kind of integration by building a software "bridge" between ELAN and FLEx that supports the following workflow: • Starting in ELAN, do transcription and time-alignment at the utterance level ("phrase" in FLEx). • Export to FLEx for lexical and morphological analysis. • Export the results back into ELAN as symbolic subdivision or symbolic association tiers. • Further annotate in ELAN; perhaps time-align at the word level. As of August 2012, a functioning version of software has been implemented in JavaScript. It is being reimplemented in Java as a Web application. We expect to put the Java version on the Web for testing in September, and also upgrade the ELAN-MannX conversion. This process results in a corpus of media files, associated annotation files, and a FLEx-created lexicon, with links between them. A subset of these materials, transformed into different formats, will form the basis of a community repository. This will be a Web application, running on a remote or localhost server, that can run on a laptop or on an Android phone...|$|E
40|$|We {{show that}} the {{usefulness}} of manually created dictionaries can be enhanced for a statistical machine translation system when new translations are automatically added which are simple morphological transformations (plural forms, different verb inflections) of the original. Further improvement is possible when assigning probabilities to the <b>lexicon</b> <b>entries.</b> We describe a method to do this {{on the basis of}} an automatically trained statistical lexicon. Experimental results are given for Chinese to English translation tasks and show a significant improvement in translation quality...|$|R
40|$|We {{present a}} {{bootstrapping}} algorithm {{to create a}} semantic lexicon {{from a list of}} seed words and a corpus that was mined from the web. We exploit extraction patterns to bootstrap the lexicon and use collocation statistics to dynamically score new <b>lexicon</b> <b>entries.</b> Extraction patterns are subsequently scored by calculating the conditional probability in relation to a non-related text corpus. We find that verbs that are highly domain related achieved the highest accuracy and collocation statistics affect the accuracy positively and negatively during the bootstrapping runs...|$|R
40|$|This paper {{describes}} {{the process of}} building and using a new comprehensive lexicon of Czech verb valency frames based on complex valency frames. The main features of the <b>lexicon</b> <b>entries</b> are designed to bring important semantic information to computer processing of predicate constructions in running texts. The most notable features include two-level semantic labels with linkage to the Princeton and EuroWordNet hierarchy and surface verb frame patterns used for automatic syntactic analysis. Some implications for other languages, particularly English, Bulgarian and Romanian, are reported. ...|$|R
40|$|Lexicon of Czech verbal {{multiword}} expressions (VMWEs) used in Parseme Shared Task 2017. [URL] Lexicon {{consists of}} 4785 VMWEs, categorized into four categories according to Parseme Shared Task (PST) typology: IReflV (inherently reflexive verbs), LVC (light verb constructions), ID (idiomatic expressions) and OTH (other VMWEs with other than verbal syntactic head). Verbal multiword expressions {{as well as}} deverbative variants of VMWEs were annotated during the preparation phase of PST. These data were published as [URL] Czech part includes 14, 536 VMWE occurences: 1611 ID 10000 IReflV 2923 LVC 2 OTH This lexicon was created out of Czech data. Each <b>lexicon</b> <b>entry</b> is represented by one line in the form: type lemmas frequency PoS [used form 1; used form 2; [...] . ] (columns are separated by tabs) where: type [...] . {{is the type of}} VMWE in PST typology lemmas [...] . are space separated lemmatized forms of all words that constitutes the VMWE frequency [...] . is the absolute frequency of this item in PST data PoS [...] . is a space separated list of parts of speech of individual words (in the same order as in "lemmas") final field contains a list of all (1 to 18) used forms found in the data (since Czech is a flective language) ...|$|E
40|$|The usual {{challenges}} of transcribing spoken language are compounded for Southern Min (Taiwanese) because it lacks a generally accepted orthography. This study reports {{the development and}} testing of software tools for assisting such transcription. Three tools are compared, each representing {{a different type of}} interface with our corpus-based Southern Min lexicon (Tsay, 2007) : our original Chinese character-based tool (Segmentor), the first version of a romanization-based <b>lexicon</b> <b>entry</b> tool called Adult-Corpus Romanization Input Program (ACRIP 1. 0), and a revised version of ACRIP that accepts both character and romanization inputs and integrates them with sound files (ACRIP 2. 0). In two experiments, naive native speakers of Southern Min were asked to transcribe passages from our corpus of adult spoken Southern Min (Tsay and Myers, in progress), using {{one or more of these}} tools. Experiment 1 showed no disadvantage for romanization-based compared with character-based transcription even for untrained transcribers. Experiment 2 showed significant advantages of the new mixed-system tool (ACRIP 2. 0) over both Segmentor and ACRIP 1. 0, in both speed and accuracy of transcription. Experiment 2 also showed that only minimal additional training brought dramatic improvements in both speed and accuracy. These results suggest that the transcription of non-Mandarin Sinitic languages benefits from flexible, integrated software tools...|$|E
40|$|Previous {{research}} has shown that word-to-picture matching for targets that cannot be named at pre-test results in improved naming relative to untreated control items for people with aphasia. This paper replicates and extends this finding and investigates its source. Is the effect a result of priming of semantic representations, or of post-semantic mechanisms in word retrieval? The first experiment shows that word-to-picture matching with unrelated distractors improves naming at short (2 - 3 minutes) and long (up to 25 minute) lags. There was no effect of being made aware of the relationship between word-to- picture matching and picture naming. People who have a semantic impairment improve only with a short lag between word-to-picture matching and naming. Participants with less semantic impairment show larger priming effects that are equal at short and long lags between word-to-picture matching and naming. The second experiment shows that the facilitation effect is just as large for word-to-picture matching with unrelated distractors as with semantically-related distractors. Furthermore, overall {{there was no difference between}} matching with coordinate items and with associated items. The results of these experiments show that facilitation of naming by word-to-picture matching in people with aphasia cannot be a result of the priming of semantic representations. Instead they are consistent with two effects: word-to-picture matching results in priming at a lemma level for aphasic people with a semantic impairment that is only found with a short lag between word-to-picture matching and naming. Word-to-picture matching causes priming of the lemma to output <b>lexicon</b> <b>entry</b> mapping that benefits participants with less semantic impairment that is evident at both a short and long lag between word-to-picture matching and naming. These findings fit well with previous research on repetition priming of naming with normal subjects...|$|E
50|$|An HPSG grammar {{includes}} {{principles and}} grammar rules and <b>lexicon</b> <b>entries</b> which are normally {{not considered to}} belong to a grammar. The formalism is based on lexicalism. This means that the lexicon {{is more than just a}} list of entries; it is in itself richly structured. Individual entries are marked with types. Types form a hierarchy. Early versions of the grammar were very lexicalized with few grammatical rules (schema). More recent research has tended to add more and richer rules, becoming more like construction grammar.|$|R
5000|$|Alpines <b>Lexicon.</b> Paul Preuss <b>entry</b> at http://www.bergsteigen.at/de/lexikon.aspx?ID=64 ...|$|R
5000|$|... #Subtitle level 3: Dictionary, <b>Lexicon,</b> and Encyclopedia <b>Entries</b> ...|$|R
40|$|This is {{the first}} of two (see Vol. 2) closely related essay {{collections}} in the field of poetics, each with a concern for the complementary practices of writing and reading; this one comes at poetics by way of the interdisciplinary notion of Performance Writing and also includes a few related essays that take a pragmatic approach to arts pedagogy, particularly from the points of view of inter- and cross-disciplinarity. Of the 21 essays eight were published within the REF assessment period, with others submitted to earlier exercises. So far as possible they retain the shape of their original context of publication, with the appropriate modes and styles of address. An author’s preface and a foreword by Dr Lynch offer a framing for the collection, its provenance and purpose., Performance Writing was born out of a literary response to textual tendencies in visual, sonic, performance and digital practices and involved an attempt to rethink writing in the light of wider technological and cultural changes. This {{is the first}} published collection on the topic. The author’s practices as poet and teacher have an explicit influence on the approach. Defining pieces include ‘Thirteen Ways of Talking About Performance Writing’ and a <b>Lexicon</b> <b>entry</b> on Performance Writing from Performance Research Journal. Others explore specific grammatical considerations from the perspective of an expanded writing practice; there is an essay on the performativity of reading the ‘illegible’; on the use of imperatives in modes that resemble traffic signs; on the relation of the ideas of privacy, domesticity and public space; and on the part that words play {{in the work of the}} performance company, Lone Twin. The most recent essay revisits the founding principles of Performance Writing and attempt to assess whether or not the project remains salient. The collection helps to provide an affirmative answer...|$|E
40|$|In this {{dissertation}} I investigate ways {{to extend}} the annotation of treebanks, or parsed corpora, {{by taking advantage of}} the rich and sophisticated grammatical analysis embodied in a modern, constraint-based wide-coverage grammar. As the underlying processing engine I implement a full typed feature structure inference and HPSG parsing system in C#. I develop a method for annotating a treebank with typed feature structure information {{with the use of the}} LmGO ERG grammar, an existing widecoverage Head-Driven Phrase Structure Grammar (HPSG). I use standard techniques to head-lexicalise and binanse the treebank and further pre-process it to make it more compatible with the general grammatical structures assumed in HPSG. I then establish a mapping between local CFG and HPSG configurations and map local trees to HPSG phrase types. Finally the typed feature structures associated with the local trees are combined to complete resolved HPSG signs through constraint resolution and by applying the rules from the HPSG grammar. Discrepancies between the treebank and the HPSG grammar are analysed with respect to implications for grammar extension and automatic rich <b>lexicon</b> <b>entry</b> acquisition is also investigated. The aim of this work is to develop a method of constraint-based grammar-driven treebank annotation combining data- and theory-driven approaches to NLP. With this I aim to produce a richer treebank to demonstrate the benefits of using an existing widecoverage grammar for treebank annotation and to explore ways of using treebanks to extend grammar coverage for sophisticated wide-coverage constraint-based grammars, with possible implications for robust parsing. In experiments the annotation method achieves a coverage of 99 8 % of the ATIS corpus, with 95 3 % non-fragment trees receiving a successful resolution, using a basic HPSG grammar. Using the full LinGO ERG grammar, 68 8 % of non-fragment trees are resolved, and lexical type mapping for main verbs and nouns achieves a level of detail close to that of pre-defined lexical items. Also several trees for which the un-annotated string cannot be parsed by the LinGO ERG grammar receive a resolution in the annotation method, and words and subcategorisation frames not in the LinGO ERG lexicon are identified and handled. With the direct use of LinGO ERG grammar in the annotation the resulting lexical and phrasal signs are fully LinGO-compatible and can be easily incorporated back in the grammar...|$|E
40|$|International audienceWe {{introduce}} a generic approach for transferring part-of-speech annotations from a resourced language to a non-resourced but etymologically close language. We first infer a bilingual lexicon {{between the two}} languages with methods based on character similarity, frequency similarity and context similarity. We then assign part-of-speech tags to these bilingual <b>lexicon</b> <b>entries</b> and annotate the remaining words {{on the basis of}} suffix analogy. We evaluate our approach on five language pairs of the Iberic peninsula, reaching up to 95 % of precision on the lexicon induction task and up to 85 % of tagging accuracy...|$|R
40|$|Abstract—The {{types of}} lexicons {{necessary}} for Transparent Intensional Logic (TIL) logical analysis will be described. We {{will show the}} algorithm for analysing the TIL verbal object as {{the core of a}} clause construction within the sentence analysis. Examples of verb frame analysis for Czech words will be presented. We also depict a way of enriching the <b>lexicon</b> <b>entries</b> as combinations of the descriptions of lexical units as they are developed within the area of lexical semantics (e. g. WordNet) with logical analysis of sentence meanings worked out within the Transparent Intensional Logic framework. Index Terms—TIL, logical analysis, verb frames...|$|R
30|$|The {{limitations}} of the proposed method is that the expansion of seed cache needs be made over different medical lexicons, instead of web lexicons, which {{may result in a}} more comprehensive expansion of initial lexicon. The increased rate of irrelevant words is due to generalized nature of web repositories, more specific health-related lexicons should be exploited to reduce the noise in expanded lexicon. Another possible future direction is to investigate the dynamic updating of <b>lexicon</b> <b>entries</b> over different online repositories, such as web thesaurus and biomedical dictionaries. Analyzing the effect of multiple senses on sentiment classification of health-related words would be another extension of the current study.|$|R
40|$|Extracting textual {{data from}} Greek corpuses poses {{additional}} difficulties than in English texts as inclinations and intonation differentiate terms of equal information weight. Pre-processing and normalization of text {{is an important}} step before the extraction procedure as it leads to fewer rules and <b>lexicon</b> <b>entries,</b> thus to less execution time and greater success of the mining process. This paper presents a system accessible via the Web which automatically extracts data from Greek texts. The domain of conference announcements is utilized for experimentation purposes. The success of the extraction procedure is discussed on the basis of an evaluative study. The conclusions and the techniques discussed are applicable to other domains as well...|$|R
