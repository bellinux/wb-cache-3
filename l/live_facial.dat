8|12|Public
40|$|If {{humans can}} detect {{the wealth of}} tactile and haptic {{information}} potentially available in <b>live</b> <b>facial</b> expressions of emotion (FEEs), they should be capable of haptically recognizing the six universal expressions of emotion (anger, disgust, fear, happiness, sadness, and surprise) at levels well above chance. We tested this hypothesis in the experiments reported here. With minimal training, subjects 2 ̆ 7 overall mean accuracy was 51...|$|E
40|$|We {{present a}} {{complete}} integrated system for <b>live</b> <b>facial</b> puppetry that enables high-resolution real-time facial expression tracking with transfer to another person’s face. The system utilizes a real-time structured light scanner that provides dense 3 D data and texture. A generic template mesh, fitted to a rigid {{reconstruction of the}} actor’s face, is tracked offline in a training stage {{through a set of}} expression sequences. These sequences are used to build a person-specific linear face model that is subsequently used for online face tracking and expression transfer. Even with just a single rigid pose of the target face, convincing real-time facial animations are achievable. The actor becomes a puppeteer with complete and accurate control over a digital face. 1...|$|E
40|$|Face {{detection}} {{in various}} {{conditions such as}} illumination, occlusion, complex background and different pose angles plays the most important role in face recognition. In this research, we proposed an improved algorithm for automatic <b>live</b> <b>facial</b> expression detection (FED) using radial basis function. Detect edges of the facial image by Genetic Algorithm and Fuzzy-C-means (GAFCM). The experimental results used various types of databases i. e. JAFFE, FEI, LFW-a, BioID, CMU and Own database. In this algorithm, detect a face using the fdlibmex algorithm. But we improved limitations of this algorithm using contrast enhancement. We solved the problem of low contrast images. In the preprocessing stage, apply median filtering for removing noise from an image. The detection rate has reached up to 100 % for expression recognition...|$|E
50|$|Using <b>live</b> models, whose <b>facial</b> {{and bodily}} {{features}} he depicted, LeQuire created two Caucasian women and one Caucasian man; an African-American man and woman; one Asian-American woman; a Native American man; and a Hispanic man and woman.|$|R
40|$|In {{this paper}} we briefly review {{and discuss the}} ideas of self, emotion, and facial expression, and {{describe}} how a digital art work {{in the form of}} an interactive installation entitled Alter Ego was created to publicly explore these concepts. This work makes use of a variety of strands of modern technology: facial feature tracking, automatic <b>facial</b> measurements from <b>live</b> video, <b>facial</b> expression detection, and realistic avatar and expression modeling in 3 D. In essence, the image of an autonomous alter ego of the user is created as a mirror reflection in real time. We further consider the place of Alter Ego in relation to contemporary human subjectivity, digital game theory, and its possible applications in research into the human psyche...|$|R
50|$|Faceware Live {{was shown}} {{for the first}} time at SIGGRAPH 2013. It was created to enable the {{real-time}} capture and retargeting of <b>facial</b> movements. The <b>live</b> capture of <b>facial</b> performance can use any video source to track and translate facial expressions into a set of animation values and transfer the captured data onto a 3D animated character in real time. In 2014, Faceware released Faceware Live 2.0. The update included the option to stream multiple characters simultaneously, instant calibration, improved facial tracking, consistent calibration, and support for high-frame-rate cameras.|$|R
40|$|With the ever-rising {{capabilities}} of motion capture systems; this project explored markerless facial motion capture programs using the Kinect Sensor for Xbox. Many systems today still use markers {{and end up}} retargeting after a motion capture recording. This project used a simpler process of setting up {{and being able to}} display the effects live. An off-the-shelf system was built using a computer, a Kinect Sensor, a plug-in from Brekel, and Autodesk software. The first goal was to create a process that was able to capture and project <b>live</b> <b>facial</b> motion for fewer than 500 USD. Anything over 500 USD was considered {{to be more of a}} professional studio set-up. With an inexpensive setup, amateur users can do motion capture outside of a studio. The second goal was to observe the outcome of the audiences 2 ̆ 7 responses and see if interaction felt more mechanical than human...|$|E
40|$|We {{describe}} a face modeling system which estimates complete facial structure and texture from a real-time video stream. The system {{begins with a}} face tracking algorithm which detects and stabilizes <b>live</b> <b>facial</b> images into a canonical 3 D pose. The resulting canonical texture is then processed by a statistical model to filter imperfections and estimate unknown components such as missing pixels and underlying 3 D structure. This statistical model is a soft mixture of eigenfeature selectors which span the 3 D deformations and texture changes across a training set of laser scanned faces. An iterative algorithm is introduced for determining the dimensional partitioning of the eigenfeatures to maximize their generalization capability over a cross-validation set of data. The model's abilities to filter and estimate absent facial components are then demonstrated over incomplete 3 D data. This ultimately allows the model to span known and regress unknown facial information from stabilized natural v [...] ...|$|E
40|$|ABSTRACT—If {{humans can}} detect {{the wealth of}} tactile and haptic {{information}} potentially available in <b>live</b> <b>facial</b> ex-pressions of emotion (FEEs), they should be capable of haptically recognizing the six universal expressions of emo-tion (anger, disgust, fear, happiness, sadness, and sur-prise) at levels well above chance. We tested this hypothesis in the experiments reported here. With minimal training, subjects ’ overall mean accuracy was 51 % for static FEEs (Experiment 1) and 74 % for dynamic FEEs (Experiment 2). All FEEs except static fear were successfully recognized above the chance level of 16. 7 %. Complementing these findings, overall confidence and information transmission were higher for dynamic than for corresponding static faces. Our performance measures (accuracy and confi-dence ratings, plus response latency in Experiment 2 only) confirmed that happiness, sadness, and surprise were all highly recognizable, and anger, disgust, and fear less so. Visual face processing is of strong evolutionary significance across many biological species because the face carries different categories of information that are all critical to survival: friend or stranger? predator or prey? potential mate? A substantial re-search literature in cognitive science and neuroscience has es-tablished that face processing is a crucial function of visual perception not only in humans, but in other species as well. Person identification by vision is highly dependent {{on the ability to}} successfully differentiate, recognize, and identify many different faces. Past research has shown that the hallmarks of visual processing of faces, relative to visual processing of other object categories, are that it is highly practiced (e. g. ...|$|E
40|$|We {{present a}} system for {{recording}} a <b>live</b> dynamic <b>facial</b> performance, capturing highly detailed geometry and spatially varying diffuse and specular reflectance information for each frame of the performance. The result is a reproduction of the performance that can be rendered from novel viewpoints and novel lighting conditions, achieving photorealistic integration into any virtual environment. Dynamic performances are captured directly, {{without the need for}} any template geometry or static geometry scans, and processing is completely automatic, requiring no human input or guidance. Our key contributions are a heuristic for estimating facial reflectance information from gradient illumination photographs, and a geometry optimization framework that maximizes a principled likelihood function combining multi-view stereo correspondence and photometric stereo, using multiresolution belief propagation. The output of our system is a sequence of geometries and reflectance maps, suitable for rendering in off-the-shelf software. We show results from our system rendered under novel viewpoints and lighting conditions, and validate our results by demonstrating a close match to ground truth photographs...|$|R
40|$|Macrostomia is a {{congenital}} deformity resulting from failure of fusion of maxillary and mandibular process. It {{is a rare}} {{congenital deformity}} with an incidence of 1 in 60, 000 to 1 in 300, 000 <b>live</b> births. Transverse <b>facial</b> clefts are more common on right side of face in unilateral cases. Males are more affected than females. Various surgical techniques have been described in the literature for the correction of these defects. We report a case of macrostomia corrected with Z-plasty closure for skin, overlapping muscle closure, and triangular mucosal flap for commissure, with a review on existing techniques...|$|R
40|$|Treacher Collins {{syndrome}} {{encompasses a}} group of closely related defects {{of the head and}} neck. It is a rare syndrome characterized by bilaterally symmetrical abnormalities derived from the first and second brachial arches and the nasal placode. It is an autosomal dominant disorder and its occurence ranges from 1 in 25, 000 to 1 in 50, 000 <b>live</b> births. The <b>facial</b> appearance of these patients can be improved by either surgical or prosthetic rehabilitation. In this case report we are presenting the features of a 13 -year-old boy with Treacher Collins syndrome. A multidisplinary approach was followed in managing the situation. The various treatment options and the steps involved in making an auricular prosthesis are also discussed...|$|R
40|$|Figure 1 : Our <b>live</b> <b>facial</b> reenactment {{technique}} {{tracks the}} expression of a source actor and transfers it to a target actor at real-time rates. The synthetic result is photo-realisticly re-rendered on top of the original input stream maintaining the target’s identity, pose and illumination. We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor. The novelty of our approach lies in the transfer and photo-realistic re-rendering of facial deformations and detail into the target video {{in a way that the}} newly-synthesized expressions are virtually indistinguishable from a real video. To achieve this, we accurately capture the facial performances of the source and target subjects in real-time using a commodity RGB-D sensor. For each frame, we jointly fit a parametric model for identity, expression, and skin reflectance to the input color and depth data, and also reconstruct the scene lighting. For expression transfer, we compute the difference between the source and target expressions in parameter space, and modify the target parameters to match the source expressions. A major challenge is the convincing re-rendering of the synthesized target face into the corresponding video stream. This requires a careful consideration of the lighting and shading design, which both must correspond to the real-world environment. We demonstrate our method in a live setup, where we modify a video conference feed such that the facial expressions of a different person (e. g., translator) are matched in real-time...|$|E
40|$|The {{importance}} of faces in human interaction explains {{the desire for}} synthetic faces as a communication vehicle in computer graphics. Unfortunately, animating a face {{is a very complex}} process, partly because of our very familiarity with the human face. Moreover, depending upon the application, there exists a wide range of faces to animate, whether realistically or artistically. We present here an animation system that captures <b>live</b> <b>facial</b> expressions from a performance actor, and uses them to animate in real-time a synthetic character. Our approach is based upon a bank of 3 D facial expressions of a synthetic model. Critical points on the face model are matched to live markers. A linear combination of the basic expressions obtained by minimizing Euclidean distance between corresponding points and markers is then used to construct the intermediate facial expression. Mapping the motion into a bank of expressions produces subtle motions more dependent on the characteristics of the model itself. This is more difficult to achieve with previous techniques deforming a model without any additional information on how the model should move. Our technique draws mostly on the technical skills of an artist to model facial expressions, and on the skills of a performance actor to bring life to a face, with most animation issues being automated. We also present improvements on the control of markers, on specific details for more delicate control of the eyelids, and on the use of filters applied to the sequence of markers displacements. More flexibility in the bank of expressions is provided by subdividing some of the expressions into independent components, and by controlling accessories such as teeth and the tongue only by means of the reconstructed expression. The resulting system is very flexible, intuitive to use, and the real-time animations provide immediate feedback to express and refine animator and performer skills. 1...|$|E
40|$|This {{dissertation}} {{thesis is}} an overview study of recognition of human individuals {{and its role}} in forensic identification. It begins with basic structures of identification, meaning and usage of mug books, facial composite identification, and for the most part, recognition. We discuss suitability and feasibility of these methods and base the reasoning on the results of research from other countries. Such methods are undoubtedly affected by eyewitness's identification accuracy and therefore we also focus on factors influencing this. We group the factors by the ones which can and the ones which cannot by affected by law enforcement while we pay a special attention to the variables which are objects of scientific research, such as sequential lineups versus simultaneous lineups, foil selection, age, race, and confidence of the eyewitnesses who are performing the identification. Finally, we include a practical administrator's manual which describes the identification procedure step by step in a greater detail: how to create photo arrays, <b>live</b> lineups, <b>facial</b> composite recognition, and mug books, how to instruct eyewitnesses, and how to supervise the entire lineup procedure up to documenting it. The guide is based on many U. S. manuals and contains standalone recommendations as well as explanations why some [...] ...|$|R
40|$|Copyright © 2014 Srikanth Gunturu et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Macrostomia is a congenital deformity resulting from failure of fusion of maxillary and mandibular process. It is a rare congenital deformity with an incidence of 1 in 60, 000 to 1 in 300, 000 <b>live</b> births. Transverse <b>facial</b> clefts are more common on right side of face in unilateral cases. Males are more affected than females. Various surgical techniques have been described in the literature for the correction of these defects. We report a case of macrostomia corrected with Z-plasty closure for skin, overlapping muscle closure, and triangular mucosal flap for commissure, with a review on existing techniques. 1...|$|R
30|$|Plessner, one of {{the main}} representatives of {{philosophical}} anthropology, highlights the expressivity of the <b>lived</b> body in <b>facial</b> expressions, gestures, postures, language as well as in laughing and crying (Plessner 2003). These expressions hint at the general, expressive character of human being-in-the-world. They materialize in practical embodiment as a subjective and social form. While externalizations are spontaneous and subjective corporal expressions of mimic and gestural nature, embodiments claim latency in the habitual. They are corporal forms of response under the condition of social orders, which become habitualized and incarnated in the mode of repetition. Within embodiment, individuals practically position themselves towards themselves and towards the social and, at the same time, comment on these acts of positioning by responding in front of others (Brinkmann 2016 a). Plessner defines embodiments as social modes of adopting a role, differentiating between elementary, representative and functional roles as modes of embodiment and as modes of disguise, understood in the original sense of persona (Plessner 1976). Within and through embodiments, we compare ourselves with others, judge them and identify with them (Waldenfels 2008, p. 168). Within processes of embodiment, the other (he/she/it) is present in a space of sociality in an elementary sense (Bedorf 2010). One’s own perspective on the lived body is therefore {{not the same as the}} perspective on the other. Embodied corporal behavior thus becomes tangible as a responsive event (Rödel 2015).|$|R
5000|$|In May 1945, he {{surrendered}} to the Red Army and became {{a prisoner of war}} of the Soviet Union. Blösche was sent to a camp administered by GUPVI (Main Administration for Affairs of Prisoners of War and Internees) shortly thereafter. In early 1946, he was repatriated to East Germany, still as an internee. In August 1946 he suffered a major accident at work which left {{the side of his face}} severely deformed. In 1947 his labour camp was dissolved, and Blösche was released. He returned to where his parents dwelled and <b>lived</b> quietly. His <b>facial</b> scars protected him from discovery as the SS soldiers were pictured in the photos of the Warsaw ghetto. He began living a normal life, was married, and had two children.File:Stroop Report - Warsaw Ghetto Uprising 06b.jpg||The famous photo, Blösche with gun at right.File:Stroop Report - Warsaw Ghetto Uprising 03.jpg||Blösche at right, with Jürgen Stroop at center watching housing blocks burn during the Warsaw Ghetto Uprising. Picture taken at Nowolipie street looking East, near intersection with Smocza street. On the left burning balcony of the townhouse Nowolipie 66.File:Stroop Report - Warsaw Ghetto Uprising 04.jpg||Photo from Nowolipie Street. In the back (from the left) are townhouses at Nowolipie 32 (fragment), 30 and 28. Second from right is Josef Blösche. , Stroop report original caption: [...] "Jewish Rabbis".File:Stroop Report - Warsaw Ghetto Uprising - NARA05.jpg||Stroop report showing Blosche at left. Original caption [...] "Die Räumung eines Betriebes wird gesprochen" [...] {Discussing the evacuation of the factory}.|$|R
40|$|Today, direct oral {{anticoagulants}} (DOAC) {{are widely}} used alternatives to Vitamin-K antagonists (VKA). Women of reproductive age may become pregnant during anticoagulation and, while VKA carry an embryotoxic potential, the risk of DOAC embryopathy is unknown. As a result, some patients elect to terminate pregnancy for fear of DOAC embryotoxicity. To assess the risk of DOAC embryopathy, we reviewed cases of DOAC exposure in pregnancy collected from physicians, literature and pharmacovigilance systems of drug authorities and manufacturers. A total of 357 reports including duplicates were available from which 233 unique cases could be identified. Information on pregnancy outcome was available in only 137 / 233 cases (58. 8 %) : 67 live births (48. 9 %); 31 miscarriages (22. 6 %); 39 elective pregnancy terminations (28. 5 %). In 93 cases (39. 9 %) no outcome data were available (including 3 cases of ongoing pregnancy). Of the 137 pregnancies with reported outcomes, seven showed abnormalities (5. 1 %) of which three (2. 2 %) could potentially be interpreted as embryopathy: <b>live</b> birth with <b>facial</b> dysmorphism; miscarriage in week 10 with limb abnormality; elective pregnancy termination due to a foetal cardiac defect in {{a woman who had}} to terminate a previous pregnancy due to Fallot tetralogy. Within its limitations (small numbers, incomplete outcome data) our results do not indicate that DOAC exposure in pregnancy carries a high risk of embryopathy or that DOAC exposure per se should be used to direct patient counselling towards pregnancy termination. Pregnancy outcome data are inconsistently captured in pharmacovigilance databases indicating the strong need for a more robust system of reportin...|$|R
40|$|This article {{contributes}} to the limited literature on the social consequences of cancer generated facial disfigurement by reporting {{the result of an}} exploratory analysis of interaction between facially disfigured cancer patients and strangers and acquaintances (secondary groups). Secondary groups are those in which membership occurs due to performance of formal and/or non-intimate roles. Interaction is studied as it takes place in different social settings. Indivi- duals who are affected by cancer {{of the head and neck}} region can now expect to survive for many years after the cancer is detected and later surgically removed. Because of surgery, these survivors live the rest of their <b>lives</b> with <b>facial</b> disfigurement and are stigmatized and socially excluded. It follows that a new and socially relevant situation has emerged: as medicine develops and allows more patients to survive, it forces them to spend significant portions of their lives dealing with the stigma associated with facial disfigurement. Research on social issues pertaining to facially disfigured cancer patients remains sparse. Limited knowledge has been produced on the “social context” within which interaction between the disfigured and relevant social groups takes place. To date most research has focused on the individual and his/her ability to adapt to the condition of facially disfigured. To address this scientific gap and document the manner through which the interaction process is socially created and evolves, interviews with fourteen facially disfigured cancer patients were carried out. These interviews were designed to reconstruct the interaction experiences of these individuals in different social contexts. Data were analyzed through the qualitative approach of grounded theory. Results indicate that patients can be divided into two groups: Occasionally Comfortable Patients and Always Comfortable Patients. Occasionally comfortable patients are individuals who experience different levels of comfort in interaction. In some situations they do not feel stigmatized, but other interactions constitute the contexts within which this discomfort emerges. Discomfort in interaction was employed as an indicator of stigmatization. Interacting groups were divided into small and large. Intrusion (unsolicited attention to patients) in interaction in large and small groups always generates uncomfortable situations. Sympathy (unsolicited comments and/or actions in support of patients) is associated with comfort in interaction in small groups and produces varying patterns in the case of large groups. Benign neglect (a situation in which interacting individuals do not pay particular attention to patients) produces comfort in interaction within large groups and varying outcomes in the case of small groups. Always comfortable patients are those who do not experience discomfort in interaction regard- less of the size and characteristics of the interacting group. The article concludes by stressing that facially disfigured cancer patients should be prepared to face different interaction patterns. Simultaneously, efforts should be made to educate patients and the general public about these interaction patterns...|$|R

