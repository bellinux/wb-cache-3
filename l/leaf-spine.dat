11|0|Public
5000|$|P Fabric — <b>Leaf-spine</b> {{physical}} Clos fabric controlled via SDN Controller ...|$|E
5000|$|P+V Fabric — <b>Leaf-spine</b> {{physical}} Clos fabric plus virtual switches {{controlled by}} SDN Controller ...|$|E
40|$|We propose Expeditus, a {{distributed}} congestion-aware load balanc-ing {{mechanism for}} Clos data center networks. The fundamental challenge in making load balancing congestion-aware is, how to collect real-time (in {{the order of}} RTT) congestion information from all possible paths, in a scalable and efficient manner. A naive solu-tion requires each edge switch to have congestion information for O(k 4) paths for a k-pod fat-tree, and recent proposals like CONGA only work for the two-tier <b>leaf-spine</b> topology. Expeditus relies on scalable one-hop information collection, where a switch only mon-itors buffer occupancy from and to its k/ 2 upstream neighbors, re-spectively. It further uses a two-stage path selection mechanism to aggregate relevant congestion information across switches and make near-optimal path selection decisions during TCP handshak-ing. We outline the basic idea of these mechanisms in this extended abstract. Preliminary ns- 3 simulations demonstrate that Expeditus outperforms ECMP in fat-tree networks, and outperforms CONGA significantly in <b>leaf-spine</b> topology...|$|E
40|$|Due to the {{requirement}} of hosting {{tens of thousands of}} hosts in today's data centers, data center networks strive for scalability and high throughput on the one hand. On the other hand, the cost for networking hardware should be minimized. Consequently, the number and complexity (e. g. TCAM size) of switches has to be minimized. These requirements led to network topologies like Clos and <b>Leaf-Spine</b> networks only requiring a shallow hierarchy of switches [...] -two levels for <b>Leaf-Spine</b> networks. The drawback of these topologies is that switches at higher levels like Spine switches need a high port density and, thus, are expensive and limit the scalability of the network. In this paper, we propose a data center network topology based on De Bruijn graphs completely avoiding a switch hierarchy and implementing a flat network topology of top-of-rack switches instead. This topology guarantees logarithmic (short) path length. We show that the required routing logic can be implemented by standard prefix matching operations in hardware (TCAM) allowing for using commodity switches without any modification. Moreover, forwarding requires only {{a very small number of}} forwarding table entries, saving costly and energy-intensive TCAM...|$|E
40|$|Abstract—Modern {{datacenter}} networks must {{support a}} mul-titude of diverse and demanding workloads at low cost {{and even the}} most simple architectural choices can impact mission-critical application performance. This forces network architects to continually evaluate tradeoffs between ideal designs and pragmatic, cost effective solutions. In real commercial envi-ronments the number of parameters that the architect can control is fairly limited and typically includes only the choice of topology, link speeds, oversubscription, and switch buffer sizes. In this paper we provide some guidance to the network architect about the impact these choices have on data path performance. We analyze <b>Leaf-Spine</b> topologies under realistic traffic workloads via high-fidelity simulations and identify what is important for performance and what is not important. I...|$|E
40|$|Low latency, {{especially}} at the tail, is increasingly demanded by interactive applications in data center networks. To im-prove tail latency, existing approaches require modifications to switch hardware and/or end-host stacks, making them dif-ficult to be deployed. We present the design, implementa-tion, and evaluation of RepFlow, an application layer trans-port based on node. js that can be deployed today. RepFlow replicates mice flows to cut tail latency in Clos topologies. node. js’s single threaded event-loop and non-blocking I/O makes flow replication highly efficient. We further imple-ment RepSYN to alleviate RepFlow’s negative impact in in-cast scenarios by only using the first connection that finishes TCP handshaking. Performance evaluation on a <b>leaf-spine</b> network testbed reveals that RepFlow is able to reduce the tail latency of small flows by more than 50 %. Also, Rep-SYN offers similar benefits in scenarios with and without incast. 1...|$|E
40|$|We {{present the}} design, implementation, and {{evaluation}} of CONGA, a network-based distributed congestion-aware load balancing mech-anism for datacenters. CONGA exploits recent trends {{including the use of}} regular Clos topologies and overlays for network vir-tualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to effi-ciently balance load and seamlessly handle asymmetry, without re-quiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed exper-iments, CONGA has 5 × better flow completion times than ECMP even with a single link failure and achieves 2 – 8 × better through-put than MPTCP in Incast scenarios. Further, the Price of Anar-chy for CONGA is provably small in <b>Leaf-Spine</b> topologies; hence CONGA is nearly as effective as a centralized scheduler while be-ing able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry...|$|E
40|$|Conventional static {{datacenter}} (DC) network designs offer extreme cost vs. performance tradeoffs—simple <b>leaf-spine</b> {{networks are}} cost-effective but oversubscribed, while “fat tree”-like solutions offer good worst-case performance but are expensive. Recent results make a promising case for augmenting an oversubscribed network with reconfigurable inter-rack wireless or optical links. Inspired {{by the promise}} of reconfigurability, this paper presents FireFly, an inter-rack network solution that pushes DC network design to the extreme on three key fronts: (1) all links are reconfigurable; (2) all links are wireless; and (3) non top-of-rack switches are eliminated altogether. This vision, if realized, can offer significant benefits in terms of increased flexibility, reduced equipment cost, and minimal cabling complexity. In order to achieve this vision, we need to look beyond traditional RF wireless solutions due to their interference footprint which limits range and data rates. Thus, we make the case for using free-space optics (FSO). We demonstrate the viability of this architecture by (a) building a proof-of-concept prototype of a steerable small form factor FSO device using commodity compo-nents and (b) developing practical heuristics to address algorithmic and system-level challenges in network design and management...|$|E
40|$|With the {{proliferation}} of cloud computing and the expected requirements of future Internet of Things (IoT) and 5 G network scenarios, more efficient and scalable Data Centers (DCs) will be required, offering very large pools of computational resources and storage capacity cost-effectively. Looking at todays' commercial DCs, they tend to rely on well-defined <b>leaf-spine</b> Data Center Network (DCN) topologies that not only offer low latency and high bisectional bandwidth, but also enhanced reliability against multiple failures. However, routing and forwarding solutions in such DCNs are typically based on IP, thus suffering from its limited routing scalability. In this work, we quantitatively evaluate the benefits that the Recursive InterNetwork Architecture (RINA) can bring into commercial DCNs. To this goal, we propose rule-based topological routing and forwarding policies tailored to the characteristics of publicly available Google's and Facebook's DCNs. These policies can be programmed in a RINA-enabled environment, enabling fast forwarding decisions in most scenarios with merely neighboring node information. Upon DCN failures, invalid forwarding rules are overwritten by exceptions. Numerical {{results show that the}} scalability of our proposal depends on the number of concurrent failures in the DCN rather than its size (e. g., number of nodes/links), dramatically reducing the total amount of routing and forwarding information to be stored at nodes. Furthermore, as routing information is only disseminated upon failures across the DCN, the associated communication cost of our proposals largely outperforms that of the traditional IP-based solutions. Peer ReviewedPostprint (published version...|$|E
40|$|The planned {{upgrades}} of {{the experiments}} at the Large Hadron Collider at CERN will require higher bandwidth networks for their data acquisition systems. The network congestion problem arising from the bursty many-to-one communication pattern, typical for these systems, will become more demanding. It is questionable whether commodity TCP/IP and Ethernet technologies in their current form will be still able to effectively adapt to the bursty traffic without losing packets due to the scarcity of buffers in the networking hardware. We continue our study {{of the idea of}} lossless switching in software running on commercial-off-the-shelf servers for data acquisition systems, using the ATLAS experiment as a case study. The flexibility of design in software, performance of modern computer platforms, and buffering capabilities constrained solely by the amount of DRAM memory are a strong basis for building a network dedicated to data acquisition with commodity hardware, which can provide reliable transport in congested conditions. In this paper we extend the popular software switch, Open vSwitch, with a dedicated, throughput-oriented buffering mechanism for data acquisition. We compare the performance under heavy congestion of typical Ethernet switches to a commodity server acting as a switch, equipped with twelve 10 Gbps Ethernet interfaces providing a total bandwidth of 120 Gbps. Preliminary results indicate that software switches with large packet buffers perform significantly better, reaching maximum bandwidth, and completely avoiding throughput degradation typical for hardware switches that suffer from high packet drop counts. Furthermore, we evaluate the scalability of the system when building a larger topology of interconnected software switches, highlighting aspects such as management, port density, load balancing, and failover. In this context, we discuss the usability of software-defined networking technologies, Open vSwitch Database and OpenFlow protocols, to centrally manage and optimize a data acquisition network. We build an IP-only <b>leaf-spine</b> network consisting of eight software switches running on separate physical servers as a demonstrator. We intend to show in this paper that building a high bandwidth lossless network based on software switches dedicated for data acquisition is feasible and can be considered as a viable solution for future small- and large-scale systems based on commodity TCP/IP and Ethernet...|$|E
40|$|The bursty many-to-one {{communication}} pattern, {{typical for}} data acquisition systems, is particularly demanding for commodity TCP/IP and Ethernet technologies. The problem arising from {{this pattern is}} widely known in the literature as incast and can be observed as TCP throughput collapse. It {{is a result of}} overloading the switch buffers, when a specific node in a network requests data from multiple sources. This will become even more demanding for future upgrades of the experiments at the Large Hadron Collider at CERN. It is questionable whether commodity TCP/IP and Ethernet technologies in their current form will be still able to effectively adapt to bursty traffic without losing packets due to the scarcity of buffers in the networking hardware. This thesis provides an analysis of TCP/IP performance in data acquisition networks and presents a novel approach to incast congestion in these networks based on software-based packet forwarding. Our first contribution lies in confirming the strong analogies between the TCP behaviour in data acquisition and datacenter networks. We also provide experimental evaluation of different proposals from the datacenter environment for application in data acquisition to improve performance and reduce buffer requirements. The second contribution lies in the design and experimental evaluation of a data acquisition network that is based on software switches. Performance has traditionally been the challenge of this approach, but this situation changes with modern server platforms. High performance load balancers, proxies, virtual switches and other network functions can be now implemented in software and not limited to specialised commercial hardware, thus reducing cost and increasing the flexibility. We first design and optimise a software-based switch with a dedicated, throughput-oriented buffering mechanism for data acquisition. Our experimental results indicate that it performs significantly better than some typical Ethernet switches under heavy congestion. The optimised software switch with large packet buffer reaches maximum bandwidth and completely avoids throughput degradation typical for hardware switches that suffer from high packet drop counts. Furthermore, we evaluate the scalability of the system when building a larger topology of interconnected software switches. We highlight aspects such as management, costs, port density, load balancing, and failover. In this context, we discuss the usability of software-defined networking technologies, Open vSwitch Database and OpenFlow, to centrally manage and optimise a data acquisition network. We have built an IP-only parallel <b>leaf-spine</b> network consisting of eight software switches running on separate physical servers as a demonstrator...|$|E

