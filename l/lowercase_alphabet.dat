12|17|Public
25|$|Georgia, in the past, used {{uppercase}} Series D with {{a custom}} <b>lowercase</b> <b>alphabet</b> on its freeway guide signs; the most distinctive {{feature of this}} typeface {{is the lack of}} a dot on lowercase i and j. More recent installations appear to include the dots.|$|E
25|$|Initially, the Division used all-uppercase Series E Modified for button-reflectorized {{letters on}} ground-mounted signs and mixed-case legend (lowercase letters with Series D capitals) for {{externally}} illuminated overhead guide signs. Several Eastern turnpike authorities blended all-uppercase Series E Modified with the <b>lowercase</b> <b>alphabet</b> for destination legends on their guide signs.|$|E
50|$|Georgia, in the past, used {{uppercase}} Series D with {{a custom}} <b>lowercase</b> <b>alphabet</b> on its freeway guide signs; the most distinctive {{feature of this}} typeface {{is the lack of}} a dot on lowercase i and j. More recent installations appear to include the dots.|$|E
30|$|Encoded into {{structures}} that mandate other requirements. For base 16 and base 32, this determines {{the use of}} upper- or <b>lowercase</b> <b>alphabets.</b> For base 64, the non-alphanumeric characters (in particular, “/”) may be problematic in file names and URLs.|$|R
50|$|Intellivision {{was also}} the first console to have a {{complete}} built-in character font. While Odyssey² had a limited character font (uppercase alphabet, numerals, and some other characters), Intellivision's system font had complete upper- and <b>lowercase</b> <b>alphabets,</b> numerals, and almost all of the punctuation and symbols found on standard computer keyboards.|$|R
50|$|The <b>Lowercase</b> Latin <b>Alphabet</b> {{subheading}} {{contains the}} standard 26-letter unaccented Latin alphabet in the minuscule.|$|R
50|$|Initially, the Division used all-uppercase Series E Modified for button-reflectorized {{letters on}} ground-mounted signs and mixed-case legend (lowercase letters with Series D capitals) for {{externally}} illuminated overhead guide signs. Several Eastern turnpike authorities blended all-uppercase Series E Modified with the <b>lowercase</b> <b>alphabet</b> for destination legends on their guide signs.|$|E
5000|$|FHWA Series A, B, C, D, E, and F were {{developed}} by the Public Roads Administration (which later became FHWA) during World War II. Draft versions of these typefaces were used in 1942 for signs on the Pentagon road network. [...] In 1949-50, {{as part of a}} research program into freeway signing carried out by the California Department of Transportation, Series E Modified was developed from Series E by thickening the stroke width to accommodate button reflectors for ground-mounted signs, while a <b>lowercase</b> <b>alphabet</b> was developed to allow mixed-case legend (consisting initially of Series D and lowercase letters) to be used on externally illuminated overhead signs. [...] The lowercase letters, paired with Series E Modified, later became the basis of a national standard for mixed-case legend on freeway guide signs with the 1958 publication of the AASHTO signing and marking manual for Interstate highways.|$|E
50|$|With {{the other}} special {{characters}} and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code. There was some debate {{at the time}} whether {{there should be more}} control characters rather than the <b>lowercase</b> <b>alphabet.</b> The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lowercase characters to sticks 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard. The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting. Locating the lowercase letters in sticks 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers.|$|E
50|$|The ZX Spectrum {{uses the}} same font as the ZX81 but adds many {{characters}} including the <b>lowercase</b> Latin <b>alphabet.</b>|$|R
5000|$|The <b>lowercase</b> Greek <b>alphabet</b> α, β, γ, ... is {{used for}} 4-dimensional spacetime, which {{typically}} take values 0 for time components and 1, 2, 3 for the spatial components.|$|R
5000|$|The <b>lowercase</b> Latin <b>alphabet</b> a, b, c, ... {{is used to}} {{indicate}} restriction to 3-dimensional Euclidean space, which take values 1, 2, 3 for the spatial components; and the time-like element, indicated by 0, is shown separately.|$|R
5000|$|Typefaces {{are born}} from the {{struggle}} between rules and results. Squeezing a square about 1% helps it {{look more like a}} square; to appear the same height as a square, a circle must be measurably taller. The two strokes in an X aren't the same thickness, nor are their parallel edges actually parallel; the vertical stems of a <b>lowercase</b> <b>alphabet</b> are thinner than those of its capitals; the ascender on a d isn't the same length as the descender on a p, and so on. For the rational mind, type design can be a maddening game of drawing things differently in order to make them appear the same. Jonathan Hoefler & Tobias Frere-Jones Similar subtle adjustments to create an even appearance occur in other fields. For example, in the game of go, the stones, which are black and white, are of slightly different sizes (black slightly larger), to give the appearance of being the same size.|$|E
5000|$|The strokes are the {{components}} of a letterform. Strokes may be straight, as in , or curved, as in [...] If straight, they may be horizontal, vertical, or diagonal; if curved, open or closed. Typographers also speak of an instroke, where one starts writing the letter, as {{at the top of}} , , and an outstroke, where the pen leaves off, as at the bottom of [...] Typefaces are born from the struggle between rules and results. Squeezing a square about 1% helps it look more like a square; to appear the same height as a square, a circle must be measurably taller. The two strokes in an X aren't the same thickness, nor are their parallel edges actually parallel; the vertical stems of a <b>lowercase</b> <b>alphabet</b> are thinner than those of its capitals; the ascender on a d isn't the same length as the descender on a p, and so on. For the rational mind, type design can be a maddening game of drawing things differently in order to make them appear the same.|$|E
5000|$|Typefaces {{are born}} from the {{struggle}} between rules and results. Squeezing a square about 1% helps it {{look more like a}} square; to appear the same height as a square, a circle must be measurably taller. The two strokes in an X aren't the same thickness, nor are their parallel edges actually parallel; the vertical stems of a <b>lowercase</b> <b>alphabet</b> are thinner than those of its capitals; the ascender on a d isn't the same length as the descender on a p, and so on. For the rational mind, type design can be a maddening game of drawing things differently in order to make them appear the same. Jonathan Hoefler & Tobias Frere-Jones In Eltra Corp. v. Ringer, the United States Court of Appeals for the Fourth Circuit held that typeface designs are not subject to copyright. However, in the USA novel and non-obvious typeface designs are subject to protection by design patents. [...] Digital fonts that embody a particular design are often subject to copyright as computer programs. [...] The names of the typefaces can be trademarked. As a result of these various means of legal protection, sometimes the same typeface exists in multiple names and implementations.|$|E
40|$|AbstractIn this paper, an Optical Character Recognition {{engine for}} Kannada and English {{character}} recognition is proposed based on zone features. The zone {{is one of}} the old concepts in case of document image analysis research. But this method is good in case of Kannada and English character recognition. The total of 2800 Kannada consonants and 2300 English <b>lowercase</b> <b>alphabets</b> sample images are classified based on the SVM classifier. All preprocessed images are normalized into 32 x 32 dimensions, it is optimum. Then the preprocessed image is divided into 64 zones of non overlapping and zone based pixel density is calculated for each of the 64 zones, there by generating 64 features. These features are fed to the SVM classifier for classification of character images. To test the performance of an algorithm 2 fold cross validation is used. The average recognition accuracy of 73. 33 % and 96. 13 % is obtained for Kannada consonants and English <b>lowercase</b> <b>alphabets</b> respectively. Further the average percentage of recognition accuracy of 83. 02 % is obtained for mixture input of both Kannada and English characters. The recognition accuracy obtained for Kannada consonants is low, because most of the characters are similar in shape. Hence, one may need to add some more dominating features to discriminating the characters. In this direction, the work is in progress. It is an initial attempt for mixture of Kannada and English characters recognition with single algorithm. The novelty of the algorithm is independent of thinning and slant of the characters...|$|R
50|$|The {{outer ring}} {{contains}} one uppercase alphabet for plaintext {{and the inner}} ring has a <b>lowercase</b> mixed <b>alphabet</b> for ciphertext. The outer ring also includes the numbers 1 to 4 for the superencipherment of a codebook containing 336 phrases with assigned numerical values.|$|R
5000|$|The Combining Diacritical Marks Supplement block {{contains}} additional medieval superscript letter diacritics, {{enough to}} complete the basic <b>lowercase</b> Latin <b>alphabet</b> except for q and y, a few small capitals and ligatures (ae, ao, av), and additional letters: ◌ᷓ◌ᷔ◌ᷕ◌ᷖ◌ᷗ◌ᷘ◌ᷙ◌ᷚ◌ᷛ◌ᷜ◌ᷝ◌ᷞ◌ᷟ◌ᷠ◌ᷡ◌ᷢ◌ᷣ◌ᷤ◌ᷥ◌ᷦ◌ᷧ◌ᷨ◌ᷩ◌ᷪ◌ᷫ◌ᷬ◌ᷭ◌ᷮ◌ᷯ◌ᷰ◌ᷱ◌ᷲ◌ᷳ◌ᷴ. There is also a combining subscript: ◌᷊.|$|R
40|$|A new {{technique}} for intelligent form removal {{has been developed}} along with a new method for evaluating its impact on optical character recognition. The form removal technique automatically detects the dominant lines in an image and erases them while preserving {{as much of the}} overlapping character strokes as possible. This method of form removal relaxes the recognition system's dependence on rigid form design, printing, and reproduction by automatically detecting and removing some of the physical structures (lines) on the form. The line detection and removal technique operates on loosely defined zones in which no image deskewing is performed. The technique was tested on a large number of randomly-ordered handprinted <b>lowercase</b> <b>alphabet</b> fields, as these letters (especially those with descenders) frequently touch and extend through the line along which they are written. It is shown that intelligent form removal can improve lowercase recognition by as much as 3 %, but this net increase in pe [...] ...|$|E
40|$|Four hundred nine {{first graders}} {{were asked to}} name {{the letters of the}} <b>lowercase</b> <b>alphabet</b> {{presented}} in fixed, nonalphabetical order. In going from the weak to the average to the strong groups of letter namers, the proportion of correct responses for each letter increased. However, an examination of their errors showed that the more competent the group was in letter naming, the higher the proportion of reversal errors in wrong responses (p <. 001). It was concluded that letter reversal errors are not necessarily indicators of a basic perceptual or cognitive deficit. Areversal error in letter recognition occurs when a child looks at a symbol and assigns it the name of its inverted, rotated, or mirror image. The error occurs most frequendy with the complex of symbols b-d-p-q, but also with other easily confused symbols such as u-n and m-w. A reversal error can interfere with a child's ability to read, {{but it can also be}} diagnostic of underlying neurological problems and is thought to carry the prognosis of long-term reading disability. Reversals were seen by Orton (1937) as the major symptom of strephosymbolia. He attributed this problem to a failure of one cerebral hemisphere to establish a clear dominance over the other. While his theory of incomplete hemispheric dominance is no longer accepted (Myers & Hammill 1976), many influential diagnosticians and reading therapists seem to be influenced by its implications. For example, Gillingham (1970) sees reversals as characteristic of children with specific learning disabilities. Gattegno (1966) observes that stu-dents who exhibit reversals are often referred for examination for indications of brain damage. Eisenberg (1966) demonstrates both the rejection of Orton's theory and the legacy of his thinking when he states, "Delayed establishment of laterality and reading defect are probably not causally related but may have a common underlying antecedent. " Not only is the literature replete with associa-tions between reversal errors and neurological disorder, but many psychological tests for organicity also include the premise that rotation...|$|E
40|$|Mathematics {{is often}} {{defined as the}} science of space and number. …it {{was not until the}} recent {{resonance}} of computers and mathematics that a more apt definition became fully evident: mathematics is the science of patterns. — Lynn Arthur Steen Ada Dietz introduced a novel method of weave design in her seminal monograph Algebraic Expressions in Handweaving [1]. Her idea was to use multivariate polynomials (polynomials in several variables) raised to different powers to produce sequences that could be used as the basis for design. Such design sequences can be used as profile sequences, color sequences, and so on [2 - 7]. Dietz Polynomials The polynomials Ada Dietz used consist of the sum of variables with unit coefficients raised to a power. An example is (a + b + c) 3. Note: Standard mathematical notation uses italic lowercase letters {{at the end of the}} alphabet, such as x, y, and z, for variables, and roman lowercase letters at the beginning of the alphabet, such as a, b, and c for constants. Our use of letters here is deliberately different, since in many uses, variables correspond to blocks, for which the first letters of the <b>lowercase</b> <b>alphabet</b> usually are used. The number of variables used corresponds to the number of blocks desired, while the power to which the polynomial is raised corresponds to the “degree of interaction ” among the blocks. For example, in (a + b + c + d) 2 there are four blocks, a, b, c, and d, with a small amount of interaction, while in (a + b) 5, there are two blocks, a and b, with a large amount of interaction. Design sequences are constructed from such expressions in the following way. First, the polynomial is multiplied out, combining like terms, to give the individual terms: 1 : (a + b + c + d) 2 = a 2 + 2 ab + 2 ac + 2 ad + b 2 + 2 bc + 2 bd + c 2 + 2 cd + d 2 2 : (a + b) 5 = a 5 + 5 a 4 b + 10 a 3 b 2 + 10 a 2 b 3 + 5 ab 4 + b 5 Next, powers are replaced by products of variables: 1 : a 2 + 2 ab + 2 ac + 2 ad + b 2 + 2 bc + 2 bd + c 2 + 2 cd + d 2 = aa + 2 ab + 2 ac + 2 ad + bb + 2 bc + 2 bd + c + 2 cd + dd 2 : a 5 + 5 a 4 b + 10 a 3 b 2 + 10 a 2 b 3 + 5 ab 4 + b 5...|$|E
50|$|The {{larger one}} is called Stabilis or fixed, the smaller {{one is called}} Mobilis movable. The {{circumference}} of each disk is divided into 24 equal cells. The outer ring contains one uppercase alphabet for plaintext and the inner ring has a <b>lowercase</b> mixed <b>alphabet</b> for ciphertext.|$|R
5000|$|Geohash-36, a {{coordinate}} encoding algorithm, uses radix 36 but uses {{a mixture}} of <b>lowercase</b> and uppercase <b>alphabet</b> characters {{in order to avoid}} vowels, vowel-looking numbers, and other character confusion.|$|R
5000|$|An {{optional}} checksum {{is represented}} using the <b>lowercase</b> English <b>alphabet.</b> It confirms the code as a Geohash-36 {{and provides a}} check for incorrect or transposed characters. It is calculated as modulus 26 of the sum of each character value (the altitude delimiters of [...] "A" [...] or [...] "a" [...] are valued at zero) multiplied by its position reading from left to right.|$|R
40|$|Abstract — Optical Character Recognition (OCR) is the {{mechanical}} or electronic translation {{of images of}} handwritten or typewritten text (usually captured by a scanner) into machine-editable text. The main aim of this project is to design an expert system which will be best to, “Optical Character Recognition ” that effectively can recognize a particular character of type format using the Feed Forward approach. OCR is a field of research in artificial intelligence, in pattern recognition and also in machine vision. Though academic {{research in the field}} that continues, the focus on OCR has been shifted to implementation of proven techniques. Optical character recognition (using optical techniques such as mirrors and lenses) and digital character recognition (using scanners and computer algorithms) were originally considered as separate fields. Because a very few applications survive that use the true optical techniques, the OCR term has been broadened now to include digital image processing as well. This system will be applicable of recognizing any number of characters including uppercase, <b>lowercase</b> <b>alphabets</b> and numerals...|$|R
50|$|In {{some forms}} of {{handwriting}} for English (and presumably other languages based on the Latin <b>alphabet),</b> <b>lowercase</b> q always has a hook tail. This is particularly evident in geometric sans-serif typefaces used to teach children how to write.|$|R
50|$|Early {{computers}} used {{a variety}} of four-bit binary coded decimal (BCD) representations and the six-bit codes for printable graphic patterns common in the U.S. Army (FIELDATA) and Navy. These representations included alphanumeric characters and special graphical symbols. These sets were expanded in 1963 to seven bits of coding, called the American Standard Code for Information Interchange (ASCII) as the Federal Information Processing Standard, which replaced the incompatible teleprinter codes in use by different branches of the U.S. government and universities during the 1960s. ASCII included the distinction of upper- and <b>lowercase</b> <b>alphabets</b> and a set of control characters to facilitate the transmission of written language as well as printing device functions, such as page advance and line feed, and the physical or logical control of data flow over the transmission media. During the early 1960s, while also active in ASCII standardization, IBM simultaneously introduced in its product line of System/360 the eight-bit Extended Binary Coded Decimal Interchange Code (EBCDIC), an expansion of their six-bit binary-coded decimal (BCDIC) representation used in earlier card punches.The prominence of the System/360 led to the ubiquitous adoption of the eight-bit storage size, while in detail the EBCDIC and ASCII encoding schemes are different.|$|R
40|$|A novel video based finger writing virtual {{character}} recognition system (FVCRS) {{is described in}} this paper. With this FVCR system, a human can enter character into a computer by just using the movement of fingertip, without any additional device such as a keyboard or a digital pen. This provides a new wireless character-inputting method. A simple but effective background model is built for segmenting human-finger movements from cluttered background. A robust fingertip detection algorithm based on feature matching is given, and recognition of the finger-writing character is by a DTW based classifier. Experiments show that the FVCRS can successfully recognize finger-writing uppercase and <b>lowercase</b> English <b>alphabet</b> with the accuracy of 95. 3 %, 98. 7 % respectively. 1...|$|R
40|$|Abstract- Online {{handwriting}} recognition today has special interest {{due to increased}} usage of the hand held devices {{and it has become}} a difficult problem because of the high variability and ambiguity in the character shapes written by individuals. One major problem encountered by researchers in developing character recognition system is selection of efficient features (optimal features). In this paper, Particle Swarm Optimization (PSO) is proposed for feature selection. However, ANN as one of the widely used classification techniques yields high accuracy but it is computationally heavy due to its iterative nature. Hence, in this work, PSO is integrated with Modified Counter Propagation Neural Network (MCPN) to enhancement the performance of the classifier in terms of recognition accuracy and recognition time. Experiments were conducted on conventional CPN, MCPN and PSO-based MCPN classifiers using 6, 200 handwritten character samples (uppercase (A-Z), <b>lowercase</b> (a-z) English <b>alphabet</b> and 10 digits (0 - 9)) collected from 100 subjects using G-Pen 450 digitizer and the system was tested with 100 character samples written by people who did not participate in the initial data acquisition process. Experimental results show promising results for the PSO-based MCPN classifier in terms of the performance measures. Keywords- Artificial Neural Network...|$|R

