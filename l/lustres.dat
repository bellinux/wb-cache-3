140|3108|Public
25|$|Lustre {{indicates}} how light reflects {{from the}} mineral's surface, {{with regards to}} its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic <b>lustres</b> include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite, resinous, such {{as members of the}} garnet group, silky which common in fibrous minerals such as asbestiform chrysotile.|$|E
25|$|Galicia {{was spared}} {{the worst of}} the {{fighting}} in that war: {{it was one of the}} areas where the initial coup attempt at the outset of the war was successful, and it remained in Nationalist (Franco's army's) hands throughout the war. While there were no pitched battles, there was repression and death: all political parties were abolished, as were all labor unions and Galician nationalist organizations as the Seminario de Estudos Galegos. Galicia's statute of autonomy was annulled (as were those of Catalonia and the Basque provinces once those were conquered). According to Carlos Fernández Santander, at least 4,200 people were killed either extrajudicially or after summary trials, among them republicans, communists, Galician nationalists, socialists and anarchists. Victims included the civil governors of all four Galician provinces; Juana Capdevielle, the wife of the governor of A Coruña; mayors such as Ánxel Casal of Santiago de Compostela, of the Partido Galeguista; prominent socialists such as Jaime Quintanilla in Ferrol and Emilio Martínez Garrido in Vigo; Popular Front deputies Antonio Bilbatúa, José Miñones, Díaz Villamil, Ignacio Seoane, and former deputy Heraclio Botana); soldiers who had not joined the rebellion, such as Generals Rogelio Caridad Pita and Enrique Salcedo Molinuevo and Admiral Antonio Azarola; and the founders of the PG, Alexandre Bóveda and Víctor Casas, as well as other professionals akin to republicans and nationalists, as the journalist Manuel <b>Lustres</b> Rivas or physician Luis Poza Pastrana. Many others were forced to escape into exile, or were victims of other reprisals and removed from their jobs and positions.|$|E
500|$|César Ritz once {{commented that}} the room was so heavily {{designed}} in bronze that it was fortunate that the hotel was built from steel, or the [...] "walls would collapse {{with the weight of}} all that bronze". Flanking the entrance to the main restaurant are two life-sized figures set in [...] "bronze vert after Clodion, holding gilded bronze <b>lustres</b> with six lights each, mounted on pedestals of polished Echaillon marble ornamented with bronze". The restaurant and adjacent guest room were designed by P. H. Remon and Sons of Paris. The ceiling is a described by Montgomery-Massingberd and Watkin as a [...] "painted trompe-l'oeil ceiling on which pinkish clouds drift across the blue sky encircled by a garlanded balustrade". Bronze chandeliers are also a feature, influenced by an 18th-century Augustin de Saint-Aubin engraving known as Le Bal Pare et Masque, and Le Festin by Moreau le Jeune, which was given by the City of Paris to the King and Queen on 21 January 1782.|$|E
5000|$|<b>Lustre</b> varies {{over a wide}} continuum, and {{so there}} are no rigid {{boundaries}} between the different types of <b>lustre.</b> (For this reason, different sources can often describe the same mineral differently. This ambiguity is further complicated by <b>lustre's</b> ability to vary widely within a particular mineral species.) The terms are frequently combined to describe intermediate types of <b>lustre</b> (for example, a [...] "vitreous greasy" [...] <b>lustre).</b>|$|R
50|$|Another {{approach}} {{used in the}} early years of <b>Lustre</b> is the liblustre library on the Cray XT3 using the Catamount operating system on systems such as Sandia Red Storm, which provided userspace applications with direct filesystem access. Liblustre was a user-level library that allows computational processors to mount and use the <b>Lustre</b> file system as a client. Using liblustre, the computational processors could access a <b>Lustre</b> file system even if the service node on which the job was launched is not a Linux client. Liblustre allowed data movement directly between application space and the <b>Lustre</b> OSSs without requiring an intervening data copy through the kernel, thus providing access from computational processors to the <b>Lustre</b> file system directly in a constrained operating environment. The liblustre functionality was deleted from <b>Lustre</b> 2.7.0 after having been disabled since <b>Lustre</b> 2.6.0, and was untested since <b>Lustre</b> 2.3.0.|$|R
40|$|In {{this paper}} {{the study of}} four {{significant}} <b>lustre</b> samples covering 9 th century AD polychrome and 10 th century AD monochrome <b>lustre</b> from Iraq is presented. The samples selected {{are representative of the}} earliest known <b>lustre</b> productions. The data obtained from the study of the medieval samples are compared to laboratory reproductions and gives important clues about the invention, perfection and success of <b>lustre</b> during this period. The change from polychrome to monochrome <b>lustre</b> decorations and the increase in the lead content of the glazes are the key parameters in the success of obtaining a golden <b>lustre...</b>|$|R
2500|$|In 1861, the {{direction}} of the theatre was assumed by Albina di Rhona, a Serbian ballerina and comic actress. She renamed it the New Royalty Theatre, and had it altered and redecorated by [...] "M. Bulot, of Paris, Decorator in Ordinary to his Imperial Majesty, Louis Napoleon", with [...] "cut-glass <b>lustres,</b> painted panels, blue satin draperies and gold mouldings". In the opening programme, di Rhona danced, the leader of the Boston Brass Band from America played a bugle solo, and a melodrama, Atar Gull, was performed, with a 14-year-old Ellen Terry in the cast. Still, the reopening was not a success.|$|E
6000|$|When {{all their}} blooms the meadows flaunt [...] To deck {{the morning of}} the year, Why tinge thy <b>lustres</b> jubilant [...] With {{forecast}} or with fear? ...|$|E
6000|$|... "By Jove! that's a flash," [...] exclaimed Lord Milford, as a {{blaze of}} {{lightning}} seemed to suffuse the chamber, and the beaming <b>lustres</b> turned white and ghastly in the glare.|$|E
40|$|We {{discuss the}} design and ongoing {{development}} of the Monitoring Extreme-scale <b>Lustre</b> Toolkit (MELT), a unified <b>Lustre</b> performance monitoring and analysis infrastructure that provides continuous, low-overhead summary information on the health and performance of <b>Lustre,</b> as well as on-demand, in- depth problem diagnosis and root-cause analysis. The MELT infrastructure leverages a distributed overlay network to enable monitoring of center-wide <b>Lustre</b> filesystems where clients are located across many network domains. We preview interactive command-line utilities that help administrators and users to observe <b>Lustre</b> performance at various levels of resolution, from individual servers or clients to whole filesystems, including job-level reporting. Finally, we discuss our future plans for automating the root-cause analysis of common <b>Lustre</b> performance problems. Comment: International Workshop on the <b>Lustre</b> Ecosystem: Challenges and Opportunities, March 2015, Annapolis M...|$|R
50|$|In {{a cluster}} with a <b>Lustre</b> file system, the system network {{connecting}} the servers and the clients is implemented using <b>Lustre</b> Networking (LNet), which provides the communication infrastructure {{required by the}} <b>Lustre</b> file system. Disk storage {{is connected to the}} <b>Lustre</b> MDS and OSS server nodes using direct attached storage (SAS, FC, iSCSI) or traditional storage area network (SAN) technologies.|$|R
50|$|<b>Lustre</b> 2.0, {{released}} in August 2010, {{was based on}} significant internally restructured code to prepare for major architectural advancements. <b>Lustre</b> 2.x clients cannot interoperate with 1.8 or earlier servers. However, <b>Lustre</b> 1.8.6 and later clients can interoperate with <b>Lustre</b> 2.0 and later servers. The Metadata Target (MDT) and OST on-disk format from 1.8 can be upgraded to 2.0 and later without the need to reformat the filesystem.|$|R
6000|$|... "Haste! {{and above}} Siberian snows [...] We'll sport amid the boreal morning; [...] Will mingle with her <b>lustres</b> gliding [...] Among the stars, the stars now hiding, [...] And now the stars adorning. [...] 95 ...|$|E
6000|$|She did not stop, however, {{till she}} had lit all the eighty, but Scheih Ibrahim was not {{conscious}} of this, and when, soon after that, Noureddin proposed {{to have some}} of the <b>lustres</b> lit, he answered: ...|$|E
6000|$|Ang. Aye, but he {{must not}} die! Spare his few years, [...] Which Grief and Shame will soon cut down to days! [...] One day of baffled crime must not efface [...] Near sixteen <b>lustres</b> crowned with brave acts.|$|E
40|$|In this paper, {{we look at}} how <b>Lustre</b> {{performs}} using Myricom's 10 Gb/s NICs. <b>Lustre</b> {{clients and}} servers can use these programmable NICs in either native Ethernet mode or in Myrinet mode. <b>Lustre</b> provides metrics for measuring metadata retrieval (small message) performance {{as well as for}} bulk read and write operations. We show that with multiple clients <b>Lustre</b> can achieve 95 % of line-rate performance using Ethernet (TCP) when reading but only 50 % when writing. We show that <b>Lustre</b> achieves a 95 % of line-rate with a single client when using the NIC in Myrinet mode. In both cases, <b>Lustre</b> can serve up to 45, 000 metadata requests per second using multiple clients...|$|R
40|$|The <b>Lustre</b> {{parallel}} {{file system}} has been widely adopted by high-performance computing (HPC) centers as an effective system for managing large-scale storage resources. <b>Lustre</b> achieves unprecedented aggregate performance by parallelizing I/O over file system clients and storage targets at extreme scales. Today, 7 out of 10 fastest supercomputers in the world use <b>Lustre</b> for high-performance storage. To date, <b>Lustre</b> development has focused on improving the performance and scalability of large-scale scientific workloads. In particular, large-scale checkpoint storage and retrieval, which is characterized by bursty I/O from coordinated parallel clients, has been the primary driver of <b>Lustre</b> development over the last decade. With the advent of extreme scale computing and Big Data computing, many HPC centers are seeing increased user interest in running diverse workloads that place new demands on <b>Lustre.</b> In March 2015, the International Workshop on the <b>Lustre</b> Ecosystem: Challenges and Opportunities was held in Annapolis, Maryland at the Historic Inns of Annapolis Governor Calvert House. This workshop series is intended to help explore improvements in the performance and flexibility of <b>Lustre</b> for supporting diverse application workloads. The 2015 workshop was the inaugural edition, and {{the goal was to}} initiate a discussion on the open challenges associated with enhancing <b>Lustre</b> for diverse applications, the technological advances necessary, and the associated impacts to the <b>Lustre</b> ecosystem. The workshop program featured a day of tutorials and a day of technical paper presentations. Comment: International Workshop on the <b>Lustre</b> Ecosystem: Challenges and Opportunities, March 2015, Annapolis M...|$|R
50|$|Since 2011 OpenSFS {{has been}} in charge of {{organizing}} the annual <b>Lustre</b> User Group (LUG) event, traditionally held in April, for discussion and seminars on <b>Lustre.</b> OpenSFS held the first annual APAC <b>Lustre</b> Users Group events in China and Japan in 2013.|$|R
6000|$|On skies {{still and}} starlit [...] White <b>lustres</b> take hold, [...] And grey flashes scarlet, [...] And red flashes gold. [...] And sun-glories cover [...] The rose, shed above her, [...] Like lover and lover [...] They flame and unfold.|$|E
60|$|But candles {{had been}} placed in the chandeliers and <b>lustres,</b> and the attendants were so far {{recovered}} from surprise as to recollect their use; the oversight was immediately remedied, and in a minute the apartment was in a blaze of light.|$|E
60|$|The {{ceiling of}} this {{apartment}} was richly painted, and richly gilt: from it were suspended three <b>lustres</b> by golden cords, which threw a softened light upon {{the floor of}} polished and curiously inlaid woods. At {{the end of the}} apartment was an orchestra.|$|E
50|$|<b>Lustre</b> 1.0.0 was {{released}} in December 2003, and provided basic <b>Lustre</b> filesystem functionality, including server failover and recovery.|$|R
50|$|In August 2011, OpenSFS {{awarded a}} {{contract}} for <b>Lustre</b> feature development to Whamcloud. This contract covered the completion of features, including improved Single Server Metadata Performance scaling, which allows <b>Lustre</b> to better take advantage of many-core metadata server; online <b>Lustre</b> distributed filesystem checking (LFSCK), which allows verification of the distributed filesystem state between data and metadata servers while the filesystem is mounted and in use; and Distributed Namespace (DNE), formerly Clustered Metadata (CMD), which allows the <b>Lustre</b> metadata to be distributed across multiple servers. Development also continued on ZFS-based back-end object storage at Lawrence Livermore National Laboratory. These features were in the <b>Lustre</b> 2.2 through 2.4 community release roadmap.In November 2011, a separate contract was awarded to Whamcloud {{for the maintenance of}} the <b>Lustre</b> 2.x source code to ensure that the <b>Lustre</b> code would receive sufficient testing and bug fixing while new features were being developed.|$|R
5000|$|Also in 2015, <b>Lustre</b> {{collaborated with}} Yassi Pressman's song, Hush, where <b>Lustre</b> showed off her rap skills. <b>Lustre</b> and partner James Reid also {{released}} {{their own version}} of the song [...] "On The Wings of Love" [...] for their teleserye with the same name.|$|R
6000|$|... "I won't bear it," [...] said Jane with a sob and a plunge {{upon the}} sofa {{that made the}} <b>lustres</b> of the chandeliers rattle. [...] "I wouldn't have asked you if I had thought you could be so hateful. I will never ask you again." ...|$|E
60|$|On the mantlepiece {{were white}} <b>lustres,</b> {{and a small}} {{soapstone}} Buddha from China, grey, impassive, locked in his renunciation. Besides these, two tablets of translucent stone beautifully clouded with rose and blood, and carved with Chinese symbols; then a litter of mementoes, rock-crystals, and shells and scraps of seaweed.|$|E
6000|$|... "Yes, they are," [...] Miss Shirley answered, {{looking around}} with a certain surprise, as if seeing them {{now for the first}} time. [...] "So much variety of color; and that {{burnished}} look that some of them have." [...] The trees, far and near, were giving their tones and <b>lustres</b> in the low December sun.|$|E
50|$|In February 2013, Xyratex Ltd., {{announced}} it acquired the original <b>Lustre</b> trademark, logo, website and associated intellectual property from Oracle. In June 2013, Intel began expanding <b>Lustre</b> usage beyond traditional HPC, such as within Hadoop. For 2013 as a whole, OpenSFS announced {{request for proposals}} (RFP) to cover <b>Lustre</b> feature development, parallel file system tools, addressing <b>Lustre</b> technical debt, and parallel file system incubators. OpenSFS also established the <b>Lustre</b> Community Portal, a technical site that provides a collection of information and documentation in one area for reference and guidance to support the <b>Lustre</b> open source community. On April 8, 2014, Ken Claffey announced that Xyratex/Seagate is donating the lustre.org domain back to the user community, and was completed in March, 2015.|$|R
40|$|<b>Lustre,</b> {{the central}} GSI file system for {{experiment}} data, is mounted now on each gStore data mover {{and supported by}} all appropriate actions of the command gstore. This has several advantages: 1. Data are transferred directly between gStore storage and <b>lustre</b> filesystem. The (desktop) node requesting the action via gstore command {{is not involved in}} the data transfer. As a result the data transfer rates between gStore data movers and <b>lustre</b> are up to the order of 100 MB/s. 2. The gStore clients initiating the data transfer don't need a mount to <b>lustre.</b> 3. Load balancing is available not only on the gStore server side, but also on the <b>lustre</b> client side utilizing the load balancing mechanisms of <b>lustre.</b> As the data transfer rates between gStore tape and <b>lustre</b> are limited by tape speed currently, tape files are retrieved directly to <b>lustre</b> and not- as elsewhere in gStore- via read cache. However, the general rule to access tape files with as few gstore commands as possible should also be followed in this case {{to reduce the number of}} tape mounts (see also chapter 3 below). When archiving files from <b>lustre</b> to gStore, the data are stored as usual at first in the gStore writ...|$|R
50|$|<b>Lustre</b> 1.8.0, {{released}} in May 2009, provided OSS Read Cache, improved {{recovery in the}} face of multiple failures, added basic heterogeneous storage management via OST Pools, adaptive network timeouts, and version-based recovery. It was a transition release, being interoperable with both <b>Lustre</b> 1.6 and <b>Lustre</b> 2.0.|$|R
60|$|Sweet reader! {{you know}} what a Toadey is? That {{agreeable}} animal which you meet every day in civilised society. But perhaps you have not speculated very curiously upon this interesting race. So much the worse! for you cannot live many <b>lustres</b> without finding it of some service to be a little acquainted with their habits.|$|E
6000|$|To and fro the Genius flies, [...] A light {{which plays}} and hovers [...] Over the maiden's head And dips {{sometimes}} {{as low as}} to her eyes. Of her faults I take no note, [...] Fault and folly are not mine; Comes the Genius,--all's forgot, Replunged again into that upper sphere He scatters wide and wild its <b>lustres</b> here.|$|E
6000|$|Who now remembers gay Cremorne, [...] And all its jaunty jills, And those wild {{whirling}} figures born [...] Of Jullien's grand quadrilles? With hats on {{head and}} morning coats There footed to his prancing notes [...] Our partner-girls and we; And the gas-jets winked, and the <b>lustres</b> clinked, And the platform throbbed as with arms enlinked [...] We {{moved to the}} minstrelsy.|$|E
40|$|There {{has been}} {{substantial}} {{development of the}} <b>Lustre</b> parallel filesystem prior to the configuration described below for this milestone. The initial <b>Lustre</b> filesystems that were deployed were directly connected to the cluster interconnect, i. e. Quadrics Elan 3. That is, the clients (OSSes) and Meta-data Servers (MDS) were all directly connected to the cluster's internal high speed interconnect. This configuration serves a single cluster very well, but does not provide sharing of the filesystem among clusters. LLNL funded the development of high-efficiency ''portals router'' code by CFS (the company that develops <b>Lustre)</b> to enable us to move the <b>Lustre</b> servers to a GigE-connected network configuration, thus {{making it possible to}} connect to the servers from several clusters. With portals routing available, here is what changes: (1) another storage-only cluster is deployed to front the <b>Lustre</b> storage devices (these become the <b>Lustre</b> OSSes and MDS), (2) this ''Lustre cluster'' is attached via GigE connections to a large GigE switch/router cloud, (3) a small number of compute-cluster nodes are designated as ''gateway'' or ''portal router'' nodes, and (4) the portals router nodes are GigE-connected to the switch/router cloud. The <b>Lustre</b> configuration is then changed to reflect the new network paths. A typical example of this is a compute cluster and a related visualization cluster: the compute cluster produces the data (writes it to the <b>Lustre</b> filesystem), and the visualization cluster consumes some of the data (reads it from the <b>Lustre</b> filesystem). This process can be expanded by aggregating several collections of <b>Lustre</b> backend storage resources into one or more ''centralized'' <b>Lustre</b> filesystems, and then arranging to have several ''client'' clusters mount these centralized filesystems. The ''client clusters'' can be any combination of compute, visualization, archiving, or other types of cluster. This milestone demonstrates the operation and performance of a scaled-down version of such a large, centralized, shared <b>Lustre</b> filesystem concept...|$|R
40|$|<b>Lustre</b> is {{a product}} of skilled {{artisans}} and its production was spread to different regions by the migration of artisans. Each <b>lustre</b> production has its own peculiarities not only in the composition and microstructure of the <b>lustre</b> but also in the paste and glaze compositions, processing and firing conditions. Syrian <b>lustre</b> is particularly interesting as it shows technological innovations with respect to the earlier Abbasid and Fatimid <b>lustre</b> productions with the use of transparent tin-free glazes (often alkaline) and stonepastes. In particular, the use of alkaline glazes required the introduction of technological novelties in order to increase the chances to produce a metallic-like shining <b>lustre.</b> We present first studies made on the <b>lustre</b> layer of early Syrian productions, namely Tell Minis (first half 12 th century AD), Raqqa and related wares (second half of the 12 th century AD and first third of the 13 th century AD) and Damascus (second half of the 13 th century AD and 14 th century AD) ...|$|R
40|$|Cluster file {{systems and}} Storage Area Networks (SAN) {{make use of}} network IO to achieve higher IO bandwidth. Effective {{integration}} of networking mechanisms is important to their performance. In this paper, we perform an evaluation of a popular cluster file system, <b>Lustre,</b> over two of the leading high speed cluster interconnects: InfiniBand and Quadrics. Our evaluation is performed with both sequential IO and parallel IO benchmarks in order to explore the capacity of <b>Lustre</b> under different communication characteristics. Experimental results show that direct implementations of <b>Lustre</b> over both interconnects can improve its performance, compared to an IP emulation over InfiniBand (IPoIB). The performance of <b>Lustre</b> over Quadrics is {{comparable to that of}} <b>Lustre</b> over InfiniBand with the platforms we have. Latest InfiniBand products can embrace latest technologies, such as PCI-Express and DDR, and provide higher capacity. Our results show that over a <b>Lustre</b> file system with two Object Storage Servers (OSS), InfiniBand with PCI-Express technology can improve <b>Lustre</b> write performance by 24 %. Furthermore, our experimental results indicate that <b>Lustre</b> meta-data operations do not scale with an increasing number of OSS, in spite of using high performance interconnects...|$|R
