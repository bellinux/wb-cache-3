1883|10000|Public
50|$|Davidson {{correction}} is {{very popular}} due to its <b>low</b> <b>computational</b> <b>cost.</b> The correction improves contribution of electron correlation to the energy. The size-consistency and size-extensivity problems of truncated CI are alleviated but still exist. In small molecules, accuracy of the corrected energies can be similar to results from coupled cluster theory calculations.|$|E
50|$|A {{lightmap}} is a {{data structure}} used in lightmapping, {{a form of}} surface caching in which the brightness of surfaces in a virtual scene is pre-calculated and stored in texture maps for later use. Lightmaps are most commonly applied to static objects in realtime 3d graphics applications, such as video games, {{in order to provide}} lighting effects such as global illumination at a relatively <b>low</b> <b>computational</b> <b>cost.</b>|$|E
50|$|Although the {{classical}} vector slime algorithms {{are far from}} an attempt at correct physical modelling, the result can, under certain conditions, trick the viewer into believing {{that there is some}} sophisticated physical simulation involved. The effect has therefore grown quite popular in the demoscene to create impressive visual effects at relatively <b>low</b> <b>computational</b> <b>cost.</b> Interactive vector slime implementations can also eventually be found in computer games as a substitute for a more correct physical simulation algorithm.|$|E
3000|$|... [...]. This {{criterion}} {{has a high}} prediction {{accuracy and}} <b>low</b> <b>computational</b> <b>costs</b> compared to CV {{because it can be}} expressed by a simple form explicitly.|$|R
30|$|In {{order to}} {{optimise}} any implementation of FMM for high accuracy and <b>low</b> <b>computational</b> <b>costs,</b> {{a good understanding}} of and accurate estimates for the errors incurred by each individual FMM interaction are required. To this end, I now perform some numerical experiments.|$|R
40|$|Asymmetric {{cryptographic}} systems using multivariate polynomials over finite fields {{have been}} proposed several times since the 1980 s. Although {{some of them have}} been successfully broken, the area is still vital and promises interesting algorithms with <b>low</b> <b>computational</b> <b>costs,</b> short message, and signature sizes...|$|R
50|$|Screening is a {{particular}} instance of a sampling-based method. The objective here is rather to identify which input variables are contributing significantly to the output uncertainty in high-dimensionality models, rather than exactly quantifying sensitivity (i.e. in terms of variance). Screening tends to have a relatively <b>low</b> <b>computational</b> <b>cost</b> {{when compared to other}} approaches, and {{can be used in a}} preliminary analysis to weed out uninfluential variables before applying a more informative analysis to the remaining set. One of the most commonly used screening method is the elementary effect method.|$|E
50|$|Regression analysis, in {{the context}} of {{sensitivity}} analysis, involves fitting a linear regression to the model response and using standardized regression coefficients as direct measures of sensitivity. The regression is required to be linear with respect to the data (i.e. a hyperplane, hence with no quadratic terms, etc., as regressors) because otherwise it is difficult to interpret the standardised coefficients. This method is therefore most suitable when the model response is in fact linear; linearity can be confirmed, for instance, if the coefficient of determination is large. The advantages of regression analysis are that it is simple and has a <b>low</b> <b>computational</b> <b>cost.</b>|$|E
5000|$|The term [...] {{gives an}} {{indication}} of the magnitude of the error {{as a function of the}} mesh spacing. In this instance, the error is halfed if the grid spacing, _x is halved, and we say that this is a first order method. Most FDM used in practice are at least second order accurate except in very special circumstances. Finite Difference method is still the most popular numerical method for solution of PDEs because of their simplicity, efficiency and <b>low</b> <b>computational</b> <b>cost.</b> Their major drawback is in their geometric inflexibility which complicates their applications to general complex domains. These can be alleviated by the use of either mapping techniques and/or masking to fit the computational mesh to the computational domain.|$|E
40|$|In this Letter, {{we use the}} {{recently}} introduced perturbed matrix method (PMM) to study in detail the electronic properties of formaldehyde in water, as obtained by applying this method to Molecular Dynamics simulation data. Results show that PMM provides an accurate description at relatively <b>low</b> <b>computational</b> <b>costs.</b> (C) 2003 Elsevier B. V. All rights reserved...|$|R
40|$|Abstract. In {{this paper}} we propose a closed-form {{approximation}} for the price of basket options under a multivariate Black-Scholes model, based on Taylor expansions and the calculation of mixed exponential-power moments of a Gaussian distribution. Our numerical results show that a second order expansion provides accurate prices of spread options with <b>low</b> <b>computational</b> <b>costs,</b> even for out-of-the-money contracts. 1...|$|R
40|$|SUMMARY Grey world {{algorithm}} {{is a well-known}} color constancy algorithm. It {{is based on the}} Grey-World assumption i. e., the average reflectance of surfaces in the world is achromatic. This {{algorithm is}} simple and has <b>low</b> <b>computational</b> <b>costs.</b> However, for the images with several colors, the light source color could not be estimated correctly using the Grey World algorithm. In this paper, we propose a Multi-scale Adaptive Grey World algorithm (MAGW). First, multi-scale images are obtained based on wavelet transformation and the illumination color is estimated from different scales images. Then according to the estimated illumination color, the original image is mapped into the image under a canonical illumination with supervision of an adaptive reliability function, which is based on the image entropy. The experimental results show that our algorithm is effective and also has <b>low</b> <b>computational</b> <b>costs.</b> key words: color constancy, Grey World, Multi-scale 1...|$|R
50|$|A {{standard}} approximation {{strategy for}} polymer field theories is the mean field (MF) approximation, which consists in replacing the many-body interaction {{term in the}} action by a term where all bodies of the system interact with an average effective field. This approach reduces any multi-body problem into an effective one-body problem by assuming that the partition function integral of the model is dominated by a single field configuration. A major benefit of solving problems with the MF approximation, or its numerical implementation {{commonly referred to as}} the self-consistent field theory (SCFT), is that it often provides some useful insights into the properties and behavior of complex many-body systems at relatively <b>low</b> <b>computational</b> <b>cost.</b> Successful applications of this approximation strategy can be found for various systems of polymers and complex fluids, like e.g. strongly segregated block copolymers of high molecular weight, highly concentrated neutral polymer solutions or highly concentrated block polyelectrolyte (PE) solutions (Schmid 1998, Matsen 2002, Fredrickson 2002). There are, however, a multitude of cases for which SCFT provides inaccurate or even qualitatively incorrect results (Baeurle 2006a). These comprise neutral polymer or polyelectrolyte solutions in dilute and semidilute concentration regimes, block copolymers near their order-disorder transition, polymer blends near their phase transitions, etc. In such situations the partition function integral defining the field-theoretic model is not entirely dominated by a single MF configuration and field configurations far from it can make important contributions, which require the use of more sophisticated calculation techniques beyond the MF level of approximation.|$|E
3000|$|... (iv)The {{algorithm}} {{must be able}} {{to operate}} in real time with a <b>low</b> <b>computational</b> <b>cost</b> and a short delay.|$|E
3000|$|..., {{despite its}} simplicity, <b>low</b> <b>computational</b> <b>cost,</b> {{and the fact}} that it does not require any {{training}} or prior knowledge.|$|E
40|$|In {{this paper}} we propose a closed-form {{approximation}} for the price of basket options under a multivariate Black-Scholes model, based on Taylor expansions and the calculation of mixed exponential-power moments of a Gaussian distribution. Our numerical results show that a second order expansion provides accurate prices of spread options with <b>low</b> <b>computational</b> <b>costs,</b> even for out-of-the-money contracts. Comment: 13 pages, 2 figure...|$|R
40|$|The paper {{presents}} a taxonomic analysis of existing hybrid multi-objective evolutionary algorithms aimed at solving multi-objective simulation optimisation problems. For that, {{the properties of}} evolutionary algorithms and the requirements made to solving the problem considered are determined. Finally, {{a combination of the}} properties, which allows one to increase the approximation accuracy of the Pareto-optimal front at relatively <b>low</b> <b>computational</b> <b>costs,</b> is revealed...|$|R
40|$|In {{this paper}} we review {{some of the}} {{possible}} ways of making membrane-like structures in artificial chemistries. Such implementations range from simulations with accurate physics and high <b>computational</b> <b>costs</b> to abstract models with very <b>low</b> <b>computational</b> <b>costs.</b> We observe that various properties of natural membranes, such as self-assembly and self-repair, are lacking {{in some of the}} systems. Additionally, we present a novel implementation based on a two-dimensional lattice that has several desirable features and is computationally cheap...|$|R
40|$|Obstacle {{detection}} {{is one of}} {{the most}} important stages in the obstacle avoidance system. This work is focused to explain the operation of a designed and implemented for the overall detection of objects with <b>low</b> <b>computational</b> <b>cost</b> strategy. This strategy of <b>low</b> <b>computational</b> <b>cost</b> is based on performing a spatial segmentation of the information obtained by the SONAR and determine the minimum distance between the SONAR (AUV) and the obstacle. Peer Reviewe...|$|E
3000|$|... [*]Edge extraction: a Sobel {{operator}} {{was chosen}} because it has <b>low</b> <b>computational</b> <b>cost</b> and low noise sensitivity (very important {{for the type of}} images used).|$|E
30|$|From {{the above}} discussion, {{we can see}} that the EXFMG method not only {{achieves}} high-order accuracy but also keeps a <b>low</b> <b>computational</b> <b>cost.</b> Hence, it is a cost-effective numerical solver.|$|E
40|$|A fuzzy Bayesian {{algorithm}} is introduced, {{allowing for the}} incorporation of both uncertainty and fuzziness into data derived models. This is applied to predicting the sea-level near the Thames Estuary at Sheerness, from tidal gauge measurements down the east coast, astronomical tidal prediction, and meteorological data. We show that this approach can result in accurate, low-dimensional models with <b>low</b> <b>computational,</b> <b>costs</b> and relatively fast execution time...|$|R
40|$|The {{aim of this}} PhD {{project was}} to develop a fast and {{reliable}} method for the calculation of exchange coupling constants which are used in the description of the coupling of unpaired electrons in di-, tri- and oligonuclear transition metal complexes. In order to achieve both accurate results and <b>low</b> <b>computational</b> <b>costs,</b> a combination of quantum chemistry (QC) and molecular mechanics (MM) calculations has been employed...|$|R
40|$|Multi-objective {{simulation}} optimisation, evolutionary algorithms, hybrid algorithms Abstract – The paper {{presents a}} taxonomic analysis of existing hybrid multi-objective evolutionary algorithms aimed at solving multi-objective simulation optimisation problems. For that, {{the properties of}} evolutionary algorithms and the requirements made to solving the problem considered are determined. Finally, {{a combination of the}} properties, which allows one to increase the approximation accuracy of the Pareto-optimal front at relatively <b>low</b> <b>computational</b> <b>costs,</b> is revealed...|$|R
40|$|This paper {{deals with}} the problem of robot {{localization}} from noisy landmark bearings measured by the robot. We present a new localization method which is based on linear constraints, one due to each bearing measurement. This linear system can be solved at <b>low</b> <b>computational</b> <b>cost</b> but yields not very accurate results. Therefore, we transform the system to an equivalent linear system which yields virtually optimal results at {{a small fraction of the}} cost of a nonlinear optimization method, which usually achieves the optimal result. Experimental results showing the quality of the results and the <b>low</b> <b>computational</b> <b>cost</b> are presented...|$|E
3000|$|... {{takes the}} value of one and {{approaches}} zero for nonsinusoidal noisy signals. One of the attractive features of Hjorth descriptors is the feasibility of their calculation in time domain with <b>low</b> <b>computational</b> <b>cost.</b>|$|E
30|$|In the {{parameterization}} stage, {{the selected}} parameter values were not those that achieved the highest accuracy {{in the test}} set, but those that obtained a good trade-off between accuracy and <b>low</b> <b>computational</b> <b>cost.</b>|$|E
40|$|In this paper, {{we propose}} a constraint-handling {{approach}} for genetic algorithms {{which uses a}} dominance-based selection scheme. The proposed approach {{does not require the}} fine tuning of a penalty function and does not require extra mechanisms to maintain diversity in the population. The algorithm is validated using several test functions taken from the specialized literature on evolutionary optimization. The results obtained indicate that the approach can produce reasonable results at <b>low</b> <b>computational</b> <b>costs...</b>|$|R
40|$|The goal of {{this thesis}} was to develop and apply field-based {{multiscale}} modeling techniques, to better understand and improve the performance of polymer-based nanodevices, used in optoelectronic applications. For this purpose, we used the SCFT technique, to compute the polymeric morphologies, in combination with a suitable DMC algorithm, to simulate the photovoltaic processes. The application of this coupled multiscale approach enabled us to treat a large variety of polymer systems of large system size at <b>low</b> <b>computational</b> <b>costs...</b>|$|R
5000|$|Finally we {{note that}} a single Householder transform, unlike a solitary Givens transform, can act on all columns of a matrix, and as such {{exhibits}} the <b>lowest</b> <b>computational</b> <b>cost</b> for QR decomposition and tridiagonalization. The penalty for this [...] "computational optimality" [...] is, of course, that Householder operations cannot be as deeply or efficiently parallelized. As such Householder is preferred for dense matrices on sequential machines, whilst Givens is preferred on sparse matrices, and/or parallel machines.|$|R
30|$|The model {{discussed}} in Section 3 {{is well suited}} to a real-time implementation, given its <b>low</b> <b>computational</b> <b>cost.</b> The implementation has been performed on the Pure Data (PD) open-source software platform, a graphical programming language [54].|$|E
40|$|Abstract. Digital {{displacement}} {{fluid power}} pumps/motors offers improved efficiency and perfor-mance compared to traditional variable displacement pump/motors. These improvements are {{made possible by}} using efficient electronically controlled seat valves and careful design of the flow geome-try. To optimize the design and control of digital displacement machines, {{there is a need}} for simulation models, preferably models with <b>low</b> <b>computational</b> <b>cost.</b> Therefore, a <b>low</b> <b>computational</b> <b>cost</b> generic lumped parameter model of digital displacement machine is presented, including a method for deter-mining the needed model parameters based on steady CFD results, in order to take detailed geometry information into account. The response of the lumped parameter model is compared to a computa-tional expensive transient CFD model for an example geometry...|$|E
40|$|An {{efficient}} sparse modeling pipeline for {{the classification}} of human actions from video is here developed. Spatio-temporal features that characterize local changes in the image are first extracted. This {{is followed by the}} learning of a class-structured dictionary encoding the individual actions of interest. Classification is then based on reconstruction, where the label assigned to each video comes from the optimal sparse linear combination of the learned basis vectors (action primitives) representing the actions. A <b>low</b> <b>computational</b> <b>cost</b> deep-layer model learning the interclass correlations of the data is added for increasing discriminative power. In spite of its simplicity and <b>low</b> <b>computational</b> <b>cost,</b> the method outperforms previously reported results for virtually all standard datasets. ...|$|E
40|$|Abstract—The {{simulation}} of eddy currents in laminated iron cores by the {{finite element method}} (FEM) is of great interest {{in the design of}} electrical machines and transformers. The overall dimensions of an iron core and the thickness of the laminates are very different. A finite element model which considers each laminate requires many finite elements (FEs) leading to extremely large systems of equations and prohibitively high <b>computational</b> <b>costs.</b> Therefore, an improved multi-scale FEM to resolve this problem is studied. A numerical example demostrates the accuracy and the <b>low</b> <b>computational</b> <b>costs.</b> Index Terms—Eddy currents, multi-scale finite element meth-ods, laminates, numerical simulation. I...|$|R
40|$|We use Rayleigh-Bénard {{convection}} simulations {{to compare}} the properties of stationary states obtained via Principal Orthogonal Decomposition (POD) to those derived by embedding the global angular momentum of the system. We nd that the results obtained with POD and embedding techniques provide the same information. The <b>low</b> <b>computational</b> <b>costs</b> of embedding analysis suggests to use this procedure whenever a global observable reecting the symmetry of the system can be identied, while the POD should be preferred when such information is not available...|$|R
40|$|International audienceA {{new method}} for a {{complete}} theoretical description of Interatomic Coulombic decay (ICD) in large polyatomic rare-gas clusters will be presented. This original method combines the projection-operator formalism of resonant scattering theory, the diatomics-in-molecules technique and a surface hopping algorithm. Such combined approach has fairly <b>low</b> <b>computational</b> <b>costs</b> and constitutes an efficient tool for studying ICD in polyatomic clusters. Benchmark examples {{will be given to}} illustrate the method. Results on ICD in large rare-gas clusters will finally be reported...|$|R
