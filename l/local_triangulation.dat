12|49|Public
50|$|Hoodwink Island is {{an island}} lying 1 nmi km east of Arrowsmith Peninsula in Lallemand Fjord, Graham Land, Antarctica. It was mapped by the Falkland Islands Dependencies Survey (FIDS) from surveys and air photos, 1955-57, and was so named by the UK Antarctic Place-Names Committee because the island hoodwinked FIDS geologists and {{surveyors}} who misinterpreted the island's geological composition and incorrectly identified a nearby survey station during a <b>local</b> <b>triangulation.</b>|$|E
5000|$|A {{few years}} after {{retirement}} of his father, in 1894, Gustav moved to the University of Kharkov. There, in 1897 he became professor in astronomy and geodesy {{and director of the}} observatory. Prior to Struve, the Kharkov Observatory was not registered within the Russian leveling network and the altitude of Kharkov was based on rather inaccurate trigonometric leveling conducted by <b>local</b> <b>triangulation.</b> It took Struve five years of hard work to include the observatory to the Russian leveling network. In 1912, he was elected as Dean of the Physics and Mathematics Department of Kharkov University. In 1914, he founded a workshop of fine mechanics at the department and headed it for five years. Prior to that, such institutions had been nonnexistent in Russia, and foreign engineers were personally invited for precision mechanical work. Struve attempted to build a national school in this area, but with limited success. He himself was a skilled engineer and constructed an instrument for the measurement of [...] "an individual error using artificial star.” This device was invented by H. G. van de Sande Bakhuyzen at the Leiden Observatory and was actively used by Struve.|$|E
40|$|We {{present a}} {{framework}} and algorithms for multiresolution modeling of point-sampled geometry. Most of our techniques extend the key ingredients of recent multiresolution editing concepts, including geometry smoothing, simplification, and offset computation, to point clouds. Using these building blocks, we designed {{a powerful and}} versatile hierarchical representation for point-based modeling. An intuitive editing metaphor along with dynamic resampling techniques facilitates modeling operations at diftbrent scales and enables us to effectively handle very large model deformations. In our approach all processing is performed directly on point-sampled geometry {{without the need for}} intermediate <b>local</b> <b>triangulation</b> or meshing...|$|E
40|$|Free form {{surfaces}} are commonly represented by triangular or quadrilateral meshes. Often these meshes are obtained from unorganized point sets sampled from some object's surface. We show that local rather than global triangulations of point sets are equally {{well suited for}} object representations and that the <b>local</b> <b>triangulations</b> proposed in this paper may even lead to fast triangulation routines. 1...|$|R
40|$|Abstract Geometric {{spanners}} can be {{used for}} efcient routing in wireless ad hoc networks. Computation of existing spanners for ad hoc networks primarily focused on geometric properties without considering network requirements. In this paper we pro-pose a new spanner called Constrained Delaunay Triangulation (CDT) which considers both geometric properties and network requirements. The CDT is formed by introducing a small set of constraint edges into <b>local</b> Delaunay <b>triangulation</b> (LDel) {{to reduce the number of}} hops between nodes in the network graph. We have simulated the CDT in ns 2. 28 and compared with Gabriel graph (GG), relative neighborhood graph (RNG), <b>local</b> Delaunay <b>triangulation</b> (LDel), and planarized <b>local</b> Delaunay <b>triangulation</b> (PLDel). The simulation results show that the minimum number of hops from source to destination is less than other spanners. We also observed the decrease in delay, jitter, and improvement in throughput. I...|$|R
5000|$|Francisco Larrión, Víctor Neumann-Lara, Miguel A. Pizaña [...] "Whitney <b>triangulations,</b> <b>local</b> girth and {{iterated}} clique graphs" [...] Discrete Mathematics 258(1-3): 123-135 (2002) ...|$|R
40|$|We {{develop a}} method for {{incorporating}} relevant non-Euclidean geometric information into {{a broad range of}} classical filtering and statistical or machine learning algorithms. We apply these techniques to approximate the solution of the non-Euclidean filtering problem to arbitrary precision. We then extend the particle filtering algorithm to compute our asymptotic solution to arbitrary precision. Moreover, we find explicit error bounds measuring the discrepancy between our locally triangulated filter and the true theoretical non-Euclidean filter. Our methods are motivated by certain fundamental problems in mathematical finance. In particular we apply these filtering techniques to incorporate the non-Euclidean geometry present in stochastic volatility models and optimal Markowitz portfolios. We also extend Euclidean statistical or machine learning algorithms to non-Euclidean problems by using the <b>local</b> <b>triangulation</b> technique, which we show improves the accuracy of the original algorithm. We apply the <b>local</b> <b>triangulation</b> method to obtain improvements of the (sparse) principal component analysis and the principal geodesic analysis algorithms and show how these improved algorithms can be used to parsimoniously estimate the evolution of the shape of forward-rate curves. While focused on financial applications, the non-Euclidean geometric techniques presented in this paper can be employed to provide improvements to a range of other statistical or machine learning algorithms and may be useful in other areas of application. Comment: 40 page...|$|E
40|$|We {{present a}} {{high-order}} accurate space-time discontinuous Galerkin method for solving two-dimensional compressible flow with fully unstructured space-time meshes. The discretization {{is based on}} a nodal for-mulation, with appropriate numerical fluxes for the first and the second-order terms, respectively. The scheme is implicit, and we solve the resulting non-linear systems using a parallel Newton-Krylov solver. The meshes are produced by a mesh moving technique with element connectivity updates, and the corresponding space-time elements are produced directly based on these local operations. To obtain globally conforming tetrahedral meshes, we first derive the required conditions on a prism boundary mesh to allow for a valid <b>local</b> <b>triangulation.</b> Next, we present an efficient algorithm for finding a global mesh that satisfies these conditions. We also show how to add and remove mesh nodes, again using local constructs for the space-time mesh. Our method is demonstrated on a number of test problems, showing the high-order accuracy for model problems, and the ability to solve flow problems on domains with complex large deformations...|$|E
40|$|AbstractIn Marinosson (2002) [10], {{a method}} to compute Lyapunov {{functions}} for systems with asymptotically stable equilibria was presented. The method uses finite differences on finite elements to generate a linear programming problem for the system in question, of which every feasible solution parameterises a piecewise affine Lyapunov function. In Hafstein (2004) [2] it was proved that the method always succeeds in generating a Lyapunov function for systems with an exponentially stable equilibrium. However, the proof could not guarantee that the generated function has negative orbital derivative locally in a small neighbourhood of the equilibrium. In this article we give {{an example of a}} system, where no piecewise affine Lyapunov function with the proposed triangulation scheme exists. This failure is due to the triangulation of the method being too coarse at the equilibrium, and we suggest a fan-like triangulation around the equilibrium. We show that for any two-dimensional system with an exponentially stable equilibrium there is a <b>local</b> <b>triangulation</b> scheme such that the system possesses a piecewise affine Lyapunov function. Hence, the method might eventually be equipped with an improved triangulation scheme that does not have deficits locally at the equilibrium...|$|E
40|$|We {{describe}} a novel Geometric Localized Routing (GLR) protocol in Disruption (Delay) Tolerant Network (DTN). Although DTNs do not guarantee the connectivity {{of the network}} all the time, geometric location information still {{could be used to}} make routing decisions in a store and forward way. Geometric planar spanners, especially <b>local</b> Delaunay <b>triangulation</b> can also be used in DTN to provide a good routing graph with constant stretch factor and shorter paths during communication. In this work, we design local distributed solutions to extract spanning trees from <b>Local</b> Delaunay <b>Triangulation</b> Graphs in the direction from source to destination. Our protocol resorts to flooding packets along the trees and with high probability packets are delivered with low delay. Through experimentation, we have shown that the proposed routing protocol achieves higher delivery ratio with lower delay and limited storage requirement than the benchmark epidemic routing protocol...|$|R
40|$|For a set S of {{distinct}} {{points in the}} x-y plane and a 3 -dimensional surface with the general quadratic equation f ey dx cxy by ax z + + + + + = 2 2 we study the problem of finding a locally optimal triangulation of S for the linear approximation of the surface under the L p norm. We show that the L p norm error is independent of the translation and rotation by 180 degrees of a single triangle in the x-y plane. This observation allows us to substantially simply the local optimization procedure, and in particular allows us to explicitly compute the separation curves. Locally optimal triangulations are important in the data dependent approximation methods. The paper generalizes earlier results by P. Desnogues and O. Devillers (presented at the Canadian Conference on Computational Geometry CCCG ` 95), who studied optimal <b>local</b> <b>triangulations</b> for the unit hyperbolic paraboloid in the L 2 norm error...|$|R
40|$|The {{paper will}} present a method to {{generate}} triangular and quadrilateral meshes on curved surfaces, using message passing between geometric modeller and a generator for meshes in the parameter plane of surface patches. The coupling is based on standard interprocess communication with a minimal set of message types. Initial mesh distortion on the surface is removed using a <b>local</b> Delaunay <b>triangulation</b> in the tangential plane. Complex examples show {{the efficiency of the}} method...|$|R
40|$|Nowadays, digital {{representations}} of real objects are becoming bigger as scanning processes are more accurate, so {{the time required}} for {{the reconstruction of the}} scanned models is also increasing. This thesis studies the application of parallel techniques in the surface reconstruction problem, in order to improve the processing time required to obtain the final mesh. It is shown how local interpolating triangulations are suitable for global reconstruction, at the time {{that it is possible to}} take advantage of the independent nature of these triangulations to design highly effcient parallel methods. A parallel surface reconstruction method is presented, based on local Delaunay triangulations. The input points do not present any additional information, such as normals, nor any known structure. This method has been designed to be GPU friendly, and two implementations are presented. To deal the inherent problems related to interpolating techniques (such as noise, outliers and non-uniform distribution of points), a consolidation process is studied and a parallel points projection operator is presented, as well as its implementation in the GPU. This operator is combined with the <b>local</b> <b>triangulation</b> method to obtain a better reconstruction. This work also studies the possibility of using dynamic reconstruction techniques in a parallel fashion. The method proposed looks for a better interpretation and recovery of the shape and topology of the target model...|$|E
40|$|Point-Based Surfaces can be {{directly}} generated by 3 D scanners and avoid the generation and storage of an explicit topology for a sampled geometry, which saves time and storage space for very dense and large objects, such as scanned statues and other archaeological artefacts [DDGM ∗]. We propose a fast processing pipeline of large point-based surfaces for real-time, appearance preserving, polygonal rendering. Our goal {{is to reduce the}} time needed between a point set made of hundred of millions samples and a high resolution visualization taking benefit of modern graphics hardware, tuned for normal mapping of polygons. Our approach starts by an out-of-core generation of a coarse <b>local</b> <b>triangulation</b> of the original model. The resulting coarse mesh is enriched by applying a set of maps which capture the high frequency features of the original data set. We choose as an example the normal component of samples for these maps, since normal maps provide efficiently an accurate local illumination. But our approach is also suitable for other point attributes such as color or position (displacement map). These maps come also from an out-of-core process, using the complete input data in a streaming process. Sampling issues of the maps are addressed using an efficient diffusion algorithm in 2 D. Our main contribution is to directly handle such large unorganized point clouds through this two pass algorithm, without the time-consuming meshing or parameterization step, required by current state-of-the-art high resolution visualization methods. One of th...|$|E
40|$|The {{application}} of extended digital surface models often reveals, that despite an acceptable global accuracy {{for a given}} dataset, the local accuracy of the model can vary in a wide range. For high resolution applications which cover the spatial extent of a whole country, {{this can be a}} major drawback. Within the Swiss National Forest Inventory (NFI), two digital surface models are available, one derived from LiDAR point data and the other from aerial images. Automatic photogrammetric image matching with ADS 80 aerial infrared images with 25 cm and 50 cm resolution is used to generate a surface model (ADS-DSM) with 1 m resolution covering whole switzerland (approx. 41000 km 2). The spatially corresponding LiDAR dataset has a global point density of 0. 5 points per m 2 and is mainly used in applications as interpolated grid with 2 m resolution (LiDAR-DSM). Although both surface models seem to offer a comparable accuracy from a global view, local analysis shows significant differences. Both datasets have been acquired over several years. Concerning LiDAR-DSM, different flight patterns and inconsistent quality control result in a significantly varying point density. The image acquisition of the ADS-DSM is also stretched over several years and the model generation is hampered by clouds, varying illumination and shadow effects. Nevertheless many classification and feature extraction applications requiring high resolution data depend on the local accuracy of the used surface model, therefore precise knowledge of the local data quality is essential. The commercial photogrammetric software NGATE (part of SOCET SET) generates the image based surface model (ADS-DSM) and delivers also a map with figures of merit (FOM) of the matching process for each calculated height pixel. The FOM-map contains matching codes like high slope, excessive shift or low correlation. For the generation of the LiDAR-DSM only first- and last-pulse data was available. Therefore only the point distribution can be used to derive a local accuracy measure. For the calculation of a robust point distribution measure, a constrained triangulation of local points (within an area of 100 m 2) has been implemented using the Open Source project CGAL. The area of each triangle is a measure for the spatial distribution of raw points in this local area. Combining the FOM-map with the local evaluation of LiDAR points allows an appropriate local accuracy evaluation of both surface models. The currently implemented strategy ("partial replacement") uses the hypothesis, that the ADS-DSM is superior due to its better global accuracy of 1 m. If the local analysis of the FOM-map within the 100 m 2 area shows significant matching errors, the corresponding area of the triangulated LiDAR points is analyzed. If the point density and distribution is sufficient, the LiDAR-DSM will be used in favor of the ADS-DSM at this location. If the <b>local</b> <b>triangulation</b> reflects low point density or the variance of triangle areas exceeds a threshold, the investigated location will be marked as NODATA area. In a future implementation ("anisotropic fusion") an anisotropic inverse distance weighting (IDW) will be used, which merges both surface models in the point data space by using FOM-map and <b>local</b> <b>triangulation</b> to derive a quality weight for each of the interpolation points. The "partial replacement" implementation and the "fusion" prototype for the anisotropic IDW make use of the Open Source projects CGAL (Computational Geometry Algorithms Library), GDAL (Geospatial Data Abstraction Library) and OpenCV (Open Source Computer Vision) ...|$|E
30|$|Zhang et al. [26] {{developed}} an algorithm by constructing a <b>local</b> Delaunay <b>triangulation</b> network. In this algorithm, the temporarily coherent point (TCP) which remains reflectivity only {{for some time}} spans is used, and the TCPs in a regular patch are connected and some regular patches are placed over the image. This algorithm can ensure the extensive TCP connections without the phase ambiguity. In addition, the constructed network by this algorithm has the connections with small lengths so as to reduce the atmospheric artifact.|$|R
50|$|Angular Recording Corporation was an {{independent}} record label founded in New Cross, South East London. It {{was established in}} June 2003 by two ex-Goldsmiths College students, Joe Daniel and Joe Margetts, who reclaimed a <b>local</b> Ordnance Survey <b>Triangulation</b> Station and made it their first artefact: ARC 001.|$|R
40|$|Abstract. This paper {{describes}} {{a novel approach}} based on fuzzy logic technique for smoothing process on the unstructured meshes in 2 D. The proposed method works <b>local</b> on <b>triangulation.</b> Therefore, remeshing does not require. It is known that other smoothing operations such as laplacian smoothing, optimization based smoothing, angle based smoothing, and hybrid smoothing operations work global. That is, in these methods, meshing is achieved, afterwards remeshing is done for smoothing. Whereas, the goal of presented method {{is to find the}} best fit location of node while meshing is done. However, remeshing process is avoided. ...|$|R
40|$|International audiencePoint-Based Surfaces can be {{directly}} generated by 3 D scanners and avoid the generation and storage of an explicit topology for a sampled geometry, which saves time and storage space for very dense and large objects, such as scanned statues and other archaeological artefacts [Duguet 2004]. We propose a fast processing pipeline of large point-based surfaces for real-time, appearance preserving, polygonal rendering. Our goal {{is to reduce the}} time needed between a point set made of hundred of millions samples and a high resolution visualization taking benefit of modern graphics hardware, tuned for normal mapping of polygons. Our approach starts by an out-of-core generation of a coarse <b>local</b> <b>triangulation</b> of the original model. The resulting coarse mesh is enriched by applying a set of maps which capture the high frequency features of the original data set. We choose as an example the normal component of samples for these maps, since normal maps provide efficiently an accurate local illumination. But our approach is also suitable for other point attributes such as color or position (displacement map). These maps come also from an out-of-core process, using the complete input data in a streaming process. Sampling issues of the maps are addressed using an efficient diffusion algorithm in 2 D. Our main contribution is to directly handle such large unorganized point clouds through this two pass algorithm, without the time-consuming meshing or parameterization step, required by current state-of-the-art high resolution visualization methods. One of the main advantages is to express most of the fine features present in the original large point clouds as textures in the huge texture memory usually provided by graphics devices, using only a lazy local parameterization. Our technique comes as a complementary tool to high-quality, but costly, out-of-core visualization systems. Direct applications are: interactive preview at high screen resolution of very detailed scanned objects such as scanned statues, inclusion of large point clouds in usual polygonal 3 D engines and 3 D databases browsing...|$|E
40|$|Sheung, Hoi. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2009. Includes bibliographical {{references}} (p. 65 - 70). Abstract also in Chinese. Abstract [...] - p. vAcknowledgements [...] - p. ixList of Figures [...] - p. xiiiList of Tables [...] - p. xvChapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Main Contributions [...] - p. 3 Chapter 1. 2 [...] - Outline [...] - p. 3 Chapter 2 [...] - Related Work [...] - p. 5 Chapter 2. 1 [...] - Volumetric reconstruction [...] - p. 5 Chapter 2. 2 [...] - Combinatorial approaches [...] - p. 6 Chapter 2. 3 [...] - Robust {{statistics in}} surface reconstruction [...] - p. 6 Chapter 2. 4 [...] - Down-sampling of massive points [...] - p. 7 Chapter 2. 5 [...] - Streaming and parallel computing [...] - p. 7 Chapter 3 [...] - Robust Normal Estimation and Point Projection [...] - p. 9 Chapter 3. 1 [...] - Robust Estimator [...] - p. 9 Chapter 3. 2 [...] - Mean Shift Method [...] - p. 11 Chapter 3. 3 [...] - Normal Estimation and Projection [...] - p. 11 Chapter 3. 4 [...] - Moving Least Squares Surfaces [...] - p. 14 Chapter 3. 4. 1 [...] - Step 1 : local reference domain [...] - p. 14 Chapter 3. 4. 2 [...] - Step 2 : local bivariate polynomial [...] - p. 14 Chapter 3. 4. 3 [...] - Simpler Implementation [...] - p. 15 Chapter 3. 5 [...] - Robust Moving Least Squares by Forward Search [...] - p. 16 Chapter 3. 6 [...] - Comparison with RMLS [...] - p. 17 Chapter 3. 7 [...] - K-Nearest Neighborhoods [...] - p. 18 Chapter 3. 7. 1 [...] - Octree [...] - p. 18 Chapter 3. 7. 2 [...] - Kd-Tree [...] - p. 19 Chapter 3. 7. 3 [...] - Other Techniques [...] - p. 19 Chapter 3. 8 [...] - Principal Component Analysis [...] - p. 19 Chapter 3. 9 [...] - Polynomial Fitting [...] - p. 21 Chapter 3. 10 [...] - Highly Parallel Implementation [...] - p. 22 Chapter 4 [...] - Error Controlled Subsampling [...] - p. 23 Chapter 4. 1 [...] - Centroidal Voronoi Diagram [...] - p. 23 Chapter 4. 2 [...] - Energy Function [...] - p. 24 Chapter 4. 2. 1 [...] - Distance Energy [...] - p. 24 Chapter 4. 2. 2 [...] - Shape Prior Energy [...] - p. 24 Chapter 4. 2. 3 [...] - Global Energy [...] - p. 25 Chapter 4. 3 [...] - Lloyd´ةs Algorithm [...] - p. 26 Chapter 4. 4 [...] - Clustering Optimization and Subsampling [...] - p. 27 Chapter 5 [...] - Mesh Generation [...] - p. 29 Chapter 5. 1 [...] - Tight Cocone Triangulation [...] - p. 29 Chapter 5. 2 [...] - Clustering Based <b>Local</b> <b>Triangulation</b> [...] - p. 30 Chapter 5. 2. 1 [...] - Initial Surface Reconstruction [...] - p. 30 Chapter 5. 2. 2 [...] - Cleaning Process [...] - p. 32 Chapter 5. 2. 3 [...] - Comparisons [...] - p. 33 Chapter 5. 3 [...] - Computing Dual Graph [...] - p. 34 Chapter 6 [...] - Results and Discussion [...] - p. 37 Chapter 6. 1 [...] - Results of Mesh Reconstruction form Noisy Point Cloud [...] - p. 37 Chapter 6. 2 [...] - Results of Clustering Based <b>Local</b> <b>Triangulation</b> [...] - p. 47 Chapter 7 [...] - Conclusions [...] - p. 55 Chapter 7. 1 [...] - Key Contributions [...] - p. 55 Chapter 7. 2 [...] - Factors Affecting Our Algorithm [...] - p. 55 Chapter 7. 3 [...] - Future Work [...] - p. 56 Chapter A [...] - Building Neighborhood Table [...] - p. 59 Chapter A. l [...] - Building Neighborhood Table in Streaming [...] - p. 59 Chapter B [...] - Publications [...] - p. 63 Bibliography [...] - p. 6...|$|E
40|$|Abstract. A simple {{criterion}} {{that allows}} the efficient <b>local</b> coarsening of <b>triangulations</b> created by bisections is devised and analyzed. Under a mild condition on the initial triangulation the proposed criterion allows to gradually reverse the entire refinement without employing its history explicitly. Numerical experiments underline {{the efficiency of the}} resulting algorithm. 1...|$|R
50|$|Incompressible 2D flows {{were first}} studied in (Oñate, Sacco & Idelsohn, 2000) using a {{projection}} method stabilized through the FIC technique. A {{detailed analysis of}} this approach {{was carried out in}} (Sacco, 2002). Outstanding achievements from that work have given the FPM a more solid base; among them, the definition of local and normalized approximation bases, a procedure for constructing local clouds of points based on <b>local</b> Delaunay <b>triangulation,</b> and a criterion for evaluating the quality of the resultant approximation. The numerical applications presented focused mainly on two-dimensional (viscous and inviscid) incompressible flows, but a three-dimensional application example was also provided.|$|R
40|$|This Note {{provides}} {{an account of}} the problems underlying local service delivery (LSD) in the Philippines and presents some prospective solutions. The problems are examined {{on the basis of the}} three components of the Triangulation Framework for LSD developed in a recently completed PIDS-UNICEF study. millennium development goal (MDG), <b>local</b> service delivery, <b>triangulation,</b> public goods and services...|$|R
40|$|We {{introduce}} a modified Regge calculus for general relativity on a triangulated four dimensional Riemannian manifold where the fundamental variables are areas {{and a certain}} class of angles. These variables satisfy constraints which are <b>local</b> in the <b>triangulation.</b> We expect the formulation to have applications to classical discrete gravity and non-perturbative approaches to quantum gravity. Comment: 7 pages, 1 figure. v 2 small changes to match published versio...|$|R
40|$|An InSAR {{analysis}} {{approach for}} identifying and extracting the temporarily coherent points (TCP) that exist between two SAR acquisitions and for determining {{motions of the}} TCP is presented for applications such as ground settlement monitoring. TCP are identified based on the spatial characteristics of the range and azimuth offsets of coherent radar scatterers. A method for coregistering TCP based on the offsets of TCP is given to reduce the coregistration errors at TCP. An improved phase unwrapping method based on the minimum cost flow (MCF) algorithm and <b>local</b> Delaunay <b>triangulation</b> is also proposed for sparse TCP data. The proposed algorithms are validated using a test site in Hong Kong. The test {{results show that the}} algorithms work satisfactorily for various ground features. Department of Land Surveying and Geo-Informatic...|$|R
40|$|This paper {{addresses}} {{the problem of}} efficiently computing optimal paths of arbitrary clearance from a polygonal representation of a given virtual environment. Key to the proposed method is {{a new type of}} triangulated navigation mesh, called a <b>Local</b> Clearance <b>Triangulation,</b> which enables the efficient and correct determination if a disc of arbitrary size can pass through any narrow passages of the mesh. The proposed approach uniquely balances speed of computation and optimality of paths by first computing high-quality locally shortest paths efficiently in optimal time. Only in case global optimality is needed, an extended search will gradually improve the current path (if not already the global optimal) until the globally shortest one is determined. The presented method represents the first solution correctly extracting shortest paths of arbitrary clearance directly from a triangulated environment...|$|R
40|$|We {{present an}} {{algorithm}} for curve skeleton extraction via Laplacian-based contraction. Our algorithm {{can be applied}} to surfaces with boundaries, polygon soups, and point clouds. We develop a contraction operation that is designed to work on generalized discrete geometry data, particularly point clouds, via <b>local</b> Delaunay <b>triangulation</b> and topological thinning. Our approach is robust to noise and can handle moderate amounts of missing data, allowing skeleton-based manipulation of point clouds without explicit surface reconstruction. By avoiding explicit reconstruction, we are able to perform skeleton-driven topology repair of acquired point clouds in the presence of large amounts of missing data. In such cases, automatic surface reconstruction schemes tend to produce incorrect surface topology. We show that the curve skeletons we extract provide an intuitive and easy-to-manipulate structure for effective topology modification, leading to more faithful surface reconstruction...|$|R
40|$|We {{present a}} {{framework}} for processing point-based surfaces via partialdifferentialequations (PDEs). Our framework efficiently and effectively brings well-known PDE-based processing techniques {{to the field of}} point-based surfaces. At the core of our method is a finite element discretization of PDEs on point surfaces. This discretization is based on the local assembly of PDE-specific mass and stiffness matrices, using a local point coupling computation. Point couplings are computed using a local tangent plane construction and a <b>local</b> Delaunay <b>triangulation</b> of point neighborhoods. The definition of tangent planes relies on moment-based computation with proven scaling and stability properties. Once local stiffness matrices are obtained, we are able to easily assemble global matrices and efficiently solve the corresponding linear systems by standard iterative solvers. We demonstrate our framework by several types of PDE-based surface processing applications, such as segmentation, texture synthesis, bump mapping, and geometric fairing...|$|R
40|$|Generalization may {{be defined}} as a {{controlled}} reduction and simplifica tion of geographical data. Despite the knowledge of basic generalization operators, the automation of generalization remains a complex issue. Ac tually the change in resolution induces numerous spatial conflicts and the use of generalization operators such as smoothing or aggregation may ac cidentally degrade geographical data. The aim {{of this paper is to}} propose a set of paradigms necessary to automate generalization. Firstly a space partitioning computed from structuring objects allows to define some working areas and to give a pre-sequence of transformations in the proc ess of generalization. Then some <b>local</b> Delaunay <b>triangulations</b> allow to compute proximity relations between non-connected objects and to propa gate displacements due to some geometric transformations. To ensure consistency during different geometric changes, both paradigms are inte grated with a classical topo-metric structure...|$|R
40|$|We {{present a}} {{framework}} for processing point-based surfaces via partial differential equations (PDEs). Our framework efficiently and effectively brings well-known PDE-based processing techniques {{to the field of}} point-based surfaces. At the core of our method is a finite element discretization of PDEs on point surfaces. This discretization is based on the local assembly of PDE-specific mass and stiffness matrices, using a local point coupling computation. Point couplings are computed using a local tangent plane construction and a <b>local</b> Delaunay <b>triangulation</b> of point neighborhoods. The definition of tangent planes relies on moment-based computation with proven scaling and stability properties. Once local stiffness matrices are obtained, we are able to easily assemble global matrices and efficiently solve the corresponding linear systems by standard iterative solvers. We demonstrate our framework by several types of PDE-based surface processing applications, such as segmentation, texture synthesis, bump mapping, and geometric fairing. ...|$|R
40|$|Abstract. Navigation meshes are {{commonly}} {{employed as a}} practical represen-tation for path planning and other navigation queries in animated virtual envi-ronments and computer games. This paper explores the use of triangulations as a navigation mesh, and discusses several useful triangulation–based algorithms and operations: environment modeling and validity, automatic agent placement, track-ing moving obstacles, ray–obstacle intersection queries, path planning with arbi-trary clearance, determination of corridors, etc. While several of the addressed queries and operations {{can be applied to}} generic triangular meshes, the efficient computation of paths with arbitrary clearance requires a new type of triangular mesh, called a <b>Local</b> Clearance <b>Triangulation,</b> which enables the efficient and correct determination if a disc of arbitrary size can pass through any narrow pas-sages of the mesh. This paper shows that triangular meshes can support the effi-cient computation of several navigation procedures and an implementation of the presented methods is available...|$|R
40|$|Abstract—We {{present an}} {{algorithm}} for curve skeleton extraction via Laplacian-based contraction. Our algorithm {{can be applied}} to surfaces with boundaries, polygon soups, and point clouds. We develop a contraction operation that is designed to work on generalized discrete geometry data, particularly point clouds, via <b>local</b> Delaunay <b>triangulation</b> and topological thinning. Our approach is robust to noise and can handle moderate amounts of missing data, allowing skeleton-based manipulation of point clouds without explicit surface reconstruction. By avoiding explicit reconstruction, we are able to perform skeleton-driven topology repair of acquired point clouds in the presence of large amounts of missing data. In such cases, automatic surface reconstruction schemes tend to produce incorrect surface topology. We show that the curve skeletons we extract provide an intuitive and easy-to-manipulate structure for effective topology modifica-tion, leading to more faithful surface reconstruction. Keywords-curve skeleton; point cloud; Laplacian; contrac-tion; topology repair; surface reconstructio...|$|R
40|$|The {{triangle}} longest-edge bisection {{constitutes an}} efficient scheme for refining a mesh {{by reducing the}} obtuse triangles, since the largest interior angles are subdivided. In this paper we specifically introduce a new <b>local</b> refinement for <b>triangulations</b> based on the longest-edge trisection, the 7 -triangle longest-edge (7 T-LE) local refinement algorithm. Each triangle to be refined is subdivided in seven sub-triangles by determining its longest edge. The conformity of the new mesh is assured by an automatic point insertion criterion using the oriented 1 -skeleton graph of the triangulation and three partial division patterns...|$|R
40|$|This paper {{concerns}} {{compression of}} digital images consisting of smoothly colored regions separated {{from each other}} by smooth contours. Wavelet transforms combined with a nonlinear thresholding step are optimal when catching point singularities, however, when trying to capture smooth line singularities in a parsimonious manner they perform suboptimal. We propose a geometrical method {{to deal with these}} inherent deficiencies of wavelets. The proposed method using normal offsets transforms the input image nonlinearly using only a series of scalar values. Special attention is given on how to adapt the concept of normal offsets towards the digital setting. Moreover, a <b>local</b> adaptive <b>triangulation</b> is issued such that the method is able to sparsely represent higher dimensional singularities, i. e., line singularities. The adaptive triangulation encourages the triangle edges to settle themselves parallel with respect to the contour. At the same time the locality of the triangulation method induces hierarchical mesh structures which are well suited for compression methods. nrpages: 29 status: publishe...|$|R
40|$|This is {{the third}} paper {{on the study of}} {{gradient}} recovery for elliptic interface problem. In our previous works [H. Guo and X. Yang, 2016, arXiv: 1607. 05898 and J. Comput. Phys., 338 (2017), 606 [...] 619], we developed gradient recovery methods for elliptic interface problem based on body-fitted meshes and immersed finite element methods. Despite the efficiency and accuracy that these methods bring to recover the gradient, there are still some cases in unfitted meshes where skinny triangles appear in the generated <b>local</b> body-fitted <b>triangulation</b> that destroy the accuracy of recovered gradient near the interface. In this paper, we propose a gradient recovery technique based on Nitsche's method for elliptic interface problem, which avoids the loss of accuracy of gradient near the interface caused by skinny triangles. We analyze the supercloseness between the gradient of the numerical solution by the Nitsche's method and the gradient of interpolation of the exact solution, which leads to the superconvergence of the proposed gradient recovery method. We also present several numerical examples to validate the theoretical results...|$|R
40|$|The {{state and}} <b>local</b> {{networks}} of <b>triangulation</b> were investigated {{with the aim}} of analysis of the correlation dependencies in triangulation, the possibility of its registration in design, assessment of accuracy, adjustment of triangulation. This paper is the first experience of systematizing and generalizing of data on correlation dependencies in triangulation. The results obtained make it possible to analyze and register the correlation dependencies and to perform the mathematical processing more accurately. New formulae of assessment of accuracy of the geodetic measurings in triangulation were developed as well as some new standard tolerancesAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
