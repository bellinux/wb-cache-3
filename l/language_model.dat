3881|10000|Public
25|$|Méndez , Olivia V. 2000. Code-Switching and the Matrix <b>Language</b> <b>Model</b> in San Lucas Quiaviní Zapotec. M.A. thesis, UCLA.|$|E
25|$|The cache <b>language</b> <b>model</b> {{and other}} {{statistical}} language models {{that are used}} in natural language processing are also examples of applications of probability theory.|$|E
25|$|In {{areas of}} {{language}} modeling, there are {{limitations on the}} applicability of any <b>language</b> <b>model</b> as the statistics for different types of text will be different. When a language technology application is put into use (applied to a new text type), it is not certain that the <b>language</b> <b>model</b> will fare {{in the same way}} as how it would when applied to the training corpus. It is found that there are substantial variations in model performance when the training corpus changes. This lack of theory types limits the assessment of the usefulness of language-modeling work.|$|E
40|$|This paper {{presents}} sub-word based <b>language</b> <b>models</b> for Amharic, a morphologically {{rich and}} under-resourced <b>language.</b> The <b>language</b> <b>models</b> {{have been developed}} (using an open source <b>language</b> <b>modeling</b> toolkit- SRILM) with different n-gram order (2 to 5) and smoothing techniques. Among the developed models, the best performing one is a 5 gram model with modified Kneser-Ney smoothing and with interpolation of n-gram probability estimates. Keywords <b>Language</b> <b>modeling,</b> sub-word based <b>language</b> <b>modeling,</b> morph-based <b>language</b> <b>modeling,</b> Amharic...|$|R
40|$|Statistical <b>language</b> <b>modelling</b> {{estimates}} the regularities in natural <b>languages.</b> <b>Language</b> <b>models</b> {{are used in}} speech recognition, machine translation and other applications for speech and language technologies. In this paper we will present a procedure for <b>language</b> <b>models</b> building for the Croatian weather domain corpus. Different types of n-gram statistic <b>language</b> <b>models</b> and smoothing methods for <b>language</b> <b>modelling</b> are presented. Those models are compared {{in terms of their}} estimated perplexity...|$|R
40|$|In this paper, {{we explore}} {{the use of}} Random Forests (RFs) (Amit and Geman, 1997; Breiman, 2001) in <b>language</b> <b>modeling,</b> the problem of {{predicting}} the next word based on words already seen before. The goal in this work {{is to develop a}} new <b>language</b> <b>modeling</b> approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition. We study our RF approach in the context of ¢-gram type <b>language</b> <b>modeling.</b> Unlike regular ¢-gram <b>language</b> <b>models,</b> RF <b>language</b> <b>models</b> have the potential to generalize well to unseen data, even when a complicated history is used. We show that our RF <b>language</b> <b>models</b> are superior to regular ¢-gram <b>language</b> <b>models</b> in reducing both the perplexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system...|$|R
25|$|An {{issue when}} using n-gram {{language}} models are out-of-vocabulary (OOV) words. They are encountered {{in computational linguistics}} and natural language processing when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a <b>language</b> <b>model</b> is estimated, the entire observed vocabulary is used. In some cases, {{it may be necessary}} to estimate the <b>language</b> <b>model</b> with a specific fixed vocabulary. In such a scenario, the n-grams in the corpus that contain an out-of-vocabulary word are ignored. The n-gram probabilities are smoothed over all the words in the vocabulary even if they were not observed.|$|E
25|$|Encoder–decoder {{frameworks}} {{are based}} on neural networks that map highly structured input to highly structured output. The approach arose {{in the context of}} machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN <b>language</b> <b>model</b> to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.|$|E
25|$|An n-gram {{model is}} a type of {{probabilistic}} <b>language</b> <b>model</b> for predicting the next item in such a sequence {{in the form of a}} (n−1)–order Markov model. n-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. Two benefits of n-gram models (and algorithms that use them) are simplicity and scalability – with larger n, a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently.|$|E
40|$|<b>Language</b> <b>modeling</b> for large-vocabulary {{conversational}} Arabic {{speech recognition}} {{is faced with}} the problem of the complex morphology of Arabic, which increases the perplexity and out-of-vocabulary rate. This problem is compounded by the enormous dialectal variability and differences between spoken and written language. In this paper we investigate improvements in Arabic <b>language</b> <b>modeling</b> by developing various morphology-based <b>language</b> <b>models.</b> We present four different approaches to morphology-based <b>language</b> <b>modeling,</b> including a novel technique called factored <b>language</b> <b>models.</b> Experimental results are presented for both rescoring and first-pass recognition experiments...|$|R
40|$|<b>Language</b> <b>modeling</b> {{is one of}} {{the most}} {{powerful}} methods in information retrieval. Many <b>language</b> <b>modeling</b> based retrieval systems have been developed and tested on English collections. Hence, the evaluation of <b>language</b> <b>modeling</b> on collections of other languages is an interesting research issue. In this study, four different <b>language</b> <b>modeling</b> methods proposed by Hiemstra [1] have been evaluated on a large Persian collection of a news archive. Furthermore, we study two different approaches that are proposed for tuning the Lambda parameter in the method. Experimental results show that the performance of <b>language</b> <b>models</b> on Persian tex...|$|R
40|$|This thesis {{presents}} {{an approach to}} create topic dependent <b>language</b> <b>models.</b> It is shown that a gain of 5 % was reached using speech recognition for news broadcast. First, this document presents the theory, on which <b>language</b> <b>models</b> are based {{and the problem of}} sparse data {{which is one of the}} biggest problems associated with the creation of adequate <b>language</b> <b>models.</b> Next, general guidelines are presented in regard of the choices and techniques implied in the creation and the adaptation of <b>language</b> <b>models.</b> Finally, experimental results are presented and commented on to sustain the concept of topic dependent <b>language</b> <b>models...</b>|$|R
2500|$|... a {{statistical}} <b>language</b> <b>model</b> based pinyin input method for IBus.|$|E
2500|$|More concisely, an n-gram model predicts [...] {{based on}} [...] In {{probability}} terms, this is [...] When used for language modeling, independence assumptions are made {{so that each}} word depends only on the last n−1 words. [...] This Markov model is used as an approximation of the true underlying language. [...] This assumption {{is important because it}} massively simplifies the problem of estimating the <b>language</b> <b>model</b> from data. [...] In addition, because of the open nature of language, it is common to group words unknown to the <b>language</b> <b>model</b> together.|$|E
2500|$|Note that in {{a simple}} n-gram <b>language</b> <b>model,</b> the {{probability}} of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution (often imprecisely called a [...] "multinomial distribution").|$|E
40|$|Here we {{consider}} {{some of our}} recent work on Good-Tuing estimators in {{the larger context of}} learning theory and <b>language</b> <b>modeling.</b> The Good-turing estimators have {{played a significant role in}} natural <b>language</b> <b>modeling</b> for the past twenty years. We have recently shown that these particular leave-one-out estimators converge rapidly. Here we present these results and consider possible consequences for <b>language</b> <b>modeling</b> in general. In particular, other leave-oneout estimators, such as that for the cross entropy of various forms of <b>language</b> <b>models,</b> might also be shown to be rapidly converging using proof methods similar to those used for the Good-Turing estimators. This could have broad ramification in the analysis and development of <b>language</b> <b>modeling</b> methods. We suggest that, in <b>language</b> <b>modeling</b> at least, leave-one-out estimation may be more significant than Occam's razor. ...|$|R
40|$|This paper {{describes}} SpeechForms, {{a system}} that uses novel techniques to automatically identify form element semantics (element type) and form element content (element values), and to semi-automatically generate <b>language</b> <b>models</b> that allow users to fill out each web form element by voice. Preliminary experimental results show that simple per element <b>language</b> <b>models</b> are faster and may be more accurate than statistical n-gram <b>language</b> <b>models</b> from large amounts of web text data. Index Terms: <b>language</b> <b>modeling,</b> form understanding, information retrieva...|$|R
40|$|Probabilistic <b>language</b> <b>models</b> {{have gained}} {{popularity}} in Natural Language Processing {{due to their}} ability to successfully capture language structures and constraints with computational efficiency. Probabilistic <b>language</b> <b>models</b> are flexible and easily adapted to language changes over time as well as to some new <b>languages.</b> Probabilistic <b>language</b> <b>models</b> can be trained and their accuracy strongly related to the availability of large text corpora. In this paper, we investigate the usability of grapheme probabilistic models, specifically grapheme n-grams models in spellchecking as well as augmentative typing systems. Grapheme n-gram models require substantially smaller training corpora {{and that is one of}} the main drivers for this thesis in which we build grapheme n-gram <b>language</b> <b>models</b> for the Albanian language. There are presently no available Albanian language corpora to be used for probabilistic <b>language</b> <b>modeling.</b> Our technique attempts to augment spellchecking and typing systems by utilizing grapheme n-gram <b>language</b> <b>models</b> in improving suggestion accuracy in spellchecking and augmentative typing systems. Our technique can be implemented in a standalone tool or incorporated in another tool to offer additional selection/scoring criteria. Natural <b>language</b> processing, <b>language</b> <b>modeling,</b> statistical <b>language</b> <b>modeling,</b> grapheme n-grams...|$|R
2500|$|Most modern {{applications}} {{that rely on}} n-gram based models, such as machine translation applications, do not rely exclusively on such models; instead, they typically also incorporate Bayesian inference. Modern statistical models are typically made up of two parts, a prior distribution describing the inherent likelihood of a possible result and a likelihood function {{used to assess the}} compatibility of a possible result with observed data. When a <b>language</b> <b>model</b> is used, it is used as part of the prior distribution (e.g. to gauge the inherent [...] "goodness" [...] of a possible translation), and even then it is often not the only component in this distribution.|$|E
2500|$|It is {{not exactly}} known how {{different}} languages are stored in the mind. A researcher, Vivian Cook, proposes that the languages are separated into distinct compartments. This is termed the separation model. An L2 speaker will speak one of the languages, but no connection is made between them in the mind (Cook 2003: 7). Another proposed model is the integration model, which suggests that rather than having two separate mental lexicons, an L2 speaker has one lexicon where words from one language are stored from one language alongside words from the other. Regarding phonology, {{it has been found}} that L2 speakers sometimes have one merged system for producing speech, not distinguished by L1 or L2. The integration model focuses on how there is a balance between the unique elements of both languages, and how they form one system. Though these two proposed models offering interesting perspectives, it is impossible to have total separation because both languages exist in the same mind. Total integration is impossible because we are able to keep the languages apart in our minds (Cook 2003: 7). Another proposed model is the link <b>language</b> <b>model.</b> This model illustrates the idea that two languages within the same mind are able to influence and interact with one another. Further, the partial integration model illustrates the idea of partial overlapping between two languages in one mind. It doesn't differentiate between the languages in the overlap, but it shows how it functions as a single, conjoined system. These systems illustrate the point that vocabulary, syntax, and other aspects of language knowledge can be shared or overlapped between different languages within one mind (Cook 2003: 8). Finally, all of the models function together to create the integration continuum, an illustration that shows the possible relationships in [...] "multi-competence" [...] (Cook 2003: 9).|$|E
5000|$|Language {{models are}} used in {{information}} retrieval in the query likelihood model. Here a separate <b>language</b> <b>model</b> is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's <b>language</b> <b>model</b> [...] Commonly, the unigram <b>language</b> <b>model</b> is used for this purpose—otherwise known as the bag of words model.|$|E
40|$|Text {{documents}} are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing <b>language</b> <b>models</b> generally fail {{to account for}} discourse structure, but it is crucial {{if we are to}} have <b>language</b> <b>models</b> that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network <b>language</b> <b>models,</b> called Document-Context <b>Language</b> <b>Models</b> (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network <b>language</b> <b>models,</b> the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence. Comment: 10 pages, 3 figure...|$|R
40|$|We {{propose to}} {{investigate}} the use of grammatical information to build improved statistical <b>language</b> <b>models.</b> Until recently, <b>language</b> <b>models</b> were primarily influenced by local lexical constraints. Today, <b>language</b> <b>models</b> often utilize longer range lexical information to aid in their predictions. All of these <b>language</b> <b>models</b> ignore grammatical considerations other than those induced by the statistics of lexical constraints. We believe that properly incorporating additional grammatical structure will achieve improved <b>language</b> <b>models.</b> We will use link grammar as our grammatical base. Being highly lexical in nature, the link grammar formalism {{will allow us to}} integrate more traditional modeling schemes with grammatical ones. An efficient robust link grammar parser will assist in this undertaking. We will initially build finite state-based <b>language</b> <b>models</b> that will utilize relatively simple grammatical information, such as part-of-speech data, along with information sources used by other lang [...] ...|$|R
40|$|<b>Language</b> <b>modeling</b> plays a {{critical}} role for automatic speech recognition. Typically, the n-gram <b>language</b> <b>models</b> suffer {{from the lack of}} a good representation of historical words and an inability to estimate unseen parameters due to insufficient training data. In this study, we explore the application of latent semantic information (LSI) to <b>language</b> <b>modeling</b> and parameter smoothing. Our approach adopts latent semantic analysis to transform all words and documents into a common semantic space. The word-to-word, word-to-document and document-to-document relations are, accordingly, exploited for <b>language</b> <b>modeling</b> and smoothing. For <b>language</b> <b>modeling,</b> we present a new representation of historical words based on retrieval of the most relevant document. We also develop a novel parameter smoothing method, where the <b>language</b> <b>models</b> of seen and unseen words are estimated by interpolating the k nearest seen words in the training corpus. The interpolation coefficients are determined according to the closeness of words in the semantic space. As shown by experiments, the proposed modeling and smoothing methods can significantly reduce the perplexity of <b>language</b> <b>models</b> with moderate computational cost...|$|R
50|$|The {{development}} of the cache <b>language</b> <b>model</b> has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache <b>language</b> <b>model</b> {{in the field of}} statistical machine translation.|$|E
5000|$|In {{practice}} the multinomial coefficient is usually {{removed from the}} calculation. The reason {{is that it is}} a constant for a given bag of words (such as all the words from a specific document [...] ). The <b>language</b> <b>model</b> [...] should be the true <b>language</b> <b>model</b> calculated from the distribution of words underlying each retrieved document. In practice this <b>language</b> <b>model</b> is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So [...] is the probability of term [...] being generated by the <b>language</b> <b>model</b> [...] of document [...] This probability is multiplied for all terms from query [...] to get a rank for document [...] in the interval [...] The calculation is repeated for all documents to create a ranking of all documents in the document collection.|$|E
50|$|The log-bilinear {{model is}} another example of an {{exponential}} <b>language</b> <b>model.</b>|$|E
5000|$|Neural <b>language</b> <b>models</b> (or Continuous space <b>language</b> <b>models)</b> use {{continuous}} representations or embeddings {{of words}} {{to make their}} predictions. These models make use of Neural networks.|$|R
40|$|<b>Language</b> <b>models</b> for {{information}} retrieval have received much attention in recent years, with many claims being made about their performance. However, previous studies evaluating the <b>language</b> <b>modelling</b> approach {{for information}} retrieval used different query sets and heterogeneous collections, which make reported results difficult to compare. This research is a broad-based study that evaluates <b>language</b> <b>models</b> against {{a variety of}} search tasks — topic finding, named-page finding and topic distillation. The standard Text REtrieval Conference (TREC) methodology is used to compare <b>language</b> <b>models</b> to the probabilistic Okapi BM 25 system. Using consistent parameter choices, we compare results of different <b>language</b> <b>models</b> on three different search tasks, multiple query sets and three different text collections. For ad hoc retrieval, the Dirichlet smoothing method {{was found to be}} significantly better than Okapi BM 25, but for named-page finding Okapi BM 25 was more effective than the <b>language</b> <b>modelling</b> methods. Optimal smoothing parameters for each method were found to be dependent on the collection and the query set. For longer queries, the <b>language</b> <b>modelling</b> approaches required more aggressive smoothing but they were found to be more effective than with shorter queries. The choice of smoothing method was also found to {{have a significant effect on}} the performance of <b>language</b> <b>models</b> for information retrieval...|$|R
40|$|Morphologically rich {{languages}} {{suffer from}} data sparsity and out-of-vocabulary words problems. As a result, researchers use morphemes (sub-words) as units in <b>language</b> <b>modeling</b> instead of full-word forms. The use of morphemes in <b>language</b> <b>modeling,</b> however, {{might lead to}} a loss of word level dependency since a word can be segmented into 3 or more morphemes and the scope of the morpheme n-gram might be limited to a single word. In this paper we propose the use of roots to capture word-level dependencies in Amharic <b>language</b> <b>modeling.</b> Our experiment shows that root-based <b>language</b> <b>models</b> are better than the word based and other factored <b>language</b> <b>models</b> when compared {{on the basis of the}} probability they assign for the test set. However, no benefit has been obtained (in terms of word recognition accuracy) as a result of using root-based <b>language</b> <b>models</b> in a speech recognition task. 1...|$|R
5000|$|Ibus-sunpinyin, a {{statistical}} <b>language</b> <b>model</b> based pinyin input method for IBus.|$|E
50|$|The query {{likelihood}} {{model is}} a <b>language</b> <b>model</b> used in information retrieval. A <b>language</b> <b>model</b> is constructed for each document in the collection. It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being {{the likelihood of a}} document being relevant given a query.|$|E
5000|$|... {{whereas in}} a trigram (n = 3) <b>language</b> <b>model,</b> the {{approximation}} is ...|$|E
3000|$|<b>Language</b> <b>models</b> are {{important}} in various applications especially in speech recognition. Statistical <b>language</b> <b>models</b> are obtained using different approaches depending on the resources and tasks requirements. Extracting [...]...|$|R
30|$|The {{developed}} word-based and phoneme-based <b>language</b> <b>models</b> {{were also}} {{presented in this}} paper, {{as an example of}} practical applications of the obtained statistical data of Polish language. The obtained statistical data open up further opportunities to continue research on improving automatic speech recognition in Polish. The plan for future research includes the development of statistical word-based and subword-based <b>language</b> <b>models</b> for Polish. The word-based and subword-based <b>language</b> <b>modelling,</b> enables to develop a hybrid <b>language</b> <b>models</b> for out of vocabulary word detection in large vocabulary conversational speech recognition [64, 68 – 70].|$|R
30|$|On {{the basis}} of {{performed}} statistical analysis of the orthographic language corpus, there have been developed the N-gram word-based <b>language</b> <b>models</b> for N= 1,…, 3, intended for Polish language. In a similar way, on {{the basis of}} statistical analysis results of the phonemic language corpus, the N-gram phoneme-based <b>language</b> <b>models</b> for N= 1,…, 3, intended for Polish language, were developed. The details of word-based and phoneme-based <b>language</b> <b>models</b> developing process are presented in the separate publication. This article presents only the example of language statistical analysis application to develop selected <b>language</b> <b>models.</b>|$|R
