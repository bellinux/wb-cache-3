14|23|Public
40|$|In the {{attitude}} determination system exists {{not only the}} high-frequency errors, but also the <b>low-frequency</b> <b>errors</b> related to the satellite orbit latitude and time. The <b>low-frequency</b> <b>errors</b> would affect the location accuracy without GCPs, especially to the horizontal accuracy. In this paper, firstly, the factors that produce <b>low-frequency</b> <b>errors</b> and the solutions are analyzed. Secondly, the <b>low-frequency</b> <b>errors</b> are detected and compensated automatically during bundle adjustment in TH- 1 satellite, thus the influence of location accuracy by low-frequency error is eliminated. At last, the verification is tested using data of TH- 1. The experimental results show: <b>low-frequency</b> <b>errors</b> compensation can resolve the system error of position without GCP, and has {{played an important role}} in the consistency of global location accuracy for TH- 1...|$|E
40|$|The {{topographic}} mapping products at 1 : 50, 000 scale {{can be realized}} using satellite photogrammetry without ground control points (GCPs), which requires the high accuracy of exterior orientation elements. Usually, the attitudes of exterior orientation elements are obtained from the attitude determination system on the satellite. Based on the theoretical analysis and practice, the attitude determination system exists not only the high-frequency errors, but also the <b>low-frequency</b> <b>errors</b> related to the latitude of satellite orbit and the time. The <b>low-frequency</b> <b>errors</b> would affect the location accuracy without GCPs, especially to the horizontal accuracy. In SPOT 5 satellite, the latitudinal model was proposed to correct attitudes using approximately 20 calibration sites data, and the location accuracy was improved. The <b>low-frequency</b> <b>errors</b> are also found in Tian Hui 1 (TH- 1) satellite. Then, the method of compensation <b>low-frequency</b> <b>errors</b> is proposed in ground image processing of TH- 1, which can detect and compensate the <b>low-frequency</b> <b>errors</b> automatically without using GCPs. This paper deal with the <b>low-frequency</b> <b>errors</b> in TH- 1 : First, the analysis about <b>low-frequency</b> <b>errors</b> of the attitude determination system is performed. Second, the compensation models are proposed in bundle adjustment. Finally, the verification is tested using data of TH- 1. The testing results show: the <b>low-frequency</b> <b>errors</b> of attitude determination system can be compensated during bundle adjustment, which can improve the location accuracy without GCPs and has {{played an important role}} in the consistency of global location accuracy...|$|E
30|$|For {{the spindle}} errors, only the {{asynchronous}} errors affect the spatial frequencies in the feed direction, and the synchronous errors {{has no effect}} on the spatial frequencies in that direction [20]. When the spindle rotation speed is 390  r/min, only the spindle errors with the frequency value over 56.8  Hz possibly influence the spatial frequencies in the cutting direction. Consequently, the <b>low-frequency</b> <b>errors</b> with the frequency value less than the fundamental frequency (i.e., 6.5  Hz) {{has no effect on}} the spatial frequencies of the machined surface in the cutting direction. Besides, if only considering the <b>low-frequency</b> <b>errors</b> in this range, when the feed rate is 0.06  mm/s, the frequencies of corresponding <b>low-frequency</b> <b>errors</b> for 8.3  mm− 1 and 0.03  mm− 1 are 0.498  Hz and 0.0018  Hz, respectively. When the frequency of low-frequency error is larger than 0.498  Hz, the corresponding spatial frequency in the feed direction will enter into Band 4, which does not influence the PSD specification. Therefore, for the error with the frequency value in the range of less than the fundamental frequency, the frequency of low-frequency error needs to be larger than 0.498  Hz.|$|E
40|$|AbstractThe <b>low-frequency</b> {{periodic}} <b>error</b> of {{star tracker}} {{is one of}} the most critical problems for high-accuracy satellite attitude determination. In this paper an approach is proposed to identify and compensate the <b>low-frequency</b> periodic <b>error</b> for star tracker in attitude measurement. The analytical expression between the estimated gyro drift and the <b>low-frequency</b> periodic <b>error</b> of star tracker is derived firstly. And then the <b>low-frequency</b> periodic <b>error,</b> which can be expressed by Fourier series, is identified by the frequency spectrum of the estimated gyro drift according to the solution of the first step. Furthermore, the compensated model of the <b>low-frequency</b> periodic <b>error</b> is established based on the identified parameters to improve the attitude determination accuracy. Finally, promising simulated experimental results demonstrate the validity and effectiveness of the proposed method. The periodic error for attitude determination is eliminated basically and the estimation precision is improved greatly...|$|R
50|$|The {{phase-locked loop}} {{detector}} requires no frequency-selective LC network to accomplish demodulation. In this system, a voltage controlled oscillator (VCO) is phase locked by a feedback loop, which forces the VCO {{to follow the}} frequency variations of the incoming FM signal. The <b>low-frequency</b> <b>error</b> voltage that forces the VCO's frequency to track {{the frequency of the}} modulated FM signal is the demodulated audio output.|$|R
40|$|The {{conventional}} current-regulated {{delta modulator}} (CRDM) {{results in a}} high current ripple and a high switching frequency at low rotational speeds, and in low-frequency current harmonics, including a fundamental current error, at high rotational speeds. An improved current controller based on CRDM is proposed which introduces a zero-vector zone and a current error correction technique. It reduces the current ripple and switching frequency at low speeds, without the need to detect the back-emf, {{as well as the}} <b>low-frequency</b> <b>error</b> at high speeds. The performance of the modulator is verified by both simulation and measurements on a permanent magnet brushless ac drive...|$|R
40|$|Abstract — <b>Low-frequency</b> <b>errors</b> of {{thin-film}} multijunction {{thermal voltage}} converters are estimated using a simple model based on easily measured parameters. The model predictions are verified {{by measuring the}} converter’s frequency characteristic using a digitally synthesized source. Index Terms — AC–DC power conversion, error analysis, frequency domain analysis, nonlinear systems, thermal converters, thin-film circuit thermal factors, thin-film circuits, transfer functions. I...|$|E
40|$|National Natural Science Foundation [51075343]; Fundamental Research Funds of Xiamen University [201212 G 011]The {{theoretical}} {{analysis of}} corrective characteristics of {{three kinds of}} polishing methods for mid-frequency errors was studied, which was aimed to confirm the possibility that computer control optical surfacing and computer control active-lap can be replaced by bonnet polishing in the machining process. The {{first step was to}} calculate the removal functions of three kinds of polishing technologies and use fast Fourier transform to figure out the frequency spectrum of each method. After that, according to the frequency spectra, curves of cut-off frequencies related to the working ranges of spatial frequencies errors were obtained. It revealed that the affected scope of spatial frequencies is determined by the polishing method, diameter size of polishing tool and shape of removal function. Moreover, only <b>low-frequency</b> <b>errors</b> could be modified and mid-frequency errors could not be corrected or created by computer control active-lap, and computer control optical surfacing can correct part of the mid-frequency errors and <b>low-frequency</b> <b>errors</b> in the polishing process, {{but at the same time}} can produce some new mid-frequency errors; as for bonnet polishing, it can be computer control active-lap-like in smoothing which only modified and created the <b>low-frequency</b> <b>errors</b> or computer control optical surfacing-like which corrected and created the mid-frequency errors in local polishing. Otherwise, the efficiency of bonnet polishing is higher than the other two methods. As a result, seen from the point of correction ability of mid-frequency or polishing efficiency, bonnet polishing could replace computer control active-lap and computer control optical surfacing for finishing two polishing stages by only one tool, which is significant to extending the application of bonnet polishing in optical manufacturing...|$|E
40|$|Motivation: Photoactivatable ribonucleoside-enhanced {{cross-linking}} and immunoprecipitation (PAR-CLIP) is {{an experimental}} method based on next-generation sequencing for identifying the RNA interaction sites {{of a given}} protein. The method deliberately inserts T-to-C substitutions at the RNA-protein interaction sites, which provides a second layer of evidence compared to other CLIP methods. However, the experiment includes several sources of noise which cause both <b>low-frequency</b> <b>errors</b> and spurious high-frequency alterations. Therefore, rigorous statistical analysis is {{required in order to}} separate true T-to-C base changes, following cross-linking, from noise. So far, most of the existing PAR-CLIP data analysis methods focus on discarding the <b>low-frequency</b> <b>errors</b> and rely on high-frequency substitutions to report binding sites, not taking into account the possibility of high-frequency false positive substitutions. Results: Here, we introduce BMix, a new probabilistic method which explicitly accounts for the sources of noise in PAR-CLIP data and distinguishes cross-link induced T-to-C substitutions from low and high-frequency erroneous alterations. We demonstrate the superior speed and accuracy of our method compared to existing approaches on both simulated and real, publicly available human datasets. Availability: The model is freely accessible within the BMix toolbox at www. cbg. bsse. ethz. ch/software/BMix, available for Matlab and R. Contact...|$|E
40|$|We {{present an}} {{interactive}} system for reconstructing surface normals {{from a single}} image. Our approach has two complementary contributions. First, we introduce a novel shape-from-shading algorithm (SfS) that produces faithful normal reconstruction for local image region (high-frequency component), but it fails to faithfully recover the overall global structure (low-frequency component). Our second contribution consists of an approach that corrects <b>low-frequency</b> <b>error</b> using a simple markup procedure. This approach, aptly called rotation palette, allows the user to specify large scale corrections of surface normals by drawing simple stroke correspondences between the normal map and a sphere image which represents rotation directions. Combining these two approaches, we can produce high-quality surfaces quickly from single images...|$|R
50|$|For the {{analysis}} of Phase detector it is usually considered the modelsof PD in signal (time) domain and phase-frequency domain.In this case for constructing of an adequate nonlinear mathematical model of PD in phase-frequency domain {{it is necessary to}} find the characteristic of phase detector.The inputs of PD are high-frequency signals and the output contains a <b>low-frequency</b> <b>error</b> correction signal, corresponding to a phase difference of input signals. For the suppression of high-frequency component of the output of PD (if such component exists) a low-pass filter is applied. Thecharacteristic of PD is the dependence of the signal at theoutput of PD (in the phase-frequency domain) on the difference of phases at the input of PD.|$|R
40|$|A multigrid {{anisotropic}} diffusion algorithm for {{image processing}} is presented. The multigrid implementation provides an efficient hierarchical relaxation method that facilitates {{the application of}} anisotropic diffusion to time-critical processes. Through a multigrid V-cycle, the anisotropic diffusion equations are successively transferred to coarser grids and used in a coarseto -fine error correction scheme. When a coarse grid with a trivial solution is reached, the coarse grid estimates of the residual error can be propagated to the original grid and used to refine the solution. The main benefits of the multigrid approach are rapid intraregion smoothing and reduction of artifacts due to the elimination of <b>low-frequency</b> <b>error.</b> In the paper, the theory of multigrid anisotropic diffusion is developed. Then, the intergrid transfer functions, relaxation techniques, diffusion coefficients, and boundary conditions are discussed. The analysis includes {{the examination of the}} storage requirements, the computational cost, and the solution quality. Finally, experimental results are reported that demonstrate the effectiveness of the multigrid approach...|$|R
40|$|Photoactivatable ribonucleoside-enhanced {{cross-linking}} and immunoprecipitation (PAR-CLIP) is {{an experimental}} method based on next-generation sequencing for identifying the RNA interaction sites {{of a given}} protein. The method deliberately inserts T-to-C substitutions at the RNA-protein interaction sites, which provides a second layer of evidence compared to other CLIP methods. However, the experiment includes several sources of noise which cause both <b>low-frequency</b> <b>errors</b> and spurious high-frequency alterations. Therefore, rigorous statistical analysis is {{required in order to}} separate true T-to-C base changes, following cross-linking, from noise. So far, most of the existing PAR-CLIP data analysis methods focus on discarding the <b>low-frequency</b> <b>errors</b> and rely on high-frequency substitutions to report binding sites, not taking into account the possibility of high-frequency false positive substitutions. Here, we introduce BMix, a new probabilistic method which explicitly accounts for the sources of noise in PAR- CLIP data and distinguishes cross-link induced T-to-C substitutions from low and high-frequency erroneous alterations. We demonstrate the superior speed and accuracy of our method compared to existing approaches on both simulated and real, publicly available human datasets. The model is implemented in the Matlab toolbox BMix, freely available at www. cbg. bsse. ethz. ch/software/BMix. Comment: The manuscript has been accepted for oral presentation at the Fifth RECOMB Satellite Workshop on Massively Parallel Sequencing (RECOMB-SEQ 2015...|$|E
40|$|Solving sparse linear {{systems from}} discretized PDEs is challenging. Direct solvers have {{in many cases}} {{quadratic}} complexity (depending on geometry), while iterative solvers require problem dependent preconditioners to be robust and efficient. Approximate factorization preconditioners, such as incomplete LU factorization, provide cheap approximations to the system matrix. However, even a highly accurate preconditioner may have deteriorating performance when the condition number of the system matrix increases. By increasing the accuracy on <b>low-frequency</b> <b>errors,</b> we propose a novel hierarchical solver with improved robustness {{with respect to the}} condition number of the linear system. This solver retains the linear computational cost and memory footprint of the original algorithm...|$|E
40|$|International audienceConsisting {{of three}} pairs of accelerometers, the {{gradiometer}} is an ideal sensor for passive navigation. This paper proposes the use of gravity gradients for spacecraft positioning and the real-world GOCE EGG data are tested to investigate the feasibility. The basic observation equation is first formulated by considering white noise only, and a Least-Square position searching method is developed. The raw GGT measurements are preprocessed before the test in order to remove the <b>low-frequency</b> <b>errors.</b> By using a 120 -degree EGM 2008 gravity model as a reference map, position solutions with an accuracy of hundreds of meters are obtained. A further semi-simulation study shows that an accuracy of tens of meters could be achieved with a better gradiometer...|$|E
40|$|Kaczmarz {{method is}} one popular {{iterative}} method for solving inverse problems, especially in computed tomography. Recently, it was established that a randomized {{version of the}} method enjoys an exponential convergence for well-posed problems, and the convergence rate is determined by {{a variant of the}} condition number. In this work, we analyze the preasymptotic convergence behavior of the randomized Kaczmarz method, and show that the <b>low-frequency</b> <b>error</b> (with respect to the right singular vectors) decays faster during first iterations than the high-frequency error. Under the assumption that the inverse solution is smooth (e. g., sourcewise representation), the result explains the fast empirical convergence behavior, thereby shedding new insights into the excellent performance of the randomized Kaczmarz method in practice. Further, we propose a simple strategy to stabilize the asymptotic convergence of the iteration by means of variance reduction. We provide extensive numerical experiments to confirm the analysis and to elucidate the behavior of the algorithms. Comment: 20 page...|$|R
40|$|After {{summarizing}} {{the advantages and}} disadvantages of current integral methods, a novel vibration signal integral method based on feature information extraction was proposed. This method took full advantage of the self-adaptive filter characteristic and waveform correction feature of ensemble empirical mode decomposition in dealing with nonlinear and nonstationary signals. This research merged the superiorities of kurtosis, mean square error, energy, and singular value decomposition on signal feature extraction. The values of the four indexes aforementioned were combined into a feature vector. Then, the connotative characteristic components in vibration signal were accurately extracted by Euclidean distance search, and the desired integral signals were precisely reconstructed. With this method, the interference problem of invalid signal such as trend item and noise which plague traditional methods is commendably solved. The great cumulative error from the traditional time-domain integral is effectively overcome. Moreover, the large <b>low-frequency</b> <b>error</b> from the traditional frequency-domain integral is successfully avoided. Comparing with the traditional integral methods, this method is outstanding at removing noise and retaining useful feature information and shows higher accuracy and superiority...|$|R
40|$|Abstract—Multitemporal and multisatellite {{studies or}} {{comparisons}} between satellite data and local ground measurements require nowadays precise and automatic geometric correction of satellite images. This paper presents a fully automatic geometric correction system capable of georeferencing satellite images with high accuracy. An orbital prediction model, which provides initial earth locations, {{is combined with}} the proposed automatic contour-matching technique. This combination allows correcting the <b>low-frequency</b> <b>error</b> component, mainly due to timing and orbital model errors, {{as well as the}} high-frequency error component, due to variations in the spacecraft’s attitude. The approach aims at exploiting the maximum reliable information in the image to guide the matching algorithm. The contour-matching process has three main steps: 1) estimation of the gradient energy map (edges) and detection of the cloudless (reliable) areas; 2) initialization of the contours positions; 3) estimation of the transformation parameters (affine model) using a contour optimization approach. Three different robust and automatic algorithms are proposed for optimization, and their main features are discussed. Finally, the performance of the three proposed algorithms is assessed using a new error estimation technique applied to Advanced Very High Resolution Radiometer (AVHRR), Sea-viewing Wide Field of view Sensor (SeaWiFS), and multisensor AVHRR–SeaWiFS imagery...|$|R
40|$|This paper {{presents}} an innovative test procedure for {{the prediction of}} the shielding effectiveness of small sample materials, consisting of a dielectric substrate coated with thin conducting film, in a wide frequency range up to 8 GHz. The proposed technique overcomes {{the limitations of the}} ASTM D 4935 test method concerning the upper operating frequency and the required minimum specimen dimensions. A new high-order equivalent circuit model of the test fixture is developed. A correction factor is applied to the measured insertion loss to eliminate both the resonance peak below cutoff appearing in the high-frequency range and the <b>low-frequency</b> <b>errors</b> due to the weak capacitive coupling between the flanges of the coaxial cell. The accurate prediction of the shielding effectiveness of the test material against a plane wave is then derived from the insertion loss measurements...|$|E
40|$|AbstractSpace Technology Experiment and Climate Exploration (STECE) {{is a small}} {{satellite}} {{mission of}} China for space technology experiment and climate exploration. A new test star tracker and one ASTRO 10 star tracker have been loaded on the STECE satellite to test the new star tracker’s measurement performance. However, there is no autonomous precession–nutation correction function for the test star tracker, which causes an apparent periodic deflection in the inter-boresight angle between the two star trackers with {{respect to each other}} of up to ± 500 arcsec, so the precession and nutation effect needs to be considered while assessing the test star tracker. This paper researches on the precession–nutation correction for the test star tracker’s attitude measurement and presents a precession–nutation correction method based on attitude quaternion data. The periodic deflection of the inter-boresight angle between the two star trackers has been greatly eliminated after the precession and nutation of the test star tracker’s attitude data have been corrected by the proposed method and the validity of the proposed algorithm has been demonstrated. The in-flight accuracy of the test star tracker has been assessed like attitude noise and <b>low-frequency</b> <b>errors</b> after the precession–nutation correction...|$|E
40|$|Abstract Most {{baseline}} {{errors of}} analog strong-motion data still exist in high-resolution data. In this study, we identify the major baseline errors of digital strong-motion data and propose a three-step algorithm to correct hese errors. The major baseline rrors found in these digital data consist of constant drift in the ac-celeration, low-frequency instrument noise, low-frequency background noise, the small initial values for acceleration and velocity, and manipulation errors. This three-step algorithm includes fitting the baseline of acceleration by the least squares, ap-plying a high-pass filter in acceleration, and subtracting the initial values in velocity. A least-squares fit of {{a straight line}} before filtering can effectively remove the base-line drift in acceleration. Then, the filtering removes the linear trend and other <b>low-frequency</b> <b>errors</b> {{that exist in the}} acceleration. Finally, the subtracting of the initial velocity removes the linear trend of displacement. Among these three steps, only the filtering in the second step may introduce a side effect. Compared to the Volume II routine developed by Trifunac and Lee (1973), this three-step rocessing significantly reduces computational efforts and side effects resulting from unneces-sary manipulation of data. This algorithm has been successfully tested on several types of digital strong-motion data. Several independent validations how that the proposed algorithm is stable...|$|E
40|$|For the {{analysis}} of PLL {{it is necessary to}} consider the models of PLL in signal space and phase space [Viterbi(1966), Gardner(1966), Shakhgil’dyan & Lyakhovkin(1972) ]). In this case for constructing of an ade-quate nonlinear mathematical model of PLL in phase space it is necessary to find the characteristic of phase detector (PD — a nonlinear element, used in PLL for matching tunable signals). The inputs of PD are high-frequency signals of reference and tunable oscillators and the output contains a <b>low-frequency</b> <b>error</b> cor-rection signal, corresponding to a phase difference of input signals. For the suppression of high-frequency component of the output of PD (if such component exists) the low-pass filter can be applied. The char-acteristic of PD is the dependence of the signal at the output of PD (in the phase space) on the phase difference of signals at the input of PD. This characteristic depends on the realization of PD and the types of signals at the input. Characteristics of the phase detector for standard types of signal are well-known to engineers [Viterbi(1966),Shakhgil’dyan & Lyakhovkin(1972),Abramovitch(2002) ]. Further following [Leonov(2008) ], on the examples of classical PLL with a phase detector in the form of mul-tiplier, we consider the general principles of computation of phase detector characteristics for different types of signals based on a rigorous mathematical analysis of high-frequency oscillations [Leonov & Seledghi(2005) ...|$|R
40|$|This letter {{addresses}} {{the impact of}} limited oscillator stability in bistatic and multistatic synthetic aperture radars (SARs). Oscillator noise deserves special attention in distributed SAR systems {{since there is no}} cancellation of <b>low-frequency</b> phase <b>errors</b> as in a monostatic SAR, where the same oscillator signal is used for modulation and demodulation. It is shown that the uncompensated phase noise may cause a time-variant shift, spurious sidelobes, and a broadening of the impulse response, as well as a low-frequency phase modulation of the focused SAR signal. Quantitative estimates are derived analytically for each of these errors based on a system-theoretic model taking into account the second-order statistics of the oscillator phase noise...|$|R
40|$|We {{propose a}} method to {{simulate}} the rich, scale-dependent dynamics of water waves. Our method preserves the dispersion properties of real waves, yet it supports interactions with obstacles and is computationally efficient. Fundamentally, it computes wave accelerations by way of applying a dispersion kernel as a spatially variant filter, which {{we are able to}} compute efficiently using two core technical contributions. First, we design novel, accurate, and compact pyramid kernels which compensate for <b>low-frequency</b> truncation <b>errors.</b> Second, we design a shadowed convolution operation that efficiently accounts for obstacle interactions by modulating the application of the dispersion kernel. We demonstrate a wide range of behaviors, which include capillary waves, gravity waves, and interactions with static and dynamic obstacles, all from within a single simulation. Funding Sources: European Research Council; Spanish Ministry of EconomyPeer Reviewe...|$|R
40|$|Any {{quantization}} introduces errors. An {{important question}} is how to suppress their visual effect. In this paper we present a new quantization method for the geometry of 3 D meshes, which enables aggressive quantization without significant loss of visual quality. Conventionally, quantization is applied directly to the 3 -space coordinates. This form of quantization introduces high-frequency errors into the model. Since high-frequency errors modify the appearance of the surface, they are highly noticeable, and commonly, this form of quantization must be done conservatively to preserve the precision of the coordinates. Our method first multiplies the coordinates by the Laplacian matrix of the mesh and quantizes the transformed coordinates which we call “δ-coordinates”. We show that the highfrequency quantization errors in the δ-coordinates are transformed into <b>low-frequency</b> <b>errors</b> when the quantized δ-coordinates are transformed back into standard Cartesian coordinates. These lowfrequency errors in the model are much less noticeable than the high-frequency errors. We call our strategy high-pass quantization, to emphasize the fact that it tends to concentrate the quantization error at the low-frequency end of the spectrum. To allow some control over the shape and magnitude of the low-frequency quantization errors, we extend the Laplacian matrix by adding a number of spatial constraints. This enables us to tailor the quantization process to specific visual requirements, and to strongly quantize the δ-coordinates...|$|E
40|$|Azimuth axis {{rotating}} modulation {{was introduced}} to improve the alignment accuracy of strapdown inertial navigation system (SINS) through compass algorithm, in which the limit accuracy was determined by equivalent sensor errors in the eastern and northern direction. In this modulation, horizontal sensor errors were modulated into zero mean periodic variables. Furthermore, two methods were introduced to ensure alignment accuracy and speed: (1) shortened rotating cycle and redesigned compass parameters were selected to eliminate or ease the amplification to <b>low-frequency</b> senor <b>error</b> inputs in compass loop caused by rotation and (2) a data repeated calculation method was designed to shorten prolonged alignment time caused by the above redesigned parameters. Based on a certain SINS, turntable test proves that alignment accuracy and time were significantly improved and slightly shortened {{in comparison with the}} classical compass alignment...|$|R
40|$|The {{research}} on electrochemical carbon molecule oxidation {{started in the}} past years as new electrochemistries were researched for new fuel cells systems and batteries that may line up as backup energy supply and storage systems in off-grid and on-grid microgrids, as modeled by our group at the University of Twente [1 - 3]. By lining up these two disciplines we hope to support the bridge between new electrochemical systems on one side, (pilot) production with partner companies, prediction, and validation of systems upon implementation in microgrids by our group. The electrochemical oxidation of glycerol in alkaline aqueous solution has been studied on gold and gold coated metals (Zn-Au and Cu-Au) by voltammetry and EIS (Electrochemical impedance spectroscopy) for possible use in a new fuel cell as an outlet for the excess glycerol that is produced in the biodiesel industry. The observations show that the gold surface may change upon cycling by cyclic voltammetry. Besides, the current density shows non-linear behavior with the square root of the scan rate, implying that the reaction is not totally controlled by diffusion. EIS analysis using the EQUIVCRT software revealed {{that one out of}} twenty tested equivalent circuits fitted the data well at potentials of - 0. 05 V,- 0. 15 V and - 0. 25 V vs. Ag/AgCl, identifying resistors and a Warburg element in parallel with the double layer capacitance, the elements are possibly related to the presence of double layers associated with hydroxypyrovate and oxalate ions. The results are consistent with the <b>low-frequency</b> <b>error</b> fitting analysis (10 - 4), AC Simulink-Matlab fitting responds and the Kronig-Kramers transform test. The tested Zn-Au and Cu-Au electrodes show similar voltammetry behavior as the gold electrode, as witnessed by the results of cycle analysis and the scan rate analysis. The discharge chronoamperometry test further shows that the Zn-Au electrode and Cu-Au have higher current densities than the gold electrode at a potential of - 0. 25 V vs. Ag/AgCl (5 mA cm- 2, 4. 5 mA cm- 2, and 3 mA cm- 2 respectively). Glicerol; Glycerol; Gold; Impedance; Impedancia; Microgrids; Oro; Oxidación; Oxidation; Voltametría; Voltammetr...|$|R
40|$|Image texture is {{the term}} given to the {{information-bearing}} fluctuations such as those for skin, grass and fabrics. Since image processing aimed at reducing unwanted fluctuations (noise are other artifacts) can also remove important texture, good product design requires {{a balance between the}} two. The texture-loss MTF method, currently under international standards development, is aimed at the evaluation of digital and mobile-telephone cameras for capture of image texture. The method uses image fields of pseudo-random objects, such as overlapping disks, often referred to as ‘dead-leaves’ targets. The analysis of these target images is based on noise-power spectrum (NPS) measurements, which are subject to estimation error. We describe a simple method for compensation of non-stationary image statistics, aimed at improving practical NPS estimates. A benign two-dimensional linear function (plane) is fit to the data and subtracted. This method was implemented and results were compared with those without compensation. The adapted analysis method resulted in reduced NPS and MTF measurement variation (20 %) and <b>low-frequency</b> bias <b>error.</b> This is a particular advantage at low spatial frequencies, where texture-MTF scaling is performed. We conclude that simple trend removal should be used...|$|R
40|$|The {{subject of}} zero-ripple torque control in Brushless DC Motors (BLDCM’s) has gained {{importance}} {{due to the}} growing popularity of small electric motors in consumer electronic applications. A low number of phases and the occurrence of production tol erances give rise to <b>low-frequency</b> torque <b>errors,</b> which manifest themselves as relatively large position errors due to the low inertia of these small drives. With regard to the tight specifications of the controlled performance, reduction of these low frequent torque errors is desirable. In literature, two main approaches have been demonstrated for the analysis and mini mization of torque ripple. One approach is based on Fourier analysis, while the other uses variation calculus to find optimal current waveforms. In this paper, a new approach for the determination of optimal current waveforms is presented. The approach is based on elementary differential calculus, and can be used even in the case when both the back-emf’s and the stator resistances show asymmetry. The new approach is compared to the Fourier method in a test case, and shows significant reduction in RMS and average values of the stator currents needed to generate a desired torque...|$|R
40|$|This is {{the author}} {{accepted}} manuscript. Vision-based systems offer a promising way for displacement measurement and receive increased attention in civil structural monitoring. However, the working performance of vision-based systems, especially the measurement accuracy and the robustness to different field conditions is not fully understood. This study reports three cases studies of vision-based monitoring tests including one in a laboratory, one on a short-span bridge and one on a long-span bridge. The tracking accuracy is quantified in laboratory conditions {{in the range of}} 0. 02 pixel to 0. 20 pixel depending on the target patterns as well as the tracking method selected. The measurement performance under several field challenges are investigated including long-range measurement (e. g. camera-to-target distance at 710 m), low-contrast target patterns, changes of target patterns and changes in lighting conditions. Three representative tracking methods for the video processing, i. e. correlation-based template matching, Lucas Kanade (LK) optical flow estimation and scale-invariant feature transform (SIFT) were used for analysis, indicating their advantages and shortcomings for field measurement. One of the main observations in field application is that changes in lighting conditions might cause some <b>low-frequency</b> measurement <b>error</b> that could be misunderstood without the prior knowledge about structural loading conditions...|$|R
40|$|Monolithic PWM voltage-mode buck {{converters}} {{with a novel}} Pseudo-Type III (PT 3) compensation are presented. The proposed compensation {{maintains the}} fast load transient response of the conventional Type III compensator; while the Type III compensator response is synthesized by adding a high-gain <b>low-frequency</b> path (via <b>error</b> amplifier) with a moderate-gain high-frequency path (via bandpass filter) at the inputs of PWM comparator. As such, smaller passive components and low-power active circuits {{can be used to}} generate two zeros required in a Type III compensator. Constant G m /C biasing technique can also be adopted by PT 3 to reduce the process variation of passive components, which is not possible in a conventional Type III design. Two prototype chips are fabricated in a 0. 35 -μm CMOS process with constant G m /C biasing technique being applied to one of the designs. Measurement result shows that converter output is settled within 7 μs for a load current step of 500 mA. Peak efficiency of 97 % is obtained at 360 mW output power, and high efficiency of 86 % is measured for output power as low as 60 mW. The area and power consumption of proposed compensator is reduced by > 75 % in both designs, compared to an equivalent conventional Type III compensator. © 2006 IEEE...|$|R
40|$|Phase unwrapping is {{the most}} {{critical}} step in the processing of synthetic aperture radar interferometry. The phase obtained by SAR interferometry is wrapped over a range from -π to π. Phase unwrapping must be performed to obtain the true phase. The least square approach attains the unwrapped phase by minimizing {{the difference between the}} discrete partial derivatives of the wrapped phase and the discrete partial derivatives of the unwrapped solution. The least square solution will result in discrete version of the Poisson's partial differential equation. Solving the discretized Poisson's equation with the classical method of Gauss-Seidel relaxation has extremely slow convergence. In this paper we have used Wavelet techniques which overcome this limitation by transforming <b>low-frequency</b> components of <b>error</b> into high frequency components which consequently can be removed quickly by using the Gauss-Seidel relaxation method. In Discrete Wavelet Transform (DWT) two operators, decomposition (analysis) and reconstruction (synthesis), are used. In the decomposition stage an image is separated into one low-frequency component (approximation) and three high-frequency components (details). In the reconstruction stage, the image is reconstructed by synthesizing the approximated and detail components. We tested our algorithm on both simulated and real data and on both unweighted and weighted forms of discretized Poisson's equation. The experimental results show the effectiveness of the proposed method...|$|R
40|$|In both {{military}} and civilian applications, the inertial navigation system (INS) and the global positioning system (GPS) are two complementary technologies that can be integrated to provide reliable positioning and navigation information for land vehicles. The accuracy enhancement of INS sensors and the integration of INS with GPS are the subjects of widespread research. Wavelet de-noising of INS sensors has had limited success in removing the long-term (<b>low-frequency)</b> inertial sensor <b>errors.</b> The primary objective {{of this research is}} to develop a novel inertial sensor accuracy enhancement technique that can remove both short-term and long-term error components from inertial sensor measurements prior to INS mechanization and INS/GPS integration. A high resolution spectral analysis technique called the fast orthogonal search (FOS) algorithm is used to accurately model the low frequency range of the spectrum, which includes the vehicle motion dynamics and inertial sensor errors. FOS models the spectral components with the most energy first and uses an adaptive threshold to stop adding frequency terms when fitting a term does not reduce the mean squared error more than fitting white noise. The proposed method was developed, tested and validated through road test experiments involving both low-end tactical grade and low cost MEMS-based inertial systems. The results demonstrate that in most cases the position accuracy during GPS outages using FOS de-noised data is superior to the position accuracy using wavelet de-noising...|$|R
40|$|Abstract Background The rapid {{evolution}} of 454 GS-FLX sequencing technology {{has not been}} accompanied by a reassessment of the quality and accuracy of the sequences obtained. Current strategies for decision-making and error-correction are based on an initial analysis by Huse et al. in 2007, for the older GS 20 system based on experimental sequences. We analyze here the quality of 454 sequencing data and identify factors {{playing a role in}} sequencing error, through the use of an extensive dataset for Roche control DNA fragments. Results We obtained a mean error rate for 454 sequences of 1. 07 %. More importantly, the error rate is not randomly distributed; it occasionally rose to more than 50 % in certain positions, and its distribution was linked to several experimental variables. The main factors related to error are the presence of homopolymers, position in the sequence, size of the sequence and spatial localization in PT plates for insertion and deletion errors. These factors can be described by considering seven variables. No single variable can account for the error rate distribution, but most of the variation is explained by the combination of all seven variables. Conclusions The pattern identified here calls for the use of internal controls and error-correcting base callers, to correct for errors, when available (e. g. when sequencing amplicons). For shotgun libraries, the use of both sequencing primers and deep coverage, combined with the use of random sequencing primer sites should partly compensate for even high error rates, although it may prove more difficult than previous thought to distinguish between <b>low-frequency</b> alleles and <b>errors.</b> </p...|$|R
40|$|The time-to-result for culture-based {{microorganism}} {{recovery and}} phenotypic {{antimicrobial susceptibility testing}} necessitate initial use of empiric (frequently broad-spectrum) antimicrobial therapy. If the empiric therapy is not optimal, {{this can lead to}} adverse patient outcomes and contribute to increasing antibiotic resistance in pathogens. New, more rapid technologies are emerging to meet this need. Many of these are based on identifying resistance genes, rather than directly assaying resistance phenotypes, and thus require interpretation to translate the genotype into treatment recommendations. These interpretations, like other parts of clinical diagnostic workflows, are likely to be increasingly automated in the future. We set out to evaluate the two major approaches that could be amenable to automation pipelines: rules-based methods and machine learning methods. The rules-based algorithm makes predictions based upon current, curated knowledge of Enterobacteriaceae resistance genes. The machine-learning algorithm predicts resistance and susceptibility based on a model built from a training set of variably resistant isolates. As our test set, we used whole genome sequence data from 78 clinical Enterobacteriaceae isolates, previously identified to represent a variety of phenotypes, from fully-susceptible to pan-resistant strains for the antibiotics tested. We tested three antibiotic resistance determinant databases for their utility in identifying the complete resistome for each isolate. The predictions of the rules-based and machine learning algorithms for these isolates were compared to results of phenotype-based diagnostics. The rules based and machine-learning predictions achieved agreement with standard-of-care phenotypic diagnostics of 89. 0 % and 90. 3 %, respectively, across twelve antibiotic agents from six major antibiotic classes. Several sources of disagreement between the algorithms were identified. Novel variants of known resistance factors and incomplete genome assembly confounded the rules-based algorithm, resulting in predictions based on gene family, rather than on knowledge of the specific variant found. <b>Low-frequency</b> resistance caused <b>errors</b> in the machine-learning algorithm because those genes were not seen or seen infrequently in the test set. We also identified an example of variability in the phenotype-based results that led to disagreement with both genotype-based methods. Genotype-based antimicrobial susceptibility testing shows great promise as a diagnostic tool, and we outline specific research goals to further refine this methodology...|$|R
40|$|The {{aim of the}} {{dissertation}} was {{to obtain}} insight in semantic effects in word identification of Dutch children. Chapter 2 focused on the predictive value of risk factors, cognitive factors, and teachers' judgments {{in a sample of}} 462 kindergartners for their early reading skills in Grade 1. The discriminatory power of all predictors appeared to be insufficient. Results of a discriminant function analysis, however, demonstrated that a combination of different factors increased the accuracy of prediction. Chapter 3 describes the results of two intervention studies in Grade 1. In the first study, 121 poor beginning readers were assigned to a semantically-oriented training, a phonologically-oriented training or a control group. At post-test, children in both experimental training programs showed similar gains in word-identification skills. In the second study, the experimental-training programs were modified and extended. About 83 poor beginning readers participated in one of both experimental groups or were assigned to a control group. Result showed that all groups performed statistically equally across all reading measures. In Chapter 4, the relationship between semantic skills and word-decoding skills was examined. In Experiment 1, results revealed no differences between poor decoders and good decoders in word-association skills, whereas poor readers were more error prone in a semantic-categorization test. In Experiment 2, children from Grades 1 to 6 participated in two types of semantic-categorization tasks and a word-decoding test. Poor readers showed longer reaction times on both types of categorization tasks than average readers and good readers. Chapter 5 focused on imageability effects in isolated-word reading in Dutch children from Grades 2 to 6. Word-reading skills were assessed by lexical decision (Experiment 1) and naming (Experiment 2). Results of Experiment 1 revealed an imageability advantage in the non-speeded task in latencies. In addition, in <b>low-frequency</b> words, more <b>errors</b> were made in low-imageability (LI-) words than in high-imageability (HI-) words. Accuracy analyses revealed a significant grade by instruction by imageability interaction; in the higher grades, only in the speeded test more errors were made in LI-words than in HI-words. Results of Experiment 2 indicated that in naming, no imageability advantage was demonstrated for accuracy score...|$|R
