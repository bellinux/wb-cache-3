1072|134|Public
25|$|We now {{compute the}} {{derivatives}} of this <b>log</b> <b>likelihood</b> as follows.|$|E
25|$|El-Manzalawi, Y. and Honavar, V. (2009). MICCLLR: Multiple-Instance Learning using Class Conditional <b>Log</b> <b>Likelihood</b> Ratio. In: Proceedings of the 12th International Conference on Discovery Science (DS 2009). Springer-Verlag Lecture Notes in Computer Science Vol. 5808, pp.80–91, Berlin: Springer.|$|E
25|$|An {{important}} application {{where such}} a (<b>log)</b> <b>likelihood</b> of the observations (given the filter parameters) is used is multi-target tracking. For example, consider an object tracking scenario where {{a stream of}} observations is the input, however, it is unknown how many objects are in the scene (or, the number of objects is known but is greater than one). In such a scenario, it can be unknown apriori which observations/measurements were generated by which object. A multiple hypothesis tracker (MHT) typically will form different track association hypotheses, where each hypothesis {{can be viewed as}} a Kalman filter (in the linear Gaussian case) with a specific set of parameters associated with the hypothesized object. Thus, it is important to compute the likelihood of the observations for the different hypotheses under consideration, such that the most-likely one can be found.|$|E
30|$|The feature {{extractor}} {{from the}} AUDIMUS and the BUT phoneme recognizers [76] outputs <b>log</b> <b>likelihoods</b> instead of PLLR features.|$|R
3000|$|The {{gradients}} of the <b>log</b> <b>likelihoods</b> in this expression {{depends on}} (12) because the HMM {{states of the}} path ([...] [...]...|$|R
3000|$|... are the <b>log</b> <b>likelihoods</b> at child yes {{and child}} no, respectively, when the {{question}} [...] "Is the gender male?" [...] is asked.|$|R
2500|$|The normal <b>log</b> <b>likelihood</b> at {{its maximum}} takes a {{particularly}} simple form: ...|$|E
2500|$|Similarly we {{differentiate}} the <b>log</b> <b>likelihood</b> {{with respect to}} σ and equate to zero: ...|$|E
2500|$|... to {{the data}} , where the {{coefficient}} [...] is included {{to ensure that the}} distribution is normalized. [...] Given a choice for , the <b>log</b> <b>likelihood</b> function becomes: ...|$|E
40|$|Motivation: Most scoring {{functions}} used {{in protein}} fold recognition employ two-body (pseudo) potential energies. The use of higher-order terms may improve {{the performance of}} current algorithms. Methods: Proteins are represented by the side chain centroids of amino acids. Delaunay tessellation of this representation defines all sets of nearest neighbor quadruplets of amino acids. Four-body contact scoring function (<b>log</b> <b>likelihoods</b> of residue quadruplet compositions) is derived by the analysis of a diverse set of proteins with known structures. A test protein {{is characterized by the}} total score calculated as the sum of the individual <b>log</b> <b>likelihoods</b> of composing amino acid quadruplets. Results: The scoring function distinguishes native from partially unfolded or deliberately misfolded structures. It also discriminates between pre- and post-transition state and native structures in the folding simulations trajectory of Chymotrypsin Inhibitor 2 (CI 2). Availability: All codes are written in C/C++. Programs are available from the authors on request...|$|R
40|$|In {{this paper}} we study the {{probabilistic}} {{properties of the}} posteriors in a speech recognition system that uses a deep neural network (DNN) for acoustic modeling. We do this by reducing Kaldi's DNN shared pdf-id posteriors to phone likelihoods, and using test set forced alignments to evaluate these using a calibration sensitive metric. Individual frame posteriors are in principle well-calibrated, because the DNN is trained using cross entropy as the objective function, which is a proper scoring rule. When entire phones are assessed, we observe that {{it is best to}} average the <b>log</b> <b>likelihoods</b> over the duration of the phone. Further scaling of the average <b>log</b> <b>likelihoods</b> by the logarithm of the duration slightly improves the calibration, and this improvement is retained when tested on independent test data. Comment: Rejected by Interspeech 2016. I would love to include the reviews, but there is no space for that here (400 characters...|$|R
40|$|We {{introduce}} {{an extension}} to annealed importance sam-pling that uses Hamiltonian dynamics to rapidly estimate normalization constants. We demonstrate this method by computing <b>log</b> <b>likelihoods</b> in directed and undirected probabilistic image models. We compare the perfor-mance of linear generative models with both Gaussian and Laplace priors, product of experts models with Laplace and Student’s t experts, the mc-RBM, and a bilinear gen-erative model. We provide code to compare additional models. ...|$|R
2500|$|In the Fisher {{information}} matrix, and {{the curvature}} of the <b>log</b> <b>likelihood</b> function, the logarithm of the geometric variance of the reflected variable (1−X) and the logarithm of the geometric covariance between X and (1−X) appear: ...|$|E
2500|$|These {{logarithmic}} variances and covariance are {{the elements}} of the Fisher information matrix for the beta distribution. [...] They are also a measure of the curvature of the <b>log</b> <b>likelihood</b> function (see section on Maximum likelihood estimation).|$|E
2500|$|Since the {{logarithm}} function {{itself is}} a continuous strictly increasing function over {{the range of the}} likelihood, the values which maximize the likelihood will also maximize its logarithm (The likelihood's logarithm is not strictly increasing). This <b>log</b> <b>likelihood</b> can be written as follows: ...|$|E
50|$|Log-linear {{models are}} {{especially}} convenient for their interpretation. A log-linear model {{can provide a}} much more compact representation for many distributions, especially when variables have large domains. They are convenient too because their negative <b>log</b> <b>likelihoods</b> are convex. Unfortunately, though {{the likelihood of a}} logistic Markov network is convex, evaluating the likelihood or gradient of the likelihood of a model requires inference in the model, which is generally computationally infeasible (see 'Inference' below).|$|R
40|$|This report {{analyzes}} {{the effects of}} merger and acquisition activity on urban hierarchy. This paper shows that firms from larger metropolitan areas are more often the predator in a merger and acquisition transaction. The larger the metropolitan area the stronger these tendencies. The analytical tool of logistic regression {{is used to calculate}} the <b>log</b> <b>likelihoods</b> and odds ratios. The paper concludes with an examination of the possible adverse complications resulting from the M 2 ̆ 6 A trend...|$|R
40|$|The {{connection}} between {{large and small}} deviation results for the signed square root statistic R is studied, both for likelihoods and for likelihood-like criterion functions. We show that if p − 1 Barlett identities are satisfied to first order, but the pth identity is violated to this order, then cum q�R � =O�n −q/ 2 � for 3 ≤ q<p, whereas cum p�R � =κ pn −�p− 2 �/ 2 + O�n −p/ 2 �. We also show that the large deviation behavior of R {{is determined by the}} values of p and κ p. The latter result is also valid for more general statistics. Affine (additive and/or multiplicative) correction to R and R 2 are special cases corresponding to p = 3 and 4. The cumulant behavior of R gives a way of characterizing the extent to which R-statistics derived from criterion functions other than <b>log</b> <b>likelihoods</b> can be expected to behave like ones derived from true <b>log</b> <b>likelihoods,</b> by looking at the number of Bartlett identities that are satisfied. Empirical and nonparametric survival analysis type likelihoods are analyzed from this perspective via the device of “dual criterion functions. ” 1. Introduction. Wha...|$|R
2500|$|This maximum <b>log</b> <b>likelihood</b> can {{be shown}} to be the same for more general least squares, even for {{non-linear}} least squares. [...] This is often used in determining likelihood-based approximate confidence intervals and confidence regions, which are generally more accurate than those using the asymptotic normality discussed above.|$|E
2500|$|As it is {{also the}} case for maximum {{likelihood}} estimates for the gamma distribution, the maximum likelihood estimates for the beta distribution do not have a general closed form solution for arbitrary values of the shape parameters. If X1, ..., XN are independent random variables each having a beta distribution, the joint <b>log</b> <b>likelihood</b> function for N iid observations is: ...|$|E
2500|$|The same {{relationship}} {{occurs in}} many other rankings unrelated to language, such as the population ranks of cities in various countries, corporation sizes, income rankings, ranks of number of people watching the same TV channel, and so on. The appearance of the distribution in rankings of cities by population was first noticed by Felix Auerbach in 1913. Empirically, a data set can be tested to see whether Zipf's law applies by checking the goodness of fit of an empirical distribution to the hypothesized power law distribution with a Kolmogorov-Smirnov test, and then comparing the (<b>log)</b> <b>likelihood</b> ratio of the power law distribution to alternative distributions like an exponential distribution or lognormal distribution. When Zipf's law is checked for cities, a better fit has been found with exponent s = 1.07; i.e. the [...] largest settlement is [...] {{the size of the}} largest settlement. While Zipf's law holds for the upper tail of the distribution, the entire distribution of cities is log-normal and follows Gibrat's law. Both laws are consistent because a log-normal tail can typically not be distinguished from a Pareto (Zipf) tail.|$|E
40|$|When {{modelling}} code-switched speech (utterances {{that contain}} {{a mixture of}} languages), the embedded language often contains phones {{not found in the}} matrix language. These are typically dealt with by either extending the phone set or mapping each phone to a matrix language counterpart. We use acoustic <b>log</b> <b>likelihoods</b> to assist us in identifying the optimal mapping strategy at a context-dependent level (that is, at triphone, rather than monophone level) and obtain new insights in the way English/Sepedi code-switched vowels are produce...|$|R
40|$|We {{obtain a}} pseudo-partial {{likelihood}} for proportional hazards models with biased-sampling data by embedding the biased-sampling data into left-truncated data. The <b>log</b> pseudo-partial <b>likelihood</b> of the biased-sampling data is {{the expectation of}} the <b>log</b> partial <b>likelihood</b> of the left-truncated data conditioned on the observed data. In addition, asymptotic properties of the estimator that maximize the pseudo-partial likelihood are derived. Applications to length-biased data, biased samples with right censoring and proportional hazards models with missing covariates are discussed. Copyright 2009, Oxford University Press. ...|$|R
30|$|In {{likelihood}} tests, {{the relative}} performance of two models is expressed as {{the difference of}} their <b>log</b> <b>likelihoods</b> per earthquake (Equation 2). If the total expected rates are the same in both models, this {{is equivalent to the}} average vertical distance to the diagonal in Figure 6 c. In contrast to Aki’s probability gain (Figure 6 b), this averaging gives the same weight to all rates. As a consequence, positive distances at high rates and negative distances at low rates cancel each other out.|$|R
5000|$|The full {{conditional}} <b>log</b> <b>likelihood</b> is then {{simply the}} sum of the log likelihoods for each strata. The estimator is then defined as the [...] that maximizes the conditional <b>log</b> <b>likelihood.</b>|$|E
5000|$|Conditional {{logistic}} regression {{is available in}} R as the function [...] in the survival package. It is in the survival package because the <b>log</b> <b>likelihood</b> of a conditional logistic model {{is the same as}} the <b>log</b> <b>likelihood</b> of a Cox model with a particular data structure.|$|E
5000|$|Thus, the Fisher {{information}} is the negative of {{the expectation of}} the second derivative {{with respect to the}} parameter α of the <b>log</b> <b>likelihood</b> function. Therefore, Fisher {{information is}} a measure of the curvature of the <b>log</b> <b>likelihood</b> function of α. A low curvature (and therefore high radius of curvature), flatter <b>log</b> <b>likelihood</b> function curve has low Fisher information; while a <b>log</b> <b>likelihood</b> function curve with large curvature (and therefore low radius of curvature) has high Fisher information. When the Fisher information matrix is computed at the evaluates of the parameters ("the observed Fisher information matrix") it is equivalent to the replacement of the true <b>log</b> <b>likelihood</b> surface by a Taylor's series approximation, taken as far as the quadratic terms. [...] The word information, in the context of Fisher information, refers to information about the parameters. Information such as: estimation, sufficiency and properties of variances of estimators. The Cramér-Rao bound states that the inverse of the Fisher information is a lower bound on the variance of any estimator of a parameter α: ...|$|E
3000|$|... where {{superscript}} n is {{an index}} defined {{for the number}} of training utterances. It should be noted that in order to obtain the above likelihood increase expression, the following simplifying assumptions have to be made [34]: 1 - The values of occupation probabilities are assumed to be fixed during the clustering procedure [34]. 2 - The overall likelihood measure is supposed to be approximated by a simple average of the <b>log</b> <b>likelihoods</b> weighted by the posterior probabilities [34]. These assumptions make the calculation of [...]...|$|R
40|$|This article {{discusses}} {{estimates of}} variance for two-stage models. We present the sandwich estimate of variance {{as an alternative}} to the Murphy-Topel estimate. The sandwich estimator has a simple formula that is similar to the formula for the Murphy ÐTopel estimator,and the two estimators are asymptotically equal when the assumed model distributions are true. The advantages of the sandwich estimate of variance are that it may be calculated for the complete parameter vector, and that it requires estimating equations instead of fully specified <b>log</b> <b>likelihoods.</b> Copyright 2002 by Stata Corporation. robust variance estimator,Murphy-Topel estimator,two-stage estimation,estimating equation...|$|R
40|$|We {{study the}} problem of {{building}} generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary con-tribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih & Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive <b>log</b> <b>likelihoods</b> on held-out data. 1...|$|R
5000|$|The <b>log</b> <b>likelihood</b> {{function}} has {{the following}} analytical expression: ...|$|E
5000|$|It {{is common}} {{practice}} {{to use the}} <b>log</b> <b>likelihood,</b> because this is easier to evaluate. As the logarithm is a monotonic function, the [...] that maximizes the function [...] also maximizes its logarithm [...] This allows us to take the logarithm of equation above, which yields the <b>log</b> <b>likelihood</b> function ...|$|E
50|$|We now {{compute the}} {{derivatives}} of this <b>log</b> <b>likelihood</b> as follows.|$|E
40|$|In this chapter, {{we discuss}} recent {{advances}} {{in the field of}} Bayesian model testing and focus on methods that aim at either estimating (<b>log)</b> marginal <b>likelihoods</b> or at directly estimating (log) Bayes factors. We start by introducing several of the most popular (<b>log)</b> marginal <b>likelihood</b> estimators, which are attractive from a computational perspective. Because these estimators have recently been shown to perform poorly, we discuss computationally more demanding, but also more accurate path sampling approaches {{that can be used to}} either estimate (<b>log)</b> marginal <b>likelihoods</b> for di↵erent models, but also to directly estimate (log) Bayes factors between two competing models. For a specific class of evolutionary models, i. e., the relaxed molecular clock models, we also discuss how such methods compare to specific Bayesian model averaging approaches that allow constructing a classifier to approximate (log) Bayes factors between the models in the candidate model set. To demonstrate their practical use, we apply the presented techniques in a simulation study on relaxed molecular clocks and in a demographic model selection study that focuses on an HIV- 1 data set. status: publishe...|$|R
40|$|Abstract: This paper {{proposes a}} quasi-maximum {{likelihood}} framework for estimating nonlinear models with continuous or discrete endogenous explanatory variables. Both joint and two-step estimation procedures are considered. The joint estimation procedure {{can be viewed}} as quasi-limited information maximum likelihood, as {{one or both of the}} <b>log</b> <b>likelihoods</b> used may be misspecified. The two-step control function approach is computationally simple and leads to straightforward tests of endogeneity. In the case of discrete endogenous explanatory variables, I argue that the control function approach can be applied to generalized residuals to obtain average partial effects. The general results are applied to nonlinear models for fractional and nonnegative responses...|$|R
40|$|We {{consider}} the finite sample {{properties of the}} regularized high-dimensional Cox regression via lasso. Existing literature focuses on linear models or generalized linear models with Lipschitz loss functions, where the empirical risk functions are the summations of independent and identically distributed (iid) losses. The summands in the negative <b>log</b> partial <b>likelihood</b> function for censored survival data, however, are neither iid nor Lipschitz. We first approximate the negative <b>log</b> partial <b>likelihood</b> function by a sum of iid non-Lipschitz terms, then derive the non-asymptotic oracle inequalities for the lasso penalized Cox regression using pointwise arguments to tackle the difficulty caused {{by the lack of}} iid and Lipschitz property. Comment: 18 page...|$|R
