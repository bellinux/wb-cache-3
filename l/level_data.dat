6153|10000|Public
25|$|Studies {{have shown}} {{that this can be}} {{significantly}} reduced while also boosting crop yield by draining the paddies to allow the soil to aerate to interrupt methane production. Studies have also shown the variability in assessment of methane emission using local, regional and global factors and calling for better inventorisation based on micro <b>level</b> <b>data.</b>|$|E
25|$|The program {{presents}} students level reports {{designed to}} enable parents {{to see their}} child's progress {{over the course of}} their schooling life, and help teachers to improve individual learning opportunities for their students. Students and school <b>level</b> <b>data</b> are also provided to the appropriate school system on the understanding that they can be used to target specific supports and resources to schools that need them most. Teachers and schools use this information, in conjunction with other information, to determine how well their students are performing and to identify any areas of need requiring assistance.|$|E
2500|$|Due to {{frustration}} with TEPCO and the Japanese government [...] "providing differing, confusing, {{and at times}} contradictory, information on critical health issues" [...] a citizen's group called [...] "Safecast" [...] recorded detailed radiation <b>level</b> <b>data</b> in Japan using off-the-shelf Geiger counter equipment.|$|E
50|$|Qatar KMCC {{is one of}} {{the most}} highest gross charity doing every year (showing as per {{different}} <b>levels</b> <b>Data</b> collection up to 2016).|$|R
40|$|This paper {{introduces}} a novel 2 -stage classification system with stacking and genetic algorithm (GA) based feature selection. Specifically, <b>Level</b> 1 <b>data</b> is first generated by stacking {{on the original}} <b>data</b> (called <b>Level</b> 0 <b>data)</b> with base classifiers. <b>Level</b> 1 <b>data</b> is then classified by a second classifier (denoted by C) with feature selection using GA. The advantage of applying GA on <b>Level</b> 1 <b>data</b> {{is that it has}} lower dimension and is more uniformity than <b>Level</b> 0 <b>data.</b> We conduct experiments on both 18 UCI data files and CLEF 2009 medical image database to demonstrate superior performance of our model in comparison with several popular combining algorithms. Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|R
40|$|The first Earth Observing System (EOS) {{satellites}} {{was launched}} in 1998 and generated massive amounts of atmospheric data in both space and time. We explore the statistical issues relating to (optimal) processing of the resulting <b>Level</b> 2 <b>data.</b> We consider an approach to constructing <b>Level</b> 3 <b>data</b> products from <b>Level</b> 2 <b>data</b> that uses the spatio-temporal dependence of the data. We discuss the impact of global-grid-system choice and of spatial resolution on the error characteristics of the <b>Level</b> 3 <b>data</b> product. Data from the Total Ozone Mapping Spectromoter (TOMS) {{will be used for}} illustration...|$|R
2500|$|Continuing {{extensive}} sea <b>level</b> <b>data</b> collection by Australia's (CSIRO) is {{summarized in}} its finding of {{mean sea level}} trend to be 3.2mm/yr. [...] As of 2003 the National Tidal Centre of the Bureau of Meteorology managed 32 tide gauges covering the entire Australian coastline, with some measurements available starting in 1880.|$|E
2500|$|Ice {{thickness}} histories {{are useful}} {{in the study of}} paleoclimatology, glaciology and paleo-oceanography. Ice thickness histories are traditionally deduced from the three types of information: First, the sea <b>level</b> <b>data</b> at stable sites far away from the centers of deglaciation give an eastimate of how much water entered the oceans or equivalently how much ice was locked up at glacial maximum. Secondly, the location and dates of terminal moraines tell us the areal extent and retreat of past ice sheets. Physics of glaciers gives us the theoretical profile of ice sheets at equilibrium, it also says that the thickness and horizontal extent of [...] ice sheets are closely related to the basal condition of the ice sheets. Thus the volume of ice locked up is proportional to their instantaneous area. Finally, the heights of ancient beaches in the sea <b>level</b> <b>data</b> and observed land uplift rates (e.g. from GPS or VLBI) can be used to constrain local ice thickness. A popular ice model deduced this way is the ICE5G model. Because the response of the Earth to changes in ice height is slow, it cannot record rapid fluctuation or surges of ice sheets, thus the ice sheet profiles deduced this way only gives the [...] "average height" [...] over a thousand years or so.|$|E
2500|$|Parallel {{computing}} {{is a form}} of computation {{in which}} many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved [...] "in parallel". There are several different forms of parallel computing: bit-level, instruction <b>level,</b> <b>data,</b> and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.|$|E
5000|$|Data can be browsed and {{retrieved}} {{in a variety}} of formats and levels of processing, with four general levels from unprocessed to modeled output. Level 0 is unprocessed data that is not usually provided to users. <b>Level</b> 1 <b>data</b> are reconstructed but either unprocessed or minimally processed. <b>Level</b> 2 <b>data</b> contain derived geophysical variables, though are not on a uniform space/time grid. <b>Level</b> 3 <b>data</b> contain derived geophysical variables binned or mapped to a uniform grid. Lastly, <b>Level</b> 4 <b>data</b> contain modeled or derived variables such as ocean primary productivity [...]|$|R
30|$|Classification of {{imbalanced}} {{data sets}} is recognized by numerous available techniques working at dissimilar levels. They are broadly considered into three <b>levels</b> viz. <b>data</b> <b>level,</b> procedure level and cost-sensitive <b>level</b> [7, 14]. <b>Data</b> <b>level</b> works with updating {{the size of}} the data sets. The predominant techniques at procedure level work with the processes to manage imbalanced Big Data sets. The cost-sensitive technique is a mixer of both techniques viz. <b>data</b> <b>level</b> and procedure level. The techniques discussed in this paper deal with the <b>data</b> <b>level</b> technique. The <b>data</b> <b>level</b> technique is categorized into three types: Undersampling, Over_sampling and Hybrid technique [7, 14]. Over_sampling may incline to reproduce noisy data whereas undersampling might lose the useful data. The easiest way to deal with under_over sampling is random approach [18]. Over_sampling results show extra advantages than the results of undersampling techniques. The recommended techniques work basically with over_sampling approach.|$|R
30|$|For {{the enzyme}} {{characterization}} and the biobleaching study, there were separated the filtrates that more produced enzymes. And the cellulase activity of theses filtrates was determined {{and did not}} detect significant <b>levels</b> (<b>data</b> not shown).|$|R
2500|$|Due to {{frustration}} with TEPCO and the Japanese government [...] "providing differing, confusing, {{and at times}} contradictory, information on critical health issues" [...] a citizen's group called [...] "Safecast" [...] recorded detailed radiation <b>level</b> <b>data</b> in Japan. The Japanese government [...] "does not consider nongovernment readings to be authentic". The group uses off-the-shelf Geiger counter equipment. A simple Geiger counter is a contamination meter and not a dose rate meter. The response differs too much between different radioisotopes to permit a simple GM tube for dose rate measurements when more than one radioisotope is present. A thin metal shield is needed around a GM tube to provide energy compensation to enable it {{to be used for}} dose rate measurements. For gamma emitters either an ionization chamber, a gamma spectrometer or an energy compensated GM tube are required. Members of the Air Monitoring station facility at the Department of Nuclear Engineering at the University of Berkeley, California have tested many environmental samples in Northern California.|$|E
50|$|The {{warehouse}} {{data service}} {{is responsible for}} the storage of package <b>level</b> <b>data</b> and distribution <b>level</b> <b>data.</b>|$|E
5000|$|Phrase {{completion}} {{scales are}} {{a type of}} psychometric scale used in questionnaires. Developed {{in response to the}} problems associated with Likert scales, Phrase completions are concise, unidimensional measures that tap ordinal <b>level</b> <b>data</b> in a manner that approximates interval <b>level</b> <b>data.</b>|$|E
50|$|In {{the course}} of {{developing}} a set of <b>levelled</b> <b>data</b> flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the transaction data in the data model.|$|R
25|$|Level 1 (regional) {{represents}} a second regional or local <b>level</b> where <b>data</b> is placed after <b>Level</b> 0 <b>data</b> has been submitted for aggregate processing. For example, New York State BOCES Regional Information Center (RIC) or large city level.|$|R
50|$|Some {{researchers}} {{prefer to}} use their own retrieval algorithms, which process <b>Level</b> 1 <b>data,</b> while others use directly the IASI <b>Level</b> 2 <b>data.</b> Multiple algorithms exist to produce <b>Level</b> 2 <b>data,</b> which differ in their assumptions and formulation and will therefore have different strengths and weaknesses (which can be investigated by intercomparison studies). The choice of algorithm is guided by knowledge of these limitations, the resources available and the specific features of the atmosphere that wish to be investigated.|$|R
5000|$|The {{response}} categories {{represent an}} ordinal level of measurement. Ordinal <b>level</b> <b>data,</b> however, varies {{in terms of}} how closely it approximates interval <b>level</b> <b>data.</b> By using a numerical continuum as the response key instead of sentiments that reflect intensity of agreement, respondents may be able to quantify their responses in more equal units.|$|E
50|$|Primary <b>level</b> <b>data</b> - an {{electronic}} {{form of the}} original patent.|$|E
5000|$|... current water <b>level</b> <b>data</b> (gauge measurements) for the Weser and Ems ...|$|E
40|$|To protect {{sensitive}} {{information in a}} cross tabulated table, {{it is a common}} practice to suppress some of the cells in the table. This paper investigates four <b>levels</b> of <b>data</b> security of a two-dimensional table concerning the effectiveness of this practice. These four <b>levels</b> of <b>data</b> security protect the information contained in, respectively, individual cells, individual rows and columns, several rows or columns as a whole, and a table as a whole. The paper presents efficient algorithms and NP-completeness results for testing and achieving these four <b>levels</b> of <b>data</b> security. All these complexity results are obtained by means of fundamental equivalences between the four <b>levels</b> of <b>data</b> security of a table and four types of connectivity of a graph constructed from that table...|$|R
40|$|The {{mainstay}} of many scientific experiments is the factorial design. These comprise {{a number of}} experimental factors which are each expressed {{over a number of}} <b>levels.</b> <b>Data</b> are collected for each factor/level combination and then analysed using Analysis of Variance (ANOVA). The ANOVA uses F-tests to examine...|$|R
40|$|This book expounds the {{principle}} and related applications of nonlinear {{principal component analysis}} (PCA), which is useful method to analyze mixed measurement <b>levels</b> <b>data.</b> In the part dealing with {{the principle}}, after a brief introduction of ordinary PCA, a PCA for categorical data (nominal and ordinal) is introduced as nonlinear PCA, in which an optimal scaling technique is used to quantify the categorical variables. The alternating least squares (ALS) is the main algorithm in the method. Multiple correspondence analysis (MCA), a special case of nonlinear PCA, is also introduced. All formulations in these methods are integrated {{in the same manner}} as matrix operations. Because any measurement <b>levels</b> <b>data</b> can be treated consistently as numerical data and ALS is a very powerful tool for estimations, the methods can be utilized in a variety of fields such as biometrics, econometrics, psychometrics, and sociology. In the applications part of the book, four applications are introduced: variable selection for mixed measurement <b>levels</b> <b>data,</b> sparse MCA, joint dimension reduction and clustering methods for categorical data, and acceleration of ALS computation. The variable selection methods in PCA that originally were developed for numerical data can be applied to any types of measurement levels by using nonlinear PCA. Sparseness and joint dimension reduction and clustering for nonlinear data, the results of recent studies, are extensions obtained by the same matrix operations in nonlinear PCA. Finally, an acceleration algorithm is proposed to reduce the problem of computational cost in the ALS iteration in nonlinear multivariate methods. This book thus presents the usefulness of nonlinear PCA which can be applied to different measurement <b>levels</b> <b>data</b> in diverse fields. As well, it covers the latest topics including the extension of the traditional statistical method, newly proposed nonlinear methods, and computational efficiency in the methods...|$|R
5000|$|For Wales, the Welsh Government holds pupil <b>level</b> <b>data</b> back to 2004.|$|E
5000|$|Organizes data at {{the micro}} <b>level,</b> <b>data</b> models, {{for a new}} application.|$|E
50|$|Players in bold have {{represented}} Latvia on senior <b>level.</b> <b>Data</b> as per LFF.|$|E
5000|$|... #Subtitle <b>level</b> 2: <b>Data</b> {{assimilation}} as statistical estimation ...|$|R
5000|$|... #Subtitle <b>level</b> 2: <b>Data</b> collection, curation, and {{dissemination}} ...|$|R
5000|$|... #Subtitle <b>level</b> 2: <b>Data</b> Confirming The Turnover-pulse Hypothesis ...|$|R
50|$|PYLL can be {{calculated}} using individual <b>level</b> <b>data</b> or using age grouped data.|$|E
5000|$|Reduce {{cognitive}} load by bringing together lower <b>level</b> <b>data</b> into a higher-level summation.|$|E
5000|$|World Bank- Agricultural Wages Dis-aggregation of Regional <b>Level</b> <b>Data.</b> A {{quantitative}} analysis (1985-86) ...|$|E
5000|$|... #Subtitle <b>level</b> 2: <b>Data</b> storage, {{management}} and accessibility ...|$|R
5000|$|... #Subtitle <b>level</b> 2: <b>Data</b> Validation and Certification Requests ...|$|R
5000|$|... #Subtitle <b>level</b> 3: <b>Data</b> acquisition, {{normalization}} and cleansing ...|$|R
