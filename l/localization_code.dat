1|62|Public
40|$|Latent Dirichlet Allocation is a {{generative}} technique, {{the application}} of which has recently gained traction in software engineering research. A particular focus has been {{the question of whether}} topic models, generated with a code base as input, can be used to support development activities. Out of this research has come a range of promising approaches in fault <b>localization,</b> <b>code</b> comprehension, and feature location, among others. An essential step in using LDA is choosing a configuration of parameters to use in the underlying algorithm. The values chosen for the parameters k, alpha, and beta, determine how many topics the algorithm produces, the distribution of topics over documents, and the distribution of topics over terms, respectively. This determines how well a topic model will fit its purpose. In the typical case, it is necessary to experiment with multiple different configurations to find a topic model that performs best. An essential step in using LDA is choosing a configuration of parameters to use in the underlying algorithm. The values chosen for the parameters k, alpha, and beta, determine how many topics the algorithm produces, the distribution of topics over documents, and the distribution of topics over terms, respectively. This determines how well a topic model will fit its purpose. In the typical case, it is necessary to experiment with multiple different configurations to find a topic model that performs best. This dissertation explores how LDA's non-determinism impacts the process of selecting a configuration of parameters. To date, the research literature has largely been silent on this issue, yet knowing the severity of the impact is crucial because: (1) not knowing the effect makes replicability of results difficult, given that a published k, alpha, and beta; may lead to different results if someone regenerates the corresponding topic model, and (2) knowing the extent of the effect has implications for how we should go about selecting a best k, alpha, and beta. This dissertation makes two primary contributions. First, it provides an assessment {{of the impact of the}} non-determinism of LDA, both in terms of how much variation it produces in the models as well in terms of how its impact is severe. Second, it introduces a new process that leads to the selection of values of the parameters that is much more stable in terms of the resulting topic models, when these parameters are used repeatedly...|$|E
40|$|The gene SegE of phage T 4 codes the sitespecific endodesoxyribonuclease. For {{the first}} time it has been shown, that the gene of the outintron and outintein <b>localization,</b> <b>coding</b> the sitesspecific endonuclease, is able to be {{in the role of the}} mobile genetic element. The strain-producer of {{proteins}} SegE of phage T 4 has been obtained, and the method of cleaning of this ferment has been developedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
3000|$|... [2008 b]). For {{single color}} data, all <b>localizations</b> are <b>coded</b> {{with the same}} color. Multi-color coding based on STORM imaging with {{multiple}} activator dyes is also possible (Bates et al., [...]...|$|R
3000|$|... [2007]). In this approach, <b>localizations</b> are color <b>coded</b> {{based on}} when they appear in a cycle {{consisting}} of “activation” and “imaging” frames (Bates et al., [...]...|$|R
40|$|Barn owls {{are capable}} of great {{accuracy}} in detecting the interaural time differences (ITDs) that underlie azimuthal sound localization. They compute ITDs in a circuit in nucleus laminaris (NL) that is reorganized with respect to birds like the chicken. The events {{that lead to the}} reorganization of the barn owl NL take place during embryonic development, shortly after the cochlear and laminaris nuclei have differentiated morphologically. At first the developing owl’s auditory brainstem exhibits morphology reminiscent of that of the developing chicken. Later, the two systems diverge, and the owl’s brainstem auditory nuclei undergo a secondary morphogenetic phase during which NL dendrites retract, the laminar organization is lost, and synapses are redistributed. These events lead to the restructuring of the ITD coding circuit and the consequent reorganization of the hindbrain map of ITDs and azimuthal space. Key words: avian development; morphogenesis; auditory; laminaris; evolution; interaural time difference The barn owl’s (hereafter referred as owl) auditory system uses interaural time differences (ITDs) and interaural level differences (ILDs) for sound <b>localization.</b> <b>Coding</b> of ITDs relies on coincidence detection by nucleus laminaris (NL) neurons, which are sensitive to binaural ongoing phase disparities (Moiseff and Konishi, 1983). The circuit responsible for the computation of ITDs in birds has been studied extensively in chickens and owl...|$|R
30|$|Many {{state-of-the-art}} bug localization {{techniques are}} based on information retrieval. Information retrieval (IR) consists of finding documents within a collection that match a search query (Manning et al. 2008). When applying IR to bug <b>localization,</b> source <b>code</b> files become the collection of documents, and the bug report represents the query. Then, the task of finding buggy files is reduced to the IR problem of determining the relevance of a document to a query. The relevance is determined by preprocessing the query and the set of documents and then calculating the similarity between each document and the query.|$|R
40|$|We {{anticipate}} the first direct detections of gravitational waves (GWs) with Advanced LIGO and Virgo later this decade. Though this groundbreaking technical achievement will be its own reward, a still greater prize could be observations of compact binary mergers in both gravitational and electromagnetic channels simultaneously. During Advanced LIGO and Virgo's {{first two years}} of operation, 2015 through 2016, we expect the global GW detector array to improve in sensitivity and livetime and expand from two to three detectors. We model the detection rate and the sky localization accuracy for binary neutron star (BNS) mergers across this transition. We have analyzed a large, astrophysically motivated source population using real-time detection and sky <b>localization</b> <b>codes</b> and higher-latency parameter estimation codes that have been expressly built for operation in the Advanced LIGO/Virgo era. We show that for most BNS events the rapid sky localization, available about a minute after a detection, is as accurate as the full parameter estimation. We demonstrate that Advanced Virgo will {{play an important role in}} sky localization, even though it is anticipated to come online with only one-third as much sensitivity as the Advanced LIGO detectors. We find that the median 90 % confidence region shrinks from ~ 500 square degrees in 2015 to ~ 200 square degrees in 2016. A few distinct scenarios for the first LIGO/Virgo detections emerge from our simulations. Comment: 17 pages, 11 figures, 5 tables. For accompanying data, see [URL]...|$|R
40|$|The {{relationship}} between physicochemical properties of odor molecules and perceived odor quality is arguably {{one of the}} most important issues in olfaction and the rules governing this relationship remain unknown. Any given odor molecule will stimulate more than one type of receptor in the nose, perhaps hundreds, and this stimulation reflects itself in the neural code of the olfactory nervous system. We present a method to investigate neural coding at the glomerular level of the olfactory bulb, the first relay for olfactory processing in the brain. Our results give insights into <b>localization</b> of <b>coding</b> sites, relevance of odorant properties for information processing, and the size of coding zones. ...|$|R
40|$|Abstract—Binaural Cue Coding (BCC) is {{a method}} for {{multichannel}} spatial rendering based on one down-mixed audio channel and BCC side information. The BCC side information has a low data rate and it {{is derived from the}} multichannel encoder input signal. A natural application of BCC is multichannel audio data rate reduction since only a single down-mixed audio channel needs to be transmitted. An alternative BCC scheme for efficient joint transmission of independent source signals supports flexible spatial rendering at the decoder. This paper (Part I) discusses the most relevant binaural perception phenomena exploited by BCC. Based on that, it presents a psychoacoustically motivated approach for designing a BCC analyzer and synthesizer. This leads to a reference implementation for analysis and synthesis of stereophonic audio signals based on a Cochlear Filter Bank. BCC synthesizer implementations based on the FFT are presented as low-complexity alternatives. A subjective audio quality assessment of these implementations shows the robust performance of BCC for critical speech and audio material. Moreover, the results suggest that the performance given by the reference synthesizer is not significantly compromised when using a low-complexity FFT-based synthesizer. The companion paper (Part II) generalizes BCC analysis and synthesis for multichannel audio and proposes complete BCC schemes including quantization and coding. Part II also describes an alternative BCC scheme with flexible rendering capability at the decoder and proposes several applications for both BCC schemes. Index Terms—Audio coding, auditory filter bank, auditory scene synthesis, binaural source <b>localization,</b> <b>coding</b> of binaural spatial cues, spatial rendering. I...|$|R
3000|$|... [2007]). For example, for a {{sequence}} of one activation frame (405 nm laser), three imaging frames (647 nm laser) followed by one activation frame (560 nm laser) and three imaging frames (647 nm laser), the localizations appearing in the frame immediately after the 405 nm activation laser are coded in one color and those appearing in the frame immediately after the 560 nm laser are coded in a different color. All other <b>localizations</b> are <b>coded</b> as “non-specific”, since they cannot be directly linked to the activation laser. Each color is displayed in a separate window, which can later be overlaid using ImageJ. The use of different activators linked to the same reporter dye is prone to color cross talk due to the non-specific or false activations as previously described (Bates et al., [...]...|$|R
40|$|This paper {{demonstrates}} {{the use of}} an integrated toolset for program understanding. By leveraging the unique capabilities of individual tools, and exploiting their power in combination, the resultant toolset is able to facilitate specific reverse engineering tasks that would otherwise be difficult or impossible. This is illustrated by applying the integrated toolset to several typical reverse engineering scenarios, including <b>code</b> <b>localization,</b> data flow analysis, pattern matching, system clustering, and visualization, using a mid-size production program as the reference system...|$|R
40|$|We {{consider}} {{the problem of}} decoding of real BCH dis-crete Fourier transform codes (RDFT) which are con-sidered for joint source channel codes to provide robust-ness against errors in communication channels. In this paper, we propose to combine the subspace based al-gorithm like MUSIC algorithm with ` 1 -norm minimiza-tion algorithm, which is promoted as a sparsity solution functional, to enhance the error <b>localization</b> of RDFT <b>codes.</b> Simulation {{results show that the}} combined al-gorithm performs better over the performances of these individual algorithms. 1...|$|R
40|$|Abstract: This work is {{dedicated}} to describing of developed algorithm for automatic reading bar code by using system that works with video information. Here described the main definition of bar coding technology and existing bar code reading systems. Developed algorithm is assigned to use in self-contained unit. FPGA Cyclone (Altera) and DSP Black Fin (Analog Devices) are computing parts of device. Using of FPGA allow to parallel work of algorithm for <b>localization</b> bar <b>code.</b> Filtering and averaging operation are performed simultaneously with acquisition of image. Algorithm work was modeling in software MathLab 6. 5 MathWork Inc. After successful modeling it was inculcate in device. The device testing indicated high reliability and quick-action (from 6 to 12 frames per second) in bar code recognition without using artificial lighting. Therein developed scanner exceeds possibilities similar scanners Opticon and OEM, that uses artificial lighting. Note: Publication language:russia...|$|R
40|$|Delta {{debugging}} tools automatically minimize failure-inducing {{input and}} enable efficient <b>localization</b> of erroneous <b>code.</b> In particular when debugging complex verification backends such as SMT solvers, delta debuggers provide an effective debugging approach where other debugging techniques are infeasible {{due to the}} input formula size. In this paper, we present ddSMT, a delta debugger for the SMT-LIB v 2 format, which supports all SMT-LIB v 2 logics and in particular handles macros and scopes defined by the commands push and pop. We introduce its architecture and describe its workflow in detail. ...|$|R
40|$|Abstract—Concern {{localization}} {{refers to}} the process of locating code units that match a particular textual description. It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and textual descriptions as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern <b>localization.</b> A <b>code</b> unit and a textual description is represented at multiple abstraction levels. Similarity of a textual description and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. Th...|$|R
40|$|Laboratory {{experiments}} {{have been performed}} to demonstrate the capabilities of a gamma-ray imaging system employing a NaI Anger camera and a rotating coded aperture mask. The mask incorporates in its design {{a new type of}} hexagonal uniformly redundant array (HURA) which is essentially antisymmetric under 60 deg rotation. The image formation techniques are described and results are presented that demonstrate the imaging capability of the system for individual and multiple point sources of gamma-ray emission. The results are compared to analytical predictions for the imaging and point source <b>localization</b> capabilities of <b>coded</b> aperture systems using continuous detectors...|$|R
40|$|Concern {{localization}} {{refers to}} the process of locating code units that match a particular textual description. It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and textual descriptions as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern <b>localization.</b> A <b>code</b> unit and a textual description is represented at multiple abstraction levels. Similarity of a textual description and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. The experiment shows that our proposed approach outperforms a baseline approach, in terms of Mean Average Precision, by up to 19. 36 %...|$|R
40|$|Recent {{techniques}} for fault <b>localization</b> leverage <b>code</b> coverage {{to address the}} high cost problem of debugging. These techniques exploit the correlations between pro-gram failures and the coverage of program entities as the clue in locating faults. Experimental evidence shows that {{the effectiveness of these}} techniques can be affected ad-versely by coincidental correctness, which occurs when a fault is executed but no failure is detected. In this paper, we propose an approach to address this problem. We re-fine code coverage of test runs using control- and data-flow patterns prescribed by different fault types. We con-jecture that this extra information, which we call context patterns, can strengthen the correlations between pro-gram failures and the coverage of faulty program entities, making it easier for fault localization techniques to locate the faults. To evaluate the proposed approach, we have conducted a mutation analysis on three real world pro-grams and cross-validated the results with real faults. The experimental results consistently show that coverage re-finement is effective in easing the coincidental correctness problem in fault localization techniques. 1 1...|$|R
40|$|Tic 20 is a polytopic protein of {{the inner}} {{envelope}} membrane of chloroplasts, and it is proposed {{to act as a}} translocation channel during chloroplast protein import. By analysing 29 sequences from diverse organisms, it was evident that Tic 20 -related proteins form two distinct clades, termed Group 1 and Group 2. The former group includes canonical Tic 20 proteins that are essential for chloroplast development, while members of the latter are of unknown function. An increased evolutionary rate, in connection with adaptation to terrestrial life, was detected in Group 1. Interestingly, the sub-cellular (genomic) <b>localization</b> of genes <b>coding</b> for Group 1 proteins differs between evolutionary lineages. Peer-reviewedPost-prin...|$|R
40|$|The {{complexity}} of modern chips is rapidly increasing. To fulfill tight time-to-market constraints, {{more and more}} blocks from previous designs are reused or third party IP blocks are licensed. However, such blocks are often only poorly documented making adjustments to the blocks a difficult task. This paper presents a technique for automatic feature localization for hardware designs. Our approach helps a developer in understanding a design by localizing parts of the code which implement a certain feature of interest. We evaluate the approach on three open source designs. For those designs, our approach yields a more precise <b>localization</b> of the <b>code</b> implementing the different features than the documentation of the design...|$|R
30|$|At present, {{the effort}} of {{deploying}} applications with vendor-specific tools across multiple PaaS cloud platforms is a non-trivial task. Developers and system operators often face the barrier of redeploying applications to other providers’ platform because tools are incompatible. However, this can be simplified using the CAMP interface common to both source and target platforms. To simplify the deployment efforts and support migration across multiple cloud platforms, CAMP defines the Platform Deployment Package (PDP). A PDP is an archive containing a plan file together with application content files such as web archives, database schemas, scripts, source <b>code,</b> <b>localization</b> bundles, icons etc. This archive {{can be used to}} move an application and its components from platform to platform, or between a development environment and an operative target platform.|$|R
40|$|In {{this paper}} we develop a signal {{processing}} algorithm for the detection, ranging and Doppler localization of moving target using Golay code as the phase coding signal. We exploit the side-lobes suppression property of Golay code, {{that occurs in}} complementary code pair, for side-lobe free target detection, range estimation and Doppler <b>localization.</b> Golay <b>code</b> based phase coding is achieved using QPSK (Quadrature Phase Shift Keying) scheme. When the transmitted signal is reflected from a moving target, its complex demodulation is carried out at the receiver. This is followed by correlation of each component of demodulated signal with {{one member of the}} complementary Golay code pair separately. The correlation results of the two channels are added together, leading to effective side-lobes suppression when phase and frequency of the demodulating carriers match with that of the received signal. The shift in the time index of the correlation peak gives the range to the target. Frequency of the demodulating carriers at which side-lobe free correlation peak occurs at the output indicates Doppler frequency corresponding to the target speed. The developed technique exhibits excellent Doppler localization, target detection and range estimation performance by overcoming the side-lobe limitations of the conventional phase coding signals in ranging applications...|$|R
40|$|Barcode {{technology}} {{is essential in}} automatic identification, and is used {{in a wide range}} of real-time applications. Different code types and applications impose special problems, so there is a continuous need for solutions with improved performance. Several methods exist for <b>code</b> <b>localization,</b> that are well characterized by accuracy and speed. Particularly, high-speed processing places need reliable automatic barcode localization, e. g. conveyor belts and automated production, where missed detections cause loss of profit. Our goal is to detect automatically, rapidly and accurately the barcode location with the help of extracted image features. We propose a new algorithm variant, that outperforms in both accuracy and efficiency other detectors found in the literature using similar ideas, and also improves on the detection performance in detecting 2 D codes compared to our previous algorithm...|$|R
40|$|This paper {{proposes a}} hybrid TOA/AOA (Time of Arrival/Angle of Arrival) -based <b>localization</b> {{algorithm}} for <b>Code</b> Division Multiple Access (CDMA) networks. The algorithm extends the Taylor Series Least Square (TS-LS) method originally developed for TOA-based systems to incorporate AOA measurements. In addition, tracking algorithms utilizing velocity and acceleration measurements are investigated. Simulation results illustrate {{that the proposed}} TOA/AOA TS-LS can provide better performance than conventional schemes in localization accuracy and in reduced likelihood of encountering non-convergence problem compared with TOA TS-LS. Tracking algorithms using the Extended and Unscented Kalman Filter (EKF and UKF) can track the objects relatively well, further decreasing the positioning error. UKF is found to provide closer tracking of the trajectory than EKF, for it truly captures the statistical mean and variance of the noises. © 2010 IEEE...|$|R
40|$|Abstract. Barcode {{technology}} {{is essential in}} automatic identification, and is used {{in a wide range}} of real-time applications. Different code types and applications impose special problems, so there is a continuous need for solutions with improved performance. Several methods exist for <b>code</b> <b>localization,</b> that are well characterized by accuracy and speed. Particularly, high-speed processing places need reliable automatic barcode localization, e. g. conveyor belts and automated production, where missed detections cause loss of profit. Our goal is to detect automatically, rapidly and accurately the barcode location with the help of extracted image features. We propose a new algorithm variant, that outperforms in both accuracy and efficiency other detectors found in the literature using similar ideas, and also improves on the detection performance in detecting 2 D codes compared to our previous algorithm. Keywords:barcode detection, morphological operations, bottom-hat filter, distance map...|$|R
40|$|This paper {{demonstrates}} {{the use of}} an integrated toolset for program understanding. By leveraging the unique capabilities of individual tools, and exploiting their power in combination, the resultant toolset is able to facilitate speci#c reverse engineering tasks that would otherwise be di#cult or impossible. This is illustrated by applying the integrated toolset to several typical reverse engineering scenarios, including <b>code</b> <b>localization,</b> data #ow analysis, pattern matching, system clustering, and visualization, using a mid-size production program as the reference system. 1 Introduction As the amount of legacy code currently in use increases, the importance of program understanding grows accordingly. Program understanding is the process of developing mental models of a software system's intended architecture, purpose, and behavior. There have been numerous research e#orts to develop tools that provide assistance during the understanding process. These tools adopt a number of diffe [...] ...|$|R
40|$|This paper {{presents}} {{a new approach}} {{in the management of}} mobile ad hoc networks. Our alternative, based on mobile agent technology, allows the design of mobile centralized server in ad hoc network, where it is not obvious to think about a centralized management, due to the absence of any administration or fixed infrastructure in these networks. The aim of this centralized approach is to provide permanent availability of services in ad hoc networks which are characterized by a distributed management. In order to evaluate the performance of the proposed approach, we apply it {{to solve the problem of}} mobile <b>code</b> <b>localization</b> in ad hoc networks. A comparative study, based upon a simulation, of centralized and distributed localization protocols in terms of messages number exchanged and response time shows that the centralized approach in a distributed form is more interesting than a totally centralized approach. Comment: 14 Pages, IJCNC Journal 201...|$|R
40|$|This paper {{deals with}} the {{application}} of a multichannel filtering-based texture segmentation method to a variety of document image processing problems: text-graphics separation, address-block location, and bar <b>code</b> <b>localization.</b> Both supervised and unsupervised methods have been used to identify regions of text or bar code in the input gray level document images. The performance of our segmentation and classification scheme is shown on a variety of document images which were scanned using a flat bed scanner. These results demonstrate the generality and effectiveness of our approach for the segmentation and classification of document images. 1 Introduction A major step in an image understanding system is the identification of "homogeneous" regions in a given image. The gray levels at individual pixels are usually not enough to perform a satisfactory segmentation of the image. Contextual information describing the spatial relationship between the gray levels of pixels in a local neighb [...] ...|$|R
40|$|This paper {{introduces}} an intelligent, rule-based {{tool set}} for the re-engineering of programs, called the Program transformation and Normalization (PROTRAN) system. One of the main objectives {{of the development of}} the PROTRAN system has been to facilitate the process of program recognition, a necessary step in automated fault <b>localization.</b> The idiosyncratic <b>code</b> written by many programmers makes program recognition a very difficult process. The PROTRAN system, built using an object oriented paradigm, eliminates many of these programming idiosyncrasies, by the recognition and transformation of certain programming fragments into an internal normalized code format. These programming fragments are defined as data objects and are normalized based on an augmented set of programming style and discourse rules developed especially for the PROTRAN system. A working prototype exists that shows the correctness and feasibility of this system. Program recognition is a relatively new area of research that makes use of artificia...|$|R
40|$|The {{built-in}} {{cameras and}} powerful processors have turned smartphones into ubiquitous barcode scanners. In smartphone-based barcode scanning, barcode localization {{is an important}} preprocessing step that quickly scans the entire camera image and passes barcode candidates to the actual decoder. This paper presents the implementation steps of a robust joint 1 D and 2 D barcode localization algorithm on the mobile GPU. The barcode probability maps are derived from the structure matrix {{and the color of}} the individual pixels. The different steps of the localization algorithm are formulated as OpenGL ES 2. 0 fragment shaders and both 1 D and 2 D barcode saliency maps are computed directly on the graph-ics hardware. The presented method can detect barcodes at various scales and orientations at 6 frames per second in HD resolution images on current generation smartphones. Index Terms — barcode, QR <b>code,</b> <b>localization,</b> smart-phone, GPGP...|$|R
40|$|Robustness to {{run-time}} faults in {{network based}} systems requires {{the ability to}} detect and repair problems when they arise in a running system. Effective fault detection and repair could be greatly enhanced by run-time fault diagnosis and localization, since it would allow the repair mechanisms to focus adaptation effort on the parts {{most in need of}} attention [CSG+ 2011]. Goals The goal of this project is to implement and evaluate (based on simulations and realistic examples) different fault localization strategies and prepare the technology for an industrial use. Description of the Task The project aims to apply statistical techniques to pinpoint the specific component in the network that has most likely caused a specific problem. To enable this fault localization the message traces between the involved components need to be analyzed and each trace sequence needs to be classified as either faulty or correct. Based on this information the component that is most often involved in faulty trace sequence (in relation to its involvement in correct trace sequences) will be the component that is most likely causing the failure. The statistical fundament for this architecture level fault localization is adapted from the fault <b>localization</b> at <b>code</b> level [AZG+ 09] and is thus build on a good theoretical basis. However, it is currently unclear how the different <b>code</b> level fault <b>localization</b> techniques perform at an architectural level and thus a thorough investigation of these techniques is needed. There could be also potential for improvement of a specific technique...|$|R
40|$|Media {{authentication}} {{is important}} in content delivery via untrusted intermediaries, such as peer-to-peer (P 2 P) file sharing. Many differently encoded versions of a media file might exist. Our previous work applied distributed source coding not only to distinguish the legitimate diversity of encoded images from tampering but also localize the tampered regions in an image already deemed to be inauthentic. An authentication decoder was supplied with a Slepian-Wolf encoded image projection as authentication data. A localization decoder required only incremental localization data beyond the authentication data since we use rate-adaptive distributed source codes. We extend the localization decoder with 1 D and 2 D spatial models to exploit the contiguity of the tampered regions. Our {{results show that the}} spatial decoders save 10 % to 17 % of authentication plus localization data size and offer greater confidence in tampering localization. Index Terms — image authentication, image tampering <b>localization,</b> distributed source <b>coding</b> 1...|$|R
40|$|In {{this paper}} 1 three {{problems}} related {{to the analysis of}} facial images are addressed: the estimation of the illuminant direction, the compensation of illumination effects and, finally, the recovery of the pose of the face, restricted to in-depth rotations. The solutions proposed for these problems rely on the use of computer graphics techniques to provide images of faces under different illumination and pose, starting from a database of frontal views under frontal illumination. Appeared as MIT AI. Memo No. 1499 1. Introduction Automated face perception (<b>localization,</b> recognition and <b>coding)</b> is now a very active research topic in the computer vision community. Among the reasons, the possibility of building applications on top of existing research {{is probably one of the}} most important. While recent results on localization and recognition open the way to automated security systems based on face identification, breakthroughs in the field of facial image coding are of practical interest [...] ...|$|R
40|$|Fracture or tearing of ductile metals is a {{pervasive}} engineering concern, yet accurate prediction {{of the critical}} conditions of fracture remains elusive. Sandia National Laboratories has been developing and implementing several new modeling methodologies to address problems in fracture, including both new physical models and new numerical schemes. The present study provides a double-blind quantitative assessment of several computational capabilities including tearing parameters embedded in a conventional finite element <b>code,</b> <b>localization</b> elements, extended finite elements (XFEM), and peridynamics. For this assessment, each of four teams reported blind predictions for three challenge problems spanning crack initiation and crack propagation. After predictions had been reported, the predictions were compared to experimentally observed behavior. The metal alloys for these three problems were aluminum alloy 2024 -T 3 and precipitation hardened stainless steel PH 13 - 8 Mo H 950. The predictive accuracies of the various methods are demonstrated, and the potential sources of error are discussed...|$|R
40|$|Nuclear {{filamentous}} actin (F-actin) {{is essential}} for nucleocapsid morphogenesis of lepidopteran nucleopoly-hedroviruses. Previously, we had demonstrated that Autographa californica multiple nucleopolyhedrovirus (AcMNPV) BV/ODV-C 42 (C 42) is involved in nuclear actin polymerization by recruiting P 78 / 83, an AcMNPV orf 9 -encoded N-WASP homology protein {{that is capable of}} activating an actin-related-protein 2 / 3 (Arp 2 / 3) complex to initiate actin polymerization, to the nucleus. To further investigate the role of C 42 in virus-induced actin polymerization, the recombinant bacmid vAcp 78 / 83 nls-gfp, with a c 42 knockout, p 78 / 83 tagged with a nuclear <b>localization</b> signal <b>coding</b> sequence, and egfp as a reporter gene {{under the control of the}} Pp 10 promoter, was constructed and transfected to Sf 9 cells. In the nuclei of vAcp 78 / 83 nls-gfp-transfected cells, polymerized F-actin filaments were absent, whereas other actin polymerization elements (i. e., P 78 / 83, G-actin, and Arp 2 / 3 complex) were present. This in vivo evidence indicated that C 42 actively participates in the nuclear actin polymerization process as a key element, besides its role in recruiting P 78 / 83 to the nucleus. In order to collect in vitro evidence for the participation of C 42 in actin polymerization, an anti-C 42 antibody was used to neutralize the viral nucleocapsid, which is capable of initiating actin polymerization in vitro. Both the kinetics of pyrene-actin polymerization and F-actin-specific staining by phalloidin indicated that anti-C 42 can signif-icantly attenuate the efficiency of F-actin formation compared to that with control antibodies. Furthermore, w...|$|R
40|$|Abstract—Software {{engineers}} organize {{source code}} into a dominant hierarchy of components and modules that may emphasize various characteristics over runtime behavior. In this way, runtime features may involve cross-cutting aspects of code from multiple components, {{and some of}} these features may be emergent in nature, rather than designed. Although source-code modularization assists software engineers to organize and find components, identifying such cross-cutting feature sets can be more difficult. This work presents a visualization that includes a static (i. e., compile-time) representation of source code that gives prominence to clusters of cooperating source-code instructions to identify dynamic (i. e., runtime) features and constituent behaviors within executions of the software. In addition, the visualization animates software executions to reveal which feature clusters are executed and in what order. The result has revealed the principal behaviors of software executions, and those behaviors were revealed to be (in some cases) cohesive, modular source-code structures and (in other cases) cross-cutting, emergent behaviors that involve multiple modules. In this paper, we describe our system (CEREBRO), envisage the uses to which it can be put, and evaluate its ability to reveal emergent runtime features and internal constituent behaviors of execution. We found that: (1) the visualization revealed emergent and commonly occuring functionalities that cross-cut the structural decomposition of the system; (2) four independent judges generally agreed in their interpretations of the code clusters, especially when informed only by our visualization; and (3) interacting with the external interface of an application while simultaneously observing the internal execution facilitated <b>localization</b> of <b>code</b> that implements the features and functionality evoked externally. I...|$|R
