5|10000|Public
5000|$|Sync Point manager {{coordinates}} logical {{units of}} work in multiple DDM servers. Two-phase commitment protocols ensure coordinated resource recovery when any <b>logical</b> <b>unit</b> <b>of</b> <b>work</b> fails.|$|E
5000|$|A {{transaction}} in SAP terminology is {{the execution}} of a program. The normal way of executing ABAP code in the SAP system is by entering a transaction code (for instance, VA01 is the transaction code for [...] "Create Sales Order"). Transactions can be called via system-defined or user-specific, role-based menus. They can also be started by entering the transaction code directly into a command field, which is present in every SAP screen. Transactions can also be invoked programmatically {{by means of the}} ABAP statements CALL TRANSACTION and LEAVE TO TRANSACTION.The general notion of a transaction is called a <b>Logical</b> <b>Unit</b> <b>of</b> <b>Work</b> (LUW) in SAP terminology. [...] and short form of transaction code is (T-code).|$|E
40|$|Abstract. It {{has been}} brought into {{attention}} that analysis of task-oriented database user sessions provides useful insight into the query behavior of database users. A database user session is a sequence of queries issued by a user (or an application) to achieve a certain task. It consists {{of one or more}} database transactions, which are in turn a sequence of operations performed as a <b>logical</b> <b>unit</b> <b>of</b> <b>work.</b> In this paper, we assume a set of session instances are already obtained, and focus on grouping these sessions into different session classes. We propose a distancebased clustering algorithm which is based on three session similarity metrics between sessions. We also show experimental results. ...|$|E
40|$|The {{notion of}} a session is {{fundamental}} in service oriented applications, as it serves to separate interactions between different instances of the same services, and to group together <b>logical</b> <b>units</b> <b>of</b> <b>work.</b> Recently, SCC has been proposed as a calculus centered around {{the concept of a}} dyadic session, where service interaction protocols and service orchestration can be conveniently expressed. In this paper we propose a generic type system to collect services' behaviors and then we fix a class of well typed processes that are guaranteed to be deadlock free. The type system is based on previous research on traditional mobile calculi, here conveniently extended and simplified thanks to the neat discipline imposed by the linguistic primitives of SCC. ...|$|R
40|$|The {{notion of}} a session is {{fundamental}} in service-oriented applications, as it serves to separate interactions between clients and different instances of the same service, and to group together <b>logical</b> <b>units</b> <b>of</b> <b>work.</b> Recently, the Service Centered Calculus (SCC) has been proposed as a process calculus designed around {{the concept of a}} dyadic session between a service side and an invoker side, where interaction protocols and service orchestration can be conveniently expressed. In this paper we propose a generic type system to collect services' behaviours and then we fix a class of well-typed processes that are guaranteed to be deadlock free, {{in the sense that they}} either diverge by invoking new service instances or reach a normal form. The type system is based on previous research on traditional mobile calculi, here conveniently extended and simplified thanks to the neat discipline imposed by the linguistic primitives of SCC...|$|R
40|$|The {{notion of}} a session is {{fundamental}} in service-oriented applications, as it serves to separate interactions between clients and different instances of the same service, and to group together <b>logical</b> <b>units</b> <b>of</b> <b>work.</b> In the area of process calculi Honda, Kubo and Vasconcelos proposed their perspective of what a session should be {{from the perspective of}} theorical foundations. They presented a calculus equipped with a notion of session types that govern the interactions between peers. This first proposal gave rise to a new research direction and to a community of researchers interested in session types and their extensions and applications. The great merit of session types is in fact to be like a classical type system, intended to describe structural properties of the data manipulated by programs. One can think of a session type as the equivalent notion of channel sorting for the π-calculus. The novelty is that well-typedness of a process implies a stronger property than any other classical type systems, namely the session safety. Session safety guarantees that at runtime any interaction inside a session will proceed without errors due to mismatching communications. Moreover, with a little additional effort, session safety implies the progress property, which in some manner prevents deadlock. Well typing of a process written in a session calculus can be easily verified at the cost to annotate certain names of the processes with session types. Here we address the problem of finding efficient procedure for checking well-typedness in absence of any type annotation or said in other words the type inference of session types. It is interesting how different notions proposed in different works on session types are used together as tools to achieve the result. At the end our study leads to establish a formal theory of session types that can be applied and transferred to various settings and formalisms. Since type inference strictly depends on a specific calculus we show the wide applicability of our result studying the problem for two particular calculi with very different mechanisms of session instantiation. Prototype implementation of the type algorithms are written in Ocaml and available at [URL]...|$|R
40|$|A {{fine-grain}} {{parallel program}} {{is one in}} which a thread is created for each <b>logical</b> <b>unit</b> <b>of</b> <b>work.</b> Fine-grain parallelism can help hide latency and balance load, which improves speedup. However, many threads packages force a user to give up efficient single-processor performance to obtain this speedup, which makes it very difficult for a programmer to obtain excellent parallel speedup relative to the sequential program. In this paper we show that when fine-grain programs contain frequent array references, they are significantly slower than the equivalent sequential program [...] -even using the most efficient threads packages. The primary obstacles to efficient single-processor performance turn out to be (1) the inability of compilers to apply a standard compiler optimization, common subexpression elimination, and (2) the use of global variables in fine-grain code, which prevents the compiler from allocating them in registers. We have designed and implemented a preprocessor for a re [...] ...|$|E
40|$|A {{transaction}} is a <b>logical</b> <b>unit</b> <b>of</b> <b>work</b> {{that includes}} {{one or more}} database access operations such as insertion, deletion, modification, and retrieval [8]. A schedule (or history) S of n transactions T 1, [...] .,Tn is an ordering of the transactions that satisfies the following two conditions: (i) the operations of Ti (i = 1, [...] .,n) in S must occur {{in the same order}} in which they appear in Ti, and (ii) operations from Tj (j 6 ¼ i) may be interleaved with Ti’s operations in S. A schedule S is serial if for every two transactions Ti and Tj that appear in S, either all operations of Ti appear before all operations of Tj, or vice versa. Otherwise, the schedule is called nonserial or concurrent. Non-serial schedules of transactions may lead to concurrency problems such as lost update, dirty read, and unrepeatable read. For instance, the lost update problem occurs whenever two transactions, while attempting to modify a data item, both read the item’s old value before either of them writes the item’s new value [2]. The simplest way for controlling concurrency is to allow only serial schedules. However, with no concurrency, database systems may make poor use of their resources and hence, be inefficient, resulting in smaller transaction execution rate for example. To broaden the class of allowable transaction schedules, serializability has been proposed as the major correctness criterion for concurrency control [7, 11]. Serializability ensures that a concurrent schedule of transactions is equivalent to some serial schedule of the same transactions [12]. While serializability has been successfully used in traditional database applications, e. g., airline reservations and banking, it has been proven to be restrictive and hardly applicable in advanced applications such as Computer- Aided Design (CAD), Computer-Aided Manufacturing (CAM), office automation, and multidatabases. These applications introduced new requirements that either prevent the use of serializability (e. g., violation of local autonomy in multidatabases) or make the use of serializability inefficient (e. g., long-running transactions in CAD/CAM applications). These limitations have motivated the introduction of more flexible correctness criteria that go beyond the traditional serializability...|$|E
40|$|In {{industrial}} and post industrial nations like Germany and the USA {{more than a}} quarter of the workforce mainly works with information. Most <b>of</b> the <b>work</b> done by these information workers is the production, supervision and dissemination of information at computer workplaces. Information workers frequently works on multiple tasks in parallel. Few guidelines regulate and structure the work process. Therefore, the successful execution <b>of</b> the <b>work</b> requires a high degree of individual planning. A common effect of ad-hoc executions of multiple tasks are memory failures: Planned activities are forgotten (prospective memory failures), or the recall <b>of</b> <b>work</b> processes' status and involved information objects fails (retrospective memory failures). The computer [...] -a multitasking machine [...] -even increases the likelihood of memory failures due to an increased number of activities executed in parallel. This dissertation investigates methods to decrease the likelihood of memory failures in information work at the computer workplace. The effort leads to the design of a tool that provides support for information work based on externalized activity data. This document is structured as follows: 1) The first part investigates information work from the perspectives of psychology, organization theory and sociology. Identified characteristics <b>of</b> information <b>work</b> relevant for this dissertation are captured in an ideal type. This includes the specification <b>of</b> the information <b>work</b> process at the computer workplace as being coordinated by interruptions and as being composed <b>of</b> <b>logical</b> <b>units</b> <b>of</b> <b>work,</b> so called knowledge actions and desktop operations. 2) The second part proposes a system design method which facilitates the analysis <b>of</b> <b>work</b> processes that can be typically observed in information work. The method seamlessly integrates into the user-centred design method. Work is modeled and analyzed in terms of so called activity system models based on activity theory and action regulation theory. System model and analysis realize two important elements of the user-centred design method: the context of use analysis and the requirement specification. The specified method is applied to the domain <b>of</b> information <b>work,</b> resulting in requirements for a tool to decrease the likelihood of memory failures in information work. 3) The third part develops methods to address memory failures in information work based on activity data. The developed methods address the requirements previously identified by applying the system design method (part 2) to the identified ideal type (part 1). The methods are implemented and evaluated in a demonstrator: a) Activity Data: A fundamental contribution to address memory failures is the collection of information about the work process. To realize this, methods to capture, analyze and organize interaction histories are developed. A core element of the process is activity mining, which is a method to identify activities in interaction histories even if the activities were interrupted during the execution process. Activity mining is modeled as a clustering problem. The proposed activity mining methods show better results than the state of the art with respect to the identification of activities. Furthermore, the proposed activity mining methods extract more details about the work execution process than the state of the art. b) Methods to Address Memory Failures: 	 Based on the extracted activity data the goal <b>of</b> this <b>work</b> is realized [...] -support methods to address memory failures at the computer workplace are developed. A support method design space to address memory failures is created. The design space is structured along three support directions (exploration, organization, recommendation). For each support direction, a respective user support method has been designed: 1) Activity-centric task management, which leverages activity data to facilitate task management and to support the recall of ongoing activities and respective work processes. 2) An interactive activity history, which enables the exploration of activity data in a work history visualization to support the recall <b>of</b> earlier <b>work</b> processes. 3) A recommender system, which analyzes the most recent <b>work</b> activities <b>of</b> the user to propose useful information objects like emails, files and websites. The system can be configured to support for more multitasking oriented or for more focused work. c) Transparency Tool: The support methods have been implemented in a demonstrator named Transparency. Using the demonstrator an evaluation of the support methods with a focus on memory support was conducted. The evaluation results indicate that the support methods decrease the likelihood of prospective and retrospective memory failures for information work at the computer workplace. The scientific contributions of this dissertation address two domains. On the one hand, information work support. Methods are developed which decrease the likelihood of prospective and retrospective memory failures based on activity data. On the other hand, system design methods. A method is introduced to design systems for work types which involve a high degree of individual planning. ...|$|R
5000|$|One or more datafiles form a <b>logical</b> <b>unit</b> <b>of</b> {{database}} storage {{called a}} tablespace.|$|R
5000|$|Function: Structuring Event-B {{developments}} into <b>logical</b> <b>units</b> <b>of</b> modelling, called modules; Model composition; Model reuse ...|$|R
50|$|Talmud of the Land of Israel: A Preliminary Translation and Explanation Jacob Neusner, Tzvee Zahavy, others. University of Chicago Press. This {{translation}} uses a form-analytical presentation {{which makes}} the <b>logical</b> <b>units</b> <b>of</b> discourse easier to identify and follow.|$|R
40|$|An eXtended Electronic Document (XED) is an XML {{document}} which aggregates data, templates, transformation {{rules and}} manipulation tools. It {{can be used}} to design complex systems on the Internet in a server-independent way, modifying the traditional asymmetry between the <b>logical</b> <b>units</b> <b>of</b> Web architecture...|$|R
25|$|Talmud of the Land of Israel: A Preliminary Translation and Explanation Jacob Neusner, Tzvee Zahavy, others. University of Chicago Press. This {{translation}} uses a form-analytical presentation {{that makes}} the <b>logical</b> <b>units</b> <b>of</b> discourse easier to identify and follow. This work has received many positive reviews. However, some consider Neusner's translation methodology idiosyncratic. One volume was negatively reviewed by Saul Lieberman of the Jewish Theological Seminary.|$|R
3000|$|... “Storage Unit”s {{can also}} be {{attached}} logically to a “Server”. Such a functionality is provided by means of “Logical Unit”s abstraction of “SAN” devices called “LUN”, whose details are covered in Section SAN storage. Each “LUN” class has the following attributes: LUNRef is used to reference the corresponding <b>logical</b> <b>unit</b> <b>of</b> a SAN device, whereas readRate and writeRate have the same definition as for their counterpart of “Storage Unit” class.|$|R
30|$|Researchers {{randomly}} selected the core subject {{for the second}} year students in each program to assess the feedback because respondents in the same cluster are required to evaluate the same teaching and learning context. The rationale behind this selection is not only because classrooms served as <b>logical</b> <b>units</b> <b>of</b> analysis, but also because this procedure greatly simplified the task of reaching students (Tinto, 1997). Hence, the core subject chosen is regarded as the secondary cluster.|$|R
50|$|DCE/DFS also {{divorced}} {{the concept}} <b>of</b> <b>logical</b> <b>units</b> <b>of</b> management (Filesets) from the underlying volume {{on which the}} fileset was stored. In doing this it allowed administrative control of the location for the fileset {{in a manner that}} was transparent to the end user. To support this and other advanced DCE/DFS features, a local journaling file system (DCE/LFS also known as Episode) was developed to provide the full range of support options.|$|R
5000|$|The Small Computer System Interface (SCSI) is {{a family}} of {{protocols}} for communicating with I/O devices, especially storage devices. SCSI is based on client-server model. SCSI clients, called [...] "initiators", issue SCSI commands to request services from components, <b>logical</b> <b>units</b> <b>of</b> a server known as a [...] "target". A [...] "SCSI transport" [...] maps the client-server SCSI protocol to a specific interconnect. An Initiator is one endpoint of a SCSI transport and a target is the other endpoint.|$|R
5000|$|Interval {{contingent}} - records data {{according to}} the passing of {{a certain period of}} time. Typically, participants are asked to self-report on the behavior of interest at pre-determined intervals which are determined on the basis of either theoretical or <b>logical</b> <b>units</b> <b>of</b> time. The selection of the interval itself is crucial for not leading to skewed perception of the behavior, but it also is important that it is not taxing on the participants. A day is the most commonly used sampling unit.|$|R
30|$|<b>Logical</b> status <b>of</b> <b>unit</b> {{commitment}} constraints.|$|R
40|$|The work is {{concerned}} with the procedures of calculation organization in the ariphmetic and <b>logical</b> <b>units</b> <b>of</b> the microprocessors. The aim is to develop the principles for construction of the modular reconstructed ariphmetic and <b>logical</b> <b>unit</b> with increased efficiency of using computer resource. The method for synthesis of the modular reconstructed ariphmetic and <b>logical</b> <b>unit,</b> 12 procedures for union of the computer sections in reconstructed ariphmetic and <b>logical</b> <b>unit</b> and also three criteria of the computer section stealing have been proposed. The efficiency of using computer resource in ALU of the microprocessors 80286 / 80287 of firm "Intel" has been increased; the procedures for union of the computer sections in reconstructed ALU provide the microprocessor power increase; the software "Object-oriented simulation system" and "Object-oriented data base" have developedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Reversible {{logic is}} one of the {{emerging}} fields of research in the areas of low power computation, Optical information processing, Fault tolerant system, bio information, quantum computation and nanotechnology. ALU is the most vital component of any processing system and need to consume as much less energy as possible in the mean while must be resistant to faults. In this paper the design of a fault tolerant function generator is brought out that can generate up to 16 different Boolean Functions. This <b>unit</b> is the <b>logical</b> <b>unit</b> <b>of</b> an ALU...|$|R
40|$|Includes music. At head <b>of</b> title: <b>Units</b> <b>of</b> <b>work</b> {{developing}} out <b>of</b> children's {{interests in}} local history. Bibliography: p. 306 - 314. introduction: {{a basis for}} the <b>unit</b> <b>of</b> <b>work</b> curriculum. [...] pt. I. Two <b>units</b> <b>of</b> <b>work</b> on Indian life and the Dutch colonial settlement as developed by one third grade, by Katharine L. Keelor. [...] pt. II. Two <b>units</b> <b>of</b> <b>work</b> on Indians and a Dutch kermis as developed by another third grade, by Mayme Sweet. Mode of access: Internet...|$|R
25|$|Hyper-V {{implements}} {{isolation of}} virtual machines {{in terms of}} a partition. A partition is a <b>logical</b> <b>unit</b> <b>of</b> isolation, supported by the hypervisor, in which each guest operating system executes. A hypervisor instance has to have at least one parent partition, running a supported version of Windows Server (2008 and later). The virtualization stack runs in the parent partition and has direct access to the hardware devices. The parent partition then creates the child partitions which host the guest OSs. A parent partition creates child partitions using the hypercall API, which is the application programming interface exposed by Hyper-V.|$|R
30|$|The {{imperative}} synchronous language Quartz implements the synchronous {{model of}} computation {{by means of}} the pause statement. While all other primitive statements do not take time (in terms of macro steps), a pause marks the end of a macro step and consumes one <b>logical</b> <b>unit</b> <b>of</b> time. Thus, the behavior of a whole macro step is defined by all actions between two consecutive pause statements. Parallel threads run in lock-step: their macro steps are executed synchronously, and the statement in both are scheduled according to the data dependencies so that all variables have a unique well-defined value in the macro step.|$|R
50|$|Hyper-V {{implements}} {{isolation of}} virtual machines {{in terms of}} a partition. A partition is a <b>logical</b> <b>unit</b> <b>of</b> isolation, supported by the hypervisor, in which each guest operating system executes. A hypervisor instance has to have at least one parent partition, running a supported version of Windows Server (2008 and later). The virtualization stack runs in the parent partition and has direct access to the hardware devices. The parent partition then creates the child partitions which host the guest OSs. A parent partition creates child partitions using the hypercall API, which is the application programming interface exposed by Hyper-V.|$|R
50|$|In hydrology, the {{drainage}} basin is a <b>logical</b> <b>unit</b> <b>of</b> focus {{for studying the}} movement of water within the hydrological cycle, {{because the majority of}} water that discharges from the basin outlet originated as precipitation falling on the basin. A portion of the water that enters the groundwater system beneath {{the drainage}} basin may flow towards the outlet of another drainage basin because groundwater flow directions do not always match those of their overlying drainage network. Measurement of the discharge of water from a basin may be made by a stream gauge located at the basin's outlet.|$|R
50|$|An {{execution}} model specifies how work takes place. Every {{programming language}} has an execution model, which is specified {{as part of}} the language specification, and is implemented {{as part of the}} language implementation. The details in the specification of an execution model cover things such as what is an indivisible <b>unit</b> <b>of</b> <b>work,</b> and what are the constraints on the order in which those <b>units</b> <b>of</b> <b>work</b> take place. For example, the addition operation is an indivisible <b>unit</b> <b>of</b> <b>work</b> in many languages, and in sequential languages such <b>units</b> <b>of</b> <b>work</b> are constrained to take place one after the other.|$|R
40|$|A task of {{constructing}} {{transformation of the}} hypercube providing monotony of the separating functions when maintaining a definite metric has been first set and solved. The pesults {{of the structure of}} random monotone functions with a small number <b>of</b> low <b>units</b> have been obtained. The programs of two-level synthesis of combinational and logic circuit have been developed. The algorithm for recognizing images in binary sign space has been proposed. The circuits synthesis programs have been developed, debugged and included in the sigma complex and are used when designing the combinational and <b>logical</b> <b>units</b> <b>of</b> large-scale integrated circuitsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
50|$|As part of ILM, SAP IQ {{allows users}} to create {{multiple}} user DBSpaces (<b>logical</b> <b>units</b> <b>of</b> storage/containers for database objects) for organizing data. This {{can be used to}} separate structured or unstructured data, group it together according to age and value, or to partition table data. DBSpaces can also be marked as read-only to enable one-time consistency checking and back-up. Another application of ILM is the ability to partition tables, and place moving portions along the storage fabric and backup capabilities; this enables a storage management process where data cycles through tiered storage, moving from faster more expensive storage to slower, cheaper storage as it ages, partitioning data according to value.|$|R
50|$|It is {{important}} to note that flits represent <b>logical</b> <b>units</b> <b>of</b> information, while phits represent the physical domain, that is, phits represent the number of bits that can be transferred in parallel in a single cycle. Let us consider the Cray T3D as an example here, It has an interconnection network which utilizes flit level message flow control where each flit comprises 8, 16-bit phits. Meaning, its flit size is 128bits while phit size is 16bits. Also consider the IBM SP2 switch, which also uses the flit level message flow control, however, its flit size is equal to its phit size which is set to 8 bits.|$|R
5000|$|Mission Area Sub Network (MASN): A <b>logical</b> group <b>of</b> <b>units</b> {{that has}} been {{previously}} defined ...|$|R
40|$|Abstract: Aiming at QPSK {{modulation}} {{digital system}} with variable rate, a novel implementation method based on field {{programmable gate array}} (FPGA) is proposed, which can support 4. 88 Kbps to 2 Mbps and even higher continuous bit rate. The design adopts mixed multiplier, numerically controlled oscillator (NCO) and integral comb filter (CIC), and describes the structure of carrier recovery circuit and signal recovery circuit, which can be ported to any FPGA device. The pro-posed design has its hardware test in the Xilinx Virtex- 5 FPGA platform. The hardware test results show that the proposed demodulator only takes up 15 % available <b>logical</b> <b>unit</b> <b>of</b> Xilinx Virtex- 5 FPGA device, revealing superior ability in effi-ciency...|$|R
50|$|A <b>Unit</b> <b>of</b> <b>Work</b> {{keeps track}} <b>of</b> {{everything}} you do during a business transaction that can affect the database. org.athenasource.framework.eo.core.UnitOfWork is Athena's implementation <b>of</b> the <b>Unit</b> <b>of</b> <b>Work</b> pattern. UnitOfWork ensures uniqueness of EOObject. Each database record results maximum one enterprise object in a UnitOfWork.|$|R
5000|$|Transactions managed on Object-relational mapping <b>Units</b> <b>of</b> <b>Work</b> ...|$|R
40|$|Scheduling in {{a shared}} memory {{multiprocessor}} is often {{complicated by the}} fact that a <b>unit</b> <b>of</b> <b>work</b> may be processed more efficiently on one processor than on any other, due to factors such as the presence of required data in a local cache. The <b>unit</b> <b>of</b> <b>work</b> is said to have an "affinity" for the given processor, in such a case. The scheduling issue that has to be considered is the tradeoff between the goals of respecting processor affinities (so as to obtain improved efficiencies in execution) and of dynamically assigning each <b>unit</b> <b>of</b> <b>work</b> to whichever processor happens to be, at the time, least loaded (so as to obtain better load balance and decreased processor idle times). A specific context in which the above scheduling issue arises is that of shared memory multiprocessors with large, per-processor caches or cached main memories. The shared-memory programming paradigm of such machines permits the dynamic scheduling <b>of</b> <b>work.</b> The data required by a <b>unit</b> <b>of</b> <b>work</b> may, however, often reside mostly in the cache of one particular processor, to which that <b>unit</b> <b>of</b> <b>work</b> thus has affinity. In this paper, two new "affinity scheduling" algorithms are proposed for a context in which the <b>units</b> <b>of</b> <b>work</b> have widely varying execution times. An experimental study of these algorithms finds them to perform well in this context...|$|R
