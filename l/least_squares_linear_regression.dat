126|10000|Public
2500|$|To {{demonstrate}} this property, {{first recall}} that the objective of <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> is: ...|$|E
2500|$|The {{standard}} approach is ordinary <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b> However, if no [...] satisfies the equation {{or more than}} one [...] does—that is, the solution is not unique—the problem {{is said to be}} ill posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations. [...] Most real-world phenomena have the effect of low-pass filters in the forward direction where [...] maps [...] to [...] [...] Therefore, in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). [...] In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of [...] that is in the null-space of , rather than allowing for a model {{to be used as a}} prior for [...] Ordinary least squares seeks to minimize the sum of squared residuals, which can be compactly written as: ...|$|E
5000|$|To {{demonstrate}} this property, {{first recall}} that the objective of <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> is: ...|$|E
30|$|In this way, {{the edge}} region is roughly located, {{and then the}} edge {{detection}} and location of the edge is required by the <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> sub-pixel edge detection method.|$|R
5000|$|Because wind {{is highly}} {{variable}} year to year, short-term (< 5 years) onsite measurements {{can result in}} highly inaccurate energy estimates. Therefore, wind speed data from nearby longer term weather stations (usually located at airports) are used to adjust the onsite data. <b>Least</b> <b>squares</b> <b>linear</b> <b>regressions</b> are usually used, although several other methods exist as well.|$|R
5000|$|... 2. [...] Now regress the {{observed}} vector of outcomes on the selected principal components as covariates, using ordinary <b>least</b> <b>squares</b> <b>regression</b> (<b>linear</b> <b>regression)</b> {{to get a}} vector of estimated regression coefficients (with dimension equal {{to the number of}} selected principal components).|$|R
5000|$|Thus, when regret is minimised, {{competition}} {{against the}} best weight vector [...] occurs.As an example, {{consider the case of}} online <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b> Here, the weight vectors come from the convex set , and nature sends back the convex loss function [...] Note here that [...] is implicitly sent with [...]|$|E
5000|$|... again a {{value that}} depends on i - or, more specifically, a value that is {{conditional}} on the values {{of one or more}} of the regressors X. Homoscedasticity, one of the basic Gauss-Markov assumptions of ordinary <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> modeling, refers to equal variance in the random error terms regardless of the trial or observation, such that ...|$|E
5000|$|If [...] is not {{correlated}} {{with any of}} the independent variables, ordinary <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> methods can be used to yield unbiased and consistent estimates of the regression parameters. However, because [...] is fixed over time, it will induce serial correlation in the error term of the regression. This means that more efficient estimation techniques are available. Random effects is one such method: it is a special case of feasible least squares which controls for the structure of the serial correlation induced by [...]|$|E
40|$|Abstract − Two linear fitting {{procedures}} {{are applied to}} internationally reference data in refractometry of solutions. <b>Least</b> <b>square</b> <b>linear</b> <b>regression</b> is compared to linear interpolation in the intervals of the two successive referenced data. The formulas and {{the validity of the}} {{procedures are}} shortly presented. The results are comparable, but for faster results, the <b>linear</b> <b>regression</b> method is preferred to the interpolation by intervals, albeit their bigger uncertainties values...|$|R
5000|$|Multiple {{meteorological}} towers {{are usually}} installed on large wind farm sites. For each tower, {{there will be}} periods of time where data is missing but has been recorded at another onsite tower. <b>Least</b> <b>squares</b> <b>linear</b> <b>regressions</b> {{can be used to}} fill in the missing data. These correlations are more accurate if the towers are located near each other (a few km distance), the sensors on the different towers are of the same type, and are mounted at the same height above the ground.|$|R
40|$|The {{procedure}} to predict solar activity indexes {{for use in}} upper atmosphere density models is given together with {{an example of the}} performance. The prediction procedure employs a <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> model to generate the predicted smoothed vinculum R sub 13 and geomagnetic vinculum A sub p(13) values. <b>Linear</b> <b>regression</b> equations are then employed to compute corresponding vinculum F sub 10. 7 (13) solar flux values from the predicted vinculum R sub 13 values. The output is issued principally for satellite orbital lifetime estimations...|$|R
5000|$|The {{standard}} approach is ordinary <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b> However, if no [...] satisfies the equation {{or more than}} one [...] does—that is, the solution is not unique—the problem {{is said to be}} ill posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where [...] maps [...] to [...] Therefore, in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of [...] that is in the null-space of , rather than allowing for a model {{to be used as a}} prior for [...] Ordinary least squares seeks to minimize the sum of squared residuals, which can be compactly written as: ...|$|E
50|$|Multicollinearity often {{occurs in}} {{high-throughput}} biostatistical settings. Due to high intercorrelation between the predictors (such as gene expression levels), the information of one predictor might be contained in another one. It {{could be that}} only 5% of the predictors are responsible for 90% of the variability of the response. In such a case, one could apply the biostatistical technique of dimension reduction (for example via principal component analysis). Classical statistical techniques like linear or logistic regression and linear discriminant analysis do not work well for high dimensional data (i.e. {{when the number of}} observations n is smaller than the number of features or predictors p: n < p). As a matter of fact, one can get quite high R2-values despite very low predictive power of the statistical model. These classical statistical techniques (esp. <b>least</b> <b>squares</b> <b>linear</b> <b>regression)</b> were developed for low dimensional data (i.e. where the number of observations n is much larger than the number of predictors p: n >> p). In cases of high dimensionality, one should always consider an independent validation test set and the corresponding residual sum of squares (RSS) and R2 of the validation test set, not those of the training set.|$|E
30|$|For {{images with}} higher {{signal-to-noise}} ratio, the least-squares linear regression and the spatial moment sub-pixel localization algorithm can achieve sub-pixel accuracy for straight-line edge detection, and satisfactory results are obtained [15]. <b>Least</b> <b>squares</b> <b>linear</b> <b>regression</b> sub-pixel positioning accuracy is approximately 0.1 pixel, spatial moment sub-pixel positioning accuracy is approximately 0.01 pixel, and the <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> sub-pixel positioning algorithm is {{much faster than}} the spatial moment sub-pixel positioning algorithm. Therefore, this paper uses <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> to reduce the two-dimensional edge fitting to one-dimensional edge location, so that the straight edge location can reach sub-pixel accuracy. In the linear filtering edge detection method, the Canny optimal operator is the most representative, {{and it is also}} one of the operators that detect the stepwise edge effect better. First, the Canny operator is used as an integer pixel level edge location function to extract the entire pixel level edge. Then, the edge sub-pixel location is performed using a <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b>|$|E
40|$|This study aims {{to examine}} the {{consequences}} of the implementation of PSAK No. 10 (2010) regarding The Effect of Changes in Foreign Exchange Rates on the informativeness of earnings. Our research sample consists of companies listed on Indonesia Stock Exchange (IDX) from 2010 until 2013. Using panel <b>least</b> <b>square</b> <b>linear</b> <b>regression,</b> this study finds that the implementation of PSAK No. 10 (2010) improves the informativeness of earnings which is measured by Earnings Response Coefficient (ERC). This is the first study that investigates the impact of the implementation of PSAK No. 10 (2010) on ERC, therefore this study contributes on providing evidence of the impact of accounting standard revision implementation on capital market aspect...|$|R
40|$|In this chapter, {{a survey}} of the theory behind the main chemometric methods used for multivariate {{calibration}} is presented. Ordinary <b>least</b> <b>squares,</b> multiple <b>linear</b> <b>regression,</b> principal component regression, partial <b>least</b> <b>squares</b> regression and principal covariate regression are discussed in detail. Tools for model diagnostics and model interpretation are presented, together with strategies for variable selection...|$|R
50|$|In {{mathematical}} statistics, polynomial <b>least</b> <b>squares</b> {{refers to}} {{a broad range of}} statistical methods for estimating an underlying polynomial that describes observations. These methods include polynomial <b>regression,</b> curve fitting, <b>linear</b> <b>regression,</b> <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> simple <b>linear</b> <b>regression,</b> <b>linear</b> <b>least</b> <b>squares,</b> approximation theory and method of moments. Polynomial <b>least</b> <b>squares</b> has applications in radar trackers, estimation theory, signal processing, statistics, and econometrics.|$|R
40|$|This paper {{presents}} {{alternative methods}} to forecast or predict failure trends when the data violates the assumptions associated with <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b> Simulations based on actual case studies validated that <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> {{may provide a}} biased model {{in the presence of}} messy data. Non-parametric regression methods provide robust forecasting models less sensitive to non-constant variability, outliers, and small data sets. 1...|$|E
30|$|The {{linearity}} {{was estimated}} by assaying calibration curves in rat plasma in duplication on three consecutive days. The calibration curves were fitted by a weighted (1 /x 2) <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> method and its acceptable criterion was that correlation coefficient (r) was ≥ 0.995.|$|E
30|$|The {{slope and}} {{intercept}} of the calibration curves were calculated through <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> analysis using Microsoft Excel Program. The slope and the intercept of the calibration curves were observed {{over a period}} of 6  weeks using six independent series of the calibration standards prepared as described earlier.|$|E
40|$|Abstract: <b>Least</b> <b>square</b> <b>linear</b> <b>regression</b> {{is widely}} used in {{analytical}} chemistry. In practice a linear relationship between substance content and measured value still has been assumed based on the correlation coefficient criterion, although not recommended. Textbooks provide the necessary formulas for the fitting process, {{based on the assumption}} that there is no error in the independent variable. In practice the ordinary <b>least</b> <b>squares</b> (OLS) textbook procedure is used even when the previously stated assumptions are not strictly fulfilled. In this paper, how to validate the calibration function is dealt with in detail using as an example based on measurements obtained for cadmium determination by flame atomic absorption spectrophotometry (FAAS). Assessing uncertainties related to linear calibration curves is also discussed. Considering uncertainties of weights and volumetric equipment and instrumental analytical signal it is observed that the most important factor that contributes to the final uncertainty is the uncertainty of the calibration function...|$|R
40|$|Background Due to the {{increasing}} evidence of their health benefits, whole grains are recommended for consumption worldwide. Such recommendations are, however, rarely quantitative. Our aim was to perform a quantitative evaluation {{of the relationship between}} whole grain consump-tion and the occurrence of type 2 diabetes (T 2 D) to support a recommendation on the daily consumption of whole grains. Methods and Findings We conducted a systematic review by searching three bibliographic databases. We included human studies addressing the relationship between whole grain consumption and T 2 D occurrence, and providing quantitative information on daily intake of whole grains. A dose-response meta-regression analysis between whole grain intake and T 2 D occurrence was performed, using a hierarchical mixed <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> model. Eight observational studies were included (all but one prospective), with a total of 15, 573 cases of T 2 D among 316, 051 participants. Quantitative meta-regression demonstrated a significan...|$|R
40|$|The {{study was}} {{conducted}} to determine factors influencing the adoption of agro-chemicals technology by small-scale farmers in Kwali area council, of the Federal Capital Territory Abuja, Nigeria. Structured interview schedule was used to collect data for the study. Ten respondents were randomly selected from each of the eight districts making the area council giving a total of eighty respondents. Statistical tools involving means, frequency and percentage were used to analyze the data. Ordinary <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> was used to determine factors influencing the adoption of agro-chemicals technology in the study area. The results of the study revealed that gender, age, farm income, marital status and years of farming experience have positive influence on the adoption of agro-chemicals in the study area. Management of Abuja Agricultural Development Project should endeavour to encourage farmers to form strong coherent group such as cooperatives to control agro-chemicals prices supplied by input representatives...|$|R
30|$|We used 108 tree {{samples from}} {{destructive}} sampling {{to develop the}} allometric equations, with maximum tree diameter of 175 cm and another 109 samples from previous studies for validating our equations. We performed ordinary <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> to explore {{the relationship between the}} AGB and the predictor variables in the natural logarithmic form.|$|E
30|$|Wilcoxon’s signed-rank {{test was}} used to test {{differences}} in stiffness and elongation between the fast and slow contractions. Linearity of average force elongation curves was tested with a <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> method. The level of significance was always set to P[*]<[*] 0.05. Errors in figures and tables are the standard error (SE) of the mean.|$|E
40|$|In {{a simple}} {{multiple}} linear regression model, the design variables have traditionally been assumed to be non-stochastic. In numerous real-life situations, however, they are stochastic and non-normal. Estimators of parameters applicable to such situations are developed. It is shown that these estimators are efficient and robust. A real-life example is given. correlation coefficient, <b>least</b> <b>squares,</b> <b>linear</b> <b>regression,</b> modified maximum likelihood, multivariate distributions, non-normality, random design,...|$|E
40|$|Abstract The Meyer-Neldel Rule (MNR), or {{compensation}} law, linearly {{relates the}} preexponent term to the logarithm of the excitation enthalpy for any {{process that is}} thermally driven in an Arrhenian manner, and MNR fits {{can be used to}} calibrate and validate laboratory experimental results. Both robust <b>least</b> <b>squares</b> <b>linear</b> <b>regressions</b> and nonrobust regressions on selected subsets for individual minerals with sufficient experimental data demonstrate that hydrogen diffusion in minerals obeys the MNR with differing MNR intercepts and gradients depending on the mineral. In particular, nominally anhydrous mantle minerals have very distinct and different MNR parameters compared to hydrous and crustal minerals, with garnet proving to be an outlier lying in between the two. Furthermore, the variations of the estimated intercepts and gradients of the various MNRs are not random, but remarkably they themselves fall on a striking linear trend. This observation, if more broadly true, has profound implications for materials sciences and under-standing of solid-state physics, as it implies that the compensation rule is itself compensated. 1...|$|R
40|$|Volterra series (VS) {{representation}} {{is a powerful}} mathematical model for nonlinear circuits. However, the difficulties in determining higher order Volterra kernels limited its broader applications. In this paper, a systematic approach that enables a convenient extraction of Volterra kernels from X-parameters is presented. A concise and general representation of the output response due to arbitrary number of input tones is given. The relationship between Volterra kernels and X-parameters is explicitly formulated. An efficient frequency sweep scheme and an output frequency indexing scheme are provided. The <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> method is employed to separate different orders of Volterra kernels at the same frequency, {{which leads to the}} obtained Volterra kernels complete. The proposed VS representation based on X-parameters is further validated for time-domain verification. The proposed method is systematic and general-purpose. It paves the way for time-domain simulation with X-parameters and constitutes a powerful supplement to the existing blackbox macromodeling methods for nonlinear circuits. Link_to_subscribed_fulltex...|$|R
40|$|The Nyström {{method is}} a well known {{sampling}} based low-rank matrix approximation approach. It is usually considered to be originated from the numerical treatment of integral equations and eigendecomposition of matrices. In this paper, we present a novel point of view for the Nyström approximation. We show that theoretically the Nyström method can be regraded {{as a set of}} pointwise ordinary <b>least</b> <b>square</b> <b>linear</b> <b>regressions</b> of the kernel matrix, sharing the same design matrix. With the new interpretation, we are able to analyze the approximation quality based on the fulfillment of the homoscedasticity assumption and explain the success and deficiency of various sampling methods. We also empirically show that positively skewed explanatory variable distributions can lead to heteroscedasticity. Based on this discovery, we propose to use non-symmetric explanatory functions {{to improve the quality of}} the Nyström approximation with almost no extra computational cost. Experiments show that positively skewed datasets widely exist, and our method exhibits good improvements on these datasets. ...|$|R
40|$|This {{paper is}} focused on dimension-free PAC-Bayesian bounds, under weak {{polynomial}} moment assumptions, allowing for heavy tailed sample distributions. It covers the estimation of the mean of a vector or a matrix, with applications to <b>least</b> <b>squares</b> <b>linear</b> <b>regression.</b> Special efforts are devoted to the estimation of Gram matrices, due to their prominent role in high-dimension data analysis. Comment: Version 1 needed some further proofreadin...|$|E
30|$|Peak VO 2 {{was defined}} as the highest 15 -s average value during the test [29]. Fifteen-second {{averaged}} ventilation (VE) and carbon dioxide production (VCO 2) data, obtained from the initiation of exercise to peak, were inputted into a spreadsheet software (Microsoft Excel, Microsoft Corp., Bellevue, WA, USA). VE and VCO 2 responses throughout exercise were used to calculate the VE/VCO 2 slope via <b>least</b> <b>squares</b> <b>linear</b> <b>regression</b> (y[*]=[*]mx[*]+[*]b, m[*]=[*]slope) [30].|$|E
40|$|This paper {{considers}} {{the problem of}} statistical inference in linear regression models whose stochastic regressors and errors may exhibit long-range dependence. A time-domain sieve-type generalized least squares (GLS) procedure is proposed based on an autoregressive approximation to the generating mechanism of the errors. The asymptotic properties of the sieve-type GLS estimator are established. A Monte Carlo study examines the finite-sample properties of the method for testing regression hypotheses. Autoregressive approximation, Generalized <b>least</b> <b>squares,</b> <b>Linear</b> <b>regression,</b> Long-range dependence, Spectral density...|$|E
40|$|The {{construction}} of a calibration curve using <b>least</b> <b>square</b> <b>linear</b> <b>regression</b> is common in many analytical measurements, and it comprises an important uncertainty component of the whole analytical procedure uncertainty. In the present work, various methodologies are applied concerning the estimation of the standard uncertainty of a calibration curve used for the determination of sulfur mass concentration in fuels. The methodologies applied include the GUM uncertainty framework, the Kragten numerical method, the Monte Carlo method (MCM) {{as well as the}} approximate equation calculating the standard error of prediction. The standard uncertainty results obtained by all methodologies agree well (0. 172 - 0. 175 ng μL- 1). Aspects of inappropriate use of the approximate equation of the standard error of prediction, which leads to overestimation or underestimation of calculated uncertainty, are discussed. Moreover, the importance of the correlation between calibration curve parameters (slope and intercept) within GUM, MCM and Kragten approaches is examined. © 2011 Springer-Verlag...|$|R
40|$|Spatial and {{temporal}} analysis of fires in Serbia for period November 2000 -August 2013 has been performed {{to investigate whether}} spatial relationships exist among fire data. MODIS active fire data were used as fire locations. On such data, different tools of spatial analysis and spatial statistics were used, {{to determine if there}} is any spatial relationship. Analysis included data screening, identification of land cover of fire locations, aspect, slope, elevation and solar radiation for each location. Later, different spatial statistics tools were executed against fire locations data, including Getis-Ord Gi* Hot Spots, Global Moran’s I Spatial Autocorrelation, Anselin Local Moran’s I Cluster and Outlier, Ordinary <b>Least</b> <b>Square</b> <b>linear</b> <b>regression</b> and Geographically Weighted Regression. Fire radiative power was used as dependant variable, while terrain morphology and solar radiation were used as explanatory variables. Results shows hot spots of fires in Serbia, and indicates that there is strong relationship between fire radiative power on one side and terrain morphology, land cover, solar radiation and spatial distribution on other side. These analysis have highlighted areas with very intensive fire use associated with land management practices...|$|R
40|$|This is {{a conference}} paper. Temperature {{coefficient}} (TC) measurements of PV devices {{are required for}} energy yield estimations. However, a large deviation in measurement results is still being reported. This paper outlines the measurement setup at CREST, the sources of uncertainty and their estimation and methods for propagating them to the final uncertainty of the TC. While for very small uncertainties Ordinary <b>Least</b> <b>Squares</b> (OLS) <b>linear</b> <b>regression</b> may be appropriate for realistic uncertainties a Weighted Total <b>Least</b> <b>Squares</b> (WTLS) is recommended...|$|R
