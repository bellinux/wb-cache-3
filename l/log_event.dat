12|1767|Public
50|$|In Log4j 2, Filters can {{be defined}} on {{configuration}} elements to give more fine-grained control over which log entries should be processed by which Loggers and Appenders. In addition to filtering by log level and regular expression matching on the message string, Log4j 2 added burst filters, time filters, filtering by other <b>log</b> <b>event</b> attributes like Markers or Thread Context Map and JSR 223 script filters.|$|E
40|$|In this work, {{we propose}} an {{approach}} based on analyzing the spatio-temporal partitions {{of a system}} log, generated by supercomputers consisting of several nodes, for alert detection without employing semantic analysis. In this case, “Spatial ” refers {{to the source of}} the <b>log</b> <b>event</b> and “Temporal” refers to the time the <b>log</b> <b>event</b> was reported. Our research shows that these spatio-temporal partitions can be clustered to separate normal activity from anomalous activity, with high accuracy. Therefore, our proposed method provides an effective alert detection mechanism...|$|E
40|$|International audienceIn our rapidly {{evolving}} society, every {{corporation is}} trying to improve its competitiveness by refactoring and improving some - if not all - of its industrial software infrastructure. This goes from mainframe applications that actually handle the company’s profit generating material, to the internal desktop applications used to manage these application servers. These applications often have extended activity logging features that notify the administrators of every event encounter at runtime. Unfortunately, the standalone nature of the event logging sources renders the correlation of <b>log</b> <b>event</b> infrastructure prone to “continuous queries”. This paper describes an approach that “adapts and employs continues queries” for distributed <b>log</b> <b>event</b> correlation with the aim to solve problems that face the present <b>log</b> <b>event</b> management systems. It will present LEC architecture that analyze a set of distributed log events that follow a set of correlation rules; then the main output is a stream of correlated log events...|$|E
50|$|Event {{collection}} {{is the process}} of collecting event occurrences in a filtered <b>event</b> <b>log</b> for analysis. A filtered <b>event</b> <b>log</b> is <b>logged</b> <b>event</b> occurrences that can be of meaningful use in the future; this implies that event occurrences can be removed from the filtered <b>event</b> <b>log</b> if they are useless in the future. <b>Event</b> <b>log</b> analysis {{is the process of}} analyzing the filtered <b>event</b> <b>log</b> to aggregate <b>event</b> occurrences or {{to decide whether or not}} an event occurrence should be signalled. Event signalling is the process of signalling event occurrences over the event bus.|$|R
25|$|Windows Vista {{includes}} a completely overhauled and rewritten <b>Event</b> <b>logging</b> subsystem, known as Windows <b>Event</b> <b>Log</b> which is XML-based and allows applications to more precisely <b>log</b> <b>events,</b> offers better views, filtering and categorization by criteria, automatic log forwarding, centrally <b>logging</b> and managing <b>events</b> {{from a single}} computer and remote access.|$|R
5000|$|BlackBerry SysLog Service — Provides Administrators with {{real-time}} monitoring of BlackBerry Enterprise Server <b>log</b> <b>events.</b>|$|R
40|$|We {{present a}} {{multiple}} views visualization {{for the security}} data in the VAST 2010 Mini Challenge 2. The visualization is used to moni-tor <b>log</b> <b>event</b> activity on the network log data included in the chal-lenge. Interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed. Additionally, a database application is used to allow SQL queries for more detailed investigation...|$|E
40|$|The goal of {{workflow}} mining is {{to obtain}} objective and valuable information from event logs. The research of workflow mining {{is of great}} significance for deploying new business process as well as analyzing and improving the already deployed ones. Many information systems <b>log</b> <b>event</b> data about executed tasks. Workflow mining {{is concerned with the}} derivation of a graphical process model out of this data. Currently, workflow mining research is narrowly focused on the rediscovery of control flow models. In this paper, we present workflow mining of more perspectives of workflow to broaden the scope of workflow mining. The mining model is described with GBMS’s VPML and we present the entire model’s workflow mining with the GBMS’s VPML...|$|E
40|$|Many {{distributed}} algorithms require {{knowledge of}} the causal relationships between events. Examples include optimistic recovery protocols, distributed debugging systems, and causal distributed shared memory. Determining causal relationships can be difficult, however, {{because there is no}} global clock and local clocks cannot be perfectly synchronized. Vector time is a useful abstraction for capturing the causal relationships between events and, unlike Lamport's logical clocks, allows identification of concurrent events. Some drawbacks of vector time include transmission and logging overhead, since the size of a vector clock is linear in the number of processes. This paper presents a technique to reduce these overheads for applications that dynamically create and destroy processes and <b>log</b> <b>event</b> information with attached vector timestamps. The reduction in logging overhead comes at the expense of a more complicated timestamp comparison protocol and more sophisticated data structures for mai [...] ...|$|E
5000|$|GOL: Big Data SIEM Analytics tool from Clusterpark - <b>Log,</b> <b>Events</b> and Security Records Search and Analytics.|$|R
50|$|Introduced {{with the}} launch of the Cisco ASA 5580 products, NetFlow Security <b>Event</b> <b>Logging</b> {{utilizes}} NetFlow v9 fields and templates in order to efficiently deliver security telemetry in high performance environments. NetFlow Security <b>Event</b> <b>Logging</b> scales better than syslog while offering the same level of detail and granularity in <b>logged</b> <b>events.</b>|$|R
40|$|<b>Event</b> <b>logs</b> are an {{important}} data source for identifying usability problems in websites. We present a web-based client-server application, Remote Web <b>Event</b> <b>Logging</b> System (RWELS), for <b>logging</b> user-interface <b>events</b> generated in the Microsoft Internet Explorer during a user’s interaction with {{the pages of a}} website. RWELS <b>logs</b> <b>events</b> without interfering with the user’s interaction – no additional interaction is required on part of a user to enable logging. RWELS is configurable and allows user-centric <b>event</b> <b>logging.</b> A usability analyst can choose the set of events to be captured and the pages of the website to be logged for a particular user. The <b>event</b> <b>logs</b> are dispatched through HTTP to the server where they are stored as text files. Users are identified uniquely and the <b>event</b> <b>logs</b> are associated with the user sessions...|$|R
40|$|Abstract—Currently, full-text searching {{can benefit}} from the {{emerging}} NoSQL databases and traditional indexing tools in the big data era. However, there are some drawbacks of current solu-tions. On one hand, the indexing documents lack of the hierarchy. On the other hand, big data have become the bottleneck of full-text searching. In the context of big data, we design a full-text searching middleware over hierarchical documents. We discuss the architecture of this middleware in detail. In addition, we propose a structure-independent hierarchical document model to present the hierarchical document. Moreover, the transformation engine is designed to translate the rich files into models. The core <b>log</b> <b>event</b> listener is responsible for capturing the changed documents and push them to the indexing storage at the same time. The experimental results show that our middleware is more advantageous than RDBMS with indexes and RDBMS with Lucene solutions. Index Terms—Full-text searching; middleware; hierarchica...|$|E
40|$|Abstract—Process mining is a {{relatively}} new research area aiming to extract process models from event logs of real systems. A lot of new approaches and algorithms are developed in this field. Researches and developers usually have a need to test end evaluate the newly constructed algorithms. In this paper we propose a new approach for generation of event logs. It serves to facilitate the process of evaluation and testing. Presented approach allows to generate event logs, and sets of event logs to support a large scale testing in a more automated manner. Another feature of the approach is a generation of event logs with noise. This feature allows to simulate real-life system execution with inefficiencies, drawbacks, and crashes. In this work we also consider other existing approaches. Their forces and weaknesses are shown. The approach presented as well as the corresponding tool can be widely used in the research and development process. Keywords—Process mining, Petri net, event <b>log,</b> <b>event</b> log generation, ProM. I...|$|E
40|$|Event log {{processing}} and analysis {{play a key role}} in applica-tions ranging from security management, IT trouble shoot-ing, to user behavior analysis. Recent years have seen a rapid growth in system scales and the corresponding rapid increase in the amount of <b>log</b> <b>event</b> data. At the same time, as logs are found to be a valuable information source, log analysis tasks have become more sophisticated demand-ing both interactive exploratory query {{processing and}} batch computation. Desirable query types include selection with time ranges and value filtering criteria, join within time win-dows, join between log data and reference tables, and various aggregation types. In such a situation, parallel solutions are necessary, but existing parallel and distributed solutions ei-ther support limited query types or perform only batch com-putations on logs. With a system called LogKV, this paper reports a first study of using Key-Value stores to support log {{processing and analysis}}, exploiting the scalability, reli-ability, and efficiency commonly found in Key-Value store systems. LogKV contains a number of unique techniques that are needed to handle log data in terms of event inges-tion, load balancing, storage optimization, and query pro-cessing. Preliminary experimental results show that LogKV is a promising solution. 1...|$|E
40|$|The 'Hospital Billing' <b>event</b> <b>log</b> was {{obtained}} from the financial modules of the ERP system of a regional hospital. The <b>event</b> <b>log</b> contains <b>events</b> that are related to the billing of medical services that have been provided by the hospital. Each trace of the <b>event</b> <b>log</b> records the activities executed to bill a package of medical services that were bundled together. The <b>event</b> <b>log</b> does not contain information about the actual medical services provided by the hospital. The 100, 000 traces in the <b>event</b> <b>log</b> are a random sample of process instances that were recorded over three years. Several attributes such as the 'state' of the process, the 'caseType', the underlying 'diagnosis' etc. are included in the <b>event</b> <b>log.</b> <b>Events</b> and attribute values have been anonymized. The time stamps of events have been randomized for this purpose, but the time between events within a trace has not been altered. More information about the <b>event</b> <b>log</b> {{can be found in the}} related publications...|$|R
5000|$|Veracity: <b>Log</b> <b>events</b> {{may not be}} accurate. This is {{especially}} problematic from systems that perform detection, such as intrusion detection systems.|$|R
50|$|Windows Vista {{includes}} {{a number of}} self-diagnostic features which help identify various problems and, if possible, suggest corrective actions. The <b>event</b> <b>logging</b> subsystem in Windows Vista also has been completely overhauled and rewritten around XML to allow applications to more precisely <b>log</b> <b>events.</b> Event Viewer has also been rewritten {{to take advantage of}} these new features. There are a large number of different types of <b>event</b> <b>logs</b> that can be monitored including Administrative, Operational, Analytic, and Debug log types. For instance, selecting the Application Logs node in the Scope pane reveals numerous new subcategorized <b>event</b> <b>logs,</b> including many labeled as diagnostic <b>logs.</b> <b>Event</b> <b>logs</b> can now be configured to be automatically forwarded to other systems running Windows Vista or Windows Server 2008. <b>Event</b> <b>logs</b> can also be remotely viewed from other computers or multiple <b>event</b> <b>logs</b> can be centrally logged and managed from a single computer. <b>Event</b> <b>logs</b> can be filtered by one or more criteria, and custom views can be created for one or more events. Such categorizing and advanced filtering allows viewing logs related only to a certain subsystem or an issue with only a certain component. Events can also be directly associated with tasks, via the redesigned Event Viewer.|$|R
40|$|In a message-flow Internet service where {{messages}} {{travel through}} multiple nodes, event log analysis {{is one of}} the most important methods to identify the root causes of problems. Traditional approaches for event log analysis have been largely based on expert systems that build static dependency models on rules and patterns defined by human experts. However, the semantic complexity and the various formats of event logs make it difficult to be modeled. In addition, it is time consuming to maintain such static model for constantly evolving Internet services. Recent research has been focused on building statistical models. However, all of these models rely on the trace information provided by J 2 EE or. NET frameworks, which are not available to all Internet services. In this thesis, we propose a framework of problem determination based on statistical analysis of event logs. We assume a unique message ID will be logged in multiple log lines to trace the message flow in the system. A generic log adaptor is defined to extract valuable information from the log entries. We also develop an algorithm of <b>log</b> <b>event</b> clustering and log pattern clustering. Frequency analysis will be performed based on the log patterns in order to build a statistical model of the system behaviors. Once the system is modeled, we can determine problems by running a chi-square goodness of fit test using a sliding window approach. As event logs are available on all major operating systems, we believe our framework is a generic solution for problem determination in message-flow Internet services. Our solution has been validated by the log data collected from the Blackberry Internet Service (BIS) engine ‎[4], a wireless email service that serves millions of users across the world. According to the test results, our solution shows high accuracy of problem determination...|$|E
30|$|Intrusion Detection {{frequently}} involves {{analysis of}} Big Data, which {{is defined as}} research problems where mainstream computing technologies cannot handle the quantity of data. Even a single security event source such as network traffic data can cause Big Data challenges. According to Nassar et al. [1], merely 1 Gbps of sustained network traffic can cause Big Data challenges for Intrusion Detection while using deep packet inspection. Another Big Data challenge that larger organizations can face is having {{an incredible amount of}} host <b>log</b> <b>event</b> data. The Cloud Security Alliance reported [2] that in 2013, it is estimated that an enterprise like HP can “generate 1 trillion events per day or roughly 12 million events per second”. They report that such large volumes of data are “overwhelming” and they even struggle to simply store the data. Enterprises dealing with such Big Data issues at this scale cannot use existing analytical techniques effectively, and so false alarms are especially problematic. Additionally, it can be very difficult to correlate events over such large amounts of data, especially when that data can be stored in many different formats. Relational database technology can commonly become a bottleneck in Big Data challenges. For example, commercial SIEMs that use relational database technologies for their storage repositories will find the databases becoming bottlenecks in deployments at larger enterprises: storage and retrieval of data begins to take longer than is acceptable. Zions Bancorporation conducted a case study [3] where it would take their traditional SIEM systems between 20 minutes to an hour to query a month’s worth of security data, however when using tools with Hadoop technology it would only take about one minute to achieve the same results. It is a clear sign that Intrusion Detection is facing Big Data challenges when a mainstream technology like relational databases becomes a bottleneck. Next generational Big Data storage technologies like Hadoop can help address these problems.|$|E
40|$|Large {{software}} {{systems are}} composed of various different run-time components, partner applications and, processes. When such systems operate they are monitored so that audits can be performed once a failure occurs or when maintenance operations are performed. However, log files are usually sizeable, and require filtering and reduction to be processed efficiently. Furthermore, there is no apparent correspondence of how logged events relate to particular use cases the system may be performing. In this thesis, we have developed a framework {{that is based on}} heuristic clustering algorithms to achieve log filtering, log reduction and, log interpretation. More specifically we define the concept of the Event Dependency Graph, and we present event filtering and use case identification techniques, that are based on event clustering. The clustering process groups together all events that relate to a collection of initial significant events that relate to a use case. We refer to these significant events as beacon events. Beacon events can be identified automatically or semiautomatically by examining <b>log</b> <b>event</b> types or event names against event types or event names in the corresponding specification of a use case being considered (e. g. events in sequence diagrams). Furthermore, the user can select other or additional initial clustering conditions based on his or her domain knowledge of the system. The clustering technique can be used in two possible ways. The first is for large logs to be reduced or sliced, with respect to a particular use case so that, operators can better focus their attention to specific events that relate to specific operations. The second is for the determination of active use cases where operators select particular seed events of interest and then examine the resulting reduced logs against events or event types stemming from different alternative known use cases being considered, in order to identify the best match and consequently provide insights on which of these alternative use cases may be running at any given time. The approach has shown very promising results towards the identification of executing use cases among various alternative ones in various runs of the Session Initiation Protocol...|$|E
40|$|<b>Event</b> <b>logging</b> on {{mobile phones}} is {{interesting}} for e. g. diary keeping. We present a logging service which – automatically {{and in the}} background on Symbian based mobile phones – <b>logs</b> <b>events</b> originated by user interactivities with mobile phones. Context {{information that can be}} obtained by the phones themselves is logged as well. The service offers direct access to the <b>logged</b> <b>events,</b> so that data can be retrieved and reviewed. This logging service was developed with aim of logging user activity related data for the Affective Diary project at SICS. The Affective Diary project aims at reflecting user’s emotional experiences. There is...|$|R
40|$|AbstractEvent logs are an {{important}} data source for identifying usability problems in websites. We present a web-based client-server application, Remote Web <b>Event</b> <b>Logging</b> System (RWELS), for <b>logging</b> user-interface <b>events</b> generated in the Microsoft Internet Explorer during a user's interaction with {{the pages of a}} website. RWELS <b>logs</b> <b>events</b> without interfering with the user's interaction — no additional interaction is required on part of a user to enable logging. RWELS is configurable and allows user-centric <b>event</b> <b>logging.</b> A usability analyst can choose the set of events to be captured and the pages of the website to be logged for a particular user. The <b>event</b> <b>logs</b> are dispatched through HTTP to the server where they are stored as text files. Users are identified uniquely and the <b>event</b> <b>logs</b> are associated with the user sessions...|$|R
40|$|Abstract. Today, <b>event</b> <b>logs</b> contain {{vast amounts}} of data that can easily {{overwhelm}} a human. Therefore, the mining of frequent patterns from <b>event</b> <b>logs</b> is an important system and network management task. This paper discusses the properties of <b>event</b> <b>log</b> data, analyses the suitability of popular mining algorithms for processing <b>event</b> <b>log</b> data, and proposes an efficient algorithm for mining frequent patterns from <b>event</b> <b>logs.</b> ...|$|R
40|$|Abstract Most {{information}} systems <b>log</b> <b>events</b> (e. g., transaction logs, audit trails) to audit and monitor the processes they support. At the same time, {{many of these}} processes have been explicitly modeled. For example, SAP R/ 3 <b>logs</b> <b>events</b> in transaction <b>logs</b> and there are EPCs (Event-driven Process Chains) describing the so-called reference models. These reference models describe how the system should be used. The coexistence of <b>event</b> <b>logs</b> and process models raises an interesting question: “Does the <b>event</b> <b>log</b> conform to the process model and vice versa?”. This paper demonstrates {{that there is not}} a simple answer to this question. To tackle the problem, we distinguish two dimensions of conformance: fitness (the <b>event</b> <b>log</b> may be the result of the process modeled) and appropriateness (the model is a likely candidate from a structural and behavioral point of view). Different metrics have been defined and a Conformance Checker has been implemented within the ProM Framework. ...|$|R
40|$|Abstract. Most {{information}} systems <b>log</b> <b>events</b> (e. g., transaction logs, audit trails) to audit and monitor the processes they support. At the same time, {{many of these}} processes have been explicitly modeled. For example, SAP R/ 3 <b>logs</b> <b>events</b> in transaction <b>logs</b> and there are EPCs (Event-driven Process Chains) describing the so-called reference models. These reference models describe how the system should be used. The coexistence of <b>event</b> <b>logs</b> and process models raises an interesting question: 2 ̆ 2 Does the <b>event</b> <b>log</b> conform to the process model and vice versa? 2 ̆ 2. This paper demonstrates {{that there is not}} a simple answer to this question. To tackle the problem, we distinguish two dimensions of conformance: fitness (the <b>event</b> <b>log</b> may be the result of the process modeled) and appropriateness (the model is a likely candidate from a structural and behavioral point of view). Different metrics have been defined and a Conformance Checker has been implemented within the ProM Framework...|$|R
40|$|The growing {{interest}} in process mining is fueled by the growing availability of event data. Process mining techniques use <b>event</b> <b>logs</b> to automatically discover process models, check conformance, identify bottlenecks and deviations, suggest improvements, and predict processing times. Lion's share of process mining research {{has been devoted to}} analysis techniques. However, the proper handling of problems and challenges arising in analyzing <b>event</b> <b>logs</b> used as input is critical for the success of any process mining effort. In this paper, we identify four categories of process characteristics issues that may manifest in an <b>event</b> <b>log</b> (e. g. process problems related to event granularity and case heterogeneity) and 27 categories of <b>event</b> <b>log</b> quality issues (e. g., problems related to timestamps in <b>event</b> <b>logs,</b> imprecise activity names, and missing events). The systematic identification and analysis of these issues calls for a consolidated effort from the process mining community. Five real-life <b>event</b> <b>logs</b> are analyzed to illustrate the omnipresence of process and <b>event</b> <b>log</b> issues. We hope that these findings will encourage systematic logging approaches (to prevent <b>event</b> <b>log</b> issues), repair techniques (to alleviate <b>event</b> <b>log</b> issues) and analysis techniques (to deal with the manifestation of process characteristics in <b>event</b> <b>logs)</b> ...|$|R
50|$|As {{with most}} {{national}} radio networks, 4MMM sometimes engages in networking of several shows. 4MMM also employs local automation overnight, where the computerised playlist system will play <b>logged</b> <b>events</b> automatically, {{eliminating the need}} for an announcer to be present.|$|R
5000|$|DILCA - Distributed IDMEF Logical Correlation Architecture : DILCA is a {{distributed}} logical correlation {{and reaction}} architecture featuring collection and correlation of IDMEF formatted <b>log</b> <b>events</b> (Intrusion Detection Message Exchange Format - RFC 4765) through a multi-step signature based system.|$|R
40|$|Abstract—Modern IT systems often produce {{large volumes}} of <b>event</b> <b>logs,</b> and <b>event</b> pattern {{discovery}} is an important log management task. For this purpose, data mining methods have been suggested in many previous works. In this paper, we present the LogCluster algorithm which implements data clustering and line pattern mining for textual <b>event</b> <b>logs.</b> The paper also describes an open source implementation of LogCluster. Keywords—event log analysis; mining patterns from <b>event</b> logs; <b>event</b> <b>log</b> clustering; data clustering; data mining I...|$|R
40|$|The Onyx Platform is a data {{processing}} framework, that utilizes a masterless coordination design and a centralized log through the ZooKeeper system. At large cluster sizes, the centralized log experiences performance issues {{due to the}} large amount of read and write requests. This thesis utilizes epidemic techniques for sharing <b>log</b> <b>events</b> in order to reduce read requests to the primary log nodes. An implementation of these techniques will be presented, together with an analysis of the results. Problems with actually applying the received <b>log</b> <b>events</b> to the local state made realistic performance testing impossible, but the actual epidemic message dissemination show some promising results {{with a high degree of}} connectivity and small average shortest path between nodes...|$|R
5000|$|The role also {{includes}} maintaining records and <b>logging</b> <b>events,</b> listing each backup that is run, each machine malfunction and program abnormal termination. Operators assist system administrators and programmers in testing and debugging of new systems and programs {{prior to their}} becoming production environments.|$|R
40|$|The paper {{presents}} a scalable method for learning probabilistic real-time automata (PRTAs), {{a new type}} of model that captures the dynamics of multi-dimensional <b>event</b> <b>logs.</b> In multi-dimensional <b>event</b> <b>logs,</b> <b>events</b> are described by several features instead of only one symbol. Moreover, it is not clear up front which events occur in an <b>event</b> <b>log.</b> The learning method to find a PRTA that models such an <b>event</b> <b>log</b> is based on the state merging of a prefix tree acceptor, which is guided by a clustering to determine the states of the automaton. To make the overall approach scalable, an online clustering method based on maximum frequent patterns (MFPs) is used. The approach is evaluated on a synthetic, a biological and a medical data set. The results show that the induction of automata using MFP-based clustering gives easy to understand and stable automata, but most importantly, makes it scalable to large data sets...|$|R
50|$|Logz.io {{provides}} a cloud-based log analytics service with additional {{features such as}} predictive fault detection, alerts, multi-user access and role definitions. The platform uses machine-learning algorithms to find critical <b>log</b> <b>events</b> before they impact operations, providing users with information about their systems and applications.|$|R
50|$|Timestamps are {{typically}} used for <b>logging</b> <b>events</b> or {{in a sequence}} of events (SOE), in which case each <b>event</b> in the <b>log</b> or SOE is marked with a timestamp. In filesystems, timestamp may mean the stored date/time of creation or modification of a file.|$|R
40|$|International audienceMany {{applications}} log a {{large amount}} of events continuously. Extracting interesting knowledge from <b>logged</b> <b>events</b> is an emerging active research area in data mining. In this context, we propose an approach for mining frequent events and association rules from <b>logged</b> <b>events</b> in XML format. This approach is composed of two-main phases: I) constructing a novel tree structure called Frequency XML-based Tree (FXT), which contains the frequency of events to be mined; II) querying the constructed FXT using XQuery to discover frequent itemsets and association rules. The FXT is constructed with a single-pass over logged data. We implement the proposed algorithm and study various performance issues. The performance study shows that the algorithm is efficient, for both constructing the FXT and discovering association rules...|$|R
