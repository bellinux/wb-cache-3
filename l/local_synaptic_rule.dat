0|417|Public
40|$|In machine learning, error {{back-propagation}} in multi-layer {{neural networks}} (deep learning) has been impressively successful in supervised and reinforcement learning tasks. As {{a model for}} learning in the brain, however, deep learning has long been regarded as implausible, since it relies in its basic form on a non-local plasticity rule. To overcome this problem, energy-based models with local contrastive Hebbian learning were proposed and tested on a classification task with networks of rate neurons. We extended this work by implementing and testing such a model with networks of leaky integrate-and-fire neurons. Preliminary results indicate {{that it is possible}} to learn a non-linear regression task with hidden layers, spiking neurons and a <b>local</b> <b>synaptic</b> plasticity <b>rule...</b>|$|R
40|$|Despite our {{extensive}} {{knowledge of}} biophysical properties of neurons, {{there is no}} commonly accepted algorithmic theory of neuronal function. Here we explore the hypothesis that single-layer neuronal networks perform online symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of the streamed data. By starting with the SNMF cost function we derive an online algorithm, which can be implemented by a biologically plausible network with local learning rules. We demonstrate that such network performs soft clustering of the data as well as sparse feature discovery. The derived algorithm replicates many known aspects of sensory anatomy and biophysical properties of neurons including unipolar nature of neuronal activity and <b>synaptic</b> weights, <b>local</b> <b>synaptic</b> plasticity <b>rules</b> and the dependence of learning rate on cumulative neuronal activity. Thus, we make a step towards an algorithmic theory of neuronal function, which should facilitate large-scale neural circuit simulations and biologically inspired artificial intelligence. Comment: 2014 Asilomar Conference on Signals, Systems and Computer...|$|R
40|$|Efficient path {{planning}} and navigation {{is critical for}} animals, robotics, logistics and transportation. We study a model in which spatial navigation problems can rapidly be solved in the brain by parallel mental exploration of alternative routes using propagating waves of neural activity. A wave of spiking activity propagates through a hippocampus-like network, altering the synaptic connectivity. The resulting vector field of synaptic change then guides a simulated animal to the appropriate selected target locations. We demonstrate that the navigation problem can be solved using realistic, <b>local</b> <b>synaptic</b> plasticity <b>rules</b> during a single passage of a wavefront. Our model can find optimal solutions for competing possible targets or learn and navigate in multiple environments. The model provides a hypothesis on the possible computational mechanisms for optimal {{path planning}} in the brain, {{at the same time}} it is useful for neuromorphic implementations, where the parallelism of information processing proposed here can fully be harnessed in hardware...|$|R
40|$|We {{show that}} the local Spike Timing-Dependent Plasticity (STDP) rule {{has the effect of}} {{regulating}} the trans-synaptic weights of loops of any length within a simulated network of neurons. We show that depending on STDP's polarity, functional loops are formed or eliminated in networks driven to normal spiking conditions by random, partially correlated inputs, where functional loops comprise synaptic weights that exceed a non-zero threshold. We further prove that STDP is a form of loop-regulating plasticity for the case of a linear network driven by noise. Thus a notable <b>local</b> <b>synaptic</b> learning <b>rule</b> makes a specific prediction about synapses in the brain in which standard STDP is present: that under normal spiking conditions, they should participate in predominantly feed-forward connections at all scales. Our model implies that any deviations from this prediction would require a substantial modification to the hypothesized role for standard STDP. Given its widespread occurrence in the brain, we predict that STDP could also regulate long range functional loops among individual neurons across all brain scales, up to, and including, the scale of global brain network topology...|$|R
40|$|Understanding how {{the brain}} learns to compute {{functions}} reliably, efficiently and robustly with noisy spiking activity is a fundamental challenge in neuroscience. Most sensory and motor tasks {{can be described as}} dynamical systems and could presumably be learned by adjusting connection weights in a recurrent biological neural network. However, this is greatly complicated by the credit assignment problem for learning in recurrent network, e. g. the contribution of each connection to the global output error cannot be determined based only on locally accessible quantities to the synapse. Combining tools from adaptive control theory and efficient coding theories, we propose that neural circuits can indeed learn complex dynamic tasks with <b>local</b> <b>synaptic</b> plasticity <b>rules</b> as long as they associate two experimentally established neural mechanisms. First, they should receive top-down feedbacks driving both their activity and their synaptic plasticity. Second, inhibitory interneurons should maintain a tight balance between excitation and inhibition in the circuit. The resulting networks could learn arbitrary dynamical systems and produce irregular spike trains as variable as those observed experimentally. Yet, this variability in single neurons may hide an extremely efficient and robust computation at the population level. Comment: In press in Neuron journa...|$|R
40|$|We propose {{and examine}} {{a model for}} how perisaccadic visual {{receptive}} field dynamics, observed {{in a range of}} primate brain areas such as LIP, FEF, SC, V 3, V 3 A, V 2 and V 1, may develop through a biologically plausible process of unsupervised visually guided learning. These dynamics are associated with remapping, which is the phenomenon where receptive fields anticipate the consequences of saccadic eye movements. We find that a neural network model using a <b>local</b> associative <b>synaptic</b> learning <b>rule,</b> when exposed to visual scenes in conjunction with saccades, can account for a range of associated phenomena. In particular, our model demonstrates predictive and presaccadic remapping, responsiveness shifts around the time of saccades, and remapping from multiple directions...|$|R
40|$|Recent {{studies have}} shown that {{synaptic}} unreliability is a robust and sufficient mechanism for inducing the stochasticity observed in cortex. Here, we introduce Synaptic Sampling Machines, a class of neural network models that uses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised learning. Similar to the original formulation of Boltzmann machines, these models {{can be viewed as a}} stochastic counterpart of Hopfield networks, but where stochasticity is induced by a random mask over the connections. Synaptic stochasticity plays the dual role of an efficient mechanism for sampling, and a regularizer during learning akin to DropConnect. A <b>local</b> <b>synaptic</b> plasticity <b>rule</b> implementing an event-driven form of contrastive divergence enables the learning of generative models in an on-line fashion. Synaptic sampling machines perform equally well using discrete-timed artificial units (as in Hopfield networks) or continuous-timed leaky integrate & fire neurons. The learned representations are remarkably sparse and robust to reductions in bit precision and synapse pruning: removal of more than 75 % of the weakest connections followed by cursory re-learning causes a negligible performance loss on benchmark classification tasks. The spiking neuron-based synaptic sampling machines outperform existing spike-based unsupervised learners, while potentially offering substantial advantages in terms of power and complexity, and are thus promising models for on-line learning in brain-inspired hardware...|$|R
40|$|AbstractIn {{this issue}} of Neuron, Stepanyants, Hof, and Chklovskii derive a simple {{mathematical}} formula to calculate, from measurable anatomical parameters, the capacity for neuronal wiring plasticity involving <b>local</b> <b>synaptic</b> rearrangements. Their work provides {{a deeper understanding of}} the potential contribution of structural plasticity to learning and memory...|$|R
3000|$|... and <b>synaptic</b> {{plasticity}} <b>rules</b> can {{be obtained}} using stochastic gradient ascent procedures for this task.|$|R
40|$|Grounding {{autonomous}} {{behavior in}} the nervous system is a fundamental challenge for neuroscience. In particular, the self-organized behavioral development provides more questions than answers. Are there special functional units for curiosity, motivation, and creativity? This paper argues that these features can be grounded in synaptic plasticity itself, without requiring any higher level constructs. We propose differential extrinsic plasticity (DEP) as a new <b>synaptic</b> <b>rule</b> for self-learning systems {{and apply it to}} a number of complex robotic systems as a test case. Without specifying any purpose or goal, seemingly purposeful and adaptive behavior is developed, displaying a certain level of sensorimotor intelligence. These surprising results require no system specific modifications of the DEP rule but arise rather from the underlying mechanism of spontaneous symmetry breaking due to the tight brain-body-environment coupling. The new <b>synaptic</b> <b>rule</b> is biologically plausible and it would be an interesting target for a neurobiolocal investigation. We also argue that this neuronal mechanism may have been a catalyst in natural evolution. Comment: 18 pages, 5 figures, 7 video...|$|R
40|$|Abstractâ€”We {{propose a}} simple retinomorphic neural network that {{consists}} of photoreceptors generating nonuni-form outputs for common optical inputs with random off-sets, an ensemble of noisyMcCulloch-Pitts neurons {{each of which has}} random threshold values, <b>local</b> <b>synaptic</b> connec-tions between the photoreceptors and the neurons with vari-able receptive fields (RFs), output cells, and <b>local</b> <b>synaptic</b> connections between the neurons and output cells. Through numerical simulations, we observed stochastic resonance among the proposed pixels. We calculated correlation val-ues between the optical inputs and the outputs {{as a function of the}} RF size and intensities of the random components in photoreceptors and the McCulloch-Pitts neurons, and then found nonzero optimal RF sizes as well as optimal noise in-tensities of the neurons under the nonidentical photorecep-tors. This implies that SR-based night-scope image sensors with an array of nonidentical photosensors would be devel-oped with less efforts to implement uniform pixel devices. 1...|$|R
40|$|We {{show how}} <b>synaptic</b> {{plasticity}} <b>rules</b> {{need to be}} matched to the statistics of stored patterns, and how recall dynamics need to be matched both to input statistics and to the plasticity rule itself {{in order to achieve}} optimal performance. In particular, for binary synapses with metastates we demonstrate {{for the first time that}} memories can be efficiently read out with biologically plausible network dynamics that we derive directly from the <b>synaptic</b> metaplasticity <b>rule</b> with virtually no free parameters...|$|R
40|$|The {{structure}} of <b>local</b> <b>synaptic</b> circuits {{is the key}} to understanding cortical function and how neuronal functional modules such as cortical columns are formed. The central problem in deciphering cortical microcircuits is the quantification of synaptic connectivity between neuron pairs. I present a theoretical model that accounts for the axon and dendrite morphologies of pre- and postsynaptic cells and provides the average number of synaptic contacts formed between them as a function of their relative locations in three-dimensional space. An important aspect of the current approach is the representation of a complex {{structure of}} an axonal/dendritic arbor as a superposition of basic structuresâ€”synaptic clouds. Each cloud has three structural parameters that can be directly estimated from twodimensional drawings of the underlying arbor. Using empirical data available in literature, I applied this theory to three morphologically different types of cell pairs. I found that, within a wide range of cell separations, the theory is in very good agreement with empirical data on (i) axonalâ€“dendritic contacts of pyramidal cells and (ii) somatic synapses formed by the axons of inhibitory interneurons. Since for many types of neurons plane arborization drawings are available from literature, this theory can provide a practical means for quantitatively deriving <b>local</b> <b>synaptic</b> circuits based on the actual observed densities of specific types of neurons and their morphologies. It can also have significant implications for computational models of cortical networks by making it possible to wire up simulated neural networks in a realistic fashion. Citation: Amirikian B (2005) A phenomenological theory of spatially structured <b>local</b> <b>synaptic</b> connectivity. PLoS Comp Biol 1 (1) : e 11...|$|R
40|$|Cognitive {{functions}} like motor planning {{rely on the}} concerted {{activity of}} multiple neuronal assemblies underlying still elusive computational strategies. During reaching tasks, we observed stereotyped sudden transitions (STs) between low and high multiunit activity of monkey dorsal premotor cortex (PMd) predicting forthcoming actions on a single-trial basis. Occurrence of STs was observed even when movement was delayed or successfully canceled after a stop signal, excluding a mere substrate of the motor execution. Anattractor model accounts for upward STs and high-frequency modulations of field potentials, indicative of <b>local</b> <b>synaptic</b> reverberation. Wefound in vivo compelling evidence that motor plans in PMd emerge from the coactivation of such attractor modules, heterogeneous in the strength of <b>local</b> <b>synaptic</b> self-excitation. Modules with strong coupling early reacted with variable times to weak inputs, priming a chain reaction of both upward and downward STs in other modules. Such web of "flip-flops" rapidly converged to a stereotyped distributed representation of the motor program, as prescribed by the long-standing theory of associative networks. Â© 2013 the authors...|$|R
40|$|The substantia gelatinosa (lamina II) of {{the spinal}} dorsal horn {{contains}} inhibitory and excitatory interneurons that {{are thought to}} {{play a critical role}} in the modulation of nociception. However, the organization of the intrinsic circuitry within lamina II remains poorly understood. We used glutamate uncaging by laser scanning photostimulation to map the location of neurons that give rise to <b>local</b> <b>synaptic</b> inputs to islet cells, a major class of inhibitory interneuron in lamina II. We also mapped the distribution of sites on the islet cells that exhibited direct (non-synaptic) responses to uncaging of excitatory and inhibitory transmitters. <b>Local</b> <b>synaptic</b> inputs to islet cells arose almost entirely from within lamina II, and these local inputs included both excitatory and inhibitory components. Furthermore, there was a striking segregation in the location of sites that evoked excitatory versus inhibitory synaptic inputs, such that inhibitory presynaptic neurons were distributed more proximal to the islet cell soma. This was paralleled in part by a differential distribution of transmitter receptor sites on the islet cell, in that inhibitory sites were confined to the peri-somatic region while excitatory sites were more widespread. This differential organization of excitatory and inhibitory inputs suggests a principle for the wiring of local circuitry within the substantia gelatinosa...|$|R
40|$|STDP (spike-timing-dependent {{synaptic}} plasticity) {{is thought}} to be a <b>synaptic</b> learning <b>rule</b> that embeds spike-timing information into a specific pattern of synaptic strengths in neuronal circuits, resulting in a memory. STDP consists of bidirectional long-term changes in synaptic strengths. This process includes long-term potentiation and long-term depression, which are dependent on the timing of presynaptic and postsynaptic spikings. In this review, we focus on computational aspects of signaling mechanisms that induce and maintain STDP as a key step toward the definition of a general <b>synaptic</b> learning <b>rule.</b> In addition, we discuss the temporal and spatial aspects of STDP, and the requirement of a homeostatic mechanism of STDP in vivo...|$|R
40|$|Mammalian {{thalamus}} is {{a critical}} site where early perception of sensorimotor signals is dynamically regulated by acetylcholine in a behavioral state-dependent manner. In this study, we examined how synaptic transmission is modulated by acetylcholine in auditory thalamus where sensory relay neurons form parallel lemniscal and nonlemniscal pathways. The former mediates tonotopic relay of acoustic signals, whereas the latter is involved in detecting and transmitting auditory cues of behavioral relevance. We report here that activation of cholinergic muscarinic receptors had opposite membrane effects on these parallel synaptic pathways. In lemniscal neurons, muscarine induced a sustained membrane depolarization and tonic firing by closing a linear K+ conductance. In contrast, in nonlemniscal neurons, muscarine evoked a membrane hyperpolarization by opening a voltage-independent K+ conductance. Depending {{on the level of}} membrane hyperpolarization and the strength of <b>local</b> <b>synaptic</b> input, nonlemniscal neurons were either suppressed or selectively engaged in detecting and transmitting synchronized synaptic input by firing a high-frequency spike burst. Immunohistochemical and Western blotting experiments showed that nonlemniscal neurons predominantly expressed M 2 muscarinic receptors, whereas lemniscal cells had a significantly higher level of M 1 receptors. Our data indicate that cholinergic modulation in the thalamus is pathway-specific. Enhanced cholinergic tone during behavioral arousal or attention may render synaptic transmission in nonlemniscal thalamus highly sensitive to the context of <b>local</b> <b>synaptic</b> activities...|$|R
40|$|The Author(s) 2010. This {{article is}} {{published}} with open access at Springerlink. com Abstract Low threshold voltage-gated T-type calcium channels {{have long been}} implicated in the electrical excitability and calcium signaling of cerebellar Purkinje neurons although the molecular composition, localization, and modulation of T-type channels within Purkinje cells have only recently been addressed. The specific functional roles that T-type channels play in <b>local</b> <b>synaptic</b> integration within Purkinje spines are also currently being unraveled. Overall, Purkinje neurons represent a powerful model system to explore the potential roles of postsynaptic T-type channels throughout the nervous system. In this review, we present an overview of T-type calcium channel biophysical, pharmacological, and physiological character-istics that provides a foundation for understanding T-type channels within Purkinje neurons. We also describe the biophysical properties of T-type channels in context of other voltage-gated calcium channel currents found within Purkinje cells. The data thus far suggest that one specific T-type isoform, Cav 3. 1, is highly expressed within Purkinje spines and both physically and functionally couples to mGluR 1 and other effectors within putative signaling microdomains. Finally, we discuss how the selective potentiation of Cav 3. 1 channels via activation of mGluR 1 by parallel fiber inputs affects <b>local</b> <b>synaptic</b> integration and how this interaction may relate to the overall excitability of Purkinje neuron dendrites...|$|R
40|$|The {{ability to}} carry out signal processing, classification, recognition, and {{computation}} in artificial spiking neural networks (SNNs) is mediated by their synapses. In particular, through activity-dependent alteration of their efficacies, synapses play {{a fundamental role in}} learning. The mathematical prescriptions under which synapses modify their weights are termed <b>synaptic</b> plasticity <b>rules.</b> These learning rules can be based on abstract computational neuroscience models or on detailed biophysical ones. As these rules are being proposed and developed by experimental and computational neuroscientists, engineers strive to design and implement them in silicon and en masse in order to employ them in complex real-world applications. In this paper, we describe analog very large-scale integration (VLSI) circuit implementations of multiple <b>synaptic</b> plasticity <b>rules,</b> ranging from phenomenological ones (e. g., based on spike timing, mean firing rates, or both) to biophysically realistic ones (e. g., calcium-dependent models). We discuss the application domains, weaknesses, and strengths of various representative approaches proposed in the literature, and provide insight into the challenges that engineers face when designing and implementing <b>synaptic</b> plasticity <b>rules</b> in VLSI technology for utilizing them in real-world applications. Mostafa Rahimi Azghadi, Nicolangelo Iannella, Said F. Al-Sarawi, Giacomo Indiveri, and Derek Abbot...|$|R
40|$|The subiculum, which {{provides}} the major hippocampal output, contains different cell types including weak/strong bursting and regular-spiking cells, and fast-spiking interneurons. These cellular populations play different {{roles in the}} generation of physiological rhythms and epileptiform activity. However, their intrinsic connectivity and the synaptic regulation of their discharge patterns remain unknown. In the present study, the <b>local</b> <b>synaptic</b> responses of subicular cell types were examined in vitro. To this purpose, slices were prepared at a specific orientation that permitted the antidromic activation of projection cells {{as a tool to}} examine local circuits. Patch recordings in cell-attached and whole-cell configurations were combined with neurobiotin labelling to classify cell types. Strong (â‰ˆ 75 %), but not weak (â‰ˆ 22 %), bursting cells typically fired bursts in response to <b>local</b> <b>synaptic</b> excitation, whereas the majority of regular-spiking cells (â‰ˆ 87 %) remained silent. Local excitation evoked single spikes in more than 70 % of fast-spiking interneurons. This different responsiveness was determined by intrinsic membrane properties and not by the amplitude and pharmacology of synaptic currents. Inhibitory GABAergic responses were also detected in some cells, typically as a component of an excitatory/inhibitory sequence. A positive correlation between the latency of the excitatory and inhibitory responses, together with the glutamatergic control (via non-NMDA receptors) of inhibition, suggested a local mechanism. The effect of local inhibition on synaptically activated firing of different cell types was evaluated. It is shown that projection bursting cells of the subiculum are strongly controlled by local inhibitory circuits...|$|R
5|$|Cooperativity is {{observed}} when two synapses are activated by weak stimuli incapable of inducing LTP when stimulated individually. But upon simultaneous weak stimulation, both synapses undergo LTP in a cooperative fashion. Synaptic tagging {{does not explain}} how multiple weak stimuli can result in a collective stimulus sufficient to induce LTP (this is explained by the postsynaptic summation of EPSPs described previously). Rather, synaptic tagging explains the ability of weakly stimulated synapses, none of which are capable of independently generating LTP, to receive the products of protein synthesis initiated collectively. As before, this may be accomplished through the synthesis of a <b>local</b> <b>synaptic</b> tag following weak synaptic stimulation.|$|R
40|$|Homeostatic {{synaptic}} plasticity describes {{the changes in}} synapse gain and function that occur in response to global changes in neuronal activity to maintain the stability of neuronal networks. In this review, we argue that a coordinated regulation of excitatory and inhibitory synaptic transmission is essential for maintaining CNS function while allowing both global and <b>local</b> changes in <b>synaptic</b> strength and connectivity. Therefore, we postulate that homeostatic {{synaptic plasticity}} depends on signaling cascades regulating in parallel the efficacy of glutamatergic and GABAergic transmission. Since neurotransmitter receptors interact closely with scaffolding proteins in the postsynaptic density, this coordinated regulation of excitatory and inhibitory synaptic transmission likely involves posttranslational modifications of scaffolding proteins, which in turn modulate <b>local</b> <b>synaptic</b> function. Here we review {{the current state of}} knowledge on the regulation of GABAA receptors and their main scaffolding protein gephyrin by posttranslational modifications; we outline future lines of research that might contribute to further our understanding of the molecular mechanisms regulating GABAergic synapse function and homeostatic plasticity...|$|R
40|$|Abstract:-RetinotopicNET is an {{efficient}} simulator for neural networks with retinotopic-like receptive fields. The system has two main characteristics: it is event-driven {{and it takes}} advantage of the retinotopic arrangement in the receptive fields. The dynamics of the simulator are driven by the spiking events of the simple integrate-and-fire neurons. By using an implicit <b>synaptic</b> <b>rule</b> to represent the synapses, RetinotopicNET achieves a great reduction of memory requirement for simulation. We show that under such conditions the system is fit for the simulation of very large networks of integrate-and-fire neurons. Furthermore we test RetinotopicNET in the simulation of a complex neural architecture for the ventral visual pathway. We prove that the system is linearly scalable with respect to the number of neurons in the simulation. Key-words:- Neural simulator; Retinotopy; Receptive fields; Event-driven; Spikes. 1...|$|R
40|$|Recent {{improvements}} in brain slice technology {{have made this}} biological preparation increasingly useful for examining pathophysiology of brain diseases in a tissue context. Brain slices maintain many aspects of in vivo biology, including functional <b>local</b> <b>synaptic</b> circuitry with preserved brain architecture, while allowing good experimental access and precise control of the extracellular environment, making them ideal platforms for dissection of molecular pathways underlying neuronal dysfunction. Importantly, these ex vivo systems permit direct treatment with pharmacological agents modulating these responses and thus provide surrogate therapeutic screening systems without recourse to whole animal studies. Virus or particle mediated transgenic expression can also be accomplished relatively easily to study the function of novel genes in a normal or injured brain tissue context...|$|R
40|$|Abstract. The {{selective}} fine-mesh {{modification of}} soma-dendrite (SD) neuronal membrane is analyzed. The mechanism of <b>local</b> <b>synaptic</b> controlled {{changes of the}} volt-conformation characteristics (VCC) of SD-membrane units is studied. These modified units, located between synapses; contain dimer receptive clusters, which can be in three conformations. The transitions between two of them result in energy accumulating and then it releasing that triggers endogenous mechanism of neuron excitation. Some part of the clusters, having VCC hysteretic loops before learning process, passes in the third conformation states, where hysteretic properties are lost as well as ability to generate the endogenous spikes. Remaining hysteretic units can read out the written information inversely {{in the form of}} "spontaneous " neuron spikes...|$|R
40|$|Changes of {{synaptic}} {{connections between}} neurons {{are thought to}} be the physiological basis of learning. These changes can be gated by neuromodulators that encode the presence of reward. We study a family of reward-modulated <b>synaptic</b> learning <b>rules</b> for spiking neurons on a learning task in continuous space inspired by the Morris Water maze. The <b>synaptic</b> update <b>rule</b> modifies the release probability of synaptic transmission and depends on the timing of presynaptic spike arrival, postsynaptic action potentials, as well as the membrane potential of the postsynaptic neuron. The family of learning rules includes an optimal rule derived from policy gradient methods as well as reward modulated Hebbian learning. The <b>synaptic</b> update <b>rule</b> is implemented in a population of spiking neurons using a network architecture that combines feedforward input with lateral connections. Actions are represented by a population of hypothetical action cells with strong mexican-hat connectivity and are read out at theta frequency. We show that in this architecture, a standard policy gradient rule fails to solve the Morris watermaze task, whereas a variant with a Hebbian bias can learn the task within 20 trials, consistent with experiments. This result does not depend on implementation details such as the size of the neuronal populations. Our theoretical approach shows how learning new behaviors can be linked to reward modulated plasticit...|$|R
40|$|Maximization of {{information}} transmission by a spiking-neuron model predicts changes of synaptic connections {{that depend on}} timing of pre- and postsynaptic spikes and on the postsynaptic membrane potential. Under the assumption of Poisson firing statistics, the <b>synaptic</b> update <b>rule</b> exhibits all {{of the features of}} the Bienenstockâ€“Cooperâ€“Munro rule, in particular, regimes of synaptic potentiation and depression separated by a sliding threshold. Moreover, the learning rule is also applicable to the more realistic case of neuron models with refractoriness, and is sensitive to correlations between input spikes, {{even in the absence of}} presynaptic rate modulation. The learning rule is found by maximizing the mutual information between presynaptic and postsynaptic spike trains under the constraint that the postsynaptic firing rate stays close to some target firing rate. An interpretation of the <b>synaptic</b> update <b>rule</b> in terms of homeostatic synaptic processes and spike-timing-dependent plasticity is discussed...|$|R
40|$|Abstractâ€”We {{describe}} a spiking neuronal network which allows <b>local</b> <b>synaptic</b> weights {{to be assigned}} to individual synapses. In previous implementations of neuronal networks, the biases that control the parameters of a particular synapse are global to all synapses of the same type regardless of the target neuron. In this new implementation, the parameters for a synapse are set by on-chip Digital-Analog-Converter (DAC) circuits, and the DACs are updated before the selected synapses are activated. Results from the fabricated chip show that the local weights are programmable and the DACs settle {{in the order of}} microseconds. These on-chip DACs allow the user to program a selected synaptic weight for connections between neurons and they can also be used for mismatch calibration. I...|$|R
40|$|In {{the visual}} pathway of frogs it is {{possible}} to apply low levels of NMDA chronically to the optic tectum and study the mechanisms underlying the stabilization of synapses developing within the CNS. Earlier studies (Cline and Constantine-Paton, 1990) found that chronic NMDA treatment of tecta innervated by two retinas results in a reduction of branching within the terminal arbors of retinal ganglion cells (RGCs). We now report that this same chronic NMDA treatment produces fine-structural changes in synaptic morphology as well as <b>local</b> <b>synaptic</b> rearrangements within the retinotectal neuropil. Chronic NMDA treatment of doubly innervated tecta was associated with a thickening or darkening of both pre- and postsynaptic densities. These changes in synapse morphology were restricted to the superficial neuropil of tect...|$|R
30|$|Memory {{networks}} with variable sparseness {{have been}} studied by Amit and Huang [15, 16] under a different learning paradigm in which old memories are gradually overwritten by new memories, and for several more involved <b>synaptic</b> (meta-)plasticity <b>rules.</b> There, inhomogeneity in the pattern sizes was shown to decrease the signal-to-noise ratio during recall as well.|$|R
5000|$|Beierlein Michael; Regehr Wade G (2006) <b>Local</b> interneurons {{regulate}} <b>synaptic</b> strength by retrograde {{release of}} endocannabinoids. Journal of Neuroscience 2006;26(39):9935-43.|$|R
40|$|National audienceThe cortex must {{permanently}} {{deal with}} stimuli {{coming from the}} environment, perceived through different sensors spatially separated. These stimuli converge in the cortex to be processed together in order to construct a multisensory and coherent view of the world. When somebody is hearing /ba/ and is simultaneously viewing the lips movements corresponding to /ga/, he is perceiving /da/. This phenomenon, known as McGurk effect, reveals the cross correlation between different modalities. Some cortical areas are mainly dedicated to the processing of a specific perception. These areas are topographically organized, meaning that two spatially close neurons respond to close stimuli. The {{purpose of this paper}} is to modify the BCM <b>synaptic</b> <b>rule</b> to obtain a self organization of a neuronal map. We introduce a feedback modulation of the learning rule, representing multimodal constraints of the environment. This feedback will be obtained by using a multimap and multilevel architecture of modality assembling...|$|R
40|$|Infusion of a GABA agonist (Reiter & Stryker, 1988) and {{infusion}} of an NMDA receptor antagonist (Bear et al., 1990), {{in the primary}} visual cortex of kittens during monocular deprivation, shifts ocular dominance toward the closed eye, in the cortical region near the infusion site. This reverse ocular dominance shift has been previously modeled by variants of a covariance <b>synaptic</b> plasticity <b>rule</b> (Bear et al., 1990; Clothiaux et al., 1991; Miller et al., 1989; Reiter & Stryker, 1988). Kasamatsu et al. (1997, 1998) showed that {{infusion of}} an NMDA receptor antagonist in adult cat primary visual cortex changes ocular dominance distribution, reduces binocularity, and reduces orientation and direction selectivity. This paper presents a novel account {{of the effects of}} these pharmacological treatments, based on the EXIN <b>synaptic</b> plasticity <b>rules</b> (Marshall, 1995), which include both an instar afferent excitatory and an outstar lateral inhibitory rule. Functionally, the EXIN plasticity rules enha [...] ...|$|R
40|$|This thesis {{presents}} a versatile {{study on the}} design and Very Large Scale Integration(VLSI) implementation of various <b>synaptic</b> plasticity <b>rules</b> ranging from phenomenological rules, to biophysically realistic ones. In particular, the thesis aims at developing novel spike timing-based learning circuits that advance the current neuromorphic systems, in terms of power consumption, compactness and synaptic modification (learning) abilities. Furthermore, the thesis investigates {{the usefulness of the}} developed designs and algorithms in specific engineering tasks such as pattern classification. To follow the mentioned goals, this thesis makes several original contributions to the field of neuromorphic engineering, which are briefed in the following. First, a programmable multi-neuron neuromorphic chip is utilised to implement a number of desired rate- and timing-based <b>synaptic</b> plasticity <b>rules.</b> Specific software programs are developed to set up and program the neuromorphic chip, in a way to show the required neuronal behaviour for implementing various <b>synaptic</b> plasticity <b>rules.</b> The classical version of Spike Timing Dependent Plasticity (STDP), as well as the triplet-based STDP and the rate-based Bienenstock-Cooper-Munro (BCM) rules are implemented and successfully tested on this neuromorphic device. In addition, the implemented triplet STDP learning mechanism is utilised to train a feedforward spiking neural network to classify complex rate-based patterns, with a high classification performance. In the next stage, VLSI designs and implementations of a variety of <b>synaptic</b> plasticity <b>rules</b> are studied and weaknesses and strengths of these implementations are highlighted. In addition, the applications of these VLSI learning networks, which build upon various <b>synaptic</b> plasticity <b>rules</b> are discussed. Furthermore, challenges in the way of implementing these rules are investigated and effective ways to address those challenges are proposed and reviewed. This review provides us with deep insight into the design and application of <b>synaptic</b> plasticity <b>rules</b> in VLSI. Next, the first VLSI designs for the triplet STDP learning rule are developed, which significantly outperform all their pair-based STDP counterparts, in terms of learning capabilities. It is shown that a rate-based learning feature is also an emergent property of the new proposed designs. These primary designs are further developed to generate two different VLSI circuits with various design goals. One of these circuits that has been fabricated in VLSI as a proof of principle chip, aimed at maximising the learning performanceâ€”but this results in high power consumption and silicon real estate. The second design, however, slightly sacrifices the learning performance, while remarkably improves the silicon area, as well as the power consumption of the design, in comparison to all previous triplet STDP circuits, as well as many pair-based STDP circuits. Besides, it significantly outperforms other neuromorphic learning circuits with various biophysical as well as phenomenological plasticity rules, not only in learning but also in area and power consumption. Hence, the proposed designs in this thesis can play significant roles in future VLSI implementations of both spike timing and rate based neuromorphic learning systems with increased learning abilities. These systems offer promising solutions for a wide set of tasks, ranging from autonomous robotics to brain machine interfaces. Thesis (Ph. D.) [...] University of Adelaide, School of Electrical and Electronic Engineering, 201...|$|R
40|$|Active {{dendrites}} can {{be viewed}} as linear classi"ers augmented by a few second-order product terms representing multiplicative synaptic interactions [4]. To quantify the degree to which <b>local</b> <b>synaptic</b> interactions could augment the memory capacity of a neuron, we have studied the family of `subsampled quadratica (SQ) classi"ers. Each SQ classi"er is a linear classi"er augmented by a subset k of the K"O(dï¿½) second-order product terms available in d dimensions. Using a randomized classi"cation task, we show that the error rate of an SQ classi"er depends only on: (1) the product term ratio p"k/K, which identi"es a family of isomorphic classi"ers, and (2) the number of bits contained in the SQ classi"er's speci"cation. Finally, we quantify the increase in memory capacity of any SQ classi"er relative to its linear counterpart...|$|R
40|$|Fura- 2 and imaging {{technology}} {{were used to}} detect intracellular Ca 2 + changes in CA 1 pyramidal cells in hippocampal slices. During focal synaptic stimulation, one or more highly localized regions of Ca 2 + elevation (hot spots) were detected in the dendrites. Ca 2 + spread {{from the center of}} hot spots with properties consistent with diffusion. Several lines of evidence indicate that these hot spots were due to Ca 2 + entry through N-methyl-D-aspartate synaptic channels. The spatial and temporal resolution of the method was sufficient to detect the response of single hot spots to single stimuli, thus providing a real-time method for monitoring <b>local</b> <b>synaptic</b> activity. Using this method, we show that synapses on the same dendrite differ in their probability of response and in their facilitation properties...|$|R
