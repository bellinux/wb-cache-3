49|593|Public
50|$|The bilingual {{method of}} foreign {{language}} teaching {{was developed by}} C.J. Dodson (1967) as a counterpart of the audiovisual method. In both methods the preferred basic texts are dialogues accompanied by a picture strip. The bilingual method, however, advocates two revolutionary principles {{based on the results}} of scientifically controlled experiments in primary and secondary schools. In contrast to the audiovisual method and the direct method, the printed text is made available from the very beginning and presented simultaneously with the spoken sentence to allow learners to see the shape of individual words. Also, from the outset meanings are conveyed bilingually as utterance equivalents in the manner of the sandwich technique, thus avoiding meaningless and hence tedious parroting of the <b>learning</b> <b>input.</b> The pictures are seen primarily as an aid to recall and practice of the related dialogue sentences rather than as conveyors of meaning. The mother tongue is again used in the oral manipulation of grammatical structures, i.e. in bilingual pattern drills.|$|E
40|$|With the {{increasing}} need {{to adapt to}} new environments, data-driven approaches {{have been developed to}} estimate terrain traversability by learning the rover’s response on the terrain based on experience. Multiple learning inputs are often used to adequately describe the various aspects of terrain traversability. In a complex learning framework, {{it can be difficult to}} identify the relevance of each <b>learning</b> <b>input</b> to the resulting estimate. This paper addresses the suitability of each <b>learning</b> <b>input</b> by systematically analyzing the impact of each input on the estimate. Sensitivity Analysis (SA) methods provide a means to measure the contribution of each <b>learning</b> <b>input</b> to the estimate variability. Using a variance-based SA method, we characterize how the prediction changes as one or more of the input changes, and also quantify the prediction uncertainty as attributed from each of the inputs in the framework of dependent inputs. We propose an approach built on Analysis of Variance (ANOVA) decomposition to examine the prediction made in a near-to-far learning framework based on multi-task GP regression. We demonstrate the approach by analyzing the impact of driving speed and terrain geometry on the prediction of the rover’s attitude and chassis configuration in a Marsanalogue terrain using our prototype rover Mawson...|$|E
40|$|Abstract: This paper {{presents}} a new totally unsupervised and 90 % effective stemming approach for classical Arabic. This stemming {{is meant to}} be a preparatory step to an unsupervised root (i. e., radicals) extraction. As a <b>learning</b> <b>input,</b> our stemming system requires no linguistic knowledge but a plain classical Arabic text. Once the <b>learning</b> <b>input</b> analyzed, our stemming system is able to extract the strongest segment of a given length, namely the stem. We start by a definition of the targeted stem, then, we show how our system performs about 90 % true positives after a leaning of less than 15000 words. Unlike the other unsupervised approaches, ours does not suppose the perfectness of the input text and deals efficiently with the eventual (practically very frequent) misspellings. The test corpus we have used is an ultimate reference in the classical Arabic and its labeling has been rigorously done by a team of experts...|$|E
50|$|The {{issue of}} catastrophic interference, comes about when {{learning}} is sequential. Sequential training involves the network learning an input-output pattern until the error is reduced below a specific criterion, then training the network on {{another set of}} input-output patterns. Specifically, a backpropagation network will forget information if it first <b>learns</b> <b>input</b> A and then next <b>learns</b> <b>input</b> B. It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns both inputs-output patterns at the same time, i.e. as AB. Weights are only changed when the network is being trained and not when the network is being tested on its response.|$|R
30|$|Applying {{unsupervised}} feature <b>learning</b> to <b>input</b> data {{using either}} RICA or SFT, improves clustering performance.|$|R
40|$|Student {{learning}} {{research literature}} {{has shown that}} students' learning approaches {{are influenced by the}} learning context (Evans, Kirby, & Fabrigar, 2003). Of the many contextual factors, assessment has been found to have the most important influence on the way students go about learning. For example, assessment that is perceived to required a low level of cognitive abilities will more likely elicit a learning approach that concentrate on reproductive learning activities. Moreover, assessment demand will also interact with learning approach to determine academic performance. In this paper an assessment specific model of learning comprising presage, process and product variables (Biggs, 2001) was proposed and tested against data obtained from a sample of introductory economics students (n= 434). The model developed was used to empirically investigate the influence of <b>learning</b> <b>inputs</b> and <b>learning</b> approaches on academic performances across assessment types (essay assignment, multiple choice question exam and exam essay). By including learning approaches in the learning model, the mechanism through which <b>learning</b> <b>inputs</b> determine academic performance was examined. Methodological limitations of the study will also be discussed...|$|R
40|$|A {{forward-backward}} training algorithm for parallel, self-organizing hierachical {{neural networks}} (PSHNN 2 ̆ 7 s) is described. Using linear algebra, it is {{shown that the}} forward-backward training of an n-stage PSHNN until convergence {{is equivalent to the}} pseudo-inverse solution for a single, total network designed in the leastsquares sense with the total input vector consisting of the actual input vector and its additional nonlinear transformations. These results are also valid when a single long input vector is partitioned into smaller length vectors. A number of advantages achieved are small modules for easy and fast learning, parallel implementation of small modules during testing, faster convergence rate, better numerical error-reduction, and suitability for <b>learning</b> <b>input</b> nonlinear transformations by other neural networks. The backpropagation (BP) algorithm is proposed for <b>learning</b> <b>input</b> nonlinearities. Better performance in terms of deeper minimum of the error function and faster convergence rate is achieved when a single BP network is replaced by a PSHNN of equal complexity in which each stage is a BP network of smaller complexity than the single BP network...|$|E
40|$|Curative factors or {{therapeutic}} events {{beneficial to}} group members {{are a major}} concern in group psychotherapy. Yalom (1975) writes extensively on curative factors in his well-known text. Corsini and Rosenberg (1955), {{in their study of}} 300 pre- 1955 articles, identified 175 statements which they subsumed under nine categories, or curative factors. Yalom and his colleagues (Liebermann et al., 1972; Yalom et al., 1967, 1975) researched the relative importance of curative factors as perceived by group members. Based partly on these works, Yalom identified 60 curative items which he clustered into 12 curative factor categories: altruism, group cohesiveness, uni-versality, interpersonal <b>learning</b> (<b>input),</b> interpersonal learning (output), guidance, catharsis, identification, family reenact-ment, self-understanding, instillation of hope, and existential factor. Subjects in the Yalom et al. study (1975) were well-educated, middle-socioeconomic-class outpatients in long-term therapy. Using the 60 items and a Q-sort method, group members ranked interpersonal <b>learning</b> (<b>input),</b> catharsis, and cohesiveness as the three important curative factors, and family reenactment, guidance, and identification as least important. Other researchers using Yalom’s method have reported similar findings with various kinds of groups in variou...|$|E
40|$|We {{describe}} a rhythmic interaction mechanism for mobile devices. A PocketPC with a three {{degree of freedom}} linear acceleration meter is used as the experimental platform for data acquisition. Dynamic Movement Primitives are used to learn the limit cycle behavior associated with the rhythmic gestures. We outline the open technical and user experience challenges {{in the development of}} usable rhythmic interfaces. Author Keywords rhythmic interaction, mobile devices, dynamic movement primitives, <b>learning,</b> <b>input</b> via inertial sensors ACM Classification Keywords H 5. m. Miscellaneous, I. 3. 6 Methodology and Technique...|$|E
40|$|Economics {{education}} research {{studies conducted in}} the UK, USA and Australia to investigate the effects of <b>learning</b> <b>inputs</b> on academic performance have been dominated by the input-output model (Shanahan and Meyer, 2001). In the Student Experience of Learning framework, however, the link between <b>learning</b> <b>inputs</b> and outputs is mediated by students' learning approaches which in turn are influenced by {{their perceptions of the}} learning contexts (Evans, Kirby, & Fabrigar, 2003). Many learning inventories such as Biggs' Study Process Questionnaires and Entwistle and Ramsden' Approaches to Study Inventory have been designed to measure approaches to academic learning. However, there is a limitation to using generalised learning inventories in that they tend to aggregate different learning approaches utilised in different assessments. As a result, important relationships between learning approaches and learning outcomes that exist in specific assessment context(s) will be missed (Lizzio, Wilson, & Simons, 2002). This paper documents the construction of an assessment specific instrument to measure learning approaches in economics. The post-dictive validity of the instrument was evaluated by examining the association of learning approaches to students' perceived assessment demand in different assessment contexts...|$|R
40|$|In {{this paper}} the {{robustness}} {{of a class}} of learning control algorithms to state disturbances, output noise, and errors in initial conditions is studied. We present a simple learning algorithm and exhibit, via a concise proof, bounds on the asymptotic trajectory errors for the <b>learned</b> <b>input</b> and the corresponding state and output trajectories. Furthermore, these bounds are continuous functions of the bounds on the initial condition errors, state disturbance, and output noise, and the bounds are zero {{in the absence of}} these disturbances...|$|R
50|$|The {{architecture}} of the network is a three-dimensional matrix (FIG.1) for which the three dimensions represent, respectively, a mini-column, a macro-column and a cortical area network. Each minicolumn is linked with a specific feature of the input stimulus and contains neurons resonating with specific values of the feature. The RF neurons in the mini-column are competitive modules (CM) working with a controlled WTA (Winner Takes All) mechanism. A macro-column is composed of mini-columns linked through synaptic connections that build paths representing configurations of the <b>learned</b> <b>input</b> patterns.The input vector distance from the stored prototype is measured using the BOX-distance with extreme flexibility because the influence field of a single vector component (one side of the Hypercube) can be different for any component (FIG.2).|$|R
40|$|MBA Professional ReportIt is well {{documented}} that the Defense Acquisition System has habitually fallen short of providing timely, cost-cognizant procurements in support of America's warfighter requirements. Hence, this MBA study employed a systems approach to more credibly pinpoint improvement areas in the Defense Acquisition System {{through the use of}} systems theory and an organizational systems model as foundational analytical tools. The results of this study identified system incongruencies with success factors, system direction, <b>learning,</b> <b>input</b> variability, and task differentiation, which are likely sources of common, ill-conceived outcomes of the Defense Acquisition System. This analysis, through its recommendations, also laid groundwork for future, solution-oriented studies of how to suitably design Defense Acquisition System processes and structure strategies in support of warfighter needs. It is {{well documented}} that the Defense Acquisition System has habitually fallen short of providing timely, cost-cognizant procurements in support of America’s warfighter requirements. Hence, this MBA study employed a systems approach to more credibly pinpoint improvement areas in the Defense Acquisition System through the use of systems theory and an organizational systems model as foundational analytical tools. The results of this analysis identified system incongruencies with success factors, system direction, <b>learning,</b> <b>input</b> variability, and task differentiation, which are likely sources of common, ill-conceived outcomes of the Defense Acquisition System. This analysis, through its recommendations, also laid groundwork for future, solution-oriented studies of how to suitably design Defense Acquisition System processes and structure strategies in support of warfighter needs. Captain, United States Air Forc...|$|E
40|$|Abstract — We {{present a}} novel {{self-organizing}} Particle Swarm algorithm, SOSwarm, that adopts unsupervised <b>learning.</b> <b>Input</b> vectors are projected onto a lower dimensional map space producing a visual {{representation of the}} input data {{in a manner similar}} to the Self-Organizing Map (SOM) artificial neural net-work. Particles in the map react to the input data by modifying their velocities using a standard Particle Swarm Optimization update function, and therefore organize themselves spatially within fixed neighborhoods in response to the input training vectors. SOSwarm is successfully applied to four benchmark classification problems from the UCI Machine Learning repos-itory with the novel SOSwarm algorithm outperforming or equaling the best reported results on all four of the problems analyzed. I...|$|E
30|$|The target {{information}} contained in the image is filtered by feature extraction to perform information processing according to the user’s needs. The biggest disadvantage of traditional machine learning is that many parameters need to be set manually during the learning process. This shortcoming is especially noticeable when dealing with big data and high-dimensional complex data. Deep learning technology belongs to machine learning [16 – 20], based on artificial neural network for data feature extraction. The original information is expressed as a feature vector through feature <b>learning,</b> <b>input</b> into a sub-learning system (such as classifier, decision maker), and then the sample is classified or detected. Feature learning {{is a means of}} detecting and classifying input samples. Through feature learning of input samples, feature extraction is automatically realized.|$|E
5000|$|Community members learn through broad eager {{attention}} and contribution during events, {{with responsibility for}} the result. Members also <b>learn</b> through <b>input</b> from their community, revealing community expectations.|$|R
40|$|Abstract: This paper demonstrates, that input {{patterns}} can be {{encoded in}} the synaptic weights by local Hebbian delay-learning of spiking neurons (SN), where, after learning, the firing time of an output neuron reflects {{the distance of}} the evaluated pattern to its <b>learned</b> <b>input</b> pattern thus realizing a kind of RBF behavior. Furthermore, the paper shows, that temporal spike-time coding and Hebbian learning is a viable means for unsupervised computation in a network of SNs, as the network is capable of clustering realistic data. Then, two versions- with and without embedded micro-controllers- of a SNN are implemented for the aforementioned task...|$|R
50|$|Interior design {{students}} <b>learn</b> through <b>input</b> {{in design}} and construction studios, in workshops and through theory. Many of the design problems selected for their projects are actual projects offered by practicing design firms.|$|R
40|$|International audienceFuzzy {{inference}} systems (FIS) {{are likely}} to play a significant part in system modeling, provided that they remain interpretable following learning from data. The aim {{of this paper is}} to set up some guidelines for interpretable FIS learning, based on practical experience with fuzzy modeling in various fields. An open source software system called FisPro has been specifically designed to provide generic tools for interpretable FIS design and learning. It can then be extended with the addition of new contributions. This work presents a global approach to design data-driven FIS that satisfy certain interpretability and accuracy criteria. It includes fuzzy partition generation, rule <b>learning,</b> <b>input</b> space reduction and rule base simplification. The FisPro implementation is discussed and illustrated through several detailed case studies...|$|E
40|$|We {{introduce}} a new theoretical framework, based on Shannon's communication theory and on Ashby's law of requisite variety, suitable for artificial agents using predictive learning. The framework quantifies the performance constraints of a predictive adaptive controller {{as a function of}} its learning stage. In addition, we formulate a practical measure, based on information flow, that can be applied to adaptive controllers which use hebbian <b>learning,</b> <b>input</b> correlation learning (ICO/ISO) and temporal difference learning. The framework is also useful in quantifying the social division of tasks in a social group of honest, cooperative food foraging, communicating agents. Simulations are in accordance with Luhmann, who suggested that adaptive agents self-organise by reducing the amount of sensory information or, equivalently, reducing the complexity of the perceived environment from the agents perspective...|$|E
40|$|We have {{developed}} a computerized system that can aid in the radiologist's diagnosis in the detection and classification of coronary artery diseases. The technique employs a neural network to analyze 201 TImyocardial SPECT bull's-eye images. This multi-layer feed-forward neural network with a backpro-pagation algorithm has 256 input units (pattern: compressed 16 x 16 -matrix images), 5 - 140 units in a single hidden layer, and eight output units (diagnosis: one normal and seven different types of abnormalities). The neural network was taught using pairs of training (<b>learning)</b> <b>input</b> data (bull's-eye &quot;EXTENT &quot; image) and desired output data (&quot;correct &quot; diagno sis). The effects of the numbers of hidden units and learning iterations in the network on the recognition performance were examined. In our initial stage, {{the results show that}} the recognition performance of the neural network is better tha...|$|E
5000|$|... {{adaptive}} to <b>learn</b> {{from the}} <b>input</b> patterns and dialects of the users ...|$|R
40|$|Changes to myriad synapses {{throughout}} the brain must be coordinated {{every time a}} memory is established, and these synapses must be appropriately reactivated every time it is remembered. Once stored, memories can be recognized (when re-experiencing a <b>learned</b> <b>input)</b> or recalled (e. g., via different input, such as a name evoking memory of a face, or a scene evoking memories of an experience) by many routes. We remember what tables are {{as well as we}} remember a specific table, and we recognize objects despite seeing them from quite different angles, different lighting, different settings. Computational simulations of synaptic modifications (e. g., long term potentiation; se...|$|R
5000|$|Thinc: A machine {{learning}} library optimized for CPU usage and deep <b>learning</b> with text <b>input.</b>|$|R
40|$|The goal of {{the present}} set of studies is to explore the {{boundary}} conditions of category transfer in causal learning. Previous {{research has shown that}} people are capable of inducing categories based on causal <b>learning</b> <b>input,</b> and they often transfer these categories to new causal learning tasks. However, occasionally learners abandon the learned categories and induce new ones. Whereas previously {{it has been argued that}} transfer is only observed with essentialist categories in which the hidden properties are causally relevant for the target effect in the transfer relation, we here propose an alternative explanation, the unbroken mechanism hypothesis. This hypothesis claims that categories are transferred from a previously learned causal relation to a new causal relation when learners assume a causal mechanism linking the two relations that is continuous and unbroken. The findings of two causal learning experiments support the unbroken mechanism hypothesis...|$|E
30|$|The {{purpose of}} this study {{addressed}} a need to examine and improve current assessments of listening at the tertiary level. In this study, two listening tests, dynamic and static, were examined and assessed. Static LC tests have been used in language research and assessment. This type of listening seems to be at odds with the way teaching listening is carried out in class in which learners are supposed to be engaged in joint activities to comprehend listening. Static assessment (SA) rests on engaging the test-takers in working on the test individually with no scaffolding on the part of mediators or test-takers. SA may be more convenient and practical than DA, especially in large-scale situations (Lantolf & Poehner 2010). In static or traditional LC tests, there is no interest allocated to the joint interactions of the learners required for approaching the <b>learning</b> <b>input</b> (Leung 2007; Lidz & Gindis 2003).|$|E
40|$|NeuroDraughts is a {{draughts}} playing program {{similar in}} approach to NeuroGammon and NeuroChess [Tesauro, 1992, Thrun, 1995]. It uses an {{artificial neural network}} trained by the method of temporal difference learning to learn by self-play {{how to play the}} game of draughts. This paper discusses the relative contribution of board representation, search depth, training regime, architecture and run time parameters to the strength of the TDplayer produced by the system. Keywords: Temporal Difference <b>Learning,</b> <b>Input</b> representation, Search, Draughts. 1 Introduction The Temporal Difference (TD) family of Reinforcement Learning procedures [Sutton, 1988], have been applied successfully to a number of domains over the last decade - including playing games. Notable among these are NeuroGammon and NeuroChess [Tesauro, 1992, Thrun, 1995]. However, the principle of TD is not new, having first been applied by Samuels, who pioneered the idea of updating evaluations based on successive predictions in his [...] ...|$|E
40|$|We {{present a}} new variant of Generalized Learning Vector Quantization (GRLVQ) in a {{computer}} vision scenario. A version with incrementally added prototypes {{is used for the}} non-trivial case of high-dimensional object recognition. Training is based upon a generic set of standard visual features, the <b>learned</b> <b>input</b> weights are used for iterative feature pruning. Thus, prototypes and input space are altered simultaneously, leading to very sparse and task-specific representations. The effectiveness of the approach and the combination of the incremental variant together with pruning was tested on the Coil 100 database. It exhibits excellent performance with regard to codebook size, feature selection and recognition accuracy. Key words: object recognition, relevance learning, feature selection, incremental learning vector quantization, adaptive metric...|$|R
40|$|Neural {{networks}} {{were used to}} analyze a complex simulated radar environment which contains noisy radar pulses generated by many different emitters. The neural network used is an energy minimizing network (the BSB model) which forms energy minima - attractors in the network dynamical system - based on <b>learned</b> <b>input</b> data. The system first determines how many emitters are present (the deinterleaving problem). Pulses from individual simulated emitters give rise to separate stable attractors in the network. Once individual emitters are characterized, {{it is possible to}} make tentative identifications of them based on their observed parameters. As a test of this idea, a neural network was used to form a small data base that potentially could make emitter identifications...|$|R
40|$|Since {{the late}} 1960 s, {{economics}} educators have carried out many research studies designed to explain variations in learning outcomes in economics. Most {{of these have}} utilised the input-output approach. Underpinning this approach {{is the assumption that}} there is a direct connection between <b>learning</b> <b>inputs</b> and <b>learning</b> output. However, the results obtained in these studies have mostly been found to be inconsistent. This paper argues for a re-focusing of research on the process of learning in economics. It reports on the development of an instrument to measure economics students’ perceptions of key elements of their learning context. Confirmatory factor analysis validates a four-factor model. Differences in students’ perceptions of three economics units in this study will also be discussed...|$|R
40|$|Abstract. In {{a recent}} {{publication}} [1], {{it was shown}} that a biologically plausible RCN (Representation-burden Conservation Network) in which conservation is achieved by bounding the summed represent-ation-burden of all neurons at constant 1, is effective in learning stationary vector quantization. Based on the conservation principle, a new approach for designing a dynamic RCN for processing both stationary and non-stationary inputs is introduced in this paper. We show that, {{in response to the}} input statistics changes, dynamic RCN improves its original counterpart in incremental learning capability as well as in self-organizing the network structure. Performance comparisons between dynamic RCN and other self-development models are also presented. Simulation results show that dynamic RCN is very effective in training a near-optimal vector quantizer in that it manages to keep a balance between the equiprobable and equidistortion criterion. Key words: dynamic network, self-development networks, competitive <b>learning,</b> <b>input</b> density map-ping, vector quantization, conscience principl...|$|E
40|$|The aim of {{this article}} is to analyze the {{existing}} e-learning environment and to detect the different learning styles of an individual. In general, the e-learning system provides assistance to the learner’s in terms of providing learning contents that spans from textual information to multimedia information. Though the existing system tries to maximize the aptitude of learning in users, it fails to act in a dynamic way. To bring the dynamic nature into the system, the learning behavior of an individual has to be modeled. In order to expedite the process, it is necessary to automate the process of detecting learning style based on the learning behavior. This paper attempts to uncover the specialties of the existing e-learning learning style models and cognitive strategies towards understanding and proposing the learning style for the learners based on the popular Felder Silverman Learning Style Model (FSLSM) <b>learning</b> <b>input</b> and dimensions...|$|E
40|$|Incremental {{training}} {{has been used}} for GA-based classifiers in a dynamic environment where training samples or new attributes/classes become available over time. In this paper, ordered incremental genetic algorithms (OIGAs) are proposed to address the incremental training of input attributes for classifiers. Rather than <b>learning</b> <b>input</b> attributes in batch as with normal GAs, OIGAs learn input attributes one after another. The resulting classification rule sets are also evolved incrementally to accommodate the new attributes. Furthermore, attributes are arranged in different orders by evaluating their individual discriminating ability. By experimenting with different attribute orders, different approaches of OIGAs are evaluated using four benchmark classification data sets. Their performance is also compared with normal GAs. The simulation results show that OIGAs can achieve generally better performance than normal GAs. The order of attributes does {{have an effect on the}} final classifier performance where OIGA training with a descending order of attributes performs the best...|$|E
40|$|Although {{extraordinarily}} complexes, {{the mental}} processes {{can be seen}} as products of the neuronal dynamical system. In this context, biological observations make it possible to emit the conjecture that recognition of a form or a stimulus leads to a reduction of neuronal dynamics. This paper proposes a generic model for the study of such dynamics by learning random stimuli. We implement a Hebb-like learning rule, which reinforces the innovation in a network stimulated by a random <b>input.</b> The network <b>learns</b> to react specifically to one or more <b>learned</b> <b>inputs.</b> An estimation of the networks reactivity after learning brings encouraging results in terms of capacity. Then the question of dynamical coding is evoked in terms of limit cycles associated to specific patterns...|$|R
40|$|Using {{statistical}} mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying {{the results to}} learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires ¡£¢¥¤§¦© ¨-training examples to <b>learn</b> <b>input</b> features of � order (¤ is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP {{can be considered as}} an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm. ...|$|R
40|$|AbstractThis paper {{presents}} a P-type iterative learning control (ILC) scheme with initial state learning {{for a class}} of α(0 ≤α< 1) fractional-order nonlinear systems. By introducing the λ-norm and using a generalized Gronwall inequality, the sufficient condition for the robust convergence of the tracking errors with respect to initial positioning errors under P-type ILC is obtained. Based on this convergence condition, the learning gain of the initial <b>learning</b> and <b>input</b> <b>learning</b> updating law can be determined. Unlike the existing methods, the ILC scheme will not fix the initial value on the expected condition {{at the beginning of}} each iteration. Finally, the validity of the methods are verified by a numerical example...|$|R
