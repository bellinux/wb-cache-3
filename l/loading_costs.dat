13|651|Public
5000|$|Baggage {{charges for}} checked bags to offset baggage {{handling}} and <b>loading</b> <b>costs</b> ...|$|E
50|$|The {{two terms}} have a {{specific}} meaning in commercial law and cannot be altered. But the FOB terms {{do not need to}} be used, and often are not. In this case the specific terms of the agreement can vary widely, in particular which party, buyer or seller, pays for the <b>loading</b> <b>costs</b> and shipment costs, and/or where responsibility for the goods is transferred. The last distinction is important for determining liability or risk of loss for goods lost or damaged in transit from the seller to the buyer.|$|E
5000|$|Indicating [...] "FOB port {{means that}} the seller pays for {{transportation}} of the goods to the port of shipment, plus <b>loading</b> <b>costs.</b> The buyer pays the cost of marine freight transport, insurance, unloading, and transportation from the arrival port to the final destination. The passing of risks occurs when the goods are loaded on board {{at the port of}} shipment. For example, [...] "FOB Vancouver" [...] indicates that the seller will pay for transportation of the goods to the port of Vancouver, and the cost of loading the goods on to the cargo ship (this includes inland haulage, customs clearance, origin documentation charges, demurrage if any, origin port handling charges, in this case Vancouver). The buyer pays for all costs beyond that point, including unloading. Responsibility for the goods is with the seller until the goods are loaded on board the ship. Once the cargo is on board, the buyer assumes the risk.|$|E
40|$|The {{absence of}} {{investor}} {{reaction to the}} poor performance of mutual funds is a widely reported phenomenon. This paper investigates the role of <b>load</b> <b>costs</b> as {{an explanation for the}} phenomenon and concludes that back-end load fees are an obstacle to reaction. We find that investors with a high likelihood of undergoing a liquidity crisis, preferring liquidity in decision making, act contrary to the reaction hypothesis, and investors with broader investment horizons do not react to poor performances {{due to the fact that}} they are “imprisoned” by back-end load fees. Mutual Fund, Performance Reaction, <b>Load</b> <b>Costs,</b> Investor Behaviour...|$|R
40|$|Abstract—This paper {{presents}} a new methodology to solve transmission expansion planning (TEP) problems based on Evolution Strategies (ES), but other heuristics {{are also used}} to assist the search process. The TEP problem includes {{the search for the}} least cost solution, bearing in mind investments and interruption costs. Unreliability costs are considered through the index LOLC – Loss of <b>Load</b> <b>Cost.</b> Moreover, the dynamic nature of the TEP is accounted for by the proposed methodology. Case studies on a small test and on a real sub-transmission network (CEMIG Company, Brazil) are presented and discussed. Index Terms—Transmission expansion planning, evolution strategies, metaheuristics, generation and transmission reliability, loss of <b>load</b> <b>cost.</b> I...|$|R
40|$|Abstract. By {{utilizing}} {{interruptible load}} management {{can be quickly}} and flexibly to eliminate {{the root causes of}} congestion on the line, based on low-bidding interruptible load have priority in electricity markets. This paper selects the interrupt interruptible load at the same time taking into account the sensitivity and interruptible <b>load</b> <b>costs</b> factor. By building mathematical models of congestion sensitivity, and combined with interruptible <b>load</b> <b>costs</b> form the new impact factors, as the basis of choice of interruptible load adjustments needed. A congestion management model was proposed with the goal of minimizing the congestion elimination cost, and has attempted to find the solution to it. Finally, a practical case is designed to validate the economic and validity of the method. And have achieved the expected effect...|$|R
40|$|Graduation date: 1978 In {{order to}} {{appraise}} timber, {{it is necessary}} to estimate logging costs with reasonable accuracy. The {{purpose of this study is}} to provide a rethod for estimating log <b>loading</b> <b>costs</b> in the Intermountain Region. Nine different log loaders were studied loading from cold decks on six USDA. Forest Service timber sales in the Intermountain Region. Regression analysis indicated that loading time per truck was a linear function of pieces per truck. It also indicated a significant difference in loading times between hydraulic loaders and cable loaders. Regression analysis of sample truck scale data for each sale indicated that both the number of pieces loaded per truck and the volume per truck were functions of average piece size. The results also suggested that timber species affected volume per truck. The time study regression equation and the sample truck scale regression equations were combined to obtain production equations for both cable and hydraulic loaders as a function of average piece size. Then estimated owning and operating costs for a typical hydraulic loader were combined with the production equation to obtain a relationship for <b>loading</b> <b>costs</b> per unit volume a a function of average piece size and hours of annual use. These costs were compared with costs obtained using the USDP Forest Service andbook (Forest Service, 1977). Results of this comparison indicated that the USDA Forest Service appraised <b>loading</b> <b>costs</b> did not vary nearly as much with average piece size as did the cost estimated in this paper...|$|E
40|$|This paper {{identifies}} {{three major}} issues facing worst-case execution time (WCET) reduction algorithms on adaptable architectures {{based on research}} carried out for the MCGREP- 2 CPU project. The issues are exposing more instruction level parallelism (ILP) in code, reduc-ing <b>loading</b> <b>costs</b> for the memory and processing elements used to reduce WCET, and making use of application-specific hardware. Potential difficulties {{in each of these}} areas are identified and possible solutions are proposed. ...|$|E
30|$|The WV ranks {{second in}} the nation for {{explosives}} consumption (Apodaca 2010). Almost 350, 000 tonnes were used in WV surface coal mining in 2009. Considering an average price of blasting agents (bulk emulsions, slurry and ANFO) of $ 1.14 per kg, the surface coal mines in WV spent almost $ 400 million for explosives alone. This does not include additional blasting accessories such as detonators, boosters, detonating cords, lead lines, additional costs for labor, explosives truck-delivery, and shot services. The addition of the drilling, digging and <b>loading</b> <b>costs</b> for the blasted material amounts to a significant overall cost to mine operators.|$|E
50|$|TVPlus {{users can}} watch KBO, {{provided}} that the user (thru text) subscribes to a low-cost service load from an ABS-CBNmobile SIM card, which can be reloaded weekly. PPV <b>loads</b> <b>cost</b> higher than the regular KBO load, which may include a bundled free data allowance from ABS-CBN's iWanTV online service.|$|R
50|$|Monmouth {{was five}} {{miles west of}} the Forest of Dean, but the poor {{transport}} facilities kept the price of coal in the town high. In 1802 a 2-ton wagon <b>load</b> <b>cost</b> upwards of 28 shillings. 10 wagons and carts and 300 horses and mules were constantly in use bringing coal to the town in 1802.|$|R
40|$|Load {{balancing}} is {{an important}} prerequisite to efficiently execute dynamic computations on parallel computers. In this context, this project has focussed on two topics: balancing dynamically generated work <b>load</b> <b>cost</b> efficiently in a network and partitioning graphs to equally distribute connected tasks on the processing nodes while reducing the communication overhead. We summarize new insights and results in these areas...|$|R
40|$|The California Department of Transportation (Caltrans) {{has used}} dowel bar {{retrofit}} (DBR) on several projects. Caltrans has experienced both success and {{problems with this}} pavement pres-ervation method. The primary question at the end is: if DBR performs as expected, is it the most cost-efficient solution? This {{presents the results of}} a Life Cycle Cost Analysis (LCCA) project comparing DBR with grinding and asphalt overlay. The performance assumptions were based on observed performance in the field and under heavy-vehicle simulator <b>loading.</b> <b>Costs</b> were col-lected from industry and Caltrans construction cost records. The analysis assumed the typical Caltrans practice of using nighttime closures to minimize road user delay. The analysis was performed using Caltrans LCCA procedures based on use of the Federal Highway Administra-tion’s (FHWA’s) software RealCost. This study used a 40 -year analysis period. It fits the plan-ning horizon for the activities considered and meets the recommendations of the FHWA. Sensi-tivity analysis was performed considering these variables: Initial remaining life: This takes into account the structural condition of the pavement that is a candidate for DBR. The analysis considered 10, 20, and 30 years of expected fatigue life re-maining. Grinding life: This captures scenarios for the interval between grinding in the absence of DBR...|$|E
40|$|Purpose – From the {{perspectives}} of the probable replacement of the national calamity funds by multi-peril grassland insurance, {{the purpose of this}} paper is to estimate demand for grassland production insurance. Design/methodology/approach – A discrete stochastic programming model with a three-year planning horizon was used to run simulations for farms raising suckler cows primarily with grasslands. In this model, the annual area insured and some production decisions are optimized under grasland yield uncertainty, with possible ex post production-system adjustments. The effects of insurance loading cost (14 levels), insurance coverage level (three levels), risk aversion (two levels) and stock levels (forage and animal stocks vary according to grassland yields and to farm management of the previous years) were analyzed. Findings – The results show that grassland insurance could be used as a flexible risk management tool, when farm becomes vulnerable to fodder shortfall. According to previous years’ grassland yields and to the subsequent states of hay stock and animal liveweight, the area insured could vary between nearly the none and full. Farmers with low-average stocking rate and important hay storage capacity have less incentive to buy grassland insurance. The author also demonstrates that for a given loading cost, more insurance is purchased at a coverage level of 70 percent of average yield than at higher coverage levels. The cost of self-insurance increases for important and rare losses while multi-peril grassland insurance premium decreases. Higher levels of risk aversion also raise the quantity of insurance subscribed. Eventually, insurance price is a key factor. Almost no insurance is bought for <b>loading</b> <b>costs</b> greater than 1. 1 under low-risk aversion and for <b>loading</b> <b>costs</b> greater than 1. 3 under moderate risk aversion. Research limitations/implications – The willingness to pay for insurance could have been overestimated for different reasons. First, basis risks have not been introduced in the simulation framework. Although the Forage Production Index performed quite well, basis risks are high enough to trigger inappropriate indemnifications in some cases. Consequences of these risks should be estimated in further research. Second, other self-insurance options and public emergency measures such as subsidized loan or reduction in social security contributions should also be considered to assess and reduce farmers vulnerability to risks. Practical implications – The launching of the multi-peril grassland insurance is likely to be successful thanks to the 65 percent of public subsidies on insurance premiuml. However, considering that the loading cost is likely to be high and that demand for grassland production insurance is rather low, multi-peril grassland production insurance may struggle to continue unsubsidized. Originality/value – This paper provides a framework that enables to estimate demand for grassland production insurance factoring in substitution with self-insurance and taking into account successive risks...|$|E
40|$|Forestry best {{management}} practices (BMPs) are used to reduce sedimentation from forest stream crossings. Three BMP treatments (BMP−, BMP-std, and BMP+) were applied to three forest road stream crossings (bridge, culvert, and ford). BMP− did not meet existing BMP guidelines, BMP-std met standard recommendations, and BMP+ treatments exceeded recommendations. Following BMP applications, three simulated rainfall intensities (low, medium, and high) were applied in order to evaluate sediment delivery from crossing type and BMP level. During rainfall simulation, sediment concentrations (mg/L) were collected with automated samplers and discharge (L/s) was estimated to calculate total sediment <b>loading.</b> <b>Costs</b> of stream crossings and BMP levels were also quantified. Mean sediment associated with the three stream crossings were 3. 38, 1. 87, and 0. 64 Mg for the BMP−, BMP-std, and BMP+ levels, respectively. Ford, culvert, and bridge crossings produced 13. 04, 12. 95, and 0. 17 Mg of sediment during construction, respectively. BMP enhancement was more critical for sediment control at the culvert and ford crossings than at the bridge. Respective costs for BMP−, BMP-std, and BMP+ levels were $ 5, 368, $ 5, 658, and $ 5, 858 for the bridge; $ 3, 568, $ 4, 166 and $ 4, 595 for the culvert; and $ 180, $ 420 and $ 1, 903 for the ford. Costs and sediment values suggest that current standard BMP levels effectively reduce stream sediment while minimizing costs...|$|E
50|$|The {{original}} {{football field}} ran perpendicular to today's configuration, and the fifty yards of gridiron nearest the current scoreboard are essentially {{the result of}} a substantial landfill operation by Joseph M. Crowley during the mid-1930s. The above landfill involved several men and a horse to gradually level the countless truckloads of dirt, rocks, bricks, concrete, and asphalt (each <b>load</b> <b>cost</b> La Salle about twenty-five cents).|$|R
5000|$|Sales {{outsourcing}} {{is expected}} to be cheaper than the fully <b>loaded</b> <b>cost</b> of employing salespeople, but calculating the cost comparison over time is far from straightforward. Nevertheless, replacing fixed costs with variable costs is attractive to budget-holders. [...] However, unlike many forms of outsourcing, the advantages of sales outsourcing does not often come from saving costs but rather increasing revenue or providing speed of response or flexibility.|$|R
30|$|When {{walking through}} the 3 D map, a current {{position}} and its closest RV are found, and then loaded are all the textures at the appropriate resolutions that belong to the nine RVs, that is, closest RV and eight neighboring RVs. The reason why the neighboring RVs are loaded is that <b>loading</b> <b>cost</b> can be reduced and scenes can smoothly change when the user moves {{to the area of}} another RV.|$|R
40|$|This paper compares two {{different}} approaches to large scale data anal-ysis, namely MapReduce and parallel database management systems. Both approaches use a cluster of nodes to compute expensive tasks in parallel. The strengths of MapReduce are fault tolerance, storage-system indepen-dence, flexibility and simplicity. While MapReduce can process the data on-the-fly by simply loading it into a distributed file system, the load-ing phase of parallel database management systems can take a long time. Once the data is loaded, parallel database management systems are ro-bust, support {{a high degree of}} parallelism and provide high-performance, since they have been developed and improved for over two decades. The user has to think about the fundamental trade-off between data <b>loading</b> <b>costs</b> and task response time. If the dataset is static rather than dynamic and the data has to be queried frequently, a parallel DBMS may be prefer-able. In contrast, if the data is dynamic and the user is interested in a few queries, the approach of MapReduce can be very useful. Another interest-ing difference is the failure model. While MapReduce makes extensively use of a fine grained failure model, parallel DBMSs prefer performance at the cost of possible re-executions of the whole task. If the computa-tion takes a long time or the cluster consists of hundreds or thousands of machines, the approach of MapReduce may be preferable. ...|$|E
40|$|This paper {{examines}} whether myopia (misperception of {{the long-term}} care (LTC) risk) and private insurance market <b>loading</b> <b>costs</b> can justify social LTC insurance and/or the subsidization of private insurance. We use a two-period model wherein individuals di¤er in three unobservable characteristics: level of productivity, survival probability and degree of ignorance concerning the risk of LTC (the former two being perfectly positively correlated). The decentralization of a �rst-best allocation requires that LTC insurance premiums of the myopic agents are subsidized (at a �Pigouvian�rate) and/or that there is public provision of the appropriate level of LTC. The support for the considered LTC policy instruments is less strong in a second-best setting. When social LTC provision is restricted to zero, a myopic agent�s tax on private LTC insurance premiums involves a tradeo¤ between paternalistic and redistributive (incentive) considerations and {{we may have a}} tax as well as a subsidy on private LTC insurance. Interestingly, savings (which goes untaxed in the �rst-best but plays the role of self-insurance in the second-best) is also subject to (positive or negative) taxation. Social LTC provision is never second-best optimal when private insurance markets are fair (irrespective of the degree of the proportion of myopic individuals and their degree of misperception). At the other extreme, when the loading factor in the private sector is su¢ ciently high, private coverage is completely crowded out by public provision. For intermediate levels of the loading factors, the solution relies on both types of insurance...|$|E
40|$|Skyrocketing {{health care}} costs {{in recent years have}} posed {{challenges}} for all businesses, but small firms and their workers are at a particular disadvantage. Declines in insurance coverage among workers in small businesses (especially businesses with less than 10 workers) have driven much of the drop in employer-sponsored health insurance coverage nationwide. It is no surprise then that the effect of health reform on small business has become a flashpoint in the ongoing health reform debate. This brief highlights the challenges faced by small businesses and the positive effect of Senate Majority Leader Reid’s health reform bill (as released on November 18, 2009) on the ability of small firms to provide quality, affordable insurance coverage to their workers. It concludes by reviewing evidence from the Congressional Budget Office that shows major savings for many small businesses and their workers. This brief documents the burden that small businesses face in providing health insurance coverage for their workers: Small firms are less likely to offer health insurance to their workers than larger firms. It is dwindling coverage in small firms that is driving much of the national decline in workplace insurance coverage. Low coverage rates among small firms are due to many elements that make purchasing insurance much more expensive for small businesses than for larger firms, including an inability to offer attractive risk pools to potential insurers, high administrative and <b>loading</b> <b>costs,</b> and little competition in insurer markets. This brief also finds {{that the vast majority of}} small businesses stand to gain from the legislation currently being considered by the Senate, and that small businesses as a group are some of the largest winners from health reform. Major improvements would include...|$|E
50|$|In this unlockable job, the {{assignment}} {{is to build}} a model of the Statue of Liberty. Given $215, the player must be frugal, since each <b>load</b> <b>costs</b> a little money. The price of the load varies, so the player must study price trends and load up when the price is right. The goal is to deliver all the sets of loads without going bankrupt, and see the celebratory animation afterwards.|$|R
40|$|The {{absence of}} {{investor}} {{reaction to the}} poor performance of mutual funds is a widely reported phenomenon. This article investigates the role of <b>load</b> <b>costs</b> as {{an explanation for the}} phenomenon and concludes that back-end load fees are an obstacle to reaction. We found evidence consistent with the hypothesis that medium and long-term investors do not react to poor performances {{due to the fact that}} they are 'imprisoned' by back-end load fees. ...|$|R
40|$|The {{research}} {{was a case}} study at PT Usaha Loka Malang titled “Layout Analysis to minimize material handling cost at PT Usaha Loka Malang. The research aimed {{to find out the}} material handling distance <b>load</b> <b>cost,</b> so that it could give alternative of production facility layout in order to reduce material handling flow and shorten the distance. Analytical tool used to find out whether the layout used by PT Usaha Loka has been efficient in moving load at every department and also analyzing the production facility layout of wood processing at PT Usaha Loka Malang was block diagram. It was done because of the existence of quantitative data whether load, distance, or accurate departments amount. From the block diagram analysis, the first layout used by PT Usaha Loka Malang had distance load 15, 015, 000, was so different with the automatic layout which had 10, 465, 000 load. The automatic layout would add efficiency in changing load. According to above conclusion, there would be better if the facility layout on the wood processing production of PT Usaha Loka Malang changed as the research told to reduce the distance <b>load</b> <b>cost.</b> ...|$|R
40|$|During 1996, the Los Alamos National Laboratory (LANL) {{developed}} two transuranic (TRU) waste workoff {{strategies that}} were estimated to save $ 270 - 340 M through accelerated waste workoff and {{the elimination of}} a facility. The planning effort included a strategy to assure that LANL would have a significant quantity (3000 + drums) of TRU waste certified for shipment to the Waste Isolation Pilot Plant (WIPP) beginning in April of 1998, when WIPP was projected to open. One of the accelerated strategies can be completed in less than ten years through a Total Optimization of Parameters Scenario ({open_quotes}TOPS{close_quotes}). {open_quotes}TOPS{close_quotes} fully utilizes existing LANL facilities and capabilities. For this scenario, funding was estimated to be unconstrained at $ 23 M annually to certify and ship the legacy inventory of TRU waste at LANL. With {open_quotes}TOPS{close_quotes} the inventory is worked off in about 8. 5 years while shipping 5, 000 drums per year at a total cost of $ 196 M. This workoff includes retrieval from earthen cover and interim storage costs. The other scenario envisioned funding at the current level with some increase for TRUPACT II <b>loading</b> <b>costs,</b> which total $ 16 M annually. At this funding level, LANL estimates it will require about 17 years to work off the LANL TRU legacy waste while shipping 2, 500 drums per year to WIPP. The total cost will be $ 277 M. This latter scenario decreases the time for workoff by about 19 years from previous estimates and saves an estimated $ 190 M. In addition, the planning showed that a $ 70 M facility for TRU waste characterization was not needed. After the first draft of the LANL strategies was written, Congress amended the WIPP Land Withdrawal Act (LWA) to accelerate the opening of WIPP to November 1997. Further, the No Migration Variance requirement for the WIPP was removed. This paper discusses the LANL strategies as they were originally developed. 1 ref., 3 figs., 2 tabs...|$|E
40|$|A {{three-phase}} {{approach was}} developed for allocating specific mission tasks to the onboard crew or the ground support team in the Spacelab project. Phase one involved a list of potential operational tasks anticipated for Spacelab missions, phase two, {{the development of a}} set of criteria for allocating task-performance location, phase three, the combination of these criteria to allow tasks to be arranged in order of priority. The task-allocation criteria included crew safety, work <b>load,</b> <b>cost,</b> efficiency, weight/space, accessibility, complexity, response time, and capability...|$|R
40|$|People {{often become}} unaware and forget {{to switch off}} lights, {{electric}} fans before leaving the room. This switched on electric <b>loads</b> <b>cost</b> several amount of power. A remote controlled method of power saving has been implemented by using the voltage driving the vibration motor of the mobile phone so that user can switch off the household loads by certain number of ring counts while giving missed call to a dedicated mobile phone even after they forget to switch off while leaving...|$|R
50|$|Synon kept precise {{productivity}} metrics {{during the}} internal development of its SMA accounting system. In total, 2,385 days of effort were expended on development and QA over a 14-month period, {{which resulted in}} the creation of 2.42 million lines of HLL code (excluding comments) in 2,081 programs. This is equivalent to the production of 1,016 lines of fully tested and documented code per person per day. The all-in, fully <b>loaded</b> <b>cost</b> (including management, design and end-user documentation) was £416 per program.|$|R
40|$|Environmental {{consciousness}} {{has gained}} {{more and more}} interest in recent years, and product life cycle design that aims to maximize total performance while minimizing its environmental <b>load</b> and <b>costs</b> should be implemented. To achieve that, {{the rise and fall}} in product value along life cycles should be evaluated properly. This paper proposes a practical evaluation method for the product value along life cycle by correlating it with product functionalities, and design guideline for maximizing product performance through product life cycle with balancing its value, environmental <b>load,</b> and <b>costs...</b>|$|R
40|$|This study {{presents}} {{an application of}} H. 264 video coding technology in the digital monitor system, in which H. 264 {{was used as a}} new method. And now the majority of monitor system use MPEG 1 /MPEG 2 for coding the video while consumes large disk and net <b>load</b> <b>cost.</b> For getting the goal of real time data compression, we proposed three sides to enhance the computing speed: the first is optimization of selection of inter-frame mode, the second is tune the computation structure of interpolation, the last is assembly optimization with MMX and SSE...|$|R
40|$|This {{paper is}} {{reporting}} an analysis energy use on Biotechnology Laboratory in Puspiptek Area. Analysis of electrical energy use concern power demand and power factor improvement. Power demand improvement {{can be done}} by reducing power purchase close to maximum power that might be happened. It is verybeneficial for reducing fix <b>load</b> <b>cost.</b> Power factor improvement as part of energy saving can reduce losses at distribution line, motor, transformer, reduce installed capacity, and also avoid penalty factor. Optimization of transformer operation can also reduce core and copper winding losses. According to the analysis gave theresult of saving potential is Rp. 218, 72 million/year...|$|R
30|$|This {{section is}} focus on {{formulations}} on operation characteristics of shapeable load and the removable one. The analyzed characteristics include <b>load</b> shifting <b>cost</b> curve, <b>load</b> shifting position and constraints for load shape and electricity consumption.|$|R
30|$|We {{propose a}} family of {{exponential}} functions that characterizes the buyers’ varying degrees of criticality, thus generalizing the value of lost <b>load</b> with <b>costs</b> associated {{to the risk of}} failed delivery (“Critical & tolerant buyers” section).|$|R
40|$|A simple {{methodology}} {{to estimate}} photovoltaic system size and life-cycle costs in stand-alone applications is presented. It {{is designed to}} assist engineers at Government agencies in determining the feasibility of using small stand-alone photovoltaic systems to supply ac or dc power to the load. Photovoltaic system design considerations are presented {{as well as the}} equations for sizing the flat-plate array and the battery storage to meet the required <b>load.</b> <b>Cost</b> effectiveness of a candidate photovoltaic system is based on comparison with the life-cycle cost of alternative systems. Examples of alternative systems addressed are batteries, diesel generators, the utility grid, and other renewable energy systems...|$|R
30|$|The {{problem is}} that the {{potential}} of smart metering may not benefit consumers if they are not well informed about the advantages and possibilities of these technologies. Moreover, customers need to be interested in developing the potential benefits that are possible thanks to the deployment of these “enabling” tools in the current decade. Moreover, customers can be engaged in DR in response to an economic signal (e.g. energy price or incentives) or in response to the need for sustainability, but to do so they need information about their possibilities and potential (i.e. they need information about their <b>load,</b> <b>costs</b> and their potential for response).|$|R
40|$|AbstractEnergy {{consumption}} in production and costs are gaining growing concern in production management. One major cost driver concerning electrical energy is often peak load management, motivated through peak <b>load</b> <b>costs.</b> Peak <b>loads</b> {{are caused by}} multiple production machines running energy intensive process patterns simultaneously. Although production planning assigns production tasks to certain time slots, the exact execution times - and therefore the times certain energy consumptions are conducted – are determined by shop floor control. Consequently, peak load management can achieve best results when applied by shop floor control/management. In this paper, a load management approach developed for a Siemens plant is introduced, using a decentralized, agent based approach...|$|R
