4|0|Public
30|$|Neural {{networks}} {{are composed of}} basic units somewhat analogous to neurons. These units are linked with each other using the connection whose strength is modifiable {{as a result of}} a learning process or algorithm. Each of these units were integrated independently (in parallel) the information provided by its synapses in order to evaluate its state of activation. The unit response was then a linear or nonlinear function of its activation. Linear algebra concepts are used, in general, to analyze linear units, with eigenvectors and eigenvalues being the core concepts involved. This analysis made clear about the strong similarity between linear neural networks and the general linear model developed by statisticians. The linear models presented here are the perceptron and the <b>linear</b> <b>associator.</b> The behavior of nonlinear networks could be described within the framework of optimization and approximation techniques with dynamical systems.|$|E
40|$|Neural {{networks}} {{are composed of}} basic units somewhat analogous to neurons. These units are linked to each other by connections whose strength is modifiable {{as a result of}} a learning process or algorithm. Each of these units integrates independently (in parallel) the information provided by its synapses in order to evaluate its state of activation. The unit response is then a linear or nonlinear function of its activation. Linear algebra concepts are used, in general, to analyze linear units, with eigenvectors and eigenvalues being the core concepts involved. This analysis makes clear the strong similarity between linear neural networks and the general linear model developed by statisticians. The linear models presented here are the perceptron, and the <b>linear</b> <b>associator.</b> The behavior of nonlinear networks can be described within the framework of optimization and approximation techniques with dynamical systems (e. g., like those used to model spin glasses). One of the main notio [...] ...|$|E
40|$|MATLAB. ??????????, ?? 1) ??? ??? ???????????? ????????? ?? ????????? ?? ?????? 40 % ???? ??? 1 -?????? ?????? ???????????? ? ???????????? ??????? ??????-????????, ???? ????????? ??????? ? ???; 2) ??????????? ??????? ?? ?????? ??? 1 -?????? ?? ???????????? ???????????? ???????????? ??????? ? ??????? ????????? ???????????? ????????? ?????????? ?? ?????? ???????????????? ???????; ???? ?? ?????? ?????????? ??????? ????????????? ?????????????. In {{order to}} create a medical image {{recognition}} system, we should take into account the range of possible changes of input signal that comes from the object. In this regard, the main requirement for pattern recognition is to provide a classifier which would be invariant under various transformations. The problem of image classification is solved experimentally based on the ART 1 -network system designed in MATLAB environment. It has been established that: 1) in case of not more than 40 % of noise, for image classification ART 1 -network selects recorded in the associative memory prototype vector which is most correlated therewith; 2) the associative memory based on ART 1 -network performs equivalently to the associative memory {{in the form of a}} single layer binary <b>linear</b> <b>associator</b> based on pseudoinverse learning rule and it is unable to perform the functions of the invariant classifier. ????? ??????? ??????? ????????????? ??????????? ???????????, ?????????? ????????? ???????? ????????? ?????????????? ???????? ???????, ???????????? ?? ??????? ??????????. ? ????? ? ???? ???????? ??????????? ??? ????????????? ??????? ???????? ???????? ?????? ??????????????, ??????? ??? ?? ???????????? ???????????? ????????? ?????????????. ?????? ????????????? ??????????? ?????? ???????????????? ?? ?????? ART 1 -???? ? ????? ??????? MATLAB. ???????????, ??? 1) ??? ????????????? ??????????? ??? ??????? ?? ????? 40 % ???? ??? 1 -???? ???????? ??????????????? ? ????????????? ?????? ??????-????????, ??????? ?????? ????? ??????????? ? ???; 2) ????????????? ?????? ?? ?????? ??? 1 ???? ?? ????????????? ???????????? ????????????? ?????? ? ???? ????????? ???????????? ????????? ?????????? ?? ?????? ??????????????? ???????; ??? ?? ???????? ????????? ??????? ????????????? ??????????????...|$|E
40|$|Cognitive {{functions}} rely on {{the extensive}} use of information stored in the brain, and the searching for the relevant information for solving some problem {{is a very complex}} task. Human cognition largely uses biological search engines, and we assume that to study cognitive function we need to understand the way these brain search engines work. The approach we favor is to study multi-modular network models, able to solve particular problems that involve searching for information. The building blocks of these multimodular networks are the context dependent memory models we have been using for almost 20  years. These models work by associating an output to the Kronecker product of an input and a context. Input, context and output are vectors that represent cognitive variables. Our models constitute a natural extension of the traditional <b>linear</b> <b>associator.</b> We show that coding the information in vectors that are processed through association matrices, allows for a direct contact between these memory models and some procedures that are now classical in the Information Retrieval field. One essential feature of context-dependent models is that they are based on the thematic packing of information, whereby each context points to a particular set of related concepts. The thematic packing can be extended to multimodular networks involving input-output contexts, in order to accomplish more complex tasks. Contexts act as passwords that elicit the appropriate memory to deal with a query. We also show toy versions of several ‘neuromimetic’ devices that solve cognitive tasks as diverse as decision making or word sense disambiguation. The functioning of these multimodular networks can be described as dynamical systems at the level of cognitive variables...|$|E

