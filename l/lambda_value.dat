41|159|Public
5000|$|A product {{introduced}} by Ensinger, called insulbar LO is a lambda (thermal conductivity) optimised solution to thermal insulation profiles. Its <b>lambda</b> <b>value</b> is only 0.18 W/m.K ...|$|E
5000|$|Lambda {{functions}} are function objects of an implementation-dependent type; this type's name is only {{available to the}} compiler. If the user wishes to take a lambda function as a parameter, the type must be a template type, or they must create a [...] or a similar object to capture the <b>lambda</b> <b>value.</b> The use of the [...] keyword can help store the lambda function,auto my_lambda_func = &(int x) { /*...*/ };auto my_onheap_lambda_func = new auto(=(int x) { /*...*/ }); ...|$|E
30|$|Table  5 {{shows the}} results of {{discriminant}} analysis for all the five items belonging to the first-ranked dimension, i.e., reliability. Items 11 and 16 have the significance values as 0.000 and <b>lambda</b> <b>value</b> less compared to other items in reliability dimension (i.e., item 12, item 16, item 23).|$|E
5000|$|... (define (make-counter <b>value)</b> (<b>lambda</b> (...) (set! <b>value</b> (+ value 1)) value));;; use the counter(define c (make-counter 10))(c) --> 11(c) --> 12 ...|$|R
40|$|We outline basic {{properties}} of a symmetric random walk in one dimension, {{in which the}} length of the nth step equals lambda^n, with lambda oo, the probability that the endpoint is at x, P_{lambda}(x;N), approaches a limiting distribution P_{lambda}(x) that has many beautiful features. For lambda< 1 / 2, the support of P_{lambda}(x) is a Cantor set. For 1 / 2 <=lambda< 1, there is a countably infinite set of <b>lambda</b> <b>values</b> for which P_{lambda}(x) is singular, while P_{lambda}(x) is smooth for almost all other <b>lambda</b> <b>values.</b> In the most interesting case of lambda=(sqrt{ 5 }- 1) / 2 =g, P_g(x) is riddled with singularities and is strikingly self-similar. The self-similarity is exploited to derive a simple form for the probability measure M(a,b) = int_a^b P_g(x) dx...|$|R
40|$|Recently, {{combined}} carbon and hydrogen isotope fractionation investigations {{have emerged as}} {{a powerful tool for}} the characterization of reaction mechanisms relevant for the removal of organic pollutants. Here, we applied this approach in order to differentiate benzene biodegradation pathways under oxic and anoxic conditions in laboratory experiments. Carbon and hydrogen isotope fractionation of benzene was studied with four different aerobic strains using a monooxygenase or a dioxygenase for the initial benzene attack, a facultative anaerobic chlorate-reducing strain as well as a sulfate-reducing mixed culture. Carbon and hydrogen enrichment factors (epsilon(C), epsilon(H)) varied for the specific pathways and degradation conditions, respectively, so that from the individual enrichment factors only limited information could be obtained for the identification of benzene biodegradation pathways. However, using the slope derived from hydrogen vs carbon isotope discriminations or the ratio of hydrogen to carbon enrichment factors (lambda = deltaH/ deltaC approximately epsilon(H) /epsilon(C)), benzene degradation mechanisms could be distinguished. Although experimentally determined <b>lambda</b> <b>values</b> partially overlapped, ranges could be determined for different benzene biodegradation pathways. Specific <b>lambda</b> <b>values</b> were 17 for anaerobic degradation. Moreover, variations in <b>lambda</b> <b>values</b> suggest that more than one reaction mechanism exists for monohydroxylation as well as for anaerobic benzene degradation under nitrate-reducing, sulfate-reducing, or methanogenic conditions. Our results show that the {{combined carbon}} and hydrogen isotope fractionation approach has potential to elucidate biodegradation pathways of pollutants in field and laboratory microcosm studie...|$|R
40|$|We have {{performed}} {{a detailed analysis}} of water clustering and percolation in hydrated Nafion configurations generated by classical molecular dynamics simulations. Our results show that at low hydration levels H(2) O molecules are isolated and a continuous hydrogen-bonded network forms as the hydration level is increased. Our quantitative analysis has established a hydration level (lambda) between 5 and 6 H(2) O/SO(3) (-) as the percolation threshold of Nation. We have also examined the effect of such a network on proton transport by studying the structural diffusion of protons using the quantum hopping molecular dynamics method. The mean residence time of the proton on a water molecule decreases by 2 orders of magnitude when the <b>lambda</b> <b>value</b> is increased from 5 to 15. The proton diffusion coefficient in Nation at a <b>lambda</b> <b>value</b> of 15 is about 1. 1 x 10 (- 5) cm(2) /s in agreement with experiment. The results provide quantitative atomic-level evidence of water network percolation in Nafion and its effect on proton conductivity...|$|E
40|$|This {{research}} {{investigates the}} use of Acoustic Emission (AE) measurement to monitor the lubrication conditions in simulated gear tooth contacts. It is experimentally shown that {{a strong correlation between}} <b>lambda</b> <b>value</b> (an indicator of the lubrication regime) and the AE exists, and that for a constant speed a simple empirical relationship exists between them. This benefits those seeking to use AE for condition monitoring of gears...|$|E
40|$|The {{efficiency}} and product formation {{as a function}} of temperature, time and gas stoichiometry, in the regeneration of an SO 2 poisoned NOx storage and reduction (NSR) catalyst has been studied. Using these results a simple model for sulphur poisoning and regeneration of this type of catalyst is proposed. SO 2 poisons the catalyst, which results in a decrease of the NOx conversion with increasing SO 2 exposure time. The NOx storage capacity can be regained after regeneration of the catalyst. The regeneration is most effective using long regeneration time, high temperature and low <b>lambda</b> <b>value.</b> A measure of the regeneration efficiency, activity based sulphur removal (ASR), is defined. ASR is used to model the regeneration efficiency with the independent variables; regeneration time, temperature and <b>lambda</b> <b>value.</b> The model describes the experimental results well. The amounts of released SO 2, H 2 S and COS, during the regeneration differ depending on regeneration conditions. Low lambda values give high amounts of H 2 S and low amounts of SO 2, whereas the reverse is true for high lambda values. The production of COS is low in all cases. The results can schematically be described with a model where sulphur can migrate between deactivating and non-deactivating positions on the catalyst surface...|$|E
40|$|We {{measured}} using the multi-current hot-wire method the thermal conductivity (lambda) {{of two different}} clay pastes suitable {{to be used as}} peloids The same wire and system was used to measure the lambda of water and the results were compared with literature data Special attention was paid to statistical data analysis The obtained <b>lambda</b> <b>values</b> for the clay pastes are higher than the lambda of water at the same temperature...|$|R
40|$|This report {{presents}} {{the application of}} polynomial regression for estimating free energy differences using thermodynamic integration. We employ linear regression to construct a polynomial that optimally fits the thermodynamic integration data, and thus reduces the bias and uncertainty of the resulting free energy estimate. Two test systems with analytical solutions were used to verify the accuracy and precision of the approach. Our results suggest that regression {{with a high degree}} of polynomials give the most accurate free energy difference estimates, but often with a slightly larger variance, compared to commonly used quadrature techniques. High degrees of polynomials possess the flexibility to closely fit the thermodynamic integration data but are often sensitive to small changes in data points. To further improve overall accuracy and reduce uncertainty, we also examine the use of Chebyshev nodes to guide the selection of non-equidistant <b>lambda</b> <b>values</b> for the thermodynamic integration scheme. We conclude that polynomial regression with non-equidistant <b>lambda</b> <b>values</b> delivers the most accurate and precise free energy estimates for thermodynamic integration data. Software and documentation is available at [URL] 18 pages, 4 figures and 4 table...|$|R
30|$|The Wilks' <b>lambda</b> (λ) <b>value</b> of 1.00 {{occurs when}} {{observed}} group mean responses are equal (i.e., all the variances are explained by {{factors other than}} difference between those group means), while a λ value less than 1.00 occurs when within-group variability is small compared to the total variability. Thus, a small λ indicates that group means appear to differ. For example, in Table  3 among five items (i.e., 11, 12, 16, 18, and 23) under dimension reliability, items 11 and 16 have the significance values as 0.000. Further, these two items have Wilks' <b>lambda</b> <b>values</b> (i.e., 0.763, 0.787) {{less than that of}} all other items (i.e., items 12, 18, and 23). Then, the mean value difference is calculated for all the four groups for item 11.|$|R
40|$|One of {{the main}} problem is an optimal {{portfolio}} selection in the financial investment decisions. In this context, the determining of optimal portfolio by using which method is a significant for researcher. On the other hand, Genetic algorithm is optimization technical to select optimal portfolio when there are the plurality of cluster solutions. In this study, it is aimed to determine of optimal portfolio for Istanbul Stock Market 30 Indices. When <b>Lambda</b> <b>value</b> (λ) is 0. 20, optimal portfolio selection is consist of 18 shares According to the application findings which is used data spanned from January- 2010 and June- 2013. When a dominance of risk factor increases, performance of algorithm decreases and optimal portfolio selection is consist of all BIST- 30 Indices...|$|E
40|$|Three-dimensional {{synchronous}} fluorescence spectrometry (TDSFS, {{a combination}} of synchronous fluorescence spectrometry and three-dimensional fluorescence spectrometry) is a new method which has been developed recently. The method has usually been used as an efficient tool to select the best Delta <b>lambda</b> <b>value</b> for synchronous fluorescence spectra. This paper studies {{the sensitivity of the}} method, which was not been done in the past. The total fluorescence intensity has been used instead of the conventional single point intensity, calculated by the Monte-Carlo method, as the experimental parameter to determine fluorescein and tryptophan. The sensitivity of the total fluorescence method is nearly one hundred times better than that of the single point method. The new method has been used to simultaneously determine naphthalene, pyrene and perylene successfully. The mechanism of the method has also been studied...|$|E
40|$|A new risk measure, <b>lambda</b> <b>value</b> at risk (), {{has been}} {{recently}} proposed as a generalization of value at risk (VaR). appears attractive for its potential ability to solve several problems of VaR. This paper provides {{the first study}} on the backtesting of. We propose three nonparametric tests which exploit different features. Two tests are based on simple results of probability theory. One test is unilateral and is more suitable for small samples of observations. A second test is bilateral and provides an asymptotic result. A third test is based on simulations and allows for a more accurate comparison among computed with different assumptions on the asset return distribution. Finally, we perform a backtesting exercise that confirms a higher performance of in respect to VaR especially when it is estimated with distributions that better capture tail behavior...|$|E
40|$|The {{experimental}} {{determination of the}} immersion factor, I(sub f) (lambda), of irradiance collectors is a requirement of any in-water radiometer. The eighth SeaWiFS Intercalibration Round-Robin Experiment (SIRREX- 8) showed different implementations, at different laboratories, of the same I(sub f) (lambda) measurement protocol. The different implementations make use of different setups, volumes, and water types. Consequently, they exhibit different accuracies and require different execution times for characterizing an irradiance sensor. In view of standardizing the characterization of I(sub f) (<b>lambda)</b> <b>values</b> for in-water radiometers, together {{with an increase in}} the accuracy of methods and a decrease in the execution time, alternative methods are presented, and assessed versus the traditional method. The proposed new laboratory methods include: a) the continuous method, in which optical measurements taken with discrete water depths are substituted by continuous profiles created by removing the water from the water vessel at a constant flow rate (which significantly reduces the time required for the characterization of a single radiometer); and b) the Compact Portable Advanced Characterization Tank (ComPACT) method, in which the commonly used large tanks are replaced by a small water vessel, thereby allowing the determination of I(sub f) (<b>lambda)</b> <b>values</b> with a small water volume, and more importantly, permitting I(sub f) (lambda) characterizations with pure water. Intercomparisons between the continuous and the traditional method showed results within the variance of I(sub f) (lambda) determinations. The use of the continuous method, however, showed a much shorter realization time. Intercomparisons between the ComPACT and the traditional method showed generally higher I(sub f) (<b>lambda)</b> <b>values</b> for the former. This is in agreement with the generalized expectations of a reduction in scattering effects, because of the use of pure water with the ComPACT method versus the use of tap water with the traditional method...|$|R
40|$|One of {{the major}} {{challenges}} in measuring efficiency in terms of resources and outcomes is {{the assessment of the}} evolution of units over time. Although Data Envelopment Analysis (DEA) has been applied for time series datasets, DEA models, by construction, form the reference set for inefficient units (<b>lambda</b> <b>values)</b> based on their distance from the efficient frontier, that is, in a spatial manner. However, when dealing with temporal datasets, the proximity in time between units should also be taken into account, since it reflects the structural resemblance among time periods of a unit that evolves. In this paper, we propose a two-stage spatiotemporal DEA approach, which captures both the spatial and temporal dimension through a multi-objective programming model. In the first stage, DEA is solved iteratively extracting for each unit only previous DMUs as peers in its reference set. In the second stage, the <b>lambda</b> <b>values</b> derived from the first stage are fed to a Multiobjective Mixed Integer Linear Programming model, which filters peers in the reference set based on weights assigned to the spatial and temporal dimension. The approach is demonstrated on a real-world example drawn from software development...|$|R
40|$|In {{this study}} we use a {{demographic}} approach to analyse the differing abundance of three congeneric columnar cacti: Neobuxbaumia macrocephala (the rarest), Neobuxbaumia tetetzo (intermediate), and Neobuxbaumia mezcalaensis (the most common). Populations of these species were studied in the Tehuacan Valley (Central Mexico) over a 3 -year period. We employed traditional models and life table response experiments (LTRE) to explore the association between particular demographic traits {{and the degree of}} rarity of each species. Most matrices showed population growth rate (<b>lambda)</b> <b>values</b> close to unit...|$|R
40|$|A new risk measure, the <b>lambda</b> <b>value</b> at risk (Lambda VaR), {{has been}} {{recently}} proposed from a theoretical {{point of view}} as a generalization of the value at risk (VaR). The Lambda VaR appears attractive for its potential ability to solve several problems of the VaR. In this paper we propose three nonparametric backtesting methodologies for the Lambda VaR which exploit different features. Two of these tests directly assess the correctness {{of the level of}} coverage predicted by the model. One of these tests is bilateral and provides an asymptotic result. A third test assess the accuracy of the Lambda VaR that depends on the choice of the P&L distribution. However, this test requires the storage of more information. Finally, we perform a backtesting exercise and we compare our results with the ones from Hitaj and Peri (2015...|$|E
40|$|Recently, {{financial}} industry and regulators have enhanced {{the debate on}} the good properties of a risk measure. A fundamental issue is the evaluation of the quality of a risk estimation. On the one hand, a backtesting procedure is desirable for assessing the accuracy of such an estimation and this can be naturally achieved by elicitable risk measures. For the same objective, an alternative approach has been introduced by Davis (2016) through the so-called consistency property. On the other hand, a risk estimation should be less sensitive with respect to small changes in the available data set and exhibit qualitative robustness. A new risk measure, the <b>Lambda</b> <b>value</b> at risk (Lambda VaR), has been recently proposed by Frittelli et al. (2014), as a generalization of VaR with the ability to discriminate the risk among P&L distributions with different tail behaviour. In this article, we show that Lambda VaR also satisfies the properties of robustness, elicitability and consistency under some conditions...|$|E
40|$|Abstract: The {{purpose of}} this paper is to {{investigate}} the effect of air-fuel ratio on the combustion and emissions characteristics of spark ignition (SI) gasoline engine fueled with bio-ethanol. A 1. 6 L SI engine with 4 cylinders was tested on EC dynamometer. In addition, lambda sensor and lambda meter were connected with universal ECU to control the <b>lambda</b> <b>value</b> which is varied from 0. 7 to 1. 3. The engine performance and combustion characteristics of bio-ethanol fuel were compared to those obtained by pure gasoline. Furthermore, the exhaust emissions such as carbon monoxide (CO), unburned hydrocarbon (HC), oxides of nitrogen (NOX) and carbon dioxide (CO 2) were measured by emission analyzers. The results showed that the brake torque and cylinder pressure of bio-ethanol fuel were slightly higher than those of gasoline fuel. Brake specific fuel consumption (BSFC) of bio-ethanol was increased while brake specific energy consumption (BSEC) was decreased. The exhaust emissions of bio-ethanol fuel were lower than those of gasoline fuel under overall experimental conditions. However, the specific emission characteristics of the engin...|$|E
40|$|This report details an {{approach}} to improve the accuracy and precision of free energy difference estimates using thermodynamic integration data (slope of the free energy {{with respect to the}} switching variable lambda) and its application to calculating solvation free energy. The central idea is to utilize polynomial fitting schemes to approximate the thermodynamic integration data to improve the accuracy and precision of the free energy difference estimates. In this report we introduce polynomial and spline interpolation techniques. Two systems with analytically solvable relative free energies are used to test the accuracy and precision of the interpolation approach (Shyu and Ytreberg, J Comput Chem 30 : 2297 - 2304, 2009). We also use both interpolation and extrapolation methods to determine a small molecule salvation free energy. Our simulations show that, using such polynomial techniques and non-equidistant <b>lambda</b> <b>values,</b> the solvation free energy can be estimated with high accuracy without using soft-core scaling and separate simulations for Lennard-Jones and partial charges. The results from our study suggest these polynomial techniques, especially with use of non-equidistant <b>lambda</b> <b>values,</b> improve the accuracy and precision for dF estimates without demanding additional simulations. To allow researchers to immediately utilize these methods, free software and documentation is provided via [URL] 14 pages, 2 figure...|$|R
40|$|A global Newton {{method is}} used to obtain {{accurate}} finite difference solutions for uniform inflows to several sudden expansion geometries, avoiding the instabilities encountered at large Reynolds numbers. Steady solutions for <b>lambda</b> <b>values</b> greater than those attained previously are obtained only if lambda is below a critical value. The presence of a singularity at this critical value renders invalid the assumptions of the boundary layer model. The results indicate that for uniform inflows and smaller values of the expansion ratio, the eddy length will no longer increase linearly with R when R becomes sufficiently large...|$|R
40|$|The {{power to}} detect disease-susceptibility loci through linkage {{analysis}} using pairs of affected relatives and affected-unaffected pairs is examined. Allelic identity by descent (ibd) for a completely polymorphic marker for sibling, uncle-nephew, grandparent-grandchild, half-sib, and first-cousin pairs is considered. Affected-unaffected pairs generally represent a poor strategy. For single-locus models, ibd depends on lambda R, the risk ratio for type R relatives compared with population prevalence, and the recombination fraction theta. The ibd for grandparent-grandchild pairs is least affected by recombination, followed by sibs, half-sib, uncle-nephew, and first-cousin pairs. For diseases with large <b>lambda</b> <b>values</b> and for small theta values, distant relatives offer greater power. For larger theta values, grandparent-grandchild pairs are best; for small <b>lambda</b> <b>values,</b> sibs are best. Additive and multiplicative multilocus models are considered. For the multiplicative model, the same formulas {{as in the}} single-locus model apply, except that lambda iR (for the ith contributing locus) is substituted for lambda R. For the additive model, the deviation from null expectation for ibd is divided among all contributing loci. Compared with the multiplicative model, for an additive model there is usually greater advantage in distant relationships. Multipoint analysis using linked marker loci for affected relative pairs is described. Simultaneous use of multiple markers diminishes the effect of recombination and allows for localization of the disease-susceptibility locus...|$|R
40|$|Abstract: Kochanek (1992) {{suggested}} that the redshifts of gravitational lens galaxies rule out a large cosmological constant. This result was questioned by Helbig & Kayser (1996), who pointed out that selection effects related to {{the brightness of the}} lens can bias the results of this test against a high lambda value; however, we did not claim that the observations favoured a high <b>lambda</b> <b>value,</b> merely that current observational data were not sufficient to say either way, using the test as proposed by Kochanek (1992) but corrected for selection effects. Kochanek (1996) pointed out that additional information (fraction of measured lens redshifts) provides additional information which restores the sensitivity of the test to the cosmological model, at least somewhat. Here, I consider three aspects. First, I examine the accuracy of the correction to the test proposed by Kochanek (1996). Second, I compare the slightly different statistical methods which have been used in connection with this test. Third, I discuss what results can be obtained today now that more and better-defined observations are available. ...|$|E
40|$|The Curve Number {{method is}} widely used in {{hydrology}} because it’s simply based on a single parameter, CN, that represents the basin absorption. In this paper CN is evaluated at basin scale from rainfall-runoff multi-daily events (Mockus, 1964), in the observation period 1940 - 1997 (record length mean equal to 20 years), for 61 Sicilian basin with three different methods: NEH 4 method, Asymptotic fitting method (Hawkins, 1990, Hawkins et al., 2002, Hawkins et al., 2009), Least squares method (Woodward et al., 2006, Hawkins et al., 2009). A first analysis of Sicilian watershed behavior indicates a major occurrence of standard CN response (42 basins), rather than complacent response (11 basins) and no violent behavior. The original assumption of Initial abstraction ratio (Ia/S or lambda) equal to 0. 20, is investigated for watersheds with standard CN response, using “natural” and “ordered” rainfall-runoff data. Results indicate a median <b>lambda</b> <b>value</b> of 0, for natural data and 0. 035, for ordered data, according to recently world-wide researches (Hawkins et al., 2010) ...|$|E
40|$|The {{increasing}} {{size of the}} {{car fleet}} makes it important {{to find ways of}} lowering the amounts of pollutants from each individual diesel or gasoline engine to almost zero levels. The pollutants from these engines predominantly originate from emissions at cold start, in the case when gasoline is utilized, and high NOx emissions and particulates from diesel engines. The cold start emissions from gasoline vehicles are primarily due to a high light-off time for the catalytic converter. Another reason is the inability to quickly heat the sensor used for controlling the air-to-fuel ratio in the exhausts, also called the <b>lambda</b> <b>value,</b> which is required to be in a particular range for the catalytic converter to work properly. This problem may be solved utilizing another, more robust sensor for this purpose. One way of treating the high NOx levels from diesel engines is to introduce ammonia in the exhausts and let it react with the NOx in a special catalytic converter to form nitrogen gas and water, which is called SCR (selective catalytic reduction). However, in order to make this system reduc...|$|E
50|$|The {{effect of}} using {{different}} thermal breaks {{can be observed}} here by comparing various Uw values obtained with different thermal breaks and systems. With outstanding thermal breaks, aluminium frames are able {{to be used for}} passive houses. The geometry and material of thermal breaks are aimed to decrease heat loss because of conduction, convection and radiation. Conduction is decreased by using materials with minimum <b>lambda</b> <b>values</b> and also using profiles with hollow chambers. Convection is decreased by using flags on insulation profiles. And radiation is decreased by using low-e folio (e.g. insulbar LEF) on these flags.|$|R
40|$|Data envelopment {{analysis}} (DEA) {{is known}} as a useful tool that produces many efficient decision-making units (DMUs). Traditional DEA provides relative efficient scores and reference sets, but does not influence and rank the efficient DMUs. This paper suggests a method that provides influence and ranking information by using PageRank as a centrality of Social Network analysis (SNA) based on reference sets and their <b>lambda</b> <b>values.</b> The social network structure expresses the DMU as a node, reference sets as link, and lambda as connection strengths or weights. This paper, with PageRank, compares the Eigenvector centrality suggested by Liu, et al. in 2009, and shows that PageRank centrality is more accurate...|$|R
5000|$|... (defun make-counter (<b>value)</b> (<b>lambda</b> (...) (incf value)));;; use the counter(defvar *c* (make-counter 10))(funcall *c*) --> 11(funcall *c*) --> 12 ...|$|R
40|$|Recently, the {{financial}} industry and regulators have enhanced {{the debate on the}} good properties of a risk measure. A fundamental issue is the evaluation of the quality of a risk estimation. On the one hand, a backtesting procedure is desirable for assessing the accuracy of such an estimation and this can be naturally achieved by elicitable risk measures. For the same objective, an alternative approach has been introduced by Davis [Stat. Risk Model. Appl. Finance Insurance, 2016, 33, 67 – 93] through the so-called consistency property. On the other hand, a risk estimation should be less sensitive with respect to small changes in the available data-set and exhibit qualitative robustness. A new risk measure, the <b>Lambda</b> <b>value</b> at risk (), has been recently proposed by Frittelli et al. [Math. Finance, 2014, 24, 442 – 463], as a generalization of VaR with the ability to discriminate the risk among P&L distributions with different tail behaviour. In this article, we show that also satisfies the properties of robustness, elicitability and consistency under some conditions...|$|E
40|$|A {{kinetic study}} of the one-electron {{oxidation}} {{of a series of}} ferrocenes (FcX: X = H, CO 2 Et, CONH 2, CH 2 CN, CH 2 OH, Et, and Met) by PING generated in CH 3 CN by reaction of N-hydroxyphthalimide (NHPI) with the cumyloxyl radical produced by 355 nm laser flash photolysis of dicumyl peroxide has been carried out. Ferrocenium cations were formed, and the reaction rate was determined by following the decay of PINO radical at 380 nm {{as a function of the}} FcX concentration. Rate constants were very,sensitive to the oxidation potential of the substrates and exhibited a good fit with the Marcus equation, from which a <b>lambda</b> <b>value</b> of 38. 3 kcal mol(- 1) was calculated for the reorganization energy required in the PINO/ferrocenes electron-transfer process. Knowing the ferrocene/ferrocenium self-exchange reorganization energy it was possible to calculate a value of 49. 1 kcal mol(- 1) for the PINO/PINO- self-exchange reaction in CH 3 CN. Moreover, from the Marcus cross relation and the self-exchange rates of ferrocene and dimethylferrocene, the intrinsic reactivity of PING in electron-transfer reactions has been calculated as 7. 6 x 10 (2) M- 1 s(- 1). The implications of these values and the comparison with the electron-transfer self-exchange reorganization energies of peroxyl radicals are briefly discussed...|$|E
40|$|An {{extensive}} {{product range}} is mandatory to {{be a key}} player in the pipe insulation market. The polyurethane preinsulated pipe producers are responding to global opportunities by diversifying their activities into industrial, marine, oil and gas applications. Moreover, the requirements for traditional district heating preinsulated pipes- including initial thermal conductivity and its ageing performance- are becoming more stringent. As a consequence, polyurethane preinsulated pipe manufacturers are faced with increasing demands for low energy consumption pipe networks and higher thermal resistant pipe composites for industrial applications. Huntsman Polyurethanes has developed a new generation of polyurethane foam system technology with a significantly reduced <b>lambda</b> <b>value.</b> The use of this system improves the cost effectiveness of the pipe network over the service period of 30 years, due to the reduction of heat loss. This new system can be processed with conventional production techniques. The use of polyurethane preinsulated pipe composites in industrial applications offers clear advantages compared to the traditional in-situ installed insulation, e. g. less water or steam ingress and subsequently less corrosion. Huntsman Polyurethanes has developed new high temperature resistant polyurethane foam. Together with traditional discontinuous production techniques, this new foam system will enable easier installation and higher reliability over longer periods. District heating systems with improved thermal insulation propertie...|$|E
40|$|Glu- 113 {{serves as}} the retinylidene Schiff base {{counterion}} in bovine rhodopsin. Purified mutant rhodopsin pigments were prepared in which Glu- 113 was replaced individually by Gln (E 113 Q), Asp (E 113 D), Asn (E 113 N), or Ala (E 113 A). E 113 Q, E 113 N, and E 113 A existed as pH-dependent equilibrium mixtures of unprotonated and protonated Schiff base (PSB) forms. The Schiff base pKa values determined by spectrophotometric titration were 6. 00 (E 113 Q), 6. 71 (E 113 N), and 5. 70 (E 113 A). Thus, mutation of Glu- 113 markedly reduced the Schiff base pKa. The addition of NaCl promoted {{the formation of a}} PSB in E 113 Q and E 113 A. An exogenously supplied solute anion replaced Glu- 113 to compensate for the positive charge of the PSB in these mutants. The <b>lambda</b> max <b>values</b> of the PSB forms of the mutants in NaCl were 496 nm (E 113 Q), 506 nm (E 113 A), 510 nm (E 113 D), and 520 nm (E 113 N). To evaluate the effect of different types of solute anions on <b>lambda</b> max <b>values,</b> mutants were prepared in sodium salts of halides, perchlorate, and a series of carboxylic acids of various sizes and acidity. The <b>lambda</b> max <b>values</b> of E 113 Q and E 113 A depended on the solute anion present and ranged from 488 nm to 522 nm for E 113 Q and from 486 nm to 528 nm for E 113 A. The solute anion affected the <b>lambda</b> max <b>values</b> of E 113 N and E 113 D to lesser degrees. The reactivities of the mutants to hydroxylamine were also studied. Whereas rhodopsin was stable to hydroxylamine in the dark, E 113 N reacted slowly and E 113 Q reacted rapidly under these conditions, indicating structural differences in the Schiff base environments. The <b>lambda</b> max <b>values</b> and solute anion dependencies of the Glu- 113 mutants indicate that interactions between Schiff base and its counterion {{play a significant role in}} determining the lambda max of rhodopsin...|$|R
40|$|OBJECTIVE: Our {{purpose was}} to {{validate}} an early enhancement time point for accurately measuring the myocardial contrast partition coefficient (lambda) using dynamic-equilibrium magnetic resonance imaging. MATERIALS AND METHODS: The pre- and post-contrast longitudinal relaxation rates (reciprocal of T 1) of the interventricular septum (R 1 (m)) and blood pool (R 1 (b)) were obtained from fifteen healthy volunteers and three diabetic patients with hypertension using two optimized T 1 mapping sequences (modified Look-Locker inversion recovery) on a 3 -Tesla magnetic resonance scanner. Reference <b>lambda</b> <b>values</b> were calculated as {{the slope of the}} regression line of R 1 (m) versus R 1 (b) at dynamic equilibrium (multi-point regression method). The simplified pre-/post-enhancement two-acquisition method (two-point method) was used to calculate lambda by relating the change in R 1 (m) and R 1 (b) using different protocols according to the acquisition stage of the post-enhancement data point. The agreement with the referential method was tested by calculating Pearson's correlation coefficient and the intra-class correlation coefficient. RESULTS: The <b>lambda</b> <b>values</b> measured by the two-point method increased (from 0. 479 ± 0. 041 to 0. 534 ± 0. 043) over time from 6 to 45 minutes after contrast and exhibited good correlation with the reference at each time point (r ≥ 0. 875, p< 0. 05). The intra-class correlation coefficient on absolute agreement with the reference lambda was 0. 946, 0. 929 and 0. 922 at the 6 th, 7 th and 8 th minutes and dropped from 0. 878 to 0. 403 from the 9 th minute on. CONCLUSIONS: The time-efficient two-point method at 6 - 8 minutes after the Gd-DTPA bolus injection exhibited good agreement with the multi-point regression method and can be applied for accurate lambda measurement in normal myocardium...|$|R
40|$|The {{electronic}} {{stress tensor}} is not uniquely defined. Therefore, shell indicators {{stemming from the}} quantum stress tensor may inherit this ambiguity. Based on a general formula of the stress tensor this ambiguity can be described by an external parameter lambda. Two functions derived from the quantum stress tensor have been evaluated according {{to their ability to}} serve as shell indicators. The influence of lambda is analyzed and the consequences for the representation of the atomic shell structure are discussed in detail. It is found that the trace of the stress tensor does not fully reveal the atomic shell structure. In contrast, the scaled trace (whereby the scaling function is proportional to the Thomas-Fermi kinetic energy density) produces fairly good representation of the atomic shell structure over a wide range of <b>lambda</b> <b>values...</b>|$|R
