1|204|Public
50|$|It {{is common}} for a <b>language</b> <b>sensitive</b> <b>editor</b> to {{represent}} a document as a parse tree with respect to language’s grammar, or as an abstract syntax tree (AST). For example, a DOM tree is essentially an AST {{with respect to a}} given DTD. Frequently, the textual view of that underlying tree is generated by prettyprinting the underlying tree. Editors associated with intentional programming and language-oriented programming for general-purpose languages and domain-specific languages share many of the features of language-sensitive editors, but aim for greater separation between the underlying representation (the intention) and the surface representation (text in a programming language).|$|E
5000|$|Fully {{customisable}} and <b>language</b> <b>sensitive</b> captive portal or any {{third party}} web page ...|$|R
40|$|This article {{discusses}} <b>language</b> <b>sensitive</b> {{teacher education}} across curriculum. First, we briefly discuss {{the importance of}} <b>language</b> <b>sensitive</b> teaching {{and take a look}} at studies on linguistically responsive teaching and teacher education. The results are based on a data from a questionnaire and an applied task in which student subject teachers (n= 221) assessed second language (SL) learners’ writing skills and challenges of studying specific school subjects from learner’s proficiency perspective. We also report tentative results on the negotiation of language in a teaching intervention which integrates language and content. Our findings indicate that the student teachers are fairly well aware of their role as a language teacher as well but they mainly focus on the word-level phenomena in their language analysis. Finally, we discuss the challenges of developing teacher education to promote <b>language</b> <b>sensitive</b> teaching and <b>language</b> awareness across the curriculum...|$|R
40|$|Any {{attempt to}} address <b>language</b> <b>sensitive</b> {{practice}} {{from the standpoint}} of the Welsh language in Wales relates to three core issues. Firstly, it requires individual practitioners to address the affective or the subjective, an exploration of personal attitudes, values and perceptions of the language. It links wit...|$|R
40|$|AbstractCochlear {{implants}} (CI) are {{the most}} successful intervention for ameliorating hearing loss in severely or profoundly deaf children. Despite this, educational performance in children with CI continues to lag behind their hearing peers. From animal models and human neuroimaging studies it has been proposed the integrative functions of auditory cortex are compromised by crossmodal plasticity. This has been argued to result partly {{from the use of}} a visual language. Here we argue that ‘cochlear implant sensitive periods’ comprise both auditory and <b>language</b> <b>sensitive</b> periods, and thus cannot be fully described with animal models. Despite prevailing assumptions, {{there is no evidence to}} link the use of a visual language to poorer CI outcome. Crossmodal reorganisation of auditory cortex occurs regardless of compensatory strategies, such as sign language, used by the deaf person. In contrast, language deprivation during early sensitive periods has been repeatedly linked to poor <b>language</b> outcomes. <b>Language</b> <b>sensitive</b> periods have largely been ignored when considering variation in CI outcome, leading to ill-founded recommendations concerning visual language in CI habilitation...|$|R
40|$|The {{rule-based}} formal <b>language</b> of "stochastic <b>sensitive</b> growth grammars" {{was designed}} to describe algorithmically the changing morphology of forest trees during their lifetime under the impact of endogenous and exogenous factors, and to generate 3 -D simulations of tree structures in a systematic manner. The description {{in the form of}} grammars allows the precise specification of structural models with functional components. These grammars (extended Lsystems) can be interpreted by the software GROGRA (Growth grammar interpreter) yielding time series of attributed 3 -D structures representing plants. With some recent extensions of the growth-grammar <b>language</b> (<b>sensitive</b> functions, local variables) it is possible to model environmental control of shoot growth and some simple allocation strategies, and to obtain typical competition effects in tree stands qualitatively in the model...|$|R
40|$|A basic result {{which gives}} a {{condition}} under which a (possibly length-decreasing) homomorphism preserves a contest. <b>sensitive</b> <b>language</b> is presented. Using this result, {{conditions under which}} pushdown transducers and linear bounded transducers preserve contest <b>sensitive</b> <b>languages</b> are given. The basic result is also applied to show that certain rewriting systems generate context <b>sensitive</b> <b>languages</b> instead of arbitrary recursively enumerable sets. Of special interest is the result that if each rule in a rewriting system has a terminal letter on its right side, then the language generated is context free...|$|R
50|$|ICU {{provides}} the following services: Unicode text handling, full character properties, and character set conversions; Unicode regular expressions; full Unicode sets; character, word, and line boundaries; <b>language</b> <b>sensitive</b> collation and searching; normalization, upper and lowercase conversion, and script transliterations; comprehensive locale data and resource bundle architecture via the Common Locale Data Repository (CLDR); multi-calendar and time zones; and rule-based formatting and parsing of dates, times, numbers, currencies, and messages. ICU provided complex text layout service for Arabic, Hebrew, Indic, and Thai, {{but that was}} deprecated in version 54, and was completely removed in version 58 in favor of HarfBuzz.|$|R
40|$|Language {{identification}} {{is an important}} part of Natural Language Processing, because most of techniques are <b>language</b> <b>sensitive,</b> and therefore in multi-language systems language should be identified before further processing steps. Techniques and tools for more popular languages are well defined and are available in commercial and open source tools, but are not defined for less popular languages. In this work we investigate techniques for Lithuanian, Russian and Azeri (Azjerbaijani) languages. Corpora for these and similar languages (Latvian, Ukrainian, Belarusian, Turkish and Turkmen (Turkman) were collected and prepared. Selected approaches were trained. Results were evaluated using precision, recall and F-score...|$|R
40|$|A three-year {{training}} program for bilingual/bicultural program staff development specialists is described. The program, which involves the cooperation of a higher education and six local education agencies (LEAs), seeks to train staff development specialists who can function in two <b>languages,</b> <b>sensitive</b> to the problems and advantages of limited-English-proficient (LEP) students, have strong background in bilingual and bicultural education, and are committed to quality education and social change. In particular, efforts to institutionalize the program are examined, based o eight indicators of institutionalization: active support of administrators; positive attitudes of non-bilingual education faculty; faculty support through institutional funds; bilingual faculty tenure and promotion; program continuation without federal funds; involvement of severa...|$|R
5000|$|Corporate Futures: The Diffusion of the Culturally <b>Sensitive</b> Corporate Form (<b>Editor,</b> Late Editions: Cultural Studies for the End of the Century, Chicago: University of Chicago Press, 1998) ...|$|R
40|$|This {{contribution}} describes INES (Integrated Standard Editor {{based on}} SGML), an advanced context <b>sensitive</b> SGML <b>editor</b> {{based on the}} publishing system Interleaf 5. This editor was developed at ZGDV to support working with the DTD of the German Institute of Standards (DIN) to write standard documents. The goal of the development was a complete SGML editor combining the advantages of WYSIWYG with the advantages of structured SGML editing. The editor should guarantee the DTD conformance of the document. Interleaf 5, which offers an object-oriented LISP programming interface, was improved by the following SGML features: (1) SGML kernel to manage the data of the DTD and its instances, (2) context <b>sensitive</b> <b>editor</b> (CSE) as a shell around this kernel which builds the connection to the user interface and the layout components, (3) interactive attribute editor to handle the SGML attributes and to control the layout, (4) online help for elements and attributes, and (5) layout editor to define the context-dependent layout. INES was developed without using the Interleaf product Interleaf Developers Toolkit. The CSE supervises {{the actions of the}} user constantly to guarantee that the document is in accordance with the DTD at any time, i. e. no postprocessing with an SGML parser is necessary. The editor realizes this with context-dependent menus for the creation of elements and entities, with controlled keyboard input, and with a special handling of cut, copy and paste. The CSE guarantees also that all required elements and attributes exist in the document. It is possible to load arbitrary DTDs into INES...|$|R
5000|$|Heather Dubrow, on {{the other}} hand, does not dismiss this sonnet [...] "as an unfortunate and {{unsuccessful}} game, with even the most <b>sensitive</b> of <b>editors</b> asserting that it is hardly worth reprinting" [...] but believes that [...] "this poem is not unimportant, for it enacts {{a version of the}} issue we are considering, the way the future can change the past" [...] (224) [...]|$|R
40|$|Dynamical {{simulation}} models of tree growth and morphology {{were developed to}} address questions about the interrelations between tree structure, functions and environmental conditions on a mesoscale level. The modelling approach has a formal basis in stochastic growth grammars (i. e., extended L-systems), which can be interpreted by the software GROGRA (Growth Grammar Interpreter) yielding time series of attributed 3 -D architectural structures representing trees. With appropriate extensions of this string rewriting <b>language</b> (<b>sensitive</b> functions, arithmetical-structural operators, local registers) {{it is possible to}} represent environmental control of shoot growth and some simple allocation strategies, and to reproduce typical competition effects in tree stands qualitatively. The new GROGRA extensions which enable these results are completely documented. Keywords: tree growth, plant architecture, competition, allocation, sensitivity, L-systems, growth grammars, GROGRA 1. Introduction I [...] ...|$|R
40|$|There is {{a dearth}} of {{preventative}} programs that enhance the Australian culturally and linguistically diverse (CALD) adults’ resilience {{to cope with the}} acculturation process. This article introduces the reader to the BRiTA Futures for Adults and Parents, a culture and <b>language</b> <b>sensitive</b> program for the CALD. The conceptual framework and the development process are described. The manualised program consisting of one introductory and eight intervention modules is presented. A training program is also developed to train facilitators, who can deliver the program in English or other languages. Preliminary trials indicated that the program was received well by the consumers. A block mode, instead of the traditional weekly sessions, appeared to be more practical for the small population for which it was trialled. Implications and future directions are discussed...|$|R
40|$|This paper {{examines}} adjectival reduplication in Chinese {{which is}} contrasted with determiner doubling in Germanic. It shows that two superficially different phenomena in two genetically unrelated <b>languages</b> are <b>sensitive</b> to similar properties. Both environments support {{an analysis of}} adjectives in terms of restrictive relative clauses and strengthen the case for decomposing the adjectiv...|$|R
50|$|More recently, {{the class}} PTIME has been {{identified}} with range concatenation grammars, which are now {{considered to be the}} most expressive of the mild-context <b>sensitive</b> <b>languages.</b>|$|R
50|$|Cherokees {{have served}} in both world wars. About 600 Cherokee and Choctaw served in the 142nd Infantry Regiment (United States) of the 36th Texas-Oklahoma National Guard Division during World War I. Comanche and Navajo code talkers are well known, but as many as 40 Cherokee men also used their native <b>language</b> for <b>sensitive</b> {{communications}} during World War II.|$|R
40|$|Modern word {{processors}} begin {{to offer a}} range of facilities for spelling, grammar and style checking in English. For the Dutch language hardly anything is available as yet. Many commercial word processing packages do include a hyphenation routine and a lexicon-based spelling checker but the practical usefulness of these tools is limited due to certain properties of Dutch orthography, as we will explain below. In this chapter we describe a text editor which incorporates {{a great deal of}} lexical, morphological and syntactic knowledge of Dutch and monitors the orthographical quality of Dutch texts. Section 1 deals with those aspects of Dutch orthography which pose problems to human authors as well as to computational <b>language</b> <b>sensitive</b> text editing tools. In section 2 we describe the design and the implementation of the text editor we have built. Section 3 is mainly devoted to a provisional evaluation of the system...|$|R
40|$|We {{present a}} {{methodology}} based on grammatical inference algorithms {{applied to the}} linguistic modeling of biological regulation networks. The linguistic approach {{to the problem of}} regulation networks was proposed by COLLADO-VIDES, who proved and formalized the need for use of context <b>sensitive</b> <b>languages</b> to represent such networks. The learning of context <b>sensitive</b> <b>languages</b> is a difficult task, our proposed methodology describes this class from language with a simpler nature that can be learned by already consolidated grammars inference algorithms. In addition to the proposed methodology, we suggest promising directions for this research...|$|R
40|$|Families of {{languages}} generated by classes of context sensitive Lindenmayer systems with tables using nonterminals are {{classified in the}} Chomsky hierarchy. It is shown that the family {{of languages}} generated by deterministic λ-free left context sensitive L systems with two tables using nonterminals coincides with the context <b>sensitive</b> <b>languages.</b> Combined {{with the fact that}} the family of languages generated by deterministic λ-free context sensitive L systems (with one table) using nonterminals is equal to the DLBA languages this shows the classic LBA problem to be equivalent to whether or not a trade-off is possible between one-sided context with two tables and two-sided context with one table for deterministic λ-free L systems using nonterminals. Without the restriction to λ-freeness such a trade-off is possible since the recursively enumerable languages are generated in both cases. By stating the results in their strongest form, a complete classification of the considered language families is obtained since the hierarchies induced by the involved parameters (λ-freeness, determinism, number of tables, amount of context, closure under various types of homomorphisms) basically collapse to the recursively enumerable <b>languages,</b> context <b>sensitive</b> <b>languages,</b> and DLBA languages...|$|R
40|$|A case <b>sensitive</b> {{intelligent}} model <b>editor</b> {{has been}} developed for constructing consistent lumped dynamic process models and for simplifying them using modelling assumptions. The approach {{is based on a}} systematic assumption-driven modelling procedure and on the syntax and semantics of process models and the simplifying assumptions...|$|R
40|$|In [AS], Anisimov and Seifert {{show that}} a group has a regular word problem {{if and only if}} it is finite. Muller and Schupp [MS] (together with Dunwoody’s {{accessibility}} result [D]) {{show that a}} group has context free word problem if and only if it is virtually free. In this note, we exhibit a class of groups where the word problem is {{as close as possible to}} being a context <b>sensitive</b> <b>language.</b> This class includes the automatic groups of [ECHLPT] and is closed under passing to finitely generated subgroups. Consequently, it is quite large. For example, it contains all finitely generated subgroups of the n-fold product of free groups, F 2 × [...] . × F 2. For n = 2, these include groups which are not finitely presented, and for n> 2, these include groups which are FPn but not FPn+ 1. Let us make clear what we mean by saying that the word problem is as close as possible to being a context <b>sensitive</b> <b>language.</b> Recall that a context <b>sensitive</b> <b>language</b> cannot contain the empty word e. Since the empty word is always an element of the word problem, strictly speaking, the word problem can never be a context <b>sensitive</b> <b>language.</b> So we will abuse terminology and say that the word problem is context sensitive if, after deleting the empty word, it is context sensitive. We feel that this is not a grievous abuse: i...|$|R
40|$|Abstract [...] The {{addition}} of context free grammar rules to a functional language simplifies {{the construction of}} interpreters from denotationaI semantic language definitions. Functional abstraction over grammar rules enables the specification and processing of context <b>sensitive</b> <b>language</b> syntax aspects in a functional style...|$|R
50|$|Within the Chomsky hierarchy, {{the regular}} languages, the {{context-free}} languages, and the recursively enumerable languages are all full AFLs. However, the context <b>sensitive</b> <b>languages</b> and the recursive languages are AFLs, but not full AFLs {{because they are}} not closed under arbitrary homomorphisms.|$|R
40|$|A truth {{conditional}} semantics for inherently/lexically reflexive verbs {{that distinguishes}} {{them from their}} “derived ” reflexive counterparts An account for the way a <b>language’s</b> morphology is <b>sensitive</b> to {{the distinction between the}} semantics of inherent reflexives vs. “derived ” reflexives (Germanic SE- vs. SELF anaphors) [still in progress!...|$|R
40|$|It {{is known}} that nonregular {{languages}} can be accepted by finite state probabilistic automata. For many years it was not known whether a finite state probabilistic automaton existed that would accept a context <b>sensitive</b> <b>language</b> that is not context free. Such a finite state probabilistic automaton is constructed...|$|R
40|$|<b>Language</b> is <b>sensitive</b> to both {{semantic}} {{and pragmatic}} effects. To capture both effects, we model language {{use as a}} cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Specifically, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions. ...|$|R
5000|$|Although speech {{perception}} {{is considered to}} be an auditory skill, it is intrinsically multimodal, since producing speech requires the speaker to make movements of the lips, teeth and tongue which are often visible in face-to-face communication. Information from the lips and face supports aural comprehension [...] and most fluent listeners of a <b>language</b> are <b>sensitive</b> to seen speech actions (see McGurk effect). The extent to which people make use of seen speech actions varies with the visibility of the speech action and the knowledge and skill of the perceiver.|$|R
25|$|The {{issue of}} the {{ethnicity}} of the Ålanders, and the correct linguistic classification of their <b>language,</b> remains somewhat <b>sensitive</b> and controversial. They may be considered either ethnic Swedes or Swedish-speaking Finns, but their language {{is closer to the}} Uppländska dialect of Sweden than to Finland Swedish. See Languages of Sweden.|$|R
40|$|Contextual Binary Feature Grammars were {{recently}} proposed by (Clark et al., 2008) as a learnable representation for richly structured context-free and con-text <b>sensitive</b> <b>languages.</b> In this pa-per {{we examine the}} representational power of the formalism, its relationship to other standard formalisms and lan-guage classes, and its appropriateness for modelling natural language. ...|$|R
40|$|A stack {{automaton}} is a pushdown automaton {{that can}} read the interior of its pushdown list without altering it. It is shown that a nondeterministic stack automation with a one-way input tape can be simulated by a deterministic linear bounded automaton. Hence, each nondeterministic one-way stack <b>language</b> is context <b>sensitive...</b>|$|R
50|$|The {{issue of}} the {{ethnicity}} of the Ålanders, and the correct linguistic classification of their <b>language,</b> remains somewhat <b>sensitive</b> and controversial. They may be considered either ethnic Swedes or Swedish-speaking Finns, but their language {{is closer to the}} Uppländska dialect of Sweden than to Finland Swedish. See Languages of Sweden.|$|R
50|$|Professor Philip Johnson-Laird {{used the}} song to {{illustrate}} issues in formal logic as contrasted with psychology of reasoning, {{noting that the}} transitive property of identity relationships expressed in natural <b>language</b> was highly <b>sensitive</b> to variations in grammar, while reasoning by models, {{such as the one}} constructed in the song, avoided this sensitivity.|$|R
50|$|In {{the months}} leading up to the {{beginning}} of the conference, negotiators held frequent informal consultations at UN headquarters in New York City, and in the two weeks before the conference was scheduled to begin, they managed to reach consensus on the <b>sensitive</b> <b>language</b> in the then proposed outcome document for the summit.|$|R
40|$|Contrary {{to recent}} claims, the Long Short Term Memory {{is not the}} only neural network which learns a context <b>sensitive</b> <b>language.</b> Both Simple Recurrent Network and Sequential Cascaded Network are able to generalize beyond {{training}} data but by utilizing di#erent dynamics. Di#erences in performance and dynamics are discussed. Keywords: Recurrent neural network, language, prediction. ...|$|R
