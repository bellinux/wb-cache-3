118|402|Public
2500|$|On January 24, 2002, Agere Systems {{announced}} that it would be closing the [...] Reading Works in 12 to 18 months. The planned changes involved closing the Breinigsville, Pennsylvania plant in Lehigh County, Pennsylvania, which was opened in 1988; selling the Orlando plant in Florida; and consolidating several locations in New Jersey. All operations were consolidated at the Allentown, Pennsylvania, headquarters location and the New Jersey locations. About 1,500 workers were transferred from Reading to Allentown where 3,200 workers were employed prior to the relocation. As Agere was leaving, Legerity, Inc., assumed some of the operations formerly done by the Muhlenberg plant in the old Building 10 on North 13th Street. In this facility Legerity assembled 40 former Agere circuit-design, physical-design, application-design and process development engineers to support the analog <b>line</b> <b>card</b> integrated circuit business it purchased from Agere. Legerity is a design facility that uses other companies foundries to manufacture its products. In May 2003, Agere Systems Inc. ended all manufacturing and began decommissioning its Muhlenberg Township plant. The last 346 manufacturing employees were laid off. About 50 employees — mostly maintenance — remained at the plant until a buyer was found.|$|E
50|$|A <b>line</b> <b>card</b> or digital <b>line</b> <b>card</b> is a modular {{electronic}} circuit {{designed to fit}} on a separate printed circuit board (PCB) and interface with a telecommunications access network.|$|E
50|$|Each input <b>line</b> <b>card</b> spreads its packets evenly to the N buffers, {{something}} it can clearly do without contention. Each buffer writes these packets {{into a single}} buffer-local memory at a combined rate of R. Simultaneously, each buffer sends packets {{at the head of}} each virtual output queue to each output <b>line</b> <b>card,</b> again at rate R/N to each card. The output <b>line</b> <b>card</b> can clearly forward these packets out the line with no contention.|$|E
50|$|The Stanford group {{investigating}} load-balanced switches is {{concentrating on}} implementations where {{the number of}} buffers {{is equal to the}} number of <b>line</b> <b>cards.</b> One buffer is placed on each <b>line</b> <b>cards,</b> and the two interconnection meshes are actually the same mesh, supplying rate 2R/N between every pair of <b>line</b> <b>cards.</b> But the basic load-balanced switch architecture does not require that the buffers be placed on the <b>line</b> <b>cards,</b> or that there be the same number of buffers and <b>line</b> <b>cards.</b>|$|R
5000|$|... #Subtitle level 2: MIDS Receiver Synthesizer <b>line</b> <b>cards</b> {{variants}} ...|$|R
5000|$|... #Caption: Cisco 6509 switch {{with four}} <b>line</b> <b>cards</b> and dual {{supervisors}} ...|$|R
5000|$|... #Caption: Northern Telecom World <b>Line</b> <b>Card</b> NT6X17BA, circa 1995 ...|$|E
5000|$|... #Caption: A Western Electric400G <b>line</b> <b>card</b> for 1A2 key systems ...|$|E
5000|$|... #Caption: Northern Telecom DMS-100 <b>Line</b> <b>Card</b> Drawer showing line cards.|$|E
5000|$|Most chassis-based Catalyst {{models have}} {{the concept of}} {{field-replaceable}} [...] "supervisor" [...] cards. These work by separating the <b>line</b> <b>cards,</b> chassis, and processing engine (mirroring most Cisco router designs). The chassis provides power and a high-speed backplane, the <b>line</b> <b>cards</b> provide interfaces to the network, and the processing engine moves packets, participates in routing protocols, etc. This gives several advantages: ...|$|R
50|$|ISDN {{lines are}} served by {{individual}} <b>line</b> <b>cards</b> in an ISLU (Integrated Services Line Unit).|$|R
5000|$|<b>Line</b> <b>cards</b> being {{inserted}} {{too quickly}} (and thus the stall removal signal is not received) ...|$|R
50|$|This {{method of}} {{forwarding}} was first introduced with the Supervisor 2 engine. When used in combination with a switch fabric module, each <b>line</b> <b>card</b> has an 8Gbit/s connection to the switch fabric and additionally {{a connection to the}} classic bus. In this mode, assuming all line cards have a switch fabric connection, an ingress packet is queued as before and its headers are sent along the dBus to the supervisor. They are looked up in the PFC (including ACLs etc.) and then the result is placed on the rBus. The initial egress <b>line</b> <b>card</b> takes this information and forwards the data to the correct <b>line</b> <b>card</b> along the switch fabric. The main advantage here {{is that there is a}} dedicated 8 Gbit/s connection between the line cards. The receiving <b>line</b> <b>card</b> queues the egress packet before sending it from the desired port.|$|E
50|$|The SwitchBlade x8100 is an {{advanced}} layer 3 chassis switch with 1.92Tbit/s of switching capacity when two SBx81CFC960 control cards are installed. It {{is available in}} two chassis sizes, 6-slot (SBx8106) and 12-slot (SBx8112). The 12-slot chassis has 10 <b>line</b> <b>card</b> slots and 2 controller card slots. The 6-slot chassis has 4 <b>line</b> <b>card</b> slots, 1 controller card slot, and one additional slot that can accommodate either a <b>line</b> <b>card</b> or controller card. It also features four hotswappable PSU bays, supporting load sharing and redundancy for both system and POE power. It {{is among the most}} power-efficient switches in its class.|$|E
50|$|The digital <b>line</b> <b>card</b> {{circuitry}} {{is mounted}} on a 31.75 cm by 25.40 cm (12.5 in by 10 in) double-sided printed circuit board. The card connects to the backplane through a 160-pin edge connector. The faceplate of the digital <b>line</b> <b>card</b> {{is equipped with a}} red LED that lights when the card is disabled.|$|E
50|$|As {{shown in}} the figure to the right, a load-balanced switch has N input <b>line</b> <b>cards,</b> each of rate R, each {{connected}} to N buffers by a link of rate R/N. Those buffers are in turn each connected to N output <b>line</b> <b>cards,</b> each of rate R, by links of rate R/N. The buffers in the center are partitioned into N virtual output queues.|$|R
50|$|Since {{an access}} network element is usually {{intended}} to interface many users (typically a few thousand), some exchanges have multiple <b>line</b> terminations per <b>card.</b> Likewise, one network element can have many <b>line</b> <b>cards.</b>|$|R
50|$|Digital <b>line</b> <b>cards</b> {{are housed}} in the Intelligent Peripheral Equipment (IPE) Modules. Up to 16 cards are supported.|$|R
5000|$|... 168 pin {{electrical}} connection (designed {{to be built}} into a <b>line</b> <b>card)</b> ...|$|E
5000|$|... 10 slots: 1-4 and 7-10 are <b>line</b> <b>card</b> slots, 5-6 are {{supervisor}} slots ...|$|E
5000|$|... 18 slots: 1-8 and 11-18 are <b>line</b> <b>card</b> slots, 9-10 are {{supervisor}} slots ...|$|E
40|$|This paper {{proposes a}} novel scheme which can {{efficiently}} reduce the energy consumption of Optical Line Terminals (OLTs) in Time Division Multiplexing (TDM) Passive Optical Networks (PONs) such as EPON and GPON. Currently, OLTs consume {{a significant amount}} of energy in PON, {{which is one of the}} major FTTx technologies. To be environmentally friendly, it is desirable to reduce energy consumption of OLT as much as possible; such requirement becomes even more urgent as OLT keeps increasing its provisioning data rate, and higher data rate provisioning usually implies higher energy consumption. In this paper, we propose a novel energy-efficient OLT structure which guarantees services of end users with the smallest number of power-on OLT <b>line</b> <b>cards.</b> More specifically, we adapt the number of power-on OLT <b>line</b> <b>cards</b> to the real-time incoming traffic. Also, in order to avoid service disruption resulted by powering off OLT <b>line</b> <b>cards,</b> proper optical switches are equipped in OLT to dynamically configure the communications between OLT <b>line</b> <b>cards</b> and ONUs. Comment: 9 Pages, 9 figure...|$|R
50|$|VoIP Line (VoIP Sets), the Meridian 1 {{can have}} IP <b>Line</b> <b>cards</b> {{added to it}} to support VoIP sets.|$|R
5000|$|<b>Line</b> <b>cards</b> being {{inserted}} {{too slowly}} (and thus {{the bus is}} stalled for too long and forces a reload).|$|R
50|$|A {{standalone}} Carrier Routing System is deployed with a <b>Line</b> <b>card</b> chassis (LCC). The {{three main}} functional units of this LCC are the Line cards, Switching fabric and Route processor. The <b>Line</b> <b>card</b> {{consists of the}} physical interface card and a modular services card. The physical connectivity could be using Fiber optic cables or using Twisted pair cables. The routing decisions are made by the route processor and the switching fabric {{takes care of the}} routing based on the Route processor input. The CRS runs IOS XR which is said to be designed for high-end carrier grade routers and was launched with CRS-1. In a multi-chassis deployment, the <b>Line</b> <b>card</b> chassis is used along with another variety of chassis called as the Fabric Card Chassis (FCC). The architecture enables scalability by increasing the number of <b>Line</b> <b>Card</b> Chassis and/or Fabric Card Chassis. In both single- and multi-chassis configurations, the CRS switch fabrics are based on a three-stage Beneš architecture. In a single-chassis system, the three switching stages—S1, S2, and S3—are all contained on one fabric card. In a multi-chassis system, the S2 stage is contained within the Fabric Card Chassis, with the S1 and S3 stages resident in the <b>Line</b> <b>Card</b> Chassis.|$|E
50|$|To prevent bus errors, {{the chassis}} has three pins in each slot which {{correspond}} with the <b>line</b> <b>card.</b> Upon insertion, the longest of these makes first contact and stalls the bus (to avoid corruption). As the <b>line</b> <b>card</b> is pushed in further, the middle pin makes the data connection. Finally, the shortest pin removes the bus stall {{and allows the}} chassis to continue operation.|$|E
50|$|When the {{subscriber}} line is called, a relay on the subscriber <b>line</b> <b>card</b> {{connects the}} ringing generator to the subscriber line. The exchange also sends a ringback tone to the calling party. When the called party answers {{by taking the}} telephone handset off the switchhook, the subscriber's telephone draws direct current from the central office battery. This current is sensed by the <b>line</b> <b>card</b> and the ringing relay is de-energized.|$|E
5000|$|Introduction of E-MetroTel UCX4.5 MDSE package by {{leveraging}} MGC <b>card,</b> <b>line</b> <b>cards</b> and cabinets provides upgrade {{path for}} existing customers ...|$|R
5000|$|<b>Line</b> <b>cards</b> being {{inserted}} incorrectly (and {{thus making}} contact {{with only the}} stall and data pins and thus not releasing the bus) ...|$|R
5000|$|Certain <b>line</b> <b>cards</b> in Cisco 12000 routers are {{potentially}} vulnerable to denial-of-service attacks. [...] Additionally, certain software versions were vulnerable to specially crafted IPv4 packets.|$|R
50|$|A <b>line</b> <b>card</b> {{typically}} interfaces {{the twisted}} pair cable of a {{plain old telephone service}} (POTS) local loop {{to the public}} switched telephone network (PSTN). Telephone line cards perform multiple tasks, such as analog-to-digital and digital-to-analog conversion of voice, off-hook detection, ring supervision, line integrity tests, and other BORSCHT functions. In some telephone exchange designs, the line cards generate ringing current and decode DTMF signals. The <b>line</b> <b>card</b> in a subscriber loop carrier is called a subscriber line interface card (SLIC).|$|E
50|$|In April 2011, Juniper {{deployed}} a 100GbE {{system on}} the UK education network JANET. In July 2011, Juniper announced 100GbE with Australian ISP iiNet on their T1600 routing platform. Juniper started shipping the MPC3E <b>line</b> <b>card</b> for the MX router, a 100GbE CFP MIC, and a 100GbE LR4 CFP optics in March 2012. In Spring 2013, Juniper Networks announced {{the availability of the}} MPC4E <b>line</b> <b>card</b> for the MX router that includes 2 100GbE CFP slots and 8 10GbE SFP+ interfaces.|$|E
50|$|Through the 2000s, Lifeboat {{expanded}} its vendor <b>line</b> <b>card</b> with established and emerging vendors such as InstallShield (later to become Flexera Software), Intel Software, TechSmith, GFI, and VMware.|$|E
40|$|Abstract—We {{estimate}} {{potential energy}} savings in IP-over-WDM networks achieved by switching off router <b>line</b> <b>cards</b> in low-demand hours. We compare three approaches to react on dynamics in the IP traffic over time, FUFL, DUFL and DUDL. They provide {{different levels of}} freedom in adjusting the routing of lightpaths in the WDM layer and the routing of demands in the IP layer. Using MILP models based on realistic network topologies and node architectures as well as realistic demands, power, and cost values, we show that already a simple monitoring of the lightpath utilization in order to deactivate empty <b>line</b> <b>cards</b> (FUFL) brings substantial benefits. The most significant savings, however, are achieved by rerouting traffic in the IP layer (DUFL), which allows emptying and deactivating lightpaths together with the corresponding <b>line</b> <b>cards.</b> A sophisticated reoptimization of the virtual topologies and the routing in the optical domain for every demand scenario (DUDL) yields nearly no additional profits in the considered networks. I...|$|R
50|$|The <b>line</b> <b>cards</b> in a {{telephone}} central office switch that are interfaced to analog lines include hybrids that adapt the four-wire network to the two-wire circuits that connect most subscribers.|$|R
40|$|Abstract—The {{scalability}} {{and performance}} of the Internet depends critically {{on the performance of}} its packet switches. Current packet switches are based on single-hop crossbar fabrics, with <b>line</b> <b>cards</b> that use virtual output-queueing to reduce head-of-line blocking. In this paper we propose to use a multi-hop network on a chip (NOC) as the crossbar fabric, with FIFO-queued <b>line</b> <b>cards.</b> The use of a multi-hop crossbar fabric has several advantages. 1) Speed-up, i. e. the crossbar fabric can operate faster because NOC inter-router wires are shorter than those in a single-hop crossbar, and because arbitration is distributed instead of centralised. 2) Load balancing because paths from different input-output port pairs share the same router buffers, unlike the internal buffers of buffered crossbar fabric that are dedicated to a single inputoutput pair. 3) Path diversity allows traffic from an input port to follow different paths to its destination output port. This results in further load balancing, especially for non-uniform traffic patterns. 4) Simpler line-card design: the use of FIFOs on the <b>line</b> <b>cards</b> simplifies both the <b>line</b> <b>cards</b> and the (interchip) flow control between the crossbar fabric and <b>line</b> <b>cards,</b> reducing the number of (expensive) chip pins required for flow control. 5) Scalability, {{in the sense that the}} crossbar speed is independent of the number of ports, which is not the case for single-hop crossbar fabrics. We analyzed the performance of our architecture both analytically and by simulation, and show that it performs well for a wide range of traffic conditions and switch sizes. Additionally we prototyped a 32 × 32 NOC-based crossbar fabric in a 65 nm CMOS technology. The unoptimised implementation operates at 413 MHz, achieving an aggregate throughput in excess of 10 10 ATM cells per second. I...|$|R
