7|80|Public
40|$|The role {{of reverse}} {{engineering}} system metadata when migrating legacy systems to enterprise software such as PeopleSoft {{has not been}} widely articulated. Bridging the gap between new enterprisewide software systems and legacy systems {{has proven to be}} an enormous and costly hurdle when attempted without sufficient understanding of the <b>legacy</b> <b>environment</b> and enterprise metadata. We present a model-based methodology for constructing a metadata-based foundation for migrating data from legacy systems to enterprise software. The methodology components include:. analysis of project organization, constraints and definition,. development of project control structure, and. deployment of toolkit components to capture, analyze and utilize metadata...|$|E
40|$|The {{widespread}} {{interest in}} software process {{has led to}} the creation of a wide range of process-centered environments, each seemingly with a different process formalism. This eventuality presents us with two questions that we must address in order to build a flexible process component for incorporation into existing software environments. Such a component would be desirable, for example, to facilitate adding a notion of process to a development organization's <b>legacy</b> <b>environment.</b> One of these questions is this question of competing formalisms; the other is the ease with which the component can be integrated with existing systems. This paper describes an environment integration experiment directed toward these questions. In this experiment, we replaced the process interpretation components of another process-based environment with our Amber rule-based process server, translating also from that environment's process formalism into our "process assembly language". Copyright 1995 Steven S [...] ...|$|E
40|$|Emulation is {{frequently}} discussed as a failsafe preservation strategy for born-digital documents {{that depend on}} contemporaneous software for access (Rothenberg, 2000). Yet little {{has been written about}} the contextual knowledge required to successfully use such software. The approach we advocate is to preserve necessary contextual information through scripts designed to control the <b>legacy</b> <b>environment,</b> and created during the preservation workflow. We describe software designed to minimize dependence on this knowledge by offering automated configuration and execution of emulated environments. We demonstrate that even simple scripts can reduce impediments to casual use of the digital objects being preserved. We describe tools to automate the remote use of preserved objects on local emulation environments.   This can help eliminate both a dependence on physical reference workstations at preservation institutions, and provide users accessing materials over the web with simplified, easy-to-use environments. Our implementation is applied to examples from an existing collection of over 4, 000 virtual CD-ROM images containing thousands of custom binary executables...|$|E
40|$|Face-off: AOP+LMP vs. legacy {{software}} This paper applies {{a mix of}} aspect-oriented programming (AOP) {{and logic}} meta-programming (LMP) to tackle some concerns of/in <b>legacy</b> <b>environments.</b> We present four different problems, and illustrate —with code — how far AOP+LMP gets us. The <b>legacy</b> <b>environments</b> subjected to this treatment encompass the two major players: C, and (of course) Cobol. The aspect code is based on, respectively, Aspicere and Cobble, two aspect languages (being) developed by the authors. 1...|$|R
50|$|Xsun is an X Window System (X11) {{display server}} {{implementation}} included with Solaris, developed by Sun Microsystems. It replaced the older Xnews server, which supported {{the display of}} not only X11 applications, but also NeWS and SunView programs. Xsun discontinued support for these <b>legacy</b> <b>environments,</b> and added support for Display PostScript.|$|R
50|$|Logical Volume Management {{allows for}} {{spanning}} a file system across multiple devices {{for the purpose}} of adding redundancy, capacity, and/or throughput. <b>Legacy</b> <b>environments</b> in Solaris may use Solaris Volume Manager (formerly known as Solstice DiskSuite). Multiple operating systems (including Solaris) may use Veritas Volume Manager. Modern Solaris based operating systems eclipse the need for Volume Management through leveraging virtual storage pools in ZFS.|$|R
40|$|The Replacement of {{a legacy}} system with newly reengineered Information System {{has always been}} a {{challenging}} task. The end-users and the integrated software environment of legacy system make things harder for the deployment team. The legacy data migration challenge complicates the very important deployment phase. In this paper however, we have discussed and analyzed three different strategies to deploy a newly reengineered Information System and replace the legacy system in a homogeneous environment. This paper mainly addresses the issues concerning minimum disturbance to the <b>legacy</b> <b>environment</b> and smooth deployment of the new Information System. It is pertinent to mention that the issues regarding the development of reengineered information system or the legacy data migration have not been discussed here. At the end we have provided a detailed analysis and finally recommended the most optimal strategy among all. The rationale behind the suggested solutions is an extensive study conducted at our research and development Lab...|$|E
40|$|Research {{during the}} latter half of 2011 has {{indicated}} that in addition to the hazard arising from the radium content of radium-dial watches, significant radon concentrations that exceed the UK Domestic and Workplace Action Levels of 200 Bq/m 3 and 400 Bq/m 3, respectively, can arise from watches stored in the built environment (1). In an extension to that earlier research, the radon emanations from the watches are being investigated in order to evaluate the radon hazard and the effective radium content of the watches. These radon measurements are made by placing the watches in a sealed chamber in a closed loop with a Durridge RAD 7. We report here preliminary results from this ongoing investigation which suggest that radon emanation is not necessarily a straightforward function of radium content, as anticipated, but also depends on chamber temperature and humidity impacting upon watch-dependent factors such as design, construction, materials and wear-and-tear. Reference. 1. Gillmore G K, Crockett R G M, Denman A R, Flowers A, Harris R; Radium dial watches, a potentially hazardous <b>legacy?</b> <b>Environment</b> International. 45, 91 – 98. 2012. doi: 10. 1016 /j. envint. 2012. 03. 013...|$|E
40|$|There is renewed {{interest}} in mobile technologies in aged care. While the technologies themselves emerged some decades ago, it is only now {{that some of the}} barriers to adoption are being overcome and implementation is seen as more practical and cost-effective. The interest in mobile technologies in aged care stems from the expectation that they will assist integration of information into clinical practice, and particularly for assisting clinicians at the point-of-care. Desktop-based systems have not been well adopted by hospital doctors and mobile technologies may better suit the work-practices of these and other mobile clinicians. There are expectations of efficiency gains from the introduction of mobile technologies in aged care. There is significant work to be done in developing the convincing business case for investment, applying project management rigor, preparing user interfaces, reviewing work-practices and otherwise exploring the changes and benefits of the technologies. Other issues to be explored include integration into the <b>legacy</b> <b>environment,</b> privacy and security. What is influencing the slow uptake of mobile solutions, especially in aged care, appears to be a framework to transform current business practices that are conducive to mobile business models. This research in progress paper provides an initial introduction to m-health, then analyses few existing e-business models and proposes framework that can transform current aged care businesses into an m-business. The paper also outlines a method to test the m-business transformation framework...|$|E
5000|$|In January 2007, Aldon joined Microsoft Corporation’s Midrange Alliance Program. The MAP {{was created}} in 2005 to help iSeries users {{modernize}} their <b>legacy</b> <b>environments</b> in the following ways: extending OS/400 applications to Windows, integrating OS/400 and Windows applications and ultimately migrating workloads to Windows from OS/400. [...] Aldon was noted as the first provider of change management software to join the group.|$|R
40|$|We {{consider}} Real-Time CORBA 1. 2 ’s distributable threads (DTs), {{whose time}} constraints are specified using time/utility functions (TUFs), operating in <b>legacy</b> <b>environments.</b> In <b>legacy</b> <b>environments,</b> system node resources—both physical and logical—are shared among time-critical DTs and local applications that {{may also be}} time-critical. Hence, DTs that are scheduled using their propagated TUFs, as mandated by Real-Time CORBA 1. 2 ’s Case 2 approach, may suffer performance degradation, if a node utility accrual (UA) scheduler achieves higher locally accrued utility by giving higher eligibility to local threads than to DTs. To alleviate this, we consider decomposing TUFs of DTs into “sub-TUFs ” for scheduling segments of DTs. We present decomposition techniques called UT, SCEQF, SCALL, OP T CON, and T UF S, which are specific to different classes of UA algorithms, {{such as those that}} use utility density, and those that use deadline as their key decision metric. Our experimental studies show that OP T CON and T UF S perform best for utility density-based UA algorithms, and SCEQF and SCALL perform best for deadline-based UA algorithms...|$|R
40|$|This paper {{describes}} {{our experiences}} with applying dynamic analysis solutions {{with the help}} of Aspect Orientation (AO) on an industrial legacy application written in C. The purpose of this position paper is two-fold: (1) we want to show that the use of Aspect Orientation to perform dynamic analysis is particularly suited for <b>legacy</b> <b>environments</b> and (2) we want to share our experiences concerning some typical pitfalls when applying any reverse engineering technique on a legacy codebase. 1...|$|R
40|$|ITC/USA 2014 Conference Proceedings / The Fiftieth Annual International Telemetering Conference and Technical Exhibition / October 20 - 23, 2014 / Town and Country Resort & Convention Center, San Diego, CADuring concept {{development}} of a new core analog acquisition system, Boeing Flight Test identified a need for a set of more efficient and cost effective Test System configuration and setup tools, preferably supported by an industry standard. Like most big test organizations we support years and years of legacy tools. Currently all new functions are required to be hosted within the <b>legacy</b> <b>environment.</b> Legacy environments tend to be big, slow, and expensive to update and maintain. In searching for a better way to do business, we evaluated iNET/MDL, IHAL, and XidML standards. For a variety of reasons which will be discussed in this paper, we have chosen to focus on the iNET MDL standard as the means for producing a new vendoragnostic, simpler and more cost-effective system interface. Our initial evaluation uncovered several gaps in the data structure and concept of operations. The iNET community acknowledged the gaps and encouraged us to work with them to enhance the standard. The iNET MDL concept of operations also represents a significant operational paradigm shift. Through an industry users group, we have been working to refine and enhance the data structures and concept of operations. This paper will describe the journey from a demonstration environment to an enterprise implementation of MDL as it relates to data acquisition setup and control...|$|E
40|$|Abstract—In this {{contribution}} {{an innovative}} platform is being presented that integrates intelligent agents in <b>legacy</b> e-learning <b>environments.</b> It introduces {{the design and}} development of a scalable and interoperable integration platform supporting various assessment agents for e-learning environments. The agents are implemented {{in order to provide}} intelligent assessment services to computational intelligent techniques such as Bayesian Networks and Genetic Algorithms. The utilization of new and emerging technologies like web services allows integrating the provided services to any web based <b>legacy</b> e-learning <b>environment...</b>|$|R
40|$|We {{consider}} RealTime CORBA 2. 0 (Dynamic Scheduling) distributable threads, {{whose time}} constraints are specified using time/utility functions (TUFs), operating in <b>legacy</b> <b>environments.</b> In <b>legacy</b> <b>environments,</b> system node resources—both physical (processor, disk, I/O, etc.) and logical (locks, etc.) —are shared among timecritical distributable threads and local applications {{that may or}} may not be timecritical. Thus, in such environments, distributable threads that are scheduled using their propagated TUFs and scheduling parameters, as mandated by RealTime CORBA 2. 0 ’s Case 2 approach, may suffer performance degradation, if a node scheduler can achieve higher local accrued utility by giving higher eligibility to local threads than to distributable threads. To alleviate this, we consider decomposing TUFs of distributable threads into “subTUFs ” that are used for scheduling segments of distributable threads. We present methods for decomposing TUFs. Furthermore, we identify conditions under which TUF decomposition can alleviate perfor mance degradation. Our experimental results reveal that the most important factors that affect the performance of TUF decomposition include the properties of node scheduling algorithms, TUF shapes, task load, Global Slack F actor, local threads and resource dependencies, and that these factors interact. Index Terms realtime distributed systems, time/utility functions, realtime scheduling, soft realtime sys tems, time constraint decomposition, realtime CORBA I...|$|R
40|$|Abstract. Dynamically {{provisioned}} virtual clusters {{provide a}} means of consolidating servers in a data center and for supporting utility computing. Data centers typically sport {{a large number of}} (layer 2) switches and very few routers, yet, the existing layer- 2 QoS are not well developed. This paper proposes the notion of virtual link as an interconnection abstraction to provide granular QoS. The paper also presents an experimental study comparing virtual link based congestion control against other alternatives for emerging 10 Gb/sec Ethernet links. It is shown that virtual links can provide desired capabilities with a small perturbation to existing standards and can work well in mixed <b>legacy</b> <b>environments.</b> ...|$|R
40|$|Porting {{software}} {{usually requires}} understanding what library functions the program being ported uses since this functionality must be either found or reproduced in the ported program's new environment. This is usually done manually through code inspections. We propose a type inference algorithm able to infer basic {{information about the}} library functions a particular C program uses {{in the absence of}} declaration information for the library (e. g., without header files). Based on a simple but efficient inference algorithm, we were able to infer declarations for much of the PalmOS API from the source of a twenty-seven-thousand-line C program. Such a tool will aid in the problem of program understanding when porting programs, especially from poorly-documented or lost <b>legacy</b> <b>environments...</b>|$|R
40|$|This paper {{describes}} {{our experiences}} of applying dynamic analysis solutions on an industrial legacy application written in C, {{with the help}} of Aspect Orientation (AO). We use a number of dynamic analysis techniques that can help in alleviating the problem of (1) establishing the quality of the available regression test and (2) regaining lost knowledge of the application. We also show why our aspect language for C, aspicere, is well-suited for using dynamic analysis in <b>legacy</b> <b>environments.</b> Finally, we present the case study itself, the results we have obtained and the validation thereof by the original developers and current maintainers of the application. We also mention some typical pitfalls that we encountered while dealing with legacy applications in a reengineering context. ...|$|R
50|$|Since {{the release}} of Windows 2000, the use of WINS for name {{resolution}} has been deprecated by Microsoft, with hierarchical Dynamic DNS now configured as the default name resolution protocol for all Windows operating systems. Resolution of (short) NETBIOS names by DNS requires that a DNS client expand short names, usually by appending a connection-specific DNS suffix to its DNS lookup queries. WINS can still be configured on clients as a secondary name resolution protocol for interoperability with <b>legacy</b> Windows <b>environments</b> and applications. Further, Microsoft DNS servers can forward name resolution requests to legacy WINS servers {{in order to support}} name resolution integration with <b>legacy</b> (pre-Windows 2000) <b>environments</b> that do not support DNS.|$|R
40|$|An {{important}} issue of distributed systems is interoperability. Lack of interoperability between distributed systems {{is a common}} problem with current and legacy applications. Since the legacy applications represent a major investment {{and for the most}} part can not be abandoned. Enterprises are looking for solutions that enable the integration of the <b>legacy</b> <b>environments</b> with the new breed of distributed applications, as well as the integration between their existing applications caused either by mergers and acquisitions, or a company's desire to move into a new and uncharted business activity. This paper discusses the affect of legacy systems migration and integration utilizing the existing of-the-shelf middleware. It presents an analysis of distributed systems concepts and design as seen from the perspective of migration and integration of a monolithic legacy system into a distributed system, and it concludes that distributed systems protocols and architecture must be tailored to various levels within the legacy systems in order for an integration project to succeed. 1...|$|R
30|$|In this section, {{we discuss}} {{some of the}} issues that impair the {{throughput}} of RoF-based WLANs and degrade the fairness between an RoF-based WLAN and <b>legacy</b> WLANs in <b>environments</b> where they coexist.|$|R
5000|$|Provide {{the right}} {{information}} at the right place: requires information sharing systems and integration platforms capable of handling information transaction across heterogeneous environments consisting of heterogeneous hardware, different operating systems and monolithic software applications (<b>legacy</b> systems). <b>Environments</b> which cross organizational boundaries and link the operation of different organisations on a temporal basis and with short set-up times and limited time horizon (extended and virtual enterprises).|$|R
40|$|From the Single Glass Program Plan, dated August 18, 1997 : Vision: Provide BCAG users {{access to}} all {{applications}} and data needed to perform their respective jobs from a single desktop environment with acceptable levels of function, performance, and reliability. The Single Glass program in the Boeing Commercial Airplane Group (BCAG) provides Engineering UNIX users the ability to access all required UNIX and PC applications and associated data via a standardized Common Desktop Environment (CDE) desktop. The goal is {{to do this with}} enough performance, reliability, and transparency so that each engineer only needs one computer (i. e., a &quot;Single piece of Glass &quot;) on their desk, thereby reducing the number of desktop devices required per engineer. There is an additional process benefit by increasing the amount of concurrent design and analysis possible. The Single Glass desktop provides unified access to over 350 locally executed applications previously provided in a number of separate <b>legacy</b> <b>environments</b> from the shell and also within a common CDE look-and-feel developed through formal usability studies done in Seattle and Wichita. Currently running on over 5000 IB...|$|R
50|$|Performance was {{especially}} problematic because early expert systems were built using {{tools such as}} Lisp, which executed interpreted (rather than compiled) code. Interpreting provided an extremely powerful development environment but with the drawback that it was virtually impossible to match {{the efficiency of the}} fastest compiled languages, such as C. System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments - programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with <b>legacy</b> <b>environments</b> such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client-server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.|$|R
5000|$|IRI Liquid Data (ILD) - The Liquid Data {{platform}} offers {{data management}} capabilities driven through hardware, software, patented algorithms, industry models, data integration and supporting applications that are {{offered as a}} hosted service or as a separate infrastructure within the customer’s <b>legacy</b> data <b>environment.</b> The capability platform holds data in a flat or unstructured universe of points and leverages predictive automated analytics to generate information and insights to support specific decisions in minutes.|$|R
50|$|Early widget toolkits for X {{included}} Xaw (the Athena Widget Set, 1983), OLIT (OPEN LOOK Intrinsics Toolkit, 1988), XView (1988), Motif (1980s) and Tk. OLIT and XView {{function as}} the base toolkits for Sun's <b>legacy</b> OpenWindows desktop <b>environment.</b>|$|R
50|$|The {{original}} version of GCOS {{was developed by}} General Electric from 1962; originally called GECOS (the General Electric Comprehensive Operating Supervisor). The operating system is still used today in its most recent versions (GCOS 7 and GCOS 8) on servers and mainframes produced by Groupe Bull, primarily through emulation, to provide continuity with <b>legacy</b> mainframe <b>environments.</b> Note that GCOS 7 and GCOS 8 are separate branches of the operating system {{and continue to be}} developed alongside each other.|$|R
40|$|Purpose: As digital {{resources}} proliferate, libraries plan {{to grant}} {{easy access to}} a distributed set of resources from one single entry point {{inside and outside the}} OPAC. The quest to manage the metadata about these resources becomes more important than ever. Thus, the term, “metadata management” is being used by various communities creating spatial data, enterprise applications, data warehouses, <b>legacy</b> <b>environments,</b> and bibliographic data. Unfortunately, metadata management is sparsely mentioned in the traditional information technology journals, grey literature, information technology company web sites, and the library science literature. The purpose of this viewpoint is to examine the limited use of the term metadata management in the library community and to introduce a new definition of it. Design/methodology/approach: This viewpoint examines the limited use of the term metadata management in the library community and introduces a new definition of it. Findings: Although the proposed definition captures the activities that libraries should be engaged as they provide access to millions of resources, this definition should constantly be examined as new technologies emerge, personnel change, and financial resources diminish. Originality/value: The author’s definition is a good start; however, to get to the complete definition of metadata management, a more comprehensive look at the workflow and procedures that exist in libraries for managing metadata is necessary...|$|R
40|$|The Microsoft. NET Framework {{represents}} a major advance over previous runtime environments available for Windows platforms {{and offers a}} number of architectural features that would be of value in scientific programs. However there are such major differences between. NET and <b>legacy</b> <b>environments</b> under both Windows and Unix, that the effort of migrating software is substantial. Accordingly, software migration is unlikely to occur unless tools are developed for supporting this process. In this {{paper we discuss a}} `relative debugger' called Guard which provides powerful support for debugging programs as they are ported from one environment or platform to another. We describe a prototype implementation developed for Microsoft's Visual Studio. NET - a rich interactive environment that supports code development for the. NET Framework. The paper discusses the overall architecture of Guard under VS. NET and highlights some of the technical challenges that were encountered during its development. A simple case study is provided that demonstrates the effectiveness of relative debugging in locating subtle errors that occur when even a minor upgrade is attempted from one version of a language to another. For this example, we illustrate the use of relative debugging using a Visual Basic program that was ported from Visual Basic 6. 0 to Visual Basic. NET...|$|R
5000|$|Service {{providers}} {{are required by}} almost all governments worldwide to enable lawful intercept capabilities. Decades ago in a <b>legacy</b> telephone <b>environment,</b> this was met by creating a traffic access point (TAP) using an intercepting proxy server that connects to the government's surveillance equipment. This is not possible in contemporary digital networks. The acquisition component of this functionality may be provided in many ways, including DPI, DPI-enabled products that are [...] "LI or CALEA-compliant" [...] can be used - when directed by a court order - to access a user's datastream.|$|R
40|$|When {{a single}} source of {{multimedia}} contents is distributed to multiple reproduction devices, the {{audio and video}} contents require synchronous play for multi-channel stereo sound and lip-synchronization. The multimedia system in vehicle, especially, has researched to move to wireless <b>environments</b> from <b>legacy</b> wired <b>environments.</b> This paper proposes the advanced algorithm for providing synchronized services of real-time multimedia traffic in IEEE 802. 11 WLANs [1]. For these, we implement the advanced IEEE 1588 Precision Time Protocol [2] and the environments for simulation. Also, we estimate and analysis performance of the algorithm, then we experiment and analysis after the porting of algorithm in wireles...|$|R
40|$|International audienceGrid {{technologies}} are appealing {{to deal with}} the challenges raised by computational neurosciences and support multi-centric brain studies. However, core grids middleware hardly cope with the complex neuroimaging data representation and multi-layer data federation needs. Moreover, <b>legacy</b> neuroscience <b>environments</b> need to be preserved and cannot be simply superseded by grid services. This paper describes the NeuroLOG platform design and implementation, shedding light on its Data Management Layer. It addresses the integration of brain image files, associated relational metadata and neuroscience semantic data in a heterogeneous distributed <b>environment,</b> integrating <b>legacy</b> data managers through a mediation layer...|$|R
40|$|Command (JITC) {{located at}} Ft. Huachuca, Arizona, has {{deployed}} the largest multi-vendor IPv 6 backbone to date. Together with the “IPv 6 Ready ” logo program, {{administered by the}} IPv 6 Forum, the Moonv 6 Project tests and promotes IPv 6 ’s most promising features, including improved multi-media streaming, Internet protocol mobility, and an alternative to less scalable and less secure network address translation (NAT) strategies. In an effort to refine this next generation of the Internet protocol (IP), IP equipment, manufacturers and network operators continue to collaborate with government agencies and independent laboratories on Moonv 6 testing. This testing aims to improve the conformance, scalability, and internetworking capability of multiple commercial implementations of IPv 6. The latest round of Moonv 6 interoperability tests began on November 28, 2005 at the UNH-IOL and ran through December 2, 2005. December test objectives addressed the sustained interest in testing IPv 6 technology in <b>legacy</b> <b>environments</b> and began new protocol testing, which extended IPv 4 equivalency further into the access layer. Eleven vendors participated in this latest round of interoperability tests. While previous Moonv 6 events focused predominately on testing core network areas such as routing protocols, {{the objective of the}} December test event was to demonstrate advances in IPv 6 applications. It tested Dynamic Host Configuration Protocol (DHCP), security, and some voice services. The tests demonstrated that these functionalities are fundamentally stable for small deployments. IPv 6 was also tested in the following areas...|$|R
40|$|Opportunistic {{networking}} {{is a new}} communication paradigm which {{explores the}} potential of inter-device contacts due to human mobility [3, 5]. Intermittent connectivity, non existence of an end-to-end path between nodes and extreme dynamism in topological structure are inherent characteristics of opportunistic networking [2, 5]. In opportunistic networking devices make control and management decisions by themselves with locally available information. The unique differences that draw the line between opportunistic networking and other <b>legacy</b> networking <b>environments</b> call for newer approaches for systems development. Distributing content in opportunistic networks is still {{at the level of}} experiments and case studies [4, 7]. Numerous opportunistic networking applications, such a...|$|R
40|$|This paper {{presents}} a programmable pre-cursor ISI equalization circuit for high-speed serial data transmission over highly lossy electrical backplane channels. Although decision-feedback-equalizer (DFE) provides {{an effective way}} to compensate various channel impairments, such as frequency dependent loss, dispersion and reflections in the <b>legacy</b> backplane <b>environment,</b> for high-speed, highly lossy band-limited channel, the pre-cursor inter-symbol interference (ISI) is still a significant problem for channel equalization. A programmable pre-cursor ISI equalizer combined with a 3 -tap DFE is implemented to work at 10 -Gb/s and compensate the channel loss of- 20 dB. The results show it outperform a traditional 5 -tap DFE. Index Terms—Backplane, ISI, equalization, decision-feedback equalizer (DFE), band-limited channel, serial link...|$|R
50|$|The {{concept of}} {{enterprise}} life cycle {{aids in the}} implementation of an enterprise architecture, and the capital planning and investment control (CPIC) process that selects, controls, and evaluates investments. Overlying these processes are human capital management and information security management. When these processes work together effectively, the enterprise can effectively manage information technology as a strategic resource and business process enabler. When these processes are properly synchronized, systems migrate efficiently from <b>legacy</b> technology <b>environments</b> through evolutionary and incremental developments, and the Agency is able to demonstrate its return on investment (ROI). The figure on top illustrates the interaction of the dynamic and interactive cycles as they would occur over time.|$|R
