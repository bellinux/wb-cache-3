11|10000|Public
50|$|When {{the motor}} moves {{a single step}} it {{overshoots}} the final resting point and oscillates round this point as it comes to rest. This undesirable ringing is experienced as motor vibration and is more pronounced in unloaded motors. An unloaded or under loaded motor may, and often will, stall if the vibration experienced is enough to cause <b>loss</b> <b>of</b> <b>synchronisation.</b>|$|E
40|$|We {{describe}} {{techniques of}} encoding compressed images and video {{such that the}} encoded bit stream is resilient to transmission errors which occur on typical noisy channels, particularly to and from mobile users. These errors may occur {{in the form of}} random bit errors, bursty bit errors or packet (cell) losses, or combinations of these, and the proposed techniques are shown to suffer only gradual and graceful degradation as the error rate increases. This contrasts markedly with the severe error sensitivity of conventional image compression standards, such as JPEG and MPEG. A key tool that we employ is the error resilient entropy code (EREC) 6 which largely overcomes the problems of <b>loss</b> <b>of</b> <b>synchronisation</b> and severe error propagation, inherent in most entropy coded data streams. Unlike traditional forward error correction methods, the error-resilient techniques which we employ do not add significant redundancy to the compressed data and therefore do not waste capacity or degrade the com [...] ...|$|E
40|$|Our {{aim was to}} {{investigate}} the possible enhancing role of long-term atmospheric CO 2 increase on wood density as an essential component of biomass sequestration. We therefore assessed the long-term evolution of wood density over pre-industrial and contemporary periods, in a regional context free of management practices, atmospheric deposition and with restricted climatic change. Dominant trees of Araucaria araucana were sampled in 37 stands distributed throughout its natural distribution over temperate forests of the Chilean Andes Cordillera. Mean ring density (MRD) at 1. 30 m was measured by X-ray micro-densitometry. A third-century MRD chronology was built after MRD standardisation by the effects of cambial age and radial growth, simultaneously estimated from a statistical model from rings of the pre-industrial period (1700 - 1850) to avoid any temporal bias. The age-alone standardised MRD chronology showed restricted fluctuations ranging between 2. 0 and 3. 2 % over the last three centuries. Multi-decennial fluctuations between ring width and MRD were found qualitatively synchronous and opposed. Accordingly, MRD fluctuations were removed with the age and growth standardisation, highlighting the absence of a historical wood density trend at constant ring size over the whole period. Over the most recent decades however, a slight increase in density was identified, with <b>loss</b> <b>of</b> <b>synchronisation</b> to radial growth. The absence of a long-term signal in wood density of A. araucana, in a context of restricted anthropogenic influence, differs markedly from reports of significant decreases in the Northern Hemisphere. We conclude as part of the analysis done that until now, increasing atmospheric CO 2 unlikely would have an impact in wood density...|$|E
40|$|This paper {{introduces}} a preliminary study <b>of</b> <b>synchronisation</b> between packet <b>loss</b> events across concurrent TCP flows. Loss synchronisation is explored over four routing paths – one inside Australia and three international paths (Australia to Asia, Europe and North America respectively). We confirm that loss synchronisation does exist but the {{level varies from}} path to path. We outline further work that is required to improve our understanding <b>of</b> <b>loss</b> <b>synchronisation...</b>|$|R
40|$|We use {{a generic}} model for type-I excitability (known as the SNIPER or SNIC model) to {{describe}} the local dynamics of nodes within a network {{in the presence of}} non-zero coupling delays. Utilising the method of the Master Stability Function, we investigate the stability of the zero-lag synchronised dynamics of the network nodes and its dependence on the two coupling parameters, namely the coupling strength and delay time. Unlike in the FitzHugh-Nagumo model (a model for type-II excitability), there are parameter ranges where the stability <b>of</b> <b>synchronisation</b> depends on the coupling strength and delay time. One important implication of these results is that there exist complex networks for which the adding of inhibitory links in a small-world fashion may not only lead to a <b>loss</b> <b>of</b> stable <b>synchronisation,</b> but may also restabilise synchronisation or introduce multiple transitions between synchronisation and desynchronisation. To underline the scope of our results, we show using the Stuart-Landau model that such multiple transitions do not only occur in excitable systems, but also in oscillatory ones. Comment: 10 pages, 9 figure...|$|R
40|$|We {{consider}} {{the performance of}} MPEG-II compressed video when transmitted over noisy channels. We present the results of bit sensitivity and resynchronisation sensitivity measurements, and propose techniques for substantially improving the resilience of MPEG-II to transmission errors without the addition of any extra redundancy into the bitstream. We find that it is errors in variable length encoded data which cause the greatest artifacts as errors in these data can cause <b>loss</b> <b>of</b> bitstream <b>synchronisation.</b> We develop {{the concept of a}} `black box transcoder' where we losslessly transcode MPEGII into a different structure for transmission. We achieve bitstream-resynchronisation using a technique known as error-resilient entropy coding (EREC). Finally we improve the errorresilience of differentially coded information by replacing the standard 1 D-DPCM with a more resilient hierarchical pyramid predictor. We {{consider the}} transmission of MPEG-II over three separate channels: a channel subject [...] ...|$|R
40|$|Babesia divergens is a tick-transmitted apicomplexan {{parasite}} {{for which}} asexual multiplication in its vertebrate hosts {{is restricted to}} erythrocytes. Current knowledge of invasion of these target cells is limited. An efficient in vitro invasion assay {{was set up to}} gain access to this information. Parasites prepared from infected RBC, lysed by electroporation, and mixed with bovine RBC in a selected synthetic medium (RPMI 1640 supplemented with calcium) were able to establish subsequent cultures with parasitemia ranging from 6 to 14 %. Free parasites remaining in the invasion medium could be eliminated by Percoll gradient and culture could be pursued with the freshly invaded erythrocytes. In this way, the invasion time window could be shortened to obtain a synchronised start of the culture or to study the kinetics of invasion. With this assay we demonstrate that 1) erythrocyte invasion by B. divergens is a rapid process since 70 % of the invasion-competent parasites invaded the RBC in less than 45 s; 2) all invasion-competent parasites achieved invasion within 10 min of contact; 3) one erythrocyte could be invaded concomitantly by two merozoites; 4) despite a synchronous start, the parasite population evolved heterogeneously resulting in a progressive <b>loss</b> <b>of</b> <b>synchronisation.</b> Western blot analysis of proteins collected from invasion medium were performed with sera from animals experimentally infected with B. divergens and highlighted several proteins. The dose-dependent, inhibitory effects of these sera on B. divergens invasion suggest that these proteins might be involved in the invasion process. Further investigations are required for their characterisation...|$|E
40|$|The {{invasion}} {{process of}} bovine erythrocyte by Babesia divergens: knowledge from an {{in vitro assay}} Yi Sun 1, 2, 3 *, Emmanuelle Moreau 1, 2, 3, Alain Chauvin 1, 2, 3 and Laurence Malandrin 1, 2 Babesia divergens is a tick-transmitted apicomplexan parasite for which asexual multiplication in its vertebrate hosts is restricted to erythrocytes. Current knowledge of invasion of these target cells is limited. An efficient in vitro invasion assay {{was set up to}} gain access to this information. Parasites prepared from infected RBC, lysed by electroporation, and mixed with bovine RBC in a selected synthetic medium (RPMI 1640 supplemented with calcium) were able to establish subsequent cultures with parasitemia ranging from 6 to 14 %. Free parasites remaining in the invasion medium could be eliminated by Percoll gradient and culture could be pursued with the freshly invaded erythrocytes. In this way, the invasion time window could be shortened to obtain a synchronised start of the culture or to study the kinetics of invasion. With this assay we demonstrate that 1) erythrocyte invasion by B. divergens is a rapid process since 70 % of the invasion-competent parasites invaded the RBC in less than 45 s; 2) all invasion-competent parasites achieved invasion within 10 min of contact; 3) one erythrocyte could be invaded concomitantly by two merozoites; 4) despite a synchronous start, the parasite population evolved heterogeneously resulting in a progressive <b>loss</b> <b>of</b> <b>synchronisation.</b> Western blot analysis of proteins collected from invasion medium were performed with sera from animals experimentally infected with B. divergens and highlighted several proteins. The dose-dependent, inhibitory effects of these sera on B. divergens invasion suggest that these proteins might be involved in the invasion process. Further investigations are required for their characterisation...|$|E
40|$|D. Ing. The digital {{transmission}} of information necessitates the compensation for disturbances introduced by the channel. The compensation method usually used in digital communications is error correcting coding. The errors usually encountered are additive in nature, i. e. errors where only symbol values are changed. Understandably, the field of additive error correcting codes has become a mature research field. Remarkable {{progress has been made}} during the past 50 years, {{to such an extent that}} near Shannon capacity can be reached using suitable coding techniques. Sometimes the channel disturbances may result in the loss and/or gain of symbols and a subsequent loss of word or frame synchronisation. Unless some precautions were made, a synchronisation error may propagate and corrupt large blocks of data. Typical precautions taken against synchronisation errors are: out-of-band clock signals distributed to the transmission equipment in a network; stringent requirements on clock stability and jitter; limits on the number of repeaters and regeneration to curb jitter and delays; line coding to facilitate better clock extraction; and - use of framing methods on the coding level. Most transmission systems in use today will stop data transmission until reliable synchronisation is restored. El multiplexing systems are still the predominantly used technology in fixed telephone line operators and GSM operators, and recovering from a <b>loss</b> <b>of</b> <b>synchronisation</b> (the FAS alarm) typically lasts approximately 10 seconds. Considering that the transmission speed is 2048 KB/s, a large quantity of data is lost in during this process. The purpose of this study is therefore to broaden the understanding of insertion/deletion correcting binary codes. This will be achieved by presenting new properties and coding techniques for multiple insertion/deletion correcting codes. Mostly binary codes will be considered, but in some instances, the results may also hold for non-binary codes. As a secondary purpose, we hope to generate interest in this field of study and enable other researchers to continue to deeper explore the mechanisms of insertion and/or deletion correcting codes...|$|E
40|$|This {{paper is}} {{concerned}} with the performance of MPEG- 2 compressed video when transmitted over noisy channels, a subject of relevance to digital terrestrial television. We present the results of introducing errors into MPEG- 2 video, and propose techniques for substantially improving the resilience of MPEG- 2 to transmission errors without the addition of any extra redundancy into the bitstream. We find that it is errors in variable length data which cause the greatest artefacts as errors in these data can cause <b>loss</b> <b>of</b> bitstream <b>synchronisation.</b> We achieve resynchronisation using a technique known as error-resilient entropy coding (EREC). Finally we improve the error-resilience of differential coded information by replacing the standard 1 D-DPCM with a more resilient hierarchical pyramid predictor. INTRODUCTION Figure 1 shows the first MPEG- 2 (1) intra picture of a coded sequence. It can be clearly seen that a few errors can cause a cause large areas of the picture to be corrupted. Fig. [...] ...|$|R
40|$|IEC Technical Committee 57 (TC 57) {{published}} {{a series of}} standards and technical reports for “Communication networks and systems for power utility automation” as the IEC 61850 series. Sampled value (SV) process buses allow {{for the removal of}} potentially lethal voltages and damaging currents inside substation control rooms and marshalling kiosks, reduce the amount of cabling required in substations, and facilitate the adoption of non-conventional instrument transformers. IEC 61850 - 9 - 2 provides an inter-operable solution to support multi-vendor process bus solutions. A time synchronisation system is required for a SV process bus, however the details are not defined in IEC 61850 - 9 - 2. IEEE Std 1588 - 2008, Precision Time Protocol version 2 (PTPv 2), provides the greatest accuracy of network based time transfer systems, with timing errors of less than 100 ns achievable. PTPv 2 is proposed by the IEC Smart Grid Strategy Group to synchronise IEC 61850 based substation automation systems. IEC 61850 - 9 - 2, PTPv 2 and Ethernet are three complementary protocols that together define the future of sampled value digital process connections in substations. The suitability of PTPv 2 for use with SV is evaluated, with preliminary results indicating that steady state performance is acceptable (jitter < 300 ns), and that extremely stable grandmaster oscillators are required to ensure SV timing requirements are met when recovering from <b>loss</b> <b>of</b> external <b>synchronisation</b> (such as GPS) ...|$|R
30|$|Better {{definition}} and understanding <b>of</b> the <b>synchronisation</b> system and dynamics <b>of</b> <b>synchronisation,</b> particularly {{the distribution of}} coupling strength between sites.|$|R
40|$|Tracking is an {{essential}} requirement for any high energy particle physics experiment. The Compact Muon Solenoid (CMS) detector at the Large Hadron Collider (LHC) employs an all silicon tracker, the largest of its kind, for the precise measurement of track momentum and vertex position. With approximately 10 million detector channels in the strip tracker alone, the analogue non-sparsified readout system {{has been designed to}} handle the large data volumes generated at the 100 kHz Level 1 (L 1) trigger rate. Fluctuations in the event rate are controlled using buffers whose occupancies are constantly monitored to prevent overflows, otherwise causing <b>loss</b> <b>of</b> <b>synchronisation</b> and data. The status of the tracker is reported by the APV emulator (APVe), which has now been successfully commissioned within the silicon strip tracker readout system. The APVe plays {{a crucial role in the}} synchronisation of the tracker by deterministic calculation of the front end buffer occupancy and by monitoring the status of the Front End Drivers (FEDs), where the tracker data is received and processed. In the event that the buffers are close to overflow, the APVe is required to veto L 1 triggers until the system is ready. As such, it is important that APVe is correctly implemented so that the tracker can operate with minimal dead time. The integration of the APVe with the tracker readout and trigger control systems is discussed and the steps taken to en sure its correct operation are presented. The Super-LHC is a proposed LHC machine upgrade to increase the luminosity by a factor of 10. The increased particle fluxes and radiation environment will necessitate the complete replacement of the current CMS tracker while presenting the design of a new tracker with severe challenges. Power consumption is one of the main challenges for the tracker readout system since a higher granularity detector will be required. Physics performance must not be compromised so the tracker material contribution should be lowered where possible. In addition, it is likely that the Level 1 system will require information from the tracker in order to reduce the trigger rate. A method of reducing the on-detector data rate for input into a L 1 trigger using closely separated pixel layers is presented. A detailed simulation of a concept tracker geometry has been developed and the triggering performance has been estimated. The simulations report that the presented tracking trigger layer would be viable for use at SLHC. A layer would be capable of reducing the detector data rate by a factor of ~ 20 while maintaining efficiencies in excess of 96 % for tracks with pT< 2 GeV/c. The information provided by a single stacked layer would not be useful for reducing the L 1 trigger rate, but two stacked layers are able to reconstruct tracks with dpT/pT< 20 % for pT< 20 GeV/c and with sufficient resolution so as to match tracks with L 1 calorimeter objects...|$|E
40|$|Shannon’s source-channel coding {{separation}} theorem {{states that}} near-capacity communication is theoretically possible, when employing Separate Source and Channel Codes (SSCCs), provided that an unlimited encoding/decoding delay and complexity can be afforded. However, it is typically impossible {{to remove all}} source redundancy {{with the aid of}} practical finite-delay and finite-complexity source encoding, which leads to capacity loss. As a potential remedy, Joint Source and Channel Codes (JSCCs) have been proposed for exploiting the residual redundancy and hence for avoiding any capacity loss. However, all previous JSCCs have been designed for representing symbols values that are selected from a set having a low cardinality and hence they suffer from an excessive decoding complexity, when the cardinality of the symbol value set is large, leading to an infinite complexity, when the cardinality is infinite. Motivated by this, we propose the family of Unary Error Correction (UEC), Elias Gamma Error Correction (EGEC) and Reordered Elias Gamma Error Correction (REGEC) codes in this thesis. Our family of codes belong to the JSCC class designed to have only a modest complexity that is independent of the cardinality of the symbol value set. We exemplify the application of each of the codes {{in the context of a}} serially concatenated iterative decoding scheme. In each coding scheme, the encoder generates a bit sequence by encoding and concatenating codewords, while the decoder performs iterative decoding using the classic Logarithmic Bahl, Cocke, Jelinek and Raviv (Log-BCJR) algorithm. Owing to this, our proposed codes are capable of mitigating any potential capacity loss, hence facilitating near-capacity operation. Our proposed UEC code is the first JSCC that maintains a low decoding complexity, when invoked for representing symbol values that are selected from a set having large or even infinite cardinality. The UEC trellis is designed to describe the unary codewords so that the transitions between its states are synchronous with the transitions between the consecutive codewords in the bit sequence. The unary code employed in the UEC code has a simple structure, which can be readily exploited for error correction without requiring an excessive number of trellis transitions and states. However, the UEC scheme has found limited applications, since the unary code is not a universal code. This motivates the design of our EGEC code, which is the first universal code in our code family. The EGEC code relies on trellis representation of the EG code, which is generated by decomposing each symbol into two sub-symbols, for the sake of simplifying the structure of the EG code. However, the reliance on these two parts requires us to carefully tailor the Unequal Protection (UEP) of the two parts for the specific source probability distribution encountered, whilst the actual source distribution may be unknown or non-stationary. Additionally, the complex structure of the EGEC code may impose further disadvantages associated with an increased decoding delay, <b>loss</b> <b>of</b> <b>synchronisation,</b> capacity loss and increased complexity due to puncturing. This motivates us to propose a universal JSCC REGEC code, which has a significantly simpler structure than the EGEC code. The proposed codes were benchmarked against SSCC benchmarkers throughout this thesis and they were found to offer significant gains in all cases. Finally, we demonstrate that our code family proposed in this thesis can be extended by several potential directions. The sophisticated techniques that have been subsequently proposed in the thesis for extending the UEC code, such as irregular trellis designs and the adaptive distribution-learning algorithm, can be readily applied to the REGEC codes which is an explicit benefit of its simple trellis structure. Furthermore, our proposed REGEC code can be extended using techniques that been subsequently proposed for extending the EGEC both to Rice Error Correction (RiceEC) codes and to Exponential Golomb Error Correction (ExpGEC) codes...|$|E
40|$|This {{thesis is}} {{concerned}} with the computation of buffer capacities that guarantee satisfaction of timing and resource constraints for task graphs with aperiodic task execution rates that are executed on run-time scheduled resources. Stream processing applications such as digital radio baseband processing and audio or video decoders are often firm real-time embedded systems. For a real-time embedded system, guarantees on the satisfaction of timing constraints are based on a model. This model forms a load hypothesis. In contrast to hard real-time embedded systems, firm real-time embedded systems have no safety requirements. However, firm real-time embedded systems have to be designed to tolerate the situation that the load hypothesis is inadequate. For stream processing applications, a deadline miss can lead to a drastic reduction in the perceived quality, for instance the <b>loss</b> <b>of</b> <b>synchronisation</b> with the radio stream in case of a digital radio can result in a loss of audio for seconds. For power and performance reasons, stream processing applications typically require a multiprocessor system, on which these applications are implemented as task graphs, with tasks communicating data values over buffers. The established time-triggered and event-triggered design paradigms for real-time embedded systems are not applicable. This is because time-triggered systems are not tolerant to an inadequate load hypothesis, for example non-conservative worst-case execution times, and event-triggered systems have no temporal isolation from their environment. Therefore, we introduce our data-driven approach. In our data-driven approach, the interfaces with the environment are time-triggered, while the tasks that implement the functionality are data-driven. This results in a system where guarantees on the satisfaction of the timing constraints can be provided given that the load hypothesis is adequate. If the load hypothesis is inadequate, then for instance non-conservative worst-case execution times do not immediately result in corrupted data values and undefined functional behaviour. Stream processing applications are increasingly adaptive to their environment, for example digital radios that adapt to the channel condition. This adaptivity results in increasingly intricate sequential control in stream processing applications. The implementations of stream processing applications as task graphs, consequently, have inter-task synchronisation behaviour that is dependent on the processed data stream. Currently, cyclo-static dataflow is the most expressive model that is applicable in our data-driven approach and that can provide guarantees on the satisfaction of timing constraints. However, cyclo-static dataflow cannot express inter-task synchronisation behaviour that is dependent on the processed data stream. Boolean dataflow can express inter-task synchronisation behaviour that is dependent on the processed data stream. However, for boolean dataflow, and models with similar expressiveness, deadlock-freedom is an undecidable property, and there is no known approach to conservatively decide on deadlock-freedom. Since deadlock-freedom guarantees progress, the ability to (conservatively) decide on deadlock-freedom is necessary to guarantee satisfaction of timing constraints. A second trend is that stream processing applications increasingly process more independent streams. Typically, timing constraints are specified per stream. We apply on every shared resource a run-time scheduler that by construction guarantees every task a resource budget. These schedulers allow to provide timing guarantees per stream that are only dependent on the load hypothesis for the processing of this stream. However, currently, there is only limited support for the inclusion of the effects of run-time scheduling in dataflow graphs in order to provide guarantees on the satisfaction of the timing constraints for this stream. The programming of stream processing applications on embedded multiprocessor systems involves the partitioning of the application in a task graph, binding tasks to processors and buffers to memories, selection of scheduler settings, and determination of buffer capacities. All these decisions together should result in a configuration for which we can guarantee that the timing constraints of the application are satisfied. The determination of buffer capacities that are sufficient to guarantee satisfaction of the timing constraints is an essential kernel of automated programming flows for stream processing applications. However, currently, buffer capacities are determined with dataflow analysis by iterating through the possible buffer capacities and for every selection of buffer capacities analyse whether the timing constraints are satisfied. Both the iteration as well as the analysis have an exponential complexity in terms of the graph size. This thesis presents an algorithm that uses a new dataflow model, variable-rate phased dataflow, to compute buffer capacities that guarantee satisfaction of timing and resource constraints for run-time scheduled task graphs that have inter-task synchronisation behaviour that is dependent on the processed data stream. Variable-rate phased dataflow allows to model task graphs with inter-task synchronisation behaviour that is dependent on the processed stream, examples include DRM and DAB digital radio, MP 3 decoding and H. 263 video decoding. Previously, no techniques were available to guarantee the satisfaction of timing constraints for this class of applications. Furthermore, we show that the effects of run-time schedulers can be conservatively included in variable-rate phased dataflow, given that by construction these run-time schedulers provide every task a resource budget. These two essential extensions together with the low run-time and low computational complexity of our algorithm enable automated programming flows for a significantly broader class of applications and architectures...|$|E
40|$|Markovian Process Algebras {{approximate}} {{their model}} <b>of</b> <b>synchronisation</b> events {{in order to}} preserve their Markovian nature. This paper investigates synchronisation models in a stochastic context and focuses on how the Markovian approximation <b>of</b> <b>synchronisation</b> a#ects the accuracy of the performance model. TIPP and PEPA are used as speci#c cases throughout, and their di#erent methods <b>of</b> <b>synchronisation</b> are compared for e#ectiveness in performance modelling. The paper ends with a generally distributed example <b>of</b> real-world <b>synchronisation,</b> whichwe are able to solve analytically and then approximately with four Markovian Process Algebra models. From the results of this analysis, we are able to suggest other Markovian synchronisation models which complement and improve on those presented by TIPP and PEPA...|$|R
50|$|Three levels <b>of</b> <b>synchronisation</b> were defined: asynchronous, synchronous, and adaptive.|$|R
40|$|The {{integration}} of concurrent and object-oriented programming {{has been a}} challenge for over two decades. In this thesis we explore synchronisation in concurrent object-oriented programming and present a clear perspective on the problems in terms of five aspects of synchronisation: exclusion, state, coordination, response and scheduling. These aspects show synchronisation as the multi-faceted concern that it is, rather than the monolithic concern it is normally presented as. For each aspect we identify a set of goals that a concurrent object-oriented programming system should meet for effective support <b>of</b> <b>synchronisation,</b> and for the reuse <b>of</b> <b>synchronisation</b> and functional code. We present the Synchronisation Rings model, a compositional approach to enforcing synchronisation constraints that layers synchronisation controls around a functional core. This composition of different layers <b>of</b> <b>synchronisation</b> control maps neatly onto the different aspects <b>of</b> <b>synchronisation.</b> Using this app [...] ...|$|R
40|$|The {{objective}} {{of this study was}} to explore the basis of variation in the size and composition of grape berries. The investigation focussed on selected aspects of berry development and ripening that were subject to variation. Shiraz and Chardonnay were chosen as experimental varieties because these cultivars presented a large range of variability in the field – Shiraz is susceptible to variation in colour development at veraison, whereas Chardonnay often displays variation in berry size at harvest. The extent of variation within each of the recorded berry parameters was assessed using the coefficient of variation (CV), a unitless measure of sample variability relative to the sample mean, ideally suited to comparative studies. Chapter 1 is a literature review that documents research on selected aspects of grape berry development and ripening which are subject to variation. Berry development is explained in terms of berry set, berry growth and ovule/seed development. Berry composition is described by the relative concentration of sugars, acids, phenolics and flavour compounds in the berry tissues. Variation is discussed with respect to the Australian wine industry and the problem of supply and demand. Techniques for identifying and measuring components of variation are recommended. Experimental hypotheses are developed. Chapter 2 describes an experiment designed to identify when variation in berry size and composition was initiated. The hypothesis was that relative levels of variation in size and composition would remain constant throughout the postflowering period of berry development. The physical properties of individual Shiraz berries were described in terms of their deformability, mass, volume, surface area and seed mass. The phenolic composition of these same individual berries was assessed. A comparison of CVs between sequential developmental stages indicated when variation in a particular physicochemical parameter was initiated. The CV in berry deformability reached a maximum at softening. The CV for berry mass was above 50 % at berry set, but declined as the berries approached harvest maturity. The CVs for berry volume and berry surface area followed a similar trend. Interactions among these parameters were described by linear regressions, multiple regressions and correlation matrices. Seed mass and berry phenolics were analysed for individual berries during the growing, softening and preharvest stages. Both of these parameters were significantly correlated with berry mass, but the relationships were peculiar to each developmental stage. The CV for seed mass increased with maturity. The CV for berry phenolics was lowest during the softening stage. For most parameters, CVs had already attained high levels during the earliest growth stage (setting). The implication is that variation must have arisen at an even earlier time. This places considerable importance on the impact that preflowering events may have on cell division in the floral primordia at budburst. Chapter 3 describes an experiment that sought to identify the extent of variation present during the early developmental stages of berry growth. The hypothesis was that variation in berry size was already significant in the early postflowering period of berry development. Individual Chardonnay berries on two bunches from both ungirdled and girdled vines were assessed on four occasions throughout the flowering period. Individual flowers that had opened during the intervening time period were tagged. One bunch from each vine was sampled at 15 days and another at 43 days after the first flower had opened, giving a range of berry ages: bunch at 15 days comprised berry ages of 1 - 4, 5 - 7, 8 - 11 and 12 - 15 days; bunch at 43 days comprised berry ages of 29 - 32, 33 - 35, 36 - 39 and 40 - 43 days. Frequency distributions of berry mass were plotted for each age class for ungirdled and girdled vines. Distributions were negatively skewed for ungirdled vines and positively skewed for girdled vines. No “shot” berries were observed among bunches sampled from girdled vines at 43 days after flowering. Absolute and relative growth rates were typically higher for berries from girdled vines. The relationship between berry mass and seed mass was unaffected by trunk girdling. CVs for berry mass at all ages in the early bunch sample (15 days) were below 44 %, and were generally lower for the girdled vines. In the later bunch sample (43 days), CVs in berry mass were all higher than those associated with the early bunch sample, and two- to three-fold lower for the girdled vines. This reduction is most likely the result of increased organic nutrition to the bunch counteracting the variation that arises from differences in hormonal stimulus to growth by the developing seed. Chapter 4 describes a novel technique that was developed for the in situ measurement of cell shape and cell size using confocal laser scanning microscopy (CLSM). The technique encompassed the preparation, clearing, staining and whole mounting of large sections of berry tissue. Optical slices were collected at 1 μm intervals to a depth of 150 μm. The digital images were empirically corrected for attenuation of fluorescence intensity and axial distortion due to refractive index mismatch. Cell size and shape were determined from digital 3 D reconstructions of the collected image stack. Cell volumes exhibited a 15 -fold range with polysigmoidal distribution and groupings around specific size classes. The volume of individual, whole parenchyma cells within a block of grape berry mesocarp tissue could be measured in situ to a precision of 2 μm³. Chapter 5 describes an experiment that sought to resolve the relationship between fruit size and cell size in a developing grape berry. The hypothesis was that variation in cell size occurs early in the postflowering period of berry development and that this variation was responsible for the subsequent macroscopic size differences observed between berries. Chardonnay berries belonging to the eight age classes derived in Chapter 3 were sampled from ungirdled and girdled vines. Three berries from each age class were analysed under the CLSM-one “shot”, one “chick” and one “hen”. Volumes were calculated for ten cells from the inner mesocarp tissue of each berry. Differences in cell volume were observed between berry types. Larger berries often had a larger range of cell sizes, indicating they had undergone more cell expansion during the course of their development, but the distribution of cell sizes in “shot” and “chick” berries was similar. Girdling did not affect the berry cell size distribution, but the CVs of girdled vines were ~ 50 % higher. Cell volume for “chicks and “hens” increased between day 15 and day 43, although the increase was proportionally greater for “hens”. Variation in cell size remains relatively constant during the early postflowering development, but variation in berry mass increase regardless. This indicates the operation of different metabolic controls over these two facets of berry growth. Chapter 6 is a general discussion and an attempt to synthesise the salient points from the preceding experimental chapters. In light of the experimental results, it addresses the validity of each of the original hypotheses on variation in berry size and composition. Concepts of developmental synchronisation among berries are viewed from the perspective of changing levels of variation between developmental stages. Increases in CVs signify a <b>loss</b> <b>of</b> <b>synchronisation</b> (asynchronisation) among berries. Reductions in CVs signify a gain in synchronisation (resynchronisation) among berries. The general conclusion is that averages of measurements of the population reveal nothing about the variation within that population. Calculation of variance permits this, but requires that all individuals be measured. Cell size is only one component of the berry size equation. Cell number is the other. While cell size is determined by cell expansion, cell number is linked to cell division. Future applications of techniques developed in this thesis could resolve the interplay between cell division versus cell expansion. A better understanding of these processes might ultimately minimise the impact of variability on the quantity and quality of the Australian winegrape crop. Thesis (Ph. D) [...] University of Adelaide, Dept. of Horticulture, Viticulture and Oenology, 200...|$|E
40|$|Neural {{synchronisation}} plays {{a critical}} role in information processing, storage and transmission. Characterising the pattern <b>of</b> <b>synchronisation</b> is therefore <b>of</b> great interest. It has recently been suggested that the brain displays broadband criticality based on two measures <b>of</b> <b>synchronisation</b> - phase locking intervals and global lability <b>of</b> <b>synchronisation</b> - showing power law statistics at the critical threshold in a classical model <b>of</b> <b>synchronisation.</b> In this paper, we provide evidence that, within the limits of the model selection approach used to ascertain the presence of power law statistics, the pooling of pairwise phase-locking intervals from a non-critically interacting system can produce a distribution that is similarly assessed as being power law. In contrast, the global lability <b>of</b> <b>synchronisation</b> measure is shown to better discriminate critical from non critical interaction. Comment: (v 3) Fixed error in Figure 1; (v 2) Added references. Minor edits throughout. Clarified relationship between theoretical critical coupling for infinite size system and 'effective' critical coupling system for finite size system. Improved presentation and discussion of results; results unchanged. Revised Figure 1 to include error bars on r and N; results unchanged; (v 1) 11 pages, 7 figure...|$|R
50|$|GPS is {{the most}} {{accurate}} time protocol in terms <b>of</b> <b>synchronisation.</b> It is, however, the most expensive.|$|R
30|$|It {{is clear}} {{that some of the}} large {{displacement}} events in New Zealand occur at near to the same time. Thus the same large displacement events can be observed synchronously at several stations (Wallace et al. 2017, Fig.  2). This represents <b>synchronisation</b> <b>of</b> large displacements over distances of at least 220  km. Recently, Wallace et al. (2017) have proposed that the magnitude 7.8 seismic Kaikōura event triggered large displacement events 250 to 600  km away on the North Island for 1 to 2  weeks after the South Island event. Whilst such synchronisation is clear, it is of interest to see if more subtle forms <b>of</b> <b>synchronisation</b> exist and, if so, over what length and time scales does synchronisation occur? Also an understanding <b>of</b> where such <b>synchronisation</b> sits in the classification <b>of</b> <b>synchronisation</b> types described earlier in the paper and details of the frequencies at which synchronisation occurs would shed light on the dynamics of crustal deformation. In what follows, we employ cross-recurrence and joint recurrence plots to detect synchronisation between stations, to clarify details <b>of</b> the <b>synchronisation</b> and to classify the mode <b>of</b> <b>synchronisation.</b>|$|R
5000|$|The {{accuracy}} <b>of</b> frequency <b>synchronisation</b> <b>of</b> {{the transmitter}} oscillators with the receiver oscillators.|$|R
50|$|During 1985 the {{national}} center for the dispatch and control of the electricity network was gradually transferred from the center of Rome to Settebagni, and made a part of a bigger European network <b>of</b> <b>synchronisation</b> <b>of</b> the production of electricity.|$|R
40|$|We {{consider}} the parallel composition of two cyclic programs. The interaction {{of these programs}} consists of a form <b>of</b> <b>synchronisation</b> {{sometimes referred to as}} ‘mutual inclusion’. For a given implementation <b>of</b> this <b>synchronisation</b> by means <b>of</b> semaphore operations, we prove the correctness of the programs and we prove the absence of the danger of deadlock...|$|R
40|$|The role <b>of</b> {{neuronal}} <b>synchronisation</b> in {{the information}} processing performed by (actual) neuronal networks is an actively debated question in neuroscience. Direct experimental measurement <b>of</b> <b>synchronisation</b> requires the recording of the activities of ”as many neurons as possible ” with a fine time resolution. In this context, multi-electrode arrays (MEA) recordings constitute nowadays th...|$|R
50|$|Used by German/Swiss channel Teleclub in {{the early}} 1990s, this system {{employed}} various methods such as video inversion, modification <b>of</b> <b>synchronisation</b> signals, and a pseudo line delay effect.|$|R
40|$|It is {{hypothesised}} that Parkinson’s {{disease is}} explained as {{a change in}} the pattern <b>of</b> <b>synchronisation</b> <b>of</b> discharges between neurons within the subthalamo-pallidal-thalamo-cortical circuit. By simulating a single GPi neuron receiving non-synchronous and oscillatory inputs the conditions required for the generation of normal and Parkinsonian patterns are determined...|$|R
40|$|We {{show that}} the {{probability}} <b>of</b> appearance <b>of</b> <b>synchronisation</b> in chaotic coupled map lattices {{is related to the}} distribution of the maximum of a certain observable evaluated along almost all orbit. We show that such distribution belongs to the family of extreme value laws, whose parameters, namely the extremal index, allow us to get {{a detailed description of the}} probability <b>of</b> <b>synchronisation.</b> Theoretical results are supported by robust numerical computations that allow to go beyond the theoretical framework provided and are potentially applicable to physically relevant systems. Comment: 32 pages, 6 figure...|$|R
40|$|Abstract—This paper {{discusses}} {{formal specification}} and verification <b>of</b> the <b>synchronisation</b> classes <b>of</b> the Java API. In many verification systems for concurrent programs, synchronisation {{is treated as}} a primitive operation. As a result, verification rules for synchronisation are hard-coded in the logic, and not verified. These rules describe the concrete semantics <b>of</b> the given <b>synchronisation</b> primitive, and manage how resources are protected by synchronisation. In contrast, this paper describes several synchronisation primitives at the specification level, by specifying the behaviour <b>of</b> <b>synchronisation</b> routines from the Java API at method level using permission-based Separation Logic. This gives a generalised, high-level, and easily extendable approach to formalisation <b>of</b> arbitrary <b>synchronisation</b> mechanisms, which allows for modular treatment <b>of</b> <b>synchronisation</b> in verification. Notably, our approach does not only apply to locks, but also to other synchronisation mechanisms such as semaphores and latches that we also discuss in the paper. Finally, we used the verification tool that we are developing and successfully verified (so far simplified) reference implementations of all presented synchronisers; the paper discusses the verification of one of them. I...|$|R
40|$|Without {{the aid of}} {{licensed}} channel, deploying long-term evolution (LTE) networks over unlicensed spectrum (named standalone LTE-U networks) {{faces the}} difficulty of establishing and maintaining synchronisation between user equipments and base stations. In this work, considering the two modes of listen-before-talk-based channel access scheme, frame-based equipment (FBE) and load-based equipment (LBE), the authors propose analytical frameworks to study the successful probability <b>of</b> <b>synchronisation</b> and the energy consumption <b>of</b> <b>synchronisation</b> in a standalone LTE-U network. Specifically, for the LBE mode, the authors also propose a Lattice-Poisson algorithm-based approach to derive {{the distribution of the}} channel non-occupancy period of a standalone LTE-U network. Furthermore, the authors explore the impact of diverse protocol parameters of both FBE and LBE modes on the two studied performance metrics. Simulation results demonstrate the accuracy of the analysis, and shed some light on the selection of FBE and LBE for standalone LTE-U networks, in terms <b>of</b> <b>synchronisation,</b> energy consumption, and throughput of standalone LTE-U and Wi-Fi networks...|$|R
40|$|In this paper, {{there is}} a {{possibility}} of nonlinear dynamic systems control described. A <b>synchronisation</b> <b>of</b> two coupled chaotic oscillators based on the state reconstruction is presented. As an example the well known Lorenz system was chosen and the same method <b>of</b> <b>synchronisation</b> was applied on the Rössler system as well...|$|R
30|$|We {{conclude}} {{that there is}} strong nonlinear generalised synchronisation between stations CNST and PAWA on the North Island with station KAIK on the South Island in a punctuated manner {{for a period of}} at least 9  months before the magnitude 7.8 Kaikōura event. For ≈[*] 270  days before the Kaikōura event, the degree <b>of</b> <b>synchronisation</b> between CNST and KAIK intensifies dramatically and ceases at the Kaikōura event. However, similar patterns <b>of</b> <b>synchronisation</b> occur associated with every displacement event at CNST for the previous 3  years. These synchronisation events act as powerful precursors to both minor and major displacement events.|$|R
40|$|Concurrent {{programming}} is now becoming {{more important than}} ever. But for concurrent programs to work deterministically, sections of the code must be synchronised. The most common method of synchronising code {{is to protect the}} code with locks. However, code which uses locks is difficult to write and even more difficult to debug. Locking also makes it difficult to compose large programs from smaller ones. A relatively new method <b>of</b> <b>synchronisation,</b> known as Software Transactional Memory, is promising to be a much easier method <b>of</b> <b>synchronisation.</b> This thesis describes the design and implementation of a Software Transactiona...|$|R
40|$|This paper {{addresses}} the effects <b>of</b> <b>synchronisation</b> errors (time delay, carrier phase, and carrier frequency) {{on the performance}} of linear decorrelating detectors (LDDs). A major effect is that all LDDs require certain degree of power control in the presence <b>of</b> <b>synchronisation</b> errors. The multi-shot sliding window algorithm (SLWA) and hard decision method (HDM) are analysed and their power control requirements are examined. Also, a more efficient one-shot detection scheme, called “hard-decision based coupling cancellation”, is proposed and analysed. These schemes are then compared with the isolation bit insertion (IBI) approach in terms of power control requirements...|$|R
40|$|Bordo and Helbing (2003) {{examine the}} {{business}} cycle in Western economies over the 1881 - 2001 period. They examine four distinct periods in economic history and conclude {{that there is a}} secular trend towards greater <b>synchronisation</b> for much <b>of</b> the 20 th century, and that it takes place across these different regimes. Most of the analytical techniques used in {{the business cycle}} convergence literature rely upon the estimation of an empirical correlation matrix of time series data of macroeconomic aggregates in the various countries. However due to the finite size of both the number of economies and the number of observations, a reliable determination of the correlation matrix may prove to be problematic. The structure of the correlation matrix may be dominated by noise rather than by true information. Random matrix theory was developed in physics to overcome this problem, and to enable true information in a matrix to be distinguished from noise. It has been successfully applied in the analysis of financial data. Using a very similar data set to Bordo and Helbing, I use random matrix theory, and the associated technique of agglomerative hierarchical clustering, to examine the evolution of convergence of the business cycle between the capitalist economies. The results confirm that there is a very clear degree <b>of</b> <b>synchronisation</b> <b>of</b> the business cycle across countries during the 1973 - 2006 period. In contrast, during the pre-First World War period {{it is not possible to}} speak of an international business cycle in any meaningful sense. The crosscountry correlations of annual real GDP growth are indistinguishable from those which could be generated by a purely random matrix. Contrary to the findings of Bordo and Helbing, it does not seem possible to speak of a ?secular trend? towards greater synchronisation over the 1886 - 2006 period as a whole. The periods 1920 - 1938 and 1948 - 1972 do show a certain degree <b>of</b> <b>synchronisation</b> – very similar in both periods in fact – but it is weak. In particular, the cycles of the major economies cannot be said to be synchronised during these periods. Such synchronisation as exists in the overall data set is due to meaningful comovements in sub-groups. So the degree <b>of</b> <b>synchronisation</b> has evolved fitfully, and it is only in the most recent period, 1973 - 2006, that we can speak of a strong level <b>of</b> <b>synchronisation</b> <b>of</b> business cycles between countries. More detailed analysis of the evolution <b>of</b> <b>synchronisation</b> <b>of</b> the 6 major economies since 1948 suggests it can vary considerably over relatively short periods of time. During the 1990 s, for example, the degree <b>of</b> <b>synchronisation</b> <b>of</b> the cycle was similar to that of the 1950 s, and lower than it was in the 1970 s and 1980 s following the oil shocks. [...] International business cycle,synchronisation,random matrix theory...|$|R
