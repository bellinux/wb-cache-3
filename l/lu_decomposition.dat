468|56|Public
5|$|The <b>LU</b> <b>decomposition</b> factors {{matrices}} as {{a product}} of lower (L) and an upper triangular matrices (U). Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form. Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A {{as a product}} UDV∗, where U and V are unitary matrices and D is a diagonal matrix.|$|E
25|$|Decomposition {{techniques}} like <b>LU</b> <b>decomposition</b> {{are much}} faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.|$|E
25|$|Gauss–Jordan {{elimination}} is an algorithm {{that can}} be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the <b>LU</b> <b>decomposition</b> which generates upper and lower triangular matrices which are easier to invert.|$|E
40|$|International audienceTo solve sparse {{systems of}} linear equations, multifrontal methods rely on dense partial <b>LU</b> <b>decompositions</b> of {{so-called}} frontal matrices; we consider a parallel asynchronous {{setting in which}} several frontal matrices can be factored simultaneously. In this context, to address performance and scalability issues of acyclic pipelined asynchronous factorization kernels, we study models to revisit properties of left and right-looking variants of partial <b>LU</b> <b>decompositions,</b> study the use of several levels of blocking, before focusing on communication issues. The general purpose sparse solver MUMPS has been modified to implement the proposed algorithms and confirm the properties demonstrated by the models...|$|R
50|$|This {{decomposition}} {{is called}} the Cholesky decomposition. The Cholesky decomposition always exists and is unique — provided the matrix is positive definite. Furthermore, computing the Cholesky decomposition is more efficient and numerically more stable than computing some other <b>LU</b> <b>decompositions.</b>|$|R
40|$|Abstract. In this paper, we {{introduce}} the Pascal k-eliminated functional matrix and the Pascal symmetric functional matrix with 2 n variables. Some algebraic properties of these matrices are presented and proved. In addition, we demonstrate a direct {{application of these}} properties for <b>LU</b> <b>decompositions</b> of some well-known matrices (such as symmetric Pascal matrices). Key words. Pascal matrix, Pascal k-eliminated functional matrix, Pascal symmetric functiona...|$|R
25|$|Much {{effort has}} been put in the {{development}} of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, <b>LU</b> <b>decomposition,</b> Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.|$|E
25|$|Algorithms {{can also}} be {{assessed}} according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or <b>LU</b> <b>decomposition)</b> method is of order O(n3), but the bit length of intermediate values can become exponentially long. The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.|$|E
25|$|Another {{point of}} view, which {{turns out to}} be very useful to analyze the algorithm, is that row {{reduction}} produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then {{the first part of the}} algorithm computes an <b>LU</b> <b>decomposition,</b> while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.|$|E
40|$|AbstractIn this paper, we {{introduce}} the generalized Pascal functional matrix {{and show that}} the existing variations of Pascal matrices are special cases of this generalization. We study some algebraic properties of such generalized Pascal functional matrices. In addition, we demonstrate a direct application of these properties by deriving several novel combinatorial identities and a nontraditional approach for <b>LU</b> <b>decompositions</b> of some well-known matrices (such as symmetric Pascal matrices) ...|$|R
5000|$|Such {{matrices}} {{are known}} as sparse matrices, and there are efficient solvers for such problems (much more efficient than actually inverting the matrix.) In addition, [...] is symmetric and positive definite, so a technique such as the conjugate gradient method is favored. For problems that are not too large, sparse <b>LU</b> <b>decompositions</b> and Cholesky decompositions still work well. For instance, MATLAB's backslash operator (which uses sparse LU, sparse Cholesky, and other factorization methods) can be sufficient for meshes with a hundred thousand vertices.|$|R
40|$|Abstract. This paper proposes an {{algorithm}} for satisfying {{systems of}} linear equality and inequality constraints with hierarchical strengths or preferences. Basically, it is a numerical method that incrementally obtains the <b>LU</b> <b>decompositions</b> of linear constraint systems. To realize this, it introduces a novel technique for analyzing hierarchical systems of linear constraints. In addition, it improves performance by adopting techniques that utilize the sparsity and disjointness of constraint systems. Based on this algorithm, the HiRise constraint solver {{has been designed}} and implemented {{for the use of}} constructing interactive graphical user interfaces. This paper shows that HiRise is scalable up to thousands of simultaneous constraints in real-time execution. ...|$|R
25|$|While {{systems of}} three or four {{equations}} can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the <b>LU</b> <b>decomposition</b> of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.|$|E
2500|$|The <b>LU</b> <b>decomposition</b> expresses A {{in terms}} of a lower {{triangular}} matrix L, an upper triangular matrix U and a permutation matrix P: ...|$|E
2500|$|Combining the Householder {{transformation}} {{with the}} <b>LU</b> <b>decomposition</b> {{results in an}} algorithm with better convergence than the QR algorithm. For large Hermitian sparse matrices, the Lanczos algorithm {{is one example of}} an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.|$|E
40|$|Deterministic {{recursive}} algorithms for the computation of matrix triangular decompositions with permutations like <b>LU</b> and Bruhat <b>decomposition</b> {{are presented}} {{for the case}} of commutative domains. This decomposition {{can be considered as}} a generalization of <b>LU</b> and Bruhat <b>decompositions,</b> because they both may be easily obtained from this triangular decomposition. Algorithms have the same complexity as the algorithm of matrix multiplication...|$|R
40|$|AbstractThe {{effectiveness}} of relaxation schemes for solving the systems of algebraic equations which arise from spectral discretizations of elliptic equations is examined. Iterative methods are an attractive alternative to direct methods because Fourier transform techniques enable the discrete matrix-vector products to be computed almost {{as efficiently as}} for corresponding but sparse finite difference discretizations. Preconditioning {{is found to be}} essential for acceptable rates of convergence. Preconditioners based on second-order finite difference methods are used. A comparison is made of the performance of different relaxation methods on model problems with a variety of conditions specified around the boundary. The investigations show that iterations based on incomplete <b>LU</b> <b>decompositions</b> provide the most efficient methods for solving these algebraic systems...|$|R
40|$|The system {{resulting}} from the coupled Finite Element Method and Boundary Element Method formulations inherits all characteristics of both finite element and boundary element equation system, i. e., the system is partially sparse and symmetric and partially full and nonsymmetric. Consequently, to solve the resulting coupled equation system is not a trivial task. This paper proposes a new efficient lifting-based two level preconditioner for the coupled global system. The proposed approach is applied to solve the coupled systems {{resulting from}} the electromagnetic scattering problem and its performance is evaluated {{based on the number}} of iterations and the computational time. Traditional methods based on incomplete and complete <b>LU</b> <b>decompositions</b> are used for comparison...|$|R
2500|$|The Gauss {{decomposition}} is a {{generalization of}} the <b>LU</b> <b>decomposition</b> {{for the general}} linear group and a specialization of the Bruhat decomposition. For [...] it states that {{with respect to a}} given orthonormal basis [...] an element [...] of [...] can be factorized in the form ...|$|E
2500|$|Given {{a matrix}} A, some methods compute its {{determinant}} by writing A {{as a product}} of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the <b>LU</b> <b>decomposition,</b> the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!) ...|$|E
2500|$|The L·D·LT {{decomposition}} of the innovation covariance matrix Sk {{is the basis}} for another type of numerically efficient and robust square root filter. The algorithm starts with the <b>LU</b> <b>decomposition</b> as implemented in the Linear Algebra PACKage (LAPACK). These results are further factored into the L·D·LT structure with methods given by Golub and Van Loan (algorithm 4.1.2) for a symmetric nonsingular matrix. Any singular covariance matrix is pivoted so that the first diagonal partition is nonsingular and well-conditioned. The pivoting algorithm must retain any portion of the innovation covariance matrix directly corresponding to observed state-variables Hk·xk|k-1 that are associated with auxiliary observations in ...|$|E
40|$|The {{effectiveness}} of relaxation schemes for solving the systems of algebraic equations which arise from spectral discretizations of elliptic equations is examined. Iterative methods are an attractive alternative to direct methods because Fourier transform techniques enable the discrete matrix-vector products to be computed almost {{as efficiently as}} for corresponding but sparse finite difference discretizations. Preconditioning {{is found to be}} essential for acceptable rates of convergence. Preconditioners based on second-order finite difference methods are used. A comparison is made of the performance of different relaxation methods on model problems with a variety of conditions specified around the boundary. The investigations show that iterations based on incomplete <b>LU</b> <b>decompositions</b> provide the most efficient methods for solving these algebraic systems...|$|R
40|$|AbstractIn this work, we {{consider}} the numerical solution of a large eigenvalue problem resulting from a finite rank discretization of an integral operator. We are interested in computing a few eigenpairs, with an iterative method, so a matrix representation that allows for fast matrix-vector products is required. Hierarchical matrices are appropriate for this setting, and also provide cheap <b>LU</b> <b>decompositions</b> required in the spectral transformation technique. We illustrate the use of freely available software tools to address the problem, in particular SLEPc for the eigensolvers and HLib {{for the construction of}} H-matrices. The numerical tests are performed using an astrophysics application. Results show the benefits of the data-sparse representation compared to standard storage schemes, in terms of computational cost as well as memory requirements...|$|R
40|$|Implicit {{methods for}} {{hyperbolic}} equations are analyzed using <b>LU</b> <b>decompositions.</b> It is {{shown that the}} inversion of the resulting tridiagonal matrices is usually stable even when diagonal dominance is lost. Furthermore, these decompositions {{can be used to}} construct stable algorithms in multidimensions. When marching to a steady state, the solution is independent of the time. Alternating direction methods which solve for u n+ 1 − u n are unconditionally unstable in three-space dimensions and so the new method is more appropriate. Furthermore, only two factors are required even in threespace dimensions and the operation count per time step is low. Acceleration to a steady state is analyzed, and it is shown that the fully implicit method with large time steps approximates a Newton-Raphson iteration procedure...|$|R
50|$|The <b>LU</b> <b>decomposition</b> is an {{excellent}} general-purpose linear equation solver. The biggest disadvantage is that it fails {{to take advantage of}} coefficient matrix to be a sparse matrix. The <b>LU</b> <b>decomposition</b> of a sparse matrix is usually not sparse, thus, for a large system of equations, <b>LU</b> <b>decomposition</b> may require a prohibitive amount of memory and number of arithmetical operations.|$|E
50|$|In {{numerical}} analysis and linear algebra, <b>LU</b> <b>decomposition</b> (where 'LU' stands for 'lower upper', and also called LU factorization) factors a matrix {{as the product}} of a lower triangular matrix and an upper triangular matrix. The product sometimes includes a permutation matrix as well. The <b>LU</b> <b>decomposition</b> can be viewed as the matrix form of Gaussian elimination. Computers usually solve square systems of linear equations using the <b>LU</b> <b>decomposition,</b> and it is also a key step when inverting a matrix, or computing the determinant of a matrix. The <b>LU</b> <b>decomposition</b> was introduced by mathematician Tadeusz Banachiewicz in 1938.|$|E
50|$|Existence: An LUP {{decomposition}} {{exists for}} any square matrix A. When P is an identity matrix, the LUP decomposition reduces to the <b>LU</b> <b>decomposition.</b> If the <b>LU</b> <b>decomposition</b> exists, then the LDU decomposition exists.|$|E
40|$|In this work, we {{consider}} the numerical solution of a large eigenvalue problem resulting from a finite rank discretization of an integral operator. We are interested in computing a few eigenpairs, with an iterative method, so a matrix representation that allows for fast matrix-vector products is required. Hierarchical matrices are appropriate for this setting, and also provide cheap <b>LU</b> <b>decompositions</b> required in the spectral transformation technique. We illustrate the use of freely available software tools to address the problem, in particular SLEPc for the eigensolvers and HLib {{for the construction of}} H-matrices. The numerical tests are performed using an astrophysics application. Results show the benefits of the data-sparse representation compared to standard storage schemes, in terms of computational cost as well as memory requirements. Keywords: iterative eigensolvers, integral operator, hierarchical matrices...|$|R
40|$|Streaming SIMD Extensions (SSE) is {{a unique}} feature {{embedded}} in the Pentium III and IV classes of microprocessors. By fully exploiting SSE, parallel algorithms can be implemented on a standard personal computer and a theoretical speedup of four can be achieved. In this paper, we demonstrate {{the implementation of a}} parallel <b>LU</b> matrix <b>decomposition</b> algorithm for solving linear systems with SSE and discus...|$|R
40|$|We {{consider}} UL (and <b>LU)</b> <b>decompositions</b> of the one-step {{transition probability}} matrix of a random walk with state space the nonnegative integers, {{with the condition}} that both upper and lower triangular matrices in the factorization are also stochastic matrices. We give conditions on the free parameter of the UL factorization in terms of certain continued fraction such that this stochastic factorization is possible. By inverting the order of multiplication (also known as a Darboux transformation) we get a new family of random walks where it is possible the identify the spectral measures {{in terms of a}} Geronimus transformation. The same can be done for the LU factorization but now without a free parameter. Finally, we apply our results in two examples, the random walk with constant transition probabilities and the random walk generated by the Jacobi orthogonal polynomials. In both situations we give urn models associated with all the random walks in question. Comment: 22 pages, 4 figure...|$|R
5000|$|A Toeplitz matrix {{can also}} be {{decomposed}} (i.e. factored) in O(n2) time. [...] The Bareiss algorithm for an <b>LU</b> <b>decomposition</b> is stable. [...] An <b>LU</b> <b>decomposition</b> gives a quick method for solving a Toeplitz system, and also for computing the determinant.|$|E
5000|$|Substituting {{these values}} into the <b>LU</b> <b>decomposition</b> above yields ...|$|E
5000|$|<b>LU</b> <b>Decomposition</b> - A robust {{algorithm}} {{for solving}} linear equations.|$|E
40|$|Streaming SIMD Extensions (SSE) is {{a unique}} feature {{embedded}} in the Pentium III class of microprocessors. By fully exploiting SSE, parallel algorithms can be implemented on a standard personal computer and a theoretical speedup of four can be achieved. In this paper, we demonstrate {{the implementation of a}} parallel <b>LU</b> matrix <b>decomposition</b> algorithm for solving power systems network equations with SSE and discuss advantages and disadvantages of this approach...|$|R
40|$|We present two {{new ways}} of {{preconditioning}} sequences of nonsymmetric linear systems in the special case where the implementation is matrix-free. Both approaches are fully algebraic, {{they are based on}} the general updates of incomplete <b>LU</b> <b>decompositions</b> recently introduced in [1], and they may be directly embedded into nonlinear algebraic solvers. The first of the approaches uses a new model of partial matrix estimation to compute the updates. The second approach exploits separability of function components to apply the updated factorized preconditioner via function evaluations with the discretized operator. Experiments with matrix-free implementations of test problems show that both new techniques offer useful, robust and black-box solution strategies. In addition, they show that the new techniques are often more efficient in matrix-free environment than either recomputing the preconditioner from scratch for every linear system of the sequence or than freezing the preconditioner throughout the whole sequence. Copyright c © 2000 John Wiley & Sons, Ltd. key words: preconditioned iterative methods, matrix-free environment, factorization updates, inexact Newton-Krylov methods, incomplete factorizations 1...|$|R
40|$|We propose an {{efficient}} algorithmic framework for time domain circuit simulation using exponential integrator. This work addresses several critical issues exposed by previous matrix exponential based circuit simulation research, {{and makes it}} capable of simulating stiff nonlinear circuit system at a large scale. In this framework, the system's nonlinearity is treated with exponential Rosenbrock-Euler formulation. The matrix exponential and vector product is computed using invert Krylov subspace method. Our proposed method has several distinguished advantages over conventional formulations (e. g., the well-known backward Euler with Newton-Raphson method). The matrix factorization is performed only for the conductance/resistance matrix G, without being performed for the combinations of the capacitance/inductance matrix C and matrix G, which are used in traditional implicit formulations. Furthermore, due to the explicit nature of our formulation, {{we do not need}} to repeat <b>LU</b> <b>decompositions</b> when adjusting the length of time steps for error controls. Our algorithm is better suited to solving tightly coupled post-layout circuits in the pursuit for full-chip simulation. Our experimental results validate the advantages of our framework. Comment: 6 pages; ACM/IEEE DAC 201...|$|R
