1722|3880|Public
25|$|The <b>learning</b> <b>rule</b> is an {{algorithm}} which modifies {{the parameters}} of the neural network, in order for a given input to the network to produce a favored output. This learning process typically amounts to modifying the weights and thresholds of the variables (i.e. {{the parameters of}} the ANN) within the network.|$|E
25|$|An (artificial) {{neural network}} is {{a network of}} simple {{elements}} called neurons, which receive input, change their internal state (i.e. the activation) according to that input and an activation function, and produce output depending on the input and the activation. The network forms by connecting the output of certain neurons to the input of other neurons forming a directed and weighted graph, where the neurons are the nodes and {{the connection between the}} neurons are weighted directed edges. The weights and the activation functions can be modified by a process called learning, which is governed by a <b>learning</b> <b>rule.</b>|$|E
25|$|Inspired by {{the success}} of Boltzmann {{machines}} based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a <b>learning</b> <b>rule</b> analogous to that of classical Boltzmann machines.|$|E
40|$|We {{summarize}} the Storkey <b>Learning</b> <b>Rules</b> for the Hopfield Model, and evaluate performance {{relative to other}} <b>learning</b> <b>rules.</b> Hopfield Models are normally used for auto-association, and Storkey <b>Learning</b> <b>Rules</b> {{have been found to}} have good balance between local learning and capacity. In this paper we outline different <b>learning</b> <b>rules</b> and summarise capacity re-sults. Hopfield networks are related to Boltzmann Machines: they are the same as fully visible Boltzmann Machines in the zero temperature limit. Perhaps renewed interest in Boltzmann machines will produce renewed interest in Hopfield <b>learning</b> <b>rules?...</b>|$|R
40|$|We study a {{model of}} local {{evolution}} in which agents located on a network interact strate-gically with their neighbours. Strategies are chosen {{with the help of}} <b>learning</b> <b>rules</b> that are themselves based on the success of strategies observed in the neighbourhood. Previous work on local evolution assumes fixed <b>learning</b> <b>rules</b> while we study <b>learning</b> <b>rules</b> that are determined endogenously. Simulations indicate that endogenous <b>learning</b> <b>rules</b> put more weight on a learning player’s own experience than on the experience of an observed neighbour. Nevertheless stage game behaviour is similar to behaviour with symmetric <b>learning</b> <b>rules...</b>|$|R
40|$|This paper {{considers}} a <b>learning</b> <b>rules</b> for {{environments in which}} little prior in-formation is available to the decision maker. Two properties of such <b>learning</b> <b>rules,</b> absolute expediency and monotonicity, are studied. The paper pro-vides some necessary, and some su¢cient conditions for these properties. A number of examples show that the there is a quite a large variety of <b>learning</b> <b>rules</b> which have the properties. It is also shown that all <b>learning</b> <b>rules</b> which have these properties are, in some sense, related to the replicator dynamics of evolutionary game theory. ...|$|R
25|$|In 2009, {{a simple}} {{electronic}} circuit consisting of an LC network and a memristor {{was used to}} model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, {{in collaboration with the}} Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011 they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian <b>learning</b> <b>rule.</b>|$|E
2500|$|In {{the late}} 1940s, D.O. Hebb created a {{learning}} hypothesis {{based on the}} mechanism of neural plasticity that {{is now known as}} Hebbian learning. Hebbian learning is an unsupervised <b>learning</b> <b>rule.</b> This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines [...]|$|E
2500|$|In a 1969 Special Convention, {{the motto}} devised in 1900, [...] "The Love of Learning Rules all Mankind", {{was changed to}} [...] "Let the Love of <b>Learning</b> <b>Rule</b> Mankind" [...] due to {{membership}} insistence that the former was, {{in the words of}} one member, [...] "the most barefaced lie that had ever been cast in bronze." ...|$|E
40|$|This paper {{considers}} <b>learning</b> <b>rules</b> for {{environments in}} which little prior and feedback information is available to the decision maker. Two properties of such <b>learning</b> <b>rules</b> are studied: absolute expediency and monotonicity. Both require that {{some aspect of the}} decision maker’s performance improves from the current period to the next. The paper provides some necessary, and some sufficient conditions for these properties. It turns out that there is a large variety of <b>learning</b> <b>rules</b> that have the properties. However, all <b>learning</b> <b>rules</b> that have these properties are related to the replicator dynamics of evolutionary game theory. For the case in which there are only two actions, it is shown that one of the absolutely expedient <b>learning</b> <b>rules</b> dominates all others...|$|R
40|$|Humans {{and other}} animals learn by {{updating}} synaptic weights in the brain. Rapid learning allows animals to adapt quickly to changes in their environment, giving them a large selective advantage. As brains have been evolving for several hundred million years, we might expect biological <b>learning</b> <b>rules</b> {{to be close to}} optimal, by exploiting all locally available in-formation in order to learn as rapidly as possible. However, no previously proposed <b>learning</b> <b>rules</b> are optimal in this sense. We therefore use Bayes theorem to derive optimal <b>learning</b> <b>rules</b> for supervised, unsupervised and reinforcement learning. As expected, these rules prove to be significantly more effective than the best classical <b>learning</b> <b>rules.</b> Our <b>learning</b> <b>rules</b> make two predictions about the results of plasticity experiments in active networks. First, we predict that learning rates should vary across time, increasing when fewer inputs are active. Second, we predict that learning rates should vary across synapses, being higher for synapses whose presy-naptic cells have a lower average firing rate. Finally, our methods are extremely flexible, allowing the derivation of optimal <b>learning</b> <b>rules</b> based solely on the information that is assumed, or known, to be available to the synapse. This flexibility should allow for the derivation of optimal <b>learning</b> <b>rules</b> for progressively more complex and realistic synaptic and neural models — allowing us to connect theory with complex biological reality. ...|$|R
40|$|Experimental {{data has}} shown that {{synaptic}} strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian <b>learning</b> <b>rules</b> motivated by this data have been proposed. We argue that such <b>learning</b> <b>rules</b> are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify {{the weight of a}} silicon spiking neuron according to those <b>learning</b> <b>rules.</b> Test results from the fabrication of the circuit using a 0. 6 m CMOS process are given...|$|R
2500|$|Neural network {{models can}} be viewed as simple {{mathematical}} models defining a function [...] or a distribution over [...] or both [...] and [...] Sometimes models are intimately associated with a particular <b>learning</b> <b>rule.</b> A common use of the phrase [...] "ANN model" [...] is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity [...] ).|$|E
2500|$|The Honor Society of Phi Kappa Phi (or simply Phi Kappa Phi or [...] ) is {{an honor}} society {{established}} in 1897 to recognize and encourage superior scholarship without restriction as to area of study and to promote the [...] "unity and democracy of education". [...] It is the third academic society in the United States to be organized around recognizing academic excellence, and is the oldest all-discipline honor society. The society's motto is [...] (Philosophía Krateítõ Phõtôn), which is translated as [...] "Let the love of <b>learning</b> <b>rule</b> humanity", and its mission is [...] "to recognize and promote academic excellence in all fields of higher education and to engage the community of scholars in service to others." ...|$|E
5000|$|... #Subtitle level 3: Hebbian <b>learning</b> <b>rule</b> for Hopfield {{networks}} ...|$|E
40|$|Modeling {{self-organization}} {{of neural}} networks for unsupervised learning using Hebbian and anti-Hebbian plasticity {{has a long}} history in neuroscience. Yet, derivations of single-layer networks with such local <b>learning</b> <b>rules</b> from principled optimization objectives became possible only recently, with the introduction of similarity matching objectives. What explains the success of similarity matching objectives in deriving neural networks with local <b>learning</b> <b>rules?</b> Here, using dimensionality reduction as an example, we introduce several variable substitutions that illuminate the success of similarity matching. We show that the full network objective may be optimized separately for each synapse using local <b>learning</b> <b>rules</b> both in the offline and online settings. We formalize the long-standing intuition of the rivalry between Hebbian and anti-Hebbian rules by formulating a min-max optimization problem. We introduce a novel dimensionality reduction objective using fractional matrix exponents. To illustrate the generality of our approach, we apply it to a novel formulation of dimensionality reduction combined with whitening. We confirm numerically that the networks with <b>learning</b> <b>rules</b> derived from principled objectives perform better than those with heuristic <b>learning</b> <b>rules.</b> Comment: Accepted for publication in Neural Computatio...|$|R
40|$|It {{is shown}} by example that <b>learning</b> <b>rules</b> of the fictitious play type fail to {{converge}} in certain kinds of coordination games. By contrast, <b>learning</b> <b>rules</b> in which past actions are eventually forgotten and which incorporate small stochastic perturbations are better behaved: over the long run, players manage to coordinate with probability one...|$|R
40|$|The utility {{problem in}} explanation-based {{learning}} concerns {{the ability of}} <b>learned</b> <b>rules</b> or plans to actually improve {{the performance of a}} problem solving system. Previous research on this problem has focused on the amount, content, or form of learned information. This paper examines the effect of the use of learned information on performance. Experiments and informal analysis show that unconstrained use of <b>learned</b> <b>rules</b> eventually leads to degraded performance. However, constraining the use of <b>learned</b> <b>rules</b> helps avoid the negative effect of learning and lead to overall performance improvement. Search strategy is also shown to have a substantial effect on the contribution of learning to performance by affecting the manner in which <b>learned</b> <b>rules</b> arc used. These effects help explain why previous experiments have obtained a variety of different results concerning the impact of explanation-based learning on performance. 1...|$|R
5000|$|<b>Learning</b> <b>rule</b> or Learning {{process is}} a method or a {{mathematical}} logic which improves the artificial neural network's performance and usually this rule is applied repeatedly over the network. It is done by updating the weights and bias levels of a network when a network is simulated in a specific data environment. A <b>learning</b> <b>rule</b> may accept existing condition ( [...] weights and bias [...] ) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias. [...] Depending on the complexity of actual model, which is being simulated, the <b>learning</b> <b>rule</b> of the network {{can be as simple}} as an XOR gate or Mean Squared Error or it can be the result of multiple differential equations. The <b>learning</b> <b>rule</b> is one of the factors which decides how fast or how accurate the artificial network can be developed. Depending upon the process to develop the network there are three main models of machine learning: ...|$|E
5000|$|GHA {{combines}} Oja's rule {{with the}} Gram-Schmidt process {{to produce a}} <b>learning</b> <b>rule</b> of the form ...|$|E
5000|$|<b>Learning</b> <b>rule</b> {{that uses}} {{conditional}} [...] "local" [...] {{information can be}} derived from the reversed form of , ...|$|E
40|$|In this paper, {{we define}} an {{evolutionary}} stability criterion for <b>learning</b> <b>rules.</b> Using simulations, we then apply this criterion to {{three types of}} symmetric 2 x 2 games for a class of <b>learning</b> <b>rules</b> that can be represented by the parametric model of Camerer and Ho [1999. Experience-weighted attraction learning in normal form games. Econometrica 67, 827 - 874]. This class contains stochastic versions of reinforcement and fictitious play as extreme cases. We find that only <b>learning</b> <b>rules</b> with high or intermediate levels of hypothetical reinforcement are evolutionarily stable, but that the stable parameters depend on the game. ...|$|R
40|$|This paper investigates {{potential}} <b>learning</b> <b>rules</b> in the cerebellum. We review {{evidence that}} input to the cerebellum is sparsely expanded by granule cells {{into a very}} wide basis vector, and that Purkinje cells learn to compute a linear separation using that basis. We review <b>learning</b> <b>rules</b> employed by existing cerebellar models, and show that recent results from Computational Learning Theory suggest that the standard delta rule would not be efficient. We suggest that alternative, attribute-efficient <b>learning</b> <b>rules,</b> such as Winnow or Incremental Delta-Bar-Delta, are more appropriate for cerebellar modeling, and support this position with results from a computational model...|$|R
40|$|AbstractIn {{this article}} we {{approach}} one key aspect of the utility problem in explanation-based learning (EBL) —the expensive-rule problem—as an avoidable defect in the learning procedure. In particular, we {{examine the relationship between}} the cost of solving a problem without learning versus the cost of using a <b>learned</b> <b>rule</b> to provide the same solution, and refer to a <b>learned</b> <b>rule</b> as expensive if its use is more costly than the original problem solving from which it was learned. The key idea we explore is that expensiveness is inadvertently and unnecessarily introduced into <b>learned</b> <b>rules</b> by the <b>learning</b> algorithms themselves. This becomes a particularly powerful idea when combined with an analysis tool which identifies these hidden sources of expensiveness, and modifications of the learning algorithms which eliminate them. The result is learning algorithms for which the cost of <b>learned</b> <b>rules</b> is bounded by the cost of the problem solving that they replace...|$|R
5000|$|A <b>learning</b> <b>rule</b> for modifying {{connections}} {{based on}} experience, {{represented by a}} change in the weights based on any number of variables.|$|E
5000|$|The {{synaptic}} weight is changed {{by using a}} <b>learning</b> <b>rule,</b> {{the most basic of}} which is Hebb's rule, which is usually stated in biological terms as ...|$|E
5000|$|Butts, DA, Kanold, PO, Shatz CJ (2007) [...] "A burst-based 'Hebbian' <b>learning</b> <b>rule</b> at retinogeniculate synapses links retinal {{waves to}} {{activity}} dependent refinement". PLoS Biology 5: E61.|$|E
40|$|Abstract. Biological brains {{can adapt}} {{and learn from}} past experience. In neuroevolution, i. e. {{evolving}} artificial neural networks (ANNs), one way that agents controlled by ANNs can evolve the ability to adapt is by encoding local <b>learning</b> <b>rules.</b> However, a significant problem with most such approaches is that local <b>learning</b> <b>rules</b> for every connection in the network must be discovered separately. This paper aims to show that <b>learning</b> <b>rules</b> can be effectively indirectly encoded by extending the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) method. Adaptive HyperNEAT is introduced to allow not only patterns of weights across the connectivity of an ANN to be generated by a function of its geometry, but also patterns of arbitrary <b>learning</b> <b>rules.</b> Several such adaptive models with different levels of generality are explored and compared. The long-term promise of the new approach is to evolve large-scale adaptive ANNs, which is a major goal for neuroevolution...|$|R
40|$|Abstract: In this paper, {{we review}} an {{extension}} of the <b>learning</b> <b>rules</b> in a Principal Component Analysis network which has been derived to be optimal for a specific probability density function. We note that this probability density function is one of a family of pdfs and investigate the <b>learning</b> <b>rules</b> formed in order to be optimal for several members of this family. We show that, whereas previous authors [5] have viewed the single member of the family as {{an extension of}} PCA, it is more appropriate to view the whole family of <b>learning</b> <b>rules</b> as methods of performing Exploratory Projection Pursuit. We illustrate this on artificial data sets. 1...|$|R
40|$|This paper {{describes}} {{the application of}} explanationbased learning, a machine learning technique, to the SRI Core Language Engine, a large scale general purpose natural language analysis system. The idea is to bypass normal morphological, syntactic and (partly) semantic processing, for most input sentences, instead using a set of <b>learned</b> <b>rules.</b> Explanation-based <b>learning</b> is used to extract the <b>learned</b> <b>rules</b> automatically from sample sentences submitted by a user and thus tune the system for that particular user. By indexing the <b>learned</b> <b>rules</b> efficiently, {{it is possible to}} achieve dramatic speedups. Performance measurements were carried out using a training set of 1500 sentences and a separat...|$|R
5000|$|Simple {{recurrent}} {{networks have}} three layers, {{with the addition}} of a set of [...] "context units" [...] in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step, the input is propagated in a standard feedforward fashion, and then a backpropagation-like <b>learning</b> <b>rule</b> is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the <b>learning</b> <b>rule</b> is applied).|$|E
5000|$|Local: A <b>learning</b> <b>rule</b> {{is local}} if each weight is updated using {{information}} available to neurons {{on either side of}} the connection that is associated with that particular weight.|$|E
5000|$|The {{simplest}} <b>learning</b> <b>rule</b> {{known is}} Hebb's rule, which states in conceptual terms that neurons that fire together, wire together. In component form as a difference equation, {{it is written}} ...|$|E
5000|$|Outputs are same: {{one of the}} {{suitable}} <b>learning</b> <b>rules</b> {{is applied}} to the weights ...|$|R
40|$|Many {{learning}} systems {{must confront}} {{the problem of}} run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). This paper shows how the cost increase of a <b>learned</b> <b>rule</b> in an EBL system can be analyzed by characterizing the learning process as a sequence of transformations from a problem solving episode to a <b>learned</b> <b>rule.</b> The analysis of how the cost changes through the transformations can be {{a useful tool for}} revealing the sources of cost increase in the learning system. Once all of the sources are revealed, by avoiding these sources, the <b>learned</b> <b>rule</b> will never be expensive. That is, the cost of the <b>learned</b> <b>rule</b> will be bounded by the problem solving. We performed such a transformational analysis of chunking in Soar. The chunking process has been decomposed into a sequence of transformations from the problem solving to a chunk. By analyzing these transformations, we have identifi [...] ...|$|R
40|$|I {{describe}} metarules which order predicates {{contained in}} {{first order logic}} rules. These metarules are applied to rules created by a learning Go program. After the ordering of the predicates, the rules are matched orders of magnitude faster than the original <b>learned</b> <b>rules.</b> Key words : Metaprogramming, Metareasoning, First Order Logic, Game of Go, Machine Learning. 1 Introduction One aspect of machine learning systems is {{the efficiency of the}} <b>learned</b> <b>rules.</b> This problem has been referenced as the utility problem [Minton 1988]. I have written a learning program which transforms its problem solving activity in the efficient matching of first order rules [Cazenave 1996]. A component of this system is the compilation of the <b>learned</b> <b>rules.</b> This paper describes how <b>learned</b> <b>rules</b> are compiled so as to match them orders of magnitude faster. This problem has already been adressed in papers such as [Ishida 1988]. My approach makes explicit use of metaprogramming, my system reasons on itself so a [...] ...|$|R
