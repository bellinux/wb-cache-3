0|10000|Public
40|$|In this paper, {{the author}} {{considers}} how boundedly rational agents learn rational expectations when all equilibrium price functions or forecasts of future equilibrium prices {{are required to}} be computable. The paper examines two learning environments. In the first, agents have perfect information {{about the state of}} nature. In this case, the theory of machine inference can be applied to show that there is a broad class of computable economies whose rational expectations equilibria can be <b>learned</b> <b>by</b> <b>inductive</b> <b>inference.</b> In the second environment, agents do not have perfect information about the state of nature. In this case, a version of Godel's incompleteness theorem implies that rational expectations equilibria cannot be <b>learned.</b> Copyright 1989 <b>by</b> The Econometric Society. ...|$|R
40|$|AbstractA {{class of}} computable {{functions}} ismaximaliff {{it can be}} incrementally <b>learned</b> <b>by</b> some <b>inductive</b> <b>inference</b> machine (IIM), but no infinitely larger class of computable functions can be so learned. Rolf Wiehagen posed the question whether there exist such maximal classes. This question and many interesting variants are answered herein in the negative. Viewed positively, each IIM can be infinitely improved upon! Also discussed are the problems of algorithmically finding the improvements proved to exist...|$|R
5000|$|The axiom of {{uniformity}} of law [...] {{is necessary in}} order for scientists to extrapolate (<b>by</b> <b>inductive</b> <b>inference)</b> into the unobservable past. [...] The constancy of natural laws must be assumed {{in the study of}} the past; else we cannot meaningfully study it.|$|R
40|$|Berger, Bernardo and Sun’s thought-provoking {{paper offers}} a Bayesian {{resolution}} to the difficult philo-sophical problem raised <b>by</b> <b>inductive</b> <b>inference.</b> In a nutshell, the philosophical problem plaguing <b>inductive</b> <b>inference</b> is that no finite number of past occurrences of an event can prove its continuing occurrence in the future. It is thus natural to seek probabilistic reassurance for our instinctive feeling that an event re...|$|R
50|$|The {{probability}} function gives probabilities for results based on initial probabilities given <b>by</b> Boolean <b>inductive</b> <b>inference.</b>|$|R
40|$|AbstractWe present here {{a method}} for {{deriving}} a regular language that characterizes the set of reachable states of a given parameterized ring (made of N identical components). The method basically proceeds in two steps: first one generates a regular language L <b>by</b> <b>inductive</b> <b>inference</b> from a finite sample of reachable states; second one formally checks that L characterizes the whole set of reachable states...|$|R
40|$|Abstract: In this paper, a data {{acquisition}} system for QRS detections in 12 -lead electrocardiogram (ECG) diagnosis is proposed. This system collect data learns rules, synthesizing decision trees <b>by</b> <b>inductive</b> <b>inference</b> from examples. An extended form of Quinlan's algorithm is used. There is also possibility of continual improving the knowledge base maintained, <b>by</b> <b>learning</b> new rules and merging them with the old ones. As a start point for the knowledge base learned, the rules of an already developed expert system are used...|$|R
40|$|The {{rules of}} <b>inductive</b> <b>inference</b> are formalized using a {{transition}} system. The rejection of a consequence obtained <b>by</b> <b>inductive</b> <b>inference</b> is formalized <b>by</b> a revision rule. An inductive process is de ned as {{a sequence of}} versions of a theory generated by alternatively applying the <b>inductive</b> <b>inference</b> rules and the revision rule. An inductive procedure is constructed. It takes a sequence EM of instances of a given model M and a given formal theory as its inputs, and generates the inductive processes. It is proved that if EM contains all instances of the model M, then every <b>inductive</b> sequence generated <b>by</b> the procedure is convergent. Its limit is the set of all true statements of the model M...|$|R
40|$|The {{technology}} for building knowledge-based systems <b>by</b> <b>inductive</b> <b>inference</b> from examples {{has been demonstrated}} successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees {{that has been used}} in a variety of systems, and it describes one such system, ID 3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions...|$|R
40|$|Studies on the Isotropic Precursor Pitch for General Purpose Carbon Fiber[前田]管開口端からの圧縮波の放出により形成されるパルス波の特性に関する研究[安信]高電圧ガス遮断器用超音速ノズルに関する研究[岩本]Loacal Search and Approximtion for Combinatorial Optimization[下薗]Studies on Dynamical Behaviors of Fininfe Cellular Automata[李]Studies on Concept <b>Learning</b> <b>by</b> <b>Inductive</b> Influcence and Analogical Reasoning[宮原...|$|R
40|$|Using as {{examples}} Akerlof's 'market for ''lemons''' and Schelling's 'checkerboard' model of racial segregation, this paper asks how economists' abstract theoretical models can explain {{features of the}} real world. It argues that such models are not abstractions from, or simplifications of, the real world. They describe counterfactual worlds which the modeller has constructed. The gap between model world and real world can be filled only <b>by</b> <b>inductive</b> <b>inference,</b> and we can have more confidence in such inferences, the more credible the model is as an account of {{what could have been}} true. Methodology Of Economics, Economic Models, Induction,...|$|R
40|$|The Van der Luyt Transport {{company has}} {{installed}} performance sensors {{on a few}} of their transport vehicles, and now {{wants to be able to}} detect complex patterns in the data from these sensors in real-time. We designed a framework for the real-time detection of these patterns, we call this framework the Syntactic Timed Event Detection (STED) system. The STED system uses a syntactic pattern matching framework enhanced with the notion of time. We give the theoretical foundations, a prototype implementation, and a proof of concept of this framework. We show that a syntactic timed pattern can be learned automatically from positive and negative examples <b>by</b> <b>inductive</b> <b>inference,</b> a prototype <b>inductive</b> <b>inference</b> system which uses a <b>learning</b> <b>by</b> refinement framework is explained and implemented. It is our belief that the STED system is highly applicable to the problem of real-time event detection. i Preface This thesis is the result of my work on my masters thesis at the Delft University o...|$|R
40|$|INGENS is a {{prototype}} of GIS which integrates a geographic knowledge discovery engine to mine several kinds of spatial KDD objects from the topographic maps stored in a spatial database. In this paper we describe the main principles of an inductive spatial database in INGENS. Inductive database allows to keep permanent KDD objects and integrate database technology with systems for the geographic knowledge generation. In contrast to traditional spatial database technology, inductive database allows to answer queries which require synthesizing and applying plausible knowledge which is generated <b>by</b> (<b>inductive)</b> <b>inference</b> from both spatial objects and KDD objects (prior knowledge) stored in the same database. ...|$|R
40|$|In {{this paper}} we apply some recent work of Angluin (1982) to the {{induction}} of the English auxiliary verb system. In general, the induction of finite automata is computationally intractable. However, Angluin shows that restricted finite automata, the kreversible automata, can he <b>learned</b> <b>by</b> efficient (polynomial time) algorithms. We present an explicit computer model demonstrating that the English auxiliary verb system can {{in fact be}} learned as a 1 -reversible automaton, and hence in a computationally feasible amount of time. The entire system can be acquired by looking at only half the possible auxiliary verb sequences, and the pattern of generalization seems compatible with {{what is known about}} human acquisition of&quot;auxiliaries. We conclude that certain linguistic subsystems may well be learnable <b>by</b> <b>inductive</b> <b>inference</b> methods of this kind, and suggest an extension to context-free languages. ...|$|R
2500|$|Advocates of a Bayesian {{approach}} sometimes {{claim that}} the goal of a researcher is most often to objectively assess the probability that a hypothesis is true based on the data they have collected. [...] Neither Fisher's significance testing, nor Neyman–Pearson hypothesis testing can provide this information, and do not claim to. The probability a hypothesis is true can only be derived from use of Bayes' Theorem, which was unsatisfactory to both the Fisher and Neyman–Pearson camps due to the explicit use of subjectivity {{in the form of the}} prior probability. Fisher's strategy is to sidestep this with the p-value (an objective index based on the data alone) followed <b>by</b> <b>inductive</b> <b>inference,</b> while Neyman–Pearson devised their approach of inductive behaviour.|$|R
40|$|For several years, {{through the}} “material theory of induction, ” I have urged that <b>inductive</b> <b>inferences</b> are not {{licensed}} by universal schemas, but by material facts that hold only locally (Norton, 2003, 2005). My goal {{has been to}} defend <b>inductive</b> <b>inference</b> against <b>inductive</b> skeptics <b>by</b> demonstrating how <b>inductive</b> <b>inferences</b> are properly made. Since I have always admire...|$|R
5000|$|<b>By</b> now, <b>inductive</b> <b>inference</b> {{has been}} shown to exist, but is found rarely, as in {{programs}} of machine learning in Artificial Intelligence (AI). [...] Popper's stance on induction is strictly falsified—enumerative induction exists—but is overwhelmingly absent from science. Although much talked of nowadays by philosophers, abduction or IBE lacks rules of inference and the discussants provide nothing resembling such, as the process proceeds by humans' imaginations and perhaps creativity.|$|R
40|$|We have {{developed}} an efficient algorithm for determining if a Finite State Automaton de-scribes a Strictly Local (SL) stringset, the sim-plest class of the Sub-Regular Hierarchy, and to determine if that stringset is a subclass of SL that is learnable <b>by</b> an <b>Inductive</b> <b>Inference</b> Machine. We have used this to categorize the phonotactic patterns in a catalog including es-sentially all of the currently attested patterns occurring in natural languages, most of which {{turn out to be}} learnable stringsets in this sim-plest class. ...|$|R
40|$|It {{has been}} widely {{recognized}} that size, shape, and distance perception are not the mere translation of images in the eyes, as retinal images are inherently ambiguous. Some form of knowledge and/or assump-tions <b>by</b> unconscious <b>inductive</b> <b>inference</b> seems to be necessary (Gregory, 1997). With respect to this topic, visual illusions are a valuable tool for understanding the neuro-cognitive systems underlying visual perception by indirectly revealing the hid-den constraints of the perceptual system {{in a way that}} normal perception cannot. In humans, such constraints have been often summarized as the so-called “Gestal...|$|R
40|$|We present here {{a method}} for {{deriving}} a regular language that characterizes the set of reachable states of a given parametrized ring (made of N of identical components). The method basically proceeds in two steps: first one generates a regular language L <b>by</b> <b>inductive</b> <b>inference</b> from a finite sample of reachable states; second one formally checks that L characterizes the whole set of reachable states. 1 Introduction During these last years, several kinds of methods have been explored {{in order to prove}} a property P about a ring of N identical finite-state processes irrespective of its size N. They are essentially three. The first is by induction (see, e. g., [20, 19, 13]), but often relies on human help for the introduction of appropriate `lemmas' or `invariants'. The second is by reduction to the verification problem for a fixed small size (e. g., N= 2) (see, e. g., [10, 17]), but works only for restrictive classes of rings. The third is by abstraction (see, e. g., [8, 18, 15]) : an abstract mode [...] ...|$|R
40|$|The {{classical}} {{framework of}} our business system, i. e. businesses, households, and government, has not been questioned thus far. This study challenges to destruct it not by deductive approaches but <b>by</b> <b>inductive</b> <b>inferences</b> {{especially in terms of}} actual start-up processes which so-called entrepreneurial groups enact. Based on both the actual data and evidences by the semi-structured interviews conducted in 2014 by the author, an alternative concept of entrepreneurial groups or organizations, i. e. the center of the gravity of organizations, and its framework will be introduced. They stem from“a field of personal forces”(Barnard， 1968) ．It is true that preceding studies pay attention to the roles of spouse/cohabiting partner (typical strong-ties) ，but their focus is financial assistance. This study would rather illuminate the fact that their mental support, whether overt or covert, makes entrepreneur groups enact start-up processes in the long run. In addition, some educational problems around support for potential start-ups in Japan will be discussed. 長崎大学経済学部創立 110 周年記念論文集Essays in Commemoration of the 110 th Anniversary of the Faculty of Economics, Nagasaki Universit...|$|R
40|$|AbstractMost {{theories}} of learning consider inferring a function f from either (1) observations about f or, (2) questions about f. We consider a scenario whereby the learner observes f and asks queries to some set A. If I is {{a notion of}} learning then I[A] is the set of concept classes I-learnable <b>by</b> an <b>inductive</b> <b>inference</b> machine with oracle A. A and B are I-equivalent if I[A] = I[B]. The equivalence classes induced are the degrees of inferability. We prove several results about when these degrees are trivial, and when the degrees are omniscient (i. e., the set of recursive function is learnable) ...|$|R
40|$|Expert systems divide {{neatly into}} two categories: {{those in which}} (1) the expert {{decisions}} result in changes to some external environment (control systems), and (2) the expert decisions merely seek to describe the environment (classification systems). Both the explanation of computer-based reasoning and the "bottleneck" (Feigenbaum, 1979) of knowledge acquisition are major issues in expert systems research. We have contributed to these areas of research in two ways. Firstly, we have implemented an expert system shell, the Mugol environment, which facilitates knowledge acquisition <b>by</b> <b>inductive</b> <b>inference</b> and provides automatic explanation of run-time reasoning on demand. RuleMaster, a commercial version of this environment, {{has been used to}} advantage industrially in the construction and testing of two large classification systems. Secondly, we have investigated a new technique called sequence induction which can be used in the construction of control systems. Sequence induction is based on theoretical work in grammatical learning. We have improved existing grammatical learning algorithms as well as suggesting and theoretically characterising new ones. These algorithms have been successfully applied to the acquisition of knowledge for a diverse set of control systems, including inductive construction of robot plans and chess end-game strategies...|$|R
40|$|We {{prove that}} every set of partial {{recursive}} functions {{which can be}} identified <b>by</b> an <b>inductive</b> <b>inference</b> machine is included in some identifiable function set with index set in Σ 3 ∩ Π 3. An identifiable set is presented with index set in Σ 2 ∩ Π 3 but neither in Σ 2 nor in Π 2. Furthermore we {{show that there is}} no nonempty identifiable set with index set in Σ 1. In Π 1 it is possible to locate this king of set. In {{the last part of the}} paper we show that the problem to identify all partial recursive functions and the halting problem are of the same degree of unsolvability...|$|R
40|$|Word {{pronunciation}} can be <b>learned</b> <b>by</b> <b>inductive</b> machine <b>learning</b> algorithms when it {{is represented}} as a classification task: classify a letter within its local word context as mapping to its pronunciation. On the basis of generalization accuracy results from empirical studies, we argue that word pronunciation, particularly in complex spelling systems {{such as that of}} English, should not be modelled in a way that abstracts from exceptions. Learning methods such as decision tree and backpropagation learning, while trying to abstract from noise, also throw away a large number of useful exceptional cases. Our empirical results suggest that a memory-based approach which stores all available word-pronunciation knowledge as cases in memory, and generalises from this lexicon via analogical reasoning, is at a [...] ...|$|R
40|$|Representation and {{inference}} {{of spatial}} knowledge play {{a fundamental role in}} spatial reasoning, which itself {{is an important}} component of many applications such as Geographic Information Systems or robotics. This paper discusses how an existing relation between speaker-relative and absolute spatial reference systems can be automatically <b>learned</b> <b>by</b> an <b>inductive</b> relational machine learning system...|$|R
40|$|Biological systems often detect species-specific {{signals in}} the environment. In humans, speech and {{language}} are species-specific signals of fundamental biological importance. To detect the linguistic signal, human brains must form hierarchical representations from {{a sequence of}} perceptual inputs distributed in time. What mechanism underlies this ability? One hypothesis is that the brain repurposed an available neurobiological mechanism when hierarchical linguistic representation became an efficient solution to a computational problem posed to the organism. Under such an account, a single mechanism must {{have the capacity to}} perform multiple, functionally related computations, e. g., detect the linguistic signal and perform other cognitive functions, while, ideally, oscillating like the human brain. We show that a computational model of analogy, built for an entirely different purpose-learning relational reasoning-processes sentences, represents their meaning, and, crucially, exhibits oscillatory activation patterns resembling cortical signals elicited by the same stimuli. Such redundancy in the cortical and machine signals is indicative of formal and mechanistic alignment between representational structure building and "cortical" oscillations. <b>By</b> <b>inductive</b> <b>inference,</b> this synergy suggests that the cortical signal reflects structure generation, just as the machine signal does. A single mechanism-using time to encode information across a layered network-generates the kind of (de) compositional representational hierarchy that is crucial for human language and offers a mechanistic linking hypothesis between linguistic representation and cortical computation...|$|R
40|$|An {{important}} {{technique for}} investigating derivability in formal systems of arithmetic {{has been to}} embed such systems into semi-formal systems with the omega-rule. This paper exploits this notion within the domain of automated theorem-proving and discusses the implementation of such a proof environment, namely the CORE system which implements {{a version of the}} primitive recursive omega-rule. This involves providing an appropriate representation for infinite proofs, and a means of verifying properties of such objects. By means of the CORE system, from a finite number of instances a conjecture for a proof of the universally quantified formula is automatically derived <b>by</b> an <b>inductive</b> <b>inference</b> algorithm, and checked for correctness. In addition, candidates for cut formulae are generated <b>by</b> an explanation-based <b>learning</b> algorithm...|$|R
40|$|This paper investigates {{methods to}} {{automatically}} infer structural information from large XML documents. Using XML {{as a reference}} format, we approach the schema generation problem <b>by</b> application of <b>inductive</b> <b>inference</b> theory. In doing so, we review and extend results relating to the search spaces of grammatical inferences for large data set. We evaluate {{the result of an}} inference process using the concept of Minimum Message Length. Comprehensive experimentation reveals our new hybrid method to be the most effective for large documents. Finally tractability issues, including scalability analysis, are discussed...|$|R
40|$|Fundamental learninng {{strategies}} {{are discussed in}} the context of knowledse acquisition for expert systems. These strategies reflect the type of inference performed by the learner on the input infor- mation in order to derive the desired knowledge. They include learnin 8 from instruction, learain 8 <b>by</b> deduction, <b>learning</b> <b>by</b> analogy and learnin 8 by induction. Special attention is given to two basic types of <b>learning</b> <b>by</b> induction: learnin 8 from examples (concept acquisition) and learning from observation (concept formation without teacher). A specific form of learning from'observa- tion, namely, conceptual clustering, is discussed in detail, and illustrated by an example. Conceptual clustering is a process of structuring given observations into a hierarchy of conceptual catego- ties. An inductive learning system generates knowledge <b>by</b> drawing <b>inductive</b> <b>inferences</b> from the given facts under the guidance of background knowledge. The backsround knowledge contains previously learned concepts, goals of learning, the criteria for evaluating hypotheses from the viewpoint of these goals, the propeies of attributes and relations used to chracterize observed events, and various inference rules for transformin 8 concepts or expressing them at different lev- els of abstraction...|$|R
40|$|ION Bertjan Busser Walter Daelemans Antal van den Bosch ILK / Computational Linguistics, Tilburg University, The Netherlands fG. J. Busser,Walter. Daelemans,Antal. vdnBoschg@kub. nl ABSTRACT Word {{pronunciation}} can be <b>learned</b> <b>by</b> <b>inductive</b> machine <b>learning</b> algorithms when it {{is represented}} as a classification task: classify a letter within its local word context as mapping to its pronunciation. On the basis of generalization accuracy results from empirical studies, we argue that word pronunciation, particularly in complex spelling systems {{such as that of}} English, should not be modelled in a way that abstracts from exceptions. Learning methods such as decision tree and backpropagation learning, while trying to abstract from noise, also throw away a large number of useful exceptional cases. Our empirical results suggest that a memory-based approach which stores all available word-pronunciation knowledge as cases in memory, and generalises from this lexicon via analogical reasoning, is at al [...] ...|$|R
40|$|Abstract. In this paper, {{we present}} a {{probabilistic}} method of dealing with multiclass classification using Stochastic Logic Programs (SLPs), a Probabilistic Inductive Logic Programming (PILP) framework that integrates probability, logic representation and learning. Multi-class prediction attempts to classify an observed datum or example into its proper classification given {{that it has been}} tested to have multiple predictions. We apply an SLP parameter estimation algorithm to a previous study in the protein fold prediction area and a multi-class classification working example, in which logic programs have been <b>learned</b> <b>by</b> <b>Inductive</b> Logic Programming (ILP) and a large number of multiple predictions have been detected. On the basis of several experiments, we demonstrate that PILP approaches (eg. SLPs) have advantages for solving multi-class prediction problems with the help of learned probabilities. In addition, we show that SLPs outperform ILP plus majority class predictor in both predictive accuracy and result interpretability. ...|$|R
40|$|It {{is widely}} held that {{children}} learn {{the concept of}} natural number <b>by</b> <b>inductive</b> <b>inference</b> from {{their knowledge of the}} first few numbers, helped by their ability to count. Rips et al. (2006) argue that this view is wrong, and I argue that their argument is wrong. What do number words mean? If a panel of linguists, psychologists, and philosophers was convened, it is more than likely that they would soon con-verge on an answer along the following lines: (1) The n-th number word denotes that property which a collection x has iff card(x) = n. (Here “iff ” is short for “if and only if”, and “card(x) ” is the number of individuals in x.) This answer is not unproblematic in every respect: in particular, the notion of “collection ” raises some rather deep issues. But apart from that, (1) surely represents the majority view across disciplines. Natural though it may seem, this consensus shouldn’t be taken for granted, for there are perfectly reasonable alternatives. For example, one might view number words as being on a par with quantifiers like all, and for some time it was fashionable, in some circles at least, to construe two as “two or more”. But such dissident views are becoming increasingly marginal (Geurts 2006). Next question: How do children learn number words and the associated concepts? This question, too, has a standard answer, according to which the acquisition process involves three parts. First, children learn to recite the count words in sequence. Secondly, learning the meanings of the firs...|$|R
40|$|This paper proposes {{an expert}} system called VIBEX (VIBration EXpert) to aid plant {{operators}} in diagnosing {{the cause of}} abnormal vibration for rotating machinery. In order to automatize the diagnosis, a decision table based on the cause-symptom matrix {{is used as a}} probabilistic method for diagnosing abnormal vibration. Also a decision tree is used as the acquisition of structured knowledge in the form of concepts is introduced to build a knowledge base which is indispensable for vibration expert systems. The decision tree is a technique used for building knowledge-based systems <b>by</b> the <b>inductive</b> <b>inference</b> from examples and plays a role itself as a vibration diagnostic tool. The proposed system has been successfully implemented on Microsoft Windows environment and is written in Microsoft Visual Basic and Visual CCC. To validate the system performance, the diagnostic system was tested with some examples using the two diagnostic methods...|$|R
40|$|R. Freivalds and C. H. Smith [FS 92] {{proved that}} {{probabilistic}} limited memory <b>inductive</b> <b>inference</b> machines can learn some classes of total recursive functions with a probability I which cannot be <b>learned</b> <b>by</b> deterministic limited memory <b>inductive</b> <b>inference</b> machines. We introduce quantum limited memory <b>inductive</b> <b>inference</b> machines as quantum finite automata used as <b>inductive</b> <b>inference</b> machines. Our main result below shows that quantum limited memory <b>inductive</b> <b>inference</b> machines can learn classes of total recursive functions not learnable by any deterministic and even by probabilistic limited memory <b>inductive</b> <b>inference</b> machines...|$|R
40|$|AbstractFreivalds and Smith [R. Freivalds, C. H. Smith Memory limited <b>inductive</b> <b>inference</b> machines, Springer Lecture Notes in Computer Science 621 (1992) 19 – 29] {{proved that}} {{probabilistic}} limited memory <b>inductive</b> <b>inference</b> machines can learn with probability  1 certain classes of total recursive functions, which cannot be <b>learned</b> <b>by</b> deterministic limited memory <b>inductive</b> <b>inference</b> machines. We introduce quantum limited memory <b>inductive</b> <b>inference</b> machines as quantum finite automata acting as <b>inductive</b> <b>inference</b> machines. These machines, we show, can learn classes of total recursive functions not learnable by any deterministic, nor even by probabilistic, limited memory <b>inductive</b> <b>inference</b> machines...|$|R
