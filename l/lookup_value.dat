0|27|Public
5000|$|Proteus {{supports}} associative arrays (called sets) and AVL trees, {{which are}} very useful and powerful to quickly sortand <b>lookup</b> <b>values.</b>|$|R
40|$|This paper {{presents}} {{a new type}} of AES implementation using normal basis. The method is based on a lookup technique which makes use of inversion and shift registers which leads to a smaller size of lookup for the S-box than its corresponding implementations. The reduction in lookup size is based on grouping sets of inverses into cosets which in turn leads to a {{reduction in the number of}} <b>lookup</b> <b>values.</b> The above technique is implemented in a regular AES architecture using register files which requires less interconnect and is suitable for security applications. The results for the implementation are competitive in throughput and area compared with the corresponding solutions in polynomial basis. ...|$|R
5000|$|... (In some languages, only {{actual data}} types are allowed as <b>values</b> in the <b>lookup</b> table. In other languages, {{it is also}} {{possible}} to assign functions as <b>lookup</b> table <b>values,</b> gaining the same flexibility as a real [...] statement. See Control table article for more detail on this).|$|R
5000|$|Property <b>lookup</b> {{support for}} <b>values</b> {{defined in the}} {{configuration}} file, system properties, environment variables, the ThreadContext Map, and data present in the event ...|$|R
2500|$|Lookup Fields, which {{get their}} values by [...] "looking up" [...] some {{value in a}} table, have been updated to support multi <b>valued</b> <b>lookups.</b>|$|R
40|$|In {{this short}} report, we {{consider}} {{two types of}} equation of state (EOS) scalings, which will {{be referred to as}} type I (or density scaling) and type II (or Thomas-Fermi scaling). EOS scalings are used to correct for the fact that EOS tables are constructed with a fixed isotopic composition. Even if the isotopics in a computational zone are evolving through chemical or nuclear reactions, the EOS lookups for pressure and energy (and their derivatives with respect to density and temperature) aren't normally cognizant of this fact. The EOS scalings are an attempt to fix this shortcoming. They typically modify the incoming density and/or temperature based on ratios of isotopic quantities (like {sub table}/{sub zone}, {sub table}/{sub zone}), and then modify the table <b>lookup</b> <b>values.</b> In this way, the EOS can dynamically respond to the changing isotopics...|$|R
5000|$|As {{described}} above, a skiplist {{is capable}} of fast [...] insertion and removal of values from a sorted sequence, but it has only slow [...] <b>lookups</b> of <b>values</b> at a given position in the sequence (i.e. return the 500th value); however, with a minor modification the speed of random access indexed lookups can be improved to [...]|$|R
50|$|Before {{the advent}} of computers, <b>lookup</b> tables of <b>values</b> were used to speed up hand {{calculations}} of complex functions, such as in trigonometry, logarithms, and statistical density functions.|$|R
25|$|Most {{software}} applications will compute small factorials by direct multiplication or table <b>lookup.</b> Larger factorial <b>values</b> can be approximated using Stirling's formula. Wolfram Alpha can calculate exact {{results for the}} ceiling function and floor function applied to the binary, natural and common logarithm of n! for values of n up to 249999, and up to 20,000,000! for the integers.|$|R
30|$|Each {{coordinator}} {{maintains a}} <b>lookup</b> table for <b>values</b> within each store. Each participant within a sub-region periodically gossips {{a subset of}} keys which it is currently responsible for. Keys are chosen uniformly at random. Every other node in that sub-region receives those messages and updates its lookup table. A timeout, proportional to the average time between updates {{and the number of}} keys in the sub-region is used to remove stale entries.|$|R
40|$|An {{approach}} for fast discrete function evaluation based on multi-valued decision diagrams (MDD) is proposed. The MDD for a logic function is {{translated into a}} table on which function evaluation is performed by a sequence of address <b>lookups.</b> The <b>value</b> of a function for a given input assignment is obtained with at most one lookup per input. The main application is to cycle-based logic simulation of digital circuits, where the prin-cipal difference from other logic simulators is that only values of the output and latch ports are computed. Theoretically, deci-sion-diagram based function evaluation offers orders-of-mag-nitude potential speedup over traditional logic simulation methods. In practice, memory bandwidth becomes the domi-nant consideration on large designs. We describe techniques to optimize usage of the memory hierarchy. ...|$|R
40|$|We present four {{approaches}} to the Amharic- French bilingual track at CLEF 2005. All experiments use a dictionary based approach to translate the Amharic queries into French Bags-of-words, but while one approach uses word sense discrimination on the translated side of the queries, the other one includes all senses of a translated word in the query for searching. We used two search engines: The SICS experimental engine and Lucene, hence four runs with the two approaches. Non-content bearing words were removed {{both before and after}} the dictionary <b>lookup.</b> TF/IDF <b>values</b> supplemented by a heuristic function was used to remove the stop words from the Amharic queries and two French stopwords lists were used to remove them from the French translations. In our experiments, we found that the SICS search engine performs better than Lucene and that using the word sense discriminated keywords produce a slightly better result than the full set of non discriminated keywords...|$|R
40|$|Abstract. In {{object-oriented}} programming languages, multiple dispatching provides increased expressive power over single dispatching by guiding method <b>lookup</b> using the <b>values</b> of all arguments {{instead of the}} receiver only. There have been several programming languages supporting this mechanism and they demonstrate its usefulness. However, efficient implementation of multi-methods is critical with regard to its success as a standard. In this paper, we present a new mechanism for implementing multi-methods dynamic lookup based on automaton techniques. Analysis and experimental results show that our strategy is time and space efficient. The presented result can {{provide the basis for}} designing new object-oriented paradigms based on multi-methods. ...|$|R
40|$|Abstract. We {{describe}} {{the execution of}} Epigram on a stock architecture, compiling via a core type theory and a supercombinator language. We show, via optimising transformations on the core type theory, that unused or duplicated values can be erased at run-time. Thus there exists a phase distinction, not between types and values, but between values which are used at compile-time only and values which are used at runtime. Through a simple example, <b>lookup</b> of a <b>value</b> in a sized vector, we show how our optimisations remove compile-time only values from terms and, furthermore, how we can use straightforward static analysis with our rich type information to avoid the need for any run-time bounds check when executing the lookup function. ...|$|R
40|$|Abstract. CoCoE {{stands for}} Complexity, Coherence and Entropy, and {{presents}} an extensible methodology for empirical analysis of Linked Open Data (i. e., RDF graphs). CoCoE can offer {{answers to questions}} like: Is dataset A better than B for knowledge discovery since it is more complex and informative?, Is dataset X better than Y for simple <b>value</b> <b>lookups</b> due its flatter structure?, etc. In order to address such questions, we introduce a set of well-founded measures based on complementary no-tions from distributional semantics, network analysis and information theory. These measures {{are part of a}} specific implementation of the Co-CoE methodology that is available for download. Last but not least, we illustrate CoCoE by its application to selected biomedical RDF datasets. ...|$|R
40|$|We {{describe}} {{the execution of}} EPIGRAM on a stock architecture such as the G-machine, compiling via a core type theory and a supercombinator language. We show, via optimising transformations on the core type theory, that unused or duplicated values can be erased at run-time. Thus there exists a phase distinction, not between types and values, but between values which are used at compile-time only and values which are used at run-time. Through a simple example, <b>lookup</b> of a <b>value</b> in a sized vector, we show how our optimisations remove compile-time only values from terms and, furthermore, how we can use straightforward static analysis with our rich type information to avoid the need for any run-time bounds check when executing the lookup function. 1...|$|R
40|$|A {{number of}} gearbox {{failures}} {{can be attributed}} to lubricant related problems. One measure of the condition of gearbox oil is its viscosity. In electrically powered systems, motor current signal analysis allows online estimation of the viscosity of gearbox oil without requiring additional sensors. Previous work on this problem entailed monitoring the power (and change in power) of sidebands of the shaft frequency in the induction motor current spectrum. Sideband frequencies in the current spectrum can however be influenced by other potential problems in the electromechanical system ranging from bearing faults to gearbox teeth damage. Changes in the lubricant viscosity result in changes in the mechanical and thermal losses in the system. These small deviations in the mechanical and thermal losses in the system become visible in the ratio of the electrical energy demanded by the induction motor to the kinetic energy of the rotating mechanical parts. Speed and load invariance can be ensured by normalizing the measured energy ratio with <b>lookup</b> table <b>values</b> obtained when the system attained thermal equilibrium. Speed or load perturbations in the system give rise to small deviations in the normalized energy ratio curve. The distributions of these deviations are significantly different (in a statistical sense) for different oil viscosity values...|$|R
40|$|We {{present a}} novel {{technique}} for multiple-dispatching. In object-oriented programming languages, multiple-dispatching provides increased expressive power over single-dispatching by guiding method <b>lookup</b> using the <b>values</b> of all arguments {{instead of the}} receiver only. However, the efficient implementation for multiple-dispatching is still critical with regard to its success as a standard. There have been several time-efficient dispatching techniques proposed, {{but they are very}} space consuming. In this paper, we transform multiple-dispatching into a lookup automaton problem. Analysis and experiments show that our approach is space-efficient while providing the same time-efficiency as the previous known techniques. Moreover, we present a technique to further minimize the space-complexity of lookup automata. Key Words. Multi-methods, Multiple-dispatching, Lookup, Type hierarchy, Automata. 1 Introduction Today most programming languages are based on the notion of types. A data type consi [...] ...|$|R
40|$|Multiple {{dispatching}} provides increased expressive {{power over}} single dispatching by guiding method <b>lookup</b> using the <b>values</b> of all arguments instead {{of only the}} receiver. However, existing languages with multiple dispatching do not encourage the dataabstraction -oriented programming style that is encouraged by traditional single-dispatching languages; instead existing multiple-dispatching languages tend to foster a functionoriented programming style organized around generic functions. We propose an alternative view of multiple dispatching that is intended to promote a data-abstraction-oriented programming style. Instead of viewing a multi-method as "outside" of all objects, we view a multi-method as "inside" the objects for which the multi-method applies (on which it dispatches). Because objects are closely connected to the multi-methods implementing their operations, the internals of an object can be encapsulated by being accessible only to the closely-connected multi-methods. We are e [...] ...|$|R
5000|$|Before {{the advent}} of computers, printed <b>lookup</b> tables of <b>values</b> were used by people to speed up hand {{calculations}} of complex functions, such as in trigonometric tables, logarithm tables, and tables of statistical density functions School children are often taught to memorize [...] "times tables" [...] to avoid calculations {{of the most commonly}} used numbers (up to 9 x 9 or 12 x 12). Even as early as 493 A.D., Victorius of Aquitaine wrote a 98-column multiplication table which gave (in Roman numerals) the product of every number from 2 to 50 times and the rows were [...] "a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144" ...|$|R
40|$|CoCoE {{stands for}} Complexity, Coherence and Entropy, and {{presents}} an extensible methodology for empirical analysis of Linked Open Data (i. e., RDF graphs). CoCoE can offer {{answers to questions}} like: Is dataset A better than B for knowledge discovery since it is more complex and informative?, Is dataset X better than Y for simple <b>value</b> <b>lookups</b> due its flatter structure?, etc. In order to address such questions, we introduce a set of well-founded measures based on complementary notions from distributional semantics, network analysis and information theory. These measures {{are part of a}} specific implementation of the CoCoE methodology that is available for download. Last but not least, we illustrate CoCoE by its application to selected biomedical RDF datasets. Comment: A current working draft of the paper submitted to the ISWC' 14 conference (track information available here: [URL]...|$|R
40|$|Under the European Union’s Thematic Strategy for Soil Protection, the European Commission’s Directorate-General for the Environment (DG Environment) has {{identified}} the mitigation of soil losses by erosion {{as a priority}} area. Policy makers call for an overall assessment of soil erosion in their geographical area of interest. They have asked that risk areas for soil erosion be mapped under present land use and climate conditions, and that appropriate measures be taken to control erosion within the legal and social context of natural resource management. Remote sensing data help to better assessment of factors that control erosion, such as vegetation coverage, slope length and slope angle. In this context, the data availability of remote sensing data {{during the past decade}} facilitates the more precise estimation of soil erosion risk. Following the principles of the Universal Soil Loss Equation (USLE), various options to calculate vegetative cover management (C-factor) have been investigated. The use of the CORINE Land Cover dataset in combination with <b>lookup</b> table <b>values</b> taken from the literature is presented as an option that has the advantage of a coherent input dataset but with the drawback of static input. Recent developments in the Copernicus programme have made detailed datasets available on land cover, leaf area index and base soil characteristics. These dynamic datasets allow for seasonal estimates of vegetation coverage, and their application in the G 2 soil erosion model which represents a recent approach to the seasonal monitoring of soil erosion. The use of phenological datasets and the LUCAS land use/cover survey ar...|$|R
40|$|For this {{assignment}} {{you will need}} to implement a spell checker. Our spell checker works by looking up every word from a text in a word list. When a word is not found in the list it will be reported as a possible spelling error. This is the easy part, and the spell-checker. c file that does this {{can be found on the}} course website. The tricky part, which you have to implement, is to make these lookups fast. You will be spell checking whole books, the longest of which is 565000 words. And because the word list you will be using contains more than 600. 000 words, you will have to perform these lookups very efficiently using a hash table. A hash table is an efficient way to implement a dictionary. It allows you to store and <b>lookup</b> (key, <b>value)</b> pairs. It is a generalized version of an array. With an array the key is always an integer index directly into the array. With a hash table you can use any kind of key, and it still allows you to lookup a word nearly as fast as if you where indexing an ordinary array. It works by taking the key and using a function to transform the key into an integer index. The index is then used to access the array to retrieve the value associated with the key. The function that takes the key and computes the index is called a hash function. A good hash function is quick and uses all the key data for the index calculation. For your spell checker {{you will need to}} write a hash function that hashes string...|$|R
40|$|In this paper, {{we present}} an {{analytical}} study of proximity-aware structured peer-to-peer networks under churn. We use a master-equation-based approach, {{which is used}} traditionally in non-equilibrium statistical mechanics to describe steady-state or transient phenomena. In earlier work we have demonstrated that this methodology is in fact also well suited to describing structured overlay networks under churn, by showing how we can accurately predict {{the average number of}} hops taken by a <b>lookup,</b> for any <b>value</b> of churn, for the Chord system. In this paper, we extend the analysis so as to also be able to predict lookup latency, given an average latency for the links in the network. Our results show that there exists a region in the parameter space of the model, depending on churn, the number of nodes, the maintenance rates and the delays in the network, when the network cannot function as a small world graph anymore, due to the farthest connections of a node always being wrong or dead. We also demonstrate how it is possible to analyse proximity neighbour selection or proximity route selection within this formalism...|$|R

