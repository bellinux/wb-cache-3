87|29|Public
5000|$|... #Caption: Adaptive <b>linear</b> <b>combiner,</b> compact representation. k = sample number, n=input {{variable}} index, x = reference inputs, d = desired input, ε = error output, Σ = summation.|$|E
5000|$|The LMS {{algorithm}} {{does not}} require that the X values have any particular relationship; therefor {{it can be used}} to adapt a <b>linear</b> <b>combiner</b> as well as an FIR filter. In this case the update formula is written as: ...|$|E
5000|$|... #Caption: Adaptive <b>linear</b> <b>{{combiner}}</b> {{showing the}} combiner and the adaption process. k = sample number, n=input variable index, x = reference inputs, d = desired input, W = set of filter coefficients, ε = error output, Σ = summation, upper box=linear combiner, lower box=adaption algorithm.|$|E
40|$|A new {{theoretical}} {{framework for the}} analysis of <b>linear</b> <b>combiners</b> is presented in this paper. This framework extends the scope of previous analytical models, and provides some new theoretical results which improve the understanding of <b>linear</b> <b>combiners</b> operation. In particular, we show that the analytical model developed in seminal works by Tumer and Ghosh is included in this framework...|$|R
40|$|In this paper, a {{theoretical}} and experimental analysis of <b>linear</b> <b>combiners</b> for multiple classifier systems is presented. Although <b>linear</b> <b>combiners</b> {{are the most}} frequently used combining rules, many important issues related to their operation for pattern classification tasks lack {{a theoretical}} basis. After a critical review of the framework developed in works by Tumer and Ghosh [30], [31] on which our analysis is based, we focus on the simplest and most widely used implementation of <b>linear</b> <b>combiners,</b> which consists of assigning a nonnegative weight to each individual classifier. Moreover, we consider the ideal performance of this combining rule, i. e., that achievable when the optimal values of the weights are used. We do not consider the problem of weights estimation, which has been addressed in the literature. Our theoretical analysis shows how the performance of <b>linear</b> <b>combiners,</b> in terms of misclassification probability, depends on the performance of individual classifiers, and on the correlation between their outputs. In particular, we evaluate the ideal performance improvement that can be achieved using the weighted average over the simple average combining rule and investigate in what way it depends on the individual classifiers. Experimental results on real data sets show that the behavior of <b>linear</b> <b>combiners</b> agrees with the predictions of our analytical model. Finally, we discuss the contribution to {{the state of the art}} and the practical relevance of our theoretical and experimental analysis of <b>linear</b> <b>combiners</b> for multiple classifier systems...|$|R
50|$|VEST ciphers {{consist of}} four {{components:}} a non-linear counter, a linear counter diffusor, a bijective non-linear accumulator {{with a large}} state and a <b>linear</b> output <b>combiner</b> (as illustrated by {{the image on the}} top-right corner of this page). The RNS counter consists of sixteen NLFSRs with prime periods, the counter diffusor is a set of 5-to-1 <b>linear</b> <b>combiners</b> with feedback compressing outputs of the 16 counters into 10 bits {{while at the same time}} expanding the 8 data inputs into 9 bits, the core accumulator is an NLPFSR accepting 10 bits of the counter diffusor as its input, and the output combiner is a set of 6-to-1 <b>linear</b> <b>combiners.</b>|$|R
50|$|The {{adaptive}} <b>linear</b> <b>combiner</b> (ALC) {{resembles the}} adaptive {{tapped delay line}} FIR filter except {{that there is no}} assumed relationship between the X values. If the X values were from the outputs of a tapped delay line, then the combination of tapped delay line and ALC would comprise an adaptive filter. However, the X values could be the values of an array of pixels. Or they could be the outputs of multiple tapped delay lines. The ALC finds use as an adaptive beam former for arrays of hydrophones or antennas.|$|E
30|$|The {{adaptive}} system {{is composed of}} an adaptive <b>linear</b> <b>combiner</b> W (Section 3.1).|$|E
3000|$|... {{generalized}} RBF {{network is}} applied, then a <b>linear</b> <b>combiner</b> at their outputs makes an online {{identification of the}} nonlinear system. The weights of <b>linear</b> <b>combiner</b> are updated by the normalized LMS algorithm. We have showed that the proposed method is {{more than three times}} faster in comparison with the FX-LMS algorithm with 30 % lower error. Also the DM_RBF method will converge in changing the input frequency, while it makes the FX-LMS cause divergence.|$|E
40|$|Abstract—For {{classifier}} ensembles, {{an effective}} combination {{method is to}} combine the outputs of each classifier using a linearly weighted combination rule. There are multiple ways to linearly combine classifier outputs and it is beneficial to analyze them as a whole. We present a unifying framework for multiple linear combination types in this paper. This unification enables using the same learning algorithms for different types of <b>linear</b> <b>combiners.</b> We present various ways to train the weights using regularized empirical loss minimization. We propose using the hinge loss for better performance {{as compared to the}} conventional least-squares loss. We analyze the effects of using hinge loss for various types of linear weight training by running experiments on three different databases. We show that, in certain problems, <b>linear</b> <b>combiners</b> with fewer parameters may perform as well as the ones with much larger number of parameters even in the presence of regularization. Keywords-classifier fusion, <b>linear</b> <b>combiners,</b> stacked generalization, <b>linear</b> classifier learning. I...|$|R
40|$|So far few {{theoretical}} works {{investigated the}} conditions under which specific fusion rules can work well, and a unifying framework for comparing rules of different complexity is clearly beyond the state of the art. A clear theoretical comparison is lacking even if one focuses on specific classes of <b>combiners</b> (e. g., <b>linear</b> <b>combiners).</b> In this paper, we theoretically compare simple and weighted averaging rules for fusion of imbalanced classifiers. Continuing the work reported in [10], we get a deeper knowledge of classifiers' imbalance effects in <b>linear</b> <b>combiners.</b> In addition, we experimentally compare the performance of linear and order statistics combiners for ensembles with different degrees of classifiers imbalance...|$|R
40|$|Several {{researchers}} have experimentally shown that substantial improvements {{can be obtained}} in difficult pattern recognition problems by combining or integrating the outputs of multiple classifiers. This paper provides an analytical framework to quantify the improvements in classification results due to combining. The results apply to both <b>linear</b> <b>combiners</b> and the order statistics combiners introduced in this paper. We show that combining networks in output space reduces the variance of the actual decision region boundaries around the optimum boundary. For <b>linear</b> <b>combiners,</b> we show that {{in the absence of}} classifier bias, the added classification error is proportional to the boundary variance. For non-linear combiners, we show analytically that the selection of the median, the maximum and in general the ith order statistic improves classifier performance. The analysis presented here facilitates the understanding of the relationships among error rates, classifier boundary distributions [...] ...|$|R
40|$|This paper {{presents}} the results from a hardware demonstration of a noise radiation sensor for a baffled rectangular plate. The sensor consists of an array of independent piezoelectric patches connected to a <b>linear</b> <b>combiner.</b> The coefficients of the <b>linear</b> <b>combiner</b> are computed in order to reconstruct (in real time) the volume displacement (or velocity). The robustness of the volume displacement sensor {{with respect to the}} location of the disturbance source is investigated. ...|$|E
40|$|Classifier {{combination}} {{has been}} an important research area because of their contribution to the accuracy and robustness. Supervised <b>linear</b> <b>combiner</b> types are shown to be strong combiners; but nonlinear types are not well investigated. In this work, we show a method to obtain non-linear versions of simple <b>linear</b> <b>combiner</b> types. Experiments are conducted on four different databases and results are examined. It is observed that we can obtain better accuracies with non-linear combinations for certain types...|$|E
30|$|Neural network (NN) {{approaches}} {{have been widely}} applied for modeling and identification of nonlinear multiple-input multiple-output (MIMO) systems. This paper proposes a stochastic analysis of a class of these NN algorithms. The class of MIMO systems considered in this {{paper is composed of}} a set of single-input nonlinearities followed by a <b>linear</b> <b>combiner.</b> The NN model consists of a set of single-input memoryless NN blocks followed by a <b>linear</b> <b>combiner.</b> A gradient descent algorithm is used for the learning process. Here we give analytical expressions for the mean squared error (MSE), explore the stationary points of the algorithm, evaluate the misadjustment error due to weight fluctuations, and derive recursions for the mean weight transient behavior during the learning process. The paper shows {{that in the case of}} independent inputs, the adaptive <b>linear</b> <b>combiner</b> identifies the linear combining matrix of the MIMO system (to within a scaling diagonal matrix) and that each NN block identifies the corresponding unknown nonlinearity to within a scale factor. The paper also investigates the particular case of linear identification of the nonlinear MIMO system. It is shown in this case that, for independent inputs, the adaptive <b>linear</b> <b>combiner</b> identifies a scaled version of the unknown linear combining matrix. The paper is supported with computer simulations which confirm the theoretical results.|$|E
40|$|For {{classifier}} ensembles, {{an effective}} combination {{method is to}} combine the outputs of each classifier using a linearly weighted combination rule. There are multiple ways to linearly combine classifier outputs and it is beneficial to analyze them as a whole. We present a unifying framework for multiple linear combination types in this paper. This unification enables using the same learning algorithms for different types of <b>linear</b> <b>combiners.</b> We present various ways to train the weights using regularized empirical loss minimization. We propose using the hinge loss for better performance {{as compared to the}} conventional least-squares loss. We analyze the effects of using hinge loss for various types of linear weight training by running experiments on three different databases. We show that, in certain problems, <b>linear</b> <b>combiners</b> with fewer parameters may perform as well as the ones with much larger number of parameters even in the presence of regularization...|$|R
40|$|In this paper, a {{theoretical}} and experimental {{analysis of the}} error-reject trade-off achievable by linearly combining the outputs of an ensemble of classifiers is presented. To this aim, the theoretical framework previously developed by Tumer and Ghosh {{for the analysis of}} the simple average rule without the reject option has been extended. Analytical results that allow to evaluate the improvement of the error-reject trade-off achievable by simple averaging their outputs under different assumptions about the distributions of the estimation errors affecting a posteriori probabilities, are provided. The conditions under which the weighted average can provide a better error-reject trade-off than the simple average are then determined. From the theoretical results obtained under the assumption of unbiased and uncorrelated estimation errors, simple guidelines for the design of multiple classifier systems using <b>linear</b> <b>combiners</b> are given. Finally, an experimental evaluation and comparison of the error-reject trade-off of the simple and weighted averages is reported for five real data sets. The results show the practical relevance of the proposed guidelines in the design of <b>linear</b> <b>combiners...</b>|$|R
40|$|We {{investigate}} the theoretical links between a regression ensemble and a linearly combined classification ensemble. First, we reformulate the Tumer & Ghosh model for <b>linear</b> <b>combiners</b> in a regression context; we then exploit this new formulation to generalise {{the concept of}} the “Ambiguity decomposition”, previously defined only for regression tasks, to classification problems. Finally, we propose a new algorithm, based on the Negative Correlation Learning framework, which applies to ensembles of linearly combined classifiers...|$|R
40|$|In {{this paper}} a new H type delay {{synchronisation}} and tracking approach is presented. It {{is based on}} the_ partitioning of the PN-code matrix of the CDMA users in order to form a <b>linear</b> <b>combiner.</b> This <b>linear</b> <b>combiner</b> {{may be used to}} find the noise subspace in order to synchronise the receiver code signal with the transmitted signal. The problem is reformulated as a state-space model and an H solution provides delay-_ tracking even in the presence of modelling errors such as over and underestimation of the number of signals present. The proposed novel formulation and approach provide a powerful near-far resistant solution for synchronisation and reception of DS-CDMA signals in a multiuser environment. ...|$|E
40|$|This paper {{describes}} {{the concept of}} adaptive <b>linear</b> <b>combiner</b> adds up the intermediate estimates at the output of each prediction stage to give a final estimate of the RLS–LMS predictor. In the RLS–LMS predictor, the first prediction stage is a simple first-order predictor with a fixed coefficient value 1. The second prediction stage uses the recursive least square algorithm to adaptively update the predictor coefficients. The subsequent prediction stages use the normalized least mean square algorithm to update the predictor coefficients. The coefficients of the <b>linear</b> <b>combiner</b> are then updated using the sign–sign least mean square algorithm. In chapter 1 an adaptive filter {{is defined as a}} self-designing system that relies for its operation on a recursive algorithm which perform satisfactorily in an environmen...|$|E
40|$|Abstract—In {{this paper}} we {{consider}} the issue of distributed adaptive estimation over sensor networks. To deal with more realistic scenario, different variance for observation noise is assumed for sensors in the network. To {{solve the problem of}} different variance of observation noise, the proposed method is divided into two phases: I) Estimating each sensor’s observation noise variance and II) using the estimated variances to obtain the desired parameter. Our proposed algorithm is based on a diffusion least mean square (LMS) implementation with <b>linear</b> <b>combiner</b> model. In the proposed algorithm, the step-size parameter the coefficients of <b>linear</b> <b>combiner</b> are adjusted according to estimated observation noise variances. As the simulation results show, the proposed algorithm considerably improves the diffusion LMS algorithm given in literature. Keywords—adaptive filter, distributed estimation, sensor network, diffusion. I...|$|E
40|$|In this paper, we {{continue}} the theoretical and experimental analysis of two widely used combining rules, namely, the simple and weighted average of classifier outputs, that we started in previous works. We analyse {{and compare the}} conditions which affect the performance improvement achievable by weighted average over simple average, and over individual classifiers, under the assumption of unbiased and uncorrelated estimation errors. Although our theoretical results have been obtained under strict assumptions, the reported experiments show {{that they can be}} useful in real applications, for designing multiple classifier systems based on <b>linear</b> <b>combiners...</b>|$|R
40|$|Several {{researchers}} have experimentally shown that substantial improvements {{can be obtained}} in difficult pattern recognition problems by combining or integrating the outputs of multiple classifiers. This chapter provides an analytical framework to quantify the improvements in classification results due to combining. The results apply to both <b>linear</b> <b>combiners</b> and order statistics combiners. We first show that to a first order approximation, the error rate obtained {{over and above the}} Bayes error rate, is directly proportional to the variance of the actual decision boundaries around the Bayes optimum boundary. Combining classifiers in output space reduces this variance, and hence reduces the "added" error. If N unbiased classifiers are combined by simple averaging, the added error rate can be reduced by a factor of N if the individual errors in approximating the decision boundaries are uncorrelated. Expressions are then derived for <b>linear</b> <b>combiners</b> which are biased or correlated, and the effect of output correlations on ensemble performance is quantified. For order statistics based non-linear combiners, we derive expressions that indicate how much the median, the maximum and in general the ith order statistic can improve classifier performance. The analysis presented here facilitates the understanding of the relationships among error rates, classifier boundary distributions, and combining in output space. Experimental results on several public domain data sets are provided to illustrate the benefits of combining and to support the analytical results. Comment: 31 page...|$|R
40|$|We {{investigate}} {{the performance of}} wideband massive MIMO base stations that use one-bit ADCs for quantizing the uplink signal. Ourmain result {{is to show that}} the many taps of the frequency-selective channel make <b>linear</b> <b>combiners</b> asymptotically consistent and the quantization noise additive and Gaussian, which simplifies signal processing and enables the straightforward use of OFDM. We also find that single-carrier systems and OFDM systems are affected in the same way by one-bit quantizers in wideband systems because the distribution of the quantization noise becomes the same in both systems as the number of channel taps grows...|$|R
40|$|In this paper, a novel {{approach}} for vibration based damage detection is proposed. The approach relies {{on the use of}} a large network of sensors (possibly hundreds of them) to which a programmable <b>linear</b> <b>combiner</b> is attached. The <b>linear</b> <b>combiner</b> is programmed to work as a modal filter. The frequency content of the output of the modal filter is proposed as feature for damage detection. It is shown that if a local damage is present, spurious peaks appear in the FRF of the modal filter whereas if temperature changes are considered, the FRF of the modal filter is shifted but its shape remains unchanged. The approach is interesting because of the ability to differentiate between local damage and global environmental changes to a structure. Issues about the practical implementation of the method are discussed. © 2005 Elsevier Ltd. All rights reserved. info:eu-repo/semantics/publishe...|$|E
40|$|Abstract — Surgical {{accuracy}} of the hand-held instruments depends on the active compensation of disturbance and tremor. Physiological tremor {{is one of the}} main causes for imprecision in micro-surgery procedures. One of the popular tremor compensation methods is based on weighted-frequency Fourier <b>linear</b> <b>combiner</b> (WFLC) algorithm, that can adapt to the changes in frequency as well as amplitude of the tremor signal. WLFC estimates the dominant frequency and the amplitude. For the case of tremor with frequency variation or comprising of two or three frequencies close in spectral domain, the WFLC performance is degraded. In this paper, we present a bandlimited multiple Fourier <b>linear</b> <b>combiner</b> that can track the modulated signals with multiple frequency components. We also discuss the tremor sensing with accelerometers. Using the proposed algorithm the drift caused by the accelerometers is also eliminated. The proposed filter is tested in real-time for 1 -DOF cancellation of tremor. I...|$|E
40|$|We {{have shown}} that duct {{modeling}} using the generalized RBF neural network (DM RBF), which has the capability of modeling the nonlinear behavior, can suppress a variable-frequency narrow band noise of a duct more efficiently than an FX-LMS algorithm. In our method (DM RBF), at first the duct is identified using a generalized RBF network, after that N stageoftimedelayofthe input signal to the N generalized RBF network is applied, then a <b>linear</b> <b>combiner</b> at their outputs makes an online identification of the nonlinear system. The weights of <b>linear</b> <b>combiner</b> are updated by the normalized LMS algorithm. We have showed that the proposed method is {{more than three times}} faster in comparison with the FX-LMS algorithm with 30 % lower error. Also the DM RBF method will converge in changing the input frequency, while it makes the FX-LMS cause divergence. Copyright © 2007 Hindawi Publishing Corporation. All rights reserved. 1...|$|E
40|$|This {{innovation}} {{represents a}} {{method by which}} single-to-multi-input, single-to-many-output system transfer functions can be estimated from input/output data sets. This innovation can be run in the background while a system is operating under other means (e. g., through human operator effort), or may be utilized offline using data sets created from observations of the estimated system. It utilizes a set of fuzzy membership functions spanning the input space for each input variable. <b>Linear</b> <b>combiners</b> associated with combinations of input membership functions are used to create the output(s) of the estimator. Coefficients are adjusted online {{through the use of}} learning algorithms...|$|R
40|$|Classifier {{ensembles}} {{have been}} one of the main topics of interest in the neural networks, machine learning and pattern recognition communities during the past fifteen years [21, 28, 16, 17, 26, 36, 27, 23, 11]. They are currently one of the state of the art techniques available for the design of classification systems and an effective option to the traditional approach based on the design of a single, monolithic classifier in many applications. Broadly speaking, two main choices have to be made in the design of a classifier ensemble: how to generate individual classifiers and how to combine them. Two main approaches have emerged to deal with these design steps: coverage optimisation, focused on generating an ensemble of classifiers as much complementary as possible, which are then fused with simple combining rules, and decision optimisation, focused on finding the most effective combining rule to exploit at best a given classifier ensemble [21]. One of the most studied and widely used combining rules, especially in the former approach, is the linear combination of classifier outputs. <b>Linear</b> <b>combiners</b> are often used for neural network ensembles, given that neural networks provide continuous outputs. The simplicity of <b>linear</b> <b>combiners</b> and their continuous nature favoured the development of analytical models for the analysis of the performance of ensembles of predictors, both for the case of regression problems and for the relatively more complex case of classification problems...|$|R
30|$|All MC {{simulations}} are run on <b>linear</b> <b>combiners</b> {{and thus}} satisfy the driving process assumptions A 1 a and A 1 b. The M taps {{of the system}} w to identify were selected randomly from N(0, 1), a fresh selection being made for each MC run. Simulations were run with filter length M= 10, 100, 400 and step-sizes μ= 1, 2, 5, 9, 12, 15, 18, 21, 24, 27, 29 × 1 /[15 M], thus ranging up to the largest possible stability bound 2 /M. The driving process had unit variance (m_x^(2)= 1) and the additive noise was Gaussian with variance 10 − 4.|$|R
40|$|International Telemetering Conference Proceedings / October 27 - 30, 1997 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe {{relative}} anti-jam (AJ) {{performance of}} several diversity combiners are investigated. The modulation is 8 -ary frequency-shift-keying (FSK), the demodulation process consists of energy detection {{of the eight}} frequency bins at each hop and the subsequent combining of detector outputs. Three combiners are considered : the <b>linear</b> <b>combiner,</b> where the detector outputs of each hop (corresponding to the same frequency bin) are summed without any processing; the self-normalized combiner, where the eight detector outputs of any particular hop are normalized so that they add to unity; and the max-normalized combiner, where the eight detector outputs of any hop are divided by the maximum value among those eight outputs. Results indicate that under worst-case tone jamming, the selfnormalized combiner performs the best, the max-normalized combiner second best, and the <b>linear</b> <b>combiner</b> performs the worst among the three...|$|E
40|$|The paper {{presents}} {{a new approach}} for the classification of transient disturbance waveforms in a power system by using a Fourier <b>linear</b> <b>combiner</b> and a fuzzy expert system. The measured voltage or current waveforms at a distribution bus are passed through a Fourier <b>linear</b> <b>combiner</b> block to provide peak or root mean square (RMS) amplitude and phase of the fundamental component at every sampling instant. The peak or RMS amplitude and computed slope of the waveforms are then passed on to a diagnostic module that computes the truth value of the signal combination and determines the class to which the waveform belongs. Computer simulated tests are carried out using emtp programs to obtain the disturbance waveform classification {{with the help of}} a new hybrid approach which is much simpler than the recently postulated neural network and wavelet based techniques. The classification is found to be robust and yields accurate results in most cases wit...|$|E
40|$|The {{performances}} of three receivers, namely, the hard-limited, the linear, and the maximum-likelihood combiners {{for the detection}} of frequency-hopped multilevel frequency-shift keyed signals transmitted from mobiles to base have been reported earlier [11]. Only the hard-limited combiner has been analyzed with respect to base-to-mobile link [1]. Here, we give new results on the performance of the likelihood and the linear combining receivers operating at the mobiles. Whereas it is possible to find exactly the union bound on the probability of hit error for a <b>linear</b> <b>combiner,</b> for a likelihood receiver, bounding and approximation techniques such as simple Chernoff bound and saddle point integration were employed. We also observe the asymptotic (SNR → ∞) equivalence of the hard-limited and the likelihood receivers. This, together with the approximate error estimates at finite SNR, leads us to believe that the likelihood receiver is only marginally superior to a hard-limited combiner. As expected, the <b>linear</b> <b>combiner</b> performs poorly...|$|E
40|$|Several {{researchers}} {{have shown that}} linearly combining outputs of multiple neural classifiers results in better performance for many applications. In this paper we introduce a family of order statistics combiners {{as an alternative to}} <b>linear</b> <b>combiners.</b> We show analytically that the selection of the median, the maximum and in general, the i th order statistic improves classification performance. Specifically, we show that order statistics combiners reduce the variance of the actual decision boundaries around the optimum boundary, and that this is directly related to classification error. 1 Introduction Training a parametric classifier involves the use of a training set of data with known labeling to estimate or "learn" the parameters of the chosen model. A test set, consisting of patterns not previously seen by the classifier, is then used to determine the classification performance, or generalization ability. The generalization ability of a classifier depends on the selection of t [...] ...|$|R
40|$|The {{performance}} of a single classifier is often inadequate in difficult classification problems. In such cases, several researchers have combined the outputs of multiple classifiers to obtain better performance. However, the amount of improvement possible through such combination techniques is generally not known. This article presents two approaches to estimating performance limits in hybrid networks. First, we present a framework that estimates Bayes error rates when <b>linear</b> <b>combiners</b> are used. Then we discuss a more general method that provides decision confidences and error bounds based on error types arising from the training data. The methods are illustrated for a difficult four class problem involving underwater acoustic data. For this data, we compute the single classifier and combiner classification performances, {{as well as the}} Bayes error rate and an error bound. INTRODUCTION In difficult classification problems with limited number of training data, high dime [...] ...|$|R
40|$|Exact {{expressions}} {{are obtained}} for the bit error rate (BER) for coherent and noncoherent decode and forward (DF) cooperative systems with upto three relays between the source and destination. The piecewise <b>linear</b> (PL) <b>combiner</b> is employed at the receiver. BER analysis is done using a contour integral approach for evaluating the Gil-Pelaez integral involving the characteristic function (CF) of the decision variable. This removes restrictions on relay location, imposed by the direct approach. Simulation results are provided to support the analysis and the relay diversity gain is demonstrated through BER plots...|$|R
