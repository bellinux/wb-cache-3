53|110|Public
5000|$|... #Caption: Error {{surface of}} a <b>linear</b> <b>neuron</b> with two input weights ...|$|E
5000|$|... #Caption: Error {{surface of}} a <b>linear</b> <b>neuron</b> for a single {{training}} case.|$|E
5000|$|... {{where the}} {{function}} [...] sets all matrix elements above the diagonal equal to 0, and note that our output [...] is a <b>linear</b> <b>neuron.</b>|$|E
40|$|The {{study of}} Articial Neural Networks {{started with the}} {{analysis}} of <b>linear</b> <b>neurons.</b> It was then discovered that networks consisting only of <b>linear</b> <b>neurons</b> cannot describe non-linear phenomena. As a result, most currently used neural networks consist of non-linear neurons. In this pa-per, we show that in many cases, <b>linear</b> <b>neurons</b> can still be successfully applied. This idea is illustrated by two examples: the PageRank algo-rithm underlying the successful Google search engine and the analysis of family happiness. 1 Linear Neural Networks: A Brief Reminder Neural networks. A general neural network consists of several neurons ex-changing signals. At each moment of time, for each neuron, we need finitely many numerical parameters to describe {{the current state of}} this neuron and the signals generated by this neuron. The state of the neuron at the next moment of time and the signals generated by the neuron at the next moment of time ar...|$|R
50|$|For {{biological}} networks, {{the effect}} of synaptic weights is {{not as simple as}} for <b>linear</b> <b>neurons</b> or Hebbian learning. However, biophysical models such as BCM theory have seen some success in mathematically describing these networks.|$|R
40|$|We {{consider}} discrete and recurrent {{neural network}} models with saturated <b>linear</b> <b>neurons.</b> A de nition of capacity is discussed and conditions that assure in nite capacity are established. The {{aim of this}} paper is to study networks with maximum capacity and to what extend maximal capacity relates to the network connecting weights. ...|$|R
5000|$|In a {{computational}} neural network, a vector {{or set of}} inputs [...] and outputs , or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights {{represented by}} the matrix , where for a <b>linear</b> <b>neuron</b> ...|$|E
5000|$|... or {{the change}} in the th {{synaptic}} weight [...] is equal to a learning rate [...] times the th input [...] times the postsynaptic response [...] Often cited is the case of a <b>linear</b> <b>neuron,</b> ...|$|E
5000|$|For small , our higher-order terms [...] go to zero. We again {{make the}} {{specification}} of a <b>linear</b> <b>neuron,</b> that is, {{the output of}} the neuron is equal to the sum of the product of each input and its synaptic weight, or ...|$|E
5000|$|An energy model, {{a kind of}} {{stimulus-response}} model, of binocular neurons {{allows for}} investigation behind the computational function these disparity tuned cells play {{in the creation of}} depth perception. [...] Energy models of binocular neurons involve the combination of monocular receptive fields that are either shifted in position or phase. [...] These shifts in either position or phase allow for the simulated binocular neurons to be sensitive to disparity. The relative contributions of phase and position shifts in simple and complex cells combine together in order to create depth perception of an object in 3-dimensional space. [...] Binocular simple cells are modeled as <b>linear</b> <b>neurons.</b> Due to the linear nature of these neurons, positive and negative values are encoded by two neurons where one neuron encodes the positive part and the other the negative part. This results in the neurons being complements of each other where the excitatory region of one binocular simple cell overlaps with the inhibitory region of another. Each neuron's response is limited such that only one may have a non-zero response for any time. This kind of limitation is called halfwave-rectifing. Binocular complex cells are modeled as energy neurons since they do not have discrete on and off regions in their receptive fields. Energy neurons sum the squared responses of two pairs of <b>linear</b> <b>neurons</b> which must be 90 degrees out of phase. [...] Alternatively, they can also be the sum the squared responses of four halfwave-rectified <b>linear</b> <b>neurons.</b>|$|R
40|$|It is {{proved that}} if we have <b>neurons</b> {{implementing}} arbitrary <b>linear</b> functions and a neuron implementing one (arbitrary but smooth) nonlinear function g(x), then for every continuous function f(x sub 1, [...] ., x sub m) of arbitrarily many variables, and for arbitrary e above 0, we can construct a network that consists of g-neurons and <b>linear</b> <b>neurons,</b> and computes f with precision e...|$|R
30|$|We {{frequently}} use <b>linear</b> integrate-and-fire <b>neurons</b> in this paper, since {{analysis is}} easiest for them. For greater biophysical realism, we also use simple (single-compartment) Hodgkinâ€“Huxley-like model neurons, {{for which we}} report numerical results, but no analysis. The theta neuron is in between: It is still simple enough {{for the sort of}} analysis that we are interested in here, but it is more realistic than the <b>linear</b> integrate-and-fire <b>neuron.</b>|$|R
50|$|ADALINE (Adaptive <b>Linear</b> <b>Neuron</b> {{or later}} Adaptive Linear Element) {{is an early}} {{single-layer}} artificial neural network {{and the name of}} the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch-Pitts neuron. It consists of a weight, a bias and a summation function.|$|E
50|$|In this case, {{the output}} unit {{is simply the}} {{weighted}} sum of its inputs plus a bias term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as harmonic analysis, and they can all be used in neural networks with this <b>linear</b> <b>neuron.</b> The bias term allows us to make affine transformations to the data.|$|E
50|$|In the 1960s Bernard Widrow and Ted Hoff {{developed}} ADALINE (Adaptive <b>Linear</b> <b>Neuron)</b> {{which used}} electrochemical cells called memistors (memory resistors) to emulate synapses of an artificial neuron. The memistors were implemented as 3-terminal devices operating {{based on the}} reversible electroplating of copper such that the resistance {{between two of the}} terminals is controlled by the integral of the current applied via the third terminal. The ADALINE circuitry was briefly commercialized by the Memistor Corporation in the 1960s enabling some applications in pattern recognition. However, since the memistors were not fabricated using integrated circuit fabrication techniques the technology was not scalable and was eventually abandoned as solid state electronics became mature.|$|E
40|$|In {{this paper}} a {{reconfigurable}} analog VLSI neural network architecture is presented. The analog architecture implements a Multi-Layer Perceptron whose topology can be programmed without any {{modification of the}} off-chip connections. The architecture is scaleable and modular since {{it is based on}} a single-chip configurable basic module. To obtain a robust behaviour with respect to noise and errors introduced in the computation by analog circuits, we use non-linear synapses and <b>linear</b> <b>neurons</b> as neural primitives...|$|R
30|$|We {{have shown}} {{that there is a}} natural {{analogous}} STDP learning for spiking neurons in our case of <b>linear</b> <b>neurons.</b> This asymmetric rule converges to a final connectivity which can be decomposed into symmetric and skew-symmetric parts. The first one is similar to the symmetric Hebbian learning case, emphasizing that the STDP is nothing more than an asymmetric Hebbian-like learning rule. The skew-symmetric part of the final connectivity is the cross-correlation between the inputs and their derivatives.|$|R
40|$|Abstract. Gaussian {{processes}} {{have been}} favourably compared to backpropagation neural networks {{as a tool}} for regression. We show that a recurrent neural network can implement exact Gaussian process inference using only <b>linear</b> <b>neurons</b> that integrate their inputs over time, inhibitory recurrent connections, and one-shot Hebbian learning. The network amounts to a dynamical system which relaxes to the correct solution. We prove conditions for convergence, show how the system can act as its own teacher in order to produce rapid predictions, and comment on the biological plausibility of such a network. ...|$|R
50|$|Besides simple Boolean {{functions}} with binary {{inputs and}} binary outputs, the GEP-nets algorithm can handle {{all kinds of}} functions or neurons (<b>linear</b> <b>neuron,</b> tanh neuron, atan neuron, logistic neuron, limit neuron, radial basis and triangular basis neurons, all kinds of step neurons, and so on). Also interesting is that the GEP-nets algorithm can use all these neurons together and let evolution decide which ones work best {{to solve the problem}} at hand. So, GEP-nets can be used not only in Boolean problems but also in logistic regression, classification, and regression. In all cases, GEP-nets can be implemented not only with multigenic systems but also cellular systems, both unicellular and multicellular. Furthermore, multinomial classification problems can also be tackled in one go by GEP-nets both with multigenic systems and multicellular systems.|$|E
40|$|Recently, van Elberg and van Ooyen (2009) {{published}} a generalization of the Event-Based Integration Scheme for an Integrate-and-Fire neuron model with exponentially decaying excitatory currents and double exponential inhibitory synaptic currents, introduced by Carnevale and Hines. In the paper, {{it was shown}} that the constraints on the synaptic time constants imposed by the Newton-Raphson iteration scheme, can be relaxed. In this note, we show that according to the results published in D'Haene et al. (2009), a further generalization is possible eliminating any constraint on the time constants. We also demonstrate that in fact {{a wide range of}} <b>linear</b> <b>neuron</b> models can be efficiently simulated with this computation scheme, including neuron models mimicking complex neuronal behavior. These results can change the way complex neuronal spiking behavior is modeled: instead of highly non <b>linear</b> <b>neuron</b> models with few state variables, it is possible to efficiently simulate linear models with a large number of state variables...|$|E
3000|$|For {{a single}} <b>linear</b> <b>neuron</b> that {{receives}} a stimulus pattern x=(x_ 1, [...]...,x_n [...]) with synaptic weights w=(w_ 1,...,w_n), the neuronal response is v=wÂ·x. The results we present {{in this section}} are specific to when n= 2 and when there are two patterns. In this case, the neuronal response is v = w_ 1 x_ 1 + w_ 2 x_ 2. In the next section, we explore a more general setting.|$|E
50|$|A {{fairly simple}} {{non-linear}} function, the sigmoid function {{such as the}} logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It was previously commonly seen in multilayer perceptrons. However, recent work has shown sigmoid neurons to be less effective than rectified <b>linear</b> <b>neurons.</b> The {{reason is that the}} gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons, making it difficult to optimize neural networks using multiple layers of sigmoidal neurons.|$|R
40|$|Part 7 : Intelligent Signal and Image ProcessingInternational audienceA linear Multi Layer Perceptron (MLP) is {{proposed}} {{as a new}} approach to identify the harmonic content of biomedical signals and to characterize them. This layered neural network uses only <b>linear</b> <b>neurons.</b> Some synthetic sinusoidal terms are used as inputs and represent a priori knowledge. A measured signal serves as a reference, then a supervised learning allows to adapt the weights and to fit its Fourier series. The amplitudes of the fundamental and high-order harmonics can be directly deduced from the combination of the weights. The effectiveness of the approach is evaluated and compared. Results show clearly that the linear MLP is able to identify in real-time the amplitudes of harmonic terms from measured signals such as electrocardiogram records under noisy conditions...|$|R
40|$|Abstraet [...] An edge {{detection}} algorithm using multi-state adaptive <b>linear</b> <b>neurons</b> (ADALINES) is presented. Although the tri-state ADALINE is only considered in this work, general multi-state input vectors with extreme values are {{shown to be}} linearly separable {{from the rest of}} the vectors with the same dimension. The input state of each ADALINE is defined using the local mean in a predefined mask. In addition to the binary input states + 1, the 0 input state is introduced for controlling the noise effect. If the input pattern matches one of the predefined edge patterns, the corresponding pixel is detected as an edge pixel. Experimental results are shown where the proposed detector is compared with both the Canny and LOG edge detectors. Edge detection Linear neural networks Pattern recognition 1...|$|R
40|$|We {{start out}} by demonstrating that an {{elementary}} learning task, {{corresponding to the}} training of a single <b>linear</b> <b>neuron</b> in a convolutional neural network, can be solved for feature spaces of very high dimensionality. In a second step, acknowledging that such high-dimensional learning tasks typically benefit from some form of regularization and arguing {{that the problem of}} scale has not been taken care of in a very satisfactory manner, we come to a combined resolution of both of these shortcomings by proposing a form of scale regularization. Moreover, using variational method, this regularization problem can also be solved rather efficiently and we demonstrate, on an artificial filter learning problem, the capabilities of our basic <b>linear</b> <b>neuron.</b> From a more general standpoint, we see this work as prime example of how learning and variational methods could, or even should work to their mutual benefit. Comment: Original submission to SSVM 2015 with a few minor corrections. Version with minor changes to appear in Proceedings of the BMVC 2017 as Supervised Scale-Regularized Linear Convolutionary Filters...|$|E
40|$|One of {{the most}} {{interesting}} domains of feedforward networks is the processing of sensor signals. There do exist some networks which extract most of the information by implementing the maximum entropy principle for Gaussian sources. This is done by transforming input patterns to the base of eigenvectors of the input autocorrelation matrix with the biggest eigenvalues. The basic building block of these networks is the <b>linear</b> <b>neuron,</b> learning with the Oja learning rule...|$|E
40|$|The {{speed control}} of the three phase {{induction}} motor is still a challenging problem. Although the results obtained {{by means of the}} conventional control are very good, many researches in this area are ongoing. The authors propose a different control approach based on artificial intelligence. The control signals for speed, torque and flux regulation are computed using three ADALINE (Adaptive <b>Linear</b> <b>Neuron)</b> neural networks. The numerical simulations are made in Simulink and the obtained results are compared with the conventional drive approach (cascaded PI controller...|$|E
40|$|High {{computational}} {{demand in}} solving the optimization {{problems associated with the}} model predictive control (MPC) schemes is a major obstacle when applying the methods to large-scale or fast-sampling systems. In this paper, we propose a new structured neural network approach to solving the quadratic programming problem in the constrained MPC. This new approach has the advantage of solving large-scale quadratic programming problems in a massively parallel fashion. The structured neural network consists of a projection network and a network for implementing the gradient projection algorithm. where the projection network is constructed from specially structured <b>linear</b> <b>neurons</b> with a special training algorithm. We prove that the training algorithm converges to the optimal solution. Finally, we test the method on a simplified paper machine model. (C) 2001 Elsevier Science Ltd. All rights reserved...|$|R
40|$|In {{this paper}} we {{evaluate}} {{and compare the}} performance of self-organizing neural networks applied {{to the task of}} image compression. The networks investigated are two-layered architectures with <b>linear</b> <b>neurons,</b> and variants of Hebbian learning rules are used to reduce the dimensionality of the inputs while preserving a maximum of information in the output units. Although in theory all networks considered are effectively equivalent to performing the Karhunen-Loeve transform, which is the optimal image compression method {{in the sense that it}} allows linear reconstruction of the input information with minimal squared error, the results obtained in practice reveal significant differences between the networks. An experimental study has been conducted to demonstrate these differences and thus some light is shed on the suitability of self-organizing neural networks for image compression, particularly in comparison to more conventional methods...|$|R
40|$|This {{research}} is to investigate {{the application of a}} power signal processor in power system classification and assess power quality of supply. This study specifically applies adaptive neural network approach consisting of <b>linear</b> adaptive <b>neurons</b> called Adaline for tracking simultaneous change in system frequency and harmonic content in power system signals...|$|R
40|$|For {{switched}} {{systems that}} switch between distinct globally stable equilibria, we offer closed-form formulas that lock oscillations in the required neighborhood of the equilibria. Motivated by non-spiking neuron models, {{the main focus}} of the paper is on the case of planar switched affine systems, where we use properties of nested cylinders coming from quadratic Lyapunov functions. In particular, for the first time ever, we use the dwell-time concept in order to give an explicit condition for non-spiking of <b>linear</b> <b>neuron</b> models with periodically switching current. An extension to the general nonlinear case is also given...|$|E
40|$|Abstract: This {{research}} {{investigated the}} use of predictive models and a low cost spectroscopy in non-invasive Soluble Solids Content (SSC) assessment. The challenge is to model complex and high-dimensional spectral data. Adaptive <b>Linear</b> <b>Neuron</b> (ADALINE) that uses a single layer network was proposed for modelling and variable selection in spectroscopic analysis. Findings show that ADALINE achieves the best performance and the proposed variable selection improved the accuracy of these predictive models. Particularly, this {{research shows that the}} SSC of pineapples can be non-destructively assessed using the three influential wavelengths of 718, 768, and 856 nm that identified by the proposed variable selection...|$|E
40|$|One of {{the most}} {{interesting}} domains of feedforward networks is the processing of sensor signals. There do exist some networks which extract most of the information by implementing the maximum entropy principle for Gaussian sources. This is done by transforming input patterns to the base of eigenvectors of the input autocorrelation matrix with the biggest eigenvalues. The basic building block of these networks is the <b>linear</b> <b>neuron,</b> learning with the Oja learning rule. Nevertheless, some researchers in pattern recognition theory claim that for pattern recognition and classification clustering transformations are needed which reduce the intra-class entropy. This leads to stable, reliable features and is implemented for Gaussian sources by a linear transformation using the eigenvectors with the smallest eigenvalues. In another paper (Brause 1992) it is shown that the basic building block for such a transformation can be implemented by a <b>linear</b> <b>neuron</b> using an Anti-Hebb rule and restricted weights. This paper shows the analog VLSI design for such a building block, using standard modules of multiplication and addition. The most tedious problem in this VLSI-application is the design of an analog vector normalization circuitry. It can be shown that the standard approaches of weight summation will not give the convergence to the eigenvectors for a proper feature transformation. To avoid this problem, our design differs significantly from the standard approaches by computing the real Euclidean norm. Keywords: minimum entropy, principal component analysis, VLSI, neural networks, surface approximation, cluster transformation, weight normalization circuit...|$|E
50|$|Depending on the {{specific}} model used they may be called a semi-linear unit, Nv <b>neuron,</b> binary <b>neuron,</b> <b>linear</b> threshold function, or McCulloch-Pitts (MCP) neuron.|$|R
40|$|International audienceEvent-driven {{strategies}} {{have been used}} to simulate spiking neural networks exactly. Previou work is limited to <b>linear</b> integrate-and-fire <b>neurons.</b> In this note we extend event driven schemes to a class of nonlinear integrate-and-fire models. Results are presented for the quadratic integrate-and-fire model with instantaneous or exponential synaptic currents. Extensions to conductance-based currents and exponential integrate-and-fire neurons are discussed...|$|R
40|$|Abstractâ€”This brief {{discusses}} a {{class of}} discrete-time recurrent neural networks with complex-valued <b>linear</b> threshold <b>neurons.</b> It addresses the boundedness, global attractivity, and complete stability of such networks. Some conditions for those properties are also derived. Examples and simulation results are used to illustrate the theory. Index Termsâ€”Complex-valued neural networks (NNs), discrete-time recurrent neural networks (RNNs), linear threshold (LT) ...|$|R
