2|11|Public
40|$|Program {{language}} structures, translation, loading, execution, {{and storage}} allocation; compilation of simple expressions and statements; organization of compiler including compile-time and run-time symbol tables, <b>lexical</b> <b>scan,</b> syntax scan, object code generation, error diagnostics, object code optimization techniques, and overall design; use of compiler writing languages and bootstrapping. Prerequisite According {{to the course}} listing, the prerequisites are: CSC 4101 or equivalent. Credit will not be given for both this course and CSC 7001. Office Hour...|$|E
40|$|SoWhat is an easy-to-learn, {{menu-driven}} {{program that}} allows for a new interactive approach to simulate dynamic systems. No cumbersome programming is necessary. Dynamic models {{are more or less}} entered into the program as they are written on paper. Syntactical errors are immediately recognized and located by the <b>Lexical</b> <b>Scan</b> feature. Endogenous variables are automatically detected, parameter values of the exogenous variables, initial values and simulation options are entered interactively. Simulations can be stopped and continued after a change of parameter values. A Policy Section allows for an interactive definition of logical constructions of system conditions and conditional changes of parameters and system equations. This feature provides a simple device for using the software for policy simulations, class room exercises, and computer based business games. Having read [SoWhat Quick Start] a novice in the theory of dynamical systems should be capable to enter and run, for example, the Solow Growth Model within 5 minutes. The software contains also an extensive help system {{that can be used for}} further reference. The software was developed and distributed by Bachmann and Strulik GbR aka Milestones Scientific Software. It is now freely available. ...|$|E
5000|$|Chapter 9 - <b>Lexical</b> <b>scanning</b> (also {{includes}} string {{search and}} data compression) ...|$|R
5000|$|Since the <b>lexical</b> <b>scanning</b> and {{syntactic}} parsing processing is combined, the resulting parser {{tends to be}} harder to understand and debug for more complex languages ...|$|R
40|$|This masters thesis {{describes}} a transformation process from PowerBuilder client/server applications to Java Three-Tier web applications. The transformation process operates on source code that progresses from PowerBuilder source code to Java source code in several phases. The rewriting techniques {{that are used}} include manual rewriting, <b>lexical</b> <b>scanning</b> and rewriting using XML and XSLT...|$|R
40|$|A {{method for}} quickly {{producing}} compilers for high level languages is described. The technique consists of feeding {{a description of}} the language to be translated to a general-purpose macro processor. Used in this way, the macro processor functions as a compiler-compiler, providing automatic parsing, <b>lexical</b> <b>scanning,</b> symbol table operations, and handling of syntactic errors. A complete syntactic and semantic description of a WHILE statement (except for Boolean expression processing) is given in only seven lines, as an example. A system programming language implemented by this method is discussed in order to illustrate the main ideas. The compiler produced for this language is compared to other compilers produced by conventional methods. Copyright © 1976 by The Institute of Electrical and Electronics Engineers, Inc...|$|R
40|$|Wide {{distribution}} of the computer algebra system Mathematica 1 has encouraged numerous programmers, researchers and educators to produce libraries of programs in its special language, for incorporation as " into Mathematica systems. Although some features of the language are quite interesting, some authors have found that for their purposes the Mathematica proprietary computer program has problematical and di cult-to-alter semantics. Therefore certain kinds of experiments and developments are necessarily inconvenient. An initial step in opening up such user-written libraries to re-use is an independent re-implementation of the language via a non-proprietary parser. In principle, this allows other implementations of semantics, as well as experiments in data representation, while still using the language basically {{as described in the}} Mathematica references. We describe a parser written in Common Lisp, a language which is appropriate for three reasons: (1) It is a standard and has wide distribution � (2) It supports numerous useful features including automatic storage allocation and garbage collection, arbitrary-precision integers, and tools for <b>lexical</b> <b>scanning</b> of languages � and (3) Lisp is the host language for several algebraic manipulation systems whose subroutines may be of some interest for programmers implementing alternative semantics...|$|R
40|$|<b>Lexical</b> {{analysis}} or <b>scanning</b> is {{the process}} where the stream of characters making up the source program is read from left-to-right and grouped into tokens. Tokens are sequences of characters with a collective meaning. There are usually {{only a small number}} of tokens for a programming language: constants (integer, double, char, string, etc.), operators (arithmetic, relational, logical), punctuation, and reserved words. while (i> 0) i = i- 2; source languag...|$|R
40|$|INTRODUCTION Whitespace plays several {{roles in}} {{programming}} languages: it separates other tokens, provides the programmer {{with a degree}} of control over the visual presentation of the source code, and in some languages even serves as a syntactic construct. Batch compilers and environments handle whitespace in a simple manner, discarding it as the <b>lexical</b> analyzer <b>scans</b> each region of the text. y Incremental environments, This research has been sponsored in part by the Advanced Research Projects Agency (ARPA) under Grant MDA 972 - 92 -J- 1028, and in part by NSF institutional infrastructure grant CDA- 8722788. The content of this paper does not necessarily reflect the position or policy of the U. S. Government. Authors' addresses: Tim A. Wagner, Reasoning, Inc., 700 E. El Camino, Suite # 300, Mountain View, CA 94040 and Susan L. Graham, 771 Soda Hall; Department of Electrical Engineering and Computer Science, Computer Science Division, University...|$|R
40|$|Query {{matching}} on XML streams {{is challenging}} work for querying efficiency when {{the amount of}} queried stream data is huge and the data can be streamed in continuously. In this paper, the method Syntactic Twig-Query Matching (STQM) is proposed to process queries on an XML stream and return the query results continuously and immediately. STQM matches twig queries on the XML stream in a syntactic manner by using a lexical analyzer and a parser, {{both of which are}} built from our lexical-rules and grammar-rules generators according to the user's queries and document schema, respectively. For query matching, the <b>lexical</b> analyzer <b>scans</b> the incoming XML stream and the parser recognizes XML structures for retrieving every twig-query result from the XML stream. Moreover, STQM obtains query results without a post-phase for excluding false positives, which are common in many streaming query methods. Through the experimental results, we found that STQM matches the twig query efficiently and also has good scalability both in the queried data size and the branch degree of the twig query. The proposed method takes less execution time than that of a sequence-based approach, which is widely accepted as a proper solution to the XML stream query. (C) 2011 Elsevier Inc. All rights reserved...|$|R
40|$|The lexical {{analyzer}} {{is the first}} phase of the compiler and commonly the most time consuming. The compilation of large programs is still far from optimized in today’s compilers. With modern processors moving more towards improving parallelization and multithreading, it has become impossible for performance gains in older compilersas technology advances. Any multicore architecture relies on improving parallelism than on improving single core performance. A compiler that is completely parallel and optimized is yet to be developed and would require significant effort to create. On careful analysis we find that the performance of a compiler is majorly affected by the <b>lexical</b> analyzer’s <b>scanning</b> and tokenizing phases. This effort is directed towards the creation of a completelyparallelized {{lexical analyzer}} designed to run on the Cell/B. E. processor that utilizes its multicore functionalities to achieve high performance gains in a compiler. Each SPE reads a block of data from the input and tokenizes them independently. To prevent dependence of SPE’s, a scheme for dynamically extending static block-limits isincorporated. Each SPE is given a range which it initially scans and then finalizes its input buffer to a set of complete tokens from the range dynamically. This ensures parallelization of the SPE’s independently and dynamically, with the PPE scheduling load for each SPE. The initially static assignment of the code blocks is made dynamic as soon as one SPE commits. This aids SPE load distribution and balancing. The PPE maintains the output buffer until all SPE’s of a single stage commit and move to the next stage before being written out to the file, to maintain order of execution. The approach can be extended easily to other multicore architectures as well. Tokenization is performed by high-speed string searching, with the keyword dictionary of the language, using Aho-Corasick algorithm...|$|R
40|$|The study {{examines}} {{the nature of}} eye movement control and word recognition during scanning for a specific topic, compared to reading for comprehension. Experimental trials included a manipulation of word frequency: the critical word was frequent (and orthographically familiar) or infrequent (two conditions: orthographically familiar, orthographically unfamiliar). First-pass reading times showed effects of word frequency for both reading and scanning, with no interactions between word characteristics and task. Therefore, {{in contrast to the}} task of searching for a single specific word (Rayner & Fischer, 1996), there are immediate and localised influences of <b>lexical</b> processing when <b>scanning</b> for a specific topic, indicating that early word recognition processes are similar during reading and topic scanning. In contrast, there were interactions for later measures, with larger effects of word frequency during reading than scanning, indicating that reading goals can modulate later processes such as the integration of words into sentence context. Additional analyses of the distribution of first-pass single fixation durations indicate that first-pass fixations of all durations were shortened during scanning compared to reading, and reading for comprehension produced a larger subset of longer first-pass fixations compared to scanning. Implications for the nature of word recognition and eye movement control are discussed. Peer-reviewedPost-prin...|$|R

