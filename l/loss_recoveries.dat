4|1031|Public
50|$|Transurance is an {{innovation}} at non-catastrophic levels akin to parametric insurance at catastrophic levels, according to K C Mishra, former Director of the National Insurance Academy, Pune, India. Although businesses spend huge amounts annually on {{property and casualty}} insurance premiums (in 2005-06 this amount may exceed 1500 billion dollars globally), insured <b>loss</b> <b>recoveries</b> are becoming a smaller portion of the economic costs of insured events. In effect, losses that are collateral to insurable events are becoming larger.|$|E
50|$|Transurance {{eliminates}} complex coverage {{definitions and}} loss adjustment processes by defining its coverage {{as a percentage}} of the <b>loss</b> <b>recoveries</b> under selected traditional insurance policies. This approach enables the insured to express its view of the relationship between insurable losses and collateral losses as a proportion of the underlying loss recovery.In short, Transurance makes uninsurable losses insurable, in a way that is effective and efficient, and helps insureds deal with the full impact of loss events. By supplementing insurance with Transurance, insurance recoveries will more closely resemble the total economic loss of insured events. Transurance may be substituted for ambiguous policy wording in traditional insurance so as to eliminate coverage disputes.|$|E
40|$|Learn about {{mortgage}} loans and mortgage backed securities. ◮ Build a borrower behavior model including prepayments, defaults, and <b>loss</b> <b>recoveries.</b> ◮ Build a mortgage security cash flow waterfall tool ◮ Calibrate the a HJM model and simulate interest rate term structures ◮ Derive the implied future home price growt...|$|E
30|$|The <b>loss</b> <b>recovery</b> {{ability of}} the current <b>loss</b> <b>recovery</b> {{algorithms}} varies {{with the number of}} packet losses in a window, leading to frequent RTO expiration and lower throughput. We show the weaknesses of the <b>loss</b> <b>recovery</b> algorithms in detail with the simulation results.|$|R
30|$|We {{proposed}} a new <b>loss</b> <b>recovery</b> architecture for TCP and validated its effectiveness through extensive simulation and emulation experiments. The new architecture maintains packet transmission order so that <b>loss</b> <b>recovery</b> is more effective. This {{feature is the}} greatest difference from the conventional <b>loss</b> <b>recovery</b> in TCP whose <b>loss</b> <b>recovery</b> ability is seriously affected {{by the number of}} lost packets and frequently causes unnecessary RTO expiry. Our method sustains the <b>loss</b> <b>recovery</b> ability in TCP irrespective of the number of lost packets. We combined the new <b>loss</b> <b>recovery</b> algorithm with various TCP congestion control algorithms such as AIMD, TCP-Westwood, and TCP-RR and demonstrated higher throughput for higher loss rates. We also demonstrated that the {{proposed a}}lgorithm can be used for video transmission over a lossy channel. We will extend our TCP for multimedia transmission based on overlay multicast in which TCP sessions are used either to construct a logical multicast tree or to deliver multimedia streams.|$|R
40|$|Optical burst {{switching}} {{is one of}} {{the most}} promising next-generation all-optical data transport paradigms. In this paper, we discuss forward error correction as a candidate for providing <b>loss</b> <b>recovery</b> in an optical burstswitched network. We develop a network-level analytical model to evaluate the packet loss probability of forward error correction <b>loss</b> <b>recovery</b> mechanism. We also develop a simulation model to investigate the proposed forward error correction <b>loss</b> <b>recovery</b> mechanism and to compare the performance of our proposed mechanism with the existing burst retransmission <b>loss</b> <b>recovery</b> mechanism. Our results show that the proposed mechanism significantly reduces the packet loss in an optical burst-switched network...|$|R
40|$|We {{present a}} new {{approach}} to analyse historical recovery rates on distressed bank assets. Our approach uses banks’ reported impaired assets and the corresponding specific provisions. The dynamics and drivers of this credit loss recovery proxy are studied for a comprehensive sample of Australian banks from 1989 to 2005. We find that macroeconomic and bank-specific factors influence banks’ estimates of loan <b>loss</b> <b>recoveries,</b> consistent with banks smoothing their earnings. In contrast with findings based on prices of distressed corporate bonds, banks record lower recoveries in years of strong economic growth. banking; credit risk; loan loss recoveries; loss given default; Australia...|$|E
40|$|Local <b>loss</b> <b>recovery</b> for {{reliable}} multicast {{can provide}} significant performance improvement {{in terms of}} <b>loss</b> <b>recovery</b> latency, bandwidth consumption and network throughput. In this paper, an analytical model for studying the optimal cache allocation for ARM is constructed. Using standard optimization techniques, the optimal cache allocation pattern can be found. Our numerical results show that using optimal cache allocation yields significantly smaller <b>loss</b> <b>recovery</b> latency than using uniform cache allocation. To further enhance the <b>loss</b> <b>recovery</b> performance when the amount of cache at an active router is limited, we propose a probabilistic caching policy. We derive the optimal caching probabilities for each active router in a given multicast tree with a given cache allocation pattern. We show that {{with the use of}} probabilistic caching policy, a further reduction in <b>loss</b> <b>recovery</b> latency can be obtained. link_to_subscribed_fulltex...|$|R
30|$|The {{proposed}} new <b>loss</b> <b>recovery</b> algorithm consistently recovers lost packets {{at a higher}} packet loss rate. We also explained the reason to use packet transmission order to address the aforementioned problem of the current <b>loss</b> <b>recovery</b> algorithms.|$|R
40|$|Local <b>loss</b> <b>recovery</b> for {{reliable}} multicast {{can provide}} significant performance improvement {{in terms of}} <b>loss</b> <b>recovery</b> latency, bandwidth consumption and network throughput. Active reliable multicast (ARM) is a novel <b>loss</b> <b>recovery</b> scheme for large-scale reliable multicast. When sufficient active resources of active routers (ARs) and associated cache are available, ARM gives good <b>loss</b> <b>recovery</b> latency performance. The authors build an analytical model for studying the optimal active resource allocation for ARM. They propose a top-down layering analysis (TDLA) approach to derive the expected <b>loss</b> <b>recovery</b> latency (ELRL). For a given multicast tree, the ELRL is derived {{as a function of}} the active resource allocation pattern. Simulation experiments reveal that high performance gains can be attained with limited active resources, which are properly allocated. It is also shown that simulation results have a good match with the analytical results. The analytical framework developed can be used for further investigation of various active resource allocation schemes and caching policies...|$|R
40|$|This thesis {{presents}} a new cell <b>loss</b> <b>recovery</b> scheme for ATM networks using an interleaved burst-error-correcting convolutional code. In particular, the proposed cell <b>loss</b> <b>recovery</b> scheme implements interleaved Berlekamp-Preparata(BP) code at the convergence sublayer(CS) of ATM adaptation layer(AAL) {{to compensate for}} cell loss...|$|R
30|$|Even with {{numerous}} proposals, limited {{attention has been}} paid to the <b>loss</b> <b>recovery</b> algorithm or architecture of TCP. Most work has focused on congestion control rather than <b>loss</b> <b>recovery.</b> Even SACK or FACK implementations, though they enhance <b>loss</b> <b>recovery,</b> maintain the single linked list architecture. We previously proposed a new TCP architecture based on two lists in order to improve the TCP loss performance [17, 18]. We implemented the new idea in a linux setting and performed an evaluation of the method.|$|R
30|$|The rest of {{of article}} is {{organized}} as follows. Section 2 proposes new <b>loss</b> <b>recovery</b> architecture and algorithm for wireless TCP. It analyzes {{the limitations of}} current <b>loss</b> <b>recovery</b> algorithms and shows why packet transmission order {{should be used to}} improve the <b>loss</b> <b>recovery</b> ability. Section 3 validates the proposed idea with various simulation results. Section 4 describes the Linux implementation of the proposed idea and Section 5 shows experiment results. Section 6 finalizes this paper with conclusions and future research direction.|$|R
40|$|Local <b>loss</b> <b>recovery</b> for {{reliable}} multicast {{can provide}} significant performance improvement {{in terms of}} <b>loss</b> <b>recovery</b> latency, bandwidth consumption and network throughput. Active reliable multicast (ARM) is a novel <b>loss</b> <b>recovery</b> scheme for large-scale reliable multicast. In ARM, local <b>loss</b> <b>recovery</b> is realised by retrieving repairs at near active routers. In practice, active resources are limited and active resources allocation strategies are required for optimising performance {{in terms of the}} <b>loss</b> <b>recovery</b> and/or network bandwidth consumption. An active router may need to support the local <b>loss</b> <b>recovery</b> for multiple multicast sessions simultaneously. How to partition efficiently the cache resource to these sessions is crucial to the performance of local <b>loss</b> <b>recovery.</b> In the paper, the authors propose a cache partitioning method called 'layered greedy cache partitioning' to deal with this dilemma. For this method, the active routers are first grouped into different layers according to the tree topologies and the design begins from the lowest layer. The cache partitioning of an upper layer can be performed separately by making use of relevant design information of the lower layer. An optimisation for cache partitioning at each layer can then be conducted and a 'local' optimal solution can be achieved. On a global scale, however, the solution is only suboptimal. The performance of using the proposed cache partitioning method is compared with that using uniform cache partitioning and proportional partitioning. A significant performance improvement is found. link_to_subscribed_fulltex...|$|R
40|$|Interest {{aggregation}} is {{the primary}} feature of named-data networking (NDN). The interest aggregation reduces dupli-cated transmissions for multiple requests on the same data, but can delay <b>loss</b> <b>recovery</b> when data packets are lost. Since retransmitted interest packets for <b>loss</b> <b>recovery</b> cannot be distinguished from regular interest packets, the lost data packets can be recovered after the timeout of the interest aggregation. The delayed <b>loss</b> <b>recovery</b> can largely increase flow completion time, particularly on small content down-loads. In this paper, we propose an active interest manage-ment (AIM) scheme based on random early NACK to avoid the delayed <b>loss</b> <b>recovery</b> as well as keeping the benefit of interest aggregation. Using NS- 3, we show that our scheme significantly reduced flow completion time of both small and large content...|$|R
40|$|This memo defines an Experimental Protocol for the Internet community. It {{does not}} specify an Internet {{standard}} of any kind. Discussion {{and suggestions for}} improvement are requested. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2003). All Rights Reserved. The Eifel detection algorithm allows a TCP sender to detect a posteriori whether it has entered <b>loss</b> <b>recovery</b> unnecessarily. It requires that the TCP Timestamps option defined in RFC 1323 be enabled for a connection. The Eifel detection algorithm makes use {{of the fact that}} the TCP Timestamps option eliminates the retransmission ambiguity in TCP. Based on the timestamp of the first acceptable ACK that arrives during <b>loss</b> <b>recovery,</b> it decides whether <b>loss</b> <b>recovery</b> was entered unnecessarily. The Eifel detection algorithm provides a basis for future TCP enhancements. This includes response algorithms to back out of <b>loss</b> <b>recovery</b> by restoring a TCP sender’s congestion control state...|$|R
30|$|LE under AIMD {{exhibits}} {{the same}} throughput curve {{as do the}} other <b>loss</b> <b>recovery</b> algorithms. We replaced the <b>loss</b> <b>recovery</b> part of TCP-Reno with our algorithm. To our surprise, when typical AIMD control of TCP was used, the throughputs of different TCP implementations were very similar. LE performed slightly better, but the impact was marginal, as shown in [[22], Figure 6 a].|$|R
30|$|We {{compared}} {{the impacts of}} the <b>loss</b> <b>recovery</b> algorithms with the congestion control algorithms, additive increase multiplicative decrease (AIMD), fixed window, and that of Westwood. Congestion control referred to the way cwnd was set by TCP. In the case of AIMD, traditional Reno type congestion window control was used. Fixed window control does not change its window size; this algorithm was introduced to determine the impact of <b>loss</b> <b>recovery</b> {{in the absence of}} the congestion control algorithm. The Westwood congestion control utilizes available bandwidth information for setting cwnd values. We compared five different <b>loss</b> <b>recovery</b> algorithms: Reno, newReno, SACK, FACK and LE. The <b>loss</b> <b>recovery</b> of Reno and that of newReno are similar, but newReno improves retransmission during the fast recovery phase. SACK is an improvement over Reno and newReno due to utilizing of the selective ACK option. FACK aims to trigger faster retransmission by utilizing additional variables compared to those of SACK.|$|R
40|$|Reliable {{multicast}} protocols design {{takes in}} consideration several criteria aiming to overcome multicast communication {{problems such as}} the control packet implosion at the source, the retransmission scoping, the efficient distribution of <b>loss</b> <b>recovery</b> burden, the reduction of <b>loss</b> <b>recovery</b> latency {{as well as the}} congestion control. The appearance of active networks opened new perspectives for the development of multicast protocols based on active services. In comparison with the end to end approach, where the <b>loss</b> <b>recovery</b> is exclusively provided by the source and the receivers, the active services allows the attribution of <b>loss</b> <b>recovery</b> task to the active routers that are near the point of loss. This mode of recovery is called local recovery. In this paper we propose a reliable multicast transport protocol based on active services. The proposed protocol adopts an approach combining the use of positive and negative acknowledgments. The approach will be highlighted along this paper by its comparison with reliable multicast protocols based on active services...|$|R
5000|$|PRR is {{incorporated}} in Linux kernels to improve <b>loss</b> <b>recovery</b> since version 3.2. (January 2012) ...|$|R
30|$|The gain of LE {{increases}} as cwnd becomes larger. To remove {{the impact of}} congestion control on <b>loss</b> <b>recovery,</b> the congestion window was set to 32 packets and the test was repeated. The results of this simulation are shown in Figure 6 b. The throughput of LE was superior {{to those of other}} <b>loss</b> <b>recovery</b> algorithms, demonstrating the effectiveness of LE's <b>loss</b> <b>recovery</b> algorithm. The throughputs of Reno, newReno, SACK, and FACK were inferior to that of LE because they frequently experience RTO expiry, during which they cannot send packets. As LE can quickly detect the loss of retransmitted packets or tolerate insufficient duplicate ACKs, LE does not experience RTO expiry and therefore saves time, maintaining the transmission rate.|$|R
30|$|Though the {{architecture}} and the algorithm {{were presented in}} our conference papers [17, 18], we summarize them here for completeness. We named our proposal LE, an abbreviation of L oss rE silience. The main design goal of LE is to achieve <b>loss</b> <b>recovery</b> ability resilient to ACK starvation. To realize the goal, TCP needs to maintain the correct number of pending packets in networks even during <b>loss</b> <b>recovery.</b>|$|R
40|$|Recent {{advances}} in high-speed mobile networks have revealed new bottlenecks in ubiquitous TCP protocol {{deployed in the}} Internet. In addition to differentiating non-congestive loss from congestive loss, our experiments revealed two significant performance bottlenecks during the <b>loss</b> <b>recovery</b> phase: flow control bottleneck and application stall, resulting in degradation in QoS performance. To tackle these two problems we firstly develop a novel opportunistic retransmission algorithm to eliminate the flow control bottleneck, which enables TCP sender to transmit new packets even if receiver's receiving window is exhausted. Secondly, application stall can be significantly alleviated by carefully monitoring and tuning the TCP sending buffer growth mechanism. We implemented and modularized the proposed algorithms in the Linux kernel thus they can plug-and-play with the existing TCP <b>loss</b> <b>recovery</b> algorithms easily. Using emulated experiments we showed that, compared to the existing TCP <b>loss</b> <b>recovery</b> algorithms, the proposed optimization algorithms improve the bandwidth efficiency by up to 133 % and completely mitigate RTT spikes, i. e., over 50 % RTT reduction, over the <b>loss</b> <b>recovery</b> phase...|$|R
30|$|SACK and FACK cannot {{recover the}} loss of retransmitted packets until timer-expiration. The {{proposed}} method can handle heavy losses including loss of retransmitted packets by keeping the transmission order information in WAITLIST, {{a natural extension of}} [19]. Existing <b>loss</b> <b>recovery</b> algorithms limit the scope of packets for recovery to those that have a sequence number between lastAcked and lastSent. After recovering the packets up to lastSent, SACK returns to normal phase. Therefore, when packets after lastSent are lost, SACK reenters the <b>loss</b> <b>recovery</b> phase. Considering the cost of each transition, freezing the transmission for the third duplicate ACKs, and re-initializing the scoreboard data structure, scope limitation can cause inefficiency. By not limiting the scope, our method can be considered as a constant <b>loss</b> <b>recovery</b> mechanism.|$|R
30|$|RAF-rtx {{is devoted}} to helping the SCTP sender {{possibly}} detect packet loss timely and trigger fast retransmission for <b>loss</b> <b>recovery.</b>|$|R
30|$|By {{retaining}} the acket transmission order, LE measures {{the exact number}} of lost packets upon a single duplicate ACK, making it possible to strictly extend the self clocking property. In other words, LE transmits the number of packets equal to that exiting networks even during <b>loss</b> <b>recovery.</b> Therefore, even when there are not enough duplicate ACKs to transmit lost packets during <b>loss</b> <b>recovery,</b> LE does not incur RTO expiry and successfully recovers lost packets.|$|R
40|$|SUMMARY It {{has been}} a very {{important}} issue to evaluate the performance of transmission control protocol (TCP), and the importance is still growing up because TCP will be deployed more widely in future wireless as well as wireline networks. It is also the reason why {{there have been a lot}} of efforts to analyze TCP performance more accurately. Most of these works are focusing on overall TCP end-to-end throughput that is defined as the number of bytes transmitted for a given time period. Even though each TCP’s fast recovery strategy should be considered in computation of the exact time period, it has not been considered sufficiently in the existing models. That is, for more detailed performance analysis of a TCP implementation, the fast recovery latency during which lost packets are retransmitted should be considered with its relevant strategy. In this paper, we extend the existing models in order to capture TCP’s <b>loss</b> <b>recovery</b> behaviors in detail. On the basis of the model, the <b>loss</b> <b>recovery</b> latency of three TCP implementations can be derived with considering the number of retransmitted packets. In particular, the proposed model differentiates the <b>loss</b> <b>recovery</b> performance of TCP using selective acknowledgement (SACK) option from TCP NewReno. We also verify that the proposed model reflects the precise latency of each TCP’s <b>loss</b> <b>recovery</b> by simulations. key words: transmission control protocol (TCP), congestion control, <b>loss</b> <b>recovery,</b> performance modeling and validation, fast recovery latency 1...|$|R
40|$|Providing {{reliable}} multicast {{service is}} very challenging in Ad Hoc networks. In this paper, we propose an efficient <b>loss</b> <b>recovery</b> scheme for reliable multicast (CoreRM). Our basic {{idea is to}} apply the notion of cooperative communications to support local <b>loss</b> <b>recovery</b> in multicast. A receiver node experiencing a packet loss tries to recover the lost packet through progressively cooperating with neighboring nodes, upstream nodes or even source node. In order to reduce recovery latency and retransmission overhead, CoreRM caches not only data packets but also the path {{which could be used}} for future possible use to expedite the <b>loss</b> <b>recovery</b> process. Both analytical and simulation results reveal that CoreRM significantly improves the reliable multicast performance in terms of delivery ratio, throughput and recovery latency compared with UDP and PGM...|$|R
30|$|An {{advantage}} of the separate RTXLIST is the decoupling transmission decision from the <b>loss</b> <b>recovery.</b> Note that in our architecture, <b>loss</b> <b>recovery</b> is not affected by transmission of lost packets, and its role ends after moving packets to RTXLIST. The transmission is handled by congestion control. Nor does the transmitter depend on whether the packet is new or a retransmitted one except that it gives priority to those in RTXLIST. This decoupling simplifies the source code.|$|R
30|$|Often, {{proposed}} ML-based networking {{solutions are}} assessed and evaluated against existing non-ML frameworks. These latter act as baseline {{and are used}} to demonstrate the benefits, if any, of using ML. Unfortunately, these baseline solutions are often deprecated and outdated. For instance, ML-based congestion control mechanisms are often compared against default TCP implementations, e.g. CTCP, CUBIC, or BIC with typical <b>loss</b> <b>recovery</b> mechanisms, such as Reno, NewReno, or SACK. However, Yang et al. [486] applied supervised learning techniques to identify the precise TCP protocol used in Web traffic and uncovered that though majority of the servers employ the default, {{there is a small}} amount of web traffic that employs non-default TCP implementation for congestion control and <b>loss</b> <b>recovery.</b> Therefore, it is critical to consider TCP variants as comparison baselines that have taken the lead, and are prominently employed for congestion control and <b>loss</b> <b>recovery.</b>|$|R
40|$|Many {{multicast}} applications require reliable {{delivery of}} data packets to multiple receivers. Scalability {{is one of}} the key challenges in the design of reliable multicast. The major obstacles of the scalability are feedback implosion and retransmissions. Furthermore, a real network changes with time. A reliable multicast protocol must adapt to such dynamic change of multicast sessions. Thus, it is necessary to design an efficient and adaptive <b>loss</b> <b>recovery</b> scheme for reliable multicast. In this thesis, we present an efficient and adaptive <b>loss</b> <b>recovery</b> scheme, which is based on the performance evaluation of reliable multicast. The multicast performance depends on the <b>loss</b> <b>recovery</b> mechanism, the underlying tree topology, the loss characteristics, and the locations of repair servers. We present an efficient performance evaluation of these basic performance parameters, which is useful for adequate determination of the locations of repair server...|$|R
40|$|Although {{the rate}} of repair traffic is {{generally}} well controlled in unicast, {{little attention has been}} paid to the rate control over repair traffic in the current literature of distributed multicast <b>loss</b> <b>recovery.</b> In this paper we show that rate control over repair traffic in multicast is necessary for traffic stability and congestion alleviation. We also propose a general method for controlling {{the rate of}} repair traffic in multicast, which can be used to enhance any of the existing distributed multicast <b>loss</b> <b>recovery</b> schemes...|$|R
40|$|This paper {{presents}} a novel <b>loss</b> <b>recovery</b> scheme, Active Reliable Multicast (ARM), for large-scale reliable multicast. ARM is "active" in that routers in the multicast tree {{play an active}} role in <b>loss</b> <b>recovery.</b> Additionally, ARM utilizes soft-state storage within the network to improve performance and scalability. In the upstream direction, routers suppress duplicate NACKs from multiple receivers to control the implosion problem. By suppressing duplicate NACKs, ARM also lessens the traffic that propagates back through the network. In the downstream direction, routers limit the delivery of repair packets to receivers experiencing loss, thereby reducing network bandwidth consumption. Finally, to reduce wide-area recovery latency and to distribute the retransmission load, routers cache multicast data on a "best-effort" basis. ARM is flexible and robust in that it does not require all nodes to be active, nor does it require any specific router or receiver to perform <b>loss</b> <b>recovery.</b> Analysis [...] ...|$|R
30|$|Though the new <b>loss</b> <b>recovery</b> {{algorithm}} achieved {{better performance}} with TCP-Westwood, the performance improvement {{was not as}} significant as that in the simulation of Figure 6 c.|$|R
40|$|In this Article, Professor Lois R. Lupica {{examines}} {{whether the}} electric utility industry, currently j. n {{the midst of}} deregulation, ought to sustain the resulting transition losses. Due to the signifi· cant modification of legal rules affecting the electric power market and changes in regulatory policy, the utilities currently have expenditures and expectations that are unrecoverable in a competitive market. In recent years, momentum has moved {{in the direction of}} compensating the electric utilities and their investors for these losses. Professor Lupica challenges the arguments for transition loBS recovery and ultimately concludes that the doctrinal premises in support oftransition <b>loss</b> <b>recovery</b> are flawed. The Article begins by examining the history of the electric power market and continues by addressing the central arguments in favor oftransition <b>loss</b> <b>recovery.</b> Proponents oftransition <b>loss</b> <b>recovery</b> argue that investors will suffer losses as a result ofa change in market dynamics or legal rules, and because the changes were not foreseeable, investors should be insulated from these resulting losses. Advocates of transition <b>loss</b> <b>recovery</b> further perceive the regulatory environment as contract-based, and thus argue that the modification ofthe market 2 ̆ 7 s legal rules constitutes a breach ofcontract. Finally, some advocates claim that changes in legal rules and the resulting transition losses is a taking of property under the Fifth Amendment. Professor Lupica addresses each of these arguments and contends that the premises underlying these arguments are faulty. She further argues that transition losses are not unique to this context, and that, in addition to acknowledging the doctrinal challenges to recovery advocates 2 ̆ 7 arguments, policy makers must evaluate transition <b>loss</b> <b>recovery</b> as an issue of fundamental fairness to utility consumers...|$|R
3000|$|... + {{possible}} to identify the lost packets timely and enable <b>loss</b> <b>recovery</b> rapidly in a lossy wireless environment, by detecting the jitter indicator of the retransmission queue length.|$|R
