10000|10000|Public
5|$|Three {{aspects of}} {{communication}} patterns are of clinical interest: poor prosody, tangential and circumstantial speech, and marked verbosity. Although inflection and intonation {{may be less}} rigid or monotonic than in classic autism, people with AS often have a limited range of intonation: speech may be unusually fast, jerky or loud. Speech may {{convey a sense of}} incoherence; the conversational style often includes monologues about topics that bore the <b>listener,</b> fails to provide context for comments, or fails to suppress internal thoughts. Individuals with AS may fail to detect whether the <b>listener</b> is interested or engaged in the conversation. The speaker's conclusion or point may never be made, and attempts by the <b>listener</b> to elaborate on the speech's content or logic, or to shift to related topics, are often unsuccessful.|$|E
5|$|Conversely, Gallagher {{was voted}} the most overrated guitarist {{of the last}} {{millennium}} in a 1999 poll of fellow players, and the ninth-most overrated ever in a 2002 <b>listener</b> survey â€“ he cited the former as the award he most enjoyed receiving.|$|E
5|$|Further {{to these}} affixes, verbs may be marked {{for the number}} of the subject, be it dual or plural, and also for clusivity; whether the <b>listener</b> is {{included}} in the described event (inclusive) or is excluded from the event (exclusive).|$|E
5000|$|It {{is assumed}} that {{preferences}} are similar for expert <b>listeners</b> and naive <b>listeners</b> and thus results of expert <b>listeners</b> are also predictive for consumers. In agreement with this assumption Schinkel-Bielefeld et al. [...] found no differences in the rank order between expert <b>listeners</b> and untrained <b>listeners</b> when using test signals containing only timbre and no spatial artifacts. However, Rumsey et al. [...] showed that for signals containing spatial artifacts, expert <b>listeners</b> weigh spatial artifacts slightly stronger than untrained <b>listeners,</b> who primarily focus on timbre artifacts.|$|R
50|$|As media currency, the {{so-called}} <b>listeners</b> per hour (for pre-determined hours) is reported {{as well as}} the average <b>listeners</b> per hour and the <b>listeners</b> per day.|$|R
50|$|<b>Listeners</b> to Audio-Reader's service must {{be unable}} to use normal printed materials. In most cases, <b>listeners</b> are {{partially}} or completely blind, but some <b>listeners</b> have other disabilities, such as dyslexia or allergies to newsprint. A number of Audio-Reader <b>listeners</b> are elderly and have the condition macular degeneration.|$|R
5|$|This is a concise work, {{in terms}} of its duration, the economy of {{thematic}} material presented, and also the narrow registers within which the four parts operate. The first movement is in 6/8 time and a monothematic sonata form. The development and recapitulation sections feature an example of Haydn's musical jokes. In this case he tinkers with the movement's sonata form by reference to an historical variation of it. The recapitulation starts only with a statement of the second phrase of the movement's theme, which is in the dominant. This would have been a common technique earlier in the eighteenth century that, in this instance, is liable to confuse the unknowing <b>listener</b> looking for the statement of the first phrase of the theme in the tonic. It is not until 27 measures later that the <b>listener</b> is presented with a more emphatic reprise, which is actually a coda. The coda involves the statement of the main theme in the tonic that the <b>listener</b> might have been expecting, and it does so after two measures of pointed silence.|$|E
5|$|In simplistic terms, the NS-10 {{possesses}} sonic {{characteristics that}} allow record producers {{to assume that}} if a recording sounds good on these monitors, then it should sound good on most playback systems. Whilst it can reveal any shortcomings in the recording mix {{as well as the}} monitoring chain, it may lead to <b>listener</b> fatigue with prolonged use in the domestic setting.|$|E
5|$|Predators {{such as the}} sarcophagid fly Emblemasoma hunt cicadas by sound, being {{attracted}} to their song. Singing males soften their song so that {{the attention of the}} <b>listener</b> gets distracted to neighbouring louder singers, or cease singing altogether as a predator approaches. It has been asserted that loud cicada song, especially in chorus, repels predators, but observations of predator responses refute the claim.|$|E
50|$|The {{number of}} radio <b>listeners</b> are {{decreasing}} {{and are being}} slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost {{have a lot of}} radio <b>listeners.</b> Kabul and Jalalabad have moderate number of <b>listeners.</b> However, Mazar-e-Sharif and especially Herat have very few radio <b>listeners.</b>|$|R
50|$|AppleMIDI {{implementation}} defines two kind of session controllers: session initiators and session <b>listeners.</b> Session initiators are {{in charge}} of inviting the session <b>listeners,</b> and are responsible of the clock synchronization sequence.Session initiators can generally be session <b>listeners,</b> but some devices (e.g.: iOS devices) can be session <b>listeners</b> only.|$|R
40|$|Native and nonnative <b>listeners</b> {{categorized}} final /v/ versus /f/ in English nonwords. Fricatives followed phonetically long originally /v/-preceding {{or short}} originally /f/-preceding vowels. Vowel duration was constant {{for each participant}} and sometimes mismatched other voicing cues. Previous results showed that English but not Dutch <b>listeners</b> whose L 1 has no final voicing contrast nevertheless used the misleading vowel duration for /v/-/f/ categorization. New analyses showed that Dutch <b>listeners</b> did use vowel duration initially, but quickly reduced its use, whereas the English <b>listeners</b> used it consistently throughout the experiment. Thus, nonnative <b>listeners</b> adapted to the stimuli more flexibly than native <b>listeners</b> did...|$|R
5|$|Savage {{strongly}} supported Donald Trump, a regular guest on his talk show, since Trump's June 2015 announcement of his candidacy in the United States 2016 presidential election. Trump has {{claimed to be}} a <b>listener</b> and a fan of Savage's show, and an April 2016 Salon article described Savage as having been a major influence on Trump's campaign.|$|E
5|$|Wild orangutans (P. pygmaeus wurmbii) in Tuanan, Borneo, were {{reported}} to use tools in acoustic communication. They use leaves to amplify the kiss squeak sounds they produce. The apes may employ this method of amplification to deceive the <b>listener</b> into believing they are larger animals.|$|E
5|$|For {{greatest}} {{efficiency and}} best coupling to the room's air volume, subwoofers {{can be placed}} {{in a corner of the}} room, far from large room openings, and closer to the <b>listener.</b> This is possible since low bass frequencies have a long wavelength; hence there is little difference between the information reaching a listener's left and right ears, and so they cannot be readily localized. All low frequency information is sent to the subwoofer. However, unless the sound tracks have been carefully mixed for a single subwoofer channel, it's possible to have some cancellation of low frequencies if bass information in one channel is out of phase with another.|$|E
5000|$|Both, MUSHRA and ITU BS.1116 tests {{call for}} trained expert <b>listeners</b> {{who know what}} typical {{artifacts}} sound like {{and where they are}} likely to occur. Expert <b>listeners</b> also have a better internalization of the rating scale which leads to a better retest reliability than untrained <b>listeners.</b> Thus, fewer <b>listeners</b> are needed to achieve significant results.|$|R
40|$|Prior investigations, using {{isolated}} {{words as}} stimuli, {{have shown that}} older <b>listeners</b> tend to require longer temporal cues than younger <b>listeners</b> to switch their percept from one word to its phonetically contrasting counterpart. The extent to which this age effect occurs in sentence contexts is investigated in the present study. The hypothesis was that perception of temporal cues differs for words presented in isolation and a sentence context and that this effect may vary between younger and older <b>listeners.</b> Younger and older <b>listeners</b> with normal-hearing and older <b>listeners</b> with hearing loss identified phonetically contrasting word pairs in natural speech continua that varied by a single temporal cue: voice-onset time, vowel duration, transition duration, and silent interval duration. The words were presented in isolation and in sentences. A context effect was shown for most continua, in which <b>listeners</b> required longer temporal cues in sentences than in isolated words. Additionally, older <b>listeners</b> required longer cues at the crossover points than younger <b>listeners</b> for most but not all continua. In general, the findings support the conclusion that older <b>listeners</b> tend to require longer target temporal cues than younger normal-hearing <b>listeners</b> in identifying phonetically contrasting word pairs in isolation and sentence contexts...|$|R
40|$|In this paper, we {{show that}} adult <b>listeners</b> who speak the same native {{language}} but live in different linguistic environments differ {{in their use of}} prosodic cues that signal word boundaries in the native language. Non-utterance-final word-final syllables have higher fundamental frequency in French. Adult native French <b>listeners</b> living in France or in the US completed an artificial-language segmentation task where fundamental frequency cued word-final boundaries (experimental). Other native French <b>listeners</b> living in France completed the corresponding task without prosodic cues (control). Results showed that France French <b>listeners</b> outperformed US French <b>listeners</b> and control French <b>listeners,</b> but US French <b>listeners</b> did not outperform control French <b>listeners.</b> The poorer performance of US French <b>listeners</b> is attributed to their regular exposure to (and thus interference from) English, a language where fundamental frequency signals word-initial boundaries. This suggests speech segmentation is adaptive, with <b>listeners</b> tuning in to the prosody of their linguistic environment. This material is based upon work supported by the National Science Foundation under grant no. BCS- 1423905 awarded to Dr. Annie Tremblay. Support for this research also comes from a Language Learning small research grant awarded to Dr. Annie Tremblay...|$|R
5|$|Goffman {{uses the}} metaphor of {{conversation}} being a stage play. A plays tone will shift throughout the performance due to the actions taken by the actors; {{this is similar to}} how a discussion is keyed â€“ based on what either person says or does over the course of an interaction, the key will change accordingly. The parallels go further, though. Goffman also claims that a speaker details a drama more often than they provide information. They invite the <b>listener</b> to empathize and, as was explained above, theyâ€™re often not meant to be stirred to take action, but rather to show appreciation; during a play this generally takes the form of applause.|$|E
5|$|In Vikram 1628 (1572 CE), Tulsidas left Chitrakuta for Prayag {{where he}} stayed during the Magha Mela (the annual fair in January). Six {{days after the}} Mela ended, he had the Darshan of the sages Yajnavalkya and Bharadvaja under a banyan tree. In {{one of the four}} dialogues in the Ramcharitmanas, Yajnavalkya is the speaker and Bharadvaja the <b>listener.</b> Tulsidas {{describes}} the meeting between Yajnavalkya and Bharadvaja after a Magha Mela festival in the Ramcharitmanas, it is this meeting where Yajnavalkya narrates the Ramcharitmanas to Bharadvaja.|$|E
5|$|Low-tech {{communication}} {{aids are}} defined as {{those that do not}} need batteries, electricity or electronics. These are often very simple communication boards or books, from which the user selects letters, words, phrases, pictures, and/or symbols to communicate a message. Depending on physical abilities and limitations, users may indicate the appropriate message with a body part, light pointer, eye-gaze direction, or a head/mouth stick. Alternatively, they may indicate yes or no while a <b>listener</b> scans through possible options.|$|E
50|$|The Bill Carroll Morning Show saw an {{increase}} of 11.2% <b>listeners</b> during the Winter BBM Ratings. Female <b>listeners</b> of the show aged 25-54 have increased 36.6% while male <b>listeners</b> aged 25-54 have only increased 25.8%.|$|R
40|$|Fundamental {{frequency}} (F 0) {{variation is}} {{one of a number of}} acoustic cues normal hearing <b>listeners</b> use for guiding lexical segmentation of degraded speech. This study examined whether F 0 contour facilitates lexical segmentation by <b>listeners</b> fitted with cochlear implants (CIs). Lexical boundary error patterns elicited under unaltered and flattened F 0 conditions were compared across three groups: <b>listeners</b> with conventional CI, <b>listeners</b> with CI and preserved low-frequency acoustic hearing, and normal hearing <b>listeners</b> subjected to CI simulations. Results indicate that all groups attended to syllabic stress cues to guide lexical segmentation, and that F 0 contours facilitated performance for <b>listeners</b> with low-frequency hearing...|$|R
40|$|In {{this study}} we {{investigated}} to what extent a meaningful sentence context facilitates spoken word processing in young and older <b>listeners</b> if listening is made taxing by time-compressing the speech. Even though elderly <b>listeners</b> {{have been shown to}} benefit more from sentence context in difficult listening conditions than young <b>listeners,</b> time compression of speech may interfere with semantic comprehension, particularly in older <b>listeners</b> because of cognitive slowing. The results of a target detection experiment showed that, unlike young <b>listeners</b> who showed facilitation by context at both rates, elderly <b>listeners</b> showed context facilitation at the intermediate, but not at the fastest rate. This suggests that semantic interpretation lags behind target identification...|$|R
5|$|Minneapolis has a mix {{of radio}} {{stations}} and healthy <b>listener</b> support for public radio. In the commercial market three radio broadcasting companies iHeartMedia (formerly Clear Channel), CBS Radio, and Cumulus Media operate {{the majority of the}} radio stations in the market. Listeners support three Minnesota Public Radio non-profit stations and two community non-profit stations, the Minneapolis Public Schools and the University of Minnesota each operate a station, and religious organizations run four stations.|$|E
5|$|More than 99 {{percent of}} the {{population}} speaks Japanese as their first language. Japanese is an agglutinative language distinguished by a system of honorifics reflecting the hierarchical nature of Japanese society, with verb forms and particular vocabulary indicating the relative status of speaker and <b>listener.</b> Japanese writing uses kanji (Chinese characters) and two sets of kana (syllabaries based on cursive script and radical of kanji), as well as the Latin alphabet and Arabic numerals.|$|E
5|$|The album {{appeared}} in many 1997 critics' lists and <b>listener</b> polls for best {{album of the}} year. It topped the year-end polls of Mojo, Vox, Entertainment Weekly, Hot Press, Muziekkrant OOR, HUMO, Eye Weekly and Inpress, and tied for first place with Daft Punk's Homework in The Face. The album came second in NME, Melody Maker, Rolling Stone, Village Voice, Spin and Uncut. Q and Les Inrockuptibles both listed the album in their unranked year-end polls.|$|E
40|$|This paper {{investigates the}} {{differences}} in the perception of six culturally encoded French social affects for Japanese and native <b>listeners.</b> Half of the Japanese <b>listeners</b> have followed six months of training about both prosodic and facial realization of French social affects. Audio-visual stimuli were presented to <b>listeners,</b> who guess speakerâ€™s intended attitude and rate the intensity of the expressiveness. Results showed that the trained Japanese <b>listeners</b> recognized better than the untrained ones; however, culturally specific attitudes (i. e. suspicious irony and obviousness) were confused by Japanese <b>listeners</b> (including trained <b>listeners).</b> Facial information cues seem to be more salient than audio ones. Index Terms: prosodic social affects, language learning, French...|$|R
50|$|As of June 2017, {{the station}} has 128,000 weekly <b>listeners</b> and a 2.7% market share. Radio Cymru {{has seen its}} weekly {{audience}} increase by 25,000 <b>listeners</b> since June 2016, where its weekly audience amounted to 103,000 <b>listeners.</b>|$|R
40|$|Hearing impairment, {{cochlear}} implantation, {{background noise}} and other auditory degradations {{result in the}} loss or distortion of sound information thought to be critical to speech perception. In many cases, <b>listeners</b> can still identify speech sounds despite degradations, but understanding of how this is accomplished is incomplete. Experiments presented here tested the hypothesis that <b>listeners</b> would utilize acoustic-phonetic cues differently if one or more cues were degraded by hearing impairment or simulated hearing impairment. Results supported this hypothesis for various listening conditions that are directly relevant for clinical populations. Analysis included mixed-effects logistic modeling of contributions of individual acoustic cues for various contrasts. <b>Listeners</b> with cochlear implants (CIs) or normal-hearing (NH) <b>listeners</b> in CI simulations showed increased use of acoustic cues in the temporal domain and decreased use of cues in the spectral domain for the tense/lax vowel contrast and the word-final fricative voicing contrast. For the word-initial stop voicing contrast, NH <b>listeners</b> made less use of voice-onset time and greater use of voice pitch in conditions that simulated high-frequency hearing impairment and/or masking noise; influence of these cues was further modulated by consonant place of articulation. A pair of experiments measured phonetic context effects for the "s/sh" contrast, replicating previously observed effects for NH <b>listeners</b> and generalizing them to CI <b>listeners</b> as well, despite known deficiencies in spectral resolution for CI <b>listeners.</b> For NH <b>listeners</b> in CI simulations, these context effects were absent or negligible. Audio-visual delivery of this experiment revealed enhanced influence of visual lip-rounding cues for CI <b>listeners</b> and NH <b>listeners</b> in CI simulations. Additionally, CI <b>listeners</b> demonstrated that visual cues to gender influence phonetic perception {{in a manner consistent}} with gender-related voice acoustics. All of these results suggest that <b>listeners</b> are able to accommodate challenging listening situations by capitalizing on the natural (multimodal) covariance in speech signals. Additionally, these results imply that there are potential differences in speech perception by NH <b>listeners</b> and <b>listeners</b> with hearing impairment that would be overlooked by traditional word recognition or consonant confusion matrix analysis...|$|R
5|$|The {{original}} {{scores for}} God of War, God of War II, and God of War III were nominated for Best Original Score at the 2005, 2007, and 2010 Spike Video Game Awards, respectively. The God of War Trilogy Soundtrack was {{included with the}} God of War III Ultimate Edition and Ultimate Trilogy Edition collections as downloadable content. The Trilogy Soundtrack consists of the original scores for God of War, God of War II, and God of War III. It was praised by critics {{as the best way}} to experience the series' musical development, and allows the <b>listener</b> to note the development of the composers during the series.|$|E
5|$|The {{practice}} of long-distance radio listening {{began in the}} 1920s when shortwave broadcasters were first established in the US and Europe. Audiences discovered that international programming was available on the shortwave bands of many consumer radio receivers, {{and a number of}} magazines and <b>listener</b> clubs catering to the practice arose as a result. Shortwave listening was especially popular during times of international conflict such as World War II, the Korean War and the Persian Gulf War.|$|E
5|$|The {{prescribed}} readings for {{the second}} of three Easter feast days included the narration of the Road to Emmaus. The cantata was Bach's first composition for Easter as Thomaskantor in Leipzig. He derived it from his earlier Serenata, which had a similar celebratory mood. An unknown librettist solved the problem that Bach's congratulatory cantata was a dialogue of tenor and alto by retaining a dialogue in three movements, assigned to Hope and Fear. They represent different attitudes to {{the news of the}} Resurrection of Jesus, which may be found in the two disciples, discussing the events on their walk, but also within the <b>listener</b> of the cantata.|$|E
40|$|The {{present study}} {{investigated}} whether <b>listeners</b> perceptually map phonetic information to phonological feature categories or to phonemes. The test {{case is a}} phonological feature that occurs {{in most of the}} worldâ€™s languages, namely vowel height, and its acoustic correlate, the first formant (F 1). We first simulated vowel discrimination in virtual <b>listeners</b> who perceive speech sounds through phonological features and virtual <b>listeners</b> who perceive through phonemes. The simulations revealed that feature <b>listeners</b> differed from phoneme <b>listeners</b> in their perceptual discrimination of F 1 along a front-back boundary continuum as compared to a front (or back) continuum. The competing predictions of phoneme-based versus feature-based vowel discrimination were explicitly tested in real human <b>listeners.</b> The real listenersâ€™ vowel discrimination did not resemble the simulated phoneme <b>listeners,</b> and was compatible with that of the simulated feature <b>listeners.</b> The findings suggest that humans perceive vowel F 1 through phonological feature categories like /high/ and /mid/. 5 page(s...|$|R
40|$|Purpose: To {{investigate}} {{the effect of}} <b>listenerâ€™s</b> native language (L 1) {{and the types of}} noise on English vowel identification in noise. Method: Identification of twelve English vowels was measured in quiet and in long-term speech-shaped noise and multi-talker babble noise for English- (EN), Chinese- (CN) and Korean-native (KN) <b>listeners</b> at various signal-to-noise ratios. Results: Compared to non-native <b>listeners,</b> EN <b>listeners</b> performed significantly better in quiet and in noise. Vowel identification in long-term speech-shaped noise and in multi-talker babble noise was similar between CN and KN <b>listeners.</b> This is different from our previous study in which KN <b>listeners</b> performed better than CN <b>listeners</b> in English sentence recognition in multi-talker babble noise. Discussion: Results from the current study suggest that depending on speech materials, the effect of non-native listenersâ€™ L 1 on speech perception in noise may be different. That is, in the perception of speech materials with little linguistic cues like isolated vowels, the characteristics of non-native <b>listenerâ€™s</b> native language may not play a significant role. On the other hand, in the perception of running speech in which <b>listeners</b> need to use more linguistic cues (e. g., acoustic-phonetic, semantic, and prosodic cues), the non-native <b>listenerâ€™s</b> native language background might result in a different masking effect...|$|R
40|$|Japanese <b>listeners</b> detect {{speech sound}} targets which {{correspond}} precisely to a mora (a phonological unit {{which is the}} unit of rhythm in Japanese) more easily than targets which do not. English <b>listeners</b> detect medial vowel targets more slowly than consonants. Six phoneme detection experiments investigated these effects in both subject populations, presented with native- and foreign-language input. Japanese <b>listeners</b> produced faster and more accurate responses to moraic than to nonmoraic targets both in Japanese and, where possible, in English; English <b>listeners</b> responded differently. The detection disadvantage for medial vowels appeared with English <b>listeners</b> both in English and in Japanese; again, Japanese <b>listeners</b> responded differently. Some processing operations which <b>listeners</b> apply to speech input are language-specific; these language-specific procedures, appropriate for listening to input in the native language, may be applied to foreign-language input irrespective of whether they remain appropriate...|$|R
