6|10000|Public
40|$|D-Wave quantum annealers {{represent}} a novel computational architecture and have attracted significant interest, {{but have been}} used for few real-world computations. Machine learning has been identified as an area where quantum annealing may be useful. Here, we show that the D-Wave 2 X can be effectively used as part of an unsupervised machine learning method. This method can be used to analyze large datasets. The D-Wave only limits the number of features that can be extracted from the dataset. We apply this method to <b>learn</b> <b>the</b> <b>features</b> <b>from</b> a set of facial images...|$|E
30|$|CNN has {{provided}} an end-to-end learning {{model in which}} the parameters can be trained by the traditional gradient descent method. The trained convolutional neural network can <b>learn</b> <b>the</b> <b>features</b> <b>from</b> the image and complete the extraction and classification of image features. As an important branch in the research of neural networks, the convolutional neural network {{is characterized by the}} practicability that the features of each layer can be obtained through the convolution kernel of the shared weight by the local region of the upper layer. Thus, this characteristic makes the convolutional neural network more suitable for the learning and expressing the image features than other neural network methods.|$|E
40|$|Convolutional Neural Networks(CNN) {{has had a}} {{great success}} in the recent past, because of the advent of faster GPUs and memory access. CNNs are really {{powerful}} as they <b>learn</b> <b>the</b> <b>features</b> <b>from</b> data in layers such that they exhibit {{the structure of the}} V- 1 features of the human brain. A huge bottleneck, in this case, is that CNNs are very large and have a very high memory footprint, and hence they cannot be employed on devices with limited storage such as mobile phone, IoT etc. In this work, we study the model complexity versus accuracy trade-off on MNSIT dataset, and give a concrete framework for handling such a problem, given the worst case accuracy that a system can tolerate. In our work, we reduce the model complexity by 236 times, and memory footprint by 19. 5 times compared to the base model while achieving worst case accuracy threshold...|$|E
30|$|Although sparse <b>feature</b> <b>learning</b> {{effectively}} <b>learns</b> <b>the</b> <b>feature</b> <b>from</b> <b>the</b> data, appropriate preprocessing is {{a prerequisite}} to obtaining a good feature. In addition, we need to decrease the dimensionality {{as much as possible}} without losing important parts of the information. We perform several steps for preprocessing, as described below.|$|R
40|$|Deep-HiTS is a {{rotation}} invariant {{convolutional neural network}} (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs have <b>the</b> advantage of <b>learning</b> <b>the</b> <b>features</b> automatically <b>from</b> <b>the</b> data while achieving high performance...|$|R
30|$|Before bases <b>learning,</b> all <b>the</b> <b>feature</b> vectors <b>from</b> <b>the</b> {{corresponding}} GTZAN {{data set}} are pooled together and randomly shuffled. Then, {{each of the}} matrix decomposition method is applied and <b>the</b> respective dictionaries <b>learned.</b>|$|R
40|$|A key {{challenge}} in generic object detection is being to handle large variations in object scale, poses, viewpoints, especially part deformations when determining the location for specified object categories. Recent advances in deep neural networks have achieved promising results for object detection by extending the traditional detection methodologies using the {{convolutional neural network}} architectures. In this paper, we make an attempt to incorporate another traditional detection schema, Regionlet into an end-to-end trained deep learning framework, and perform ablation studies on its behavior on multiple object detection datasets. More specifically, we propose a "region selection network" and a "gating network". The region selection network serves as a guidance on where to select regions to <b>learn</b> <b>the</b> <b>features</b> <b>from.</b> Additionally, the gating network serves as a local feature selection module to select and transform feature maps to be suitable for detection task. It acts as soft Regionlet selection and pooling. The proposed network is trained end-to-end without additional efforts. Extensive experiments and analysis on the PASCAL VOC dataset and Microsoft COCO dataset show that the proposed framework achieves comparable state-of-the-art results...|$|E
40|$|Automatic Offline Handwritten Signature Verification {{has been}} {{researched}} {{over the last few}} decades from several perspectives, using insights from graphology, computer vision, signal processing, among others. In spite of the advancements on the field, building classifiers that can separate between genuine signatures and skilled forgeries (forgeries made targeting a particular signature) is still hard. We propose approaching the problem from a feature learning perspective. Our hypothesis is that, {{in the absence of a}} good model of the data generation process, it is better to <b>learn</b> <b>the</b> <b>features</b> <b>from</b> data, instead of using hand-crafted features that have no resemblance to the signature generation process. To this end, we use Deep Convolutional Neural Networks to learn features in a writer-independent format, and use this model to obtain a feature representation on another set of users, where we train writer-dependent classifiers. We tested our method in two datasets: GPDS- 960 and Brazilian PUC-PR. Our experimental results show that the features learned in a subset of the users are discriminative for the other users, including across different datasets, reaching close to the state-of-the-art in the GPDS dataset, and improving the state-of-the-art in the Brazilian PUC-PR dataset. Comment: Accepted as a conference paper to The International Joint Conference on Neural Networks (IJCNN) 201...|$|E
30|$|In recent years, {{learning}} features from unlabeled data using unsupervised {{feature learning}} and deep learning approaches have achieved superior performance in solving many computer vision problems [22 – 25]. Feature learning is attractive as it exploits {{the availability of}} large amount of data and avoids the need of feature engineering. It has also {{attracted the attention of}} stereo vision researchers in recent years. The method proposed in [26] uses the deep convolutional neural network for learning similarity measure on small image patches, and the training is carried in a supervised manner by constructing a binary classification dataset with examples of similar and dissimilar pair of patches. Based on the learned similarity measure, the disparity map is estimated using state-of-the-art local stereo methods. Here, the learning is done on small size patches instead of entire image, i.e., global contextual constraint is not taken into account while learning the similarity measure. The method does not provide a single framework for dense disparity estimation though it improves the results of state of the art stereo methods. In this work, we focus on the approaches which use feature matching cost in a global energy minimization framework for estimating the dense disparities. In [27], authors proposed unsupervised feature learning for dense stereo matching within a energy minimization framework. They <b>learn</b> <b>the</b> <b>features</b> <b>from</b> a large amount of image patches using K-singular value decomposition (K-SVD) dictionary learning approach. The limitation of their approach is that the features are learned from a set of image patches and do not consider the entire image, i.e., global contextual constraint is not taken into account while learning the features. Also, higher level features are not learned, instead, they are estimated using a simple max pooling operation from the layer beneath. Here, the higher layer correspondence matches are used to initialize the lower layer matching and hence the accuracy depends on the higher layer matches only. Recently, unsupervised feature learning and deep learning methods have shown superior performance in learning efficient representation of images at multiple layers [24, 28 – 33].|$|E
30|$|Deng et al. [9] {{has used}} sparse-autoencoder for {{acoustic}} <b>features</b> extraction <b>from</b> human speech signal for human emotion recognition. Shu and Fyshe [12] has used sparse-autoencoder for <b>feature</b> extraction <b>from</b> magnetoencephalography signal. <b>The</b> <b>learned</b> <b>features</b> <b>from</b> autoencoders {{are in the}} form of its hidden layer weights.|$|R
40|$|The inner {{structure}} of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which opens doors for huge uncertainties. Since the microstructure could {{be a combination of}} different phases with complex substructures its automatic classification is very challenging and just a little work in this field has been carried out. Prior related works apply mostly designed and engineered features by experts and classify microstructure separately <b>from</b> <b>feature</b> extraction step. Recently Deep Learning methods have shown surprisingly good performance in vision applications by <b>learning</b> <b>the</b> <b>features</b> <b>from</b> data together with the classification step. In this work, we propose a deep learning method for microstructure classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Networks (FCNN) accompanied by max-voting scheme. Our system achieves 93. 94 % classification accuracy, drastically outperforming the state-of-the-art method of 48. 89 % accuracy, indicating the effectiveness of pixel-wise approaches. Beyond the success presented in this paper, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation...|$|R
40|$|Abstract—Reading {{text from}} {{photographs}} is a challenging {{problem that has}} received {{a significant amount of}} attention. Two key components of most systems are (i) text detection from images and (ii) character recognition, and many recent methods have been proposed to design better feature representations and models for both. In this paper, we apply methods recently developed in machine learning–specifically, large-scale algorithms for <b>learning</b> <b>the</b> <b>features</b> automatically <b>from</b> unlabeled data–and show that they allow us to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end-to-end system. Keywords-Robust reading, character recognition, feature learning, photo OCR I...|$|R
40|$|A {{new musical}} {{instrument}} classification method using convolutional neural networks (CNNs) {{is presented in}} this paper. Unlike the traditional methods, we investigated a scheme for classifying musical instruments using <b>the</b> <b>learned</b> <b>features</b> <b>from</b> CNNs. To create <b>the</b> <b>learned</b> <b>features</b> <b>from</b> CNNs, we not only used a conventional spectrogram image, but also proposed multiresolution recurrence plots (MRPs) that contain the phase information of a raw input signal. Consequently, we fed the characteristic timbre of the particular instrument into a neural network, which cannot be extracted using a phase-blinded representations such as a spectrogram. By combining our proposed MRPs and spectrogram images with a multi-column network, the performance of our proposed classifier system improves over a system that uses only a spectrogram. Furthermore, the proposed classifier also outperforms the baseline result <b>from</b> traditional handcrafted <b>features</b> and classifiers. Comment: 14 pages, 5 figures, 1 tabl...|$|R
3000|$|... 2 distance. Li et al. [29] extract LBP, Gabor, {{discrete}} cosine transform (DCT), {{and some}} global appearance-based statistical features for image representation. Then, {{a combination of}} SVMs using a modified AdaBoost.M 1 is used {{in order to improve}} classification performance. Liu and Wang [11] adopt a deep learning scheme to automatically <b>learn</b> <b>the</b> discriminative <b>features</b> <b>from</b> dense image patches. Following a BoW pipeline, a linear SVM is <b>learned</b> based on <b>the</b> BoW representations. Ghosh and Chaudhary [30] test the performance of various features like BoW representation based on speeded-up robust features (SURF), region-of-interest (ROI)-based feature, texture-based feature, and normalized histogram of orientated gradients (HOG) features. Experimental results show that the combination of HOG, texture-, and ROI-based features using SVM classifier achieves the best classification performance. All the participated methods are reported in [11] for reference.|$|R
40|$|The {{success of}} {{automatic}} classification of variable stars strongly {{depends on the}} lightcurve representation. Usually, lightcurves are represented as a vector of many statistical descriptors designed by astronomers called features. These descriptors commonly demand significant computational power to calculate, require substantial research effort to develop and do not guarantee good performance on the final classification task. Today, lightcurve representation is not entirely automatic; algorithms that extract lightcurve features are designed by humans and must be manually tuned up for every survey. The vast amounts of data that will be generated in future surveys like LSST mean astronomers must develop analysis pipelines that are both scalable and automated. Recently, substantial efforts {{have been made in}} <b>the</b> machine <b>learning</b> community to develop methods that prescind from expert-designed and manually tuned features for features that are automatically learned from data. In this work we present what is, to our knowledge, <b>the</b> first unsupervised <b>feature</b> <b>learning</b> algorithm designed for variable stars. Our method first extracts a large number of lightcurve subsequences from a given set of photometric data, which are then clustered to find common local patterns in the time series. Representatives of these patterns, called exemplars, are then used to transform lightcurves of a labeled set into a new representation that can then be used to train an automatic classifier. <b>The</b> proposed algorithm <b>learns</b> <b>the</b> <b>features</b> <b>from</b> both labeled and unlabeled lightcurves, overcoming the bias generated when <b>the</b> <b>learning</b> process is done only with labeled data. We test our method on MACHO and OGLE datasets; the results show that the classification performance we achieve is as good and in some cases better than the performance achieved using traditional <b>features,</b> while <b>the</b> computational cost is significantly lower...|$|R
30|$|In this paper, {{we present}} a deep {{indicator}} for fine-grained classification of the precise ripening stages of bananas based on images. It is accomplished through a novel CNN architecture designed specifically for the unique characteristics of banana appearance. The proposed CNN framework takes triple images as input, from which the triplet loss (similarity loss) is produced. Through the joint optimization of classification accuracy and similarity loss, our proposed technique can effectively <b>learn</b> <b>the</b> fine-grained <b>feature</b> representations <b>from</b> <b>the</b> ripening process of a banana.|$|R
40|$|We present {{methods for}} {{recognizing}} object categories which {{are able to}} combine various feature types (e. g. image patches and edge boundaries). Our objective is to detect object instances in an image, {{as opposed to the}} easier task of image categorization. To this end, we investigate two algorithms for learning and detecting object categories which both benefit <b>from</b> combining <b>features.</b> <b>The</b> first uses a naive combination method for detectors each employing only one type of <b>feature,</b> <b>the</b> second <b>learns</b> <b>the</b> best <b>features</b> (<b>from</b> a pool of patches and boundaries). In experiments we achieve comparable results to {{the state of the art}} over a number of datasets, and for some categories we even achieve the lowest errors that have been reported so far. The results also show that certain object categories prefer certain feature types (e. g. boundary fragments for airplanes) ...|$|R
40|$|In this paper, a {{mathematical}} framework for <b>learning</b> <b>the</b> acoustic <b>features</b> <b>from</b> a central auditory representation is presented. We adopt a statistical approach that models <b>the</b> <b>learning</b> process as {{to achieve a}} maximum likelihood estimation of the signal distribution. An algorithm, called statistical matching pursuit (SMP), is introduced to identify regions on the cortical surface where <b>the</b> <b>features</b> for each sound class are most prominent. We model <b>the</b> <b>features</b> with distributions of Gaussian mixture densities, and employ the expectation-maximization (EM) procedure to both improve the parameterization and refine iteratively the selection of cortical regions <b>from</b> which <b>the</b> <b>features</b> are extracted. <b>The</b> <b>learning</b> algorithm is applied to vowel classification on TIMIT database where all the vowels (excluding diphthongs, nine in total) are regarded as individual classes. Experimental results show that models trained under SMP/EM algorithm achieve a comparable recognition accuracy to that of conventional recognizers...|$|R
40|$|International audienceRecognition {{of spatial}} {{relations}} between pairs ofsubexpressions {{is a key}} problem of recognition of handwrittenmathematical expressions. Most methods for spatial relation classiﬁcation are based on handcrafted rules and geometric indicesextracted from the subexpression bounding boxes. In this work,we propose new spatial relation features that combine subexpression bounding box and intra-subexpression information, alongwith prior knowledge about the general position and size ofsymbols. Instead of handcrafting features, we train artiﬁcialneural networks to <b>learn</b> <b>the</b> useful <b>features</b> <b>from</b> two kinds ofhistograms. The ﬁrst type captures the relative positions and sizesof the subexpression bounding boxes. The second captures therelative positions and shape {{of a pair of}} symbols, called dominantsymbols, extracted from the main baselines of the evaluatedsubexpressions. We evaluate and compare our features with twostate-of-the-art features on a benchmark dataset. Experimentalresults show that our features obtain better accuracy than thesetwo features...|$|R
40|$|This paper {{proposes a}} novel deep {{learning}} framework named bidirectional-convolutional long {{short term memory}} (Bi-CLSTM) network to automatically <b>learn</b> <b>the</b> spectral-spatial <b>feature</b> <b>from</b> hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract <b>the</b> spatial <b>feature.</b> Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, <b>the</b> <b>learned</b> <b>features</b> are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate {{the effectiveness of the}} proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods...|$|R
40|$|Most {{existing}} star-galaxy classifiers use {{the reduced}} summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically <b>learn</b> <b>the</b> <b>features</b> directly <b>from</b> data, minimizing {{the need for}} input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering. Comment: 13 page, 13 figures. Accepted for publication in the MNRAS. Code available at [URL]...|$|R
40|$|We {{introduce}} Deep-HiTS, {{a rotation}} invariant {{convolutional neural network}} (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs have <b>the</b> advantage of <b>learning</b> <b>the</b> <b>features</b> automatically <b>from</b> <b>the</b> data while achieving high performance. We compare our CNN model against a feature engineering approach using random forests (RF). We show that our CNN significantly outperforms the RF model reducing the error by almost half. Furthermore, for a fixed number of approximately 2, 000 allowed false transient candidates per night {{we are able to}} reduce the miss-classified real transients by approximately 1 / 5. To the best of our knowledge, {{this is the first time}} CNNs have been used to detect astronomical transient events. Our approach will be very useful when processing images from next generation instruments such as the Large Synoptic Survey Telescope (LSST). We have made all our code and data available to the community for the sake of allowing further developments and comparisons at [URL]...|$|R
40|$|University of Louisville We propose new Continuous Hidden Markov Model (CHMM) {{structure}} that integrates feature weighting component. We assume that each feature vector could include different subsets of <b>features</b> that come <b>from</b> different {{sources of information}} or different feature extractors. We modify the probability density function that characterizes the standard CHMM to include state and component dependent feature relevance weights. To <b>learn</b> <b>the</b> optimal <b>feature</b> weights <b>from</b> <b>the</b> training data, we modify the maximum likelihood based Baum-Welch algorithm and we derive the necessary conditions. The proposed approach is validated using synthetic and real data sets. The results are shown to outperform the standard CHMM. 1...|$|R
40|$|In this paper, {{we propose}} deep {{architecture}} to dynamically <b>learn</b> <b>the</b> most discriminative <b>features</b> <b>from</b> data for both single-cell and object tracking in computational biology and computer vision. Firstly, <b>the</b> discriminative <b>features</b> are automatically learned via a convolutional deep belief network (CDBN). Secondly, we design a simple yet effective method to transfer <b>features</b> <b>learned</b> <b>from</b> CDBNs on <b>the</b> source tasks for generic purpose {{to the object}} tracking tasks using only limited amount of training data. Finally, to alleviate the tracker drifting problem caused by model updating, we jointly consider three different types of positive samples. Extensive experiments validate the robustness and effectiveness of the proposed method...|$|R
40|$|Traditional image {{aesthetic}} {{evaluation method}} usually involves {{the extraction of}} a set of relevant image aesthetic features and classification by a classifier trained on <b>the</b> set of <b>features.</b> <b>The</b> system's performance greatly depends on the effectiveness of <b>the</b> <b>features.</b> However, most of these features are carefully hand-crafted for specific datasets and assumed strong prior knowledge. Therefore, these features would not be optimal for general image aesthetic evaluation. The deep convolution neural network (DCNN) has the ability to automatically learn aesthetic features, and network structure of different complexity can learn aesthetic features at different scales and different point of views. Moreover, traditional image features, such as edge and saliency map, can be used as auxiliary information for the DCNN. Therefore, a Network-Paralleled and Data-Paralleled DCNN (NP-DP-DCNN) structure is proposed. The Network-Paralleled DCNN fuses networks of different complexity and the Data-Paralleled DCNN fuses original image data and derived <b>feature</b> maps to <b>learn</b> <b>the</b> aesthetic <b>features</b> <b>from</b> different scales and different point of views. Experimental results show that the proposed NP-DP-DCNN structure is able to achieve better classification performance than many existing methods. No Full Tex...|$|R
40|$|Abstract—Convolutional Neural Network is {{efficient}} in learn-ing hierarchical <b>features</b> <b>from</b> large datasets, but its model complexity and large memory foot prints are preventing {{it from being}} deployed to devices without a server backend support. Modern CNNs are always trained on GPUs or even GPU clusters with high speed computation power due to the immense size of the network. Methods on regulating {{the size of the}} network, on the other hand, are rarely studied. In this paper we present a novel compact architecture that minimizes the number of lower level kernels in a CNN by separating the color information from the original image. A 9 -patch histogram extractor is built to exploit the separated color information. A higher level classifier <b>learns</b> <b>the</b> combined <b>features</b> <b>from</b> <b>the</b> compact CNN, trained only on grayscale image with limited number of kernels, and the histogram extractor. We apply our compact architecture to CIFAR- 10 and Samsung Mobile Image Dataset. The proposed architecture has a recognition accuracy on par with those of state of the art CNNs, with 40 % less parameters. I...|$|R
40|$|Mega-city {{analysis}} with very high resolution (VHR) satellite images has been drawing increasing {{interest in the}} fields of city planning and social investigation. It is known that accurate land-use, urban density, and population distribution information is the key to mega-city monitoring and environmental studies. Therefore, how to generate land-use, urban density, and population distribution maps at a fine scale using VHR satellite images has become a hot topic. Previous studies have focused solely on individual tasks with elaborate hand-crafted features and have ignored the relationship between different tasks. In this study, we aim to propose a universal framework which can: 1) automatically <b>learn</b> <b>the</b> internal <b>feature</b> representation <b>from</b> <b>the</b> raw image data; and 2) simultaneously produce fine-scale land-use, urban density, and population distribution maps. For the first target, a deep convolutional neural network (CNN) is applied to <b>learn</b> <b>the</b> hierarchical <b>feature</b> representation <b>from</b> <b>the</b> raw image data. For the second target, a novel CNN-based universal framework is proposed to process the VHR satellite images and generate the land-use, urban density, and population distribution maps. To the best of our knowledge, this is the first CNN-based mega-city analysis method which can process a VHR remote sensing image with such a large data volume. A VHR satellite image (1. 2 m spatial resolution) of the center of Wuhan covering an area of 2606 km 2 was used to evaluate the proposed method. The experimental results confirm that the proposed method can achieve a promising accuracy for land-use, urban density, and population distribution maps...|$|R
40|$|With the {{exponential}} {{growth of the}} digital data, video content analysis (e. g., action, event recognition) has been drawing increasing attention from computer vision researchers. Effective modeling of the objects, scenes, and motions is critical for visual understanding. Recently {{there has been a}} growing interest in <b>the</b> bio-inspired deep <b>learning</b> models, which has shown impressive results in speech and object recognition. <b>The</b> deep <b>learning</b> models are formed by the composition of multiple non-linear transformations of the data, with the goal of yielding more abstract and ultimately more useful representations. The advantages of the deep models are three fold: 1) They <b>learn</b> <b>the</b> <b>features</b> directly <b>from</b> <b>the</b> raw signal in contrast to <b>the</b> hand-designed <b>features.</b> 2) <b>The</b> <b>learning</b> can be unsupervised, which is suitable for large data where labeling all the data is expensive and unpractical. 3) They learn a hierarchy of features one level at a time and the layerwise stacking of feature extraction, this often yields better representations. However, not many deep learning models have been proposed to solve the problems in video analysis, especially videos “in a wild”. Most of them are either dealing with simple datasets, or limited to the low-level local spatial-temporal feature descriptors for action recognition. Moreover...|$|R
30|$|DL {{consists}} {{of a set of}} techniques that can automatically <b>learn</b> <b>the</b> representations (i.e., <b>features)</b> <b>from</b> raw data used for classification tasks [12]. <b>The</b> ability to <b>learn</b> representations at multiple levels of abstraction merely by stacking non-linear layers allow DL methods to achieve better generalization on highly complex tasks such as image classification. DCNN is a type of DL methods that have recently become modus of operandi for image recognition tasks due to its remarkable achievements in this area [39]. This success is partly because of its robust and precise assumptions about the natural images (i.e., locality of associations between pixels and statistical stationarity) [12] and partially due to ease of optimization because of significantly lesser parameters as compared to feed-forward networks [42].|$|R
40|$|Searching {{documents}} {{that are similar}} to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-) supervised hashing method named Semi-Supervised SimHash (S 3 H) for high-dimensional data similarity search. The basic idea of S 3 H is to <b>learn</b> <b>the</b> optimal <b>feature</b> weights <b>from</b> prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. ...|$|R
40|$|This paper {{presents}} a generalized Bayesian framework for relevance feedback in content-based image retrieval. The proposed feedback technique {{is based on}} Bayesian learning method and incorporates a time-varying user model into the formulation. We define the user model with two terms: a target query and a user conception. The target query is aimed to <b>learn</b> <b>the</b> common <b>features</b> <b>from</b> relevant images so as to specify the user’s ideal query. The user conception is aimed to learn a parameter set to determine the time-varying matching criterion. Therefore, at each feedback step, <b>the</b> <b>learning</b> process updates not only the target distribution but also the target query and the matching criterion. In addition, another objective {{of this paper is}} to conduct the relevance feedback on images represented in region level. We formulate the matching criterion using a weighting scheme and proposed a region clustering technique to determine the region correspondence between relevant images. With the proposed region clustering technique, we derive a representation in region level to characterize the target query. Experiments demonstrate that the proposed method combined with time-varying user model indeed achieves satisfactory results and our proposed region-based techniques further improve the retrieval accuracy. Index terms – Content-based image retrieval, relevance feedback, Bayesian learning, target query, user conception, region clustering, region correspondence. I...|$|R
40|$|With {{the rapid}} {{development}} of the usage of digital imaging and communication technologies, {{there appears to be}} a great demand for fast and practical approaches for image quality assessment (IQA) algorithms that can match human judge-ments. In this paper, we propose a novel general-purpose no-reference IQA (NR-IQA) framework by means of learn-ing quality-aware filters (QAF). Using these filters for im-age encoding, we can obtain effective image representations for quality estimation. Additionally, random forest is used to <b>learn</b> <b>the</b> mapping <b>from</b> <b>feature</b> space to human subjec-tive scores. Extensive experiments conducted on LIVE and CSIQ databases demonstrate that the proposed NR-IQA met-ric QAF can achieve better prediction performance than all the other state-of-the-art NR-IQA approaches in terms of both prediction accuracy and generalization capabilities. Index Terms — NR-IQA, natural scene statistics, sparse filtering, random forest 1...|$|R
40|$|We {{present a}} novel {{approach}} for multi-modal affect analysis in human interactions {{that is capable of}} integrating data from multiple modalities while also taking into account temporal dynamics. Our fusion approach, Joint Hidden Conditional Random Fields (JHRCFs), combines the advantages of purely feature level (early fusion) fusion approaches with late fusion (CRFs on individual modalities) to simultaneously <b>learn</b> <b>the</b> correlations between <b>features</b> <b>from</b> multiple modalities as well as their temporal dynamics. Our approach addresses major shortcomings of other fusion approaches such as the domination of other modalities by a single modality with early fusion and the loss of cross-modal information with late fusion. Extensive results on the AVEC 2011 dataset show that we outperform the state-of-the-art on the Audio Sub-Challenge, while achieving competitive performance on the Video Sub-Challenge and the Audiovisual Sub-Challenge. Index Terms — Affect Recognition, Multimodal Fusio...|$|R
40|$|Increasing {{the size}} of {{training}} data in many computer vision tasks has shown to be very effective. Using large scale image datasets (e. g. ImageNet) with simple learning techniques (e. g. linear classifiers) one can achieve state-of-the-art performance in object recognition compared to sophisticated learning techniques on smaller image sets. Semantic search on visual data has become very popular. There are billions of images {{on the internet and}} the number is increasing every day. Dealing with large scale image sets is intense per se. They take a significant amount of memory that makes it impossible to process the images with complex algorithms on single CPU machines. Finding an efficient image representation can be a key to attack this problem. A representation being efficient is not enough for image understanding. It should be comprehensive and rich in carrying semantic information. In this proposal we develop an approach to computing binary codes that provide a rich and efficient image representation. We demonstrate several tasks in which binary features can be very effective. We show how binary features can speed up large scale image classification. We present learning techniques to <b>learn</b> <b>the</b> binary <b>features</b> <b>from</b> supervised image set (With different types of semantic supervision; class labels, textual descriptions). We propose several problems that are very important in finding and using efficient image representation...|$|R
40|$|Abstract. Automatic event {{detection}} {{in a large}} {{collection of}} unconstrained videos is a challenging and important task. The key issue is to describe long complex video with high level semantic descriptors, which should find the regularity {{of events in the}} same category while distinguish those from different categories. This paper proposes a novel unsupervised approach to discover data-driven concepts from multi-modality signals (audio, scene and motion) to describe high level semantics of videos. Our methods consists of three main components: we first <b>learn</b> <b>the</b> low-level <b>features</b> separately <b>from</b> three modalities. Secondly we discover the datadriven concepts based on <b>the</b> statistics of <b>learned</b> <b>features</b> mapped to a low dimensional space using deep belief nets (DBNs). Finally, a compact and robust sparse representation is learned to jointly model the concepts from all three modalities. Extensive experimental results on large in-thewild dataset show that our proposed method significantly outperforms state-of-the-art methods. ...|$|R
40|$|This paper {{proposes a}} method to verify the singer {{identity}} of a given song. The query song is modeled as a GMM <b>learned</b> on <b>the</b> <b>features</b> extracted <b>from</b> sustained sung notes of the song. Each note is described by the shape its spectral envelope and by the temporal variations in frequency and amplitude of its fundamental frequency. The singer identity is verified with two approaches: {{the model of the}} query song is compared to a singer-based GMM or compared to the GMM of another song performed by the same singer. The comparison is done using a dissimilarity measurement given by the Kullback Leibler divergence. When the two types of <b>features</b> are combined, <b>the</b> proposed approach verifies the singer identity of a given a cappella song with an error rate lower than 8 % when the whole song is considered and an error rate lower than 10 % when a short excerpt of the song (i. e. 15 consecutive sustained notes) is considered. 1...|$|R
