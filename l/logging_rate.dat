17|361|Public
50|$|SQLTags Historian is a {{proprietary}} data logging technology that manages historical data in any supported SQL Database. It is configured by defining an Ignition SQL database connection, then enabling the feature {{on a per}} tag basis. The system automatically creates and manages the tables without any user SQL input. Historical Scan Classes support different logging intervals by sets of tags as well as altering the <b>logging</b> <b>rate</b> dynamically. This allows the system to store high resolution history when needed, but lower resolution data for other periods. The system automatically partitions data into multiple database tables. This accomplishes two functions: ensures consistent performance over the long term, and provides a consistent mechanism for data archival and pruning.|$|E
5000|$|On 5 September 1927, Henri George Doll {{and several}} field {{engineers}} ran the first wireline electric log. This log {{was of the}} Diefenbach #2905 well, Rig 7 of the Pechelbronn Oil Company at Merkwiller-Pechelbronn in the Bas-Rhin. The well was about 500 meters deep but only the interval from 130m to 270m was logged. A measurement was made every meter. The equipment was stopped, and the logging cable {{was connected to the}} surface potentiometer and to a battery power supply to make a measurement. The <b>logging</b> <b>rate</b> was about 50 meters per hour. The Schlumberger brothers called this technique an [...] "Electrical Survey", but the more common name [...] "well log" [...] was coined a few years later in the United States.|$|E
40|$|Data of {{increment}} of {{the remnant}} trees after logging, ingrowth and mortality {{was obtained by}} assessment before logging and after 6 years, two sites of 50 ha, in Amazon forest. Logging scenarios were simulated to identify the <b>logging</b> <b>rate</b> potential for each studying sites, by diameter class projection method. The cy- cle of 35 years and the <b>logging</b> <b>rate</b> of 30 m 3 ?ha? 1 exceed the time required for recovery in the primary forest, in the studied site. The simulation showed that in the studying area, a well-planned logging, with minimum logging damage {{would be possible to}} implement an initial cycle of 25 years to the forest to re- cover 30 m 3 ?ha? 1, if 50 % of the timber stock were reserved. The forest increment, beyond important fac- tors such as the increase of individual species, is quite dependent on the remnant trees. 201...|$|E
40|$|This paper {{shows the}} {{usefulness}} of methods of three-mode factor analysis for cross-cultural compa. risous of value concepts. Data were obtained frotu : 100 German and :wo American students, using the Individual and Organizational Values SYM <b>LOG</b> <b>rating</b> form. The comparative structural analyses of the samples made it apparent that the SYM <b>LOG</b> <b>rating</b> forms used did not yield the expected correspom. lcncc to the theoretical three-dimensional value space. Reasons for and r. onsequen. ces of these fit,ulings for further studies in this field are discussed. The methodological approach outlined in this paper eau supplement the criteria llales et al. (1987) formulated for adaptation o'f SYM <b>LOG</b> <b>rating</b> items to particular populations and cultural contexts, and provides more adequate procedures for da. ta ana. lysis...|$|R
30|$|While this {{argument}} may seem trivial at first, {{it is important}} when we estimate {{the effects of the}} interfirm buyer–seller network on aggregate fluctuations in “Network effect on aggregate fluctuations” section. As noted in the Introduction, since our interest is in aggregate fluctuation we are interested not in each firm’s <b>log</b> growth <b>rate,</b> but in the average <b>log</b> growth <b>rate</b> of all firms in an economy at a particular year. Additional zero mean measurement errors for each firm disappear when we take the average of these growth rates, and thus have no impact on the overall dynamics of the average <b>log</b> growth <b>rate.</b> However, we are trying to estimate these underlying parameters from <b>log</b> growth <b>rates</b> including additional measurement errors. In this case, our estimated parameters (e.g., the parameter estimates reported in Table 4) would be different from the true structural parameters responsible for generating the aggregate fluctuations in the average <b>log</b> growth <b>rate</b> of firms.|$|R
50|$|Large {{areas will}} {{continue}} to be deforested if current <b>logging</b> <b>rates</b> and population growth do not decrease. An additional problem when land is deforested for agriculture is that forests often grow on poor soils that are unsuitable for agriculture. Therefore, farmers continue deforesting new areas when soil nutrients become too scarce to support crop farming.|$|R
40|$|Timber {{harvesting}} {{has always}} been a major business cost component for forest owners and or forest management companies. One major development in improving harvesting efficiency was the recognition that using independent contractors captured both the innovation skills as well as the motivation to improve productive effectiveness for those managing operations. Since the 1960 ’s most developed countries extensively use a contractual based harvesting workforce. What combines the forest manager and the harvesting contractor financially, through a contract, is the <b>logging</b> <b>rate.</b> In an idealized open market system the <b>logging</b> <b>rate</b> would be determined by supply and demand of services. However, a true open market system is rarely used to set harvesting rates. This paper discusses what constitutes a harvesting rate and reviews three different methodologies {{that can be used to}} develop them. Information was captured through interviews with company representatives in the USA and New Zealand, as well as reviewing relevant literature...|$|E
40|$|Copyright © 2014 Evaldo Muñoz Braz et al. This is an {{open access}} article {{distributed}} under the Creative Com-mons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, pro-vided the original work is properly cited. In accordance of the Creative Commons Attribution License all Copy-rights © 2014 are reserved for SCIRP {{and the owner of}} the intellectual property Evaldo Muñoz Braz et al. All Copyright © 2014 are guarded by law and by SCIRP as a guardian. Data of increment of the remnant trees after logging, ingrowth and mortality was obtained by assessment before logging and after 6 years, two sites of 50 ha, in Amazon forest. Logging scenarios were simulated to identify the <b>logging</b> <b>rate</b> potential for each studying site, by diameter class projection method. The cy-cle of 35 years and the <b>logging</b> <b>rate</b> of 30 m 3 ∙ha− 1 exceed the time required for recovery in the primary forest, in the studied site. The simulation showed that in the studying area, a well-planned logging, with minimum logging damage would be possible to implement an initial cycle of 25 years to the forest to re-cover 30 m 3 ∙ha− 1, if 50 % of the timber stock were reserved. The forest increment, beyond important fac-tors such as the increase of individual species, is quite dependent on the remnant trees...|$|E
40|$|The {{deformation}} behaviour {{of a model}} Ti-Mn alloy {{is described}} in terms of the deformation response of the component phases. Room temperature compression tests were conducted at different strain rates. A high <b>logging</b> <b>rate</b> thermocouple was employed to measure the real sample temperature during the experiments in order to study the effect of deformation heating. As expected, the 0. 2 % proof stress, sample temperature rise and strain rate sensitivity increased with the beta phase volume fraction. The alpha+beta titanium alloy was treated as a composite material and the rule of mixtures (ROM) was employed to describe the flow behaviour based on constitutive modelling of the component phases. A model implemented under iso-strain and iso-work assumptions gave the best prediction...|$|E
5000|$|Recover regular {{mortality}} rates by calculating the exponential of the forecasted <b>log</b> mortality <b>rates.</b>|$|R
3000|$|... fxt is the {{smoothed}} <b>logged</b> death <b>rates</b> at age x {{and time}} t, using weighted penalized regression splines.|$|R
5000|$|Use the {{forecast}} kt {{with the original}} bx and ax to calculate <b>logged</b> mortality <b>rates</b> for each forecast year.|$|R
40|$|Introduction At the Fermilab Tevatron proton and {{antiproton}} beams are {{collided with}} a center of mass energy of 1. 8 TeV. This is a very favorable environment to study B physics because the production cross section for b quarks is large and the resulting number of produced b hadrons is huge. An extrapolation from CDF cross section measurements indicates a total cross section of about 30 b in the central region (j j j ! 1), where CDF has most of its muon and tracking coverage. For typical Run I instantaneous luminosities (10 31 cm Γ 2 sec Γ 1) the bb production rate was 300 Hz, which was one order of magnitude larger than the CDF data <b>logging</b> <b>rate.</b> During run I the Tevatron delivered an integrated luminosity of 110 pb Γ 1 : summing the neutral and charged species and...|$|E
30|$|The videos, {{as well as}} {{the data}} files are saved onto an SD card. The <b>logging</b> <b>rate</b> is ten samples per second, which was {{considered}} as adequate enough to cover the needs of the experiment. As an example, an 8 -gigabyte high capacity SD card, logging video on the highest quality setting was able to log approximately 160  min of video. Apparently, this is an approximation, as the size of the recorded video depended on what was being recorded. The motion, color and complexity of the subject matter affected the size of the video file created. It should also be mentioned that the size of the data file was particularly small, in comparison {{to the size of the}} video file. The recorded data were downloaded daily right after the execution of the experiment from the flash memory to an external High Capacity HD that was given to the riders.|$|E
40|$|Introduction At the {{electron}} proton collider Hera at the Desy laboratory in Hamburg, positrons of 27. 6 GeV momentum are collided with 820 GeV protons. A schematic {{view of the}} H 1 detector 1 [...] {{one of the four}} big experiments operated at Hera [...] is shown in figure 1. The bunch crossing rate at Hera is 10. 41 MHz giving a bunch separation of 96 ns. The H 1 experiment is taking data at a <b>logging</b> <b>rate</b> of about 10 Hz or about 500 KB/s on average, resulting in a yearly raw data volume of about 5 TB. Data is reconstructed quasi-online, yielding another 5 TB of reconstructed data after physics oriented event classification. Raw and reconstructed data are kept on tape only. Part of the reconstructed event information is kept on disk in form of DST data. After software compression, the DST data amounts to about 300 GB 2; 3. These data volumes are a new challenge for experiments at HERA, even if thes...|$|E
3000|$|... fxti is the {{smoothed}} <b>logged</b> death <b>rates</b> at age x, time t {{and population}} i, using weighted penalized regression splines.|$|R
5000|$|... #Caption: Quarterly {{inflation}} in the Confederacy during the war. Inflation is calculated as <b>log</b> growth <b>rate</b> of Lerner's price index.|$|R
3000|$|... 2 are the {{marginal}} in-programme and post-programme effects, respectively, {{of participation in}} labour market programmes on the <b>log</b> hazard <b>rate</b> to employment.|$|R
40|$|Experiments at Fermilab {{require an}} ongoing program of {{development}} for high speed, distributed data acquisition systems. The physics {{program at the}} lab has recently started the operation of a Fixed Target run in which experiments are running the DART[1] data acquisition system. The CDF and DØ experiments are preparing {{for the start of}} the next Collider run in mid 2000. Each will read out on the order of 1 million detector channels. In parallel, future experiments such as BTeV R&D and Minos have already started prototype and test beam work. BTeV in particular has challenging data acquisition system requirements with an input rate of 1500 Gbytes/sec into Level 1 buffers and a <b>logging</b> <b>rate</b> of 200 Mbytes/sec. This paper will present a general overview of these data acquisition systems on three fronts � those currently in use, those to be deployed for the Collider Run in 2000, and those proposed for future experiments. It will primarily focus on the CDF and DØ architectures and tools...|$|E
40|$|The {{influence}} of felling {{on the distribution}} of rodents and their predators in a transitional coniferous-deciduous forest in northern Belarus was investigated in relation to stand age, forest type, and soil richness. The study was conducted in two areas differing by top-grounds (clay and sand soils) and, in turn, having different habitat carrying capacities. Three forest parts were investigated: 1) 10 %, 2) 20 - 30 %, and 3) 40 - 60 % covered by recent clearcuts. Three age classes of the clearcuts, namely 1) less than 2 years old, 2) 2 - 5 years old and 3) 6 - 12 years old, were considered. In total, we obtained data on small rodent numbers in 84 clearcuts, and the data on predators – in 67 clearcuts and the woodland parts differed by <b>logging</b> <b>rate.</b> Eventually, we became convinced that felling generally led to an increase in the abundance and species richness of rodents and their predators and that was attributable in the clearcuts aged up to 12 years. First, logging led to higher densities of Apodemus mice, the red fox Vulpes vulpes L., weasel Mustela nivalis L., tawny owl Strix aluco L., common buzzard Buteo buteo L. and adder Vipera berus L. Also, with the increased felling rate Microtus voles and the long-eared owl Asio otus L. penetrated in transitional woodlands. Too intensive forest harvesting (more than 40 % of recent clearcuts) led to the decline in the populations of several predatory species such as the pine marten Martes martes L., Tengmalm’s owl Aegolius funereus L., Ural owl Strix uralensis Pall., and pygmy owl Glaucidium passerinum L. The decline in rodent predators found in the conditions of too intensive <b>logging</b> <b>rate</b> was different in the woodlands on sand and clay top-grounds. In the conditions of clay soil too intensive felling led to the pronounced decline of a marked part of the rodent predatory guild inhabiting woodlands, and the species densities decreased to the level that was lower than the initial one. Conversely, in initially poor habitats in the woodland on sandy deposits, logging of any rate led to the increase in numbers of rodents and their predators compared to undisturbed forest. But moderate logging was found to be the most favourable for the community there...|$|E
40|$|A new FASTBUS-based {{calorimeter}} {{software trigger}} for the upgraded Mark II at the Stanford Linear Collider (SLC) is p resented. The trigger requirements for SLC {{and a short}} description of the hardware {{used for this purpose}} are given,-followed by {{a detailed description of the}} software. Some preliminary results are presented [11. - _ 1. MARK II AT SLC The new SLC was designed and built both to test linear collider technology [2] and for the production of 2 ’ bosons for physics analysis [3, 41. The beam-crossing frequency at SLC is sufficiently low that only a single level of triggering is required for an event <b>logging</b> <b>rate</b> of a few Hertz. 2. THE HARDWARE The upgraded Mark II detector [5] uses a lead-liquid argon sampling device as the barrel calorimeter and a lead-proportional tube device as the endcap calorimeter. A brief summary of the relevant facts is given here. Figure 1 shows the components of the trigger hardware. Invited talk presented at the Conference on Com,puting in High Energy Physics...|$|E
40|$|We {{consider}} the focusing quintic nonlinear Schrödinger equation posed on a rotationally symmetric surface, typically the sphere S^ 2 or the two dimensional hyperbolic space H^ 2. We prove {{the existence and}} the stability of solutions blowing up on a suitable curve with the log log speed. The Euclidean case is handled in Raphaël (2006) and our result shows that the <b>log</b> <b>log</b> <b>rate</b> persists in other geometries with the assumption of a radial symmetry of the manifold...|$|R
40|$|Includes bibliographical {{references}} (leaves 132 - 142). This research examines {{child mortality}} {{risk associated with}} short preceding birth intervals in Mozambique in quinquennial periods between 1978 to 1998 {{using data from the}} 1997 and 2003 DHS. A <b>log</b> <b>rate</b> model for piecewise constant rates is applied. The piecewise hazard function assumes a constant hazard rate of child mortality in each 6 month category of the preceding birth interval. The negative binomial regression model is applied to account for the overdispersion present in the Poisson model...|$|R
30|$|We <b>logged</b> <b>rate</b> {{limiting}} {{messages from}} the Twitter Streaming API and found that few tweets were omitted per day due to rate limiting. We experienced no rate limiting at all for 176 days and only slight rate limiting on other days (median 4.5 tweets lost on days with rate limiting messages). Power interruptions and network connectivity resulted in additional data loss, {{but there is no}} indication of any systematic bias from these interruptions. During our time window, 1, 980, 600 individual users sent a geolocated tweet which fell within our bounding box.|$|R
40|$|This item {{has been}} {{permanently}} {{restricted to the}} OSU Community at the author's request. Several online post-processing services compute high-accuracy geodetic coordinates from static global navigation satellite system (GNSS) data collected on a single mark. The accuracy of these services depends largely on {{the duration of the}} observational session. Five services (OPUS-S, AUSPOS, CSRS-PPP, GAPS, TrimbleRTX) were compared by processing the same 490 GNSS observational data files of varying session duration from 2 to 10 hours on six passive marks with minimal or moderate overhead obstructions. Only the global positioning system (GPS) observables at a 30 -second <b>logging</b> <b>rate</b> were initially tested. Afterwards, the effects of including data at faster logging rates and adding observables from Russia’s GNSS (i. e., GLONASS) were investigated by processing the same data files with both GPS and GLONASS observables enabled at logging rates of 30, 15, and 10 seconds in TrimbleRTX. The resulting coordinates from the post-processing were differenced with coordinates derived at each mark from a campaign-style static GNSS survey. Increasing the logging rates did not significantly reduce the root-mean-square error (RMS) of the differences; however, adding GLONASS observables reduced the RMS, for session durations no more than 4 hours, by approximately 29...|$|E
40|$|A {{survey of}} cable logging {{contracts}} {{was conducted in}} 5 of the 8 Alpine countries, namely: France, Germany, Italy, Slovenia and Switzerland. The goals of the survey were to set a general reference for cable logging rates, to identify eventual differences between countries and {{to determine the effect}} of technical work parameters (i. e. piece size, removal per hectare, line length) on actual contract rates. With a total sample size of 566 units, the mean removal and rate were 165 m 3 ha– 1 and 42. 9 € m– 3, respectively. Both removal per hectare and contract logging rates varied considerably and the study found significant differences between countries. Switzerland stood out from the group with the highest removal (345 m 3 ha– 1), but also the highest contract rate (79. 5 € m– 3). Removal per hectare was lowest in Italy with just 58. 3 m 3 ha– 1, and <b>logging</b> <b>rate</b> lowest in Slovenia at 29. 3 € m– 3. Logging rates were highly correlated with the average labour rate of each country. Technical factors such as tree size, line length and tract size explained about 40 % of the variability in logging rates. Therefore, 60 % of the variability is explained by other technical factors not included in our data and by non-technical factors, such as local market dynamics...|$|E
40|$|A thesis {{describes}} a virtualization of x 86 platform. In {{the first part}} the virtualization, its utilization and specific types of virtualization are described in a general way. The specific types of virtualization are full virtualization, paravirtualization, virtualization on core level, hardware virtualization and application virtualization. There is also described a general function of virtual machine, reasons of virtualization implementing, current development, focusing on future development and virtualization security. In the second part the thesis tries {{to find a solution}} with establishment charges of virtualization. To test purposes the following software programs were used. The first one is freeware benchmark Sandra from SiSoft company and second one is a software created {{on the basis of the}} thesis. For the tests the CPU overhead both an arithmetic-logical unit and a unit for work with numbers and floating decimal point, a RAM data transmission efficiency and a hard disk <b>logging</b> <b>rate</b> were chosen. The following test machines were described: VMWare, VirtualBox and Xen. In the third and the last part of the thesis is a description of network interface virtualization methods with a utilization of tun and tap system. In each particular step a virtual network interface creation and implementation are presented. Model situations of communication among virtual and physical machines with usage of these devices are described at the end of the thesis...|$|E
40|$|The {{standard}} errors {{of the natural}} <b>log</b> <b>rate</b> ratio (RR) were incorrectly input during the statistical process, leading to false-negative treatment results of interferon-beta (IFN-β) on annualized relapse rate [1]. The corrected statistical data showed that high-dose IFN-β significantly reduced annualized relapse rate at one year or two years of follow-up (RR = 0. 690, 95 %CI: 0. 600 - 0. 790, P = 0. 000, Figure 4 a, and RR = 0. 680, 95 %CI: 0. 610 - 0. 770, P = 0. 000, Figure 4 b, respectively) ...|$|R
30|$|Third, I use {{employment}} {{rates to}} capture the employment opportunities of natives—this strategy follows the studies by Altonji and Card (1991), Card (2001), Angrist and Kugler (2003), and Glitz (2012). For each skill group, I compute the <b>log</b> employment <b>rate</b> to labor force and the <b>log</b> employment <b>rate</b> to population. Note that the two employment rates can be combined to infer the participation rate of natives. At the numerator of the two employment rates, I use the number of native workers in full-time employment. This choice {{is consistent with the}} sample used to compute average wages per skill cell.|$|R
3000|$|... {{which is}} the {{weighted}} sum of the estimated variances for the estimated <b>log</b> relative <b>rate</b> of the exposed versus the unexposed over k groups. The details of deviation are given in the Appendix B.|$|R
40|$|In this thesis, it {{has been}} designed, {{constructed}} and tested an integrated measurement system for performing electric field strength measurements in the far-field, mounted on a drone, known as multicopters. The system uses a small portable spectrum analyzer with a single board computer that handles data from the flight controller on the drone and controls the measurements. The method used to solve the project is an experimental approach, a prototype was created to solve the problem. The system is tested and constructed for the Digital Audio Broadcast frequency band at (174 - 240) MHz, using a target frequency of 221. 208 MHz. The antennas selected for the measurements were two common DAB antennas for vehicles, one mounted on the roof with a magnetic base and one mounted on the windshield at the driver side. The measurement results {{show that there is}} some inaccuracy due to a low position <b>logging</b> <b>rate</b> of the Global Navigation Satellite System receiver on the drone. The results are used in Matlab to represent the radiation patterns of the antennas in both the azimuth and elevation plane, but with the use of interpolation to define the most likely form of the radiation diagram. It is also possible to see from the results where the directivity of the antennas is pointing. The results show that the logging of the positions should be improved to create a better resolution of the radiation patterns. It should also be developed a program in Matlab that can display the radiation patterns in three-dimensional graphs, in this way creating a better picture of the coverage of the antennas. Also, to make the system more versatile, it should be tested for other types of antennas and with other frequencies, so it can be explored if the system is durable for all types of antenna measurements...|$|E
40|$|The use of GPS {{technology}} {{for training and}} research purposes requires {{a study of the}} reliability, validity and accuracy of the data generated (Petersen et al., 2009). To date, studies have focused on devices with a <b>logging</b> <b>rate</b> of 1 Hz and 5 Hz (Coutts and Duffield, 2010; Duffield et al., 2010; Jennings et al., 2010; MacLeod et al., 2009; Petersen et al., 2009; Portas et al., 2010), although it seems that more frequent sampling can increase the accuracy of the information provided by these devices (Jennings et al., 2010; MacLeod et al., 2009, Portas et al., 2010). However, we are unaware of any study of the reliability and accuracy of GPS devices using a sampling frequency of 10 Hz. Thus, the aim of the present research was to determine the reliability and accuracy of GPS devices operating at a sampling frequency of 10 Hz, in relation here to sprints of 15 m and 30 m and using both video and photoelectric cells. Nine trained male athletes participated in the study. Each participant completed 7 and 6 linear runs of 15 m and 30 m, respectively (n = 117), with only one GPS device being used per participant. Each repetition required them to complete the route as quickly as possible, with 1 min recovery between sets. Distance was monitored through the use of GPS devices (MinimaxX v 4. 0, Catapult Innovations, Melbourne, Australia) operating at the above mentioned sampling frequency of 10 Hz. In addition, all tests were filmed with a video camera operating at a sampling frequency of 25 frames. Data were collected during what were considered to be good GPS conditions in terms of the weather and satellite conditions (number of satellites = 10. 0 ± 0. 2 and 10. 3 ± 0. 4 for sprints of 15 m and 30 m, respectively). Distance was measured using a tape measure. Electronic timing gates (TAG- Heuer, CP 520 Training model, Switzerland) were used to obtain a criterion sprint time accurate to 0. 01 s, with gates being placed {{at the beginning and end}} of the route (Petersen et al., 2009). Logan Plus v. 4. 0 software was used to synchronize the GPS files with the video, establishing the beginning of action when the participant crossed the initial photocell; this was then added to the duration obtained through the photoelectric cells. The accuracy of data within and between devices is shown in Table 1. The average values are close to those established in tests of 15 m and 30 m, with errors getting smaller when the devices were used over 30 m. The intra-device reliability is depicted in Figure 1, showing greater stability over 30 m than 15 m. The inter-device reliability yielded a CV = 1. 3 % and CV = 0. 7 % for sprints over 15 m and 30 m, respectively. To our knowledge this is the first study to assess the reliability and accuracy of GPS devices operating at a sampling frequency of 10 Hz. A further point of note is that studies of intra- and inter-device reliability for the same model of device (and therefore the same sampling rate) have traditionally used only two devices (Duffield et al., 2010; Petersen et al., 2009), whereas here a total of nine devices were studied. The distance data were found to be highly accurate and only slightly underestimated by the GPS devices. Furthermore, high intra- and inter-device reliability was observed. Accuracy improved with increased distance, and the mean SEM of 10. 9 % when running 15 m was reduced by half over 30 m (Table 1). Using similar statistics and methodology, Petersen et al., 2009 found SEM values of between 5 % and 24 % for MinimaxX devices, and between 3 % and 11 % with SPI-Pro devices, both at a sampling frequency of 5 Hz. Here, only one device (number 1) produced values above 6 % in the 15 m test, while another device (number 2) did so for runs of 30 m. We conclude that the increase in sampling frequency led to increased accuracy of the devices. As regards intra-device reliability, high values were obtained in all cases, and increased when used over 30 m (Figure 1). Some studies have reported differences between devices, even of the same model, suggesting that a player must always be monitored with the same device (Coutts and Duffield, 2010; Duffield et al., 2010). However, we only found small variations between devices, with a CV of 1. 3 % and 0. 7 % in runs of 15 m and 30 m, respectively. Therefore, we conclude that it is not always necessary to monitor players with the same device...|$|E
40|$|In this paper, {{we present}} an {{efficient}} failure recovery scheme for mobile applications based on movement-based checkpointing and logging. Current approaches take checkpoints periodically {{without regard to}} the mobility rate of the user. Our movementbased checkpointing scheme takes a checkpoint only after a threshold of mobility handoffs has been exceeded. The optimal threshold is governed by the failure <b>rate,</b> <b>log</b> arrival <b>rate,</b> and the mobility rate of the application and the mobile host. This allows the tuning of the checkpointing rate on a per application and per mobile host basis. We identify the optimal movement threshold which will minimize the recovery cost per failure {{as a function of the}} mobile node’s mobility rate, failure <b>rate</b> and <b>log</b> arrival <b>rate.</b> We also calculate the recoverability, i. e., the probability that the recovery can be done by a specified recovery time, and discuss the applicability of the approach...|$|R
40|$|Rain rate {{near the}} ground can be {{interpreted}} as a collection of random variables forming a random field parameterised by three co-ordinates: two spatial co-ordinates and one temporal. Interest among the radio propagation community has been focused on the first order statistics of these random variables, principally on the average annual probability distribution of rain rate as a function of location. However, increasingly interest is turning to second order statistics describing high-resolution rain rate variation in both time and space. This information is important for the design of fade countermeasures and for efficient spectrum management. In this paper radar data from a wide spread, slow moving and intense, convective rain event experienced by the South Eastern UK on 1 / 5 / 2001 are used to calculate the second order statistics of spatial-temporal <b>log</b> rain <b>rate</b> variation. It is demonstrated that <b>log</b> rain <b>rate</b> is self-similar in all three co-ordinates and that symmetry exists between the spatial and temporal variation. These data are used to develop an isotropic, spatial-temporal <b>log</b> rain <b>rate</b> model assuming that <b>log</b> rain <b>rate</b> is a homogeneous Gaussian random field. This model is developed further to yield closed form expressions for the second order statistics of the associated rain rate, specific attenuation and log specific attenuation random fields. Furthermore, expressions for the temporal covariance of the rain attenuation experienced by pairs of radio links are developed and these expressions are tested against measured rain attenuation data from an experimental link...|$|R
30|$|We {{deliberately}} computed {{the accuracy}} and the robustness measures directly for life expectancy at age 65, {{because of the}} use of this indicator in the pension reforms. For different contexts, e.g. life insurance and pension valuation, an evaluation of other outcomes (e.g. death rates or probabilities) would be relevant and could lead to different outcomes. That is, for different age groups the model fit, and subsequently, the choice of the jump-off rates might be different. Booth et al. (2006) compared both errors in life expectancy and <b>log</b> death <b>rates</b> when analysing the accuracy for different choices of the jump-off rate. They concluded that the accuracy in <b>log</b> death <b>rates</b> does not necessarily translate into accuracy in life expectancy. Analysis based on forecasted <b>log</b> death <b>rates</b> might therefore lead to different conclusions, but in general, last observed values as jump-off rates would give the most accurate forecast (Booth et al. 2006). The above indicates that the context of the forecast determines the outcome measure used in the analysis of the jump-off rates and, hence, the final choice for the best jump-off rates.|$|R
3000|$|... {{corresponds}} to the (<b>log)</b> exchange <b>rate</b> defined {{as the price of}} the foreign currency in terms of home currency units and △et+ 1 to the change in it. r^ω_t+ 1 is equal to the stock market return, and [...]...|$|R
