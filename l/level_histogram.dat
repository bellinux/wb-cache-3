71|84|Public
5000|$|Otsu’s method {{exhibits}} {{the relatively}} good performance if the histogram {{can be assumed}} to have bimodal distribution and assumed to possess a deep and sharp valley between two peaks. But if the object area is small compared with the background area, the histogram no longer exhibits bimodality. And if the variances of the object and the background intensities are large compared to the mean difference, or the image is severely corrupted by additive noise, the sharp valley of the gray <b>level</b> <b>histogram</b> is degraded. Then the possibly incorrect threshold determined by Otsu’s method results in the segmentation error. (Here we define the object size to be {{the ratio of the}} object area to the entire image area and the mean difference to be the difference of the average intensities of the object and the background) ...|$|E
40|$|The linear Kalman-Filter with {{appropriate}} modifications is introduced {{to solve the}} task of reconstruction of multiphase flow in pipes. The grey <b>level</b> <b>histogram</b> thresholding seg¬mentation described here was adopted to these images and applied as an alternative way of reconstructed image quantization by simple step-function...|$|E
30|$|In this section, {{we apply}} the Otsu method to {{automatically}} separate fractures and vugs {{from the rock}} matrix. The fundamental principle of the Otsu method is that we determine the optimal threshold to divide the gray <b>level</b> <b>histogram</b> of the images into two classes whose variance is the largest.|$|E
30|$|The basic {{pre-requisite}} condition {{prior to}} using the groundwater level data for geostatistical modeling is that the data should follow normal distribution. To check and confirm presence of normality in the pre- and post-monsoon groundwater <b>levels,</b> <b>histograms</b> were plotted and Shapiro–Wilk test was applied. All the {{statistical analyses were performed}} using STATISTICA software.|$|R
40|$|An {{automatic}} {{procedure to}} estimate proportions of components from grey <b>level</b> <b>histograms</b> is proposed. This procedure {{is based on}} statistical methods for parameter estimation in mixtures of normal distributions by maximum likelihood. The major advantage is that proportions of components can be estimated properly even when the grey level distributions of the components overlap considerably. ...|$|R
40|$|The {{binarization}} {{of transmission}} electron micrographs of thin granular films {{is an essential}} step for the statistical study of the film morphology {{in the framework of}} scaling laws and fractal theory. An image processing is developed, leading to an unambiguous optimum binarization threshold, without any loss of information. This treatment, efficient even on films with unimodal gray <b>level</b> <b>histograms,</b> is tested on different granular metallic or metal-dielectric films...|$|R
30|$|On {{the other}} hand, Hou et al. [11] used a CNN on patches of gigapixel Whole Slide Tissue Images to {{differentiate}} between cancer subtypes. The authors start by dividing the image into patches and classify each patch into discriminative/non-discriminative using a CNN and expectation maximization. They then use the patch-level predictions to create an image <b>level</b> <b>histogram</b> {{that is used to}} train a logistic regression classifier. This is an ES method.|$|E
30|$|On {{the other}} hand, Otsu’s method (Rodríguez 2006; Huang and Wang 2009) a {{high-speed}} and effective thresholding approach is applied for image binarization. In this method, it is mainly exploited to discriminate {{the background and}} objects on a gray <b>level</b> <b>histogram.</b> It has indeed the advantage to be highly efficient and demands less computation time {{when the number of}} classes is two (Tamim et al. 2015; Filipczuk et al. 2013).|$|E
40|$|This paper {{describes}} an automatic threshold selection method for picture segmentation, using {{the entropy of}} the grey <b>level</b> <b>histogram.</b> It is shown that, by an a priori maximation of an entropy determined a posteriori, a picture can successfully be thresholded into a two-level image. Several experimental results are presented to show {{the validity of the}} method. An extension to multithresholding and to multidimensional histogram processing is also discussed...|$|E
40|$|Abstract. In this article, {{scale and}} {{orientation}} invariant object detection is performed by matching intensity <b>level</b> <b>histograms.</b> Unlike other global measurement methods, the present one uses a local feature description that allows {{small changes in}} the histogram signature, giving robustness to partial occlusions. Local features over the object histogram are extracted during a Boosting learning phase, selecting the most discriminant features within a training histogram image set. The Integral Histogram {{has been used to}} compute local histograms in constant time. ...|$|R
40|$|Histogram Thresholding is {{an image}} {{processing}} technique whose aim is that of separating the objects and {{the background of the}} image into non overlapping regions. In gray scale images this task is obtained by properly detecting, on the corresponding gray <b>levels</b> <b>histogram,</b> the valleys that space out the concentration of the pixels around the characteristic gray levels of the different image structures. In this paper, a novel procedure will be discussed exploiting fuzzy set theory and fuzzy entropy to find automatically the optimal number of thresholds and their location in the image histograms...|$|R
40|$|Thresholding {{based on}} {{variance}} analysis of gray <b>levels</b> <b>histogram</b> {{is a very}} effective technology for image segmentation. However, its performance is limited in conventional forms. In this paper, a novel method based on two-dimensional extension of within-class variance is proposed to improve segmentation performance. The two-dimensional histogram of the original and local average image is projected to one-dimensional space firstly, and then the minimum within-class variance criterion is constructed for threshold selection. The effectiveness of the proposed method is demonstrated by using examples from the synthetic and real-word images...|$|R
40|$|Abstract. Most {{grey level}} {{thresholding}} methods produce good results {{only when the}} illumination of the image is homogeneous. An automatic binarization technique suitable for images containing regions of different brightness is presented. It does not use image’s grey <b>level</b> <b>histogram</b> {{as a source of}} information. Rather, the method is based on the transformation of the raster image {{in such a way that}} the transformed image can be easy thresholded. A comparison with histogram based threshold selection technique is given. ...|$|E
40|$|In this paper, {{the problem}} of {{classification}} of defects occurring in a textile manufacture is addressed. A new classification scheme is devised in which different features, extracted from the gray <b>level</b> <b>histogram,</b> the shape, and cooccurrence matrices, are employed. These features are classified using a Support Vector Machines (SVM) based framework, and an accurate analysis of different multiclass classification schemes and SVM parameters has been carried out. The system has been tested using two textile databases showing very promising results. 1...|$|E
40|$|A novel {{method of}} 2 -D entropic {{segmentation}} using a {{linear discriminant function}} is presented. This segmentation method automatically highlights desired objects against background with no user input or parameter specification. Improved class separation, and therefore, reduced classification error {{can be obtained by}} adding a second feature to the gray <b>level</b> <b>histogram.</b> Examples of aerial and medical images were segmented using features such as intensity, and fractal error or standard deviation with lower segmentation errors than 1 -D and 2 -D entropy-based methods. < 3 1999 Elsevier Science B. V. All rights reserved...|$|E
40|$|Presentado al 3 rd Iberian Conference (IbPRIA- 2007) celebrado en Girona (Spain) del 6 al 8 de junio. In this article, {{scale and}} {{orientation}} invariant object detection is performed by matching intensity <b>level</b> <b>histograms.</b> Unlike other global measurement methods, the present one uses a local feature description that allows {{small changes in}} the histogram signature, giving robustness to partial occlusions. Local features over the object histogram are extracted during a Boosting learning phase, selecting the most discriminant features within a training histogram image set. The Integral Histogram {{has been used to}} compute local histograms in constant time. Peer Reviewe...|$|R
40|$|The {{object of}} this bachelor‘s thesis is create an {{application}} for editing the histogram. The program is realized by programming language C++ and programming tool Microsoft Visual C++ 2005 Express Edition. It defines fundamental terms for this work as histogram or transformation of grey level scale. Above all, it analyses individual grey level transformations, which used for editing the histogram. It describes designing and realization of application. Input data of program is any input digital image, which user order, selected grey level transformation and its parameters. Output data are input image in grey <b>levels,</b> <b>histogram</b> input image, transformed input image and its histogram. Output images can be saved...|$|R
40|$|AbstractPrior to genome sequencing, {{information}} on base composition (GC level) and its variation in mammalian genomes {{could be obtained}} using density gradient ultracentrifugation. Analyses using this approach led {{to the conclusion that}} mammalian genomes are organized into mosaics of fairly homogeneous regions, called isochores. We present an initial compositional overview of the chromosomes of the recently available draft human genome sequence, in the form of color-coded moving window plots and corresponding GC <b>level</b> <b>histograms.</b> Results obtained from the draft human genome sequence agree well with those obtained or deduced earlier from CsCl experiments. The draft sequence now permits the visualization of the mosaic organization of the human genome at the DNA sequence level...|$|R
40|$|We {{describe}} a component based face detection system trained only on positive examples. On the first layer, SVM classifiers detect predetermined rectangular portions of faces in gray scale images. On the second <b>level,</b> <b>histogram</b> based classifiers judge the pattern using only {{the positions of}} maximization of the first level classifiers. Novel aspects of our approach are: a) using selected parts of the positive pattern as negative training for component classifiers, b) The use of pair wise correlation between facial component positions to bias classifier outputs and achieve superior component localization...|$|E
40|$|Abstract. Mammography is {{currently}} the optimal technique for reliable detection of early, curable breast cancer. Detection and/or characterization are demanding tasks, especially in case of low contrast spiculated masses. In this study, we investigate the differentiation of dense regions containing spiculated masses from regions of normal dense tissue, by means of feature analysis on wavelet-processed mammograms. The biorthogonal 2 D wavelet transform with compactly supported spline wavelets was used to decompose regions of interest of mammograms originating from DDSM database. A three-dimensional feature vector of first order statistics ({{mean and standard deviation}} of gray <b>level</b> <b>histogram</b> from low-frequency wavelet coefficients and standard deviation of gradient-orientation histogram from high-frequency wavelet coefficients) was extracted to differentiate spiculated masses from normal dense tissue. Additionally, multiresolution texture features of second order statistics were extracted from spatial Gray Level Co-occurrence Matrices of wavelet coefficients at different scales with fixed distance between pixel pairs at each scale. Initial results in a pilot sample demonstrate that mean and standard deviation of gray <b>level</b> <b>histogram</b> as well as standard deviation of gradient-orientation histogram differentiates spiculated masses from normal dense tissue at all scales (Students ’ t-test, p< 0. 05). Texture features of second order statistics, such as mean and range of Inverse Difference Moment, Entropy and Contrast as well as mean of Angular Second Moment, Difference Entropy and Sum Entropy at th...|$|E
40|$|Ultrasound image {{uniformity}} is a parameter {{often used}} in medical ultrasound system testing, as an object can be displayed in different shapes and textures {{within the field of}} view, depending on instrumentation performances. Therefore Ultrasound Image Uniformity evaluation can be used for failures detection as well for quality assurance. In this paper a novel method is developed to measure B-mode image uniformity over the whole field of view or its part (Region Of Interest) : {{it is based on the}} image gray <b>level</b> <b>histogram</b> weighted by a sigmoid function to detect non uniformities. Preliminary results are explained and discussed...|$|E
40|$|Best-so-far ABC is a {{modified}} version of the artificial bee colony (ABC) algorithm used for optimization tasks. This algorithm is one of the swarm intelligence (SI) algorithms proposed in recent literature, in which the results demonstrated that the best-so-far ABC can produce higher quality solutions with faster convergence than either the ordinary ABC or the current state-of-the-art ABC-based algorithm. In this work, we aim to apply the best-so-far ABC-based approach for object detection based on template matching by using the difference between the RGB <b>level</b> <b>histograms</b> corresponding to the target object and the template object as the objective function. Results confirm that the proposed method was successful in both detecting objects and optimizing the time used to reach the solution...|$|R
40|$|Fine and sparse details, {{as defined}} in this study, occur in several {{applications}} where image processing can be applied. The definition of fine and sparse details is given {{in the domain of}} gray <b>level</b> <b>histograms</b> by establishing sufficient statistical properties. In experiments, artificial data was generated from a model based on the definition, and several thresholding methods were compared. Based on the experiments on artificial data, it seems that the minimum error thresholding by Kittler and Illingworth outperforms the other methods. The result was verified using images from IGT picking paper printability assessment where small surface defects must be detected. Furthermore, image enhancement and a minor adjustment to the minimum error thresholding are introduced to achieve optimal performance in IGT picking paper printability assessment. ...|$|R
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Best-so-far ABC is {{a modified version}} of the artificial bee colony (ABC) algorithm used for optimization tasks. This algorithm is one of the swarm intelligence (SI) algorithms proposed in recent literature, in which the results demonstrated that the best-so-far ABC can produce higher quality solutions with faster convergence than either the ordinary ABC or the current state-of-the-art ABC-based algorithm. In this work, we aim to apply the best-so-far ABC-based approach for object detection based on template matching by using the difference between the RGB <b>level</b> <b>histograms</b> corresponding to the target object and the template object as the objective function. Results confirm that the proposed method was successful in both detecting objects and optimizing the time used to reach the solution. 1...|$|R
40|$|Iris recognition, a {{relatively}} new biometric technology, has great advantages, such as variability, stability and security, thus is the most promising for high security environment. Iris recognition is proposed in this report. We describe some methods, {{the first one is}} based on grey <b>level</b> <b>histogram</b> to extract the pupil, the second is based on elliptic and parabolic HOUGH transformation to determinate the edge of iris, upper and lower eyelids, the third we used 2 D Gabor Wavelets to encode the iris and finally we used the Hamming distance for authentication. Comment: 7 pages, 13 figures,International Multi-Conference on Systems Signals and Device...|$|E
40|$|An {{efficient}} image {{retrieval system}} for use in medical applications is proposed. The system addresses both global and local features {{and is able to}} perform similarity retrieval on the whole image as well as on local regions within the image. The retrieval algorithm is based on a simple grey <b>level</b> <b>histogram</b> (GLH) algorithm to capture the intensity distribution, and a novel block-based retrieval technique to localize the image. The algorithm is tested on a database of computed tomography (CT) images of head provided by a local hospital. Promising results are reported which pave the way for the integration of the block-based method with a more advanced feature extraction method...|$|E
40|$|This paper {{describes}} {{the contribution of}} the TZI to the shot detection task of the TREC 2003 video analysis track (TRECVID). The approach comprises a feature extraction step and a shot detection step. In the feature extraction, three features are extracted: a frequency-domain approach based on FFT-features, a spatial-domain approach based on changes in the image luminance values, and another spatial domain approach based on gray <b>level</b> <b>histogram</b> differences. Shot boundary detection uses then adaptive thresholds based on all extracted features of the complete video. The final shot list is a combination of shots which result from an independent examination of all three features...|$|E
40|$|Includes bibliographical {{references}} (pages 99 - 100) Includes digitized image(s) in TIFF format. The project {{entails the}} design and implementation of a real time histogram generating and processing system for use in image processing. This system involved {{the use of a}} micro controller and Host Intel 3000 bit slice micro computer. Two independent 12 bit intensity <b>level</b> <b>histograms</b> (frequency of occurrence) are generated for two separate areas of an image. For each of the two histograms, three percentile points are calculated in real time and returned to the Host Intel 3000 for further processing. The hardware consists of two cards, one containing the micro controller, accumulator, and incrementor, the other containing the interfacing hardware and sample counters. Software for the micro controller was produced manually using timing diagrams, while software for the Intel 3000 micro computer was made using an assembler written to produce the Intel 3000 micro code instruction set...|$|R
3000|$|... (q) denotes a {{histogram}} {{value for}} the pth frame, with q being one of G (256 for intensity and 360 for hue) possible <b>levels</b> of the <b>histogram.</b> S [...]...|$|R
40|$|We {{study the}} European river Danube and the South American river Negro daily water levels. We present a {{fit for the}} Negro daily water level period and {{standard}} deviation. Unexpectedly, we discover that the river Negro and Danube are mirror rivers {{in the sense that}} the daily water <b>levels</b> fluctuations <b>histograms</b> are close to the BHP and reversed BHP, respectively...|$|R
40|$|Abstract [...] In this paper, we {{show that}} Otsu's image {{thresholding}}, Kittler and Illingworth's minimum error thresholding, and Huang and Wang's fuzzy thresholding methods can be derived under a similar mathematical formulation. The difference among the three methods is the choice of different weighting functions for computing a criterion function that {{can be considered as}} a weighted summation of the image gray <b>level</b> <b>histogram.</b> We can {{have a better understanding of}} the three thresholding techniques and derive other thresholding methods based on this unified formulation. Copyright © 1996 Pattern Recognition Society. Published by Elsevier Science Ltd. Image thresholding Otsu's method Minimum error thresholding Fuzzy thresholding method Unified formulation of thresholding methods 1...|$|E
40|$|Abstract. Chan-Vese {{model is}} one of {{classical}} active contour models for segmentation based on level set methods. It is the region-based model {{but in some cases}} it is still sensitive to the location of initial contours. The image thresholding is a simple but effective tool to separate objects from the background. In this paper, we integrate these two techniques and propose a new method to improve the initialization for Chan-Vese Model. First analyze the distribution of image gray <b>level</b> <b>histogram</b> and find the optimum threshold values, then set the model’s initial contours with thresholds and construct energy functional, lastly iterate the functional formulations until convergence to the object boundary. The method is tested on the plaque images and gives considerable increase in performance...|$|E
40|$|Canny is {{a classic}} {{algorithm}} of edge detection which has been widely applied in various fields of image processing for years. However, the algorithm has some defects. The most serious defect is that the traditional canny algorithm can’t set threshold adaptively. If the threshold set manually is not accurate, it will seriously {{affect the quality of}} the algorithm to detect the edge. This makes the poor adaptability of the algorithm. This paper proposes a method which combines maximum entropy method with Otsu method to determine the high and low threshold of Canny algorithm. Experiments show that the modified algorithm has stronger robustness than traditional method. For the images which have complex distributions of grey <b>level</b> <b>histogram,</b> the modified algorithm has better performance...|$|E
40|$|Abstract — The main {{objective}} of Image Segmentation is to partition an image into different parts. Image segmentation basically {{used to detect}} the edges and boundaries. This is done to simplify and/or change the representation of an image in a more meaningful and easier way. Many image segmentation techniques {{are available in the}} literature. Some of them used gray <b>level</b> <b>histograms,</b> some used spatial and some used thresholding techniques. Under thresholding techniques there are different methods. One of those methods is entropy. Entropy is a measure of unpredictability. A good segmentation will be one that maximize the uniformity of pixels within the regions and minimize the uniformity across the regions. So we can say that entropy is a natural characteristic to be incorporated in evaluation function. This paper attempts to provide a brief review for image segmentation using entropy. Index Terms— 2 D and 3 D images, entropy, image segmentation, thresholding. I...|$|R
40|$|Abstract. In {{this paper}} we show how genetic {{programming}} {{can be used}} to discover useful texture feature extraction algorithms. Grey <b>level</b> <b>histograms</b> of different textures are used as inputs to the evolved programs. One dimensional K-means clustering is applied to the outputs and the tightness of the clusters is used as the fitness measure. To test generality, textures from the Brodatz library were used in learning phase and the evolved features were used on classification problems based on the Vistex library. Using the evolved features gave a test accuracy of 74. 8 % while using Haralick features, the most commonly used method in texture classification, gave an accuracy of 75. 5 % on the same problem. Thus, the evolved features are competitive with those derived by human intuition and analysis. Furthermore, when the evolved features are combined with the Haralick features the accuracy increases to 83. 2 %, indicating that the evolved features are finding texture regularities not used in the Haralick approach. ...|$|R
40|$|Color based image {{retrieval}} is {{an important}} and challenging problem in image and object classification. Many techniques work by predefining a number of dimensions to reduce an original histogram and evaluating the image similarities using norms defined on the reduced spaces. Metric histograms, on the other hand, do not predefine this number, and explore the correlations between significant points and their neighborhoods {{in order to find}} a small number of control points to represent the histogram. It lacks though a proper way to deal with color images, since it considers only normalized gray <b>level</b> <b>histograms.</b> In this paper we propose tridimensional metric histograms for considering the color space. We introduce a procedure to compute a parameter to span the range of inflection points between minimum and maximum for the specific data. An extended distance metric for it is also presented. Experiments ran with a database of 2090 color images show better performance of the proposed approach than the original one. 1...|$|R
