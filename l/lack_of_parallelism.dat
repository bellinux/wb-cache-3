56|10000|Public
25|$|The in-core {{ceilings}} are roofline-like curve {{beneath the}} actual roofline {{that may be}} present {{due to the lack}} of some form of parallelism. These ceilings effectively limit how high performance can reach. Performance cannot exceed an in-core ceiling until the underlying <b>lack</b> <b>of</b> <b>parallelism</b> is expressed and exploited. The ceilings can be also derived from architectural optimization manuals other than benchmarks.|$|E
2500|$|<b>Lack</b> <b>of</b> <b>parallelism</b> of beds on the {{opposite}} sides of the contact:Angular unconformity ...|$|E
2500|$|The side-pivot mount, {{which used}} two pivots– {{one on each}} side of the helmet, is shown in funerary monuments and other {{pictorial}} or sculptural sources of the 1340s. One of the early depictions of a doubly pivoted visor on a bascinet is the funerary monument of Sir Hugh Hastings (d. 1347) in St. Mary's Church, Elsing, Norfolk, England. [...] The pivots were connected to the visor by means of hinges to compensate for any <b>lack</b> <b>of</b> <b>parallelism</b> between the pivots. The hinges usually had a removable pin holding them together, this allowed the visor to be completely detached from the helmet, if desired. The side-pivot system was commonly seen in Italian armours.|$|E
30|$|The <b>lack</b> <b>of</b> notion <b>of</b> <b>parallelism</b> among systems {{makes it}} {{difficult}} to represent system compositions in a plain diagram. Additionally, the <b>lack</b> <b>of</b> notion of multiplicity of systems and actions hinders, for example, the representation of how many systems are affected by an event/action and how many systems have a similar state. Also, the <b>lack</b> <b>of</b> notion of interaction {{makes it difficult}} to model relationships among states of different systems (see the “...” in Fig. 2).|$|R
50|$|These {{instances}} of parasitic gaps are all marginal to varying degrees. The marginality is {{probably due to}} the <b>lack</b> <b>of</b> syntactic <b>parallelism</b> indicated by the brackets, the gaps no longer appearing {{on the same side of}} the brackets. In any case, there is a noticeable drop in acceptability when the parallelism in the examples further above is removed. What exactly explains this drop in acceptability is not entirely clear, although it may have to do with ease of processing. Parallel structures are easier for humans to process, and hence parasitic gaps are reliant on a low processing load.|$|R
40|$|A snake-in-the box code, first {{described}} in [Kautz, 1958], is an achordal open path in a hypercube. Finding bounds {{for the longest}} snake at each dimension {{has proven to be}} a difficult problem in mathematics and computer science. Evolutionary techniques have succeeded in tightening the bounds of longest snakes in several dimensions [Potter, 1994] [Casella, 2005]. This thesis utilizes an Iterated Local Search heuristic with adaptive memory on the snake-in-the-box problem. The results match the best published results for problem instances up to dimension 8. The <b>lack</b> <b>of</b> implicit <b>parallelism</b> segregates this experiment from previous heuristics applied to this problem. As a result, this thesis provides insight into those problems in which evolutionary methods dominate...|$|R
5000|$|<b>Lack</b> <b>of</b> <b>parallelism</b> of beds on the {{opposite}} sides of the contact:Angular unconformity ...|$|E
50|$|The in-core {{ceilings}} are roofline-like curve {{beneath the}} actual roofline {{that may be}} present {{due to the lack}} of some form of parallelism. These ceilings effectively limit how high performance can reach. Performance cannot exceed an in-core ceiling until the underlying <b>lack</b> <b>of</b> <b>parallelism</b> is expressed and exploited. The ceilings can be also derived from architectural optimization manuals other than benchmarks.|$|E
5000|$|The side-pivot mount, {{which used}} two pivots - {{one on each}} side of the helmet, is shown in funerary monuments and other {{pictorial}} or sculptural sources of the 1340s. One of the early depictions of a doubly pivoted visor on a bascinet is the funerary monument of Sir Hugh Hastings (d. 1347) in St. Mary's Church, Elsing, Norfolk, England. [...] The pivots were connected to the visor by means of hinges to compensate for any <b>lack</b> <b>of</b> <b>parallelism</b> between the pivots. The hinges usually had a removable pin holding them together, this allowed the visor to be completely detached from the helmet, if desired. The side-pivot system was commonly seen in Italian armours.|$|E
40|$|Due to the <b>lack</b> <b>of</b> {{operational}} <b>parallelism</b> {{and structured}} data storage/retrieval, limited search trellis decoding algorithms have been traditionally ruled out for applications demanding high throughput convolutional code decoding. Among various limited search algorithms, the T-algorithm performs breadth-first limited search and has good potential for parallel decoding. In this paper, we propose two techniques at the algorithm and VLSI architecture levels for the T-algorithm {{to improve the}} decoding parallelism and tackle the data storage/retrieval problem, which enables the high throughput path-parallel T-algorithm decoder VLSI implementation. This work provides a vehicle for exploiting {{the merits of the}} T-algorithm, i. e., low computational complexity that is adaptive to the channel distortion, in high throughput applications. 1...|$|R
40|$|In {{the era of}} {{multi-core}} computing, {{the push}} for creating true parallel applications that can run on individual CPUs is on the rise. Application of parallel discrete event simulation (PDES) to hardware design verification looks promising, given the complexity of today’s hardware designs. Unfortunately, the challenges imposed by <b>lack</b> <b>of</b> inherent <b>parallelism,</b> suboptimal design partitioning, synchronization and communication overhead, and load balancing, render this approach largely ineffective. This thesis presents three techniques for accelerating simulation at three levels of abstraction namely, RTL, functional gate-level (zero-delay) and gate-level timing. We review contemporary solutions and then propose new ways of speeding up simulation at the three levels of abstraction. We demonstrate {{the effectiveness of the}} proposed approaches on several industrial hardware designs...|$|R
40|$|Tuning the {{performance}} of applications requires understanding the interactions between code and target architecture. This paper describes a performance modeling approach that not only makes accurate predictions about the behavior of an application on a target architecture for different inputs, but also provides guidance for tuning by highlighting the factors that limit performance in each section of a program. We introduce two new performance metrics that estimate the maximum gain expected from tuning different parts of an application, or from {{increasing the number of}} machine resources. We show how this metric helped identify a bottleneck in the ASCI Sweep 3 D benchmark where the <b>lack</b> <b>of</b> instruction-level <b>parallelism</b> limited performance. Transforming one frequently executed loop to ameliorate this bottleneck improved performance by 16 % on an Itanium 2 system. ...|$|R
40|$|A {{radioimmunoassay}} for {{the measurement}} of gonadotrophin releasing hormone (GnRH) in plasma and urine using readily available reagents was developed. The GnRH assay showed good precision, recovery, and parallelism {{over a wide range}} of GnRH concentrations with a sensitivity of 15 pg/ml. The assay was compared with a commercially available kit (Buhlmann Laboratories). Although the Buhlmann kit showed acceptable precision, recovery, sensitivity, and correlation with the developed GnRH assay for plasma samples, <b>lack</b> <b>of</b> <b>parallelism</b> of serially diluted plasma and urine samples was consistently observed, together with a poor correlation with the developed GnRH assay for urine, suggesting a matrix effect with the Buhlmann kit. The developed assay is suitable for measuring GnRH in samples obtained from patients receiving pulsatile infusions of GnRH. In contrast, the commercially available Buhlmann kit was unsuitable for measuring plasma GnRH as the kit had a top standard of only 160 pg/ml, well below the peak plasma concentration. It would not be possible to dilute samples for analysis because of the <b>lack</b> <b>of</b> <b>parallelism</b> of diluted samples compared with standards obtained with the Buhlmann assay...|$|E
30|$|Rutkunas et al. [24], {{showed the}} open tray {{technique}} {{to be more}} accurate with highly non-axially oriented implants. Jang et al. [25] found that various implant divergent angles (0, 5, 10, 15, and 20 °), particularly those with the internal connection, were more accurately recorded with the open tray technique. It seems that while unfavorable parallelism may be corrected prosthetically, the <b>lack</b> <b>of</b> <b>parallelism</b> still creates a path of removal that may distort the impression material, leading to an inaccurate master cast.|$|E
30|$|The {{adoption}} of tilted implants for {{the rehabilitation of}} both edentulous mandibles and maxillae has been proposed in the recent years. In the mandible, tilting of the distal implants may prevent damage to the mandibular nerve. Implants of conventional length can be placed, allowing engagement of as much cortical bone as possible, thus increasing primary stability [11]. However, the <b>lack</b> <b>of</b> <b>parallelism</b> between implants may result in increased distortion of impression material during removal from the mouth that may generate an inaccurate model [12 – 14].|$|E
40|$|This paper {{addresses}} {{the problem of}} measuring and analyzing the performance of fine-grained parallel programs running on shared-memory multiprocessors. Such processors use locking (either directly in the application program, or indirectly in a subroutine library or the operating system) to serialize accesses to global variables. Given sufficiently high rates of locking, the chief factor preventing linear speedup (besides <b>lack</b> <b>of</b> adequate inherent <b>parallelism</b> in the application) is lock contention - the blocking of processes {{that are trying to}} acquire a lock currently held by another process...|$|R
40|$|A {{number of}} compute-intensive {{applications}} suffer from performance loss {{due to the}} <b>lack</b> <b>of</b> instruction-level <b>parallelism</b> in sequences <b>of</b> dependent instructions. This is particularly accurate on wide-issue architectures with large register banks, when the memory hierarchy (locality and bandwidth) is not the dominant bottleneck. We consider two real applications from computational biology and from cryptanalysis, characterized by long sequences of dependent instructions, irregular control-flow and intricate scalar and array dependence patterns. Although these applications exhibit excellent memory locality and branch-prediction behavior, state-ofthe -art loop transformations and back-end optimizations are unable to exploit much instruction-level parallelism. We show that good speedups can be achieved through deep jam, a new transformation of the program control- and data-flow. Deep jam combines scalar and array renaming with a generalized form of recursive unrolland -jam; it brings together independent instructions across irregular control structures, removing memory-based dependences. This optimization contributes to the extraction <b>of</b> fine-grain <b>parallelism</b> in irregular applications. We propose a feedback-directed deep jam algorithm, selecting a jamming strategy, function of the architecture and application charactristics...|$|R
40|$|In {{this paper}} we study the {{critical}} behavior of an N-component ϕ^ 4 -model in hyperbolic space, {{which serves as}} a model of uniform frustration. We find that this model exhibits a second-order phase transition with an unusual magnetization texture that results from the <b>lack</b> <b>of</b> global <b>parallelism</b> in hyperbolic space. Angular defects occur on length scales comparable to the radius of curvature. This phase transition is governed by a new strong curvature fixed point that obeys scaling below the upper critical dimension d_uc= 4. The exponents of this fixed point are given by the leading order terms of the 1 /N expansion. In distinction to flat space no order 1 /N corrections occur. We conclude that the description of many-particle systems in hyperbolic space is a promising avenue to investigate uniform frustration and non-trivial critical behavior within one theoretical approach. Comment: 12 pages, 5 figure...|$|R
40|$|In two {{consecutive}} series of hypertensive patients the hypotensive effect, the hyporeninaemic effect and the blockade of cardiac beta-receptors was studied using weekly increasing doses of propranolol or atenolol. With both beta-blockers, cardiac blockade and hypotensive effect increased {{in a parallel}} fashion when the dosage was increased suggesting that the hypotensive effect is related to cardiac beta-blockade. On the other hand <b>lack</b> <b>of</b> <b>parallelism</b> between the hypotensive effect and the hyporeninaemic effect suggests that the hypotensive effect {{was not related to}} a major extent to the hyporeninaemic effect of the drugs in the dosage range studied here. status: publishe...|$|E
40|$|Abstract: The {{main problem}} {{concerning}} the hardware implementation of turbo codes is the <b>lack</b> <b>of</b> <b>parallelism</b> in the MAP-based decoding algorithm. This paper proposes {{to overcome this}} problem with a new family of turbo codes, named Slice Turbo Codes. This family is based on two ideas: the encoding of each dimension with P independent tail-biting codes and a constrained interleaver structure that allows parallel decoding of the P independent codewords in each dimension. The optimization of the interleaver is described. A high degree of parallelism is obtained with equivalent or better performance than the best known turbo codes. The parallel architecture allows reduced complexity turbo decoder for very high throughput applications...|$|E
40|$|Achieving high {{performance}} in cryptographic processing is important {{due to the}} increasing connectivity among today's computers. Despite steady improvements in microprocessor and system performance, private-key cipher implementations continue to be slow. Irrespective of the cipher used, {{the main reason for}} the low performance is <b>lack</b> <b>of</b> <b>parallelism,</b> which fundamentally comes from encryption modes such as the Cipher Block Chaining (CBC) mode. In CBC, each plaintext block is XOR'ed with the previous ciphertext block and then encrypted, essentially inducing a tight recurrence through the ciphertext blocks. To deliver {{high performance}} while maintaining high level of security assurance in real systems, the cryptography community has proposed Interleaved Cipher Block Chaining (ICBC) mode...|$|E
40|$|This paper {{primarily}} disputes Dreyfus’s {{account of}} Heidegger’s critique of Husserl’s theory of intentionality. Specifically, it raises {{objections to the}} three central claims of such an account; namely: that Searle’s theory of intentional action {{can be used as}} a stand-in for Husserl’s; that Heidegger rejects the primordiality of the intentionality of consciousness; and that Heidegger distinguishes between conscious and unconscious types of intentional actions and he privileges the latter over the former. I show the first to be unwarranted owing to a <b>lack</b> <b>of</b> fundamental <b>parallelisms</b> between Searle’s and Husserl’s theories of intentionality. I show the second to be mistaken for failing to take into account Heidegger’s strategic handling of the concept of consciousness and for contradicting Heidegger’s concept of care as the essential meaning of Dasein’s being-in-the-world. Lastly, I show the third to be highly problematic for lacking in textual evidence and explanatory power...|$|R
40|$|Matrix {{transposition}} is {{an important}} algorithmic building block for many numeric algorithms such as FFT. It has also been used to convert the storage layout of arrays. With more and more algebra libraries offloaded to GPUs, a high per-formance in-place transposition becomes necessary. Intu-itively, in-place transposition should be {{a good fit for}} GPU architectures due to limited available on-board memory ca-pacity and high throughput. However, direct application of CPU in-place transposition algorithms <b>lacks</b> the amount <b>of</b> <b>parallelism</b> and locality required by GPUs to achieve good performance. In this paper we present the first known in-place matrix transposition approach for the GPUs. Our im-plementation is based on a novel 3 -stage transposition al-gorithm where each stage is performed using an elementar...|$|R
40|$|Abstract. Computers {{are very}} {{important}} for all of us. But brute force disruptive architectural develop-ments in industry and threatening unaffordable operation cost by excessive power consumption are a mas-sive future survival problem for our existing cyber infrastructures, which we must not surrender. The pro-gress of performance in high performance computing (HPC) has stalled because of the „programming wall“ caused by <b>lacking</b> scalability <b>of</b> <b>parallelism.</b> This chapter shows that Reconfigurable Computing is the sil-ver bullet to obtain massively better energy efficiency as well as much better performance, also by the up-coming methodology of HPRC (high performance reconfigurable computing). We need a massive cam-paign for migration of software over to configware. Also because <b>of</b> the multicore <b>parallelism</b> dilemma, we anyway need to redefine programmer education. The impact is a fascinating challenge to reach new hori-zons of research in computer science. We need {{a new generation of}} talented innovative scientists and engi-neers to start the beginning second history of computing. This paper introduces a new world model. In Reconfigurable Computing, e. g. by FPGA (Table 15), practically everything can be implemented which is running on traditional computing platforms. For instance, recently the historical Cray 1 supercomputer has been reproduced cycle-accurate binary-compatible using a single Xilinx Spartan- 3 E 1600 developmen...|$|R
40|$|The {{common use}} of global {{positioning}} systems, and specially GPS, in work like survey, cartography, photogrammmetry, LIDAR [...] . make it necessary have a gemid model to transform ellipsoidal to orthometric heights used in engineering. Because of the <b>lack</b> <b>of</b> <b>parallelism</b> between both reference systems {{not to have}} these models can produce important errors since the variation can take values of 10 cm/Km or more in some areas, for example in the SW of Spain. If we do not consider this difference in the elaboration of DTM used to design substructure can happen that in gravity canalization the water does not arrive at the wanted poin...|$|E
40|$|The {{main problem}} with the {{hardware}} implementation of turbo codes is the <b>lack</b> <b>of</b> <b>parallelism</b> in the MAP-based decoding algorithm. This paper proposes to overcome this problem with a new family of turbo codes, called Slice Turbo Codes. This family is based on two ideas: the encoding of each dimension with P independent tail-biting codes and a constrained interleaver structure that allows parallel decoding of the P independent codewords in each dimension. The optimization of the interleaver is described. A high degree of parallelism is obtained with equivalent or better performance than the best known turbo codes. The parallel architecture allows reduced complexity turbo decoding for very high throughput applications...|$|E
40|$|Previous {{results are}} {{contradictory}} regarding the concen-tration of thyroxin in human milk. Using a sensitive radio-immunoassay, {{we have found}} a <b>lack</b> <b>of</b> <b>parallelism</b> be-tween the standard curve for thyroxin and the curve for serial dilutions of whole human milk, skimmed milk, or ethanol extracts of milk. Nonspecific binding also indicated the presence of analytical artifacts. Thus we have sepa-rated thyroxin from other milk components {{by means of a}} strongly basic Bio-Rad anion-exchange resin with qua-ternary ammonium exchange groups attached to a styrene divinyl benzene copolymer lattice, radioimmunoassaying the fractions eluted with an equivolume mixture of acetic acid and water. Parallelism with the standard curve was good, and results were the same whether or not the resin eluate was further purified by paper chromatography. Th...|$|E
40|$|Communicated {{concepts}} of property ownership, including intellectual property, depend on {{cultural values and}} norms. In {{many parts of the}} world, conceptual private ownership lacks definitive regulations that apply in the Western world. This <b>lack</b> <b>of</b> cultural <b>parallelism</b> reflects and engenders significant problems in an age where growing technological advances spread ideas and devices across cultural boundaries bringing philosophical, financial, and other practical concerns that create questions about the role of local norms in governing international transference of innovations. Intellectual property rights (IPRs) are the focus of enormous contemporary international diplomatic efforts to the business of inno-vative technology and the artistic arena of music, literature, and art. This article briefly out-lines the historical development of Western IPRs, illustrates many problems from a non-ethnocentric study of the topic. Intellectual property refers to a broad range of confidential information some-times protected by patents, trademarks, and copyrights that grant exclusive rights to sell new products. At the heart of almost all disputes over intellectual property lies a fundamental question: Who owns and is entitled to profit from a...|$|R
40|$|Abstract — The {{explosive}} growth of 802. 11 -based wireless LANs has attracted interest in providing higher data rates and greater system capacities. Among the IEEE 802. 11 standards, the 802. 11 a standard based on OFDM (Orthogonal Frequency Division Multiplexing) modulation scheme {{can be defined}} to address high-speed and large-system-capacity challenges. Although DSPs {{have been used to}} implement the 802. 11 a standard, they can only support limited data rates due to the <b>lack</b> <b>of</b> global <b>parallelism</b> found at the application level. Hence, it is still a major challenge to develop a software implementation for the 802. 11 a standard on a DSP to meet the high-data-date requirements. To meet these requirements this paper proposes the design of 256 point SDF FFT (Fast Fourier Transform) architecture for OFDM wireless LANs because FFT and IFFT (Inverse Fast Fourier Transform) are major hardware requirements in OFDM. The data transmission is by interfacing the QPSK modulator with IFFT at transmitter and QPSK demodulator with FFT, at receiver for easy of designing. The design has been coded in verilog. The simulations and synthesis is carried out by using Xilinx 9. 2 i simulator...|$|R
40|$|Processing {{very large}} graphs like social networks, {{biological}} and chemical compounds is a challenging task. Distributed graph processing systems process the billion-scale graphs efficiently but incur overheads of efficient partitioning and distribution of the graph over a cluster of nodes. Distributed processing also requires cluster management and fault tolerance. In order {{to overcome these problems}} GraphChi was proposed recently. GraphChi significantly outperformed all the representative distributed processing frameworks. Still, we observe that GraphChi incurs some serious degradation in performance due to 1) high number of non-sequential I/Os for processing every chunk of graph; and 2) <b>lack</b> <b>of</b> true <b>parallelism</b> to process the graph. In this paper we propose a simple yet powerful engine BiShard Parallel Processor (BPP) to efficiently process billions-scale graphs on a single PC. We extend the storage structure proposed by GraphChi and introduce a new processing model called BiShard Parallel (BP). BP enables full CPU parallelism for processing the graph and significantly reduces the number of non-sequential I/Os required to process every chunk of the graph. Our experiments on real large graphs show that our solution significantly outperforms GraphChi. Comment: 5 pages, Published in ICCA, 201...|$|R
40|$|Abstract We {{introduce}} virtual multi-level iterative methods (VML) which {{attempt to}} remove the low frequency errors by conducting some special smoothing (residual norm minimization) procedure {{with respect to the}} coarse grids. However, there is no coarse grid formed explicitly, no inter-grid transfer operator is needed, and even the smoothing procedure can be done almost locally. These properties are attractive to parallel computers. VML with different relaxation schemes and different smoothing techniques constitutes a class of VML iterative methods. They may be used to accelerate general (single-level) iterative methods or be used with the standard (real) multigrid method to alleviate the inherent <b>lack</b> <b>of</b> <b>parallelism.</b> Numerical experiments with some relaxation and smoothing techniques are used to show how the VML iterative methods work...|$|E
40|$|An {{investigation}} of the “normal” bacterial content of {{the air of a}} large general hospital is described. In many different places within five different areas 70 to 200 settle-plate or slit-sampler bacterial counts were carried out. Average counts were most often of the same order as or lower than other published results and were proportional to human activity. The use of the logarithms of the counts showed no advantage, and conventional statistics should be applied with caution in evaluating such studies. Slitsampler and settle-plate counts of all bacteria showed no correlation, whereas those of Staph. aureus were correlated. There is a <b>lack</b> <b>of</b> <b>parallelism</b> between hospital infection and air bacteria counted by current methods, which are, therefore, not suitable for routine use...|$|E
40|$|Introduction: While {{using an}} implant-supported {{removable}} partial prosthesis, the implant abutments should be parallel {{to one another}} along the path of insertion. If the implants and their attachments are placed vertically on a similar occlusal plane, not only is the retention improved, the prosthesis will also be maintained for a longer period. Case Report: A 65 -year-old male patient referred to the School of Dentistry in Mashhad, Iran with complaints of discomfort with the removable partial dentures for his lower mandible. Due to the <b>lack</b> <b>of</b> <b>parallelism</b> in the supporting implants, prefabricated ball abutment could not be used. As a result, a customized ball abutment was fabricated in order to correct the non-parallelism of the implants. Conclusion: Using UCLA abutments could be a cost-efficient approach for the correction of misaligned implant abutments in implant-supported overdentures...|$|E
40|$|The {{periclinal}} {{walls of}} cambial cells in neighboring lineages (rows) {{may not be}} parallel when viewed in their radial aspect. This <b>lack</b> <b>of</b> longitudinal <b>parallelism</b> may be so extensive that in active cambium pairs of cells from neighboring rows may be in contact only along restricted segments. This means that the initial cells, rather than farming a continuous layer, may be arranged in an irregular network pattern from which some parts project inward or outward from the layer of their mutual cantacts. The longitudinal nonparallelism of cambial cells becomes more pronounced during symplasitic radial growth. Unequal periclinal divisions counteract this, and in initial cells abscission of the parts projecting from the layer of mutual contact occurs. When the cambium passes from a period of activity to a Period of rest a continuous layer of initials is reestabhshed. This involves elongation by intrusive growth of those cells previously shortened {{as the result of}} irregular periclinal divisions. The division walls in cambial cells may be warped, that is they change their orientation along the longitudinal direction perhaps even similar to an aircraft propeller. A division wall may thus be periclinal {{in one part of the}} cell and anticlinal in another...|$|R
40|$|In this paper, {{we propose}} an {{empirical}} method {{for evaluating the}} performance of parallel code. Our method {{is based on a}} simple idea that is surprisingly effective in helping to identify causes of poor performance, such as high parallelization overheads, <b>lack</b> <b>of</b> adequate <b>parallelism,</b> and memory effects. Our method relies on only the measurement of the run time of a baseline sequential program, the run time of the parallel program, the single-processor run time of the parallel program, and the total amount of time processors spend idle, waiting for work. In our proposed approach, we establish an equality between the observed parallel speedups and three terms that we call parallel work, idle time, and work-inflation, where all terms except work inflation can be measured empirically, with precision. We then use the equality to calculate the difficult-to-measure work-inflation term, which includes increased communication costs and memory effects due to parallel execution. By isolating the main factors of poor performance, our method enables the programmer to assign blame to certain properties of the code, such as parallel grain size, amount <b>of</b> <b>parallelism,</b> and memory usage. We present a mathematical model, inspired by the work-span model, that enables us to justify the interpretation of our measurements. We also introduce a method to help the programmer to visualize both the relative impact of the various causes of poor performance and the scaling trends in the causes of poor performance. Our method fits in a sweet spot in between state-of-the-art profiling and visualization tools. We illustrate our method by several empirical studies and we describe a few experiments that emphasize the care that is required to accurately interpret speedup plots...|$|R
40|$|ISBN: 978 - 1 - 4244 - 6967 - 3 - WOSInternational audienceData-flow {{has proven}} to be an {{attractive}} computation model for programming digital signal processing (DSP) applications. A restricted version of data-flow, termed synchronous data-flow (SDF), offers strong compile-time predictability properties, but has limited expressive power. A new type of hierarchy (Interface-based SDF) has been proposed allowing more expressivity while maintaining its predictability. One of the main problems with this hierarchical SDF model is the <b>lack</b> <b>of</b> trade-off between <b>parallelism</b> and network clustering. This paper presents a systematic method for applying an important class of loop transformation techniques in the context of interface-based SDF semantics. The resulting approach provides novel capabilities for integrating <b>parallelism</b> extraction properties <b>of</b> the targeted loop transformations with the useful modeling, analysis, and code reuse properties provided by SDF...|$|R
