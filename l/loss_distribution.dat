707|1356|Public
25|$|The Clayton {{canonical}} vine copula {{allows for}} the occurrence of extreme downside events and has been successfully applied in portfolio choice and risk management applications. The model is able to reduce the effects of extreme downside correlations and produces improved statistical and economic performance compared to scalable elliptical dependence copulas such as the Gaussian and Student-t copula. Other models developed for risk management applications are panic copulas that are glued with market estimates of the marginal distributions to analyze the effects of panic regimes on the portfolio profit and <b>loss</b> <b>distribution.</b> Panic copulas are created by Monte Carlo simulation, mixed with a re-weighting of the probability of each scenario.|$|E
5000|$|While AMA {{does not}} specify {{the use of}} any {{particular}} modeling technique, one common approach taken in the banking industry is the <b>Loss</b> <b>Distribution</b> Approach (LDA). With LDA, a bank first segments operational losses into homogeneous segments, called units of measure (UoMs). For each unit of measure, the bank then constructs a <b>loss</b> <b>distribution</b> that represents its expectation of total losses that can materialize in a one-year horizon. Given that data sufficiency is a major challenge for the industry, annual <b>loss</b> <b>distribution</b> cannot be built directly using annual loss figures. Instead, a bank will develop a frequency distribution that describes the number of loss events in a given year, and a severity distribution that describes the loss amount of a single loss event. The frequency and severity distributions are assumed to be independent. The convolution of these two distributions then give rise to the (annual) <b>loss</b> <b>distribution</b> ...|$|E
50|$|Where σ2(E) {{represents}} energy straggling {{per unit}} length (or) variance of energy <b>loss</b> <b>distribution</b> {{per unit length}} for particles of energy E. E(x)is the mean energy at depth x.|$|E
40|$|Photovoltaic systems {{technological}} development {{is driven by}} the request for higher efficiency and safety. These concerns influence also the choice of the power converter stage. Several topologies have been proposed {{and many of them are}} available commercially. Among them, the neutral point clamped (NPC) and derived topologies offers high efficiency, low leakage current, and low EMI. However, one main disadvantage of the NPC inverter is given by an unequal <b>distribution</b> of the <b>losses</b> in the semiconductor devices, which leads to an unequal distribution of temperature that can affect lifetime. By using the active NPC (ANPC) topology, where the clamping diodes are replaced by bidirectional switches, the power <b>losses</b> <b>distribution</b> problem is alleviated. The modulation strategy is a key issue for <b>losses</b> <b>distribution</b> in this topology. In this paper, two known strategies are discussed and a new PWM strategy, namely the adjustable <b>losses</b> <b>distribution</b> is proposed for better <b>losses</b> <b>distribution</b> in the ANPC topology. Simulations and experimental results help in evaluating the modulation strategiesPeer ReviewedPostprint (author's final draft...|$|R
40|$|The Neutral Point Clamped {{topology}} {{due to high}} efficiency, low {{leakage current}} and EMI, its integration is widely used in the distributed generation (DG) systems. However the main disadvantage of the NPC inverter is given by an unequal <b>distribution</b> of the <b>losses</b> in the semiconductor devices, which leads to an unequal distribution of temperature. By using the Active NPC topology, the power <b>losses</b> <b>distribution</b> problem is alleviated. The modulation strategy is a key issue for <b>losses</b> <b>distribution</b> in this topology. In this paper two known strategies are discussed and a new proposed PWM strategy, namely the Adjustable <b>Losses</b> <b>Distribution</b> (ALD) PWM strategy is proposed for better <b>losses</b> <b>distribution</b> in the Active NPC (ANPC) topology. Simulations using Simulink and the PLECS toolbox have been done for evaluating efficiency of different NPC topologies and some experimental results are {{presented in this paper}} to validate the operation of the different strategies. Peer ReviewedPostprint (published version...|$|R
40|$|We {{provide a}} {{rigorous}} proof of granularity adjustment (GA) formulas to evaluate <b>loss</b> <b>distributions</b> and risk measures (value-at-risk) {{in the case}} of heterogenous portfolios, multiple systemic factors and random recoveries. As a significant improvement with respect to the literature, we detail all the technical conditions of validity and provide an upper bound of the remainder term at a finite distance. Moreover, we deal explicitly with the case of general <b>loss</b> <b>distributions,</b> possibly with masses. For some simple portfolio models, we prove empirically that the granularity adjustments do not always improve the infinitely granular first-order approximations. This stresses the importance of checking our conditions of regularity before relying of such techniques. And smoothing the underlying <b>loss</b> <b>distributions</b> through random recoveries or exposures improves the GA performances in general...|$|R
5000|$|Endwall {{losses are}} high in stator (Francis turbine/Kaplan turbine) and nozzle vane (Pelton turbine) and <b>loss</b> <b>distribution</b> is {{different}} for turbine and compressor due to flow is opposite to each other.|$|E
5000|$|The {{output is}} an {{estimate}} of the losses that the model predicts would be associated with a particular event or set of events. When running a probabilistic model, the output is either a probabilistic <b>loss</b> <b>distribution</b> or a set of events {{that could be used to}} create a loss distribution; probable maximum losses (PMLs) and average annual losses (AALs) are calculated from the <b>loss</b> <b>distribution.</b> When running a deterministic model, losses caused by a specific event are calculated; for example, Hurricane Katrina or [...] "a magnitude 8.0 earthquake in downtown San Francisco" [...] could be analyzed against the portfolio of exposures.|$|E
5000|$|Expected {{shortfall}} (ES) {{is a risk}} measure—a concept used in {{the field}} of financial risk measurement to evaluate the market risk or credit risk of a portfolio. The [...] "expected shortfall at q% level" [...] is the expected return on the portfolio in the worst % of cases. ES is an alternative to Value at Risk that is more sensitive to the shape of the tail of the <b>loss</b> <b>distribution.</b>|$|E
40|$|A {{considerable}} number of equivalent formulas defining conditional value-at-risk and expected shortfall are gathered together. Then we present a simple method to bound the conditional value-at-risk of compound Poisson <b>loss</b> <b>distributions</b> under incomplete information about its severity distribution, which is assumed to have a known finite range, mean, and variance. This important class of nonnormal <b>loss</b> <b>distributions</b> finds applications in actuarial science, where {{it is able to}} model the aggregate claims of an insurance-risk business...|$|R
40|$|Abstract. It firstly {{illustrates}} {{the meaning of}} studying the line <b>loss</b> of <b>distribution</b> network briefly, analyzes the main reasons influencing the line <b>loss</b> of <b>distribution</b> network subsequently and finally puts forward specific loss reduction measures. Theoretical analysis demonstrates that these measures can effectively lower the line <b>loss</b> of <b>distribution</b> network...|$|R
40|$|Flux {{and iron}} <b>loss</b> <b>distributions</b> of {{three-phase}} reactor are analyzed using the {{finite element method}} considering 2 -D B-H curves and iron losses in arbitrary directions which are measured up to high flux density. It is shown that the total iron loss of reactor yoke does not change so much by the yoke dimension, although the local iron loss is increased when the width of yoke is decreased. The experimental verification of flux and iron <b>loss</b> <b>distributions</b> are also carried out </p...|$|R
50|$|The second {{market model}} {{assumes that the}} market only has finitely many {{possible}} changes, drawn from a risk factor return sample of a defined historical period. Typically one performs a historical simulation by sampling from past day-on-day risk factor changes, and applying them to {{the current level of}} the risk factors to obtain risk factor price scenarios. These perturbed risk factor price scenarios are used to generate a profit (<b>loss)</b> <b>distribution</b> for the portfolio.|$|E
50|$|The {{third market}} model {{assumes that the}} {{logarithm}} of the return, or, log-return, of any risk factor typically follows a normal distribution. Collectively, the log-returns of the risk factors are multivariate normal. Monte Carlo algorithm simulation generates random market scenarios drawn from that multivariate normal distribution. For each scenario, the profit (loss) of the portfolio is computed. This collection of profit (loss) scenarios provides {{a sampling of the}} profit (<b>loss)</b> <b>distribution</b> from which one can compute the risk measures of choice.|$|E
5000|$|One {{to three}} times VaR are normal occurrences. You expect {{periodic}} VaR breaks. The <b>loss</b> <b>distribution</b> typically has fat tails, and you might get more than one break {{in a short period}} of time. Moreover, markets may be abnormal and trading may exacerbate losses, and you may take losses not measured in daily marks such as lawsuits, loss of employee morale and market confidence and impairment of brand names. So an institution that can't deal with three times VaR losses as routine events probably won't survive long enough to put a VaR system in place.|$|E
40|$|The {{potential}} for portfolio diversification is driven broadly by two characteristics: {{the degree to}} which systematic risk factors are correlated with each other and the degree of dependence individual firms have to the different types of risk factors. Using a global vector autoregressive macroeconometric model accounting for about 80 % of world output, we propose a model for exploring credit risk diversification across industry sectors and across different countries or regions. We find that full firm-level parameter heterogeneity along with credit rating information matters a great deal for capturing differences in simulated credit <b>loss</b> <b>distributions.</b> Imposing homogeneity results in overly skewed and fat-tailed <b>loss</b> <b>distributions.</b> These differences become more pronounced in the presence of systematic risk factor shocks: increased parameter heterogeneity reduces shock sensitivity. Allowing for regional parameter heterogeneity seems to better approximate the <b>loss</b> <b>distributions</b> generated by the fully heterogeneous model than allowing just for industry heterogeneity. The regional model also exhibits less shock sensitivity. risk management, default dependence, economic interlinkages, portfolio choice...|$|R
40|$|Abstract: This {{three part}} paper {{addresses}} {{the task of}} modelling the right hand tail of a severity distribution. In Part I the excess ratio function is used to de…ne a discrete sequence of <b>loss</b> <b>distributions</b> with related moments and similar tail behavior. Part II extends this to continuous one-parameter families and provides some examples. Part III provides the main result: that under some reasonable conditions, each such family has a limiting distribution which is exponential. The paper then exploits this to 1) group <b>loss</b> <b>distributions</b> based on tail behavior and 2) promote the choice of (mixed) exponentials to model tail behavior. ...|$|R
40|$|Credit default events show cross {{sectional}} {{as well as}} serial correlation. While the latter is often neglected by current credit risk models, this work incorporates both types of dependence. A Bernoulli mixture model is considered, where in each rating grade the probit of the stochastic Bernoulli parameter follows an autoregressive stationary process with exogenous variables. The model parameters are estimated for a large retail portfolio. Exemplarily, prediction intervals of the default probabilities {{of the best and}} worst nondefault rating grade are given and predicted credit portfolio <b>loss</b> <b>distributions</b> are plotted in contrast to the unconditional <b>loss</b> <b>distributions.</b> ...|$|R
5000|$|Tranching can add {{complexity}} to deals. Beyond {{the challenges}} posed by {{estimation of the}} asset pool's <b>loss</b> <b>distribution,</b> tranching requires detailed, deal-specific documentation {{to ensure that the}} desired characteristics, such as the seniority ordering the various tranches, will be delivered under all plausible scenarios. In addition, complexity may be further increased by the need to account for the involvement of asset managers and other third parties, whose own incentives to act in the interest of some investor classes {{at the expense of others}} may need to be balanced.|$|E
5000|$|... 3. Large {{fraction}} of energy loss: for fractional energy {{loss in the}} region of 0.2< ΔE/E ≤ 0.8,the energy dependence of stopping power causes the energy <b>loss</b> <b>distribution</b> to differ from Bohr’s straggling function. Hence the Bohr theory can not be applicable for this case. [...] Various theoretical advances were made in understanding energy straggling in this case.An expression of energy for straggling is proposed by Symon {{in the region of}} 0.2< ΔE/E ≤ 0.5 is a function of momentums Mi( [...] Mi = M1+M2 where M1 is stopping power, M2 is variation in straggling with depth of a stopping power) ...|$|E
50|$|The Clayton {{canonical}} vine copula {{allows for}} the occurrence of extreme downside events and has been successfully applied in portfolio choice and risk management applications. The model is able to reduce the effects of extreme downside correlations and produces improved statistical and economic performance compared to scalable elliptical dependence copulas such as the Gaussian and Student-t copula. Other models developed for risk management applications are panic copulas that are glued with market estimates of the marginal distributions to analyze the effects of panic regimes on the portfolio profit and <b>loss</b> <b>distribution.</b> Panic copulas are created by Monte Carlo simulation, mixed with a re-weighting of the probability of each scenario.|$|E
40|$|The Basel II Accord {{requires}} participating {{banks to}} quantify operational risk {{according to a}} matrix of business lines and event types. Proper modeling of univariate <b>loss</b> <b>distributions</b> and dependence structures across those categories of operational losses is critical for proper assessment of overall annual operational <b>loss</b> <b>distributions.</b> We illustrate our proposed methodology using Loss Data Collection Exercise 2004 (LDCE 2004) data on operational losses across five loss event types. We estimate a multivariate likelihood-based statistical model, which illustrates the benefits and risks of using extreme value theory (EVT) in modeling univariate tails of event type <b>loss</b> <b>distributions.</b> We find that abandoning EVT leads to unacceptably low estimates of risk capital requirements, while indiscriminate use of EVT to all data leads to unacceptably high ones. The judicious middle approach is to use EVT where dictated by data, and after separating clear outliers {{that need to be}} modeled via probabilistic scenario analysis. We illustrate all computational steps in estimation of marginal distributions and copula with an application to one bank’s data (disguising magnitudes to ensure tha...|$|R
40|$|This paper {{introduces}} a new method to up-scale dependent <b>loss</b> <b>distributions</b> from natural hazards to higher spatial levels, explicitly incorporating their dependency structure over the aggregation process. The method is applied for flood risk in Europe. Based on this "hybrid convolution" approach, flood <b>loss</b> <b>distributions</b> {{for nearly all}} European countries are calculated and presented. Such risk-based estimates of extreme event losses are useful for determining suitable risk management strategies on various spatial levels for different risk bearers. The method is not only applicable for natural disaster risk but can be extended for other cases as well, i. e., where comonotonic risks have to be "summed up" without loss of risk information...|$|R
2500|$|Hogg, Robert V., Klugman, Stuart A. <b>Loss</b> <b>distributions.</b> With the {{assistance}} of Charles C. Hewitt and Gary Patrik. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics. John Wiley & Sons, Inc., New York, 1984. x+235 pp..|$|R
40|$|Selecting <b>loss</b> <b>distribution</b> {{model by}} using {{statistical}} tests {{should be done}} to make sure it is accepted and the model selected can be applied in insurance. This study aims to select the best <b>loss</b> <b>distribution</b> model by applying statistical fitting test on seveal fitted <b>Loss</b> <b>distribution</b> model. The <b>loss</b> <b>distribution</b> model represents sample data from motor insurance claims in Malaysia. The model is obtained by using maximum likelihood estimation. The statistical tests used are Pearson: goodness-of fit statistic and Kolmogorov-Smirnov statistic. Based on results from maximum likelihood estimation and statistical tests, a single best model will be selected and the model will be applied in insurance...|$|E
40|$|In {{this study}} we apply a {{macroeconomic}} credit risk model which links a set of macroeconomic factors and industry-specific corporate sector default rates using Romanian data over the time period from 2002 : 2 to 2007 : 2. We will model and estimate industry-specific default rates, simulate with Monte Carlo method a <b>loss</b> <b>distribution</b> of a hypothetical corporate credit portfolio and analyze the impact of interest rate developments on the portfolio <b>loss</b> <b>distribution.</b> macroeconomic credit risk, credit risk model, Monte Carlo method, credit <b>loss</b> <b>distribution,</b> portfolio stress testing...|$|E
40|$|Efficient {{numerical}} {{methods for}} evaluating the loss distributions of synthetic CDOs are important for both pricing and risk management purposes. In this paper we discuss several methods for <b>loss</b> <b>distribution</b> evaluations. We first develop a stable recursive method. Then the improved compound Poisson approximations proposed by Hipp [12] are introduced. Finally, the normal power approximation method {{that has been used}} in actuarial science is described. The recursive method computes the <b>loss</b> <b>distribution</b> exactly, whereas the other two methods compute the <b>loss</b> <b>distribution</b> approximately. Numerical results based on these three and some known methods for synthetic CDO pricing are given...|$|E
40|$|When {{estimating}} <b>loss</b> <b>distributions</b> in insurance, {{large and}} small losses are usually split because {{it is difficult to}} find a simple parametric model that fits all claim sizes. This approach involves determining the threshold level between {{large and small}} losses. In this article, a unified approach to the estimation of <b>loss</b> <b>distributions</b> is presented. We propose an estimator obtained by transforming the data set with a modification of the Champernowne cdf and then estimating the density of the transformed data by use of the classical kernel density estimator. We investigate the asymptotic bias and variance of the proposed estimator. In a simulation study, the proposed method shows a good performance. We also present two applications dealing with claims costs in insurance...|$|R
40|$|Expected Shortfall (ES) {{in several}} {{variants}} {{has been proposed}} as remedy for the de- ciencies of Value-at-Risk (VaR) which in general is not a coherent risk measure. In fact, most denitions of ES lead to the same results when applied to continuous <b>loss</b> <b>distributions.</b> Dierences may appear when the underlying <b>loss</b> <b>distributions</b> have discontinuities. In this case even the coherence property of ES can get lost unless one {{took care of the}} details in its denition. We compare some of the denitions of Expected Shortfall, pointing out that there is one which is robust in the sense of yielding a coherent risk measure regardless of the underlying distributions. Moreover, this Expected Shortfall can be estimated eectively even in cases where the usual estimators for VaR fail...|$|R
40|$|Experimental {{investigations}} {{were made}} on four two-dimensional impellers and on a well-designed commercial three-dimensional Francis impeller. The over-all performance {{of each of these}} impellers was measured and internal-energy loss and pressure-distribution data were also obtained for several impellers. The exit angle of the two-dimensional impellers was fixed and the inlet angle was systematically varied. However, the hydraulic characteristics of these impellers were all found to differ, the source of the variation being in the various <b>loss</b> <b>distributions</b> and hence internal flow patterns in the impellers. The two-dimensional and three-dimensional impeller-loss distributions were also different. The Francis-impeller performance agreed better with potential theory than that of the two-dimensional impellers, and it is included that the different <b>loss</b> <b>distributions</b> of the two types are responsible...|$|R
40|$|We derive {{the exact}} <b>loss</b> <b>distribution</b> for {{portfolios}} of bonds or cor-porate loans {{when the number}} of risks grows indefinitely. We show that in many cases this distribution lies in the maximal domain of attraction of the Weibull (Type III) limit law. Knowledge of the dis-tribution and its tail behavior is important for risk management in order not to over- or underestimate the likelihood of extreme credit losses for the portfolio as a whole. Conform to the credit risk literature, we assume that bond (or loan) defaults are triggered by a latent variable model involving two stochastic variables: systematic and idiosyncratic risk of the bond. It is shown that the tail behavior of these two variables translates into the tail behavior of the whole credit <b>loss</b> <b>distribution.</b> Surprisingly, even if both variables are thin-tailed, the credit <b>loss</b> <b>distribution</b> can have a finite tail index. Moreover, if idiosyncratic risk exhibits heavier tails than the systematic risk factor the tail index of the credit <b>loss</b> <b>distribution</b> can become extremely high, giving rise to a non-conventional shape of the credit <b>loss</b> <b>distribution.</b> credit risk; value-at-risk; tail events; tail index. ...|$|E
40|$|We {{examine the}} {{question}} of deposit insurance {{through the lens of}} risk management by addressing three key issues: 1) how big should the fund be; 2) how should coverage be priced; and 3) who pays in the event of loss. We propose a risk-based premium system that is explicitly based on the <b>loss</b> <b>distribution</b> faced by the FDIC. The <b>loss</b> <b>distribution</b> can be used to determine the appropriate level of fund adequacy and reserving in terms of a stated confidence interval and to identify risk-based pricing options. We explicitly estimate that distribution using two different approaches and find that reserves are sufficient to cover roughly 99. 85 % of the <b>loss</b> <b>distribution</b> corresponding to about a BBB+ rating. We then identify three risk-sharing alternatives addressing who is responsible for funding losses {{in different parts of the}} <b>loss</b> <b>distribution.</b> We show in an example that expected loss based pricing, while appropriately penalizing riskier banks, also penalizes smaller banks. By contrast, unexpected loss contribution based pricing significantly penalizes very large banks because large exposures contribute disproportionately to overall (FDIC) portfolio risk. Deposit insurance pricing, <b>loss</b> <b>distribution,</b> risk-based premiums. ...|$|E
40|$|This {{paper will}} discuss a {{proposed}} method for {{the estimation of}} <b>loss</b> <b>distribution</b> using information {{from a combination of}} internally derived data and data from external sources. The relevant context for this analysis is the estimation of operational loss distributions used in the calculation of capital adequacy. We present a robust, easy-to-implement approach that draws on Bayesian inferential methods. The principal intuition behind the method is to let the data itself determine how they should be incorporated into the <b>loss</b> <b>distribution.</b> This approach avoids the pitfalls of managerial choice on data weighting and cut-off selection and allows for the estimation of a single <b>loss</b> <b>distribution.</b> Risk...|$|E
40|$|In this paper, {{we present}} a Value-at-Risk (VaR) and Con-ditional Value-at-Risk (CVaR) {{estimation}} technique for dy-namic hedging and investigate the effect of higher order moments in the underlying on the hedging loss distribu-tions. At first, we approximate the underlying stock process through its first four moments including skewness and kur-tosis using a general parameterization of multinomial lat-tices, and solve the mean square optimal hedging problem. Then our recently developed technique is applied to extract the hedging <b>loss</b> <b>distributions</b> in option hedge positions. Fi-nally, we demonstrate how the hedging error distribution changes with respect to non-zero kurtosis and skewness in the underlying through numerical experiments, and exam-ine the relation between VaR and CVaR of the hedging <b>loss</b> <b>distributions</b> and kurtosis of the underlying...|$|R
40|$|In theory the {{potential}} for credit risk diversi 8 ̆ 5 cation for banks could be substantial. Portfolios are large enough that idiosyncratic risk is diversi 8 ̆ 5 ed away leaving exposure to systematic risk. The potential for portfolio diversi 8 ̆ 5 cation is driven broadly by two characteristics: {{the degree to which}} systematic risk factors are correlated with each other and the degree of dependence individual rms have to the di¤erent types of risk factors. We propose a model for exploring these dimensions of credit risk diversi 8 ̆ 5 cation: across industry sectors and across di¤erent coun-tries or regions. We 8 ̆ 5 nd that full 8 ̆ 5 rm-level parameter heterogeneity matters a great deal for capturing di¤erences in simulated credit <b>loss</b> <b>distributions.</b> Imposing homogeneity results in overly skewed and fat-tailed <b>loss</b> <b>distributions.</b> These di¤erences become more pronounced in the presence of systematic risk factor shocks: increased parameter heterogeneity greatly reduces shock sensitivity. Allowing for regional parameter heterogeneity seems to better approximate the <b>loss</b> <b>distributions</b> generated by the fully heterogeneous model than allowing just for industry heterogeneity. The regional model also exhibits less shock sensitivity. Key Words: Risk management, default dependence, economic interlinkages, portfolio choic...|$|R
40|$|The new Basel II {{regulation}} {{contains a}} number of new regulatory features. Most importantly, internal ratings will be given {{a central role in the}} evaluation of the riskiness of bank loans. Another novelty is that retail credit and loans to small and medium-sized enterprises will receive a special treatment in recognition of the fact that the riskiness of such exposure derives to a greater extent from idiosyncratic risk and much less from common factor risk. Much of the work done on the differences between the risk properties of retail, SME and corporate credit has been based on parameterized models of credit risk. In this paper we present new quantitative evidence on the implied credit <b>loss</b> <b>distributions</b> for two Swedish banks using a non-parametric Monte Carlo re-sampling method following Carey [1998]. Our results are based on a panel data set containing both loan and internal rating data from the banks 9 ̆ 2 complete business loan portfolios over the period 1997 - 2000. We compute the credit <b>loss</b> <b>distributions</b> that each rating system implies and compare the required economic capital implied by these <b>loss</b> <b>distributions</b> with the regulatory capital under Basel II. By exploiting the fact that a subset of all businesses in the sample is rated by both banks, we can generate <b>loss</b> <b>distributions</b> for SME, retail and corporate credit portfolios with a constant risk profile. Our findings suggest that a special treatment for retail credit and SME loans may not be justified. We also investigate if any alternative definition of SME 9 ̆ 2 s and retail credit would warrant different risk weight functions for these types of exposure. Our results indicate that it may be difficult to find a simple risk weight function that can account for the differences in portfolio risk properties between banks and asset types...|$|R
