111|34|Public
25|$|Starting from a {{sequence}} of characters, the <b>lexical</b> <b>analyzer</b> builds {{a sequence}} of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The <b>lexical</b> <b>analyzer</b> and the parser handle the regular and context-free parts of the programming language's grammar.|$|E
25|$|Finite automata {{are often}} used in the {{frontend}} of programming language compilers. Such a frontend may comprise several finite state machines that implement a <b>lexical</b> <b>analyzer</b> and a parser.|$|E
50|$|The {{compiler}} {{generated by}} Yacc requires a <b>lexical</b> <b>analyzer.</b> <b>Lexical</b> <b>analyzer</b> generators, such as lex or flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.|$|E
5000|$|Lex is a {{computer}} program that generates <b>lexical</b> <b>analyzers</b> ("scanners" [...] or [...] "lexers").|$|R
40|$|<b>Lexical</b> <b>analyzers</b> {{partition}} input characters into tokens. When ambiguities arise during lexical analysis, the longest-match rule {{is generally}} adopted {{to resolve the}} ambiguities. The longest-match rule causes the look-ahead problem in traditional <b>lexical</b> <b>analyzers,</b> {{which are based on}} Moore machines. In Moore machines, output tokens are associated with states of the automata. By contrast, because Mealy machines associate output tokens with state transitions, the look-ahead behaviors can be encoded in their state transition tables. Therefore, we believe that <b>lexical</b> <b>analyzers</b> should be based on Mealy machines, rather than Moore machines, in order to solve the look-ahead problem. We propose techniques to construct Mealy machines from regular expressions and to perform sequential and data-parallel lexical analysis with these Mealy machines. Key Words: automata, finite-lookahead automata, lexical analysis, Mealy machines, Moore machines, parallel algorithms, regular expressions, suffix autom [...] ...|$|R
50|$|OpenSearchServer is an {{open source}} {{application}} server allowing development of index-based applications such as search engines. Available since April 2009 on SourceForge for download, OpenSearchServer was developed under the GPL v3 license and offers a series of full text <b>lexical</b> <b>analyzers.</b> It can be installed on different platforms (Windows, Linux, Macintosh).|$|R
50|$|Finite automata {{are often}} used in the {{frontend}} of programming language compilers. Such a frontend may comprise several finite state machines that implement a <b>lexical</b> <b>analyzer</b> and a parser.Starting from a sequence of characters, the <b>lexical</b> <b>analyzer</b> builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The <b>lexical</b> <b>analyzer</b> and the parser handle the regular and context-free parts of the programming language's grammar.|$|E
5000|$|A <b>lexical</b> <b>analyzer</b> {{generally}} {{does nothing}} with combinations of tokens, a task {{left for a}} parser. For example, a typical <b>lexical</b> <b>analyzer</b> recognizes parentheses as tokens, but does nothing to ensure that each [...] "(" [...] is matched with a [...] ")".|$|E
50|$|Simulation of <b>Lexical</b> <b>Analyzer</b> and Parser using C.|$|E
40|$|Abstract-Lexical analyzers {{partition}} input characters into tokens. When ambiguities arise during lexical analysis, the longest-match rule {{is generally}} adopted {{to resolve the}} ambiguities. The longest-match rule causes the look-ahead problem in traditional <b>lexical</b> <b>analyzers,</b> {{which are based on}} Moore machines. In Moore machines, output tokens are associated with states of the automata. By contrast, because Mealy machines associate output tokens with state transitions, the look-ahead behaviors can be encoded in their state transition tables. Therefore, we believe that <b>lexical</b> <b>analyzers</b> should be based on Mealy machines. rather than Moore machines, in order to solve the look-ahead problem. We propose techniques to construct Mealy machines from regular expressions and to perform sequential and data-parallel lexical analysis with these Mealy machines. Copyright c 1996 Elsevier Science Ltd automata finite-lookahead automata lexical analysis Mealy machines Moore machrncs parallel algorithms regular expressions suffix automata 1...|$|R
50|$|One {{style of}} metaprogramming is to employ domain-specific {{languages}} (DSLs). A fairly common example of using DSLs involves generative metaprogramming: lex and yacc, two tools {{used to generate}} <b>lexical</b> <b>analyzers</b> and parsers, let the user describe the language using regular expressions and context-free grammars, and embed the complex algorithms required to efficiently parse the language.|$|R
5000|$|JetPAG's garmmars {{are written}} in a meta {{language}} based on the EBNF form and regular expressions, with extensive additions and tweaks. The meta language of JetPAG grammars {{was designed to be}} maximally flexible handle both simple grammars and large, complicated ones easily. Parsers and <b>lexical</b> <b>analyzers</b> are similarly defined and generated for simplicity and ease of use. This is a simple example of a grammar for a basic calculator: ...|$|R
5000|$|Quex - fast {{universal}} <b>lexical</b> <b>analyzer</b> generator for C and C++ ...|$|E
50|$|Tokens {{are defined}} often by regular expressions, which are {{understood}} by a <b>lexical</b> <b>analyzer</b> generator such as lex. The <b>lexical</b> <b>analyzer</b> (generated automatically by a tool like lex, or hand-crafted) reads {{in a stream}} of characters, identifies the lexemes in the stream, and categorizes them into tokens. This is termed tokenizing. If the lexer finds an invalid token, it will report an error.|$|E
50|$|Yacc {{produces}} only a parser (phrase analyzer); {{for full}} syntactic analysis this requires an external <b>lexical</b> <b>analyzer</b> {{to perform the}} first tokenization stage (word analysis), which is then followed by the parsing stage proper. <b>Lexical</b> <b>analyzer</b> generators, such as Lex or Flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.|$|E
40|$|The aim of {{this work}} is to design a {{language}} of higher level of abstraction for programming mobile intelligent agents and implement a compiler for this language. There will also be presented the ANTLR tool for generating syntax and <b>lexical</b> <b>analyzers.</b> The reader will become familiar with theoretical and practical aspects of implementation of the compiler and also with programming in this language. There will be shown the environment for programming in this language and some examples of agent codes at the end...|$|R
40|$|This article {{reports on}} the utility in {{question}} three years after public release. Precc generates standard ANSI C and is `plug compatible' with lex-generated <b>lexical</b> <b>analyzers</b> prepared for the UNIX yacc compilercompiler. In contrast to yacc, however, the generated code is modular, which allows parts of scripts to be compiled separately and linked together incrementally. The constructed code is relatively efficient, as is demonstrated by the example Occam parser treated in depth here, but the main advantages we claim are ease of use, separation of specification and implementation concerns, and maintainabilit...|$|R
40|$|Top-down (LL) parsers {{are easy}} to express in {{functional}} programming languages, but the elegant functional programming model can also serve as an exact prototype for a `real' implementation of the technology in ansi C. The result is a compiler-compiler that takes unlimited lookahead and backtracking, the extended BNF notation, and parameterized grammars with (higher order) metaparameters {{to the world of}} C programming. The generated code is standard ansi C and is `plug compatible' with Lex-generated <b>lexical</b> <b>analyzers</b> prepared for the Unix Yacc compiler-compiler. In contrast to Yacc, however, the generated code is modular and thus allows parts of scripts to be compiled separately and linked in incrementally, but it remains efficient, as is demonstrated by the example occam parser treated in depth here. Keywords: Compiler-compiler, parsers, Yacc, infinite-lookahead, top-down parsing, grammars, Occam...|$|R
5000|$|... {{documents}} {{filled with}} a large number of characters, crashing the <b>lexical</b> <b>analyzer</b> parsing the document.|$|E
50|$|Lex reads {{an input}} stream specifying the <b>lexical</b> <b>analyzer</b> and outputs source code {{implementing}} the lexer in the C programming language.|$|E
50|$|JetPAG (Jet Parser Auto-Generator) is an {{open source}} LL(k) parser and <b>lexical</b> <b>analyzer</b> generator, {{licensed}} under the GNU General Public License. It is a personal work of Tareq H. Sharafy, and is currently at final beta stages of development.|$|E
40|$|This paper {{presents}} a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), {{which aims to}} incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame. A class-based HMM is applied in word segmentation, and in this level unknown words are treated {{in the same way}} as common words listed in the lexicon. Unknown words are recognized with reliability in role-based HMM. As for disambiguation, the authors bring forth an n-shortest-path strategy that, in the early stage, reserves top N segmentation results as candidates and covers more ambiguity. Various experiments show that each level in HHMM contributes to lexical analysis. An HHMM-based system ICTCLAS was accomplished. The recent official evaluation indicates that ICTCLAS is one of the best Chinese <b>lexical</b> <b>analyzers.</b> In a word, HHMM is effective to Chinese lexical analysis. ...|$|R
40|$|This paper {{discusses}} a new {{methodology for}} scanner generation that supports language independent lexicon specification and automatic generation of stand alone <b>lexical</b> <b>analyzers</b> from specifications. The mechanism that {{sits at the}} basis of this methodology is the layering of the lexicon specification on two levels: in the first level, "universal" lexical constructs which are used as building blocks by most programming languages are defined, and in the second level, customized lexical constructs of specific programming languages are specified in terms of universal lexical constructs. The universal lexicon is specified by regular expressions over a global alphabet used by most programming languages, such as the character set of a keyboard, and is efficiently implemented by deterministic finite automata. The customized lexicon is conveniently specified by regular expressions of properties of universal lexical constructs and is implemented by nondeterministic automata whose transition functi [...] ...|$|R
40|$|The {{construction}} of minimal acyclic deterministic partial finite automata to represent large natural language vocabularies is described. Applications of such automata include: spelling checkers and advisers, multilanguage dictionaries, thesauri, minimal perfect hashing and text compression. Part {{of this research}} was supported by a grant awarded by the Brazilian National Council for Scientific and Technological Development (CNPq) to the second author. Authors' Address: Cl'audio L. Lucchesi and Tomasz Kowaltowski, Department of Computer Science, University of Campinas, Caixa Postal 6065, 13081 Campinas, SP, Brazil. E-mail: lucchesi@dcc. unicamp. br and tomasz@dcc. unicamp. br. 1 Introduction The use of finite automata (see for instance [5]) to represent sets of words is a well established technique. Perhaps the most traditional application is found in compiler construction where such automata can be used to model and implement efficient <b>lexical</b> <b>analyzers</b> (see [1]). Applications of finit [...] ...|$|R
50|$|Paxson is {{also the}} {{original}} author of the flex <b>lexical</b> <b>analyzer</b> and the Bro intrusion detection system. Backscatter is a term coined by Vern Paxson to describe Internet background noise resulting from a DDoS attack using multiple spoofed addresses.|$|E
50|$|Lex is {{commonly}} used with the yacc parser generator. Lex, originally written by Mike Lesk and Eric Schmidt and described in 1975, is the standard <b>lexical</b> <b>analyzer</b> generator on many Unix systems, and an equivalent tool is specified {{as part of the}} POSIX standard.|$|E
50|$|An {{interpreter}} {{might well}} {{use the same}} <b>lexical</b> <b>analyzer</b> and parser as the compiler and then interpret the resulting abstract syntax tree.Example data type definitions for the latter, and a toy interpreter for syntax trees obtained from C expressions are shown in the box.|$|E
40|$|As new {{high-throughput}} {{technologies have}} created an explosion of biomedical literature, there arises a pressing need for automatic information extraction from the literature bank. To this end, biomedical named entity recognition (NER) from natural language text is indispensable. Current NER approaches include: dictionary based, rule based, or machine learning based. Since there is no consolidated nomenclature for most biomedical NEs, any NER system relying on limited dictionaries or rules {{does not seem to}} perform satisfactorily. In this paper, we consider a machine learning model, CRF, for the construction of our NER framework, which is a well known model for solving other sequence tagging problems. In our framework, we fully utilize available resources including dictionaries and <b>lexical</b> <b>analyzers,</b> and represent them as linguistic features in the CRF model. In our experiment on the JNLPBA 2004 data, without any post-processing, our system achieves a satisfactory F-score of 69. 7 %. ...|$|R
40|$|We present general {{algorithms}} for minimizing sequential finite-state transducers that output strings or numbers. The algorithms {{are shown}} to be efficient since {{in the case of}} acyclic transducers and for output strings they operate in O(S + jEj + jV j + (jEj Γ jV j + jF j) Δ (jP max j + 1)) steps, where S is the sum of the lengths of all output labels of the resulting transducer, E the set of transitions of the given transducer, V the set of its states, F the set of final states, and P max one of the longest of the longest common prefixes of the output paths leaving each state of the transducer. The algorithms apply to a larger class of transducers which includes subsequential transducers. Key words: Finite automata, finite-state transducers, rational power series, semiring, shortest-paths algorithms. 1 Introduction Finite-state automata and transducers are currently used in many applications ranging from <b>lexical</b> <b>analyzers</b> [2], language and speech processing [23], to the desi [...] ...|$|R
40|$|We {{present the}} first fully general {{approach}} {{to the problem of}} incremental lexical analysis. Our approach utilizes existing generators of (batch) <b>lexical</b> <b>analyzers</b> to derive the information needed by an incremental run-time system. No changes to the generator’s algorithms or run-time mechanism are required. The entire pattern language of the original tool is supported, including such features as multiple user-defined states, backtracking, ambiguity tolerance, and non-regular pattern recognition. No a priori bound is placed on the amount of lookahead; dependencies are tracked dynamically as required. This combined flexibility makes it possible to specify the lexical rules for real programming languages in a natural and expressive manner. The incremental lexers produced by our approach require little additional storage, run in optimal time, accommodate arbitrary (mixed) structural and textual modifications, and can retain conceptually unchanged tokens within the updated regions through aggressive reuse. We present a correctness proof and a complete performance analysis and discuss the use of this algorithm as part of a system for fine-grained incremental recompilation...|$|R
50|$|Traditionally, a <b>lexical</b> <b>analyzer</b> {{represents}} tokens (the {{small units}} of indivisible character values) as discrete string objects. This approach is designated extractive parsing. In contrast, non-extractive tokenization mandates that one keeps the source text intact, and uses offsets and lengths to describe those tokens.|$|E
50|$|Assigned to {{work with}} the National Military Command Center, the {{information}} processing branch of the Joint Chiefs of Staff, Barr rewrote and enhanced FFS, implementing three of its five major components: retrieval, sorting, and file update. His work featured the innovation of a uniform <b>lexical</b> <b>analyzer</b> for all languages in the system with a uniform method of handling all error messages.|$|E
50|$|MySQL {{is written}} in C and C++. Its SQL parser {{is written in}} yacc, but it uses a home-brewed <b>lexical</b> <b>analyzer.</b> MySQL works on many system platforms, {{including}} AIX, BSDi, FreeBSD, HP-UX, eComStation, i5/OS, IRIX, Linux, macOS, Microsoft Windows, NetBSD, Novell NetWare, OpenBSD, OpenSolaris, OS/2 Warp, QNX, Oracle Solaris, Symbian, SunOS, SCO OpenServer, SCO UnixWare, Sanos and Tru64. A port of MySQL to OpenVMS also exists.|$|E
40|$|This {{document}} {{explains how}} {{to construct a}} compiler using lex and yacc. Lex and yacc are tools used to generate <b>lexical</b> <b>analyzers</b> and parsers. I assume you can program in C, and understand data structures such as linked-lists and trees. The introduction describes the basic building blocks of a compiler and explains the interaction between lex and yacc. The next two sections describe lex and yacc in more detail. With this background, we construct a sophisticated calculator. Conventional arithmetic operations and control statements, such as if-else and while, are implemented. With minor changes, we convert the calculator into a compiler for a stack-based machine. The remaining sections discuss issues that commonly arise in compiler writing. Source code for examples may be downloaded from the web site listed below. Permission to reproduce portions of this document is given provided the web site listed below is referenced, and no additional restrictions apply. Source code, when part o...|$|R
40|$|We {{present an}} {{algorithm}} for minimizing sequential transducers. This algorithm {{is shown to}} be efficient, since {{in the case of}} acyclic transducers it operates in O(jEj + jV j + (Ej Γ jV j + jF j) :(jPmax j + 1) steps, where E is the set of edges of the given transducer, V the set of its vertices, F the set of final states, and Pmax the longest of the greatest common prefixes of the output paths leaving each state of the transducer. It can be applied to a larger class of transducers which includes subsequential transducers. 1 Introduction Finite automata and transducers are used in many efficient programs. They allow to produce in a very easy way <b>lexical</b> <b>analyzers</b> for complex languages. In some applications as in Natural Language Processing the involved finite-state machines can contain several hundreds of thousands of states. Reducing the size of these graphs without losing their recognition properties is then crucial. This problem has been solved in the case of deterministic autom [...] ...|$|R
40|$|We {{report the}} {{development}} of a program, called KtoMTran, that converts KENO input files to MCNP input files. The code is based on parser and <b>lexical</b> <b>analyzers</b> Bison and Flex for translation. It has been tested with the sample cases that are provided with the KENO code as part of the SCALE package by RSICC. The input file conversion program still lacks capability to identify and translate certain special features in KENO input files. However, twenty-six out of the thirty-two input files for KENO sample problems are converted correctly and do not need any manual “fixing ” before they can be run with MCNP. Five cases required minor modifications due to special boundary conditions. Since the translator program is not yet capable of handling criticality problems with a hole {{at the center of the}} geometry, it is not able to translate one such case in the KENO package. We also report a comparison of results of some selected problems obtained using KENO and MCNP. Key Words: KENO, MCNP, geometry, input file, translato...|$|R
