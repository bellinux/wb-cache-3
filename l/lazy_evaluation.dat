637|68|Public
25|$|Jan-Willem Maessen, in 2002, and Simon Peyton Jones, in 2003, {{discussed}} {{problems associated}} with <b>lazy</b> <b>evaluation</b> while also acknowledging the theoretical motives for it, in addition to purely practical considerations such as improved performance. They note that, in addition to adding some performance overhead, <b>lazy</b> <b>evaluation</b> {{makes it more difficult}} for programmers to reason about the performance of their code (particularly its space use).|$|E
25|$|These primitives, {{which produce}} or handle values known as promises, {{can be used}} to {{implement}} advanced <b>lazy</b> <b>evaluation</b> constructs such as streams.|$|E
25|$|Related to <b>lazy</b> <b>evaluation</b> is the {{construction}} of lazy lists using gather and take, behaving somewhat like generators in languages like Icon or Python.|$|E
40|$|AbstractConstraint Satisfaction Problems (CSPs) {{represent}} a widely used framework for many real-life problems. Constraint Logic Programming (CLP) languages are an effective tool for modeling problems {{in terms of}} CSPs and solving them efficiently. <b>Lazy</b> domain <b>evaluation</b> is a solving technique that has proven effective for solving CSPs, allowing the minimization {{of the number of}} constraint checks. However, exploiting <b>lazy</b> domain <b>evaluation</b> in CLP is not very effective, mainly because of the chronological backtracking rule used in CLP. After each backtracking step, in fact, all the obtained results are lost, even if they {{had nothing to do with}} the culprit of the failure. The intelligent backtracking rule widely studied in the past does not solve the problem either. In this paper, we propose a backtracking rule useful for dealing efficiently and declaratively with <b>lazy</b> domain <b>evaluation</b> in CLP, and we show a simple implementation of a metainterpreter providing the depicted functionality...|$|R
40|$|We {{present a}} generic C++ design to perform {{efficient}} and exact geometric computations using <b>lazy</b> <b>evaluations.</b> Exact geometric computations {{are critical for}} the robustness of geometric algorithms. Their efficiency is also critical for most applications, hence the need for delaying the exact computations at run time until they are actually needed. Our approach is generic and extensible {{in the sense that}} it is possible to make it a library which users can extend to their own geometric objects or primitives. It involves techniques such as generic functor adaptors, dynamic polymorphism, reference counting for the management of directed acyclic graphs and exception handling for detecting cases where exact computations are needed. It also relies on multiple precision arithmetic as well as interval arithmetic. We apply our approach to the whole geometric kernel of Cgal...|$|R
40|$|Abstract. We {{realized}} operations {{appeared in}} the theory of automata using Haskell languages. Using the benefits of functions of <b>lazy</b> <b>evaluations</b> in Haskell, we can express a language set which contains infinite elements as concrete functional notations like mathematical notations. Our modules can be used not only for analyzing the properties about automata and their application systems but also for self study materials or a tutorial to learn automata, grammar and language theories. We also implemented the modules for sticker systems. Paun and Rozenberg explained a concrete method to transform an automaton to a sticker system in 1998. We modified their definitions and improved their insufficient results. Using our module functions, we can easily define finite automata and linear grammars and construct sticker systems which have the same power of finite automata and linear grammars...|$|R
25|$|Whether a term is {{normalising}} or not, and {{how much}} work {{needs to be done}} in normalising it if it is, depends to a large extent on the reduction strategy used. The distinction between reduction strategies relates to the distinction in functional programming languages between eager evaluation and <b>lazy</b> <b>evaluation.</b>|$|E
25|$|Haskell {{features}} <b>lazy</b> <b>evaluation,</b> pattern matching, list comprehension, type {{classes and}} type polymorphism. It is a purely functional language, {{which means that}} functions generally have no side effects. A distinct construct exists to represent side effects, orthogonal {{to the type of}} functions. A pure function can return a side effect that is subsequently executed, modeling the impure functions of other languages.|$|E
25|$|Most purely {{functional}} programming languages (notably Miranda and its descendents, including Haskell), and the proof languages of theorem provers, use <b>lazy</b> <b>evaluation,</b> {{which is essentially}} the same as call by need. This is like normal order reduction, but call by need manages to avoid the duplication of work inherent in normal order reduction using sharing. In the example given above, (λx.xx) ((λx.x)y) reduces to ((λx.x)y) ((λx.x)y), which has two redexes, but in call by need they are represented using the same object rather than copied, so when one is reduced the other is too.|$|E
40|$|AbstractWe {{present a}} generic C++ design to perform exact {{geometric}} computations efficiently using <b>lazy</b> <b>evaluations.</b> Exact geometric computations {{are critical for}} the robustness of geometric algorithms. Their efficiency is also important for many applications, hence the need for delaying the costly exact computations at run time until they are actually needed, if at all. Our approach is generic and extensible {{in the sense that}} it is possible to make it a library that users can apply to their own geometric objects and primitives. It involves techniques such as generic functor-adaptors, static and dynamic polymorphism, reference counting for the management of directed acyclic graphs, and exception handling for triggering exact computations when needed. It also relies on multi-precision arithmetic as well as interval arithmetic. We apply our approach to the whole geometry kernel of Cgal...|$|R
40|$|MI: Global COE Program Education-and-Research Hub for Mathematics-for-IndustryグローバルCOEプログラム「マス･フォア･インダストリ教育研究拠点」We {{realized}} operations {{appeared in}} the theory of automata using Haskell languages. Using the benefits of functions of <b>lazy</b> <b>evaluations</b> in Haskell, we can express a language set which contains infinite elements as concrete functional notations like mathematical notations. Our modules can be used not only for analyzing the properties about automata and their application systems but also for self study materials or a tutorial to learn automata, grammar and language theories. We also implemented the modules for sticker systems. Paun and Rozenberg explained a concrete method to transform an automaton to a sticker system in 1998. We modified their definitions and improved their insufficient results. Using our module functions, we can easily define finite automata and linear grammars and construct sticker systems which have the same power of finite automata and linear grammars...|$|R
40|$|Huet and Levy pioneered <b>lazy</b> {{sequential}} <b>evaluation</b> of equational programs {{based on}} the concepts of strong-sequentiality and needed redexes. Natural extensions of their strategy are not well-suited for parallel evaluation since they do not support independent searches for needed redexes along different paths in the input term. Furthermore, the size of compiled code can be exponential in program size. We therefore propose a different notion of sequentiality called path-sequentiality that overcomes these drawbacks and thus provides a natural framework for <b>lazy</b> parallel <b>evaluation.</b> We present a sound and complete algorithm for lazy parallel normalization of path-sequential systems. We show that our algorithm is optimal {{in the sense that}} its time complexity is bounded only by the time required to perform the needed reductions. The results presented in this paper are applicable to functional languages as well through the transformation of Laville...|$|R
25|$|In a {{language}} with <b>lazy</b> <b>evaluation,</b> like Haskell, a list is evaluated {{only to the}} degree that its elements are requested: for example, if one asks for the first element of a list, only the first element will be computed. With respect to usage of the list monad for non-deterministic computation that means that we can non-deterministically generate a lazy list of all results of the computation and ask for the first of them, and only as much work will be performed as is needed to get that first result. The process roughly corresponds to backtracking: a path of computation is chosen, and then if it fails at some point (if it evaluates mzero), then it backtracks to the last branching point, and follows the next path, and so on. If the second element is then requested, it again does just enough work to get the second solution, and so on. So the list monad is a simple way to implement a backtracking algorithm in a lazy language.|$|E
2500|$|Perl 6 {{uses the}} {{technique}} of <b>lazy</b> <b>evaluation</b> of lists {{that has been a}} feature of some functional programming languages such as Haskell: ...|$|E
2500|$|... $squares {{will be an}} {{infinite}} list of square numbers, but <b>lazy</b> <b>evaluation</b> of the gather ensures that elements are only computed when they are accessed.|$|E
40|$|Many {{problems}} in artificial intelligence require adaptively making {{a sequence of}} decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using <b>lazy</b> <b>evaluations.</b> We illustrate {{the usefulness of the}} concept by giving several examples of adaptive submodular objectives arising in diverse AI applications including management of sensing resources, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations. 1...|$|R
40|$|In {{cellular}} heterogeneous networks (HetNets), offloading {{users to}} {{small cell base stations}} (SBSs) {{leads to a}} degradation in signal to interference plus noise ratio (SINR) and results in high outage probabilities for offloaded users. In this paper, we propose a novel framework to solve the cell association problem with the intention of improving user outage performance while achieving load balancing across different tiers of BSs. We formulate a combinatorial utility maximization problem with weighted BS loads that achieves proportional fairness among users and also takes into account user outage performance. A formulation of the weighting parameters is proposed to discourage assigning users to BSs with high outage probabilities. In addition, we show that the combinatorial optimization problem can be reformulated as a monotone submodular maximization problem and it can be readily solved via a greedy algorithm with <b>lazy</b> <b>evaluations.</b> The obtained solution offers a constant performance guarantee to the cell association problem. Simulation results show that our proposed approach leads to over 30 % reduction in outage probabilities for offloaded users and achieves load balancing across macrocell and small cell BSs...|$|R
40|$|This thesis {{describes}} the design, implementation {{and use of}} a run-time debugging tool for understanding the lazy semantics and locating failures in the functional logic language Curry. We provide a means for programmers to step in the evaluation order of program expressions at a source code level. Every expression evaluated is detected by a program coverage in a layout of the source code. Its run-time value can be represented to the user. The user can stop the execution of a program whenever he or she chooses to do so. A means to backward stepping is also provided. For large programs, we record only partial computations that are generated by evaluating selected expressions from the user. To achieve these means, we suggest and use some annotations in programs. Repre- sentation of intermediate steps of evaluations in a single-step mode is also provided by a distributed programming technique. Stepping in the real order of <b>lazy</b> <b>evaluations</b> could be helpful in searching for failures in simple programs and to beginners in understanding the behavior of functions in functional logic languages...|$|R
2500|$|Phil Abrams' much-cited paper [...] "An APL Machine" [...] {{illustrated}} how APL {{could make}} {{effective use of}} <b>lazy</b> <b>evaluation</b> where calculations would not actually be performed until the results were needed and then only those calculations strictly required. An obvious (and easy to implement) <b>lazy</b> <b>evaluation</b> is the J-vector: when a monadic iota is encountered in the code, it is kept as a representation instead of being expanded in memory; in future operations, a J-vectors contents are the loop's induction register, not reads from memory.|$|E
2500|$|Haskell {{features}} a type system with type inference and <b>lazy</b> <b>evaluation.</b> Type classes [...] {{first appeared in}} the Haskell programming language. Its main implementation is the Glasgow Haskell Compiler.|$|E
2500|$|The {{pipeline}} {{structure of}} the bind operator ensures that the [...] and [...] operations get evaluated only once and in the given order, so that the side-effects of extracting text from the input stream and writing to the output stream are correctly handled in the functional pipeline. This remains true even if the language performs out-of-order or <b>lazy</b> <b>evaluation</b> of functions.|$|E
50|$|This {{mathematical}} {{definition is}} structured {{so that it}} represents the result, and {{not the way it}} gets calculated. However the result may be different between <b>lazy</b> and eager <b>evaluation.</b> This difference is described in the evaluation formulas.|$|R
40|$|We {{refer to}} {{strategies}} setting {{the objective of}} a computation step rather than to evaluation strategies (as eager or <b>lazy</b> <b>evaluations).</b> We use these strategies to define semantics of a system starting from its elementary operational entities, and then combining them. The tactics of a strategy are given by rewriting steps. While a strategy is {{at a higher level}} and defines ”what is the goal ” of a computation, the tactics are at a lower level and tell ”how it is possible to reach the goal”. We use rewriting systems as a general model of computing. In this framework we give an operational semantics of the strategic transitions in terms of tactical rewritings. The approach is inspired by a new model of computation given by membrane systems. We define the strategy semantics for membrane systems involving the maximal parallel rewriting and priorities. We show that strategies are not powerful enough to define alone the semantics of membrane systems involving promoters. This is possible when we encode the state of the membrane in a richer structure. Key words: strategic transitions, tactical rewritings, operational semantics, membrane computing...|$|R
50|$|In lazy {{programming}} languages such as Haskell, {{although the}} default is to evaluate expressions {{only when they}} are demanded, it is possible {{in some cases to}} make code more eager—or conversely, to make it more lazy again after it has been made more eager. This can be done by explicitly coding something which forces evaluation (which may make the code more eager) or avoiding such code (which may make the code more <b>lazy).</b> Strict <b>evaluation</b> usually implies eagerness, but they are technically different concepts.|$|R
2500|$|Robert Harper, one of {{the authors}} of Standard ML, has given his reasons for not using Haskell to teach {{introductory}} programming. Among these are the difficulty of reasoning about resource use with non-strict evaluation, that <b>lazy</b> <b>evaluation</b> complicates the definition of data types and inductive reasoning, and the [...] "inferiority" [...] of Haskell's (old) class system compared to ML's module system.|$|E
2500|$|Eugenio Moggi first {{described}} the general use of monads to structure programs in 1991. Several people built on his work, including programming language researchers Philip Wadler and Simon Peyton Jones (both {{of whom were}} involved in the specification of Haskell). Early versions of Haskell used a problematic [...] "lazy list" [...] model for I/O, and Haskell 1.3 introduced monads as a more flexible way to combine I/O with <b>lazy</b> <b>evaluation.</b>|$|E
2500|$|Ben Lippmeier {{designed}} Disciple as a strict-by-default (lazy by explicit annotation) dialect of Haskell with a type-and-effect system, {{to address}} Haskell's difficulties in reasoning about <b>lazy</b> <b>evaluation</b> and in using traditional data {{structures such as}} mutable arrays. [...] He argues (p.20) that [...] "destructive update furnishes the programmer with two important and powerful tools... a set of efficient array-like data structures for managing collections of objects, and ... the ability to broadcast a new value to all parts of a program with minimal burden on the programmer." ...|$|E
40|$|When {{monitoring}} spatial phenomena, {{which can}} often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove {{that the problem of}} finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomialtime approximation that is within (1 − 1 /e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit <b>lazy</b> <b>evaluations</b> and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets...|$|R
40|$|Efficiently {{maintaining}} molecular conformations {{is important}} for molecular modeling and protein engineering. This paper reviews the widely used simple rotations scheme, simple local frames method, and introduces a new atom group local frames method for maintaining the molecular conformation changes due to the changes of torsional angles. The simple rotations scheme applies a sequence of rotations to update all atom positions. The order of the updates is important and some bookkeeping of the atom positions is necessary. Numeric errors can accumulate as rotations around the bonds are repeated. The simple local frames method builds local frames at the bonds, and relational matrices {{between parents and children}} frames are constructed. The atom positions are computed by chaining series of such matrices. No bookkeeping is necessary and numeric errors do not accumulate upon rotations. Multiple local frames are needed at a bond if it has more than one child. This paper introduces a new atom group local frames method to efficiently maintain molecular conformations. A single local frame is attached to each atom group. Bookkeeping is not necessary and numeric errors do not accumulate upon rotations. This method also provides <b>lazy</b> <b>evaluations</b> for atom positions. Thus, the conformational maintenance cost is greatly reduced, especially when many conformations are generated and updated such as in a minimization process...|$|R
40|$|Solving {{stochastic}} optimization problems under partial observability, {{where one}} needs to adaptively make decisions with uncertain outcomes, is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using <b>lazy</b> <b>evaluations.</b> We illustrate {{the usefulness of the}} concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations. Comment: 60 pages, 6 figures. Version 5 addresses a flaw in the proof of Theorem 13 identified by Nan and Saligrama (2017). The revision includes a weaker version of Theorem 13, guaranteeing squared logarithmic approximation under an additional strong adaptive submodularity condition. This condition is met by all applications considered in the paper, as discussed in the revised Sections 7, 8 and...|$|R
50|$|<b>Lazy</b> <b>evaluation</b> {{can lead}} to {{reduction}} in memory footprint, since values are created when needed. However, <b>lazy</b> <b>evaluation</b> is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate. <b>Lazy</b> <b>evaluation</b> can introduce space leaks.|$|E
50|$|The usual {{implementation}} strategy for <b>lazy</b> <b>evaluation</b> in functional languages is graph reduction. <b>Lazy</b> <b>evaluation</b> {{is used by}} default in several pure functional languages, including Miranda, Clean, and Haskell.|$|E
5000|$|... {{argues for}} <b>lazy</b> <b>evaluation</b> as a {{mechanism}} for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams. Launchbury 1993 [...] describes some difficulties that <b>lazy</b> <b>evaluation</b> introduces, particularly in analyzing a program's storage requirements, and proposes an operational semantics to aid in such analysis. Harper 2009 proposes including both strict and <b>lazy</b> <b>evaluation</b> in the same language, using the language's type system to distinguish them.|$|E
40|$|Software of {{microcontrollers}} {{is getting}} more and more complex. It is mandatory to extensively analyze their software as errors can lead to severe failures or cause high costs. Model checking is a formal method used to verify whether a system satisfies certain properties. This thesis describes a new approach for model checking software for microcontrollers. In this approach, assembly code is used for model checking instead of an intermediate representation such as C code. The development of [mc]square, which is a microcontroller assembly code model checker implementing this approach, is detailed. [mc]square has a modular architecture to cope with the hardware dependency of this approach. The single steps of the model checking process are divided into separate packages. The creation of the states is conducted by a specific simulator, which is the only hardware-dependent package. Within the simulator, the different microcontrollers are modeled accurately. This work describes the modeling of the ATMEL ATmega 16 microcontroller and details implemented abstraction techniques, which are used to tackle the state-explosion problem. These abstraction techniques include <b>lazy</b> interrupt <b>evaluation,</b> <b>lazy</b> stack <b>evaluation,</b> delayed nondeterminism, dead variable reduction, and path reduction. Delayed nondeterminism introduces symbolic states, which represent a set of states, into [mc]square while still explicit model checking techniques are used. Thus, we successfully combined explicit and symbolic model checking techniques. A formal model of the simulator, which we developed to prove the correctness of abstraction techniques, is described. In this work, the formal model is used to show the correctness of delayed nondeterminism. To show the applicability of the approach, two case studies are described. In these case studies, we used programs of different sizes. All these programs were created by students in lab courses, during diploma theses, or in exercises without the intention to use them for model checking...|$|R
40|$|A {{medium sized}} algebra system {{supporting}} rational functions and some elementary functions, which {{is written in}} the purely functional subset of LISP is described. This is used to investigate the practicability of writing systems in a no-side effect, no property list, pure style. In addition, using the experimental LISP system in Bath that allows for full environment closures, ways have been discovered in which eager (applicative) <b>evaluation</b> and <b>lazy</b> (normal) <b>evaluation</b> strategies {{can be applied to}} computer algebra. The system is demonstrated on some well known sample programs...|$|R
40|$|We {{present a}} new {{approach}} for accelerated global illumination computation in scenes with glossy surfaces. Our algorithm combines sparse illumination computation used in the radiance caching algorithm with BRDF importance sampling. To make this approach feasible, we extend the idea of <b>lazy</b> illumination <b>evaluation,</b> used in the caching approaches, from the spatial to the directional domain. Using importance sampling allows us to apply caching not only on low-gloss but also on shiny materials with high-frequency BRDFs, for which the radiance caching algorithm breaks down. Categories and Subject Descriptors (according to ACM CCS) ...|$|R
