115|0|Public
50|$|In {{computational}} linguistics, <b>lemmatisation</b> is the {{algorithmic process}} {{of determining the}} lemma of a word based on its intended meaning. Unlike stemming, <b>lemmatisation</b> depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient <b>lemmatisation</b> algorithms is an open area of research.|$|E
5000|$|In {{search engine}} optimization, {{the process of}} <b>lemmatisation</b> {{includes}} four steps: ...|$|E
50|$|<b>Lemmatisation</b> is {{the process}} of {{converting}} a word to its canonical form.|$|E
50|$|Prior to the keyword clustering, {{search engine}} {{optimization}} experts developed keyword grouping tools {{based on the}} process known as <b>lemmatisation.</b> Lemma is a base or dictionary form of a word (without inflectional endings). In linguistics, <b>lemmatisation</b> {{is a process of}} grouping together the different inflected forms of a word so they can be analyzed as a single item.|$|E
5000|$|MontyLemmatiser: part-of-speech {{sensitive}} <b>lemmatisation.</b> Strips plurals (geese-->goose) {{and tense}} (were-->be, had-->have). Includes regexps from Humphreys and Carroll's morph.lex, and UPENN's XTAG corpus ...|$|E
5000|$|The word [...] "walk" [...] is {{the base}} form for word [...] "walking", and hence this is matched in both {{stemming}} and <b>lemmatisation.</b>|$|E
5000|$|Roberto Busa {{finishes}} the Index Thomisticus, a complete <b>lemmatisation</b> of the 56 printed volumes of Saint Thomas Aquinas {{and of a}} few related authors.|$|E
5000|$|<b>Lemmatisation</b> {{is closely}} related to stemming. The {{difference}} is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster. The reduced [...] "accuracy" [...] may not matter for some applications. In fact, when used within information retrieval systems, stemming improves query recall accuracy, or true positive rate, when compared to <b>lemmatisation.</b> Nonetheless, stemming reduces precision, or true negative rate, for such systems.|$|E
50|$|<b>Lemmatisation</b> (or lemmatization) in {{linguistics}} {{is the process}} of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.|$|E
50|$|Morphological {{analysis}} of published biomedical literature can yield useful results. Morphological processing of biomedical text {{can be more}} effective by a specialised <b>lemmatisation</b> program for biomedicine, and may improve the accuracy of practical information extraction tasks.|$|E
5000|$|The word [...] "meeting" [...] can {{be either}} the base form of a noun or a form of a verb ("to meet") {{depending}} on the context; e.g., [...] "in our last meeting" [...] or [...] "We are meeting again tomorrow". Unlike stemming, <b>lemmatisation</b> attempts to select the correct lemma depending on the context.|$|E
50|$|In {{morphology}} and lexicography, a lemma (plural lemmas or lemmata) is {{the canonical}} form, dictionary form, or citation {{form of a}} set of words (headword). In English, for example, run, runs, ran and running are forms of the same lexeme, with run as the lemma. Lexeme, in this context, refers to the set of all the forms that have the same meaning, and lemma refers to the particular form that is chosen by convention to represent the lexeme. In lexicography, this unit is usually also the citation form or headword by which it is indexed. Lemmas have special significance in highly inflected languages such as Arabic, Turkish and Russian. The process of determining the lemma for a given word is called <b>lemmatisation.</b> The lemma can be viewed as the chief of the principal parts, although <b>lemmatisation</b> is at least partly arbitrary.|$|E
5000|$|Stochastic {{algorithms}} involve using probability {{to identify}} the root form of a word. Stochastic algorithms are trained (they [...] "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed {{in the form of}} complex linguistic rules, similar in nature to those in suffix stripping or <b>lemmatisation.</b> Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and <b>lemmatisation,</b> except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied {{on the grounds that the}} output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).|$|E
50|$|A {{more complex}} {{approach}} {{to the problem of}} determining a stem of a word is <b>lemmatisation.</b> This process involves first determining the part of speech of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.|$|E
50|$|More {{advanced}} methods, <b>lemmatisation</b> methods, group {{together the}} inflected forms {{of a word}} through more complex rule sets based on a word’s part of speech or its record in a lexical database, transforming an inflected word through lookup or a series of transformations to its lemma. For a long time, it was taken to be proven that morphological normalisation by and large did not help retrieval performance.|$|E
50|$|He was {{a senior}} {{lecturer}} at the Department of Comparative and General Linguistics, Faculty of Arts, University of Ljubljana. His subjects of instruction are language technologies with stress on <b>Lemmatisation.</b> From 2001 to 2012 he was the Head of the Corpus Laboratory at the Fran Ramovš Institute of Slovene Language (within the Scientific Research Centre of the Slovenian Academy of Sciences and Arts). He participated {{in a number of}} European projects on language resources.|$|E
50|$|Machine {{translation}} can use {{a method}} based on dictionary entries, {{which means that the}} words will be translated as a dictionary does - word by word, usually without much correlation of meaning between them. Dictionary lookups may be done with or without morphological analysis or <b>lemmatisation.</b> While this approach to machine translation is probably the least sophisticated, dictionary-based machine translation is ideally suitable for the translation of long lists of phrases on the subsentential (i.e., not a full sentence) level, e.g. inventories or simple catalogs of products and services.|$|E
50|$|Some <b>lemmatisation</b> {{algorithms}} are stochastic in that, given a word {{which may}} belong to multiple parts of speech, a probability {{is assigned to}} each possible part. This may {{take into account the}} surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.|$|E
5000|$|Hybrid {{approaches}} use {{two or more}} of {{the approaches}} described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of [...] "frequent exceptions" [...] like [...] "ran => run". If the word is not in the exception list, apply suffix stripping or <b>lemmatisation</b> and output the result.|$|E
50|$|RetrievalWare is a {{relevancy}} ranking text search {{system with}} processing enhancements {{drawn from the}} fields of natural language processing (NLP) and semantic networks. NLP algorithms include dictionary-based stemming (also known as <b>lemmatisation)</b> and dictionary-based phrase identification. Semantic networks are used by RetrievalWare to expand the query words entered by the user to related terms with terms weights determined by {{the distance from the}} user's original terms. In addition to automatic expansion, a feedback-mode whereby users could choose {{the meaning of the word}} before performing the expansion was available. The first semantic networks were built using WordNet.|$|E
50|$|Suffix {{stripping}} approaches {{enjoy the}} benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those lexical categories which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. <b>Lemmatisation</b> attempts to improve upon this challenge.|$|E
50|$|Automatic content {{analysis}} represents a slight departure from McMillan's online {{content analysis}} procedure in that human coders are being supplemented by a computational method, {{and some of}} these methods do not require categories to be defined in advanced. Quantitative textual analysis models often employ 'bag of words' methods that remove word ordering, delete words that are very common and very uncommon, and simplify words through <b>lemmatisation</b> or stemming that reduces the dimensionality of the text by reducing complex words to their root word. While these methods are fundamentally reductionist in the way they interpret text, they can be very useful if they are correctly applied and validated.|$|E
5000|$|In {{addition}} to the main user interface of the electronic dictionary, a ‘hotkey’ feature allows the user to click on a word in any program that uses editable text including web browsers and PDF documents, and source code. When a word is clicked, the translation or definition is displayed in a small pop-up window. The hotkey tool does not require previous launching of the software application.The word stemming function allows searches from inflected forms of a word into the root word, such as in French, (allez → aller) with further extension by <b>lemmatisation.</b> Full verb conjugation in all tenses, and number conversion from digits to text in the available languages (e.g. [...] "123" [...] → [...] "one hundred twenty-three"), are available through the program interface, as well as free access to online examples of language in use, and a discussion forum moderated by linguists and lexicographers.|$|E
50|$|Constraint Grammar (CG) is a methodological {{paradigm}} for {{natural language processing}} (NLP). Linguist-written, context dependent rules are compiled into a grammar that assigns grammatical tags ("readings") to words or other tokens in running text. Typical tags address <b>lemmatisation</b> (lexeme or base form), inflexion, derivation, syntactic function, dependency, valency, case roles, semantic type etc. Each rule either adds, removes, selects or replaces a tag or a set of grammatical tags in a given sentence context. Context conditions {{can be linked to}} any tag or tag set of any word anywhere in the sentence, either locally (defined distances) or globally (undefined distances). Context conditions in the same rule may be linked, i.e. conditioned upon each other, negated, or blocked by interfering words or tags. Typical CGs consist of thousands of rules, that are applied set-wise in progressive steps, covering ever more advanced levels of analysis. Within each level, safe rules are used before heuristic rules, and no rule is allowed to remove the last reading of a given kind, thus providing a high degree of robustness.|$|E
40|$|We {{present a}} novel {{approach}} {{to the task of}} word <b>lemmatisation.</b> We formalise <b>lemmatisation</b> as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language. In this way, a <b>lemmatisation</b> system can be trained and tested using any supervised tagging model. In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. We test our approach on eight languages reaching a new state-of-the-art level for the <b>lemmatisation</b> task. ...|$|E
40|$|The aim of {{this article}} is to provide a {{perspective}} on lexicographic traditions, <b>lemmatisation</b> strategies and <b>lemmatisation</b> approaches in Bantu language dictionaries from a South African point of view. It will be argued that Bantu language lexicography reflects a complex interplay of lexicographic traditions and <b>lemmatisation</b> approaches. The focus will be on Sepedi 1 – English dictionaries and on the analysis of the Oxford Northern Sotho School Dictionary, henceforth (ONSD). The ONSD will be evaluated in terms of the presumed best practices in terms of <b>lemmatisation</b> and against the background of the user-perspective. 1...|$|E
40|$|We {{present a}} novel {{approach}} to noun phrase <b>lemmatisation</b> where the main phase is cast as a tagging problem. The idea draws on the observation that the <b>lemmatisation</b> of almost all Polish noun phrases may be decomposed into transformation of singular words (tokens) that make up each phrase. We perform evaluation, which shows results similar to those obtained earlier by a rule-based system, while our approach allows to separate chunking from <b>lemmatisation.</b> ...|$|E
40|$|We {{present a}} novel tool for {{morphological}} analysis of Serbian, {{which is a}} low-resource language with rich morphology. Our tool produces <b>lemmatisation</b> and morphological analysis reaching accuracy that is considerably higher compared to the existing alternative tools: 83. 6 % relative error reduction on <b>lemmatisation</b> and 8. 1 % relative error reduction on morphological analysis. The system is trained on a small manually annotated corpus with an approach based on Bidirectional Sequence Classification and Guided Learning techniques, which have recently been adapted with success to a broad set of NLP tagging tasks. In the system presented in this paper, this general approach to tagging {{is applied to the}} <b>lemmatisation</b> task for the first time thanks to our novel formulation of <b>lemmatisation</b> as a category tagging task. We show that learning <b>lemmatisation</b> rules from annotated corpus and integrating the context information in the process of morphological analysis provides a state-of-the-art performance despite the lack of resources. The proposed system can be used via a web GUI that deploys its best scoring configuration...|$|E
40|$|This project {{sets out}} to {{discover}} and develop techniques for the <b>lemmatisation</b> of a historical corpus of the Cornish language in order that a lemmatised dictionary macrostructure can be generated from the corpus. The system should be capable of uniquely identifying every lexical item that is attested in the corpus. A survey of published and unpublished Cornish dictionaries, glossaries and lexicographical notes was carried out. A corpus was compiled incorporating specially prepared new critical editions. An investigation {{into the history of}} Cornish <b>lemmatisation</b> was undertaken. A systemic description of Cornish inflection was written. Three methods of corpus <b>lemmatisation</b> were trialed. Findings were as follows. Lexicographical history shapes current Cornish lexicographical practice. Lexicon based tokenisation has advantages over character based tokenisation. System networks provide the means to generate base forms from attested word types. Grammatical difference is the most reliable way of disambiguating homographs. A lemma that contains three fields, the canonical form, the part-of-speech and a semantic field label, provides of a unique code for every lexeme attested in the corpus. Programs which involve human interaction during the <b>lemmatisation</b> process allow bootstrapping of the <b>lemmatisation</b> database. Computerised morphological processing may be used at least to partially create the <b>lemmatisation</b> database. Disambiguation of {{at least some of the}} most common homographs may be automated by the use of computer programs...|$|E
40|$|This paper {{presents}} the results of recent experiments on application of string distance metrics to the problem of named entity <b>lemmatisation</b> in Polish. It extends of our work in [1] by introducing new results for organisation names. Furthermore, the results presented here and in [2, 3] centering around the same topic were used to make a comparative study of the average usefulness of the numerous examined string distance metrics to <b>lemmatisation</b> of Polish named-entities of various types. In particular, we focus on <b>lemmatisation</b> of country names, organisation names and person names. JRC. G. 2 -Global security and crisis managemen...|$|E
40|$|The aim of {{this article}} is to devise the method of <b>lemmatisation</b> of strong verbs from a corpus of Old English with a view to {{maximising}} the automatic search for the inflectional forms, with the corresponding minimisation of manual revision of the verbs under analysis. The search algorithm, which consists of query strings and filters, is launched on the lemmatiser Norna, a component of the lexical database of Old English Nerthus. The conclusions of the article insist on the limits of automatic <b>lemmatisation</b> as well as the paths of refinement of the <b>lemmatisation</b> method in order to accomodate less predictable forms...|$|E
40|$|Automatic <b>lemmatisation</b> for Afrikaans Automatic <b>lemmatisation</b> is {{a general}} {{normalisation}} procedure in text processing, where all inflected forms of a lexical word are normalised to a single lemma (i. e. a meaningful, uninflected base form from which more complex word forms could be formed). Traditionally, lemmatisers are developed by writing language-specific rules to identify lemmas. In this article an alternative approach is investigated, namely a machine learning approach, to develop a lemmatiser for Afrikaans (LIA: “Lemmaidentifiseerder vir Afrikaans”). An overview regarding the process of inflection in Afrikaans is provided {{with the aim of}} identifying the categories of inflection that are relevant for <b>lemmatisation</b> in Afrikaans. The format of the input and output is described with special reference to the nine inflectional categories for Afrikaans that the system should be able to handle. Then the task of <b>lemmatisation</b> as a classification task for machine learning is described, and a concise introduction to memory-based learning is provided. The development and evaluation of LIA is discussed in detail, and it is illustrated how the performance of the initial classifier is improved through feature selection and parameter optimisation. The best classifier reaches an accuracy of 92, 8 %. The article concludes with a view on some future work...|$|E
40|$|The Nepali National Corpus (NNC) was, in {{the process}} of its creation, {{annotated}} with part-of-speech (POS) tags. This paper describes the extension of automated text and corpus annotation in Nepali from POS tags to <b>lemmatisation,</b> enabling a more complex set of corpus-based searches and analyses. This work also addresses certain practical compromises embodied in the initial tagging of the NNC. First, some particular aspects of Nepali morphology – in particular the complexity of the agglutinative verbal inflection system – necessitated improvements to the underlying tokenisation of the text before <b>lemmatisation</b> could be satisfactorily implemented. In practical terms, both the tokenisation and <b>lemmatisation</b> procedures require linguistic knowledge resources to operate successfully: a set of rules describing the default case, and a lexicon containing a list of individual exceptions: words whose form suggests a particular rule should apply to them, but where that rule in fact does not apply. These resources, particularly the lexicons of irregularities, were created by a strongly data-driven process working from analyses of the NNC itself. This approach to tokenisation and <b>lemmatisation,</b> and associated linguistic knowledge resources, may be illustrative and of use to researchers looking at other languages of the Himalayan region, most especially those that have similar morphological behaviour to Nepali...|$|E
40|$|The <b>lemmatisation</b> of idiomatic {{expressions}} {{represents a}} difficult task for any lexicographer due to their multiword composition. This results in lexicographers having to select one or more elements, under which {{they consider to be}} the most suitable location(s) to list the expression. An alternative strategy may be to divide idioms into three groups according to their semantic compositionality; pure idioms, semi idioms and figurative idioms, as each category has an individual semantic identity. In light of the proposed categorisation, this paper examines the <b>lemmatisation</b> of idioms in six bilingua...|$|E
40|$|The Euroling stemmer is {{developed}} {{for a commercial}} web site and intranet search engine called SiteSeeker. SiteSeeker is basically used in the Swedish domain but to some extent also for the English domain. CST’s lemmatiser comes from the Center for Language Technology, University of Copenhagen and was originally developed as a research prototype to create <b>lemmatisation</b> rules from training data. In this paper we compare {{the performance of the}} stemmer that uses handcrafted rules for Swedish, Danish and Norwegian as well one stemmer for Greek with CST’s lemmatiser that uses training data to extract <b>lemmatisation</b> rules for Swedish, Danish, Norwegian and Greek. The performance of the two approaches are about the same with around 10 percent errors. The handcrafted rule based stemmer techniques are easy to get started with if the programmer has the proper linguistic knowledge. The machine trained sets of <b>lemmatisation</b> rules are very easy to produce without having linguistic knowledge given that one has correct training data. 1...|$|E
40|$|International audienceThis paper aims at {{presenting}} {{some preliminary}} results for data driven <b>lemmatisation</b> for Italian. Based on a joint <b>lemmatisation</b> and part-of-speech tagging models, our system {{relies on a}} architecture {{that has already been}} proved successful for French. 'Besides' intrinsic evaluation for this task, we want to measure its usefulness and adequacy by using our system as input for the task of parsing. This approach achieves state-of-the-art parsing accuracy on unlabeled text without any gold information supplied (83. 70 % of F 1 score in a 10 -fold cross-validation setting), without requiring any prior knowledge of the language. This shows that our methodology is perfectly suitable for wide coverage parsing of Italia...|$|E
