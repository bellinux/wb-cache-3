0|54|Public
40|$|Supremum <b>norm</b> <b>loss</b> is {{intuitively}} {{more meaningful}} to quantify estimation error in statistics. In {{the context of}} multivariate nonparametric regression with unknown error, we propose a Bayesian procedure based on spike-and-slab prior and wavelet projections to estimate the regression function f and its derivatives. We show that their posteriors contract to the truth optimally and adaptively under supremum <b>norm</b> <b>loss.</b> We discovered {{that there is a}} lower limit in the range of smoothness that we can adapt to and this limit grows with dimension of the function's domain. The master theorem through exponential error test used in Bayesian nonparametrics was not adequate to deal with this problem, and we developed a new idea by bounding posterior under the regression model with a posterior arising from some quasi-white noise model, where the latter model greatly simplifies our calculations. Comment: 37 page...|$|R
40|$|This paper {{develops}} a conceptual model to link social norms and social capital with key socio-ecological characteristics of small scale economies {{in order to}} derive implications for the sustainability {{of natural resources and}} survival of these economies. Specifically, the extent of punishment required to deter defection is derived as a function of group size, rewards from defection, resource stock, and the risk of groundwater <b>loss.</b> Social <b>norms</b> which ordain collective participation and punishments can be endogenous, dynamic, and exhibit resilience. The implication of these possibilities on the sustainability of small scale economies is also evaluated. 16 page(s...|$|R
40|$|Precision matrix is of {{significant}} importance {{in a wide}} range of applications in multivariate analysis. This paper considers adaptive minimax estimation of sparse precision matrices in the high dimensional setting. Optimal rates of convergence are established for a range of matrix <b>norm</b> <b>losses.</b> A fully data driven estimator based on adaptive constrained ℓ 1 minimization is proposed and its rate of convergence is obtained over a collection of parameter spaces. The estimator, called ACLIME, is easy to implement and performs well numerically. A major step in establishing the minimax rate of convergence is the derivation of a rate-sharp lower bound. A “two-directional ” lower bound technique is applied to obtain the minimax lower bound. The upper and lower bounds together yield the optimal rates of convergence for sparse precision matrix estimation and show that the ACLIME estimator is adaptively minimax rate optimal for a collection of parameter spaces and a range of matrix <b>norm</b> <b>losses</b> simultaneously...|$|R
50|$|The 1977 election, which {{expanded}} the Legislative Assembly from 51 to 55 members, saw the Liberal Party gain four seats and come within one seat {{of being able}} to govern in their own right. The NCP made the abolition of probate on estates passing to a spouse a condition of forming a coalition with the Liberals. This change in government policy was announced by the Premier after the election. However, due to a reduction in the numbers of NCP members in the Parliament, the NCP's allocation of Ministerial positions in the 13-member Ministry, went from 3 to 2, with the <b>loss</b> of <b>Norm</b> Baxter. Neil McNeill retired from the Ministry, allowing Ian Medcalf to be appointed Attorney-General.|$|R
5000|$|Perry was {{a three-time}} All-Star {{and won the}} 1970 AL Cy Young Award, when he posted a record of 24-12. Jim and Gaylord Perry are the only {{brothers}} in Major League history to win Cy Young Awards. He also won 20 games in 1969, and won at least 17 games five times. As a batter, Perry was a switch-hitter and posted a respectable [...]199 batting average in his career. On July 3, 1973, brothers Gaylord Perry (Indians) and Jim Perry (Tigers) pitched against {{each other for the}} only regular season game in their careers. Neither finished the game, but Gaylord was charged with the 5-4 <b>loss.</b> Two <b>Norm</b> Cash home runs helped Detroit.|$|R
40|$|Loss {{functions}} {{are central to}} machine learning {{because they are the}} means by which the quality of a prediction is evaluated. Any loss that is not proper, or can not be transformed to be proper via a link function is inadmissible. All admissible losses for n-class problems can be obtained in terms of a convex body in Rn. We show this explicitly and show how some existing results simplify when viewed from this perspective. This allows the development of a rich algebra of losses induced by binary operations on convex bodies (that return a convex body). Furthermore it allows us to define an “inverse loss ” which provides a universal “substitution function ” for the Aggregating Algorithm. In doing so we show a formal connection between proper <b>losses</b> and <b>norms...</b>|$|R
40|$|We {{consider}} {{recovery of}} low-rank matrices from noisy data by shrinkage of singular values, {{in which a}} single, univariate nonlinearity is applied {{to each of the}} empirical singular values. We adopt an asymptotic framework, in which the matrix size is much larger than the rank of the signal matrix to be recovered, and the signal-to-noise ratio of the low-rank piece stays constant. For a variety of loss functions, including Mean Square Error (MSE - square Frobenius norm), the nuclear <b>norm</b> <b>loss</b> and the operator <b>norm</b> <b>loss,</b> we show that in this framework there is a well-defined asymptotic loss that we evaluate precisely in each case. In fact, each of the loss functions we study admits a unique admissible shrinkage nonlinearity dominating all other nonlinearities. We provide a general method for evaluating these optimal nonlinearities, and demonstrate our framework by working out simple, explicit formulas for the optimal nonlinearities in the Frobenius, nuclear and operator norm cases. For example, for a square low-rank n-by-n matrix observed in white noise with level σ, the optimal nonlinearity for MSE loss simply shrinks each data singular value y to √(y^ 2 - 4 nσ^ 2) (or to 0 if y 0...|$|R
40|$|We {{consider}} {{in this paper}} the problem of noisy 1 -bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax {{upper and lower bounds}} together yield the optimal rate of convergence for the Frobenius <b>norm</b> <b>loss.</b> Computational algorithms and numerical performance are also discussed. Comment: 33 pages, 3 figure...|$|R
40|$|The paper {{deals with}} the density {{estimation}} on Rd under sup- <b>norm</b> <b>loss.</b> We provide with fully data-driven estimation procedure and establish for it so called sup-norm oracle inequality. The pro- posed estimator allows {{to take into account}} not only approximation properties of the underlying density but eventual independence struc- ture as well. Our results contain, as a particular case, the complete solution of the bandwidth selection problem in multivariate density model. Usefulness of the developed approach is illustrated by appli- cation to adaptive estimation over anisotropic Nikolskii classes...|$|R
40|$|Theoretically and {{practically}} (judicial review), include two types, namely formal verification (formale toetsingrecht) and material verification (materielle toetsingrecht). The formal verification is an authority of assessing {{whether or not}} a legislative product is produced in accordance with the prevailing procedures. While a material verification is an authority to examine and measure {{whether or not a}} legal regulation contradicts with a higher level regulation, as well as whether or not an authority has the right to establish a certain regulation. Article 51 paragraph (3) of Law Number 24 Year 2003 regarding Constitutional Court state about it. In this context, material verification include extensive material, ie the whole matter of law, in part, or a small part of a word or even punctuation that can affect the norm. Thus, the provisions not only affect the meaning, but even the existence of a norm should be examined by the Constitutional Court. In addition, the <b>loss</b> of <b>norms</b> can violate constitutional the rights of citizens 2 ̆ 7...|$|R
40|$|Sequence model {{learning}} algorithms typically maximize log-likelihood {{minus the}} norm of the model (or minimize Hamming <b>loss</b> + <b>norm).</b> In cross-lingual part-of-speech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $k$ languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the $k$ source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation. Comment: Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learnin...|$|R
40|$|Using a lab-in-the-field {{experiment}} in Uganda we study how risk sharing influences investment behaviour. Depending on the treatment, an investor {{may decide to}} share profits with a paired person, and/or the paired person may compensate the investor for investment <b>losses.</b> Following sharing <b>norms</b> in African societies, predicted investment is higher if loss sharing is possible, and/or profit sharing is not possible. Contrary to these predictions, we find that investment is higher when losses may not be shared or when profits may be shared with friends. A combination of directed altruism and expected reciprocity appears most plausible to explain these results...|$|R
40|$|We {{characterize}} {{a family}} of regularized loss min-imization problems that satisfy three properties: scaled uniform convergence, super-norm regular-ization, and norm-loss monotonicity. We show several theoretical guarantees within this frame-work, including <b>loss</b> consistency, <b>norm</b> consis-tency, sparsistency (i. e. support recovery) as well as sign consistency. A number of regularization problems can be shown to fall within our frame-work and we provide several examples. Our re-sults {{can be seen as}} a concise summary of exist-ing guarantees but we also extend them to new settings. Our formulation enables us to assume very little about the hypothesis class, data distri-bution, the loss, or the regularization. In particu-lar, many of our results do not require a bounded hypothesis class, or identically distributed sam-ples. Similarly, we do not assume boundedness, convexity or smoothness of the loss nor the regu-larizer. We only assume approximate optimality of the empirical minimizer. In terms of recov-ery, in contrast to existing results, our sparsis-tency and sign consistency results do not require knowledge of the sub-differential of the objective function. 1...|$|R
40|$|The paper {{deals with}} the problem of penalized {{empirical}} risk minimization over a convex set of linear functionals on the space of Hermitian matrices with convex <b>loss</b> and nuclear <b>norm</b> penalty. Such penalization is often used in low rank matrix recovery in the cases when the target function can be well approximated by a linear functional generated by a Hermitian matrix of relatively small rank (comparing with the size of the matrix). Our goal is to prove sharp low rank oracle inequalities that involve the excess risk (the approximation error) with constant equal to one and the random error term with correct dependence on the rank of the oracle...|$|R
40|$|Last year, poker machine losses surged {{past the}} $ 2. 3 b mark, up almost 10 % - or $ 141 m - on the {{previous}} year. This growth occurred despite the industry introducing its own Responsible Code of Practice in 1997 and despite the Bracks Labor government introducing a range of new socially responsible gaming policies {{over the past two}} and a half years. Breaking a nasty habit? Gaming Policy and Politics in the State of Victoria looks at the various actors involved in the provision and use of poker machines, and how and why they combine to ensure that turnover growth and, by implication, growth in <b>losses</b> are the <b>norm...</b>|$|R
40|$|Autonomous {{software}} agents {{operating in}} dynamic environments need to constantly reason about actions {{in pursuit of}} their goals, while taking into consideration norms which might be imposed on those actions. Normative practical reasoning supports agents making decisions about {{what is best for}} them to (not) do in a given situation. What makes practical reasoning chal- lenging is the interplay between goals that agents are pursuing and the norms that the agents are trying to uphold. We offer a formalisation to allow agents to plan for multiple goals and norms in the presence of durative actions that can be executed concurrently. We compare plans based on decision-theoretic notions (i. e. utility) such that the utility gain of goals and utility <b>loss</b> of <b>norm</b> violations are the basis for this comparison. The set of optimal plans consists of plans that maximise the overall utility, each of which can be chosen by the agent to execute. We provide an implementation of our proposal in Answer Set Programming, thus allowing us to state the original problem in terms of a logic program that can be queried for solutions with specific properties...|$|R
40|$|International audienceThis paper {{reports on}} a study of how students' {{reasoning}} about socioscientific issues is framed by three dynamics: societal structures, agency and how trust and security issues are handled. Examples from gene technology were used as the forum for interviews with 13 Swedish high-school students (year 11, age 17 - 18). A grid based on modalities from the societal structures described by Giddens was used to structure the analysis. The results illustrate how the participating students used both modalities for 'Legitimation' and 'Domination' to justify positions that accept or reject new technology. The analysis also showed how norms and knowledge {{can be used to}} justify opposing positions in relation to building trust in science and technology, or in democratic decisions expected to favour personal norms. Here, students accepted or rejected the authority of experts based on perceptions of the knowledge base that the authority was seen to be anchored in. Difficulty in discerning between material risks (reduced safety) and immaterial risks (<b>loss</b> of <b>norms)</b> was also found. These outcomes are used {{to draw attention to the}} educational challenges associated with students' using knowledge claims (Domination) to support norms (Legitimation) and how this is related to the development of a sense of agency in terms of sharing norms with experts or with laymen...|$|R
40|$|This is an expository {{paper that}} reviews recent {{developments}} on optimal estimation of structured high-dimensional covariance and precision matrices. Minimax rates of convergence for estimating several classes of structured covariance and precision matrices, including bandable, Toeplitz, sparse, and sparse spiked covariance matrices {{as well as}} sparse precision matrices, are given under the spectral <b>norm</b> <b>loss.</b> Data-driven adaptive procedures for estimating various classes of matrices are presented. Some key technical tools including large deviation results and minimax lower bound arguments {{that are used in}} the theoretical analyses are discussed. In addition, estimation under other losses and a few related problems such as Gaussian graphical models, sparse principal component analysis, factor models, and hypothesis testing on the covariance structure are considered. Some open problems on estimating high-dimensional covariance and precision matrices and their functionals are also discussed...|$|R
40|$|Missing data occur {{frequently}} {{in a wide}} range of applications. In this paper, we consider estimation of high-dimensional covariance matrices in the presence of missing observations under a general missing completely at random model in the sense that the missingness is not dependent on the values of the data. Based on incomplete data, estimators for bandable and sparse covariance matrices are proposed and their theoretical and numerical properties are investigated. Minimax rates of convergence are established under the spectral <b>norm</b> <b>loss</b> and the proposed estimators are shown to be rate-optimal under mild regularity conditions. Simulation studies demonstrate that the estimators perform well numerically. The methods are also illustrated through an application to data from four ovarian cancer studies. The key technical tools developed in this paper are of independent interest and potentially useful for a range of related problems in high-dimensional statistical inference with missing data...|$|R
40|$|Matrix {{completion}} {{has been}} well studied under the uniform sampling model and the trace-norm regularized methods perform well both theoretically and numerically in such a setting. However, the uniform sampling model is unrealistic {{for a range of}} applications and the standard trace-norm relaxation can behave very poorly when the underlying sampling scheme is non-uniform. In this paper we propose and analyze a max-norm constrained empirical risk minimization method for noisy matrix completion under a general sampling model. The optimal rate of convergence is established under the Frobenius <b>norm</b> <b>loss</b> in the context of approximately low-rank matrix reconstruction. It is shown that the max-norm constrained method is minimax rate-optimal and yields a unified and robust approximate recovery guarantee, with respect to the sampling distributions. The computational effectiveness of this method is also discussed, based on first-order algorithms for solving convex optimizations involving max-norm regularization. Comment: 33 page...|$|R
40|$|Consider p {{independent}} distributions each {{belonging to}} the one parameter exponential family with distribution functions absolutely continuous with respect to Lebesgue measure. For estimating the natural parameter vector with p >= p 0 (p 0 is typically 2 or 3), a general class of estimators dominating the minimum variance unbiased estimator (MVUE) or an estimator which is a known constant multiple of the MVUE is produced under different weighted squared error losses. Included as special cases are some results of Hudson [13] and Berger [5]. Also, for a subfamily of the general exponential family, a class of estimators dominating the MVUE of the mean vector or an estimator which is a known constant multiple of the MVUE is produced. The major tool is to obtain a general solution to a basic differential inequality. Admissibility minimaxity natural parameter vector mean vector squared <b>norm</b> <b>loss</b> weighted squared error loss normal gamma...|$|R
40|$|Estimation of {{low-rank}} matrices is {{of significant}} {{interest in a}} range of contemporary applications. In this paper, we introduce a rank-one projection model for low-rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low-rank matrices in the noisy case. The procedure is adaptive to the rank and robust against small low-rank perturbations. Both upper and lower bounds for the estimation accuracy under the Frobenius <b>norm</b> <b>loss</b> are obtained. The proposed estimator is shown to be rate-optimal under certain conditions. The estimator is easy to implement via convex programming and performs well numerically. The main results obtained in the paper also have implications to other related statistical problems. An application to estimation of spike covariance matrices from one-dimensional random projections is considered. The results demonstrate {{that it is possible to}} accu-rately estimate the covariance matrix of a high-dimensional distribution based only on one-dimensional projections...|$|R
40|$|This paper {{studies the}} problem of {{estimating}} a large coefficient matrix in a multiple response linear regression model when the coefficient matrix could be both of low rank and sparse {{in the sense that}} most nonzero entries concentrate on a few rows and columns. We are especially interested in the high dimensional settings where the number of predictors and/or response variables can be much larger than the number of observations. We propose a new estimation scheme, which achieves competitive numerical performance {{and at the same time}} allows fast computation. Moreover, we show that (a slight variant of) the proposed estimator achieves near optimal non-asymptotic minimax rates of estimation under a collection of squared Schatten <b>norm</b> <b>losses</b> simultaneously by providing both the error bounds for the estimator and minimax lower bounds. The effectiveness of the proposed algorithm is also demonstrated on an in vivo calcium imaging dataset...|$|R
40|$|We study a high-dimensional {{regression}} model. Aim is {{to construct}} a confidence set for a given group of regression coefficients, treating all other regression coefficients as nuisance parameters. We apply a one-step procedure with the square-root Lasso as initial estimator and a multivariate square-root Lasso for constructing a surrogate Fisher information matrix. The multivariate square-root Lasso is based on nuclear <b>norm</b> <b>loss</b> with ℓ_ 1 -penalty. We show that this procedure leads to an asymptotically χ^ 2 -distributed pivot, with a remainder term depending only on the ℓ_ 1 -error of the initial estimator. We show that under ℓ_ 1 -sparsity conditions on the regression coefficients β^ 0 the square-root Lasso produces to a consistent estimator of the noise variance and we establish sharp oracle inequalities which show that the remainder term is small under further sparsity conditions on β^ 0 and compatibility conditions on the design. Comment: 22 page...|$|R
40|$|The current paper {{presents}} a novel machinery for studying non-asymptotic minimax estimation of high-dimensional matrices, which yields tight minimax rates {{for a large}} collection of loss functions {{in a variety of}} problems. Based on the convex geometry of finite-dimensional Banach spaces, we first develop a volume ratio approach for determining minimax estimation rates of unconstrained normal mean matrices under all squared unitarily invariant <b>norm</b> <b>losses.</b> In addition, we establish the minimax rates for estimating mean matrices with submatrix sparsity, where the sparsity constraint introduces an additional term in the rate whose dependence on the norm differs completely from the rate of the unconstrained problem. Moreover, the approach is applicable to the matrix completion problem under the low-rank constraint. The new method also extends beyond the normal mean model. In particular, it yields tight rates in covariance matrix estimation and Poisson rate matrix estimation problems for all unitarily invariant norms...|$|R
40|$|The {{demand for}} {{electricity}} from renewable sources continues to rise, and with it, {{the need for}} more efficient wind energy conversion systems. Wind turbines equipped with a wound-rotor induction machine can be operated in a more efficient configuration than currently popular, with little to no change in system hardware. By coupling the machine to the grid inversely to the <b>norm,</b> <b>losses</b> in the system can be reduced. The theory describing this configuration and mode of operation is detailed, with steady-state operating conditions calculated. Theoretical analysis shows that core loss due to magnetic hysteresis can be reduced, and efficiency increased. Operation in the proposed configuration is compared experimentally to the conventional one, with results indicating a substantial improvement of energy-conversion efficiency for the machine under test. Losses associated with the electronic power converters and auxiliary hardware are not expected to change significantly, and are not considered...|$|R
40|$|Following the {{liberalisation}} {{of network}} industries {{there has been}} a number of innovations in incentive regulation. This paper examines the effects of the application of norm models within an ex-post incentive regulation of electricity distribution networks in Sweden. We first examine the empirical equivalence of norm models to real utilities. Next, we estimate the effect of regulation on pricing behaviour and performance of utilities in average costs, quality of service, and network energy <b>losses.</b> The <b>norm</b> models seem to reflect the main network features, demand characteristics, and capital stocks of real utilities. However, the price of labour affects relative performance. Also, quality of service has not affected the relative performance of utilities, indicating that incentives may be weak. Moreover, on the whole, utilities respond to norm models and incentives and reduce their average prices. However, investor-owned utilities that perform better than their norm models behave strategically and increase their prices. We also find that investor-owned utilities reduce (inflate) their average cost if they perform worse (better) than the benchmark. Public utilities have not adjusted their costs significantly in response to the incentives. Furthermore, we do not find evidence of improvement in quality of service and reduction in network energy losses although less efficient investor-owned networks seem to have improved on both fronts. Finally, efficient investor-owned utilities seem to have reduced their quality of service in terms of outage length. Regulation, incentive, electricity, Sweden...|$|R
40|$|When {{applied to}} {{training}} deep neural networks, stochastic gradient descent (SGD) often incurs steady progression phases, interrupted by catastrophic episodes in which <b>loss</b> and gradient <b>norm</b> explode. A possible mitigation of such events is {{to slow down}} the learning process. This paper presents a novel approach to control the SGD learning rate, that uses two statistical tests. The first one, aimed at fast learning, compares the momentum of the normalized gradient vectors to that of random unit vectors and accordingly gracefully increases or decreases the learning rate. The second one is a change point detection test, aimed at the detection of catastrophic learning episodes; upon its triggering the learning rate is instantly halved. Both abilities of speeding up and slowing down the learning rate allows the proposed approach, called SALeRA, to learn as fast as possible but not faster. Experiments on standard benchmarks show that SALeRA performs well in practice, and compares favorably to the state of the art...|$|R
40|$|Migration {{has contributed}} to the {{richness}} in diversity of cultures, ethnicities and races in developed countries. Individuals who migrate experience multiple stresses that can impact their mental well being, including the <b>loss</b> of cultural <b>norms,</b> religious customs, and social support systems, adjustment to a new culture and changes in identity and concept of self. Indeed, the rates of mental illness are increased in some migrant groups. Mental health practitioners need to be attuned to the unique stresses and cultural aspects that affect immigrants and refugees in order to best address the needs of this increasing and vulnerable population. This paper will review the concepts of migration, cultural bereavement and cultural identity, and explore the interrelationship between these three aspects of the migrant's experience and cultural congruity. The complex interplay of the migration process, cultural bereavement, cultural identity, and cultural congruity, along with biological, psychological and social factors, is hypothesized as playing {{a major role in the}} increased rates of mental illness in affected migrant groups...|$|R
40|$|International audienceSparsity regularization allows {{handling}} {{the curse of}} dimensionality, a problem commonly found in fMRI data. In this paper, we compare LASSO (L 1 regularization) and the recently introduced k-support norm {{on their ability to}} predict real valued variables from brain fMRI data for cocaine addiction, in a principled model selection setting. Furthermore, in the context of those two regularization methods, we compare two loss functions: squared loss and absolute loss. With the squared <b>loss</b> function, k-support <b>norm</b> outperforms LASSO in predicting real valued behavioral variables measured in an inhibitory control task given fMRI data from a different task, designed to capture emotionally-salient reward. The absolute loss function leads to significantly better predictive performance for both methods in almost all cases and the k-support norm leads to more interpretable and more stable solutions often by an order of magnitude. Our results support the use of the k-support norm for fMRI analysis and the generalizability of the I-RISA model of cocaine addiction...|$|R
40|$|We {{consider}} {{the problem of}} linear regression where the ℓ_ 2 ^n <b>norm</b> <b>loss</b> (i. e., the usual least squares loss) is replaced by the ℓ_p^n norm. We show how to solve such problems up to machine precision in O^*(n^| 1 / 2 - 1 /p|) (dense) matrix-vector products and O^*(1) matrix inversions, or alternatively in O^*(n^| 1 / 2 - 1 /p|) calls to a (sparse) linear system solver. This improves {{the state of the}} art for any p∈{ 1, 2,+∞}. Furthermore we also propose a randomized algorithm solving such problems in input sparsity time, i. e., O^*(Z + poly(d)) where Z is the size of the input and d is the number of variables. Such a result was only known for p= 2. Finally we prove that these results lie outside the scope of the Nesterov-Nemirovski's theory of interior point methods by showing that any symmetric self-concordant barrier on the ℓ_p^n unit ball has self-concordance parameter Ω̃(n). Comment: 16 page...|$|R
40|$|Globalisation raises {{hopes that}} {{communities}} will become self reliant through development success and thus become an integrated whole (Offiong 2001). But {{the creation of}} global village leads to fragmentation of communities, a <b>loss</b> of <b>norms</b> and local values. This study focuses on communities of the Eastern Cape Province, exploring the nature of and extent to which children use alcohol {{as a consequence of}} cultural rituals. The researcher aims to develop a culturally sensitive psycho-social approach to address alcohol use among children. The purpose of the research was to investigate the extent to which certain cultural practices contribute towards alcohol use among children in the rural areas of the Eastern Cape Province. Thus exploring the nature and extent to which children use alcohol as a consequence of cultural rituals and to explore whether families and communities are aware of the impact of alcohol use by children in the community. The research was qualitative in nature and through semi-structured interview schedules the researcher was able to gain insight in the area of substance use amongst children in the rural areas of the Eastern Cape Province. Community members, learners from schools, educators and probation officers formed part of the research sample. The research findings suggest that community members are aware that the traditional rituals that are carried out in the rural areas have been contributing to the issue of children using alcohol. However, the community members feel challenged as they become conflicted by their traditional rituals and this impacts upon children using alcohol...|$|R
40|$|Consider {{estimating}} the mean vector ` from data N n (`; oe 2 I) with l q <b>norm</b> <b>loss,</b> q 1, when ` {{is known to}} lie in an n-dimensional l p ball, p 2 (0; 1). For large n, the ratio of minimax linear risk to minimax risk can be arbitrarily large if p ! q. Obvious exceptions aside, the limiting ratio equals 1 only if p = q = 2. Our arguments are mostly indirect, involving a reduction to a univariate Bayes minimax problem. When p ! q, simple non-linear co-ordinatewise threshold rules are asymptotically minimax at small signal-to-noise ratios, and within a bounded factor of asymptotic minimaxity in general. Our results are basic to a theory of estimation in Besov spaces using wavelet bases (to appear elsewhere). Key Words. Minimax Decision Theory. Minimax Bayes Estimation. Fisher Information. Non-linear estimation. White noise model. Loss convexity. Estimating a bounded normal mean. Running Title: Minimax risk over l p -balls. AMS 1980 Subject Classification (1985 Rev) : Primary: 62 C 20 [...] ...|$|R
40|$|The aim of {{this paper}} is to recover the {{regression}} function with sup <b>norm</b> <b>loss.</b> We construct an asymptotically sharp estimator which converges with the spatially dependent rate r_n, μ(x) = P (n / (n μ(x))) ^s / (2 s + 1), where μ is the design density, s the regression smoothness, n the sample size and P is a constant expressed in terms of a solution to a problem of optimal recovery as in Donoho (1994). We prove this result under the assumption that μ is positive and continuous. This estimator combines kernel and local polynomial methods, where the kernel is given by optimal recovery, which allows to prove the result up to the constants for any s > 0. Moreover, the estimator does not depend on μ. We prove that r_n, μ(x) is optimal in a sense which is stronger than the classical minimax lower bound. Then, an inhomogeneous confidence band is proposed. This band has a non constant length which depends on the local amount of data...|$|R
40|$|Recently, {{the sparse}} {{representation}} based classification methods have received particular {{attention in the}} classification of hyperspectral imagery. However, current sparse representation based classification models have not considered all the test pixels simultaneously. In this paper, we propose a hyperspectral classification method with spatial filtering and l_(2, 1) norm (SFL) that can deal with all the test pixels simultaneously. The l_(2, 1) norm regularization is used to extract relevant training samples among the whole training data set with joint sparsity. In addition, the l_(2, 1) <b>norm</b> <b>loss</b> function is adopted to make it robust for samples that deviate significantly {{from the rest of}} the samples. Moreover, to take the spatial information into consideration, a spatial filtering step is implemented where all the training and testing samples are spatially averaged with its nearest neighbors. Furthermore, the non-negative constraint is added to the sparse representation matrix motivated by hyperspectral unmixing. Finally, the alternating direction method of multipliers is used to solve SFL. Experiments on real hyperspectral images demonstrate that the proposed SFL method can obtain better classification performance than some other popular classifiers...|$|R
40|$|Kernel partial {{least squares}} (KPLS) {{has been known}} as a generic kernel {{regression}} method and proven to be competitive with other kernel regression methods such as support vector machines for regression (SVM) and kernel ridge regression. Kernel boosted latent features (KBLF) is a variant of KPLS for any differentiable convex loss functions. It provides a more flexible framework for various predictive modeling tasks such as classification with logistic loss and robust regression with L 1 <b>norm</b> <b>loss,</b> etc. However, KPLS and KBLF solutions are dense and thus not suitable for large-scale computations. Sparsification of KPLS solutions has been studied for dual and primal forms. For dual sparsity, it requires solving a nonlinear optimization problem at every iteration step and its computational burden limits its applicability to general regression tasks. In this paper, we propose simple heuristics to approximate sparse solutions for KPLS and the framework is also applied for sparsifying KBLF solutions. The algorithm provides an interesting “path ” from a maximum residual criterion based algorithm with orthogonality conditions to the dense KPLS/KBLF. With the orthogonality, it differentiates itself from many existing forward selection-type algorithms. The computational advantage is illustrated by benchmark datasets and comparison to SVM is done...|$|R
