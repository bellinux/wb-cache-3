32|81|Public
2500|$|In decibels, the {{high-frequency}} roll-off {{is therefore}} 20n dB/decade, or 6n dB/octave (the factor of 20 is used because {{the power is}} proportional to the square of the voltage gain; see 20 <b>log</b> <b>rule.)</b> ...|$|E
50|$|Using the {{industry}} standard 20 <b>log</b> <b>rule...</b>|$|E
5000|$|Power gain, in {{decibels}} (dB), {{is defined}} by the 10 <b>log</b> <b>rule</b> as follows: ...|$|E
40|$|Graduation date: 1981 A {{comparative}} study {{was done on}} data collected by Weyerhaeuser Company on l, 6144 second-growth Douglas-fir, Pseudotsuga menziesii, trees. The variable {{used in the study}} was the percent difference, positive or negative, between the volumes given by each of three different <b>log</b> <b>rules</b> and the standard tree volumes determined by Weyerhaeuser Company. The data were analyzed in a two-way analysis of variance and factors, <b>log</b> <b>rules</b> and DBH classes were highly significant (99...|$|R
5000|$|To {{approximate}} {{a common}} log (to {{at least one}} decimal point accuracy), a few <b>log</b> <b>rules,</b> and the memorization of a few logs is required. One must know: ...|$|R
3000|$|Throughput optimal schedulers, like Exp and <b>Log</b> <b>rules,</b> {{can also}} be used for {{scheduling}} elastic flows which are often modeled as full/infinitely backlogged buffers instead of dynamic queues with random arrivals that are independent of service rate. This is done by using virtual token queues that are fed by deterministic arrivals at a constant rate [...]...|$|R
5000|$|One of {{the most}} common ways to {{calculate}} volume of a standing tree or of a log is by using the Doyle <b>log</b> <b>rule.</b> This formula uses the small end diameter of a log (D) along with the log length (L)to estimate the volume of a log. The Doyle <b>log</b> <b>rule</b> on average under estimates the volume of a log. See formula below: ...|$|E
50|$|This {{simplified}} formula, the 20 <b>log</b> <b>rule,</b> is used {{to calculate}} a voltage gain in decibels and is equivalent to a power gain only if the impedances at input and output are equal.|$|E
5000|$|In decibels, the {{high-frequency}} roll-off {{is therefore}} 20n dB/decade, or 6n dB/octave (the factor of 20 is used because {{the power is}} proportional to the square of the voltage gain; see 20 <b>log</b> <b>rule.)</b> ...|$|E
3000|$|... are not feasible, then recent {{asymptotic}} {{analysis of}} Exp [25] and <b>Log</b> [26] <b>rules</b> {{show that the}} average rates [...]...|$|R
40|$|Buying {{and selling}} logs and {{standing}} trees {{based on an}} estimate of the number of board feet they contain is an everyday practice in the lumber industry. However, it can also be a very confusing practice. There are a number of issues addressed in this publication. In addition, remember that log and tree scaling are just estimates of the final board footage expected and that different individuals will likely obtain somewhat different estimates. Tables and formulas to estimate the board-foot volume of logs and trees by three of the most commonly used <b>log</b> <b>rules</b> are provided. Background information concerning the various rules as well as scaling techniques is also provided...|$|R
50|$|In 1815, Peter Mark Roget {{invented the}} <b>log</b> <b>log</b> slide <b>rule,</b> which {{included}} a scale displaying the logarithm of the logarithm. This allowed the user to directly perform calculations involving roots and exponents. This was especially useful for fractional powers.|$|R
50|$|The {{company was}} founded by Edward Taylor Lufkin, an American Civil War veteran of the Sixtieth Regiment Ohio Volunteer Infantry in Cleveland, Ohio 1869 and was {{originally}} named E.T. Lufkin Board and <b>Log</b> <b>Rule</b> Manufacturing Company. Its Canada-based plant was in Barrie, Ontario.|$|E
5000|$|Combining the Doyle <b>log</b> <b>rule</b> {{along with}} the Mesavage-Girard Form-Class, and Girard Upper-Log Taper Tables gives a quality {{estimation}} of log volume on and off stem. Using these Girard's tables one can estimate not only the first log of the tree but also the logs following the butt log.|$|E
5000|$|The {{signal-to-noise}} ratio (SNR) {{is used in}} imaging as a physical measure of the sensitivity of a (digital or film) imaging system. Industry standards measure SNR in decibels (dB) of power and therefore apply the 10 <b>log</b> <b>rule</b> to the [...] "pure" [...] SNR ratio (a ratio of 1:1 yields 0 decibels, for instance). In turn, yielding the [...] "sensitivity." [...] Industry standards measure and define sensitivity {{in terms of the}} ISO film speed equivalent; SNR:32.04 dB = excellent image quality and SNR:20 dB = acceptable image quality.|$|E
30|$|LTE is {{a purely}} {{scheduled}} {{system that allows}} dynamic scheduling for diverse traffic types including delay-sensitive flows. By leveraging recent results on resource allocation and scheduling, we design a practical LTE downlink scheduler and characterized its performance for three traffic scenarios, namely, full-buffer, streaming video (loose delay constraint), and mixed streaming and live video (tight delay constraint). We show that the proposed utility maximizing scheduler offers good control over the rate CDF for the full buffer case. Similarly, we show that Exp and <b>Log</b> <b>rules</b> can support a mix of QoS traffic while increasing system capacity in terms of number of users that can be supported and, at the same time, reducing resource utilization.|$|R
40|$|In {{order to}} realize {{intrusion}} detection {{of the new}} unknown industrial virus, a self-learning algorithm is needed to study the information model and behavior characteristics of network communication in industrial control system, automatically generating a comprehensive rules white-list for industrial firewall detection. This paper proposes a self-learning algorithm based on statistical analysis of <b>log</b> <b>rules,</b> it dynamically generates and updates rules, using hash algorithm to add up the number of data flow, and resulting in a new filtering rules to provide intelligent self-learning ability for industrial firewall. The experimental results show that, compared with traditional IT firewall, the industrial firewall can detect a variety of illegal access, and automatically update and generate a new industry with the firewall rules.  ...|$|R
5000|$|A Volume {{table is}} a chart {{to aid in the}} {{estimation}} of standing timber volume. These tables are based on volume equations and use correlations between certain aspects of a tree to estimate the volume to a degree of certainty. The diameter at breast height (DBH) and the merchantable height are used to determine the total volume. Difficulties occur when estimating the form class of the tree in question. The Mesavage and Girard form classes used to classify the trees to decide which volume table should be used. These volume tables are also based on different <b>log</b> <b>rules</b> such a Scribner, Doyle, and International ¼” scale. In order to be effective, the proper form class must be selected as well as accurate DBH and height measurements.|$|R
3000|$|The results {{get more}} {{favorable}} to the <b>Log</b> <b>rule</b> as the system load increases to that mentioned in case (b) above (see Figure 7). QoS degrades more gracefully under the <b>Log</b> <b>rule,</b> in that 1 user under the <b>LOG</b> <b>rule</b> versus 19 under the Exp rule miss the soft delay target of 250 [*]milliseconds. However, Exp rule still maintains a lower delay spread across users than the <b>Log</b> <b>rule.</b> Clearly, the Exp rule's strong bias toward balancing delays is excessively compromising the realized throughput, and eventually the mean delays and tails for almost all users. Although Exp rule asymptotically minimizes the exponential decay rate of the max-queue distribution irrespective {{of the values of}} parameters [...]...|$|E
3000|$|... [...]. The RSM {{property}} of the <b>Log</b> <b>rule</b> naturally calibrates the scheduler to increased load. So unless parameters can be carefully tuned to possibly changing loads and unpredictable channel capacities, the <b>Log</b> <b>rule</b> {{appears to be more}} robust a scheduling policy. Intuitively, this is what one would expect from optimizing for the average/overall versus worst case asymptotic tail (see Section 2.1).|$|E
3000|$|Throughput optimal schedulers MaxWeight, Exp rule, and <b>Log</b> <b>rule</b> {{are defined}} as follows: when users' queues are in state [...]...|$|E
40|$|Many proper scoring rules {{such as the}} Brier and <b>log</b> scoring <b>rules</b> implicitly reward a {{probability}} forecaster relative to a uniform baseline distribution. Recent work has motivated weighted proper scoring rules, which have an additional baseline parameter. To date two families of weighted proper scoring rules have been introduced, the weighted power and pseudospherical scoring families. These families are compatible with the log scoring rule: when the baseline maximizes the <b>log</b> scoring <b>rule</b> over some set of distributions, the baseline also maximizes the weighted power and pseudospherical scoring rules over the same set. We characterize all weighted proper scoring families and prove a general property: every proper scoring rule is compatible with some weighted scoring family, and every weighted scoring family is compatible with some proper scoring rule...|$|R
30|$|Strict Priority Given to Live Video Flows. Live video flows are {{scheduled}} first (according to <b>Log</b> and Exp <b>rules</b> with parameters set {{according to the}} delay target of 80 [*]ms), if any RBs are left over after scheduling the live video flows, those are allocated to the streaming flows (again using <b>Log</b> and Exp <b>rules</b> with parameters set according to the delay target of 250 [*]milliseconds). This scheduling method will {{be referred to as}} priority-Exp and priority-Log rules.|$|R
40|$|Abstract: Network {{performance}} highly {{depends on}} {{efficiency of the}} firewall because for each network packet which enters or leaves the network a decision {{has to be made}} whether to accept it or reject it. This paper presents one approach to rule optimization solutions for improving firewall performance. The new software solution has been developed based on relations between rules. Its main purpose is to remove anomalies in ordering of Linux firewall rules and to merge similar rules. Developed rule optimization software (FIRO) is intended to be used with IP Tables Linux firewall command tool, but it can be easily adapted for other tool, as well. FIRO works in several passes through revised rule lists. In each step of optimization process FIRO generates a different rule list. Unlike existing solutions, FIRO also analyzes <b>log</b> <b>rules</b> and takes into account other rule parameters besides IP addresses, ports, protocols and action...|$|R
3000|$|The delay-based {{versions}} of <b>Log</b> <b>rule</b> and MaxWeight {{can also be}} computed by first approximating those as queue-based rules like this: let [...]...|$|E
3000|$|..., {{overflow}} (or, more precisely, the asymptotic {{exponential decay}} rate of max-queue distribution). Similarly, <b>Log</b> <b>rule</b> {{has been shown}} [26] to minimize the asymptotic probability of sum-queue, [...]...|$|E
3000|$|... from Section 2.1, that is, the {{partition}} of state space of delay (or queue) where <b>Log</b> <b>rule</b> and PF take the same scheduling decision. Then the magnitude of vector [...]...|$|E
40|$|Abstract. In this paper, the {{conducting}} model {{which is}} more suitable to describe the reservoir {{in the process of}} polymer flooding was selected, according to the reservoir properties. Based on the polymer solution conductance laws and the polymer flooding rock resistivity experiments, by injecting various types of polymers and water with different salinities, the rock resistivity change rule was studied. The change rule of the Archie model parameters in the polymer flooding process was analyzed and the accuracy of the dual water model was analyzed by means of the Litho-electric experiment data. On the basis of rock physical property analysis data, combined with the actual <b>logging</b> <b>rules,</b> the parameters interpretation model of porosity, permeability, irreducible water saturation and shale content were established. Using the core analyze data to contrast the practical application effect of the interpretation model, the result show that, the conclusion of the model corresponds to reality...|$|R
40|$|Field {{failure data}} {{play a key}} role in complex {{distributed}} system, as they often represent the only available source of information useful to control the dependability level of the system. However, the analysis of these data can be compromised by several factors, such as log heterogeneity and inaccuracy, which increase the level on distrust on logs, and make it difficult to compare different analyses to provide general results. The paper proposes a framework to overcome these limitations, based on three key aspects: (i) the use of an “accurate enough ” model of the system in hand, (ii) the definition of common <b>logging</b> <b>rules,</b> to be used at design and development time to enhance the accuracy, and (iii) the design of a logging platform to orchestrate the collection and analysis processes. A case study of the proposed framework is presented, in the context of a real world complex system. 1...|$|R
40|$|In August 1984, the Federal Communications Commission {{released}} the Report and Order in the Matter of the Revision of Programming and Commercialization Policies, Ascertainment Requirements, and Program Log Requirements for Commercial Television Stations, affecting the FCC regulations concerning programming policies, ascertainment requirements, program <b>logging</b> <b>rules</b> and commercialization policies. This Note analyzes these regulatory changes from this Report and Order according the following structure: first, a historical exposition {{of radio and}} television regulation {{in general and of}} the areas affected by the deregulation in particular; second, an assessment of the changes {{in the context of the}} modern television marketplace; and third, a discussion of television content regulation and the first amendment. This Note concludes that, although the deregulation is a major step forward in accommodating the changing broadcasting marketplace, the FCC has not made any real progress on the crucial issue of full first amendment protection in broadcasting...|$|R
3000|$|Therefore, as the queues grow linearly, (i.e., scaled up by a constant), <b>Log</b> <b>rule</b> (or any {{scheduler}} satisfying RSM) schedules in {{a manner}} that de-emphasizes queue-balancing in favor of increasing the total weighted service rate (with respect to weight vector [...]...|$|E
40|$|Mobile {{telecommunications}} technology gradually {{evolved to}} support better {{services such as}} voice, data, and video to users of telecommunications services. LTE (Long Term Evolution) is a network based on Internet Protocol (IP) standardized by 3 rd Generation Partnership Project (3 GPP). To support it, LTE requires a mechanism that can support. One of them by applying methods of scheduling packets in each service. Scheduling is a different treatment to packets that come {{in accordance with the}} priorities of the scheduling algorithm. In this research, to analyze the performance of LTE with paramater delay, packet loss ratio, throughput and fairness index uses a scheduling algorithms Frame Level Schedule (FLS) and <b>Log</b> <b>Rule</b> on LTE-Simulator with scenarios using Voip traffic, Video and Best Effort (BE). The results is scheduling algorithms FLS is better than <b>log</b> <b>rule</b> in term of throughput values, while of scheduling algorithms <b>log</b> <b>rule</b> is better than FLS in terms of delay based on the number and speed of the users. This indicates that both scheduling algorithms suitable for use in LTE networks within conditions of traffic real time services, but not for non real time services such as BE...|$|E
3000|$|There are queue- and channel-aware schedulers {{that are}} throughput-optimal, that is, they ensure the queues' {{stability}} without {{any knowledge of}} arrival and channel statistics if indeed stability can be achieved under any other scheduler. Examples are MaxWeight [3], Exponential (Exp) rule [4], and <b>Log</b> <b>rule</b> [5], which have the same form as (2). Moreover, necessary and sufficient conditions on [...]...|$|E
30|$|We first {{determine}} by trail the highest arrival rate that all live video flows {{can be set}} to while still meeting the delay targets under both the priority-Exp and priority-Log rules. This turns out be around 200 [*]kbps. The detailed results from this trial are not shown, however, we present the following interesting observation: even though the channel is loaded to its capacity under the priority-Exp and priority <b>Log</b> <b>rules</b> when all live video flows are set to 200 [*]kbps, {{we find that the}} system can still admit up to 10 streaming video users (5 higher SNR users at rate 360 [*]kbps and 5 lower SNR users at 90 [*]kbps) under priority-Exp and priority-Log rules while meeting their delay targets of 250 [*]milliseconds. This is because the capacity (in terms of number of users that can be supported) of a time-varying channel is constrained by the delay targets: the longer the delay targets, the greater the opportunity to wait for a good channel thus exploiting opportunistic gain.|$|R
40|$|Abstract. Process/work ow mining aims at {{discovering}} the underlying processes {{to help in}} improving or rebuilding business processes. Most of the current practices of pro-cess mining are based on event logs from Transactional Information Systems (TIS) (such as WFM, ERP, CRM, SCM and B 2 B systems). However, with the popular deployment and use of business rule engine with TIS, {{a great number of}} <b>rule</b> <b>logs</b> are generated, but they are rarely utilized for discovering processes. This paper intends to propose a differ-ent perspective for process discovery as compared with the traditional way based on the event logs. Firstly, it illustrates a motivation scenario about process mining from <b>rule</b> <b>logs</b> and then brings forward a framework for process discovery based on <b>rule</b> <b>logs.</b> After that, the mining algorithm called Alpha-r with a case study is introduced to discover a process through mining the relations of traces in <b>rule</b> ow <b>log.</b> Finally, some experiments show the effectiveness and performance of the method...|$|R
40|$|Because Japanese cedar shows lower {{mechanical}} performance, glued-laminated timber (glulam) can be {{a better}} way to utilize Japanese cedar for structural purpose. However, low yield of higher grade lami-nation from log makes it difficult to design structural glulam. This study was aimed to increase the yield of higher grade lamination and provide higher efficiency of manufacturing structural lamination by ultra-sonic log sorting technology. Logs were sorted by an existing <b>log</b> grading <b>rule</b> regulated by Korea Forest Research Institute (KFRI). It was found that the KFRI <b>log</b> grading <b>rule</b> contributed to finding better logs in viewpoint of the volumetric yield and it can reduce the number of rejected lumber by visual grading. However, it could not identify better logs to produce higher-grade products. To find an appropriate log-sorting-method for structural products, log diameter and ultrasonic time of flight (TOF) for the log were considered as factors to affect mechanical performance of resulting products. However, it was found that influence of log diameter on mechanical performance of resulting products was very small. The TOF showed a possibility to sort logs by mechanical performance of resulting products even though a co-efficient of correlation was not strong (R = 0. 6). In a case study, the log selection based on the ultrasonic TOF of the log increased the yield of the outermost tension lamination (E 8 or better grade, KS F 3021) from 2. 6 % to 12. 5 % and reduced LTE 5 (lower than E 5 grade) lamination from 43. 6 % to 10. 3 %, compared with the existing KFRI <b>log</b> grading <b>rule...</b>|$|R
