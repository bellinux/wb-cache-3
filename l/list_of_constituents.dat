14|10000|Public
25|$|For all {{railways}} see <b>List</b> <b>of</b> <b>constituents</b> of the LMS.|$|E
2500|$|There {{were also}} joint {{railways}} between the Big Four {{and a few}} light railways to consider (see <b>list</b> <b>of</b> <b>constituents</b> of British Railways). Excluded from nationalisation were industrial lines like the Oxfordshire Ironstone Railway. The London Underground – publicly owned since 1933 – was also nationalised, becoming the London Transport Executive of the British Transport Commission. The Bicester Military Railway was already run by the government. The electric Liverpool Overhead Railway was also excluded from nationalisation.|$|E
50|$|For all {{railways}} see <b>List</b> <b>of</b> <b>constituents</b> of the LMS.|$|E
5000|$|<b>List</b> <b>of</b> <b>Constituent</b> Colleges/Institutes {{under the}} University of Dhaka ...|$|R
5000|$|The {{following}} is a partial <b>list</b> <b>of</b> <b>constituent</b> colleges <b>of</b> the university: ...|$|R
5000|$|The early Buddhist scriptures give various <b>lists</b> <b>of</b> the <b>constituents</b> <b>of</b> the person: ...|$|R
5000|$|There {{were also}} joint {{railways}} between the Big Four {{and a few}} light railways to consider (see <b>list</b> <b>of</b> <b>constituents</b> of British Railways). Excluded from nationalisation were industrial lines like the Oxfordshire Ironstone Railway. The London Underground - publicly owned since 1933 - was also nationalised, becoming the London Transport Executive of the British Transport Commission. The Bicester Military Railway was already run by the government. The electric Liverpool Overhead Railway was also excluded from nationalisation.|$|E
5000|$|Chemical {{waste is}} a waste that is made from harmful {{chemicals}} (mostly produced by large factories). Chemical waste may fall under regulations such as COSHH in the United Kingdom, or the Clean Water Act and Resource Conservation and Recovery Act in the United States. In the U.S., the Environmental Protection Agency (EPA) and the Occupational Safety and Health Administration (OSHA), as well as state and local regulations also regulate chemical use and disposal. Chemical waste {{may or may not}} be classed as hazardous waste. A chemical hazardous waste is a solid, liquid, or gaseous material that displays either a “Hazardous Characteristic” or is specifically “listed” by name as a hazardous waste. There are four characteristics chemical wastes may have to be considered as hazardous. These are Ignitability, Corrosivity, Reactivity, and Toxicity. This type of hazardous waste must be categorized as to its identity, constituents, and hazards so that it may be safely handled and managed. [...] Chemical waste is a broad term and encompasses many types of materials. Consult the Material Safety Data Sheet (MSDS), Product Data Sheet or Label for a <b>list</b> <b>of</b> <b>constituents.</b> These sources should state whether this chemical waste {{is a waste}} that needs special disposal.|$|E
40|$|AutoChem is a {{suite of}} Fortran 90 {{computer}} programs for the modeling of kinetic reaction systems. AutoChem performs automatic code generation, symbolic differentiation, analysis, and documentation. It produces a documented stand-alone system for the modeling and assimilation of atmospheric chemistry. Given databases of chemical reactions and a <b>list</b> <b>of</b> <b>constituents</b> defined by the user, AutoChem automatically does the following: 1. Selects the subset of reactions that involve a user-defined <b>list</b> <b>of</b> <b>constituents</b> and automatically prepares a document listing the reactions; 2. Constructs the ordinary differential equations (ODEs) that describe the reactions as functions of time and prepares a document containing the ODEs; 3. Symbolically differentiates the time derivatives to obtain the Jacobian and prepares a document containing the Jacobian; 4. Symbolically differentiates the Jacobian to obtain the Hessian and prepares a document containing the Hessian; and 5. Writes all the required Fortran 90 code and datafiles for a stand-alone chemical modeling and assimilation system (implementation of steps 1 through 5). Typically, the time taken for steps 1 through 5 is about 3 seconds. The modeling system includes diagnostic components that automatically analyze each ODE at run time, {{the relative importance of}} each term, time scales, and other attributes of the model...|$|E
50|$|The full <b>list</b> <b>of</b> all <b>constituent</b> stocks can {{be found}} at SZSE.|$|R
50|$|Standard Dry Air -- an {{agreed-upon}} {{gas composition}} for air {{from which all}} water vapour has been removed. There are various standards bodies which publish documents that define a dry air gas composition. Each standard provides a <b>list</b> <b>of</b> <b>constituent</b> concentrations, a gas density at standard conditions and a molar mass.|$|R
40|$|The article {{examines}} the phenomena of national idea as a consolidating factor for poly-ethnic society; provides <b>listing</b> <b>of</b> <b>constituent</b> elements <b>of</b> this idea. Among major objectives of national idea there is defining of common priorities {{in the area of}} ethno-policy for various ethnic entities, and outlining of historical perspective of their existence. In such a case it is capable of becoming an efficient preventer of international dissention. ? ?????? ?????? ??????????? ??????? ???????????? ???? ??? ???????????????? ??????? ??????????????????? ????????, ???????? ?? ???????????? ????????. ????? ?? ???????? ????? ???????????? ???? ???????? ??????????? ????? ??????????? ? ????? ???????????? ??? ?????? ?????????? ????????? ? ??????????? ???????????? ??????????? ?? ?????????????. ? ???? ?????? ??? ????? ????? ??????????? ??????????????? ??????????????? ??????????...|$|R
40|$|In {{contemporary}} moral philosophy, {{the standard}} {{way of understanding}} the constituents of the human good {{is in terms of}} a fairly limited number of features that contribute to our happiness independently of how they are situated in our lives. Even when this approach is supplemented by Moorean ideas about organic wholes, it still cannot do justice to the deep importance of how things are situated and even when meaning is seen as an important factor, it still tends to be treated as simply another item on the <b>list</b> <b>of</b> <b>constituents.</b> It is argued here that we should abandon this approach in favor of one that recognizes that our lives are best understood in terms of their narrative structure and that treats narrative meaning as a pervasive phenomenon that strongly influences the importance that different features have in making our lives go more or less well...|$|E
40|$|Based on the {{identification}} of market dynamics, capital allocation in long positions can be dynamically controlled by means of interrupting an otherwise strictly-long investment strategy allowing for an overall improved risk profile and faster response times during periods of persistent negative market returns. Herein, a portfolio selection methodology updating a reasonably diversified selection of competing S&P 500 constituents within and across various predefined industry groups and which produced above average long-term returns with minimized downside-risk, is proposed. Within the various predefined groups of stocks, Simugram methods are used to model and optimize {{on the distribution of}} returns up to and including a horizon of interest. Improvements to previous methods are focused toward calibrating the sampling distribution based on an empirical dataset within the various groups comprising the investor's portfolio, optionally allowing for a varying sampling frequency as dictated by the various group dynamics. By combining within-group optimization alongside with the capability of exiting aggressive long-strategies at seemingly riskier times, focus is on providing more frequent updates on a <b>list</b> <b>of</b> <b>constituents</b> with improved performance in both terms of risk and return...|$|E
40|$|Chloroplasts are {{fundamental}} organelles enabling plant photoautotrophy. Besides their outstanding physiological role in fixation of atmospheric CO 2, they harbor many important metabolic {{processes such as}} biosynthesis of amino acids, vitamins or hormones. Technical advances in MS allowed the recent identification of most chloroplast proteins. However, for {{a deeper understanding of}} chloroplast function it is important to obtain a complete <b>list</b> <b>of</b> <b>constituents,</b> which is so far limited by the detection of low-abundant proteins. Therefore, we developed a two-step strategy for the enrichment of low-abundant soluble chloroplast proteins from Pisum sativum and their subsequent identification by MS. First, chloroplast protein extracts were depleted from the most abundant protein ribulose- 1, 5 -bisphosphate carbox-ylase/oxygenase by SEC or heating. Further purification was carried out by affinity chro-matography, using ligands specific for ATP- or metal-binding proteins. By these means, we were able to identify a total of 448 proteins including 43 putative novel chloroplast proteins. Additionally, the chloroplast localization of 13 selected proteins was confirmed using yellow fluorescent protein fusion analyses. The selected proteins included a phosphoglycerate mutase, a cysteine protease, a putative protein kinase and an EF-hand containing substrate carrier protein, which are expected to exhibit important metabolic or regulatory functions...|$|E
50|$|<b>List</b> <b>of</b> the BELEX15 <b>constituents</b> as <b>of</b> 31 March 2015.|$|R
40|$|Contains {{material}} {{pertaining to}} the establishment of the organization, including a <b>list</b> <b>of</b> incorporators, and <b>lists</b> <b>of</b> <b>constituent</b> members, minutes <b>of</b> meetings (November 1970 -February 1972), a speech giving a review of the group's activities (1972), and financial records (July 1970 -March 1972). Also contains general correspondence (March 1970 -March 1972), correspondence with Combined Jewish Philanthropies of Greater Boston (1970 - 1971), and information on the group's Chavurat Shabbat project (1972), and Jewish Students Projects. Gifts, in part, of Morey Schapira and Robert Lapidus, 1975 - 1976. Finding Aid available in Reading Room and on Internet...|$|R
25|$|The EPA has officially removed {{saccharin}} and its salts {{from their}} <b>list</b> <b>of</b> hazardous <b>constituents</b> and commercial chemical products. In a 14 December 2010 release, the EPA stated that saccharin {{is no longer}} considered a potential hazard to human health.|$|R
40|$|The Hanford Site 200 Area Treated Effluent Disposal Facility (TEDF) has {{operated}} since June 1995. Groundwater monitoring {{has been conducted}} quarterly in the three wells surrounding the facility since 1992, with contributing data from nearby B Pond System wells. Cumulative hydrologic and geochemical information from the TEDF well network and other surrounding wells indicate no discernable effects of TEDF operations on the uppermost aquifer {{in the vicinity of}} the TEDF. The lateral consistency and impermeable nature of the Ringold Formation lower mud unit, and the contrasts in hydraulic conductivity between this unit and the vadose zone sediments of the Hanford formation suggest that TEDF effluent is spreading laterally with negligible mounding or downward movement into the uppermost aquifer. Hydrographs of TEDF wells show that TEDF operations have had no detectable effects on hydraulic heads in the uppermost aquifer, but show a continuing decay of the hydraulic mound generated by past operations at the B Pond System. Comparison of groundwater geochemistry from TEDF wells and other, nearby RCRA wells suggests that groundwater beneath TEDF is unique; different from both effluent entering TEDF and groundwater in the B Pond area. Tritium concentrations, major ionic proportions, and lower-than-background concentrations of other species suggest that groundwater in the uppermost aquifer beneath the TEDF bears characteristics of water in the upper basalt confined aquifer system. This report recommends retaining the current groundwater well network at the TEDF, but with a reduction of sampling/analysis frequency and some modifications to the <b>list</b> <b>of</b> <b>constituents</b> sought...|$|E
40|$|During {{second quarter}} 1995, samples from seven new AMB {{groundwater}} monitoring wells at the Metallurgical Laboratory Hazardous Waste Management Facility (Met Lab HWMF) were analyzed {{for a comprehensive}} <b>list</b> <b>of</b> <b>constituents.</b> Two parameters exceeded standards during the quarter. Lead and nickel appear to exceed final Primary Drinking Water Standards (PDWS) in AMB- 18 A. These data were suspect and a rerun of the samples showed levels below flagging criteria. This data will be monitored in 3 Q 95. Aluminum, iron, manganese, boron, silver and total organic halogens exceeded Flag 2 criteria {{in at least one}} well each during second quarter 1995. This data, as well, will be confirmed by 3 Q 95 testing. Groundwater flow directions in the M-Area Aquifer Zone were similar to previous quarters; the flow rate estimate, however, differs because of an error noted in the scales of measurements used for previous estimates. The estimate was 470 ft/year during second quarter 1995. Reliable estimates of flow directions and rates in the Upper Lost Lake Aquifer Zone could not be determined in previous quarters because data were insufficient. The first estimate from second quarter 1995 shows a 530 ft/year rate. Reliable estimates of flow directions and rates in the Lower Lost Lake Aquifer Zone and in the Middle Sand Aquifer Zone of the CBCU could not be calculated because of the low horizontal gradient and the near-linear distribution of the monitoring wells. During second quarter 1994, SRS received South Carolina Department of Health and Environmental Control approval for constructing five point-of-compliance wells and two plume definition wells near the Met Lab HWMF. This project began in July 1994 and was completed in March of this year. Analytical data from these wells are presented in this report for the first time...|$|E
40|$|The pace {{at which}} science and {{technology}} is developing is mind boggling. Faster, higher, bigger, better, smaller {{are but a few}} of the accolades bestowed upon the modern scientific fraternity. The water industry is also part of this rapid progress to perfection. We are able to monitor for a wider variety of water quality constituents at much lower concentration levels than previously. Improved epidemiology studies, better application of statistical methods complimented by state of the art screening and toxicity tests now provide us with a better understanding of the health risks of an increasing <b>list</b> <b>of</b> <b>constituents</b> in water. Despite this we still argue about the value of chlorine and fluoride, We concern ourselves with the possible health risks of an ever increasing list of organic substances in water, which in many instances we have difficulty in measuring and do not fully understand. We grapple with the risk of pathogenic protozoa such as Cryptosporidium and Giardia in an attempt to remove risk from society. Amidst these differences of opinion, the water industry is, on occasion, confronted with a new idea or concept that has universal support. The Water Safety Plan (WSP) is one such an example, which has found wide approval and little resistance. Whilst WSP’s, are great, they are not new in the sense that they rely on sound integrated water quality management principles that have been around for many years. They are very logic and in fact quite simple to implement, but, as with most management systems, they require a lot of attention to detail and diligence. They are no different from other sound management plans in that they are only as good as the effort that you put into them and the manner in which they are sustained. The key implementation challenges during the development and implementation of a WSP will be discussed inclusive of the little foxes that may hinder this from happening...|$|E
40|$|This article follows on from part 1 {{published}} in the previous issue of the European Journal of Palliative Care {{and looks at the}} ten core interdisciplinary competencies in palliative care in more detail. For each competency, a short description of its rationale and focus is followed by a <b>list</b> <b>of</b> its <b>constituents...</b>|$|R
40|$|Since the {{introduction}} of turbo codes {{a large amount of}} research effort has been spent on improving turbo coding performance. One way of improving the performance is to choose the best constituent codes. Previously [1] the constituent codes have been chosen by maximising the input weight 2 effective freedistance d 2 eff. The number of nearest neighbours Neff is also an important parameter. This paper present a <b>list</b> <b>of</b> <b>constituent</b> codes that maximise d 2 eff and minimise the Neff. By choosing constituent codes that minimise Neff the performance of turbo codes can be improved with no increase in complexity. I. Introduction Turbo codes are powerful error correction schemes which were first proposed in [2]. Since turbo codes are a parallel concatenation of two or more convolutional codes, the characteristic <b>of</b> each <b>constituent</b> encoder as well as the interleaver is important in order to achieve good performance for turbo codes. One way to improve the performance of turbo codes is [...] ...|$|R
40|$|SYNTAGMA is a {{rule-based}} parsing system, structured on two levels: {{a general}} parsing engine and a language specific grammar. The parsing engine is a language independent program, while grammar and language specific rules and resources are given as text files, consisting in a <b>list</b> <b>of</b> <b>constituent</b> structuresand a lexical database with word sense related features and constraints. Since its theoretical background is principally Tesniere's Elements de syntaxe, SYNTAGMA's grammar emphasizes {{the role of}} argument structure (valency) in constraint satisfaction, and allows also horizontal bounds, for instance treating coordination. Notions such as Pro, traces, empty categories are derived from Generative Grammar and some solutions are close to Government&Binding Theory, although they {{are the result of}} an autonomous research. These properties allow SYNTAGMA to manage complex syntactic configurations and well known weak points in parsing engineering. An important resource is the semantic network, which is used in disambiguation tasks. Parsing process follows a bottom-up, rule driven strategy. Its behavior can be controlled and fine-tuned...|$|R
30|$|A {{water quality}} index (WQI) is a {{mechanism}} for presenting a cumulatively derived numerical expression defining {{a certain level of}} water quality. In other words, WQI summarizes large amounts of water quality data into simple terms (e.g., excellent, good, bad, etc.) for reporting to management and the public in a consistent manner. The concept of WQI is based on the comparison of the water quality parameters with respective regulatory standards and gives a single value to the water quality of a source, which translates the <b>list</b> <b>of</b> <b>constituents</b> and their concentrations present in a sample (Khan et al. 2003; Abbasi 2002). The index method was initially proposed by Horton in (1965). Since then, the formulation and use of indices has been strongly advocated by agencies responsible for water supply and control of water pollution. Landwehr (1979) points out that an index is a performance measurement that aggregates information into a usable form, which reflects the composite influence of significant physical, chemical and biological parameters of water quality conditions. House and Newsome (1989) states that the use of a water quality index (WQI) allows ‘good’ and ‘bad’ water quality to be quantified by reducing a large quantity of data on a range of physico-chemical and biological parameters to be a single number in a simple, objective and reproducible manner. Various types of aggregation methods used for aggregation of quality-monitoring data to yield an overall quality index. Over the last three decades, a number of mathematical functions for aggregation of water quality and water pollution indices have been suggested (Horton 1965; Brown et al. 1970; Prati et al. 1971; Dinius 1972; Dee et al. 1973; McDuffie and Haney 1973; Inhaber 1974; Walski and Parker 1974; Truett et al. 1975; Landwehr and Deininger 1976; Ross 1977; Ott 1978; Stoner 1978; Ball and Church 1980; Bhargava 1983; Dinius 1987; House and Ellis 1987; Smith 1989, 1990; Dojlido et al. 1994; Štambuk-Giljanovic 1999; Pesce and Wunderlin 2000; Swamee and Tyagi 2000; Jonnalagadda and Mhere 2001; Cude 2001; Abbasi 2002; Nagels et al. 2002; Said et al. 2004; Debels et al. 2005; Bordalo et al. 2006; Kannel et al. 2007; Swamee and Tyagi 2007). The different aggregation functions can be of additive, multiplicative, minimum or maximum operator forms. Each functions have their own merits and demerits and applicable for limited situations. The most appropriate aggregation function is the one that is either free from or minimizes the overestimation (ambiguity), underestimation (eclipsing) and rigidity problems.|$|E
40|$|The U. S. Department of Energy Order 435. 1, Radioactive Waste Management, {{requires}} a disposal authorization statement authorizing operation (or continued operation) for low-level waste disposal facilities. In fulfillment of these requirements, a disposal authorization statement was issued on October 25, 1999, authorizing the Hanford Site to transfer, receive, possess, {{and dispose of}} low-level radioactive waste at the 200 East Area burial grounds and the 200 West Area burial grounds. One of the conditions is that monitoring plans for the 200 East Area and 200 West Area low-level burial grounds be written {{and approved by the}} Richland Operations Office. As a result of a record of decision for the Hanford Site Solid Waste Program and acceptance of the Hanford Site Solid Waste Environmental Impact Statement, the use of the low-level burial ground (LLBG) as a disposal facility for low-level and mixed low-level wastes has been restricted to lined trenches and the Navy reactor-compartment trench only. Hence, as of July 2004, only the two lined trenches in burial ground 218 -W- 5 (trenches 31 and 34, see Appendix A) and the Navy reactor-compartment trench in burial ground 218 E 12 B (trench 94) are allowed to receive waste. When the two lined trenches are filled, the LLBG will cease to operate except for reactor compartment disposal at trench 94. Remaining operational lifetime of the LLBG is dependent on waste volume disposal rates. Existing programs for air sampling and analyses and subsidence monitoring are currently adequate for performance assessment at the LLBG. The waste disposal authorization for the Hanford Site is based (in part) on the post-closure performance assessments for the LLBG. In order to maintain a useful link between operational monitoring (e. g., Resource Conservation and Recovery Act [RCRA], Comprehensive Environmental Response, Compensation, and Liability Act, and State Waste Discharge Permits), constituents, monitoring frequencies, and boundaries require regular review and comparison. The annual reports discussed here are the primary sources for these reviews. The pathways of interest are air and groundwater for both operational and post-closure conditions at the LLBG, with groundwater considered to be the most significant long-term exposure pathway. Constituents that contributed at least 0. 1 % of the total relative hazard were selected as target analytes for monitoring. These are technetium- 99, uranium, and iodine- 129. Because of its environmental unavailability, carbon 14 was removed from the <b>list</b> <b>of</b> <b>constituents.</b> Given the potential uncertainties in inventories at the 200 Area LLBG and the usefulness of tritium as a contaminant indicator, tritium will be monitored as a constituent of concern at all burial grounds. Preexisting contamination plumes in groundwater beneath low-level waste management areas are attributed to other past-practice liquid waste disposal sites. Groundwater and air will be sampled and analyzed for radiogenic components. Subsidence monitoring will also be performed on a regular basis. The existing near-facility and surveillance air monitoring programs are sufficient to satisfy the performance assessment monitoring. Groundwater monitoring will utilize the existing network of wells at the LLBG, and co-sampling with RCRA groundwater monitoring, to be sampled semiannually. Installation of additional wells is currently underway to replace wells that have gone dry...|$|E
50|$|The FTSE 350 Index {{index is}} a market {{capitalization}} weighted stock market index incorporating the largest 350 companies by capitalization which have their primary listing on the London Stock Exchange. It {{is a combination of}} the FTSE 100 Index of the largest 100 companies and the FTSE 250 Index of the next largest 250. The index is maintained by FTSE Russell, a subsidiary of the London Stock Exchange Group. See the articles about those indices for <b>lists</b> <b>of</b> the <b>constituents</b> <b>of</b> the FTSE 350.|$|R
50|$|Both the {{weighted}} and unweighted indices {{use the same}} <b>list</b> <b>of</b> stocks. The <b>constituents</b> <b>of</b> the ASE indices are evaluated each year. The 70 stocks listed here are the 2007 version of the index. They are categorized by sector: banking sector, insurance sector, service sector and industrial sector. The lists are alphabetized by stock symbol.|$|R
30|$|We {{transform}} each tweet into a vector {{of features}} as follows. Firstly, we flatten out our ontology into a <b>list</b> <b>of</b> its <b>constituent</b> concepts. As stated in “General rationale” section each concept {{is associated with}} a group of words or tokens referred to as the concept dictionary. Each concept is effectively a <b>list</b> <b>of</b> words and the full ontology is basically a <b>list</b> <b>of</b> <b>lists.</b> In this sense it is a heavily redacted English dictionary containing only words we consider to be of epidemiological relevance. To obtain the feature vector we simply tokenize each tweet and for each token we do a dictionary look up in our flattened ontology.|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. This study sought to reflect the diversity of rural contexts and farm structure across Western Europe. It consists of two surveys - the baseline survey and the final survey. The baseline survey, conducted in 1987, covered basic conditions, work and income patterns of farm households in different socio-economic situations in Europe {{with a view to}} further investigation {{over the next three years}} of the reasons, extent and effects of change experienced by some of them at farm, local, regional and national levels with special reference to multiple job-holding. The final survey was carried out in 1991. The baseline survey data are held as separate study numbers for each country (see <b>list</b> <b>of</b> <b>constituent</b> studies). The dataset containing the merged data of the baseline and final surveys for all countries is held as SN: 2973. Main Topics : Farm size and tenure; agricultural production and livestock; farm buildings and machinery; finance and income. Residence. Household members and farm work forces; agricultural and farm-based activities; off-farm activities. Picardie only: market relations; development network. Savoie only: building and equipment...|$|R
40|$|The target {{chemical}} {{list was}} used to select chemicals of potential concern (CoPCs). The target <b>list</b> <b>of</b> chemicals evaluated {{was based on the}} <b>list</b> <b>of</b> “concentrate <b>constituents</b> ” and excluded bismuth, calcium, chloride, gallium, germanium, gold, silicon, sulfate, and sulfur. The latter chemicals were not included on the <b>list</b> because <b>of</b> the Pareto principle, which states that “ [...] . a relatively large number of problems (for example, a large proportion of site attributable risk) in a given situation will be found to be caused by only a few factors (or a few hazardous substances) … the target analyte list [substances] … are those manufactured and used in the greatest amounts and that are the most toxic. ...|$|R
5000|$|In 2002, one of Habay's former staffers husband George Radich, {{notified}} the Pennsylvania Bureau of Commissions, Elections and Legislation that Habay had improperly recorded campaign expenses {{and failed to}} account for funds that were spent by his staff in 1998. [...] The initial complaint was referred to the Pennsylvania Attorney General's Office and the House Ethics Committee. In 2004, the Pennsylvania State Ethics Commission launched its own investigation, finding that Habay directed his employees to perform campaign-related activity while on state time and lied about it. The commission found that staffers had including organized political fund-raisers, processed political mailings, telephoned supporters to seek donations, built election signs, collected signatures on nominating petitions, and assembled <b>lists</b> <b>of</b> <b>constituents</b> they had helped for invitations to campaign events. On the day before elections, he had his employees stand by the road to greet motorists with signs in a ritual dubbed [...] "the Habay wave." [...] The commission found that [...] "Habay certainly has discretion as to management of staff. However, Habay may not use his position to direct district office employees to perform campaign or political activity during Commonwealth working hours, which is precisely what he did." [...] Habay claimed that the Radiches were retaliating for Rebecca's dismissal and accused the Ethics Commission of bias because he co-sponsored legislation to reform the state's open records laws.|$|R
40|$|Multivariate {{techniques}} {{based on}} engineered features have found wide adoption {{in the identification}} of jets resulting from hadronic top decays at the Large Hadron Collider (LHC). Recent Deep Learning developments in this area include the treatment of the calorimeter activation as an image or supplying a <b>list</b> <b>of</b> jet <b>constituent</b> momenta to a fully connected network. This latter approach lends itself well to the use of Recurrent Neural Networks. In this work the applicability of architectures incorporating Long Short-Term Memory (LSTM) networks is explored. Several network architectures, methods of ordering <b>of</b> jet <b>constituents,</b> and input pre-processing are studied. The best performing LSTM network achieves a background rejection of 100 for 50 % signal efficiency. This represents more than a factor of two improvement over a fully connected Deep Neural Network (DNN) trained on similar types of inputs...|$|R
40|$|Lipid domains, {{also known}} as lipid rafts, are {{segregated}} from {{the bulk of the}} plasma membrane and have been attributed a multitude of important cellular functions in both health and disease. The large number of recent proteomic studies of their composition has produced a stunning <b>list</b> <b>of</b> potential <b>constituents,</b> leading to many contradictory conclusions. The actual methodology used in the different studies therefore seems to be of pivotal importance with regard to the derived lipid domain proteomes. In this review, we attempt to interpret recent findings in light of the methodology used and identify potential artifacts. This integrative view tries to tentatively define the core composition, the associated functions, the topology, as well as the dynamics of lipid domain proteomes. In other words: who's in and who's ou...|$|R
40|$|The {{incorporation}} of high [...] level prior information in Bayesian imaging {{has been proposed}} via the theoretical frameworks both of Grenander and Miller (1994) and Baddeley and Van Lieshout (1993). Recent developments in Markov chain Monte Carlo algorithms for varying dimension problems (Green, 1995) have made the application of these high [...] level models more widely accessible, although this remains more complicated than using the usual low [...] level Markov random fields models. One application area in which incorporating information on the object level in a scene would be of particular benefit is in confocal microscopy. In this paper we outline two such applications, the first addresses uncertainty in segmenting confocal microscope images, {{and the second is}} an object recognition problem. 1 INTRODUCTION High [...] level models specify an image in global terms, describing it as a <b>list</b> <b>of</b> its <b>constituent</b> objects rather than as the <b>list</b> <b>of</b> pixels on which the data are recorded or digitised. The compl [...] ...|$|R
40|$|A {{purpose of}} work is {{creation}} of model of readiness of graduating student to implementation of official questions of guidance, organization and leadthrough of physical preparation {{in the process of}} military-professional activity. An analysis is conducted more than 40 sources and questionnaire questioning of a 21 expert. For introduction of model to the system of physical preparation <b>of</b> students the <b>list</b> <b>of</b> its basic <b>constituents</b> is certain: theoretical methodical readiness; functionally-physical readiness; organizationally-administrative readiness. It is certain that readiness of future officers to military-professional activity foresees determination of level of forming of motive capabilities, development of general physical qualities...|$|R
