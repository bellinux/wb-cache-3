127|49|Public
2500|$|For all serial digital {{interfaces}} (excluding the obsolete composite encodings), {{the native}} color encoding is [...] YCbCr format. The <b>luminance</b> <b>channel</b> (Y) is encoded at full bandwidth (13.5MHz in 270Mbit/s SD, ~75MHz in HD), {{and the two}} chrominance channels (Cb and Cr) are subsampled horizontally, and encoded at half bandwidth (6.75MHz or 37.5MHz). The Y, Cr, and Cb samples are co-sited (acquired at the same instance in time), and the Y' sample is acquired at the time halfway between two adjacent Y samples.|$|E
5000|$|This {{narrowband}} {{signal is}} used to correct the wideband <b>luminance</b> <b>channel</b> at low frequencies, so the monchrome signal transmitted becomes: ...|$|E
5000|$|... bump lemur_bump.tga # {{bump map}} (which by default uses <b>luminance</b> <b>channel</b> of the image) disp lemur_disp.tga # {{displacement}} map decal lemur_stencil.tga # stencil decal texture (defaults to 'matte' channel of the image) ...|$|E
40|$|Two new {{technique}} that test color vision using motion stimuli have {{allowed us to}} exploit optokinetic nystagmus as a response measure. We {{are now able to}} screen for colorvision deficits in adults and in nonverbal infants by observing their eyes as they watch a computer-controlled TV display. It was shown that the relative contribution of red and green cones to the <b>luminance</b> <b>channels</b> is already in place within {{the first few months of}} life...|$|R
30|$|We {{extracted}} {{the edges}} of each RGB channel with the SUSAN detector and {{the edges of}} the <b>luminance</b> (grayscale) <b>channel</b> with the Sobel operator and combined their results into one edge mask.|$|R
50|$|NOTE: Frequencies given are for <b>luminance</b> carriers. For <b>channel</b> center frequencies, add 1.75 MHz.|$|R
50|$|SMPTE 367M, {{also known}} as SMPTE D-11, is the SMPTE {{standard}} for HDCAM. The standard specifies compression of high-definition digital video. D11 source picture rates can be 24, 24/1.001, 25 or 30/1.001 frames per second progressive scan, or 50 or 60/1.001 fields per second interlaced; compression yields output bit rates ranging from 112 to 140 Mbit/s. Each D11 source frame is composed of a <b>luminance</b> <b>channel</b> at 1920 x 1080 pixels and a chrominance channel at 960 x 1080 pixels. During compression, each frame's <b>luminance</b> <b>channel</b> is subsampled at 1440 x 1080, while the chrominance channel is subsampled at 480 x 1080.|$|E
5000|$|Philips Research Lab., {{following}} RCA's lead, also {{contemplated the}} construction of a four-tube camera. Their new camera was to include an experimental 2" [...] Plumbicon tube which was being developed for use in the <b>luminance</b> <b>channel</b> of this camera. The work was discontinued.|$|E
50|$|The EMI 2001 used band {{defining}} filters in {{all four}} channels. For the colour channels and narrow-band luminance, the low-pass filters had a Gaussian shaped pass-band and, although such filters were not ‘sharp-cut’, they were linear phase and gave negligible overshoots on transients. The wide-band <b>luminance</b> <b>channel</b> had its bandwidth defined by a linear phase low pass filter with a 3 dB cut-off at 6.8 MHz. Its design follows the lattice filter methods of Bode.|$|E
40|$|International audienceAlthough polar colour {{spaces are}} being {{increasingly}} {{used in the}} context of colour mathematical morphology, mainly due to their intuitiveness, the processing of the circular hue band continues to be their main drawback. In this paper, we discuss the two principal problems concerning the morphological processing of hue, first its lack of a lattice structure, which we propose to introduce by means of a distance based formulation from multiple representative reference hue points. And second, the distinction of chromatic from achromatic pixels, since the hue component is of no significance for “low” saturation levels. For this purpose, we present a new weighting scheme, based on the combination of saturation and <b>luminance</b> <b>channels.</b> Application results on texture classification, asserting the superior performance of this approach, are also included...|$|R
40|$|AbstractTo {{determine}} {{the relationship between}} the spatial <b>channels</b> for <b>luminance</b> and shape-from-stereo-disparity processing we measured disparity modulation sensitivity as a function of disparity spatial frequency for sinusoidal modulations of a field of Gabor micropatterns of differing luminance spatial frequency. We first examine the effects of contrast, spatial bandwidth and element density and show that it is only the last of these which is critical for the shape of the disparity modulation threshold function. We show that the shape of this function depends on the luminance spatial frequency of the surface that is modulated in depth. Specifically, low corrugation frequencies enjoy a greater scale support from the early luminance spatial filters than do high corrugation frequencies. The results are consistent with higher spatial frequency disparity channels receiving a greater input from higher spatial frequency <b>luminance</b> <b>channels...</b>|$|R
40|$|Human {{visual system}} is more {{sensitive}} to luminance than to chrominance. In order to reduce information that is not perceived by human visual system, color channels are downsampled while keeping luminance as original. Similarly in stereo case, human visual system uses high frequency information from the high resolution image of the mixed resolution image pair. By downsampling one of the pair, higher compression is achieved in stereo image coding. In this paper, we have examined downsampling color channels in higher ratios in color stereo image pairs. In our experiments, we have used “double-stimulus continuous-quality scale ” (DSCQS) method. We have {{found out that the}} depth perception is not changed by compression or filtering. However, in order to keep perceived image quality similar to the original stereo pair, filtering should be applied to chrominance but not to <b>luminance</b> <b>channels.</b> 1...|$|R
50|$|For all serial digital {{interfaces}} (excluding the obsolete composite encodings), {{the native}} color encoding is 4:2:2 YCbCr format. The <b>luminance</b> <b>channel</b> (Y) is encoded at full bandwidth (13.5 MHz in 270 Mbit/s SD, ~75 MHz in HD), {{and the two}} chrominance channels (Cb and Cr) are subsampled horizontally, and encoded at half bandwidth (6.75 MHz or 37.5 MHz). The Y, Cr, and Cb samples are co-sited (acquired at the same instance in time), and the Y' sample is acquired at the time halfway between two adjacent Y samples.|$|E
5000|$|Another {{advantage}} of Y′UV {{is that some}} of the information can be discarded in order to reduce bandwidth. The human eye has fairly little spatial sensitivity to color: the accuracy of the brightness information of the <b>luminance</b> <b>channel</b> has far more impact on the image detail discerned than that of the other two. Understanding this human shortcoming, standards such as NTSC and PAL reduce the bandwidth of the chrominance channels considerably. (Bandwidth is in the temporal domain, but this translates into the spatial domain as the image is scanned out.) ...|$|E
5000|$|In 1963, {{prior to}} the {{development}} of the 2001, an experimental four tube camera was constructed by EMI engineers. This experimental camera had been inspired by RCA's new four tube camera, the TK42, and used the same tube arrangement, i.e. a 4½ inch image Orthicon tube in the <b>luminance</b> <b>channel</b> and three 1” Vidicon tubes in the colour channels. In addition, the experimental camera had an integrally mounted Varotal III zoom lens. It was demonstrated to the BBC in 1964 [...] where it received a mixed reception. Pictures from the camera had disappointing colorimetry, but sharp luminance detail.|$|E
40|$|To {{determine}} {{the relationship between}} the spatial <b>channels</b> for <b>luminance</b> and shape-from-stereo-disparity processing we measured disparity modulation sensitivity as a function of disparity spatial frequency for sinusoidal modulations of a field of Gabor micropatterns of differing luminance spatial frequency. We first examine the effects of contrast, spatial bandwidth and element density and show that it is only the last of these which is critical for the shape of the disparity modulation threshold function. We show that the shape of this function depends on the luminance spatial frequency of the surface that is modulated in depth. Specifically, low corrugation frequencies enjoy a greater scale support from the early luminance spatial filters than do high corrugation frequencies. The results are consistent with higher spatial frequency disparity channels receiving a greater input from higher spatial frequency <b>luminance</b> <b>channels.</b> © 1998 Elsevier Science Ltd. All rights reserved...|$|R
40|$|The {{aim of the}} thesis, {{on which}} {{this article is based}} / 10 /, was to study {{visibility}} and annoyance of digital watermarks. The main focus was on code reading applications for camera phones. Two experiments were performed, annoyance measurement and visibility measurement. In the annoyance measurement, digital watermarks were embedded in different color channels with different embedding strengths. In the visibility measurement, the aim was {{to find out if the}} visibility of a watermark can be predicted from the intensity distribution of a color channel. The results of visibility and annoyance of specific watermarks were in agreement with previous studies. Watermarks embedded in blue or yellow color channels were considered to be imperceptible and the least annoying in contrast to watermarks embedded in red, green or <b>luminance</b> <b>channels</b> that were regarded as visible and annoying. No clear connection was found between the tonal range of a specific color channel and the visibility of a watermark...|$|R
40|$|The visual evoked {{magnetic}} response (VEMR) {{was measured}} over the occipital cortex to pattern and flash stimuli in 86 normal subjects aged 15 - 86 years. The latency {{of the major}} positive component (outgoing magnetic field) to the pattern reversal stimulus (P 100 M) increased with age, particularly after 55 years, while the amplitude of the P 100 M decreased more gradually over the lifespan. By contrast, the latency of the major positive component to the flash stimulus (P 2 M) increased more slowly with age after about 50 years, while its amplitude may have decreased in only a proportion of the elderly subjects. The changes in the P 100 M with age may reflect senile changes {{in the eye and}} optic nerve, e. g. senile miosis, degenerative changes in the retina or geniculostriate deficits. The P 2 M may be more susceptible to senile changes in the visual cortex. The data suggest that the contrast channels of visual information processing deteriorate more rapidly with age than the <b>luminance</b> <b>channels...</b>|$|R
50|$|In 1962, {{in order}} to address these {{stability}} problems, RCA announced their prototype four-tube camera. The aims of the designers of the camera were, firstly, to produce a camera that was more tolerant to mis-registration and, secondly, to achieve a lighter camera by using smaller vidicon tubes to replace some of the large heavy IO tubes. The camera had an image orthicon tube for the <b>luminance</b> <b>channel</b> and three vidicon tubes for the colour channels. In addition, the camera was fully transistorized, apart from the four pick-up tubes. The camera went into full production in 1963 and sales of several hundred of the model were achieved over the next few years.|$|E
50|$|Color {{sensitivity}} improves steadily {{over the}} first year of life for humans due to strengthening of the cones of the eyes. Like adults, infants have chromatic discrimination using three photoreceptor types: long-, mid- and short-wavelength cones. These cones recombine in the precortical visual processing to form a <b>luminance</b> <b>channel</b> and two chromatic channels that help an infant to see color and brightness. The particular pathway used for color discrimination is the parvocellular pathway. There is a general debate among researchers with regards to the exact age that infants can detect different colors/chromatic stimuli due to important color factors such as brightness/luminance, saturation, and hue. Regardless of the exact timeline for when infants start to see particular colors, it is understood among researcher that infants' color sensitivity improves with age.|$|E
5000|$|The Mach bands {{effect is}} due to the spatial high-boost {{filtering}} performed by the human visual system on the <b>luminance</b> <b>channel</b> of the image captured by the retina. Mach reported the effect in 1865, conjecturing that filtering is performed in the retina itself, by lateral inhibition among its neurons. [...] This conjecture is supported by observations on other (non-visual) senses, as pointed out by von Békésy. [...] The visual pattern is often found on curved surfaces subject to a particular, naturally-occurring illumination, so the occurrence of filtering can be explained as the result of learnt image statistics. The effect of filtering can be modeled as a convolution between a trapezoidal function that describes the illumination and one or more bandpass filters. A tight approximation is obtained by a model employing 9 even-symmetric filters scaled at octave intervals.|$|E
30|$|We {{implemented}} a prototype on an Android device {{and used the}} camera burst mode to take 10 images to be fed into the SR system. The images enter the modules described in the previous subsections as a whole set. Individual images are processed in each module in a sequential manner. Like other SR works, we only performed our proposed SR method on the <b>luminance</b> (Y) <b>channel</b> of the images and simply performed bicubic interpolation on the chroma channels (UV).|$|R
40|$|Transferring artistic styles onto {{everyday}} photographs {{has become}} an extremely popular task in both academia and industry. Recently, offline training has replaced on-line iterative optimization, enabling nearly real-time stylization. When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style. This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks. Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and <b>luminance</b> <b>channels,</b> and performs stylization hierarchically with multiple losses of increasing scales. Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by conducting much more sophisticated training offline. By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones. That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales. Comment: Accepted by CVPR 201...|$|R
40|$|AbstractResearch {{has shown}} that the {{sensitivity}} to second-order modulations of carrier contrast is lower than that to first-order luminance modulations stimuli. We sought to compare the efficiency of processing first- and second-order information. Employing a phase-discrimination paradigm we found that when humans were given sufficient a priori information of signal parameters they detected both luminance and contrast modulations of 0. 6 and 2 c/deg by a phase-sensitive algorithm. The overall detection efficiency for second-order patterns, however, was lower that that for first-order stimuli. To study the factors which limit the efficiency of first- and second-order vision, we measured detection performance for luminance and contrast modulations of 0. 6 and 2 c/deg embedded in Gaussian noise. The results showed that the detection of second-order patterns had lower sampling efficiency and higher additive internal noise as compared to the detection of first-order stimuli. Classification images for detecting contrast modulations of 2 c/deg resembled the side-band component of the contrast modulations which suggests that human observers may detect contrast modulations of a sinusoidal carrier using first-order <b>luminance</b> <b>channels.</b> The lower sensitivity of the mechanism detecting second-order patterns might be due to higher levels of additive internal noise and lower sampling efficiency than those of the mechanism analysing first-order patterns...|$|R
50|$|Following on {{from the}} TK-42, RCA {{produced}} several more cameras with a similar format. The TK-43 was {{a version of the}} TK-42, but with an external lens, The TK-44 had an Isocon tube in the <b>luminance</b> <b>channel</b> and three Plumbicons for the colour channels. (The Isocon tube was more sensitive than the image orthicon so enabling the camera to operate at very low light levels, in outside broadcast use. However, it was a short-lived tube). There was also a version of the TK-42 in which selenicon tubes replaced the vidicons. None of these cameras sold in anything like the quantities of the TK-42. RCA also produced a four vidicon film camera, the TK-27. The advent of the Plumbicon tube brought about the end for the image orthicon type tubes and {{from the middle of the}} 1960s onwards, virtually all television cameras contained Plumbicons, regardless of the number of tubes used in a camera. (This tube had first been announced ten years previously, initially for medical use, but much time and effort had been invested in making it suitable for broadcast TV use).|$|E
5000|$|The various {{manufacturers}} of 4-tube cameras used different methods {{to resolve this}} problem. RCA, in their TK-42, placed an optical filter {{in front of the}} luminance tube which transmitted light according to the required luminosity function. [...] Marconi, in the Mk VII, used a dichroic mirror to reflect light to the luminance tube which had the required luminosity function. [...] EMI, in their 2001 camera, formed a low frequency luminance signal from the band-limited colour channels, according to 3-tube practice. The wideband luminance signal was corrected by this so as to achieve the required luminance characteristic at low frequencies. This process was referred to as the delta-L correction method. In the GE four-tube cameras a neutral splitting prism, with a semi-silvered surface, was used to provide the image for the luminance tube, so no spectral shaping was used in the <b>luminance</b> <b>channel.</b> Instead, the cameras possessed an eight-position filter wheel, located just after the zoom lens, so that a filter could be chosen to give optimum colorimetry in the tv picture, according to the lighting conditions encountered in a scene ...|$|E
5000|$|EMI {{engineers}} {{visited the}} USA in 1963, {{in order to}} view RCA's new four-tube colour camera, the TK42. Immediately following this visit, EMI Research Labs. embarked on a program to build an experimental camera using the same format. The construction took only six weeks of intensive effort, aided by the cannibalization of parts from existing EMI cameras. Items were taken from an EMI Type 203 image Orthicon monochrome studio camera, for the <b>luminance</b> <b>channel,</b> and a Type 204 industrial colour camera, for the colour channels. This camera contained 3 Vidicon tubes and a colour-splitting system using plate glass dichroic mirrors. In addition, a Varotal III zoom lens was integrated {{into the body of}} the experimental camera. The camera was housed in a simple box-shaped structure with ribs of extruded aluminium and with plain side panels. The experimental camera was demonstrated to the BBC in 1963 where it received a mixed reception. At that time, the BBC was evaluating an early Philips 3-tube camera which used some newly available Plumbicon pick-up tubes. It had been set up by BBC engineers to give highly saturated colour pictures and they were unimpressed by the 'tinted' pictures of the EMI camera. [...] In order to better judge the performance of the then existing cameras, the BBC organized comparison tests between the experimental EMI camera, a Philips camera, and a Marconi three I.O. camera. In these tests, the colorimetry of the pictures from the EMI camera compared unfavourably with the other two, but it did give the sharpest pictures.|$|E
40|$|Mainline {{approaches}} for content description for copy detection utilize global or local descriptors from video and comparing these descriptors for similarity. In the literature [16], {{it has been}} shown that local features perform better in terms of robustness on the other hand global features are computationally simpler. Local features for content description can be extracted around pixels returned by interest point detectors [17]. Thus, an interest point detector followed by a feature extractor is enough for describing most local aspects of a video scene. Our approach to CCD is based on the clustering of SIFT descriptors and comparing video scenes by their memberships to these clusters. A codebook C, which holds the information for SIFT clusters, is created. SIFT descriptors obtained from <b>luminance</b> <b>channels</b> of sample videos are clustered with k-means algorithm and resulting cluster centers are stored in this codebook. Further extracted SIFT descriptors are assigned a code, which is the index of the nearest cluster center to the descriptor, from this codebook. A reference database R, inside which queries will be searched, is created. This reference database holds the codes of every interest point from selected frames of reference videos. Query clips (Q), whose copies will searched inside the database, are summarized with the exact method of obtaining codes from interest points. Query codes are searched inside the reference database and matching reference video locations ar...|$|R
40|$|We have {{investigated}} the effect of ageing on the visual system using the relatively new technique of magentoencephalography (MEG). This technique measures the magnetic signals produced by the visual system using a SQUID magnetometer. The magnetic visual evoked field (VEF) was measured over the occipital cortex to pattern and flash stimuli in 86 normal subjects aged 15 - 86 years. Factors that influenced subject defocussing or defixating the stimulus or selective attention were controlled as far as possible. The latency of the major positive component to the pattern reversal stimulus (P 100 M) increased with age particularly {{after the age of}} 55 years while the amplitude of the P 100 M decreased over the life span. The latency of the major flash component (P 2 M) increased much more slowly with age, while its amplitude decreased in only a proportion of elderly subjects. Changes in the P 100 M with age may reflect senile changes in the eye and optic nerve, e. g. senile miosis or degenerative changes in the retina. The P 2 M may be more susceptible to senile changes in the retina. The data suggest that the spatial frequency channels deteriorate more rapidly with age than the <b>luminance</b> <b>channels</b> and that MEG may be an effective method of studying ageing in the visual system...|$|R
30|$|Secondly, {{histogram}} rescaling is applied[*]to[*]both enhanced <b>luminance</b> and chrominance <b>channel</b> {{to realize}} normalization of the results. This statistical rescaling for {{black and white}} point correction removes the influences of some extreme intensity of pixels. The hue component in CIELCh remains unchanged to maintain the hue constancy property, and the image is reconstructed with enhanced luminance and chrominance together with the unchanged hue component from CIELCh space to RGB space.|$|R
30|$|The CSI {{experiments}} are {{performed on the}} image block with different sizes cropped {{from the center of}} the full-size images. Our {{experiments are}} performed in the <b>luminance</b> <b>channel</b> of all images because the <b>luminance</b> <b>channel</b> contains information of all the three RGB channels. In fact, experiments in the other channel are also performed and have similar results.|$|E
30|$|Using {{only the}} {{luminance}} Y (ITU-R BT. 601), {{as in the}} original experiments of SUSAN [21], is not appropriate {{because there are many}} edge samples that do not appear on the <b>luminance</b> <b>channel,</b> but only on the chrominance channels. For example, two neighboring pixels with the same luminance, but opposite extreme values of chrominance show no edges on the <b>luminance</b> <b>channel.</b>|$|E
40|$|A psychophysical {{experiment}} {{was performed to}} measure the visibility of chromatic noise. Through Principle Component Analysis (PCA) {{on the results of}} this experiment, an orthogonal color space with the <b>luminance</b> <b>channel</b> independent of chromatic channels was constructed. By transforming noise images into this space, the visibility of chromatic noise can be predicted. Comparison with other opponent color spaces illustrates their relative properties regarding cross-talk of chromatic noise into the <b>luminance</b> <b>channel</b> (or vice versa). 1...|$|E
30|$|Visual image {{enhancement}} algorithms predominantly focus on enhancing high spatial frequency components for augmented low-vision aid. The high spatial frequency components {{of an image}} are where sudden large changes in pixel value occur over a short period, such as an objects edges or fine detail within an image. This has led to edge detection, a common digital image processing technique, being a critical underlying process in high spatial frequency enhancement. Peli et al. originally proposed augmented vision models utilising visual multiplexing techniques such as the wideband enhancement algorithm that superimposed modified image edges over the original image, enhancing the high spatial frequency components [8, 9]. Further work includes modifying the discrete cosine transform to increase the mid-range spatial frequencies of JPEG images [10], which was then extended to include MPEG video enhancement [11] and recently a modified edge detection algorithm for contour shape enhancement applied to image <b>luminance</b> <b>channels</b> [12]. Wolffsohn et al. demonstrated a significant increase of perceived image quality of television video amongst visually impaired individuals by recording television segments onto a computer with user-selectable software overlaid coloured edges on the original scene [7]. Atabany et al. developed a robust scene edge enhancement algorithm that focused on efficiently blurring the original image with anisotropic smoothing to obtain edge enhancement of only the major image edge boundaries while effectively ignoring minor detail edge occurrences [14]. In the previous literature binary edge detection algorithms, e.g. Sobel and Canny edge detection, are extensively used for spatial frequency enhancement. Gibson et al. demonstrated a significant subjective preference in perceived image quality among simulated low-vision subjects with a gradient-based statistical edge detection algorithm applied to real-world scenes, where major object boundaries have a high magnitude level and more subtle edges, common of fine detail, are highlighted with lower magnitude edges [16].|$|R
5000|$|Within {{the active}} {{portion of the}} video, the data words {{correspond}} to signal levels of the respective video components. The <b>luminance</b> (Y) <b>channel</b> is defined such that a signal level of 0 mV is assigned the codeword 64 (40 hex), and 700 millivolts (full scale) is assigned the codeword 940 (3AC) [...] For the chroma channels, 0 mV is assigned the code word 512 (200 hex), -350mV is assigned a code word of 64 (0x40), and +350mV is assigned a code word of 960 (3C0). Note that the scaling of the luma and chroma channels is not identical. The minimum and maximum of these ranges represent the preferred signal limits, though the video payload may venture outside these ranges (providing that the reserved code words of 0 - 3 and 1020 - 1023 are never used for video payload).|$|R
40|$|International audienceIn most digital cameras, color {{images are}} {{captured}} by a sensor overlaid by the Bayer color filter array (CFA). Denoisaicking (joint demosaicking and denoising) consists in reconstructing a color {{image from the}} noisy “Bayerized” data output by the sensor. We show that the frequency analysis of the sampling pattern induced by the Bayer CFA provides {{a simple way to}} reconstruct the <b>luminance</b> and chrominance <b>channels</b> of the image. The process is reduced to adequate linear filtering operations and denoising of the grayscale luminance image...|$|R
