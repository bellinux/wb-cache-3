219|5475|Public
5000|$|To {{provide a}} well defined easy-to-parse audit record structure, {{as well as}} {{scalable}} and reliable storage for the <b>logged</b> <b>data.</b>|$|E
5000|$|Visualization of {{real time}} sensor data and path planner status, {{as well as}} {{visualization}} of <b>logged</b> <b>data</b> and simulation data.|$|E
50|$|CompactRIO is {{commonly}} used as headless systems (without a user interface) which are designed to run in a confined space, under harsh conditions. CompactRIO systems can also be connected to a host PC {{which can be used}} for supervisory purposes and for displaying <b>logged</b> <b>data.</b>|$|E
30|$|The {{data from}} Well#A (1046 core and <b>log</b> <b>data)</b> {{were chosen to}} provide the {{training}} patterns. This well was chosen because, it had the most complete set of core and <b>log</b> <b>data.</b> It was randomly divided into training data (70  %) and testing data (30  %). The data from Well#B (152 core and <b>log</b> <b>data),</b> Well#C (91 core and <b>log</b> <b>data)</b> and Well#D (40 core and <b>log</b> <b>data)</b> were {{used to test the}} model’s ability to predict porosity in the oilfield.|$|R
40|$|Collaborative {{applications}} usually register {{user interaction}} {{in the form}} of semi-structured plain text event <b>log</b> <b>data.</b> Extracting and structuring this data is a prerequisite for later key processes such as the analysis of interactions, assessment of group activity, or the provision of awareness and feedback. Yet, in real situations of online collaborative activity the processing of <b>log</b> <b>data</b> is usually done offline since structuring event <b>log</b> <b>data</b> is, in general, a computationally costly process and the amount of <b>log</b> <b>data</b> tends to be very large. Techniques to speed and scale up the structuring and processing of <b>log</b> <b>data</b> with minimal impact on the performance of the collaborative application are thus desirable {{in order to be able}} to process <b>log</b> <b>data</b> in real time. In this paper we present a parallel Grid-based implementation for processing in real time the event <b>log</b> <b>data</b> generated in collaborative applications. Our results show the feasibility of using grid middleware to speed and scale up the process of structuring and processing semi-structured event <b>log</b> <b>data.</b> Our Grid prototype follows the Master-Worker paradigm, is implemented using the Globus Toolkit 3. 2 and is tested on the Planetlab platform...|$|R
40|$|Mining {{frequent}} patterns from web <b>log</b> <b>data</b> {{can help}} to optimise {{the structure of a}} web site and improve the performance of web servers. Web users can also benefit from these frequent patterns. Many efforts have been done to mine frequent patterns efficiently. Candidate-generation-and-test approach (Apriori and its variants) and pattern-growth approach (FP-growth and its variants) are the two representative frequent pattern mining approaches. Neither candidate-generation-and-test approach nor pattern-growth approach is always good on web <b>log</b> <b>data.</b> We have conducted extensive experiments on real world web <b>log</b> <b>data</b> to analyse the characteristics of web logs and the behaviours of these two approaches on web <b>log</b> <b>data.</b> We propose a new algorithm – Combined Frequent Pattern Mining (CFPM) algorithm to cater for web <b>log</b> <b>data</b> specifically. We use some heuristics in web <b>log</b> <b>data</b> to prune search space and reduce many unnecessary operations in mining, so that better efficiency is achieved. Experimental results show that CFPM can significantly improve the performance of pattern-growth approach by 1. 2 ˜ 7. 8 times on frequent pattern mining on web <b>log</b> <b>data...</b>|$|R
50|$|Modern {{actuators}} {{have extensive}} diagnostic functions {{which can help}} identify {{the cause of a}} failure. They also log the operating data. Study of the <b>logged</b> <b>data</b> allows the operation to be optimised by changing the parameters and the wear of both actuator and valve to be reduced.|$|E
5000|$|Dvorak {{encoding}} {{may make}} it harder for physically present surveillance {{to find out what the}} user is typing since it happens very fast. In some cases, it can even be hard for a physical keylogger to probably guess what is being written, but since most keyloggers save the <b>logged</b> <b>data</b> for later retrieval and analysis, the original cryptographic weakness of the encoding applies.|$|E
5000|$|Since the <b>logged</b> <b>data</b> {{are stored}} {{away from the}} {{monitored}} linux devices, LUARM {{can act as a}} valuable complement to existing data forensic investigation tools. This is because it is immune to the “observer effect” and the dangers of “static” forensic analysis: dynamic information about file, network and process activity is not lost and examining/logging data does not affect the source media state).|$|E
40|$|Huge {{amounts of}} search <b>log</b> <b>data</b> have been {{accumulated}} at web search engines. Currently, a popular web search engine may every day receive billions of queries and collect tera-bytes of records about user search behavior. Beside search <b>log</b> <b>data,</b> {{huge amounts of}} browse <b>log</b> <b>data</b> have also been collected through client-side browser plug-ins. Such massive amounts of search and browse <b>log</b> <b>data</b> provide great opportunities for mining the wisdom of crowds and improving web search. At the same time, designing e↵ective and e cient methods to clean, process, and model <b>log</b> <b>data</b> also presents great challenges. In this survey, we focus on mining search and browse <b>log</b> <b>data</b> for web search. We start with an introduction to search and browse <b>log</b> <b>data</b> and an overview of frequently-used <b>data</b> summarizations in <b>log</b> mining. We then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, document ranking, user understanding, and monitoring & feedbacks. For each aspect, we survey the major tasks, fundamental principles, and state-of-the-art methods...|$|R
40|$|Abstract-Log Analytics is a {{technique}} for automatically understanding the meaningful patterns from heterogeneous <b>log</b> <b>data.</b> Every activity taking place in an application or device is recorded in a log file. Valuable information is stored on <b>log</b> <b>data</b> which can be extracted and stored into Big Data platforms. Prediction and Classification can be performed over the <b>log</b> <b>data...</b>|$|R
40|$|Abstract [...] In {{this paper}} we {{introduce}} {{the concept of}} synchronous patterns, association and correlation rule, and search {{how they can be}} mined efficiently. The title of synchronous pattern mining is indeed rich. In this paper is dedicated to methods of synchronous item set mining. We can find synchronous item sets from large amount of web <b>log</b> <b>data</b> using association and correlation rules, and the web <b>log</b> <b>data</b> are either transactional or relational. We can mine association and correlation rules in multilevel and multidimensional space. The association and correlation rules are the most interesting. The techniques learned in this paper may also be extended for more advanced forms of synchronous pattern mining. Finding such synchronous patterns plays an essential role in mining association, correlation, and many other interesting relationships among web <b>log</b> <b>data.</b> Moreover in this paper helps in web <b>log</b> <b>data</b> classification, clustering and other web <b>log</b> <b>data</b> mining task. Thus synchronous pattern mining has become an important web <b>log</b> <b>data</b> mining task and focused in web <b>log</b> <b>data</b> mining research. Keywords [...] Correlation rules, clustering, <b>data</b> mining, web <b>logs,</b> multilevel, multidimensional <b>data,</b> and association rule, item set mining. I...|$|R
50|$|Grain yield {{monitors}} {{have been}} in production since yield monitors were introduced in the early 1990s and have been progressively updated with better hardware that provides the harvester operator with a better user interface and makes the <b>logged</b> <b>data</b> information more readily available. These monitors are typically referred to as displays in reference to their purpose in many cases is to display the yield information in a spatial color coded map.|$|E
50|$|TI-RTOS Kernel {{provides}} modules {{that allow}} it {{to provide information about}} how the system is executing. This includes how different threads are loading the CPU over time as well as logging events as they occur in both the system application as well as within the TI-RTOS Kernel itself. In addition, the Code Composer Studio integrated development environment can take this <b>logged</b> <b>data</b> and graphically display it for the developer.|$|E
5000|$|Common {{hand-held}} {{metal detectors}} {{are widely used}} by archaeologists. Most of these instruments do not create a <b>logged</b> <b>data</b> set and thus cannot be used for directly creating maps, but used in a systematic manner {{they can be a}} useful tool in archaeological research. Sometimes external data loggers are attached to such detectors which collect information about detected materials and corresponding gps coordinates for further processing. Misuse of these instruments on archaeological sites by treasure hunters and artifact collectors has been a serious problem in archaeological preservation [...] however cooperative efforts between skilled amateur operators and academic teams are emerging in the field.|$|E
40|$|<b>Log</b> <b>data</b> provide {{valuable}} insight into observable behavioural patterns, {{which could be}} inferred to study a learners cognitive processes, levels of motivation and levels of knowledge acquisition. To date, {{most of the research}} work has been devoted to study the different methods to analyze and interpret <b>log</b> <b>data.</b> Little attention, however, has been given to use <b>log</b> <b>data</b> as a tool to investigate the behaviour of Bayesian learner models. In this light, this article discusses how <b>log</b> <b>data</b> could be employed to investigate the performance of Bayesian learner models. The <b>log</b> <b>data</b> were firstly transformed into a set of structured dataset, which conformed to the INQPROs learner model. The transformed dataset were then fed into different versions of INQPROs learner model to obtain their predictive accuracies. From the predictive accuracies, an optimal learner model was identified. Empirical results indicated that the <b>log</b> <b>data</b> approach provides an efficient way to study the behaviour of a Bayesian learner mode...|$|R
40|$|Abstract: Web usage mining {{deals with}} {{understanding}} the Visitor’s behaviour with a Website. It helps {{in understanding the}} concerns such as present and future probability of every website user, relationship between behaviour and website usability. It has different branches such as web content mining, web structure and web usage mining. The focus {{of this paper is}} on web mining usage patterns of an educational institution web <b>log</b> <b>data.</b> There are three types of web related <b>log</b> <b>data</b> namely web access log, error log and proxy <b>log</b> <b>data.</b> In this paper web access <b>log</b> <b>data</b> has been used as dataset because the web access <b>log</b> <b>data</b> is the typical source of navigational behaviour of the website visitor. The study of web server log analysis is helpful in applying the web mining techniques...|$|R
40|$|Abstract—Logging while {{drilling}} (LWD) is {{a drilling}} technique which obtains and transmits the <b>logging</b> <b>data</b> during oil/gas drilling operations. Because {{of the limited}} available transmission bandwidth of mud channel, how to improve the bandwidth utilization becomes a critical problem in LWD research. In this paper, by discussing the information entropy of real <b>logging</b> <b>data,</b> {{we find that the}} original encoding of <b>logging</b> <b>data</b> is inefficient. To prune the data redundancy effectively and to improve the bandwidth utilization, we propose a novel compression method on the basis of Differential Pulse Code Modulation (DPCM). To further improve the compression efficiency with less information loss, we introduce the adjustable compression parameters for different kinds of <b>logging</b> <b>data.</b> Extensive experiments with the real <b>logging</b> <b>data</b> show that our method can give enough precise results with the compression ratio of 50 % at least. The experiment results also show that our algorithm has the advantages of alterable compressing ratio, excellent decode quality and low algorithm complexity. Index Terms—logging while drilling; LWD; DPCM; <b>logging</b> <b>data</b> encode; I...|$|R
5000|$|The {{software}} {{comes with}} a second component known as [...] "KeyAlert", {{which is designed to}} monitor the use of websites and online chat services; when installed on the computer, it scans and logs keyboard input by the computer's user, searching for and logging the use of strings related to objectionable content and user-specified keywords. It can also provide e-mail notifications whenever such activity is detected. <b>Logged</b> <b>data</b> is stored on the computer's hard drive; on the Windows version, it is stored as unencrypted plain text. The OS X version does encrypt logging data with a password.|$|E
50|$|Along with {{technological}} advancements are costs and challenges associated with implementing EDM applications. These include {{the costs to}} store <b>logged</b> <b>data</b> and the cost associated with hiring staff dedicated to managing data systems. Moreover, data systems may not always integrate seamlessly {{with one another and}} even with the support of statistical and visualization tools, creating one simplified version of the data can be difficult. Furthermore, choosing which data to mine and analyze can also be challenging, making the initial stages very time consuming and labor-intensive. From beginning to end, the EDM strategy and implementation requires one to uphold privacy and ethics for all stakeholders involved.|$|E
50|$|In fact, {{many other}} {{functional}} forms appear approximately linear on the log-log scale, and simply evaluating {{the goodness of}} fit of a linear regression on <b>logged</b> <b>data</b> using the coefficient of determination (R2) may be invalid, as the assumptions of the linear regression model, such as Gaussian error, may not be satisfied; in addition, tests of fit of the log-log form may exhibit low statistical power, as these tests may have low likelihood of rejecting power laws {{in the presence of}} other true functional forms. While simple log-log plots may be instructive in detecting possible power laws, and have been used dating back to Pareto in the 1890s, validation as a power laws requires more sophisticated statistics.|$|E
40|$|Log {{records are}} {{important}} part of an organization. Maintaining log records securely {{for a longer period}} of time is important for proper functioning of any organization. Since log files contain record of system events, the confidentiality and privacy of <b>log</b> <b>data</b> should be maintained and also integrity of <b>log</b> <b>data</b> and <b>logging</b> process should be ensured. The <b>log</b> <b>data</b> are stored in the server with in an organization for a fixed time and sent to the cloud. There will be a great chance of attack when <b>log</b> <b>data</b> are stored in plain text in the server of an organization. However, deploying a secure logging framework is one of the main difficulties that an organization faces in this new era. In this paper, we present an approach for secure logging by which <b>log</b> <b>data</b> can be sent to the cloud directly at run time...|$|R
5000|$|... 2006- IHS {{purchases}} Nixon Digital <b>Data</b> <b>Logs</b> {{which provides}} <b>log</b> <b>data</b> for South Texas ...|$|R
30|$|Beyond the network, {{host-based}} event <b>log</b> <b>data</b> {{has traditionally}} {{been one of the}} main sources for Intrusion Detection monitoring. An organization can have a multitude of computing hosts both in quantity as well as diversity in terms of the different types of log files being generated. All of this <b>log</b> <b>data</b> can quickly add up to Big Host Event <b>Log</b> <b>Data</b> in that it can be very high in Volume, Velocity, and Variety.|$|R
50|$|Originally AMR devices just {{collected}} meter readings electronically and matched {{them with}} accounts. As technology has advanced, additional data {{could then be}} captured, stored, and transmitted to the main computer, and often the metering devices could be controlled remotely. This can include events alarms such as tamper, leak detection, low battery, or reverse flow. Many AMR devices can also capture interval data, and log meter events. The <b>logged</b> <b>data</b> {{can be used to}} collect or control time of use or rate of use data {{that can be used for}} water or energy usage profiling, time of use billing, demand forecasting, demand response, rate of flow recording, leak detection, flow monitoring, water and energy conservation enforcement, remote shutoff, etc. Advanced Metering Infrastructure, or AMI is the new term coined to represent the networking technology of fixed network meter systems that go beyond AMR into remote utility management. The meters in an AMI system are often referred to as smart meters, since they often can use collected data based on programmed logic.|$|E
5000|$|In February 1993 the {{research}} {{contract for the}} chart was put out to tender, with a new four-year contract beginning 1 February 1994 offered. Millward Brown, Research International and Nielsen Market Research were approached, and Gallup were invited to re-apply. In May, {{it was announced that}} Millward Brown had been accepted as the next chart compilers, signing a £1-million-a-year contract. Millward Brown took over compiling the charts on 1 February 1994, increasing the sample size; {{by the end of the}} month each shop sampled used a barcode scanner linking via an Epson terminal with a modem to a central computer (called [...] "Eric"), which <b>logged</b> <b>data</b> from more than 2,500 stores. Gallup attempted to block Millward Brown's new chart by complaining to the Office of Fair Trading about the contractual clause in which BARD retailers exclusively supplied sales data to the CIN, but the interim order was rejected. In June 1995 the case was dropped, after the clause allowing BARD retailers to supply sales information to other chart compilers was deleted; because CIN retained the copyright, other compilers could not use (or sell) the information.|$|E
5000|$|The EFF did not {{consider}} the scanning portion of the software to be adequately effective due to {{a large number of}} false positives, a lack of support for web browsers other than Internet Explorer and Safari, along with an inability to distinguish between application data files and user files, or scan the contents of image files themselves. More significantly, the key logger was criticized for storing <b>logged</b> <b>data</b> in non-encrypted plain text on the user's hard drive, including passwords and other sensitive information. It was also found that an insecure connection was used to transmit the log data to a third-party server to generate e-mail notifications; log data could easily be intercepted over a public Wi-Fi hotspot using network analysis software. ComputerCop head Stephen DelGiorno denied any major problems with the software, stating that their software [...] "doesn’t give sexual predator [...] or identity thieves more access to children’s computers", as it [...] "works with the existing email and Internet access services that computer user has already engaged", but noted that they would update their privacy policy to indicate that they did not store user information.|$|E
40|$|Background: In eHealth research, limited {{insights}} {{have been}} obtained on process outcomes or how {{the use of technology}} has contributed to the users’ ability to have a healthier life, improved wellbeing, or activate new attitudes in their daily tasks. As a result, eHealth is often perceived as a black box. To open this Black Box of eHealth, methodologies must extend beyond the classic effect evaluations. The analyses of <b>log</b> <b>data</b> (anonymous records of real-time actions performed by each user) can provide continuous and objective insights into the actual usage of the technology. However, until now the possibilities of <b>log</b> <b>data</b> in eHealth research has not been exploited to its fullest extent. Objectives: The aim {{of this paper is to}} describe how <b>log</b> <b>data</b> can be used to improve the evaluation and understand the use of an eHealth technology with a broader approach than only descriptive statistics. This paper serves as a starting point for using <b>log</b> <b>data</b> analysis in eHealth research. Methods: First, we describe what <b>log</b> <b>data</b> is and an overview is given of research questions to evaluate the system, the context, the users of a technology as well as the underpinning theoretical constructs. Secondly, requirements for <b>log</b> <b>data,</b> the starting points for the data preparation and methods for data collection are explained. Results: In the third part, some methods for data analysis are described. Finally, a conclusion is drawn regarding the importance of the results for both scientific and practical applications. Conclusion: The analysis of <b>log</b> <b>data</b> can be of great value for opening the black box of eHealth. A deliberate <b>log</b> <b>data</b> analysis can give new insights into how the usage of the technology contributed to the found effects and can thereby help to improve the persuasiveness and effectiveness of the eHealth technology and the underpinning behavioral models. Keywords: eHealth, black box, evaluation, <b>log</b> <b>data</b> analysi...|$|R
40|$|<b>Log</b> <b>data</b> {{adapted for}} {{intrusion}} detection {{is a little}} explored research issue despite its importance for successful and efficient detection of attacks and intrusions. This paper presents a starting point {{in the search for}} suitable <b>log</b> <b>data</b> by providing a framework for determining exactly which <b>log</b> <b>data</b> that can reveal a specific attack, i. e. the attack manifestations. An attack manifestation consists of the log entries added, changed or removed by the attack compared to normal behaviour. We demonstrate the use of the framework by studying attacks in different types of <b>log</b> <b>data.</b> This work provides a foundation for a fully automated attack analysis. It also provides some pointers for how to define a collection of log elements that are both sufficient and necessary for detection of a specific group of attacks. We believe that this will lead to a <b>log</b> <b>data</b> source that is especially adapted for intrusion detection purposes...|$|R
40|$|Logging while {{drilling}} (LWD) is {{a drilling}} technique which obtains and transmits the <b>logging</b> <b>data</b> during oil/gas drilling operations. Because {{of the limited}} available transmission bandwidth of mud channel, how to improve the bandwidth utilization becomes a critical problem in LWD research. In this paper, by discussing the information entropy of real <b>logging</b> <b>data,</b> {{we find that the}} original encoding of <b>logging</b> <b>data</b> is inefficient. To prune the data redundancy effectively and to improve the bandwidth utilization, we propose a novel compression method on the basis of Differential Pulse Code Modulation (DPCM). To further improve the compression efficiency with less information loss, we introduce the adjustable compression parameters for different kinds of <b>logging</b> <b>data.</b> Extensive experiments with the real <b>logging</b> <b>data</b> show that our method can give enough precise results with the compression ratio of 50 % at least. The experiment results also show that our algorithm has the advantages of alterable compressing ratio, excellent decode quality and low algorithm complexity...|$|R
40|$|Problem {{diagnosis}} {{in large}} software systems is a challenging and complex task. The sheer complexity {{and size of}} the <b>logged</b> <b>data</b> make it often difficult for human operators and administrators to perform problem diagnosis and root cause analysis. A challenge in this area is to provide the necessary means, tools, and techniques for the operators to focus their attention to specific parts of the <b>logged</b> <b>data</b> reducing thus the complexity of the diagnostic process. In this paper, we propose a framework for filtering logs according to specific analysis goals and diagnostic hypotheses set by the user or by an automated process. More specifically, the proposed framework uses annotated goal trees to model the constraints and the conditions by which the functionality of a particular system is being delivered. Next, a transformation process maps such constraints and conditions to a collection of queries that can be either applied to a relational database that stores the <b>logged</b> <b>data</b> or use Latent Semantic Indexing to identify the most relevant log entries for the given query. The results of such queries provide a subset of the <b>logged</b> <b>data</b> that is compliant with the goal tree and can be used by a diagnostic SAT-solver based algorithm. Experimental results show that the filtering process can reduce the time and complexity of the diagnosis when applied to multitier heterogeneous service oriented systems. © 2010 IEEE...|$|E
30|$|The Repast Simphony data {{collection}} {{system is designed}} to gather and store information from a simulation while it is running. The system typically collects predefined sets of data from each agent at each time step: a particular piece of <b>logged</b> <b>data</b> represents {{the state of the}} simulation at a particular time. This <b>logged</b> <b>data</b> can contain aggregate and nonaggregate values. An aggregate data value represents the result of an aggregate operation (e.g., summation) performed over some collection of individual values. Typically, these individual values correspond to agent properties. A nonaggregate value is some individual value, again typically an agent property.|$|E
40|$|Off-line {{intrusion}} detection systems rely on <b>logged</b> <b>data.</b> However, the logging mechanism may be complicated and time-consuming {{and the amount}} of <b>logged</b> <b>data</b> tends to be very large. To counter these problems we suggest a very simple and cheap logging method, light-weight logging. It can be easily implemented on a Unix system, particularly on the Solaris operating system from Sun. It is based on logging every invocation of the exec(2) system call together with its arguments. We use data from realistic intrusion experiments to show the benefits of the proposed logging and in partic ular that this logging method consumes as little system resources as comparable methods, while still being more effective. ...|$|E
50|$|In {{order to}} reduce the number of system calls in the fast path to a minimum, <b>log</b> <b>data</b> is stored in shared memory, and the task of monitoring, filtering, {{formatting}} and writing <b>log</b> <b>data</b> to disk is delegated to a separate application.|$|R
40|$|With the {{explosive}} growth of competitiveness on the internet, discovery and analysis of useful information from web <b>log</b> <b>data</b> becomes crucial to satisfy customers and users of websites in general. However analysis of large web <b>log</b> <b>data</b> is a complex task not free from flaws, which are not addressed by normal web log analysers. Web analytics methods are discussed and OLAP is proposed as the tool to analyse Web server <b>log</b> <b>data</b> in order to discover patterns and/or user behaviour from a Social website. Faculty of Technolog...|$|R
40|$|Pattern mining {{from the}} web <b>log</b> <b>data</b> leads to {{discovery}} of usage {{patterns of the}} user who navigate the web. Patterns which appear frequently in the web <b>log</b> <b>data</b> are item-sets and sequences. In this paper, a novel algorithm Intelligent Generalized Sequential pattern (IGSP) is designed which shows better results than the Generalized Sequential Pattern (GSP) algorithm. Experiment is conducted with respect to running time and number of patterns discovered from the <b>log</b> <b>data</b> and results has shown that IGSP outperforms the wellknown algorithms (GSP) algorithm...|$|R
