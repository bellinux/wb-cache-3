34|429|Public
40|$|In this paper, I {{examine a}} {{taxonomy}} of internet-based learning materials and introduce The Situational <b>Learning</b> <b>Matrix.</b> The Situational <b>Learning</b> <b>Matrix</b> considers students ’ goals for engaging with online educational materials versus the motivations of producers of those materials. I further discuss goal misalignment {{as a possible}} reason for failure of an online educational experience and provide an inventory of computer-based interactional and instructional objects...|$|E
40|$|This paper {{presents}} {{a method for}} analyzing human conceptions related to e-learning, based on positioning data on what is called here a <b>learning</b> <b>matrix.</b> The set of dimensions comprising the matrix distinguish between emphasis on individuality and sociality in learning, between viewing learning as knowledge adoption and as knowledge construction, and between viewing learning as subjective and as objective to time. The <b>learning</b> <b>matrix</b> is used to visualize and compare conceptions of learning extracted from literature and from individual perceptions of learning, revealed through interviews. This study supports the development of e-learning environments and casts light on different conceptions of learning. In addition deriving and representing the <b>learning</b> <b>matrix,</b> the paper positions 13 learning theories on it. A concluding analysis of an interview with an e-learning platform designer implies that while certain aspects of a designer’s conception of learning relate to learning theories, the overall conception remains unique and dynamic...|$|E
40|$|This {{research}} project produced {{a number of}} resources to assist faculties in reviewing their assessment practices. These resources include an Evidence of <b>Learning</b> <b>Matrix,</b> Assessment Review Flowchart and Action Plan. In completing a review of assessment practices, this would then inform a curriculum review or retreat...|$|E
5000|$|... 1963 <b>Learning</b> <b>matrices</b> {{and their}} {{applications}} (together with U. A. W. Piske) ...|$|R
5000|$|Choice of [...] must be {{designed}} to <b>learn</b> <b>matrices</b> A of a given type. See [...] "Special cases" [...] below.|$|R
25|$|Born, however, had <b>learned</b> <b>matrix</b> algebra from Rosanes, {{as already}} noted, but Born had also learned Hilbert's theory of {{integral}} equations and quadratic forms for {{an infinite number}} of variables as was apparent from a citation by Born of Hilbert's work Grundzüge einer allgemeinen Theorie der Linearen Integralgleichungen published in 1912.|$|R
40|$|When {{we seek to}} {{directly}} learn basis functions from natural scenes, we are confronted {{with the problem of}} simultaneous estimation of these basis functions and the coecients of each image (when projected onto that basis). In this work, we are mainly interested in <b>learning</b> <b>matrix</b> space basis functions and the projection coecients from a set of natural images. We cast this problem in a joint optimization framework...|$|E
40|$|We {{propose a}} generic method for iteratively {{approximating}} various second-order gradient steps - Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient - in linear time per iteration, using special curvature matrix-vector products {{that can be}} computed in O(n). Two recent acceleration techniques for online <b>learning,</b> <b>matrix</b> momentum and stochastic meta-descent (SMD), in fact implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD...|$|E
40|$|Because of {{overfitting}} and {{the improvement}} of generalization capability (GC) available {{in the construction of}} forecasting models using artificial neural network (ANN), a new method is proposed for model establishment by means of making a low-dimension ANN <b>learning</b> <b>matrix</b> through principal component analysis (PCA). The results show that the PCA is able to construct an ANN model without the need of finding an optimal structure with the appropriate number of hidden-layer nodes, thus avoids overfitting by condensing forecasting information, reducing dimension and removing noise, and GC is greatly raised compared to the traditional ANN and stepwise regression techniques for model establishment...|$|E
5000|$|Learning problem [...] can be {{generalized}} to admit <b>learning</b> task <b>matrix</b> A as follows: ...|$|R
40|$|We {{propose a}} {{functional}} approach to relevance <b>learning</b> and <b>matrix</b> adaptation for <b>learning</b> vector quantization of high-dimensional functional data. We show how parametrization of the functional relevance profile or functional <b>matrix</b> <b>learning</b> {{can be established}} for a reasonable number of adaptive parameters. In particular we empha-size model sparsity in terms of structural sparsity and feature selection...|$|R
5000|$|In {{the field}} of {{statistical}} <b>learning</b> theory, <b>matrix</b> regularization generalizes notions of vector regularization to cases where the object to be <b>learned</b> is a <b>matrix.</b> The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over ...|$|R
40|$|This paper {{presents}} LAS: Language for the Analysis of Structures. It is {{a development}} environment and {{a programming language}} for <b>learning</b> <b>matrix</b> structural analysis, dynamics of structures and the finite element method. LAS is a flexible learning environment in which users must program their own solutions to solve finite element static and dynamic problems. The language includes powerful operators, conditional expressions, loop expressions, and several functions (matrix manipulations, linear algebra, direct stiffness assembly, modal analysis, time domain dynamic analysis, frequency domain dynamic analysis). The development environment includes a code editor, a matrix manager, a finite element post-processor and a Fourier-analysis tool. Key words...|$|E
40|$|This thesis {{attempts}} {{to show that}} although Christian education has lost theological location and prominence {{in the life of}} the Western Protestant church, the emerging Church movement as an intentionally postmodern approach to ministry in the contemporary context can offer insights to reinterpret Christian education in this context. Christian education is given a new interpretive framework and is theologically located at the intersection of doxology and dogma. The importance of the Christian worshipping community as the most appropriate setting for Christian education is explored, and a teaching and <b>learning</b> <b>matrix</b> developed using worship as the basis for enhancing Christian formation and ethos...|$|E
40|$|Abstract — In this paper, {{we study}} the {{behavior}} of power suppliers who submit their bids to the market place {{in order to maximize}} their payoffs. The market clearing mechanism is based on the locational marginal price. To study the interaction of the power suppliers, we rely on two different approaches and compare the results obtained. One approach consists of computing the Nash equilibria of the market, and the other models each player’s behavior by using reinforcement learning algorithms. Simulations are carried out on a five node power system. Index terms — Electricity markets modeling, spot markets, Nash equilibria, reinforcement <b>learning,</b> <b>matrix</b> games. I...|$|E
40|$|This paper {{proposes a}} robust method to esti-mate the inverse {{covariance}} under noisy mea-surements. The method {{is based on}} the estima-tion of each column in the inverse covariance ma-trix independently via robust regression, which enables parallelization. Different from previous linear programming based methods that cannot guarantee a positive semi-definite covariance ma-trix, our method adjusts the <b>learned</b> <b>matrix</b> to sat-isfy this condition, which further facilitates the tasks of forecasting future values. Experiments on time series prediction and classification under noisy condition demonstrate the effectiveness of the approach. 1...|$|R
30|$|In the {{proposed}} technique, {{the structure of}} both RBM 1 and RBM 2 is exactly same. Initially, the RBM 1 is trained with the unlabeled data set x_ul^(i) and the <b>learned</b> weight <b>matrix</b> W_ 1 and bias vectors a_ 1,b_ 1 are used as initial values of the W_ 2, a_ 2, and b_ 2 of the RBM 2. RBM 2 is then trained with the training data set x_l^(i) and the <b>learned</b> weight <b>matrix</b> W_ 2 is further used for transformation. The learning rate, ϵ = 0.01, was used in training of both RBMs.|$|R
5000|$|A common {{approach}} for learning similarity, is {{to model the}} similarity function as a bilinear form. For example, {{in the case of}} ranking similarity learning, one aims to <b>learn</b> a <b>matrix</b> W that parametrizes the similarity function [...]|$|R
40|$|We {{propose a}} novel {{parallel}} asynchronous lock-free algorithmic {{framework for the}} minimization of the sum of a smooth nonconvex function and a convex nonsmooth regularizer. This class of problems arises in many big-data applications, including deep <b>learning,</b> <b>matrix</b> completions, and tensor factorization. Key features of the proposed algorithm are: i) it deals with nonconvex objective functions; ii) it is parallel and asynchronous; and iii) it is lock-free, meaning that components of the vector variables may be written by some cores while being simultaneously read by others. Almost sure convergence to stationary solutions is proved. The method enjoys properties that improve {{to a great extent}} over current ones and numerical results show that it outperforms existing asynchronous algorithms on both convex and nonconvex problems...|$|E
40|$|The paper {{describes}} a transformation {{that can be}} used to characterize patterns independent of their position. Examples of the application of the transform for the machine recognition of letters are discussed. The program succeeded in a recognition rate of 80 – 100 % for letters having different position, distortions, inclination, rotation up to 15 ° and size variation up to 1 : 3 relative to a reference set of 10 letters. Results with a program for the autonomous learning of new varieties of a pattern (using a <b>learning</b> <b>matrix</b> as an adaptive classifier) are given. When executed on a digital computer, this transform is 10 – 100 times faster than the fast Fourier transform (depending on the number of sampling points) ...|$|E
40|$|Problem: We {{consider}} the following bilinear model in the unknowns X ∈ R N×L and Φ ∈ R M×N, which has applications is dictionary <b>learning,</b> <b>matrix</b> completion, collaborative filtering, compressive system calibration, compressive sensing with dictionary uncertainty, and Bayesian experimental design: Y = P (ΦX) +W. (1) In (1), Y are known observations, P(·) accomplishes element-wise selection or linear projection, and W models additive perturbation. Please see [1] for further details. Approach: We take a Bayesian approach to the inference problems (in particular, posterior estimation) that revolve around the bilinear model (1). In particular, we leverage the approximate message passing (AMP) framework of [2], [3] and extend it to the bilinear domain. Compared to Bayesian approaches that rely on Gibbs sampling methods or variational inference, the AMP framework allows us to full...|$|E
30|$|Normalizing {{the columns}} of the input X, as in [22], {{is not a}} good option in the context of signal {{processing}} since frames with low amplitudes are typically noise and it would amplify them. Although this is not a problem for <b>learning</b> the coefficient <b>matrix</b> W, which is a column-independant process, it would increase the contribution of noise when <b>learning</b> the dictionary <b>matrix</b> D.|$|R
40|$|We {{propose a}} general and {{efficient}} algorithm for <b>learning</b> low-rank <b>matrices.</b> The proposed algorithm converges super-linearly and {{can keep the}} <b>matrix</b> to be <b>learned</b> in a compact factorized representation without the need of specifying the rank beforehand. Moreover, we show that the framework can be easily generalized {{to the problem of}} <b>learning</b> multiple <b>matrices</b> and general spectral regularization. Empirically we show that we can recover a 10, 000 × 10, 000 matrix from 1. 2 million observations in about 5 minutes. Furthermore, we show that in a brain-computer interface problem, the proposed method can speed-up the optimization by two orders of magnitude against the conventional projected gradient method and produces more reliable solutions. 1...|$|R
50|$|The {{low-dimensional}} embedding {{is finally}} obtained by application of multidimensional scaling on the <b>learned</b> inner product <b>matrix.</b>|$|R
40|$|Developing an {{engineering}} student's awareness of sustainability through the embedding of sustainability curricula {{is widely considered}} to be essential to modernising chemical engineering degree programs. In this chapter, the chemical engineering program at James Cook University {{is used as a}} case study to illustrate the design and sequencing of embedded curricula associated with developing a students' awareness of sustainability. There are a wide range of examples of skills, techniques, and characteristics associated with developing this awareness. In this chapter, an approach is described whereby a set of generic and interdisciplinary capabilities are developed to provide a degree of flexibility in how sustainability is interpreted and taught. A cognitive <b>learning</b> <b>matrix</b> is utilised as a design tool that facilitates determination of new subject learning outcomes aligned with the sustainability capabilities. A variety of curriculum examples are introduced and described...|$|E
40|$|We {{consider}} the minimization of a smooth loss function regularized by the trace norm {{of the matrix}} variable. Such formulation finds applications in many machine learning tasks including multi-task <b>learning,</b> <b>matrix</b> classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O (1 √), where k is the k iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O (1 k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O (1 k 2) for smooth problems. Experiments on multi-task learning problems demonstrate {{the efficiency of the}} proposed algorithms. 1...|$|E
40|$|We {{describe}} {{a way of}} <b>learning</b> <b>matrix</b> representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as (2, 5) ∈ + 3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3,+ 3) ∈ plus and (+ 3, − 3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations + 3 or has wife {{even though it has}} not been trained on any first-order examples involving these relations. ...|$|E
5000|$|In {{the case}} of {{multitask}} learning, [...] problems are considered simultaneously, each related in some way. The goal is to learn [...] functions, ideally borrowing strength from the relatedness of tasks, that have predictive power. This is equivalent to <b>learning</b> the <b>matrix</b> [...]|$|R
40|$|The {{choice of}} kernel is {{essential}} in kernel-based leaning methods. One way to choose a suitable kernel is cross-validation, {{but there is an}} alternative solution, that is, learning the kernel from data. In this project, two papers about <b>learning</b> the kernel <b>matrix</b> and kernel function from data are surveyed and compared. The method of <b>learning</b> the kernel <b>matrix</b> is implemented, and empirical results are reported and discussed. 1...|$|R
40|$|The paper {{addresses}} {{the problem of}} learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The {{focus is on the}} nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the <b>learned</b> <b>matrix.</b> The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks. Peer reviewe...|$|R
40|$|There are {{numerous}} {{calls in the}} literature for research into the flipped learning approach to match the flood of popular media articles praising its impact on student learning and educational outcomes. This paper addresses those calls by proposing pedagogical strategies that promote active learning in &# 039;flipped&# 039; approaches and improved practice of educational design in ICT supported learning environments. This paper makes two main contributions. It situates flipped learning strategies within a pedagogical framework, thus linking them to higher-level pedagogy. Additionally, it proposes an approach to both analysing and designing flipped learning strategies. This exploratory approach provides a guide for educators to map how their tactics fit amongst other instances of flipped learning. Examples of flipped learning approaches are provided to illustrate this mapping. The Flipped <b>Learning</b> <b>Matrix,</b> developed by this SoTL inquiry, is a tool for both critical reflection of existing approaches and course design, empowering the educator to design their own flipped version that is pedagogically sound and fit for purpose...|$|E
40|$|The {{objectives}} of the study are (1) to produce student worksheets based on APOS Theory for Math lessons in Class X Senior High School which {{is in line with}} the 2013 Curriculum; (2) to evaluate the effectivity of the student worksheets based on APOS Theory, in terms of the activity and result of <b>learning</b> <b>Matrix.</b> The results of the study are (1) the development of the worksheets (according to APOS Theory) were carried out through five stages, namely, analysis, design, development, implementation and evaluation; (2) the assessments performed by media experts, material specialists and subject teachers showed that the worksheets developed are feasible and could be used for students of Class X Senior High School; (3) the responses given by the students towards the worksheets were good; (4) the effectivity of the learning was elevated; it was shown by the increased learning activities of the students and the higher average score of the test (87. 14), compared to the minimum criterion of completeness of 75...|$|E
40|$|We {{show that}} the series of all walks between any two {{vertices}} of any (possibly weighted) directed graph G is given by a universal continued fraction of finite depth and breadth involving the simple paths and simple cycles of G. A simple path is a walk forbidden to visit any vertex more than once. We obtain an explicit formula giving this continued fraction. Our results are based on an equivalent to the fundamental theorem of arithmetic: we demonstrate that arbitrary walks on G factorize uniquely into nesting products of simple paths and simple cycles, where nesting is a product operation between walks that we define. We {{show that the}} simple paths and simple cycles are the prime elements of the set of all walks on G equipped with the nesting product. We give an algorithm producing the prime factorization of individual walks, and obtain a recursive formula producing the prime factorization of sets of walks. Our results have already found applications in machine <b>learning,</b> <b>matrix</b> computations and quantum mechanics. Comment: Updated with links between nesting and loop-erasing. Still under review (!...|$|E
5000|$|Neo {{is offered}} {{a choice to}} remain in his {{everyday}} life and forget about the <b>Matrix</b> or to <b>learn</b> what the <b>Matrix</b> really is. Choosing to <b>learn</b> what the <b>Matrix</b> really is, he takes a drug (commonly called the [...] "red pill") designed to disrupt his body's neural connection to the Matrix, and wakes up disoriented and alarmed to find himself weak, hairless, naked in a pod full of red liquid, and connected to many wires.|$|R
40|$|In {{the present}} paper we {{investigate}} the application of differentiable kernel for generalized <b>matrix</b> <b>learning</b> vector quantization as an alternative kernel-based classifier, which additionally provides classification dependent data visualization. We show {{that the concept of}} differentiable kernels allows a prototype description in the data space but equipped with the kernel metric. Moreover, using the visualization properties of the original <b>matrix</b> <b>learning</b> vector quantization we are able to optimize the class visualization by inherent visualization mapping learning also in this new kernel-metric data space. ...|$|R
40|$|Most of {{existing}} cross-modal retrieval approaches only exploit labeled data to train coupled projection matrices for supporting retrieval tasks across heterogeneous modalities. However, the valuable information involved in unlabeled data is unfortunately ignored. In this paper, we propose a novel Semi-Supervised Distance Consistent method (SSDC) {{to solve the}} problem. Our approach firstly models the initial correlation between different modalities by constructing the pseudo label and corresponding data of unlabeled query. Then our method <b>learns</b> projection <b>matrices</b> by adaptively optimizing the pseudo label of unlabeled data. In this way, SSDC could <b>learn</b> discriminative projection <b>matrices.</b> Experimental results on two publicly available datasets demonstrate the superior performance of the proposed approach. © 2017 ACM...|$|R
