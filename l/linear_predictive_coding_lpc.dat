222|1978|Public
2500|$|The psychoacoustic masking codec {{was first}} {{proposed}} in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, New Jersey, and M. A. Krasner {{both in the}} United States. [...] Krasner {{was the first to}} publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report, did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> gain present in speech.|$|E
5000|$|<b>Linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> {{is used to}} {{synthesize}} the speech from a residual waveform. The LPC parameters are encoded as Line spectral pairs (LSP).|$|E
50|$|APC {{is related}} to <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> in that both use {{adaptive}} predictors. However, APC uses fewer prediction coefficients, thus requiring a higher sampling rate than LPC.|$|E
40|$|Experimental {{results of}} the {{quantization}} of <b>Linear</b> <b>Predictive</b> <b>Coded</b> (<b>LPC)</b> coefficients using two general approaches, scalar coefficient quantization and vector quantization, are presented. The LPC coefficients were quantized in several domains: Line Spectral Frequency (LSF), cepstral, predictor, reflection and autocorrelation. Two distortion measures were {{used to evaluate the}} quantizers; Itakura-Saito and RMS log spectral distortion measure. The vector quantizers showed good results for only 9 bits per frame of 150 speech samples. 1...|$|R
40|$|Abstract—For many {{recognition}} systems, {{the feature}} extraction unit forms the most computationally intensive and power consuming component. In this paper, {{we present a}} design of an analog-to-information converter that directly produces a pulse-encoded representation of <b>linear</b> <b>predictive</b> <b>coded</b> (<b>LPC)</b> features corresponding to an input analog signal. At the core of proposed design is a sigma-delta modulation procedure that is embedded within a learning step. Measured results from a fabricated prototype in a 0. 5 µm CMOS technology demonstrate the real-time functionality of the learner in extracting 6 -dimensional online LPC features from input speech signal while consuming only 450 µW. I...|$|R
40|$|This thesis was {{developed}} {{out of a}} need to reduce {{the time required to}} correct <b>Linear</b> <b>Predictive</b> <b>Code</b> (<b>LPC)</b> data used for training a formant tracker. A program was written to select peaks from LPC data and interpret them as Fl, F 2, and F 3, using knowledge about the phonetic transcription, the sex of the speaker, knowledge about individual phonemes, and a few heuristics. The system was tested on a database of eight speakers, four male and four female, each of whom produced ten sentences. This data set comprised 1, 011 resonant phonemes covering 17, 363 5 -msec. frames. Overall the system correctly matched Fl in 98. 9 % of the frames, F 2 in 92. 2 % of the frames, and F 3 in 88. 8 % of the frames...|$|R
50|$|In {{digital signal}} processing, linear {{prediction}} {{is often called}} <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> and can thus {{be viewed as a}} subset of filter theory. In system analysis (a subfield of mathematics), linear prediction {{can be viewed as a}} part of mathematical modelling or optimization.|$|E
50|$|The display was {{a vacuum}} {{fluorescent}} display (VFD). Speak & Read units used a membrane keyboard for input. The Speak & Read used a single-chip voice synthesizer, the TI TMC0280 (later called the TMS5100), {{that was also}} used in the Speak & Spell. The TMS5100 used a 10th-order <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> model and the electronic DSP logic.|$|E
5000|$|The Speak & Spell {{used the}} first single-chip voice synthesizer, the TMC0280, later called the TI TMS5100, which {{utilized}} a 10th-order <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> model by using pipelined electronic DSP logic. [...] A variant of this chip {{with a very}} similar voice would eventually be utilized in certain Chrysler vehicles in the 1980s as the Electronic Voice Alert.|$|E
50|$|Warped <b>linear</b> <b>predictive</b> <b>coding</b> (warped <b>LPC</b> or WLPC) is {{a variant}} of <b>linear</b> <b>predictive</b> <b>coding</b> in which the {{spectral}} representation {{of the system is}} modified, for example by replacing the unit delays used in an LPC implementation with first-order allpass filters. This can have advantages in reducing the bitrate required for a given level of perceived audio quality/intelligibility, especially in wideband audio coding.|$|R
40|$|Abstract—A set of Artificial Neural Network (ANN) based {{methods for}} the design of an {{effective}} system of speech recognition of numerals of Assamese language captured under varied recording conditions and moods is presented here. The work is related to the formulation of several ANN models configured to use <b>Linear</b> <b>Predictive</b> <b>Code</b> (<b>LPC),</b> Principal Component Analysis (PCA) and other features to tackle mood and gender variations uttering numbers as part of an Automatic Speech Recognition (ASR) system in Assamese. The ANN models are designed using a combination of Self Organizing Map (SOM) and Multi Layer Perceptron (MLP) constituting a Learning Vector Quantization (LVQ) block trained in a cooperative environment to handle male and female speech samples of numerals of Assamese- a language spoken by a sizable population in the North-Eastern part of India. The work provides a comparative evaluation of several such combinations while subjected to handle speech samples with gender based differences captured by a microphone in four different conditions viz. noiseless, noise mixed, stressed and stress-free...|$|R
40|$|Iii analysis-by-{{synthesis}} <b>linear</b> <b>predictive</b> <b>coding</b> (AbS-LPC) an <b>LPC</b> synthesis filter {{is combined}} with an analysis-by-synthesis search of the excitation signal. The synthesis filter is an estimator for the speech signal given the excitation. However in most AbS-LPC algorithms this estimator has no explicit model of the quantization noise, which {{is present in the}} excitation signal. This paper describes quantization noise modeling in a vector AbS-LPC algorithm. Methods based on recursive Bayesian filtering and Kalman filtering are considered. Simulations indicate improved signal-to-noise ratios due to quantization noise modeling...|$|R
50|$|NVP {{was used}} to send speech between {{distributed}} sites on the ARPANET using several different voice-encoding techniques, including <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> and continuously variable slope delta modulation (CVSD). Cooperating researchers included Steve Casner, Randy Cole, and Paul Raveling (ISI); Jim Forgie (Lincoln Laboratory); Mike McCammon (Culler-Harrison); John Markel (Speech Communications Research Laboratory); and John Makhoul (Bolt, Beranek and Newman).|$|E
50|$|Skype Limited {{announced}} that SILK {{can use a}} sampling frequency of 8, 12, 16 or 24 kHz and a bit rate from 6 to 40 kbit/s. It can also use a low algorithmic delay of 25 ms (20 ms frame size + 5 ms look-ahead). The reference implementation is written in the C programming language. The codec technology is based on <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC).</b> The SILK binary SDK is available.|$|E
50|$|HVXC uses <b>Linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> with block-wise {{adaptation}} every 20ms. The LPC {{parameters are}} transformed to Line spectral pair (LSP) coefficients, which are jointly quantized. The LPC residual signal {{is classified as}} either voiced or unvoiced. In the case of voiced speech, the residual is coded in a parametric representation (operating as a vocoder), while {{in the case of}} unvoiced speech, the residual waveform is quantized (thus operating as hybrid speech codec).|$|E
40|$|Copyright © 2014 Veton Z. Këpuska et al. This is an {{open access}} article {{distributed}} under the Creative Com-mons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, pro-vided the original work is properly cited. In accordance of the Creative Commons Attribution License all Copy-rights © 2014 are reserved for SCIRP {{and the owner of}} the intellectual property Veton Z. Këpuska et al. All Copyright © 2014 are guarded by law and by SCIRP as a guardian. Wake-Up-Word Speech Recognition task (WUW-SR) is a computationally very demand, particu-larly the stage of feature extraction which is decoded with corresponding Hidden Markov Models (HMMs) in the back-end stage of the WUW-SR. The state of the art WUW-SR system is based on three different sets of features: Mel-Frequency Cepstral Coefficients (MFCC), <b>Linear</b> <b>Predictive</b> <b>Coding</b> Coefficients (<b>LPC),</b> and Enhanced Mel-Frequency Cepstral Coefficients (ENH_MFCC). In (front-end of Wake-Up-Word Speech Recognition System Design on FPGA) [1], we presented an experimental FPGA design and implementation of a novel architecture of a real-time spectrogram extraction processor that generates MFCC, LPC, and ENH_MFCC spectrograms simultaneously. In this paper, the details of converting the three sets of spectrograms 1) Mel-Frequency Cepstra...|$|R
40|$|AbstractSpeech coding {{has been}} {{major issue in}} the area of digital speech processing. Speech coding is the process of {{transforming}} the speech signal in a more compressed form, which can then be transmitted with few numbers of binary digits. It is not possible to access unlimited bandwidth of a channel each time we send a signal across it which leads to code and compress speech signals. Speech compression is applied in long distance communication, high-class speech storage, and message encryption. Speech coding is a lossy type of coding and hence the output signal does not exactly sound like the input. Speech coding techniques discussed here are <b>Linear</b> <b>predictive</b> <b>coding,</b> waveform <b>coding,</b> <b>Code</b> excited <b>linear</b> <b>predictive</b> <b>coding,</b> etc. <b>Linear</b> <b>Predictive</b> <b>Coding</b> and <b>Code</b> Excited <b>Linear</b> <b>Predictive</b> <b>Coding</b> techniques are studied with the help of MATLAB to check their performance measures like compression ratio and speech audible quality...|$|R
5000|$|LPC-10, FIPS Pub 137, 2400 bit/s, {{which uses}} <b>linear</b> <b>predictive</b> <b>coding</b> ...|$|R
50|$|<b>Linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> {{is a tool}} used {{mostly in}} audio signal {{processing}} and speech processing for representing the spectral envelope of a digital signal of speech in compressed form, using the information of a linear predictive model. It {{is one of the}} most powerful speech analysis techniques, and one of the most useful methods for encoding good quality speech at a low bit rate and provides extremely accurate estimates of speech parameters.|$|E
50|$|Other {{types of}} lossy compressors, {{such as the}} <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> used with speech, are source-based coders. These coders use {{a model of the}} sound's {{generator}} (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may {{be thought of as a}} basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.|$|E
50|$|In 1978, Texas Instruments {{introduced}} the first single-chip LPC speech synthesizer. In 1976 TI began a feasibility study memory intensive applications for bubble memory then being developed. They soon focused on speech applications. This {{resulted in the}} development the TMC0280 one-chip <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> speech synthesizer {{which was the first}} time a single silicon chip had electronically replicated the human voice. This was used in several TI commercial products beginning with Speak & Spell which was introduced at the Summer Consumer Electronics Show in June 1978. In 2001 TI left the speech synthesis business, selling it to Sensory Inc. of Santa Clara, California.|$|E
50|$|Speech data {{is stored}} through pitch-excited <b>linear</b> <b>predictive</b> <b>coding</b> (PE-LPC), where words {{are created by}} a lattice filter, selectably fed by either an {{excitation}} ROM (containing a glottal pulse waveform) or an LFSR (linear feedback shift register) noise generator. <b>Linear</b> <b>predictive</b> <b>coding</b> achieves a vast reduction in data volume needed to recreate intelligible speech data.|$|R
50|$|Bishnu S. Atal (born 1933) is a noted {{researcher}} in <b>linear</b> <b>predictive</b> <b>coding.</b>|$|R
30|$|Mel {{frequency}} cepstral coefficients and <b>linear</b> <b>predictive</b> <b>coding</b> coefficients are computed {{using the}} following methods.|$|R
50|$|The codec was {{developed}} by David Rowe (Amateur Radio Call-Sign VK5DGR), with support and cooperation of other researchers (e.g., Jean-Marc Valin from Speex). Codec 2 uses sinusoidal coding to model speech. In sinusoidal coding, spoken audio is recreated by modelling speech as a sum of harmonically related sine waves with independent amplitudes called Line spectral pairs, or LSP. The fundamental frequency of the speaker's voice (pitch) and the amplitude (energy) of the harmonics is encoded, and with the LSP's are exchanged across a channel in a digital format. The LSP coefficients represent the <b>Linear</b> <b>Predictive</b> <b>Coding</b> (<b>LPC)</b> model in the frequency domain, and lend themselves to a robust and efficient quantisation of the LPC parameters.|$|E
50|$|The psychoacoustic masking codec {{was first}} {{proposed}} in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, New Jersey, and M. A. Krasner {{both in the}} United States. Krasner {{was the first to}} publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report, and did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> gain present in speech.|$|E
50|$|GSM {{has used}} a variety of voice codecs to squeeze 3.1 kHz audio into between 6.5 and 13 kbit/s. Originally, two codecs, named after the types of data channel they were allocated, were used, called Half Rate (6.5 kbit/s) and Full Rate (13 kbit/s). These used a system based on <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC).</b> In {{addition}} to being efficient with bitrates, these codecs also made it easier to identify more important parts of the audio, allowing the air interface layer to prioritize and better protect these parts of the signal. GSM was further enhanced in 1997 with the Enhanced Full Rate (EFR) codec, a 12.2 kbit/s codec that uses a full-rate channel. Finally, with the development of UMTS, EFR was refactored into a variable-rate codec called AMR-Narrowband, which is high quality and robust against interference when used on full-rate channels, or less robust but still relatively high quality when used in good radio conditions on half-rate channel.|$|E
50|$|General Instrument SP0250, <b>LPC</b> (<b>linear</b> <b>predictive</b> <b>coding)</b> speech {{synthesis}} chip {{used in the}} Sega G80 arcade system board.|$|R
40|$|Ankara : Department of Electrical and Electronics Engineering and the Institute of Engineering and Sciences of Bilkent University, 1992. Thesis (Master's) [...] Bilkent University, 1992. Includes bibliographical {{references}} leaves 30 - 32. Low {{bit rate}} speech coding techniques {{and a new}} coding scheme for vocal tract parameters are presented. Linear prediction based voice <b>coding</b> techniques (<b>linear</b> <b>predictive</b> <b>coding</b> and <b>code</b> excited <b>linear</b> <b>predictive</b> <b>coding)</b> are examined and implemented. A new interframe differential coding scheme for line spectrum pairs is developed. The new scheme reduces the spectral distortion of the <b>linear</b> <b>predictive</b> filter while maintaining a high compression ratio. Erzin, EnginM. S...|$|R
5000|$|Here, [...] is the {{precision}} of random fluctuations at the i-th level. This is known as generalized <b>predictive</b> <b>coding</b> 11, with <b>linear</b> <b>predictive</b> <b>coding</b> as a special case.|$|R
50|$|The Opus {{format is}} based on a {{combination}} of the full-bandwidth CELT format and the speech-oriented SILK format, both heavily modified: CELT {{is based on}} the MDCT that most music codecs use, using CELP techniques in the frequency domain for better prediction, while SILK uses <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> and an optional Long-Term Prediction filter to model speech. In Opus, both were modified to support more frame sizes, as well as further algorithmic improvements and integration, such as using CELT's range encoder for both types. To minimize packet overhead at low bitrates, if latency is not as pressing, SILK has support for packing multiple 20 ms frames together, sharing context and headers; SILK also allows Low Bit-Rate Redundancy (LBRR) frames, allowing low-quality packet loss recovery. CELT includes both spectral replication and noise generation, similar to AAC's SBR and PNS, and can further save bits by filtering out all harmonics of tonal sounds entirely, then replicating them in the decoder. Better tone detection is an ongoing project to improve quality.|$|E
40|$|Abstract—In this letter, we {{determine}} empirical lower bounds on the bitrate {{required to}} transparently code <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> parameters derived from wideband speech. This is achieved via extrapolation {{of the operating}} distortion-rate curve of an unconstrained vector quantizer that is trained using artificial vectors generated by a Gaussian mixture model. Memoryless coding is considered and two competing LPC parameter representations are investigated. Our results show a lower bound of 31 bits/frame when assuming high-rate linearity in the operating distortion-rate curve and 35 bits/frame for an exponential curve. We also evaluate a recent quantization scheme and compare its performance against this lower bound. Index Terms—Immittance spectral pairs, line spectral frequencies (LSFs), <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> parameters, wideband speech. I...|$|E
30|$|Generally, the HOS {{of speech}} are nonzero and {{sufficiently}} distinct {{from those of}} the Gaussian noise. Moreover, it is reported by Nemer et al. [19] that the skewness and kurtosis of the <b>linear</b> <b>predictive</b> <b>coding</b> (<b>LPC)</b> residual of the steady voiced speech can discriminate the speech from noise more effective.|$|E
50|$|The Echo II {{card was}} a speech {{synthesis}} card utilizing <b>linear</b> <b>predictive</b> <b>coding</b> technology, as embodied by the TMS 5220 speech chip.|$|R
40|$|International Telemetering Conference Proceedings / November 14 - 16, 1978 / Hyatt House Hotel, Los Angeles, CaliforniaAn {{overview}} of recent applications of source coding theory and techniques to <b>Linear</b> <b>Predictive</b> <b>Coded</b> Coded speech compression systems is presented. Several distortion measures proposed {{for use in}} speech compression systems are described and compared. These distortion measures are then combined with an algorithm for computing "optimum" (minimum distortion) vector quantizers to obtain optimum quantizers for reflection coefficient vectors in <b>Linear</b> <b>Predictive</b> <b>Coded</b> speech systems. The quality {{of the system is}} evaluated via the speech distortion measures and listening to demonstration tapes. Some implications for speech compression theory and practice are discussed...|$|R
40|$|This work is {{concerned}} with the investigation of algorithms and architectures for computer recognition of human speech. Three speech recognition algorithms have been implemented, using (a) Walsh Analysis, (b) Fourier Analysis and (c) <b>Linear</b> <b>Predictive</b> <b>Coding.</b> The Fourier Analysis algorithm made use of the Prime-number Fourier Transform technique. The <b>Linear</b> <b>Predictive</b> <b>Coding</b> algorithm made use of LeRoux and Gueguen's method for calculating the coefficients. The system was organised so that the speech samples could be input to a PC/XT microcomputer in a typical office environment. The PC/XT was linked via Ethernet to a Sun 2 / 180 s computer system which allowed the data to be stored on a Winchester disk so that the data used for testing each algorithm was identical. The recognition algorithms were implemented entirely in Pascal, to allow evaluation to take place on several different machines. The effectiveness of the algorithms was tested with a group of five naive speakers, results being in the form of recognition scores. The results showed the superiority of the <b>Linear</b> <b>Predictive</b> <b>Coding</b> algorithm, which achieved a mean recognition score of 93. 3...|$|R
