638|680|Public
25|$|Single index models {{allow some}} degree of {{nonlinearity}} {{in the relationship between}} x and y, while preserving the central role of the <b>linear</b> <b>predictor</b> β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.|$|E
25|$|In linear regression, the {{relationships}} are modeled using <b>linear</b> <b>predictor</b> functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of y given {{the value of}} X {{is assumed to be}} an affine function of X; less commonly, the median or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.|$|E
2500|$|A concept called [...] "linear sufficiency" [...] can be {{formulated}} in a Bayesian context, and more generally. First define the best <b>linear</b> <b>predictor</b> of a vector Y based on X as [...] Then a linear statistic T(x) is linear sufficient if ...|$|E
30|$|In this section, we {{investigate}} the admissibility of simultaneous prediction in class of nonhomogeneous <b>linear</b> <b>predictors,</b> and we obtain the necessary and sufficient conditions. Studies show the admissibility of simultaneous prediction {{in the class}} of nonhomogeneous <b>linear</b> <b>predictors</b> is based on the admissibility of simultaneous prediction {{in the class of}} homogeneous <b>linear</b> <b>predictors.</b>|$|R
5000|$|Let [...] be {{the vector}} ofall the <b>linear</b> <b>predictors.</b> (For {{convenience}} we always let [...] be of dimension M).Thus all the covariates comprising [...] potentially affect all the parameters through the <b>linear</b> <b>predictors</b> [...] Later, we {{will allow the}} <b>linear</b> <b>predictors</b> to be generalized to additive predictors, which {{is the sum of}} smooth functions of each [...] and each function is estimated from the data.|$|R
40|$|Abstract. Although {{fast and}} reliable, {{real-time}} template tracking using <b>linear</b> <b>predictors</b> requires a long training time. The {{lack of the}} ability to learn new templates online prevents their use in applications that require fast learning. This especially holds for applications where the scene is not known a priori and multiple templates have to be added online. So far, <b>linear</b> <b>predictors</b> had to be either learned offline [1] or in an iterative manner by starting with a small sized template and growing it over time [2]. In this paper, we propose a fast and simple reformulation of the learning procedure that allows learning new <b>linear</b> <b>predictors</b> online. Key words: template tracking, template learning, <b>linear</b> <b>predictors...</b>|$|R
2500|$|Generalized {{linear models}} allow for an {{arbitrary}} link function g that relates {{the mean of}} the response variable to the predictors, i.e. E(y) = g(β′x). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the [...] range of the <b>linear</b> <b>predictor</b> and the range of the response variable.|$|E
2500|$|Perhaps {{the menace}} {{best suited for}} the design {{specifications}} of such an automated artillery system appeared in June 1944. Not surprisingly it was another robot. The German Aeronautical Engineers aided by Wernher von Braun produced a robot of their own; the V-1 flying bomb, an automatically guided bomb and widely considered a precursor of the cruise missile. Its flight specifications almost perfectly suited the target design criteria of Director T-10, that of an aircraft flying straight and level at constant velocity, in other words a target nicely fitting the computing capabilities of a <b>linear</b> <b>predictor</b> model such as the Director T-10. Although the Germans did have a trick up their engineering sleeve by making the bomb fly fast and low to evade radar, a technique widely adopted even today. During the London Blitz one hundred Director T-10 assisted 90mm automated gun units were {{set up in a}} perimeter south of London, at the special request of Winston Churchill. The AA units included the SCR-584 radar unit produced by the Radiation Lab at MIT and the proximity fuse mechanism, developed by Merle Tuve and his special Division T at NDRC, that detonated near the target using a microwave controlled fuse called the VT or variable time fuse, enabling a larger detonation reach envelope and increasing the chances of a successful outcome. Between June 18 and July 17, 1944, 343 V-1 bombs were shot down or 10% of the total V-1 number sent by the Germans and about 20% of the total V-1 bombs shot down. From July 17 to August 31 the automated gun kills rose to 1286 V-1 rockets or 34% [...] of the total V-1 number dispatched from Germany and 50% of the V-1 actually shot down over London. From these statistics {{it can be seen that}} the automated systems that Bode helped design had a considerable impact on crucial battles of World War II. It can also be seen that London at the time of the Blitz became, among other things, the original robot battlefield.|$|E
5000|$|Here, {{instead of}} writing the logit of the probabilities pi as a <b>linear</b> <b>predictor,</b> we {{separate}} the <b>linear</b> <b>predictor</b> into two, {{one for each}} of the two outcomes: ...|$|E
40|$|Abstract—Enlarging or {{reducing}} the template size by adding new parts, or removing {{parts of the}} template according to their suitability for tracking requires the {{ability to deal with}} the variation of the template size. For instance, real-time template tracking using <b>linear</b> <b>predictors,</b> although fast and reliable, requires using templates of a fixed size and does not allow on-line modification of the predictor. To solve this problem we propose the Adaptive <b>Linear</b> <b>Predictors</b> (ALPs), which enable fast online modifications of pre-learned <b>linear</b> <b>predictors.</b> Instead of applying a full matrix inversion for every modification of the template shape, as standard approaches to learning <b>linear</b> <b>predictors</b> do, we just perform a fast update of this inverse. This allows us to learn the ALPs in a much shorter time than standard learning approaches, while performing equally well. Additionally, we propose a multi-layer approach to detect occlusions and use ALPs to effectively handle them. This allows us to track large templates and modify them according to the present occlusions. We performed exhaustive evaluation of our approach and compared it to standard <b>linear</b> <b>predictors</b> and other state of the art approaches. Index Terms—Template tracking, <b>linear</b> <b>predictors.</b> F...|$|R
40|$|Enlarging or {{reducing}} the template size by adding new parts, or removing {{parts of the}} template, according to their suitability for tracking, requires the {{ability to deal with}} the variation of the template size. For instance, real-time template tracking using <b>linear</b> <b>predictors,</b> although fast and reliable, requires using templates of fixed size and does not allow on-line modification of the predictor. To solve this problem we propose the Adaptive <b>Linear</b> <b>Predictors</b> (ALPs) which enable fast online modifications of pre-learned <b>linear</b> <b>predictors.</b> Instead of applying a full matrix inversion for every modification of the template shape as standard approaches to learning <b>linear</b> <b>predictors</b> do, we just perform a fast update of this inverse. This allows us to learn the ALPs in a much shorter time than standard learning approaches while performing equally well. We performed exhaustive evaluation of our approach and compared it to standard <b>linear</b> <b>predictors</b> and other state of the art approaches. 1...|$|R
5000|$|... 2. <b>Linear</b> <b>predictors</b> [...] {{described}} below to model each parameter , ...|$|R
5000|$|The <b>linear</b> <b>predictor</b> is the {{quantity}} which incorporates {{the information about}} the independent variables into the model. The symbol &eta; (Greek [...] "eta") denotes a <b>linear</b> <b>predictor.</b> It is related to the expected value of the data (thus, [...] "predictor") through the link function.|$|E
5000|$|Computationally, GLAM {{provides}} array algorithms {{to calculate}} the <b>linear</b> <b>predictor,</b> ...|$|E
5000|$|This {{makes it}} {{possible}} to write the <b>linear</b> <b>predictor</b> function as follows: ...|$|E
30|$|In this section, we derive the {{necessary}} and sufficient {{conditions for the}} admissibility of simultaneous prediction in class of the homogeneous <b>linear</b> <b>predictors.</b> The best <b>linear</b> unbiased <b>predictor</b> of δ is obtained. Examples are presented to give some admissible predictors.|$|R
40|$|It has {{recently}} been shown {{that the use of}} non-shared vector <b>linear</b> <b>predictors</b> can improve recognition rates compared to standard HMMs [13]. Vector <b>linear</b> <b>predictors</b> adjust the probability of the current observation based upon previous or following observations at certain offsets in time, improving the HMMs ability to model speech data. This thesis develops the theory and implementation of arbitrarily shared vector linear prediction for hidden Markov models in the area of speech recognition. B...|$|R
5000|$|For {{data with}} [...] i.i.d observations, setting , [...] and , the {{approximated}} <b>linear</b> <b>predictors</b> {{can be represented}} as [...] which {{are related to the}} means through [...]|$|R
5000|$|Link {{function}} [...] {{connecting the}} conditional mean and the <b>linear</b> <b>predictor</b> through [...]|$|E
5000|$|... can be {{formulated}} as {{requiring that}} the coefficients of a <b>linear</b> <b>predictor,</b> defined as ...|$|E
5000|$|Each <b>linear</b> <b>predictor</b> is a {{quantity}} which incorporatesinformation about {{the independent variables}} into the model. The symbol [...] (Greek [...] "eta") denotes a <b>linear</b> <b>predictor</b> and a subscript j is used to denote the jth one. It relates the jth parameter to the explanatory variables, and is expressed as linear combinations (thus, [...] "linear") of unknown parameters i.e., of regression coefficients [...]|$|E
40|$|This paper {{presents}} {{a class of}} <b>linear</b> <b>predictors</b> for nonlinear controlled dynamical systems. The basic idea is to lift (or embed) the nonlinear dynamics into a higher dimensional space where its evolution is approximately linear. In an uncontrolled setting, this procedure amounts to numerical approximations of the Koopman operator associated to the nonlinear dynamics. In this work, we extend the Koopman operator to controlled dynamical systems and apply the Extended Dynamic Mode Decomposition (EDMD) to compute a finite-dimensional approximation of the operator {{in such a way}} that this approximation has the form a linear controlled dynamical system. In numerical examples, the <b>linear</b> <b>predictors</b> obtained in this way exhibit a performance superior to existing <b>linear</b> <b>predictors</b> such as those based on local linearization or the so-called Carleman linearization. Importantly, the procedure to construct these <b>linear</b> <b>predictors</b> is completely data-driven and extremely simple [...] it boils down to a nonlinear transformation of the data (the lifting) and a linear least squares problem in the lifted space that can be readily solved for large data sets. These <b>linear</b> <b>predictors</b> can be readily used to design controllers for the nonlinear dynamical system using linear controller design methodologies. We focus in particular on model predictive control (MPC) and show that MPC controllers designed in this way enjoy computational complexity of the underlying optimization problem comparable to that of MPC for a linear dynamical system with the same number of control inputs and the same dimension of the state-space. Importantly, linear inequality constraints on the state and control inputs as well as nonlinear constraints on the state can be imposed in a linear fashion in the proposed MPC scheme. Similarly, cost functions nonlinear in the state variable can be handled in a linear fashion...|$|R
5000|$|LinUCB (Upper Confidence Bound) algorithm: {{the authors}} assume a linear {{dependency}} between the expected reward {{of an action}} and its context and model the representation space using a set of <b>linear</b> <b>predictors.</b>|$|R
40|$|We {{show that}} the {{simultaneous}} estimation of keypoint identities and poses is more reliable than the two separate steps undertaken by previous approaches. A simple linear classifier coupled with <b>linear</b> <b>predictors</b> trained during a learning phase appears to be sufficient for this task. The retrieved poses are subpixel accurate due to the <b>linear</b> <b>predictors.</b> We demonstrate the advantages of our approach on real-time 3 D object detection and tracking applications. Thanks to the high accuracy, one single keypoint is often enough to precisely estimate the object pose. As a result, we can deal in real-time with objects that are significantly less textured than the ones required by state-of-the-art methods. ...|$|R
5000|$|The {{basic idea}} of {{logistic}} regression {{is to use}} the mechanism already developed for linear regression by modeling the probability pi using a <b>linear</b> <b>predictor</b> function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials. The <b>linear</b> <b>predictor</b> function [...] for a particular data point i is written as: ...|$|E
5000|$|The <b>linear</b> <b>predictor,</b> LP (denoted η): where {X1,.. ,Xk} are regressor-variables (“affecting factors”) {{that deliver}} {{systematic}} variation to the response; ...|$|E
5000|$|Linear {{discriminant}} analysis (LDA) computes a <b>linear</b> <b>predictor</b> from {{two sets of}} normally distributed data to allow for classification of new observations.|$|E
40|$|Consider teh {{class of}} intinsically {{stationary}} spatial processes, which contains {{the class of}} second-order stationary processes. A measure of spatial dependence in the larger class is the variogram, from which optimal <b>linear</b> <b>predictors</b> can be constructed. For processes that are second-order stationary, these optimal <b>linear</b> <b>predictors</b> can also be {{expressed in terms of}} the covariogram. Traditionally, time-series forecasting has used the covariogram, but use of the variogram allows more general processes to be considered. These measures of spatial dependence are often unknown and have to be estimated from the data. In this article, we show that estimation of the variogram has important advantages over estimation of the covariogram...|$|R
40|$|We {{contrast}} {{two approaches}} {{to the prediction of}} latent variables in the model of factor analysis. The likelihood statistic is a sufficient statistic for the unobservables when sampling arises from the exponential family of distributions. <b>Linear</b> <b>predictors,</b> on the other hand, can be obtained as distribution-free statistics. We provide conditions under which a class of <b>linear</b> <b>predictors</b> is sufficient for the exponential family of distributions. We also examine various predictors {{in the light of the}} following criteria: (I) sufficiency, (ii) mean-square error, and (iii) unbiasedness and illustrate our results with the help of Chinese data on living standards. Latent variables, factor analysis, sufficiency, prediction, exponential family of distributions, living standards analysis...|$|R
40|$|Factor score {{predictors}} are computed when individual factor {{scores are}} of interest. Conditions {{for a perfect}} inter-correlation of the best <b>linear</b> factor score <b>predictor,</b> the best <b>linear</b> conditionally unbiased <b>predictor,</b> and the determinant best <b>linear</b> correlation-preserving <b>predictor</b> are presented. A transformation resulting in perfect correlations of the three predictors is proposed...|$|R
5000|$|The {{basic form}} of a <b>linear</b> <b>predictor</b> {{function}} [...] for data point i (consisting of p explanatory variables), for i = 1, ..., n, is ...|$|E
5000|$|... where E(Y) is the {{expected}} value of Y; X&beta; is the <b>linear</b> <b>predictor,</b> a linear combination of unknown parameters &beta;; g is the link function.|$|E
5000|$|The link {{function}} [...] is {{a smooth}} invertible function, that relates the conditional {{mean of the}} response [...] with the <b>linear</b> <b>predictor</b> [...] The relationship is given by [...]|$|E
40|$|R topics documented: phmm-package [...] . 2 AIC. phmm [...] . 3 cAIC [...] . 5 <b>linear.</b> <b>predictors</b> [...] . 7 loglik. cond [...] . 8 phmm [...] 8 phmm. cond. loglik [...] 11 plot. phmm [...] 11 pseudoPoisPHMM [...] . 12 traceHat [...] . 1...|$|R
3000|$|... [...]. In particular, {{determining}} the overall {{effects of an}} exposure variable on incidence density ratios is challenging especially when the <b>linear</b> <b>predictors</b> from both the mixing proportions and the Poisson mean model contain the exposure variable (Long et al. 2014).|$|R
40|$|The aim of {{this note}} is to study the {{properties}} of some nonstationary autoregressive-moving average (ARMA) processes that are considered important in real world situations. In particular, the covariance structure and <b>linear</b> <b>predictors</b> are obtained. Hilbert space linear prediction time-dependent coefficients...|$|R
