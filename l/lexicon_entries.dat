58|52|Public
50|$|An HPSG grammar {{includes}} {{principles and}} grammar rules and <b>lexicon</b> <b>entries</b> which are normally {{not considered to}} belong to a grammar. The formalism is based on lexicalism. This means that the lexicon {{is more than just a}} list of entries; it is in itself richly structured. Individual entries are marked with types. Types form a hierarchy. Early versions of the grammar were very lexicalized with few grammatical rules (schema). More recent research has tended to add more and richer rules, becoming more like construction grammar.|$|E
40|$|This paper {{discusses}} {{the problems that}} arise in NLP lexicons if German particle verbs are analyzed as words. I suggest analyzing them as phrasal constructions consisting of an adverb, an adjective, or a preposition and a verb. It is then not necessary to have separate <b>lexicon</b> <b>entries</b> for compositional particle verbs. Noncompositional particle verbs also do not need separate <b>lexicon</b> <b>entries</b> for tagging and parsing. They need multi-word <b>lexicon</b> <b>entries</b> for all those applications that operate on semantic representations...|$|E
40|$|Abstract. This project report {{presents}} the results of an approach in which synsets for Slovene wordnet were induced automatically from parallel corpora and already existing wordnets. First, multilingual lexicons were obtained from word-aligned corpora and compared to the wordnets in various languages in order to disambiguate <b>lexicon</b> <b>entries.</b> Then appropriate synset ids were attached to Slovene entries from the lexicon. In the end, Slovene <b>lexicon</b> <b>entries</b> sharing the same synset id were organized into a synset. The results were evaluated against a goldstandard and checked by hand...|$|E
5000|$|Brown, Driver, Briggs and Gesenius. Hebrew <b>Lexicon</b> <b>entry</b> for Dabaq. The Old Testament Hebrew Lexicon.|$|R
5000|$|<b>Lexicon</b> <b>entry</b> in Svenskt biografiskt handlexikon of 1906 (Swedish) Palbitzki, Mattias. In: Nordisk familjebok. Volume 20, Second Edition. Stockholm 1904-1926, p 1269 f. (Swedish) ...|$|R
5000|$|... where [...] ,and [...] Let {{there be}} an axiom for every symbol ,an axiom [...] for every {{production}} rule ,a <b>lexicon</b> <b>entry</b> [...] for every terminal symbol ,and Cut for the only rule.This categorial grammar generates the same language as the given CFG.|$|R
40|$|We {{introduce}} an Icelandic {{corpus of}} more than 250 million running words and de-scribe the methodology to build it. The re-source is available for use free of charge. We provide automatically generated mono-lingual <b>lexicon</b> <b>entries,</b> comprising fre-quency statistics, samples of usage, co-occurring words and a graphical represen-tation of the word’s semantic neighbour-hood. ...|$|E
40|$|The paper {{presents}} {{an experiment in}} which synsets for Slovene wordnet were induced automatically from several multilingual resources. Our research {{is based on the}} assumption that translations are a plausible source of semantically relevant information. More specifically, we argue that the translational relation on the one hand reduces ambiguity of a source word and on the other conveys semantic relatedness of a set of target words. We tried to identify sense distinctions of polysemous words and obtain sets of synonyms by first extracting multilingual lexicons from a word-aligned JRC-Acquis parallel corpus and then comparing them with the already existing wordnets in various languages. At this stage, <b>lexicon</b> <b>entries</b> were disambiguated and appropriate synset ids were assigned to their Slovene translation equivalents. Finally, the Slovene <b>lexicon</b> <b>entries</b> sharing the same assigned synset id were organized into a synset...|$|E
40|$|In {{this paper}} {{we present a}} {{proposal}} to help bypass the bottleneck of knowledge-based sys-tems working {{under the assumption that}} the knowledge sources are complete. We show how to create, on the fly, new <b>lexicon</b> <b>entries</b> us-ing lexico-semantic rules and how to create new concepts for unknown words, investigating a new linguistically-motivated model to trigger concepts in context. ...|$|E
40|$|Today 2 ̆ 7 s {{international}} community faces challenging but not insurmountable obstacles, as do {{you in the}} bipartite quiz that follows. To begin, utilize each of the definitions below to access an appropriate <b>lexicon</b> <b>entry</b> of indicated length. Then supplement your response with the given letter of the alphabet and anagrammatize the outcome into {{a member of the}} United Nations...|$|R
40|$|Word sense {{discrimination}} {{is the process}} of distinguishing the number of unique senses of a target word in a given corpus. This thesis approaches the task of word sense discrimination as an unsupervised clustering problem and uses semantic and syntactic features not seen in the current literature. Using the features from the clusters, this thesis constructs a new <b>lexicon</b> <b>entry</b> for the target word...|$|R
5000|$|Alpines <b>Lexicon.</b> Paul Preuss <b>entry</b> at http://www.bergsteigen.at/de/lexikon.aspx?ID=64 ...|$|R
40|$|Recognition {{using only}} visual {{evidence}} cannot always be successful due to limitations {{of information and}} resources available during training. Considering relation among <b>lexicon</b> <b>entries</b> is sometimes useful for decision making. In this paper, we present a method to capture lexical similarity of a lexicon and reliability of a character recognizer which serve to capture the dynamism of the environment. A parameter, lexical similarity, is defined by measuring these two factors as edit distance between <b>lexicon</b> <b>entries</b> and separability of each character's recognition results. Our experiments show that a utility function considering lexical similarity in a decision stage can enhance {{the performance of a}} conventional word recognizer. 1 Introduction Shapes of characters in handwriting vary widely. Some of the variation in the shpae of handwriting are categorized and methods to normalize the variations have been developed in previous literature [1]. However, still the variation of character sha [...] ...|$|E
40|$|We {{deal with}} the {{automated}} acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon. Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed <b>lexicon</b> <b>entries</b> by heuristic cognate mapping. We validated the emergent lexical translation hypotheses by determining the similarity of fixed-window context vectors {{on the basis of}} Portuguese and Spanish text corpora. ...|$|E
40|$|While {{working on}} valency lexicons for Czech and English, it was {{necessary}} to define treatment of multiword entities (MWEs) with the verb as the central lexical unit. Morphological, syntactic and semantic properties of such MWEs had to be formally specified in order to create <b>lexicon</b> <b>entries</b> and use them in treebank annotation. Such a formal specification has also been used for automated quality control of the annotation vs. the <b>lexicon</b> <b>entries.</b> We present a corpus-based study, concentrating on multilayer specification of verbal MWEs, their properties in Czech and English, and a comparison between the two languages using the parallel Czech-English Dependency Treebank (PCEDT). This comparison revealed interesting differences in the use of verbal MWEs in translation (discovering that such MWEs are actually rarely translated as MWEs, at least between Czech and English) as well as some inconsistencies in their annotation. Adding MWE-based checks should thus result in better quality control of future treebank/lexicon annotation. Since Czech and English are typologically different languages, we believe that our findings will also contribute {{to a better understanding of}} verbal MWEs and possibly their more unified treatment across languages. This work has been supported by the Grant No...|$|E
5000|$|... #Subtitle level 3: Dictionary, <b>Lexicon,</b> and Encyclopedia <b>Entries</b> ...|$|R
40|$|Abstract: There {{are many}} known Arabic lexicons {{organized}} on different ways, {{each of them}} has a different number of Arabic words according to its organization way. This paper has used mathematical relations to count a number of Arabic words, which proofs the number of Arabic words presented by Al Farahidy. The paper also presents new way to build an electronic Arabic lexicon by using a hash function that converts each word (as input) to correspond a unique integer number (as output), these integer numbers {{will be used as}} an index to a <b>lexicon</b> <b>entry...</b>|$|R
40|$|Lexicon schemas {{and their}} use are {{discussed}} in this paper {{from the perspective of}} lexicographers and field linguists. A variety of lex-icon schemas have been developed, with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and com-pared to each other in terms of conversion and usability for this particular user group, using a common <b>lexicon</b> <b>entry</b> and providing examples for each schema under consideration. The formats are assessed and the final recommendation is given for the potential users, namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards, lexicographers and field linguists. 1...|$|R
40|$|Here we {{describe}} an electronic lexical resource for German {{and the structure}} of its <b>lexicon</b> <b>entries,</b> notably the structure of verbal single-word and multi-word entries. The verb as the center of the sentence structure, as held by dependency models, is also a basic principle of the JAKOB narrative analysis application, for which the dictionary is the background. Different linguistic layers are combined for construing <b>lexicon</b> <b>entries</b> with a rich set of syntactic and semantic properties, suited to represent the syntactic and semantic behavior of verbal expressions (verb patterns), extracted from transcripts of real discourse, thereby lexicalizing the specific meaning of a specific verb pattern in a specific context. Verb patterns are built by the lexicographer by using a parser analyzing the input of a test clause and generating a machine-readable property string with syntactic characteristics and propositions for semantic characteristics grounded in an ontology. As an example, the German idiomatic expression an den Karren fahren (to come down hard on somebody) demonstrates the overall structure of a dictionary entry. The goal is to build unique dictionary entries (verb patterns) with reference to the whole of their properties...|$|E
40|$|We {{show that}} the {{usefulness}} of manually created dictionaries can be enhanced for a statistical machine translation system when new translations are automatically added which are simple morphological transformations (plural forms, different verb inflections) of the original. Further improvement is possible when assigning probabilities to the <b>lexicon</b> <b>entries.</b> We describe a method to do this {{on the basis of}} an automatically trained statistical lexicon. Experimental results are given for Chinese to English translation tasks and show a significant improvement in translation quality...|$|E
40|$|We {{present a}} {{bootstrapping}} algorithm {{to create a}} semantic lexicon {{from a list of}} seed words and a corpus that was mined from the web. We exploit extraction patterns to bootstrap the lexicon and use collocation statistics to dynamically score new <b>lexicon</b> <b>entries.</b> Extraction patterns are subsequently scored by calculating the conditional probability in relation to a non-related text corpus. We find that verbs that are highly domain related achieved the highest accuracy and collocation statistics affect the accuracy positively and negatively during the bootstrapping runs...|$|E
50|$|Most {{entries are}} in German. The Swiss <b>lexicon</b> also {{contains}} <b>entries</b> in French and Italian, and the Slovenska Biografija contains entries in Slovenian.|$|R
40|$|Abstract: There {{is always}} a lexicon in a {{language}} learner ’s mind, with the lexical factors be represented. This paper proposes a dual-mode system to present vocabulary in the lexicon with access to rules and exemplars, which aims at facilitating the process to the lexicon with accuracy, appropriateness and idiomaticity. Key words: mental <b>lexicon</b> lexical <b>entry</b> dual-mode system 1...|$|R
40|$|Treatment {{of meaning}} in NLP is greatly {{facilitated}} if semantic analysis and generation systems rely on a language-neutral, independently motivated world model, or ontology. However, {{the benefits of the}} ontology are somewhat offset in practice by the difficulty of its acquisition. This is why a number of computational linguists make a conscious choice to bypass ontology in their semantic deliberations. This decision is often justified by questioning the principles underlying ontologies and by challenging the ontology-based semantic enterprise on the grounds of its ostensible irreproducibility. In this paper we illustrate, on the example of the <b>lexicon</b> <b>entry</b> for the Spanish verb dejar, the expressive power of lexical-semantic descriptions based on the ontology used in the Mikrokosmos machine translation project. This expressive power is compared with that of some of the non-ontological approaches to lexical semantics. We argue that these approaches in reality rely on ontologies in ever [...] ...|$|R
40|$|This paper {{describes}} {{the process of}} building and using a new comprehensive lexicon of Czech verb valency frames based on complex valency frames. The main features of the <b>lexicon</b> <b>entries</b> are designed to bring important semantic information to computer processing of predicate constructions in running texts. The most notable features include two-level semantic labels with linkage to the Princeton and EuroWordNet hierarchy and surface verb frame patterns used for automatic syntactic analysis. Some implications for other languages, particularly English, Bulgarian and Romanian, are reported. ...|$|E
40|$|International audienceWe {{introduce}} a generic approach for transferring part-of-speech annotations from a resourced language to a non-resourced but etymologically close language. We first infer a bilingual lexicon {{between the two}} languages with methods based on character similarity, frequency similarity and context similarity. We then assign part-of-speech tags to these bilingual <b>lexicon</b> <b>entries</b> and annotate the remaining words {{on the basis of}} suffix analogy. We evaluate our approach on five language pairs of the Iberic peninsula, reaching up to 95 % of precision on the lexicon induction task and up to 85 % of tagging accuracy...|$|E
40|$|Abstract—The {{types of}} lexicons {{necessary}} for Transparent Intensional Logic (TIL) logical analysis will be described. We {{will show the}} algorithm for analysing the TIL verbal object as {{the core of a}} clause construction within the sentence analysis. Examples of verb frame analysis for Czech words will be presented. We also depict a way of enriching the <b>lexicon</b> <b>entries</b> as combinations of the descriptions of lexical units as they are developed within the area of lexical semantics (e. g. WordNet) with logical analysis of sentence meanings worked out within the Transparent Intensional Logic framework. Index Terms—TIL, logical analysis, verb frames...|$|E
40|$|We {{present an}} {{integrated}} approach to corpus and lexicon development, {{both for the}} language archive and a repository of materials for local community. We assume that the target audiences of the archive and the repository have different interests in the same underlying body of data, and we seek to construct that body of data {{in such a way}} that both sets of interests can be addressed. This involves integration of three pieces of software: ELAN, for work with digital video/audio and its transcription; FLEx, for grammatical analysis and lexicon development; and MannX, a browser-based video player for language learning. Integrating corpus and the lexicon means the following functionality: • Each <b>lexicon</b> <b>entry</b> has links to its tokens in the corpus, which are in turn linked, via time alignment, to the media segments in which the tokens occur. • Each word in the corpus has a link to its <b>lexicon</b> <b>entry.</b> To achieve this, the corpus and the lexicon must be integrated throughout their development: • The lexicon maintains the list of all lexical and grammatical morphemes. • When a morpheme that is already in the lexicon is encountered in the corpus, interlinear glosses are filled in from the lexicon. • When a new morpheme is encountered, a new entry is created. • A change in the lexical entry is automatically propagated through the corpus. We seek to achieve this kind of integration by building a software "bridge" between ELAN and FLEx that supports the following workflow: • Starting in ELAN, do transcription and time-alignment at the utterance level ("phrase" in FLEx). • Export to FLEx for lexical and morphological analysis. • Export the results back into ELAN as symbolic subdivision or symbolic association tiers. • Further annotate in ELAN; perhaps time-align at the word level. As of August 2012, a functioning version of software has been implemented in JavaScript. It is being reimplemented in Java as a Web application. We expect to put the Java version on the Web for testing in September, and also upgrade the ELAN-MannX conversion. This process results in a corpus of media files, associated annotation files, and a FLEx-created lexicon, with links between them. A subset of these materials, transformed into different formats, will form the basis of a community repository. This will be a Web application, running on a remote or localhost server, that can run on a laptop or on an Android phone...|$|R
50|$|The {{elements}} Lexical Resource, Global Information, <b>Lexicon,</b> Lexical <b>Entry,</b> Lemma, and Word Form {{define the}} structure of the lexicon. They are specified within the LMF document.On the contrary, languageCoding, language, partOfSpeech, commonNoun, writtenForm, grammaticalNumber, singular, plural are data categories that are taken from the Data Category Registry. These marks adorn the structure. The values ISO 639-3, clergyman, clergymen are plain character strings. The value eng is taken from the list of languages as defined by ISO 639-3.|$|R
40|$|Lexicon of Czech verbal {{multiword}} expressions (VMWEs) used in Parseme Shared Task 2017. [URL] Lexicon {{consists of}} 4785 VMWEs, categorized into four categories according to Parseme Shared Task (PST) typology: IReflV (inherently reflexive verbs), LVC (light verb constructions), ID (idiomatic expressions) and OTH (other VMWEs with other than verbal syntactic head). Verbal multiword expressions {{as well as}} deverbative variants of VMWEs were annotated during the preparation phase of PST. These data were published as [URL] Czech part includes 14, 536 VMWE occurences: 1611 ID 10000 IReflV 2923 LVC 2 OTH This lexicon was created out of Czech data. Each <b>lexicon</b> <b>entry</b> is represented by one line in the form: type lemmas frequency PoS [used form 1; used form 2; [...] . ] (columns are separated by tabs) where: type [...] . {{is the type of}} VMWE in PST typology lemmas [...] . are space separated lemmatized forms of all words that constitutes the VMWE frequency [...] . is the absolute frequency of this item in PST data PoS [...] . is a space separated list of parts of speech of individual words (in the same order as in "lemmas") final field contains a list of all (1 to 18) used forms found in the data (since Czech is a flective language) ...|$|R
30|$|The {{limitations}} of the proposed method is that the expansion of seed cache needs be made over different medical lexicons, instead of web lexicons, which {{may result in a}} more comprehensive expansion of initial lexicon. The increased rate of irrelevant words is due to generalized nature of web repositories, more specific health-related lexicons should be exploited to reduce the noise in expanded lexicon. Another possible future direction is to investigate the dynamic updating of <b>lexicon</b> <b>entries</b> over different online repositories, such as web thesaurus and biomedical dictionaries. Analyzing the effect of multiple senses on sentiment classification of health-related words would be another extension of the current study.|$|E
40|$|Extracting textual {{data from}} Greek corpuses poses {{additional}} difficulties than in English texts as inclinations and intonation differentiate terms of equal information weight. Pre-processing and normalization of text {{is an important}} step before the extraction procedure as it leads to fewer rules and <b>lexicon</b> <b>entries,</b> thus to less execution time and greater success of the mining process. This paper presents a system accessible via the Web which automatically extracts data from Greek texts. The domain of conference announcements is utilized for experimentation purposes. The success of the extraction procedure is discussed on the basis of an evaluative study. The conclusions and the techniques discussed are applicable to other domains as well...|$|E
40|$|The {{experimental}} two-level morphology of Estonian {{is under}} {{development at the}} University of Tartu. The language description, consisting of 45 two-level rules and over 200 lexicons has been implemented and tested using Xerox finite-state tools twolc and lexc. The root lexicons cover 400 most frequent stems at the present stage of development. The software {{has been designed to}} update the lexicon automatically with new stems, including the automatic generation of lexical representations of root <b>lexicon</b> <b>entries.</b> The open problems by describing of word formation processes – derivation and compounding are discussed. The advantages and disadvantages of the two-level model with respect to Estonian morphology are pointed out...|$|E
40|$|Advances in NLP {{techniques}} {{have led to}} a great demand for tagging and analysis of the sentiments from unstructured natural language data over the last few years. A typical approach to sentiment analysis is to start with a lexicon of positive and negative words and phrases. In these <b>lexicons,</b> <b>entries</b> are tagged with their prior out of context polarity. Unfortunately all efforts found in literature deal mostly with English texts. In this squib, we propose a computational technique of generating an equivalent SentiWordNet (Bengali) from publicly available English Sentiment lexicons and English-Bengali bilingual dictionary. The target language for the present task is Bengali, though the methodology could be replicated for any new language. There are two main lexical resources widely used in English for Sentiment analysis: SentiWordNet (Esuli et. al., 2006) and Subjectivity Word List (Wilson et. al., 2005). SentiWordNet is an automatically constructed lexical resource for English which assigns a positivity score and a negativity score to each WordNet synset. The subjectivity lexicon was compiled from manually developed resources augmented with entries learned from corpora. The entries in the Subjectivity lexicon have been labelled for part of speech (POS) as well as either strong or weak subjective tag depending on reliability of the subjective nature of the entry...|$|R
40|$|The usual {{challenges}} of transcribing spoken language are compounded for Southern Min (Taiwanese) because it lacks a generally accepted orthography. This study reports {{the development and}} testing of software tools for assisting such transcription. Three tools are compared, each representing {{a different type of}} interface with our corpus-based Southern Min lexicon (Tsay, 2007) : our original Chinese character-based tool (Segmentor), the first version of a romanization-based <b>lexicon</b> <b>entry</b> tool called Adult-Corpus Romanization Input Program (ACRIP 1. 0), and a revised version of ACRIP that accepts both character and romanization inputs and integrates them with sound files (ACRIP 2. 0). In two experiments, naive native speakers of Southern Min were asked to transcribe passages from our corpus of adult spoken Southern Min (Tsay and Myers, in progress), using {{one or more of these}} tools. Experiment 1 showed no disadvantage for romanization-based compared with character-based transcription even for untrained transcribers. Experiment 2 showed significant advantages of the new mixed-system tool (ACRIP 2. 0) over both Segmentor and ACRIP 1. 0, in both speed and accuracy of transcription. Experiment 2 also showed that only minimal additional training brought dramatic improvements in both speed and accuracy. These results suggest that the transcription of non-Mandarin Sinitic languages benefits from flexible, integrated software tools...|$|R
40|$|A typical {{computational}} {{approach to}} sentiment analysis starts with prior polarity <b>lexicons</b> where <b>entries</b> are tagged with their prior {{out of context}} polarity as perceived by human beings based on their cognitive knowledge. Till date, {{most of the research}} efforts found in sentiment analysis literature deal with English texts. In this paper, we propose an interactive gaming (Dr Sentiment) technology to create and vali-date SentiWordNet(s) for three Indian lan-guages, Bengali, Hindi and Telugu by involv-ing Internet population. A number of automat-ic, semiautomatic and manual validations and evaluation methodologies have been adopte...|$|R
