37|19|Public
5|$|When Peano {{formulated}} his axioms, {{the language}} of mathematical logic was in its infancy. The system of <b>logical</b> <b>notation</b> he created to present the axioms did not prove to be popular, although it was {{the genesis of the}} modern notation for set membership (∈, which comes from Peano's ε) and implication (⊃, which comes from Peano's reversed 'C'.) Peano maintained a clear distinction between mathematical and logical symbols, which was not yet common in mathematics; such a separation had first been introduced in the Begriffsschrift by Gottlob Frege, published in 1879. Peano was unaware of Frege's work and independently recreated his logical apparatus {{based on the work of}} Boole and Schröder.|$|E
25|$|In the 20th century, {{following}} {{the development of}} formal logic, the ampersand became a commonly used <b>logical</b> <b>notation</b> for the binary operator or sentential connective AND. This usage was adopted in computing.|$|E
50|$|Besides their <b>logical</b> <b>notation,</b> conjunctive queries {{can also}} be written as Datalog rules. Many authors in fact prefer the {{following}} Datalog notation for the query above: result(student, address) :- attends(student, course), gender(student, male), attends(student2, course), gender(student2, female), lives(student, address).Although there are no quantifiers in this notation, variables appearing in {{the head of the}} rule are still implicitly universally quantified, while variables only appearing {{in the body of the}} rule are still implicitly existentially quantified.|$|E
5000|$|Assuming a {{presentation}} of first-order logic without function symbols, the operator [...] models existential quantification over variable [...] in formula [...] while the operator [...] models the equality of variables [...] and [...] Henceforth, reformulated using standard <b>logical</b> <b>notations,</b> the axioms read as ...|$|R
5000|$|Infix notation, {{the common}} {{arithmetic}} and <b>logical</b> formula <b>notation,</b> such as [...] "a + b − c".|$|R
40|$|Description logics are {{valuable}} for modeling the conceptual structures {{of scientific and}} engineering research because the underlying ontologies generally have a taxonomic core. Such structures have natural representations through semantic networks that mirror the underlying description logic graph-theoretic structures and are more comprehensible than <b>logical</b> <b>notations</b> to those developing and studying the models. This article reports experience {{in the development of}} visual language tools for description logics with the objective of making research issues, past and present, more understandable. ...|$|R
5000|$|In {{mathematical}} logic, Peano-Russell notation was Bertrand Russell's {{application of}} Giuseppe Peano's <b>logical</b> <b>notation</b> to the logical notions of Frege {{and was used}} in the writing of Principia Mathematica in collaboration with Alfred North Whitehead:"The notation adopted in the present work is based upon that of Peano, and the following explanations are to some extent modelled on those which he prefixes to his Formulario Mathematico." [...] (Chapter I: Preliminary Explanations of Ideas and Notations, page 4) ...|$|E
5000|$|P, Q, and C {{are basic}} symbols in sentential logic. That fact would not ordinarily concern a reader of poetry, but since Stevens’ poem {{begins and ends}} with {{sections}} that are logical arguments it does. The similarity between the <b>logical</b> <b>notation</b> and an abbreviation of Stevens' title, [...] "PQ@C," [...] is an interesting possible sidelight. A reader can conclude from Peter's sloppy, unfinished argument that is {{that he may be}} a passionate word spinner but he is not a passionate lover.|$|E
50|$|When Peano {{formulated}} his axioms, {{the language}} of mathematical logic was in its infancy. The system of <b>logical</b> <b>notation</b> he created to present the axioms did not prove to be popular, although it was {{the genesis of the}} modern notation for set membership (∈, which comes from Peano's ε) and implication (⊃, which comes from Peano's reversed 'C'.) Peano maintained a clear distinction between mathematical and logical symbols, which was not yet common in mathematics; such a separation had first been introduced in the Begriffsschrift by Gottlob Frege, published in 1879. Peano was unaware of Frege's work and independently recreated his logical apparatus {{based on the work of}} Boole and Schröder.|$|E
50|$|Mathematical Operators is a Unicode block {{containing}} characters for mathematical, <b>logical,</b> and set <b>notation.</b>|$|R
50|$|Miscellaneous Mathematical Symbols-A is a Unicode block {{containing}} characters for mathematical, <b>logical,</b> and database <b>notation.</b>|$|R
50|$|The Mathematical Operators block (U+2200 - U+22FF) {{contains}} characters for mathematical, <b>logical,</b> and set <b>notation.</b>|$|R
5000|$|The {{standard}} approach to ontological commitment has been that, once a {{theory has been}} regimented and/or [...] "paraphrased" [...] into an agreed [...] "canonical" [...] version, which may indeed be in formal <b>logical</b> <b>notation</b> rather than the original language of the theory, ontological commitments can be read off straightforwardly from the presence of certain ontologically committing expressions (e.g. bound variables of existential quantification). Although there is substantial debate about which expressions are ontologically committing, parties to that debate generally agree that the expressions they prefer are reliable bearers of ontological commitment, imparting ontological commitment to all regimented sentences in which they occur. This assumption has been challenged.|$|E
5000|$|Starting with Peano, {{there has}} been a {{parallel}} thread of interest amongst logicians concerning the axiomatic foundations of Euclidean geometry. This can be seen, in part, in the notation used to describe the axioms. Pieri claimed that even though he wrote in the traditional language of geometry, he was always thinking in terms of the <b>logical</b> <b>notation</b> introduced by Peano, and used that formalism to see how to prove things. A typical example of this type of notation {{can be found in the}} work of E. V. Huntington (1874 [...] - [...] 1952) who, in 1913, produced an axiomatic treatment of three-dimensional Euclidean geometry based upon the primitive notions of sphere and inclusion (one sphere lying within another). Beyond notation there is also interest in the logical structure of the theory of geometry. Alfred Tarski proved that a portion of geometry, which he called elementary geometry, is a first order logical theory (see Tarski's axioms).|$|E
5000|$|Only one work by Hérigone {{is known}} to exist: [...] Cursus mathematicus, nova, brevi, et clara methodo demonstratus, per notas reales et universales, citra usum cujuscunque idiomatis intellectu faciles (published in Paris in six volumes from 1634 to 1637; second edition 1644), a {{compendium}} of elementary mathematics written in French and Latin. The work introduced a system of mathematical and <b>logical</b> <b>notation.</b> It {{has been said that}} [...] "Hérigone introduced so many new symbols in this six-volume work that some suggest that the introduction of these symbols, rather than an effective mathematics text, was his goal." [...] Florian Cajori has written that the work contains [...] "a full recognition of the importance of notation and an almost reckless eagerness to introduce an exhaustive set of symbols..."Hérigone may have been the first to introduce the mathematical symbol to express an angle. He used both the symbol below and recorded the use of [...] "<" [...] as a symbol denoting [...] "less than." ...|$|E
50|$|The Miscellaneous Mathematical Symbols-A block (U+27C0 - U+27EF) {{contains}} characters for mathematical, <b>logical,</b> and database <b>notation.</b>|$|R
40|$|AbstractThis paper {{presents}} {{an overview of}} the ECO (English COnversational System) family formalism of semantic network. In the paper, we describe the components of our semantic network, discussing its suitability as a representation of propositional knowledge. The use of our semantic network as a uniform representation mediating between specialised representations appropriate to particular task domains (e. g., understanding natural languages, etc.) is discussed. We motivate and explain a comprehensive network formalism. Special problems with respect to the use of logical connectives, quantifiers, descriptions, modalities, and certain other constructions that fail in conventional semantic networks, are systematically resolved with extensions to conventional network notations. The representation harmonizes with linear one-dimensional <b>logical</b> <b>notations,</b> illustrating the close kinship of the two notations. This kinship supports the claim that networks have inherited formal interpretability from <b>logical</b> <b>notations.</b> Several issues of network form and content, which are more fundamental than the choice of a network syntax, are addressed. These issues are: (i) primitive versus nonprimitive representations; (ii) the separation of propositional content of text from pragmatic aspects; and (iii) network normal form versus ad hoc systems. The design of computer systems for specific tasks depends in part on early commitments to these issues. The succinctness, clarity, and intuitive nature of semantic networks argues in their favour if only for purely methodological advantages. Semantic networks are readable; they suggest procedures for comprehension and inference, and the computer data structures which they resemble. Examples will demonstrate how associative processing algorithms and complex pattern matching operations are readily identifiable using networks. These examples are given in the context of natural language understanding utilizing networks in a state-based conceptual representation. We discuss how to superimpose organisational strategies into the network representations, beginning with the representation of lexical information and extending to the superimposition of topical organisations in the knowledge base. Several special purpose inference mechanisms extend the topical organisation we superimpose on concepts to aid retrieval of other types of information about concepts. The use of networks is assessed and promising areas for future research are described...|$|R
40|$|Abstract. The {{advantage}} ofthe RDF/DAML+OIL family oflanguages over ordinary XML {{is that it}} is topic-neutral and composable. However, its expressivity {{is severely}} limited. This limitation is well known, and the usual remedy is reification, in which RDF is used to describe formulas in a richer language. We propose a method for encoding typed predicate calculus using reification, which handles bound variables cleanly and causes the size to increase by only a constant multiple. The method generalizes to virtually any system, a claim which we illustrate by describing our program, PDDAML, which encodes domain specifications in PDDL using our technique. We argue that reification, while logically suspect, is in practice benign because any algorithm capable ofdoing inferences using <b>logical</b> <b>notations</b> can be easily extended to “unreify ” those notations as needed. We also argue that the ability to represent predicate calculus on the semantic web is crucial. ...|$|R
40|$|The {{role of a}} <b>logical</b> <b>notation</b> in {{a theory}} of {{discourse}} interpretation is for representing the explicit information in English texts, for expressing the knowledge required for understanding texts (which in ordinary life we ex-press in English), and for manipulation by the interpretation process. These uses lead to two principal criteria for a <b>logical</b> <b>notation.</b> Criterion I: The notation should be as close to English as possible. This {{makes it easier to}} specify the rules for translation between English and the formal language, and also makes it easier to encode in <b>logical</b> <b>notation</b> facts we normally think of in English. The ideal choice by this criterion is English itself, but it fails monumentally on the second criterion. Criterion II: The notation should be syntactically simple. Since infer-ence is dened in terms of manipulations performed on expressions in the <b>logical</b> <b>notation,</b> the simpler that notation, the easier it will be to dene the inference process. A <b>logical</b> <b>notation</b> is proposed here which is rst-order and nonintensiona...|$|E
40|$|To {{facilitate}} work in discourse interpretation, {{the logical}} form of English sentences should be both close to English and syntacti-cally simple. In this paper i propose s <b>logical</b> <b>notation</b> which is first-order and uonintensional, sad for which semantic tnmsla-tion can be naively compositional. The key move is to expand {{what kinds of}} entities one allows in one's ontology, rather than complicating the <b>logical</b> <b>notation,</b> the logical form of sentences, or the semantic translation process. Three classical problems-opaque adverbials, the distinction between de re and de ditto belief reports, {{and the problem of}} identity in intensional con-texts- are examined for the dil~cuities they pose for this <b>logical</b> <b>notation,</b> and it is shown that the difficulties can be overcome. The paper closes with s statement about the view of semantics that is presupposed by this appro,-'h. ...|$|E
40|$|This paper {{describes}} Macquarie University’s Centre for Language Technology {{contribution to}} the PASCAL 2005 Recognizing Textual Entailment challenge. Our main aim was to test the practicability of a purely logical approach. For this, atomic propositions were extracted from both the text and the entailment hypothesis and they were expressed in a custom <b>logical</b> <b>notation.</b> The text entails the hypothesis if every proposition of the hypothesis is entailed by some proposition in the text. To extract the propositions and encode them into a <b>logical</b> <b>notation</b> the system uses the output of Link Parser. To detect the independent entailment relations the system relies {{on the use of}} Otter and WordNet. 19 page(s...|$|E
5000|$|Natural {{language}} demonstrably contains its own metalanguages, {{in which}} we talk about language itself. Any other means for talking about language, such as <b>logical</b> <b>notations,</b> depends upon our prior shared 'common parlance' for our learning and interpreting it. To describe language, or to write a grammar, we cannot rely upon metalinguistic resources outside of the intrinsic metalinguistic resources within language, [...] "for any system {{in which we}} could identify the elements and meanings of a given language {{would have to have}} already the same essential structure of words and sentences as the language to be described." [...] "There is no way to define or describe the language and its occurrences except in such statements said in that same language or in another natural language. Even if the grammar of a language is stated largely in symbols, those symbols will have to be defined ultimately in a natural language." ...|$|R
5000|$|The symbolisms ⊃x and [...] "≡x" [...] {{appear at}} ✸10.02 and ✸10.03. Both are {{abbreviations}} for universality (i.e., for all) that bind the variable x to the <b>logical</b> operator. Contemporary <b>notation</b> would have simply used parentheses {{outside of the}} equality ("=") sign: ...|$|R
50|$|A discipline-specific {{modeling}} (DspM) {{language is}} focused on deliverables affiliated with a specific software development life cycle stage. Therefore, such language offers a distinct vocabulary, syntax, and notation for each stage, such as discovery, analysis, design, architecture, contraction, etc. For example, for the analysis phase of a project, the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase, however, <b>logical</b> design <b>notation</b> is used to depict relationship between software entities. In addition, the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram.|$|R
40|$|It {{has been}} a long term vision of Artificial Intelligence to develop Learning by Reading systems that can capture {{knowledge}} from naturally occurring texts, convert it into a deep <b>logical</b> <b>notation</b> and perform some inferences/reasoning on them. Such systems directly build on relatively mature areas of research, including Information Extraction (for picking out relevant information from the text), Commonsens...|$|E
40|$|We {{present a}} {{conceptual}} {{model for the}} Storage Resource Manager, the standard interface adopted for the storage systems of the Worldwide Large Hadron Collider Computing Grid. This model provides a clear and concise definition of the structural and behavioral concepts underlying the interface specification and is meant to support service and application development. Different languages (natural language, UML diagrams, and simple set-theoretic and <b>logical</b> <b>notation)</b> are used to describe {{different aspects of the}} model...|$|E
40|$|Software {{tools for}} motif and pattern scanning: program {{descriptions}} including a universal sequence reading algorithm Kerri Y. Cockwell and Ian G. Giles Two programs, MOTIF and PATTERN, that scan sequences for matches to user-defined motifs {{and patterns of}} motifs based on identity and set membership are described. The programs use a simple and <b>logical</b> <b>notation</b> to define motifs, and may be used either interactively or by using command line parameters (suitable for batch processing). The two programs described also incorporate a simple, yet reliable, algorithm that automatically detects in which of six possible formats the sequence entry is written...|$|E
40|$|This paper introduces, gently but rigorously, {{the clock}} {{approach}} to realtime programming. We present with mathematical precision, assuming no prerequisites other than familiarity with <b>logical</b> and programming <b>notations,</b> the concepts {{that are necessary}} for understanding, writing, and executing clock programs. In keeping with an expository style, all references are clustered in bibliographic remarks {{at the end of}} each section. The first appendix presents proof rules for verifying temporal properties of clock programs. The second appendix points to selected literature on formal methods and tools for programming with clocks. In particular, the timed automaton, which is a finite-state machine equipped with clocks, has become a standard paradigm for real-time model checking; it underlies the tools HyTech, Kronos, and Uppaal, discussed elsewhere in this volum...|$|R
40|$|The file {{associated}} with this record is embargoed for 24 months after publication. The final published version may be available through the links above. Born in Leipzig in 1646, Gottfried Wilhelm Leibniz is an influential figure {{in the world of}} process philosophy. In addition to his philosophical contributions, Leibniz invented the infinitesimal calculus and binary numbers, along with mathematical and <b>logical</b> forms of <b>notation</b> still used today. Leibniz believed that the worlds of theory, praxis, nature, morals, and the divine as well as the human world all formed one perfect system. This chapter examines Leibniz’s philosophy in relation to process metaphysics and its relevance to organization studies. It also discusses his responses to René Descartes and Baruch Spinoza concerning various issues, such as why he called the monads substances. It also considers which organization studies authors use Leibniz today. Peer-reviewedPost-prin...|$|R
40|$|Abstract. To make {{constraint}} programming {{easier to}} use by the nonprogrammers, {{a lot of work}} has been devoted to the design of front-end modelling languages using <b>logical</b> and algebraic <b>notations</b> instead of programming constructs. The transformation to an executable constraint program can be performed by fundamentally two compilation schemas: either by a static expansion of the model in a flat constraint satisfaction problem (e. g. Zinc, Rules 2 CP, Essence) or by generation of procedural code (e. g. OPL, Comet). In this paper, we compare both compilation schemas. For this, we consider the rule-based modelling language Rules 2 CP with its static exansion mechanism and describe with a formal system a new compilation schema which proceeds by generation of procedural code. We analyze the complexity of both compilation schemas, and present some performance figures of both the compilation process and the generated code on a benchmark of scheduling and bin packing problems. ...|$|R
40|$|In this paper, we {{analyze the}} errors novice {{students}} make when developing invariant based programs. In addition to presenting the general error types, we {{also look at}} what students have difficulty with {{when it comes to}} expressing invariants. The results indicate that an introductory course utilizing the invariant based approach is suitable {{from the very beginning of}} university studies in CS without being ``too advanced''. Although inventing the invariant was not found to be trivial, the main difficulty faced by novices when applying a correct-by-construction approach to program development seems to be related to weak skills in translating intuitive and informal statements into a symbolic form using <b>logical</b> <b>notation</b> in general and quantifiers in particular...|$|E
40|$|The aim of {{this thesis}} is Russell's {{analysis}} of Peano arithmetic. This analysis was presented by Russell and A. N. Whitehead in the book Prin- cipia Mathematica and then in Russell's book Introduction to Mathema- tical Philosophy which provided this approach in a more accessible form. The thesis focuses on Russell's critique of original Peano axioms and his effort to use only logical definitions instead of axioms. Another goal of the thesis is Russell's theory of classes and substitution of classes by propositional functions. Furthermore, the type theory for propositional functions is introduced and explained. All is converted into present-day <b>logical</b> <b>notation.</b> Moreover, the non-standard models of Russell's Peano arithmetic are studied. Finally, there are two particular arithmetic exam- ples illustrating {{the purposes of the}} thesis. Key words Bertrand Russell, Peano Arithmetic, classes, propositional funkctio...|$|E
40|$|The {{purpose of}} Husserls {{principles}} of phenomenology consists in showing {{the way in}} which transcendental subjectivity can only be brought to self-administration. If someone wants to expose the <b>logical</b> <b>notation,</b> he should go beyond the meaning of words, which are always ambiguous. This phenomenon must be considered in light of its origin in a conscious life, whose activity is theoretical. This statement contains a grain of Husserl's profound philosophical principles, the first principles of phenomenology, according to which the concept of being so closely associated with the concept of experience that brings the last justification for logical methods. Namely, we can justify giving the courts that have ontological value and only then can we be sure that we have a reflection of not only being present, but just to be...|$|E
40|$|Abstract. The {{libraries}} of deduction {{systems are}} growing constantly, {{so much that}} knowledge management concerns are becoming increasingly urgent to address. However, due to time constraints and legacy design choices, there is barely any deduction system that {{can keep up with}} the MKM state of the art. HOL Light in particular was designed as a lightweight deduction system that systematically relegates most MKM aspects to external solutions — not even the list of theorems is stored by the HOL Light kernel. We make the first and hardest step towards knowledge management for HOL Light: We provide a representation of the HOL Light library in a standard MKM format that preserves the <b>logical</b> semantics and <b>notations</b> but is independent of the system itself. This provides an interface layer at which independent MKM applications can be developed. Moreover, we develop two such applications as examples. We employ the MMT system and its interactive web browser to view and navigate the library. And w...|$|R
40|$|What is predicativity? While {{the term}} {{suggests}} that there is a single idea involved, what the history will show {{is that there are a}} number of ideas of predicativity which may lead to different logical analyses, and I shall uncover these only gradually. A central question will then be what, if anything, unifies them. Though early discussions are often muddy on the concepts and their employment, in a number of important respects they set the stage for the further developments, and so I shall give them special attention. NB. Ahistorically, modern <b>logical</b> and set-theoretical <b>notation</b> will be used throughout, as long as it does not conflict with original intentions. Predicativity emerges: Russell and Poincaré To begin with, the terms predicative and non-predicative (later, impredicative) were introduced by Russell (1906) in his struggles dating from 1901 to carry out the logicist program in the face of the set-theoretical paradoxes. Russell called a propositional function ϕ(x) predicative if it defines a class, i. e., if the class {x: ϕ(x) } exists, and non-predicative otherwise. Thus, for example, th...|$|R
40|$|Abstract. We {{introduce}} {{a new kind of}} tree automaton, a dependency tree automaton, that is suitable for deciding properties of classes of terms with binding. Two kinds of such automaton are defined, nondeterministic and alternating. We show that the nondeterministic automata have a decidable nonemptiness problem and leave as an open question whether this is true for the alternating version. The families of trees that both kinds recognise are closed under intersection and union. To illustrate the utility of the automata, we apply them to terms of simply typed lambda calculus and provide an automata-theoretic characterisation of solutions to the higher-order matching problem. Keywords: tree automata, binding terms, typed lambda calculus. 1 Introduction A standard method for solving problems over families of terms is to show thatthe solutions are tree recognisable: that is, that there is a tree automaton that accepts a term if, and only if, it is {{a solution to the problem}} [4]. In such a case,terms are built out of a finite family of (graded) symbols, that is symbols with an arity, which are naturally represented as trees. A tree automaton involves afinite set of states and a finite set of transitions. It traverses a term bottom-up or top-down labelling it with states according to the transitions and if it succeedsthen the term is accepted. Many <b>logical</b> and computational <b>notations</b> employ binders such as 9 x, uX, *x, a(x) from first-order logic, fixed-point logic, lambda calculus, ss-calculus, andso on. Although each term of such a notation can be represented as a finite tree...|$|R
