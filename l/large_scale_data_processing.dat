51|10000|Public
50|$|LingCloud is a {{suite of}} {{open-source}} cloud computing system software developed by Institute of Computing Technology, Chinese Academy of Sciences. It is licensed under Apache License 2.0. LingCloud provides a resource single leasing point system for consolidated leasing physical and virtual machines, and supports various heterogeneous application modes including high performance computing, <b>large</b> <b>scale</b> <b>data</b> <b>processing,</b> massive data storage, etc. on shared infrastructure. LingCloud can help an organization to build private cloud to manage the computing infrastructure.|$|E
50|$|After {{deploying}} OODT to the Planetary Data System {{and to the}} National Cancer Institute EDRN or Early Detection Research Network project, OODT in 2005 {{moved into}} the era of <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> and management via NASA's Orbiting Carbon Observatory (OCO) project. OODT's role on OCO was to usher in a new data management processing framework that instead of 10s of jobs per day and 10s of gigabytes of data would handle 10,000 jobs per day and 100s of terabytes of data. This required an overhaul of OODT to support these new requirements. Dr. Chris Mattmann at NASA JPL led a team of 3-4 developers between 2005-2009 and completely re-engineered OODT to support these new requirements.|$|E
40|$|An {{ecosystem}} is {{a community}} of living organisms {{in conjunction with the}} nonliving components of their environment, interacting as a system. ” Hive Ecosystem ●  Part of the larger Hadoop Ecosystem ●  Very much a living part of it. ●  Sharing data and interoperation between <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> framework...|$|E
40|$|Haskell {{is widely}} used within {{research}} and academia but is less well used for `real world' projects. This paper describes a real world project using Haskell in a <b>larger</b> <b>scale</b> <b>data</b> <b>processing</b> application. The project was undertaken jointly by British Airways and the Computing Laboratory, University of Kent...|$|R
40|$|Abstract—In {{this paper}} we present the scaling of BTWorld, our MapReduce-based {{approach}} to observing and analyzing the global BitTorrent network {{which we have}} been monitoring for the past 4 years. BTWorld currently provides a comprehensive and complex set of queries implemented in Pig Latin, with data dependencies between them, which translate to several MapReduce jobs that have a heavy-tailed distribution with respect to both execution time and input size characteristics. <b>Processing</b> BitTorrent <b>data</b> in excess of 1 TB with our BTWorld workflow required an in-depth analysis of the entire software stack and {{the design of a}} complete optimization cycle. We analyze our system from both theoretical and experimental perspectives and we show how we attained a 15 times <b>larger</b> <b>scale</b> of <b>data</b> <b>processing</b> than our previous results. I...|$|R
40|$|Biological {{information}} resources {{have been growing}} rapidly {{with the development of}} biological science and technology, and the development of computer technology and the Internet has made <b>large</b> <b>scale</b> <b>data</b> storage, <b>processing</b> and transmission possible. Machine learning is often used to learn from experience and get useful information. Protein prediction is a main part of biological information, and many prediction methods have been proposed. However, improving the prediction success rate is always a research goal. In this paper, machine learning techniques are used in bioinformatics for protein prediction, and the support vector machine algorithm is used to develop a new prediction algorithm. This method is combinatorial. Two data sets are used to verify the success rate of the modified algorithm, and the results show that the algorithm has a higher success rate. The modified algorithm can be effectively used in protein prediction...|$|R
40|$|In {{the last}} two decades, the {{continuous}} increase of computational power has produced an overwhelming flow of data which {{has called for a}} paradigm shift in the computing architecture and <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> mechanisms. MapReduce is a simple and power-ful programming model that enables easy development of scalable parallel applications to process vast amounts of data on large clus-ters of commodity machines. It isolates the application from the details of running a distributed program such as issues on data dis-tribution, scheduling and fault tolerance. However, the original im-plementation of the MapReduce framework had some limitations that have been tackled by many research efforts in several followup works after its introduction. This article provides a comprehensive survey for a family of approaches and mechanisms of <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> mechanisms that have been implemented based on the original idea of the MapReduce framework and are currently gaining a lot of momentum in both research and industrial commu-nities. We also cover a set of introduced systems that have been implemented to provide declarative programming interfaces on top of the MapReduce framework. In addition, we review several <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> systems that resemble some of the ideas of the MapReduce framework for different purposes and application scenarios. Finally, we discuss some of the future research direc-tions for implementing the next generation of MapReduce-like so-lutions. 1...|$|E
40|$|The {{development}} of <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> systems for remote sensing is studied by evaluating: (1) {{the suitability of}} several sensor types with regard to producing data required for multispectral machine analysis; (2) various types of data preprocessing necessary to prepare such data for analysis; and (3) transfer of machine processing techniques for earth resources data to user community...|$|E
40|$|AbstractApache Spark is an {{open source}} cluster {{computing}} technology specifically designed for <b>large</b> <b>scale</b> <b>data</b> <b>processing.</b> This paper deals with the deployment of Spark cluster as a cloud service on the OpenStack based cloud. HiBench benchmark suite is {{used to compare the}} performance of Spark cluster as a service and conventional Spark cluster. The results clearly depict how Spark as a cloud service gives more promising outcomes in terms of time, effort and throughput...|$|E
40|$|This paper proposes {{the use of}} service-based {{architectures}} as a way {{to bring}} the benefits of ubiquitous computing solutions to real world problems. We compare this services approach to the traditional view of computer applications as user tools, noticing that service providers can bring significant economies of scale not only in monetary terms but also in computational simplifications based on multiple user <b>data</b> aggregation and <b>large</b> <b>scale</b> <b>data</b> information <b>processing</b> and gathering. We argue for such ubiquitous services by analyzing the evolution of the World Wide Web {{in the last two decades}} and the success of service providers such as Google, eBay, and Mapquest in changing the way people live; and by showing how a services approach can make feasible many solutions to real problems such as urban congestion, preventive health care, and global trade. 1...|$|R
40|$|This paper {{describes}} the development rationale and {{the architecture of}} a prototypical expert-database system. Knowledge processing capabilities of SQL were enhanced by extending the language by recursive views. This work {{is based on an}} evolutionary approach; smooth integration with the base language was an important development aim. After a discussion of the main design alternatives, the architecture of a prototype is presented. Finally the progress of the project is described and possibilities for further extension are indicated. 1 Recursive Views 1. 1 Motivation A host of modern applications demand knowledge processing capabilities in combination with the support of <b>large</b> <b>scale</b> volume <b>data</b> <b>processing</b> capabilities and multi-user support for concurrent access and flexible combination of persistent information as provided by todays data base systems. But classical expert system shells lack important features needed in conjunction with bulk transaction processing, support for pers [...] ...|$|R
40|$|The Big data {{analysis}} {{has becoming increasingly}} more relevant for the enterprises because the efficient handling of information represents a unique competitive advantage, being its application so diverse as {{the nature of the}} data. Ejm. Fraud detection, advertising strategies, web traffic m onitoring, etc. Apache Spark is a engine for <b>large</b> - <b>scale</b> <b>data</b> <b>processing,</b> intended to be a drop in replacement for Hadoop MapReduce providing the benefit of improved performance; the main goal of this project is proof the capabilities of this system, throu gh {{the development and implementation of}} a distributed pipeline for processing and indexing at high speed and real - time multimedia data streams generated by social networks and detect trends in these, using for this purpose the Spark related projects and l ibraries: Spark Streaming and Spark MLlib. To verify the effectiveness of the algorithm, different benchmarks (with different configurations) will be performed, these results will be analyzed...|$|R
40|$|<b>Large</b> <b>scale</b> <b>data</b> <b>processing</b> has rapidly {{increased}} in nowadays. MapReduce programming model, which is firstly mentioned in functional languages, appeared in distributed system and perform excellently in <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> since 2006. Hadoop, {{which is the}} most popular framework of open-sourced MapReduce runtime environment, supplies reliable, scalable and distributed system processing large scale data across clusters of computers using this virtue programming model. In this system, files are split into many blocks and all blocks are replicated over several computers in clusters. To process these blocks efficiently, each job runs parallel and is divided into many tasks which deals with a file block. In order to fully take advantage of network bandwidth these systems, data locality is paid more and more attentions. Considering the existence of data-replica blocks, we propose a data-replicas scheduler which includes task scheduling and data allocation. The data-replicas scheduler takes fully advantage of data replicas in local Data node, reduce the costs of data transfer and improve the system performance. The results of experiments show that our scheduler not only improves the CPU ratio, but also reduces the packets that transfer in the network...|$|E
40|$|Executing {{expensive}} queries {{over many}} large tables can be prohibitively time consuming in conventional relational databases. Hadoop and its data warehouse Hive {{is a powerful}} alternative for <b>large</b> <b>scale</b> <b>data</b> <b>processing.</b> Conventionally, data is stored in Hive without compression. There is value in storing the data with compression, if the overhead of compression does not negatively impact the query processing time. This paper describes through experiments using imports, transformations and exports of Hive data in various file formats and with different compression techniques how this can be achieved...|$|E
30|$|MapReduce [1] is {{considered}} one of the most prominent programming models for problems of <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> of which the cluster analysis. It consists of two phases: Map and Reduce. The Map phase is responsible for filtering and sorting, while the Reduce phase is in charge of summarizing the outputs of the previous phase. The Map function receives records from the input files as key-value pairs and produces intermediate key-value pairs. When the Map phase is completely finished, the Reduce phase starts. Each Reducer works on the values of a specific intermediate key and produces one final value for the same key.|$|E
40|$|Both Wireless Mesh Network (WMN) and Wireless Sensor Network (WSN) are multi-hop {{wireless}} networks. WMN is {{an emerging}} community based integrated broadband wireless network which ensures high bandwidth ubiquitous internet provision to users, while, WSN is application specific and ensures <b>large</b> <b>scale</b> real-time <b>data</b> <b>processing</b> in complex environment. Both these wireless networks have some common vulnerable features which {{may increase the}} chances of different sorts of security attacks. Wireless sensor nodes have computation, memory and power limitations, which do not allow for implementation of complex security mechanism. In this paper, we discuss the common limitations and vulnerable features of WMN and WSN, along with the associated security threats and possible countermeasures. We also propose security mechanisms keeping in view the architecture and limitations of both. This article {{will serve as a}} baseline guide for the new researchers who are concern with the security aspects of WMN and WSN...|$|R
40|$|With our {{previous}} research, active learning with multi-classifier showed considering performance in <b>large</b> <b>scale</b> <b>data</b> but much calculation was involved. In this paper, we proposed an incremental multi-classifier (SVM classifiers were used) learning algorithm for <b>large</b> <b>scale</b> imbalanced image annotation. For further accelerating {{the training and}} predicting process, Grid’ 5000, French National Grid, was adopted. The result show that the best performance was reached with only 15 - 30 % of the corpus annotated and our new method could achieve almost the same precision while save nearly 50 - 60 % or even more than 94 % of the calculation time when parallel multi-threads were used. Our proposed method will be much potential on very <b>large</b> <b>scale</b> <b>data</b> for less <b>processing</b> time...|$|R
40|$|Many fields {{have a need}} {{to process}} and analyze data streams in real-time. In {{industrial}} applications the data can come from big sensor networks, where the <b>processing</b> of the <b>data</b> streams {{can be used for}} performance monitoring and fault detection in real time. Another example is in social media where <b>data</b> stream <b>processing</b> can be used to detect and prevent spam. A data stream management system (DSMS) is a system {{that can be used to}} manage and query continuously received data streams. The queries a DSMS executes are called continuous queries (CQs). In contrast to regular database queries they execute continuously until canceled. SCSQ is a DSMS developed at Uppsala university. Apache Spark is a <b>large</b> <b>scale</b> general <b>data</b> <b>processing</b> engine. It has, among other things, a component for <b>data</b> stream <b>processing,</b> Spark Streaming. In this project a system called SCSQ Spark Streaming Interface (SSI) was implemented that allows Spark Streaming applications to be called from CQs in SCSQ. It allows the Spark Streaming applications to receive input streams from SCSQ as well as emitting resulting stream elements back to SCSQ. To demonstrate SSI, two examples are shown where it is used for stream clustering in CQs using the streaming k-means implementation in Spark Streaming...|$|R
40|$|The {{programming}} paradigm Map-Reduce and {{its main}} open-source implementation, Hadoop, have had {{an enormous impact on}} <b>large</b> <b>scale</b> <b>data</b> <b>processing.</b> Our goal in this expository writeup is two-fold: first, we want to present some complexity measures that allow us to talk about Map-Reduce algorithms formally, and second, we want to point out why this model is actually different from other models of parallel programming, most notably the PRAM (Parallel Random Access Memory) model. We are looking for complexity measures that are detailed enough to make fine-grained distinction between different algorithms, but which also abstract away many of the implementation details...|$|E
40|$|Cloud Computing {{technology}} has been emerged to manage large-scale data efficiently. And due to rapid growth of data, <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> is becoming a focal point of information technique. To deal with this advancement in data collection and storage technologies, designing and implementing large-scale parallel algorithm for Data mining is gaining more interest. In this paper we design Association Rule based parallel data mining algorithm which deals with Hadoop cloud, a parallel store and computing platform. Moreover we have introduced cloud inter operation between Hadoop and Sector/Sphere Cloud which allows the same Hadoop MapReduce application to be run against data in either Hadoop Distributed File System or Sphere File System...|$|E
40|$|As massive data {{acquisition}} and storage becomes increasingly affordable, {{a large number}} of enterprises are employing statisticians to make the sophisticated data analysis. Particularly, information extraction is done when the data is unstructured or semi-structured in nature. There are emerging efforts taken by both academia and industry on pushing information extraction inside parallel DBMSs. This leads to solving an significant and important issue on what can be a better choice for <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> and analytics. To address this issue, we highlight the comparison and analysis of the three techniques which are nothing but the Parallel DBMS, MapReduce and Bulk Synchronous Processing in this paper...|$|E
40|$|Background: This paper {{introduces}} {{the notion of}} optimizing different norms in the dual problem of support vector machines with multiple kernels. The selection of norms yields different extensions of multiple kernel learning (MKL) such as L∞, L 1, and L 2 MKL. In particular, L 2 MKL is a novel method that leads to non-sparse optimal kernel coefficients, which {{is different from the}} sparse kernel coefficients optimized by the existing L ∞ MKL method. In real biomedical applications, L 2 MKL may have more advantages over sparse integration method for thoroughly combining complementary information in heterogeneous data sources. Results: We provide a theoretical analysis {{of the relationship between the}} L 2 optimization of kernels in the dual problem with the L 2 coefficient regularization in the primal problem. Understanding the dual L 2 problem grants a unified view on MKL and enables us to extend the L 2 method to a wide range of machine learning problems. We implement L 2 MKL for ranking and classification problems and compare its performance with the sparse L ∞ and the averaging L 1 MKL methods. The experiments are carried out on six real biomedical data sets and two <b>large</b> <b>scale</b> UCI <b>data</b> sets. L 2 MKL yields better performance on most of the benchmark data sets. In particular, we propose a novel L 2 MKL least squares support vector machine (LSSVM) algorithm, which is shown to be an efficient and promising classifier for <b>large</b> <b>scale</b> <b>data</b> sets <b>processing...</b>|$|R
40|$|The Cloud Computing offers service over {{internet}} with dynamically scalable resources. Cloud Computing services provides {{benefits to}} the users in terms of cost and ease of use. Cloud Computing services {{need to address the}} security during the transmission of sensitive data and critical applications to shared and public cloud environments. The cloud environments are <b>scaling</b> <b>large</b> for <b>data</b> <b>processing</b> and storage needs. Cloud computing environment have various advantages as well as disadvantages on the data security of service consumers. This paper aims to emphasize the main security issues existing in cloud computing environments. The security issues at various levels of cloud computing environment is identified in this paper and categorized based on cloud computing architecture. This paper focuses on the usage of Cloud services and security issues to build these cross-domain Internet-connected collaborations...|$|R
40|$|The Apriori {{algorithm}} that mines frequent itemsets {{is one of}} {{the most}} popular and widely used data mining algorithms. Now days many algorithms have been proposed on parallel and distributed platforms to enhance the performance of Apriori algorithm. They differ from each other on the basis of load balancing technique, memory system, data decomposition technique and data layout used to implement them. The problems with most of the distributed framework are overheads of managing distributed system and lack of high level parallel programming language. Also with grid computing there is always potential chances of node failures which cause multiple re-executions of tasks. These problems can be overcome by the MapReduce framework introduced by Google. MapReduce is an efficient, scalable and simplified programming model for <b>large</b> <b>scale</b> distributed <b>data</b> <b>processing</b> on a large cluster of commodity computers and also used in cloud computing. In this paper, we present the overview of parallel Apriori algorithm implemented on MapReduce framework. They are categorized on the basis of Map and Reduce functions used to implement them e. g. 1 -phase vs. k-phase, I/O of Mapper, Combiner and Reducer, using functionality of Combiner inside Mapper etc. This survey discusses and analyzes the various implementations of Apriori on MapReduce framework on the basis of their distinguishing characteristics. Moreover, it also includes the advantages and limitations of MapReduce framework. Comment: 12 pages, 3 figures, ICC- 201...|$|R
40|$|Support vector {{machine is}} a popular method in machine learning. Incremental support vector machine {{algorithm}} is ideal selection {{in the face of}} large learning data set. In this paper a new incremental support vector machine learning algorithm is proposed to improve efficiency of <b>large</b> <b>scale</b> <b>data</b> <b>processing.</b> The model of this incremental learning algorithm is similar to the standard support vector machine. The goal concept is updated by incremental learning. Each training procedure only includes new training data. The time complexity is independent of whole training set. Compared with the other incremental version, the training speed of this approach is improved and the change of hyperplane is reduced. </p...|$|E
40|$|XML {{has become}} a widely used and well {{structured}} data format for digital document handling and message transmission. To find useful knowledge in XML data, data warehouse and OLAP applications aimed at providing supports for decision making should be developed. Apache Hadoop is an open source cloud computing framework that provides a distributed file system for <b>large</b> <b>scale</b> <b>data</b> <b>processing.</b> In this paper, we discuss an XML data cube model which offers us the complete views to observe XML data, and present a basic algorithm to implement its building process on Hadoop. To improve the efficiency, an optimized algorithm more suitable {{for this kind of}} XML data is also proposed. The experimental results given in the paper prove the effectiveness of our optimization strategies...|$|E
40|$|MapReduce and Hadoop {{represent}} an economically compelling alternative for efficient <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> and advanced analytics in the enterprise. A key challenge in shared MapReduce clusters {{is the ability}} to automatically tailor and control resource allocations to different applications for achieving their performance goals. Currently, there is no job scheduler for MapReduce environments that given a job completion deadline, could allocate the appropriate amount of resources to the job so that it meets the required Service Level Objective (SLO). In this work, we propose a framework, called ARIA, to address this problem. It comprises of three inter-related components. First, for a production job that is routinely executed on a new dataset, we build a job profile that compactly summarize...|$|E
40|$|In the {{big data}} era, new {{technologies}} and powerful analytics {{make it possible to}} collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This {{is due to the fact}} that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the <b>larger</b> <b>scale</b> of <b>data</b> <b>processing</b> and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of <b>data</b> <b>processing.</b> On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interest...|$|R
40|$|Abstract: I {{identify}} the key challenges in xNTD computing as the <b>large</b> <b>scale</b> in <b>data</b> and <b>processing,</b> {{the use of}} focal plane arrays in radio synthesis, {{and the need for}} streamlined scientific operations. System complexity must be limited if the software costs are to be constrained. I propose an approach to xNTD computing designed to provide robust, well-documented scientific capabilities with a limited software budget. In this approach, the complexity is constrained at the top level by only supporting a limited number of observing modes. These modes are implemented by “Software Instruments”. As we gain experience, we can deliver more Software Instruments. 1. xNTD Goals and challenges The xNTD has two main purposes – to act as a demonstrator for the Square Kilometer Array and to do great science. The xNTD will be a novel type of radio telescope – consisting of about 30 – 50 conventional synthesis arrays operating in parallel. Focal plane arrays on each of twenty antennas will deliver multiple parallel streams o...|$|R
40|$|Recently, MapReduce based spatial query {{systems have}} emerged as a cost {{effective}} and scalable solution to <b>large</b> <b>scale</b> spatial <b>data</b> <b>processing</b> and analytics. MapReduce based systems achieve massive scalability by partitioning the data and running query tasks on those partitions in parallel. Therefore, effective data partitioning is critical for task parallelization, load balancing, and directly affects system performance. However, several pitfalls of spatial data partitioning make this task particularly challenging. First, data skew is very common in spatial applications. To achieve best query performance, data skew need to be reduced. Second, spatial partitioning approaches generate boundary objects that cross multiple partitions, and add extra query processing overhead. Consequently, boundary objects need to be minimized. Third, the high computational complexity of spatial partitioning algorithms combined with massive amounts of data require an efficient approach for partitioning to achieve overall fast query response. In this paper, we provide a systematic evaluation of multiple spatial partitioning methods {{with a set of}} different partitioning strategies, and study their implications on the performance of MapReduce based spatial queries. We also study sampling based partitioning methods and their impact on queries, and propose several MapReduce based high performance spatial partitioning methods. The main objective of our work is to provide a comprehensive guidance for optimal spatial data partitioning to support scalable and fast spatial <b>data</b> <b>processing</b> in massively parallel <b>data</b> <b>processing</b> frameworks such as MapReduce. The algorithms developed in this work are open source and can be easily integrated into different high performance spatial <b>data</b> <b>processing</b> systems...|$|R
40|$|Smart Cities will mainly emerge {{around the}} opening of large amounts of data, which are {{currently}} kept closed by various stakeholders within an urban ecosystem. This data requires to be cataloged and {{made available to the}} community, such that applications and services can be developed for citizens, companies and for optimizing processes within a city itself. In that scope, the current work seeks to develop concepts and prototypes, in order to enable and demonstrate, how data cataloging and data storage can be merged towards the provisioning of large amounts of data in urban environments. The developed concepts, prototype, case study and belonging evaluations are based on the integration of common technologies from the domains of Open Data and <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> in data centers, namely CKAN and Hadoop...|$|E
40|$|The growing {{popularity}} of virtualized data centers and clouds has led to virtual machine sprawl, significantly in-creasing system management costs. We present Coriolis, a scalable system that analyzes virtual machine images and automatically clusters them based on content and/or semantic similarity. Image similarity analysis can im-prove in planning many management activities (e. g., mi-gration, system administration, VM placement) and re-duce their execution cost. However, clustering images based on similarity – content or semantic – requires <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> and does not scale well. Coriolis uses (i) asymmetric similarity semantics and (ii) a hierar-chical clustering approachwith a data access requirement that is linear {{in the number of}} images. This represents a significant improvement over conventional clustering ap-proaches that incur quadratic complexity and therefore becoming prohibitively expensive in a cloud setting. ...|$|E
30|$|The {{traditional}} methods of clustering are unable {{to cope with the}} exploding volume of data that the world is currently facing. As a solution to this problem, the research is intensified in the direction of parallel clustering methods. Although there is a variety of parallel programming models, the MapReduce paradigm is considered as the most prominent model for problems of <b>large</b> <b>scale</b> <b>data</b> <b>processing</b> of which the clustering. This paper introduces a new parallel design of a recently appeared heuristic for hard clustering using the MapReduce programming model. In this heuristic, clustering is performed by efficiently partitioning categorical large data sets according to the relational analysis approach. The proposed design, called PMR-Transitive, is a single-scan and parameter-free heuristic which determines the number of clusters automatically. The experimental results on real-life and synthetic data sets demonstrate that PMR-Transitive produces good quality results.|$|E
40|$|Tyt. z nagłówka. Pozostali autorzy art. : Jarosław Jantura, Ewa Niewiadomska-Szynkiewicz, Przemysław Strzelczyk, Krzysztof Góźdź. Bibliogr. s. 76 - 78. This paper {{addresses}} {{issues associated}} with distributed computing systems {{and the application of}} mixed GPU&CPU technology to data encryption and decryption algorithms. We describe a heterogenous cluster HGCC formed by two types of nodes: Intel processor with NVIDIA graphics processing unit and AMD processor with AMD graphics processing unit (formerly ATI), and a novel software framework that hides the heterogeneity of our cluster and provides tools for solving complex scientific and engineering problems. Finally, we present the results of numerical experiments. The considered case study is concerned with parallel implementations of selected cryptanalysis algorithms. The main goal of the paper is to show the wide applicability of the GPU&CPU technology to <b>large</b> <b>scale</b> computation and <b>data</b> <b>processing.</b> Dostępny również w formie drukowanej. KEYWORDS: parallel computing, HPC, clusters, GPU computing, opencl, cryptography, cryptanalysis...|$|R
40|$|Cloud {{computing}} services need {{to address}} thesecurity during the transmission of sensitive data and criticalapplications to shared and public cloud environments. Thecloud environments are <b>scaling</b> <b>large</b> for <b>data</b> <b>processing</b> andstorage needs. Cloud computing environment have variousadvantages as well as disadvantages on the data security ofservice consumers. In Cloud computing environment dataprotection {{as the most important}} security issue. In this issue, itconcerns Include the way in which data is accessed and stored,audit requirements, compliance, and notificationRequirements, issues involving the cost of data breach, anddamage to brand value. In the cloud storage infrastructure,regulated and sensitive data needs to be properly segregated. Itis very new Concept which is use for Data securing with theHelp of Digital Signature in the cloud computing. In the serviceprovider’s data center, protecting data privacy and managingcompliance are critical by using encrypting and managingencryption keys of data in transfer to the cloud. In this ResearchPaper, we have tried to assess Cloud Storage Methodology andData Security in cloud by the Implementation of digitalsignature with CFX_MF algorithm...|$|R
40|$|We {{present an}} {{approach}} to measure pulsatile total retinal arterial blood flow in humans and rats using ultrahigh speed Doppler OCT. The axial blood velocity is measured in an en face plane by raster scanning and the flow is calculated by integrating over the vessel area, without the need to measure the Doppler angle. By measuring flow at the central retinal artery, the scan area can be very small. Combined with ultrahigh speed, this approach enables high volume acquisition rates necessary for pulsatile total flow measurement without modification in the OCT system optics. A spectral domain OCT system at 840 nm with an axial scan rate of 244 kHz was used for this study. At 244 kHz the nominal axial velocity range that could be measured without phase wrapping was ± 37. 7 mm/s. By repeatedly scanning a small area centered at the central retinal artery with high volume acquisition rates, pulsatile flow characteristics, such as systolic, diastolic, and mean total flow values, were measured. Real-time Doppler C-scan preview is proposed as a guidance tool to enable quick and easy alignment necessary for <b>large</b> <b>scale</b> studies. <b>Data</b> <b>processing</b> for flow calculation can be entirely automatic using this approach because of the simple and robust algorithm. Due to the rapid volume acquisition rate {{and the fact that}} the measurement is independent of Doppler angle, this approach is inherently less sensitive to involuntary eye motion. This method should be useful for investigation of small animal models of ocular diseases as well as total blood flow measurements in human patients in the clinic...|$|R
