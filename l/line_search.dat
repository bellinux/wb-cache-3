1404|564|Public
25|$|The {{critical}} {{points of}} Lagrangians occur at saddle points, {{rather than at}} local maxima (or minima). Unfortunately, many numerical optimization techniques, such as hill climbing, gradient descent, some of the quasi-Newton methods, among others, are designed to find local maxima (or minima) and not saddle points. For this reason, one must either modify the formulation to ensure that it's a minimization problem (for example, by extremizing {{the square of the}} gradient of the Lagrangian as below), or else use an optimization technique that finds stationary points (such as Newton's method without an extremum seeking <b>line</b> <b>search)</b> and not necessarily extrema.|$|E
2500|$|Among {{the mysteries}} {{surrounding}} the timer fragment is how, when, and by whom, it was found. [...] "A lover and his lass" [...] found the fragment while strolling in the forest, {{according to one}} police {{source close to the}} case. A man found the fragment while walking his dog, according to another version. [...] Or, in yet another story from a former investigator, police found it while combing the ground on their hands and knees. The latter became the accepted version when evidence was given at the trial. Testimony indicated that on January 13, 1989, three weeks after the bombing, two Scottish detectives engaged in a <b>line</b> <b>search</b> in woods near Lockerbie came upon a piece of charred material, later identified as the neckband of a grey Slalom-brand shirt. Because of the charring, it was sent for analysis to the DERA forensic explosives laboratory at Fort Halstead in Kent. It was not until May 12, 1989, that Dr Thomas Hayes examined the charred material. He teased out the cloth and found within it fragments of white paper, fragments of black plastic, a fragment of metal and a fragment of wire mesh—all subsequently found to be fragments of a Toshiba RT-SF 16 and its manual. Dr Hayes testified that he also found embedded a half-inch fragment of green circuit board.|$|E
50|$|At the <b>line</b> <b>search</b> step (4) the {{algorithm}} might either exactly minimize h, by solving , or loosely, by {{asking for a}} sufficient decrease in h. One example of the former is conjugate gradient method. The latter is called inexact <b>line</b> <b>search</b> and may be performed {{in a number of}} ways, such as a backtracking <b>line</b> <b>search</b> or using the Wolfe conditions.|$|E
3000|$|... is tabulated, and r is {{increased}} by one until {{the solution is}} found. The total number of <b>line</b> <b>searches</b> is tabulated, and in this figure, {{it is found that}} fewer <b>line</b> <b>searches</b> are required as r increases. Also the initial value [...]...|$|R
30|$|The {{conclusion}} of the following lemma, often called the Zoutendijk condition, is used to prove the global convergence of nonlinear conjugate gradient methods. It was originally given by Zoutendijk [19] under the Wolfe <b>line</b> <b>searches.</b> In the following lemma, we will prove the Zoutendijk condition under the general Wolfe <b>line</b> <b>searches.</b>|$|R
40|$|Solving a quadratric matrix {{equation}} by Newton’s method with exact <b>line</b> <b>searches.</b> (English summary) SIAM J. Matrix Anal. Appl. 23 (2001), no. 2, 303 – 316 (electronic). Summary: “We {{show how}} to incorporate exact <b>line</b> <b>searches</b> into Newton’s method for solving the quadratic matrix equation AX 2 + BX + C = 0, where A, B and C are square matrices. The <b>line</b> <b>searches</b> are relatively inexpensive {{and improve the}} global convergence properties of Newton’s method in theory and in practice. We also derive a condition number for the problem and show how to compute the backward error of an approximate solution. ...|$|R
5000|$|The {{gradient}} descent {{can be combined}} with a <b>line</b> <b>search,</b> finding the locally optimal step size [...] on every iteration. Performing the <b>line</b> <b>search</b> can be time-consuming. Conversely, using a fixed small [...] can yield poor convergence.|$|E
50|$|Trust region {{methods are}} in some sense dual to <b>line</b> <b>search</b> methods: trust region methods first choose a step size (the size of the trust region) and then a step {{direction}} while <b>line</b> <b>search</b> methods first choose a step direction and then a step size.|$|E
5000|$|By doing <b>line</b> <b>search</b> in each iteration, one {{automatically}} has ...|$|E
30|$|The {{following}} two theorems demonstrate that (17) satisfies the descent condition with SWP and WWP <b>line</b> <b>searches.</b>|$|R
40|$|We {{show how}} to {{incorporate}} exact <b>line</b> <b>searches</b> into Newton's method for solving the quadratic matrix equation AX^ 2 + BX + C = 0, where A, B and C are square matrices. The <b>line</b> <b>searches</b> are relatively inexpensive {{and improve the}} global convergence properties of Newton's method in theory and in practice. We also derive a condition number for the problem and show how to compute the backward error of an approximate solution...|$|R
50|$|Additionally a local search {{combining}} a quadratic interpolant of {{the function}} and <b>line</b> <b>searches</b> {{can be used to}} augment performance of the algorithm.|$|R
50|$|Octave uses BFGS with a double-dogleg {{approximation}} to the cubic <b>line</b> <b>search.</b>|$|E
50|$|Here is {{an example}} {{gradient}} method that uses a <b>line</b> <b>search</b> in step 4.|$|E
5000|$|This {{formulation}} {{is valid}} {{whether we are}} minimizing or maximizing. Note {{that if we are}} minimizing, the search direction would be the negative of z (since z is [...] "uphill"), and if we are maximizing, [...] should be negative definite rather than positive definite. We would typically do a backtracking <b>line</b> <b>search</b> in the search direction (any <b>line</b> <b>search</b> would be valid, but L-BFGS does not require exact line searches in order to converge).|$|E
40|$|We give {{conditions}} under which limited-memory quasi-Newton methods with exact <b>line</b> <b>searches</b> will terminate in n steps when minimizing n-dimensional quadratic functions. We show that although all Broyden family methods terminate in n steps in their full-memory versions, only BFGS does so with limited-memory. Additionally, we show that full-memory Broyden family methods with exact <b>line</b> <b>searches</b> terminate in at most n + p steps when p matrix updates are skipped. We introduce new limited-memory BFGS variants and test them on non-quadratic minimization problems...|$|R
40|$|We give {{conditions}} under which limited-memory quasi-Newton methods with exact <b>line</b> <b>searches</b> will terminate in $n$ steps when minimizing $n$-dimensional quadratic functions. We show that although all Broyden family methods terminate in $n$ steps in their full-memory versions, only BFGS does so with limited-memory. Additionally, we show that full-memory Broyden family methods with exact <b>line</b> <b>searches</b> terminate in at most $n+p$ steps when $p$ matrix updates are skipped. We introduce new limited-memory BFGS variants and test them on nonquadratic minimization problems. (Also cross-referenced as UMIACS-TR- 96 - 49...|$|R
40|$|Abstract. We give {{conditions}} under which limited�memory quasi�Newton methods with exact <b>line</b> <b>searches</b> will terminate in n steps when minimizing n�dimensional quadratic functions. We show that although all Broyden family methods terminate in n steps in their full�memory versions � only BFGS does so with limited�memory. Additionally � we show that full�memoryBroyden family methods with exact <b>line</b> <b>searches</b> terminate in at most n � p steps when p matrix updates are skipped. We introduce new limited�memoryBFGS variants and test them on nonquadratic minimization problems. Key words. minimization � quasi�Newton � BFGS � limited�memory � update skipping � Broyden famil...|$|R
50|$|Like other {{optimization}} methods, <b>line</b> <b>search</b> may {{be combined}} with simulated annealing {{to allow it to}} jump over some local minima.|$|E
5000|$|... with an {{adjustable}} step length [...] and performs a <b>line</b> <b>search</b> {{in this direction}} until it reaches the minimum of : ...|$|E
5000|$|Perform a {{one-dimensional}} optimization (<b>line</b> <b>search)</b> to find {{an acceptable}} stepsize [...] in the direction found in the first step, so [...]|$|E
25|$|More generally, if the {{objective}} function {{is not a}} quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on <b>line</b> <b>searches,</b> which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses trust regions. Both <b>line</b> <b>searches</b> and trust regions are used in modern methods of non-differentiable optimization. Usually a global optimizer is much slower than advanced local optimizers (such as BFGS), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points.|$|R
3000|$|... less CPU time. A hybrid method uses {{much less}} <b>line</b> <b>searches</b> from both methods, however, it consumes {{much more time}} than our method because it uses a {{projection}} method as a start. This makes our method more efficient and faster.|$|R
30|$|In this paper, we {{used the}} HS CG formula with the restart. The global {{convergence}} and descent properties were established with WWP and SWP <b>line</b> <b>searches.</b> The numerical results demonstrate that the new modification is better than other CG parameters.|$|R
5000|$|In the MATLAB Optimization Toolbox, the fminunc {{function}} uses BFGS with cubic <b>line</b> <b>search</b> {{when the}} problem size {{is set to}} [...] "medium scale." ...|$|E
5000|$|Starting with {{a maximum}} {{candidate}} step size value , using search control parameters [...] and , the backtracking <b>line</b> <b>search</b> algorithm can be expressed as follows: ...|$|E
5000|$|In (unconstrained) minimization, a {{backtracking}} <b>line</b> <b>search,</b> {{a search}} {{scheme based on}} the Armijo-Goldstein condition, is a <b>line</b> <b>search</b> method to determine the maximum amount to move along a given search direction. It involves starting with a relatively large estimate of the step size for movement along the search direction, and iteratively shrinking the step size (i.e., [...] "backtracking") until a decrease of the objective function is observed that adequately corresponds to the decrease that is expected, based on the local gradient of the objective function.|$|E
40|$|Abstract. We {{present a}} family of {{algorithms}} for local optimization that exploit the parallel architectures of contemporary computing systems to accomplish significant performance enhancements. This capability is important for demanding real time applications, as well as, for prob-lems with time–consuming objective functions. The proposed concurrent schemes namely nomadic and bundle search are based upon well es-tablished techniques such as quasi-Newton updates and <b>line</b> <b>searches.</b> The parallelization strategy consists of (a) distributed computation of an approximation to the Hessian matrix and (b) parallel deployment of <b>line</b> <b>searches</b> on different directions (bundles) and from different start-ing points (nomads). Preliminary {{results showed that the}} new parallel algorithms can solve problems in less iterations than their serial rivals...|$|R
40|$|Based on the secant {{condition}} often {{satisfied by}} quasi-Newton methods, two new {{versions of the}} Hestenes-Stiefel (HS) nonlinear conjugate gradient method are proposed, which are descent methods even with inexact <b>line</b> <b>searches.</b> The search directions of the proposed methods have the form d k = - &# 952;kg k + &# 946;kHSd k- 1, or d k = -g k + &# 946;kHSd k- 1 + &# 952;ky k- 1. When exact <b>line</b> <b>searches</b> are used, the proposed methods reduce to the standard HS method. Convergence properties of the proposed methods are discussed. These results are also extended to some other conjugate gradient methods such as the Polak-Ribiére-Polyak (PRP) method. Numerical results are reported...|$|R
50|$|It is {{integrated}} in GNOME development {{tools such as}} GNOME Builder, Glade and Anjuta, and is an official application of the GNOME project. Devhelp uses Bonobo for integration to Emacs via command <b>line</b> <b>searches</b> and is embedded in other development applications such as Anjuta.|$|R
5000|$|Given a {{starting}} position [...] and a search direction , {{the task of}} a <b>line</b> <b>search</b> is to determine a step size [...] that adequately reduces the objective function [...] (assumed smooth), i.e., to find a value of [...] that reduces [...] relative to [...] However, it is usually undesirable to devote substantial resources to finding a value of [...] to precisely minimize [...] This is because the computing resources needed to find a more precise minimum along one particular direction could instead be employed to identify a better search direction. Once an improved starting point has been identified by the <b>line</b> <b>search,</b> another subsequent <b>line</b> <b>search</b> will ordinarily be performed in a new direction. The goal, then, is just to identify a value of [...] that provides {{a reasonable amount of}} improvement in the objective function, rather than to find the actual minimizing value of [...]|$|E
5000|$|Then the {{coefficients}} [...] are multiplied by some value , chosen using <b>line</b> <b>search</b> {{so as to}} minimize the loss function, and the model is updated as follows: ...|$|E
5000|$|Thus, the {{backtracking}} <b>line</b> <b>search</b> strategy {{starts with}} a relatively large step size, and repeatedly shrinks it by a factor [...] until the Armijo-Goldstein condition is fulfilled.|$|E
40|$|Dixon’s theorem (Math. Programming, 2 (1972), PP. 383 – 387) {{states that}} all {{variable}} metric methods in the Broyden class develop identical iterates when <b>line</b> <b>searches</b> are exact. Powell’s theorem (Rep. TP 495, AERE, Harwell, England, 1972) is a variant on this, {{which states that}} under similar conditions, the Hessian approximation developed by a BFGS update at any step is independent of the updates used at earlier steps. By modifying {{the way in which}} search directions are defined, we show how to remove the restrictive assumption on <b>line</b> <b>searches</b> in these two theorems. We show also that the BFGS algorithm, modified in this way, is equivalent to the three-term-recurrence (TTR) method on quadratic functions. Algorithmic implications are discussed and the results of some numerical experimentation are reported...|$|R
40|$|Abstract. In {{this article}} I review recent {{developments}} in near-infrared emission <b>line</b> <b>searches</b> for star-forming galaxies at high redshift. Using the J-, H- & K-bands we can potentially chart the history of star formation over the range 1 10, the next frontier in the Hy-Redshift Universe. 1...|$|R
3000|$|The {{conclusion}} {{is a little}} different from (15), which results from the difference of the right hands of the <b>line</b> <b>searches</b> (9) and (12). In fact, this conclusion can be proved as that of Lemma  2.3, and we also omit it for concision. This completes the proof. □ [...]...|$|R
