6|8|Public
40|$|Nonlinear {{frequency}} compression compensates for hearing loss in frequency ranges where traditional amplification {{on its own}} does not provide sufficient benefit. The effectiveness of Phonak’s proprietary nonlinear {{frequency compression}} algorithm, SoundRecover, has been documented for more significant degrees hearing loss (Simpson, Hersbach & McDermott, 2005, 2006; Nyffeler, 2008). The {{purpose of this study}} was to test whether SoundRecover provides sufficient benefit for people with mild to moderate hearing loss. Sufficient benefit is defined as whether the person can more easily recognize quiet, high frequency sounds. This study focused particularly on the /s/ sound. To measure consonant identification, a special test, the Adaptive <b>Logatom</b> Test, was designed that is sufficiently sensitive for cases of mild and moderate hearing loss (i. e. phonemes cannot be discerned on the basis of word or sentence context). The Adaptive <b>Logatom</b> Test was administered using adaptive control of the presentation level, and the respective identification thresholds of various consonants in nonsense syllables (logatoms) were recorded. The identification threshold of the /s/ sound clearly improved with SoundRecover. In addition, subjects reported that listening with SoundRecover was more pleasant than listening without it. A summary of this study was previously published in Phonak Field Study News (April 2009). This article provides the entire study, results and discussion...|$|E
40|$|This paper {{discusses}} the finger alphabet recognition and evaluating {{the requirements for}} image quality and definition of the criteria of automatical, real-time objective evaluation without respondent involvement for speech intelligibility in the video and electronic communications. Tests with respondent were made under the <b>logatom</b> recognizability, {{as it is the}} most precise, because {{we may be able to}} identify voices or do not know, we can not infer from their context, so it causing people to avoid the tendency to repair improperly admitted syllables. This methodology is based on the intelligibility according to variable transmission channel capacity for different video formats. The aim is to determine video degradation threshold, at which the signs of one handed alphabet are still correctly understood, the degree of degradation of particular alphabet signs and, alternatively, mutual sign exchangeability. The results obtained were applied a standard scale for subjective evaluation of image quality and percentages evaluation of recognizability as used in acoustics. Based on this results of objective evaluation of <b>logatom</b> recognizability with respondent involvement we search the method, which correlate best with intelligibility. The aim of objective methods for evaluation of video quality is to design algorithms whose quality prediction is in good agreement with the results of objective human evaluation and therefore could represent a method for automatic evaluation of video intelligibility with finger alphabet...|$|E
40|$|Speech {{intelligibility}} is one {{of basic}} quality parameters of speech transmission in rooms. The methods for assessment of speech quality fall into two classes: subjective and objective methods. This paper includes an overview of selected methods of subjective listening mea-surements (ACR – Absolute Category Rating, DCR – Degradation Category Rating, speech intelligibility) recommended by ITU-T, ISO and Polish Standard and the method of speech transmission quality evaluation called “modified intelligibility test with forced choice ” (MIT-FC). The MIT-FC method provides fully automatized measurement of speech intelligibility in rooms. The experiments carried out in finding the relations between <b>logatom</b> intelligibility measured with traditional and the MIT-FC methods for the rooms have shown that there exists the multivalue and repetitive relation between them...|$|E
40|$|We {{have already}} {{presented}} {{a system that}} can track the 3 D speech movements of a speaker's face in a monocular video sequence. For that purpose, speaker-specific models of the face have been built, including a 3 D shape model and several appearance models. In this paper, speech movements estimated using this system are perceptually evaluated. These movements are re-synthesised using a Point-Light (PL) rendering. They are paired with original audio signals degraded with white noise at several SNR. We study how much such PL movements enhance the identification of <b>logatoms,</b> and also to what extent they influence the perception of incongruent audio-visual <b>logatoms.</b> In a first experiment, the PL rendering is evaluated per se. Results seem to confirm other previous studies: though less efficient than actual video, PL speech enhances intelligibility and can reproduce the McGurk effect. In the second experiment, the movements have been estimated with our tracking framework with various appearance models. No salient differences are revealed between the performances of the appearance models...|$|R
40|$|The {{determination}} of {{direction in the}} median plane is possible from information contained in direction determining frequency bands in a signal. Using <b>logatoms</b> radiated by loudspeakers, two series of tests were carried out, with the object of establishing whether pre-knowledge of the signal is necessary. A direction determination without pre-knowledge was not possible; in addition, the indeterminable signals were mainly located directly behind the listener...|$|R
40|$|International audienceThis study {{investigates the}} effect of “adverse” contexts, {{especially}} that of the consonant /ʃ/, on labial parameters for French /i,y/. Five parameters were analysed: the height, width and area of lip opening, {{the distance between the}} corners of the mouth, as well as lip protrusion. Ten speakers uttered a corpus made up of isolated vowels, syllables and <b>logatoms.</b> A special procedure has been designed to evaluate lip opening contours. Results showed that the carry-over effect of the consonant /ʃ/ can impede the opposition between /i/ and /y/ in the protrusion dimension, depending upon speakers...|$|R
40|$|Abstract — This paper {{discusses}} the cued speech recognition methods in videoconference. Cued speech {{is a specific}} gesture language that is used for communication between deaf people. We define the criteria for sentence intelligibility according to answers of testing subjects (deaf people). In our tests we use 30 sample videos coded by H. 264 codec with various bit-rates and various speed of cued speech. Additionally, we define the criteria for consonant sign recognizability in single-handed finger alphabet (dactyl) analogically to acoustics. We use another 12 sample videos coded by H. 264 codec with various bit-rates in four different video formats. To interpret the results we apply the standard scale for subjective video quality evaluation and the percentual evaluation of intelligibility as in acoustics. From the results we construct the minimum coded bit-rate recommendations for every spatial resolution. Keywords—cued speech, inteligibility, <b>logatom,</b> video I...|$|E
40|$|International audienceIntroductionThe goal of {{this study}} is to {{describe}} precisely orofacial muscle activities during labial stop consonant production (/p/, /b/), as a function of the vowel context and level of articulatory effort. This work raises several methodological problems: First, the high muscle density of the lip region raises some concerns with the use of surface electromyography to investigate lip movements, since there may be some signal crosstalk between the different pairs of electrodes positioned on different lip muscles. Second, for the production of a labial stop consonant the precise lip movement during the closing and opening gestures is crucial for speech acoustics. We propose to decompose these gestures into several basic phases, lip closing, lip compression and lip opening, during which the precise muscle coordination will be investigated. Material and methodsFour adult subjects without speech disorders were recorded in laboratory conditions while producing the logatoms /lepa/, /leba/, /lepi/, /lebi/, /lepu/ and /lebu/ with 5 levels of articulatory effort. Twelve repetitions of each <b>logatom</b> were recorded for each level of articulatory effort. Three types of signals were simultaneously recorded : the audio signal (at 44. 1 kHz), high-speed images of the lips (at 200 Hz) and the EMG signal of 6 different orofacial muscles recorded at 200 Hz with bipolar surface electrodes. The seven pairs of surface electrodes were placed on the superior orbicularis oris (OOS), the inferior orbicularis oris (OOI), the zygomatic (ZYG), the buccinator (BUC), the depressor labii inferioris (DLI), the mentalis (MNT) and the anterior belly of the digastric (DIG) The external and internal lip contours, extracted from the high speed images, were used to segment the speech gesture into four time intervals corresponding to 1 - lip closing, 2 - lip compression increase, 3 - lip compression decrease and 4 - lip re-opening. The activity of each muscle was analyzed during these 4 times intervals, considering the full-wave rectified EMG signal. Results Preliminary results suggest that the mentalis plays a major role in the lip closing phase, while the orbicularis oris is crucial for holding the compression. Simultaneous signals observed of electrodes informing on agonist/antagonist muscles suggest the existence of co-activation patterns that will be further investigated. The amplitude of muscle activation depends on the level of effort in a speaker-dependent way. ...|$|E
40|$|International audienceIntroductionThe {{goals of}} this study are 1 - to {{describe}} precisely the muscle coordination during French bilabial stop consonant production, in both time and amplitude, using surface electromyography and 2 - to find relevant and reliable descriptors of speech articulation effort. Material and methodsAn experiment in two parts was conducted. A first methodological part aimed at targeting multiple orofacial muscles, at validating the correct position of EMG electrodes on the speaker’s face, and at ensuring that the recorded EMG signals were not significantly contaminated by the activity of adjacent muscles. A second part aimed at characterizing the time activation patterns of these orofacial muscles during the production of labial stop consonants, at examining how these patterns were influenced by the following vowel and by an increasing level of articulation effort, and at finding muscle activation descriptors that reproducibly correlate with kinematic parameters such as the degree of interlip compression or articulatory velocities. To that goal, four adult speakers without speech disorders were recorded in laboratory conditions. In {{the first part of the}} experiment, they were asked to produce 6 non-speech orofacial movements (lip spreading; lip compression; lowering of respectively the jaw and the lower lip, inDLIendently; lip protrusion and raising of the lower lip), with 20 repetitions of each movement. In the second part of the experiment, they produced 6 speech logatoms /lepa/, /leba/, /lepi/, /lebi/, /lepu/ and /lebu/ by series of 5 items, with an increasing level of articulatory effort (self-evaluated, from 1 to 5). Ten repetitions of these series were recorded for each <b>logatom.</b> Two types of signals were simultaneously recorded: high-speed images of the lips (at 100 Hz) and the EMG signal of 7 different orofacial muscles recorded at 20 kHz with bipolar surface electrodes (at 200 Hz, with a Biopac system). The seven pairs of surface electrodes were placed on the superior orbicularis oris (OOS), the inferior orbicularis oris (OOI), the zygomatic (ZYG), the buccinator (BUC), the depressor labii inferioris (DLI), the mentalis (MNT) and the anterior belly of the digastric (DIG). The internal and external lip contours were extracted from the high-speed images. For the non-speech gestures of the first experimental part, two time intervals were segmented from the kinematic signals, corresponding to P 1 - the lip displacement from the rest position to their maximum displacement and P 2 - the back displacement to the rest position. For the speech gestures of the second experimental part, four time intervals were segmented from the kinematic signals, corresponding to P 1 - lip closing, P 2 - lip compression increase, P 3 - lip compression decrease and P 4 - lip re-opening. Three kinematic parameters were extracted from the external lip contour: the degree of interlip compression, the articulatory velocities of lip closing and re-opening. For both experimental parts, muscle activity was analyzed considering the integral of the full-wave rectified EMG signal over each time interval of the gestures. Two different methods were used to investigate the degree of co-activation of the different muscles in each phase of the movements (correlation between muscle activity levels) and the degree of similarity of the recorded EMG signals in each phase of the movements (raw inter-correlation between full-wave rectified EMG signals envelopes). ResultsPreliminary results on one speaker suggest that a correct position was found for all pairs of EMG electrodes but one (targeting the OOS muscle), ensuring that these electrodes recorded the activity of the targeted muscles. High levels of co-activation were observed between some muscles in several phases of non-speech gestures (R 2 > 0. 65). However, for most of the electrodes, these high levels of co-activations were not accompanied by a significant degree of similarity between the full-wave rectified EMG signals envelopes (R 2 < 0. 40), ruling out any problem of diaphony between these adjacent electrodes. On the contrary, when the DLI muscle was activated, the EMG signals from the MNT and the DLI electrodes always showed a high level of co-activation and a high degree of waveform similarity, which supported the existence of a diaphony phenomenon from the DLI electrode to the MNT one. As a result, the signal from the OOS and MNT electrodes was no longer considered to study the speech production gestures. Furthermore, results obtained from the second experimental part showed that the production of bilabial stop consonants could be characterized by a reproducible time activation pattern of orofacial muscles: the first phase of the gesture (P 1 -lip closing) was characterized by a maximal activity of the BUC muscle, while the third phase (P 3 -lip compression decrease) was underlined by a significant and maximal activity of the DLI, DIG and OOI muscles. This time activation pattern was not significantly influenced by the following vowel or by the level of articulation effort. On the other hand, the amplitude of these muscle activities was overall significantly greater in /a/ and /i/ contexts, compared to a /u/ context (except for the OOI muscle that showed, on the contrary, a greater activity in the /u/ context). These muscle activities were also highly affected by the level of articulation effort. In particular, a significant correlation was observed between the level of articulation effort and the activity of the BUC muscle in P 1 (R 2 = 0. 72), and of the DLI and DIG muscles in P 3 (R 2 = 0. 70 and R 2 = 0. 71). Finally, significant correlations were also found between these muscle activities and kinematic descriptors of the lip gesture. In particular, the activity of the DIG muscle in P 3 highly correlated with the degree of interlip compression (R 2 = 0. 71) and with the lip opening velocity (R 2 = 0. 75), while the activity of the BUC muscle in P 1 correlated significantly with the lip closing velocity (R 2 = 0. 72). ConclusionThis study brings a methodological contribution to the investigation of orofacial muscle activities in speech production, using surface electromyography, by exploring in detail the existence of signal crosstalk between adjacent pairs of electrodes. The observations provided here support the interest of decomposing the global speech gesture into phases in order to characterize more precisely the sequencing of the activations and the co-activations across muscles. Finally, reproducible descriptors of speech articulation effort were found for a first speaker in the activity of 2 muscles (BUC and DIG), which correlated significantly with both the sensation of effort and with kinematic descriptors. Results from the 4 speakers will be presented and discussed during the conference...|$|E
40|$|Preparation, recording, {{segmentation}} {{and pitch}} labelling of Slovenian diphone inventories are described. A special user friendly intert'ace package {{was developed in}} order to facilitate these operations. As acquisition of a labelled diphone inventory or adaptation of a speech synthesis system to synthesise further voices is manually intensive, an automatic procedure is required. A speech recogniser, based on Hidden Markov Models in forced segmenta-tion mode is used to outline phone boundaries within spoken <b>logatoms.</b> A statistical evalua-tion of manual and automatic segmentation dis-crepancies is performed so as to estinmte the reliability of automatically derived labels. Fi-nally, diphone boundaries are determined and pitch markers are assigned to voiced sections of the speech signal. ...|$|R
40|$|This {{dissertation}} {{deals with}} coarticulatory nasalization in Czech and English. The theoretical part introduces {{the topic of}} coarticulation, presents methods used in the research of studies, as well as studies which have analyzed this topic before. In the practical part, the dissertation presents four experiments, which have been largely conducted by means of nasometric analysis. Three experiments analyze vowel nasalance based on segmental and suprasegmental parameters, both in <b>logatoms</b> and connected speech. I was interested in mean nasalance in the respective vowels, {{as well as in}} the course of nasalance throughout vowels and vowel-nasal-vowel tokens. In one experiment, I also examine the perceptual aspect of nasality; manipulations of spectral envelope were used to generate the target speechsounds...|$|R
40|$|RESUMO Este artigo analisa a variac?a?o da durac?a?o de si?labas em diferentes domi?nios proso?dicos e pretende oferecer mais ferramentas que permitam analisar estruturas sinta?ticas atrave?s de pistas fonolo?gicas. Nossos resultados apontam que si?labas po?s-to?nicas em final de enunciado sa?o significantemente mais longas do que nos outros domi?nios. Eles tambe?m mostram que, enquanto o tipo de vogal na?o afeta a durac?a?o das si?labas a?tonas, o vozeamento das consoantes afetou os resultados. Finalmente, na?o ha? variac?a?o de resultados a depender de as palavras serem parte do le?xico ou logatomas. PALAVRAS-CHAVE Domi?nios Proso?dicos. Durac?a?o. Interface Fonologia-Sintaxe. ABSTRACT This paper {{discusses}} syllable duration {{with respect}} to different prosodic domains and presents additional tools to analyze syntactic structures through phonological cues. Our results show that post-tonic syllables are longer at the intonational phrase boundary but not at other prosodic boundaries. The results also show {{that the type of}} vowel involved does not affect the duration of the syllable, but consonant voicing does. Finally, we show that both real words and <b>logatoms</b> do not affect the results. KEYWORDS Prosodic Domains. Duration. Syntax-Phonology Interface...|$|R
30|$|In [12], a {{statistical}} AV model based on Gaussian mixture models (GMMs) is presented {{for measuring the}} coherency of audio and its corresponding video and is used for extracting speech of interest from instantaneous squared mixtures on a simple French <b>logatoms</b> AV corpus. They have extended their method in [14] and assessed it on a more general sentence corpus and also for degenerate mixtures. Wang et al. [15] have exploited a similar GMM model (but using different AV features) as a penalty term for solving convolutive mixtures. That method seems to be inefficient because it should convert the separating system from frequency to time domain repeatedly. Rajaram et al. [13] have incorporated visual information in a Bayesian AVSS for separation of two-channel noisy mixtures. Their method adopts a Kalman filter with additional independence constraint between the states (sources). Rivet et al. [16] have adopted the AV coherency of speech (measured by a trained log-Rayleigh distribution) for resolving the permutation indeterminacy in the frequency domain separation of convolutive mixtures. They have also proposed another method [11] for convolutive AVSS based on developing a VVAD and using it in a geometric separation algorithm using sparse source assumption.|$|R
40|$|International Telemetering Conference Proceedings / September 28 - 30, 1976 / Hyatt House Hotel, Los Angeles, CaliforniaThis paper {{describes}} the European experiments and test results obtained in L-band (1550 - 1650 MHz),using the NASA ATS- 6 satellite, to conduct communication and navigation tests over the North Atlantic thus assisting {{in the definition}} of modulation techniques to be used with an Aeronautical Satellite System, AEROSAT. The experiments conducted by ESA and some of its member states covered voice, data transmission and ranging measurements. The tests were performed on board a Comet IV aircraft equipped with a slot dipole array antenna, especially designed to operate within the coverage required in the AEROSAT MOU (Memorandum of Understanding for the AEROSAT programme signed by Europe (ESA), the USA (FAA) and Canada (DoT) in August 1974. The voice tests compared DELTA-PSK with adaptive NBFM using test tapes consisting of <b>logatoms,</b> SCIM sequences, and PB word lists. An investigation of multipath noise effects on the PSK data transmission system was carried out and led to the general conclusion that this problem is a serious one for coherent demodulators. The DECPSK system tested exhibited a strong tendency towards a Rayleigh channel BER situation at low antenna signal to multipath interference ratios. The ranging results show the feasibility of achieving standard deviations of range of around 500 - 600 m for the PLACE tone system with its rather short integration time of 120 ms, and 100 m for the DIOSCURES pseudo random coded system operated on a CW basis...|$|R

