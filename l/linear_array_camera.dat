6|6380|Public
40|$|At present, the on-orbit {{calibration}} of {{the geometric}} parameters {{of a space}} surveying camera is usually processed by data from a ground calibration field after capturing the images. The entire process is very complicated and lengthy and cannot monitor and calibrate the geometric parameters in real time. On {{the basis of a}} large number of on-orbit calibrations, we found that owing to the influence of many factors, e. g., weather, it is often difficult to capture images of the ground calibration field. Thus, regular calibration using field data cannot be ensured. This article proposes a real time self-calibration method for a space <b>linear</b> <b>array</b> <b>camera</b> on a satellite using the optical auto collimation principle. A collimating light source and small matrix array CCD devices are installed inside the load system of the satellite; these use the same light path as the <b>linear</b> <b>array</b> <b>camera.</b> We can extract the location changes of the cross marks in the matrix array CCD to determine the real-time variations in the focal length and angle parameters of the <b>linear</b> <b>array</b> <b>camera.</b> The on-orbit status of the camera is rapidly obtained using this method. On one hand, the camera’s change regulation can be mastered accurately and the camera’s attitude can be adjusted in a timely manner to ensure optimal photography; in contrast, self-calibration of the camera aboard the satellite can be realized quickly, which improves the efficiency and reliability of photogrammetric processing...|$|E
40|$|Abstract: The paper {{describes}} {{two different}} geometric projection models for a full-spherical camera and presents {{results of the}} calibration of full spherical camera systems based on them. Full spherical images can be generated by a rotating <b>linear</b> <b>array</b> <b>camera</b> equipped with a fisheye lens. Their projection (Fig. 1) differs considerably from the central perspective geometry. It can be modelled by introducing spherical coordinates or by applying a sensor related coordinate system for each recording position of the linear array sensor extended by fisheye lens projection equations. For a first test of the system, a calibration was performed by spatial resection from a single image of a calibration room suitable for full-spherical records. Full-spherical camera systems were calibrated with an accuracy better than 1 / 5 pixels...|$|E
40|$|This paper {{introduces}} a linescan-based stereo panoramic scanning system that employs an off-axis camera configuration mode. In this mode, the two stereo linescan cameras are mounted off-axis on the horizontal bar {{of a camera}} mast, approximately equidistant from the rotation axis. Compared to the on-axis mode used in many other linescan-based panoramic scanning systems, the off-axis mode enables the system to acquire multi-perspective stereo panoramas that can provide uniform accuracy in all 360 º directions at each depth. A prototype using a color <b>linear</b> <b>array</b> <b>camera</b> with RGB channels has been developed at the Mapping and GIS Laboratory of The Ohio State University. The future version of this system will be a Mobile Panoramic Multi-spectral Scanner (MPMS) in which each camera will include multi-spectral channels such as VNIR (Visible and Near Infrared) and SWIR (Short Wavelength Infrared). This new system {{has the potential to}} be used in future Mars landed missions for the detection and mapping of minerals, water and other habitability signatures as well as for support of Earth-based applications. 1...|$|E
50|$|<b>Linear</b> <b>array</b> <b>cameras</b> {{are also}} called scan backs.|$|R
5000|$|Implementation {{and testing}} of new sensor and {{trajectory}} models for aerial and satellite <b>Linear</b> <b>Array</b> <b>cameras,</b> including self-calibration, for calibration, geo-referencing, validation ...|$|R
40|$|International audienceThis paper {{presents}} {{a novel approach}} to capture light field in <b>camera</b> <b>arrays</b> based on the compressive sensing framework. Light fields are captured by a <b>linear</b> <b>array</b> of <b>cameras</b> with overlapping field of view. In this work, we design a redundant dictionary to exploit cross-cameras correlated structures in order to sparsely represent cameras image. We show experimentally that the projection of complex synthetic scenes into the designed dictionary yield very sparse coefficients...|$|R
40|$|A {{study of}} the Stereosat mission, which may acquire analog/digital stereo image data of cartographic quality with <b>linear</b> <b>array</b> <b>camera</b> systems, is presented, with {{attention}} given to the Stereosat system and the mission parameters, the utilization of the Stereosat studies in order to indicate the nature {{and scope of the}} problems associated with recording image data of high geometric fidelity from spacecraft equipped with line array sensors, and the assessing of the possibilities of meeting the completeness and accuracy requirements of cartographic products. A 713 -km circular, sun-synchronous, near polar orbit, with an inclination of 98. 24 deg and a repeat cycle of 48 days is designed to insure global coverage of the earth's land masses during the three-year lifetime of the mission. Three cameras (fore, vertical, and aft looking), mounted so as to record stereotriplets for a 61. 4 km wide swath at a nominal focal plane scale of 1 : 1, 000, 000, are considered. The IFOV of 15 m should be adequate for producing thematic and image maps at scales of 1 : 50, 000 to 1 : 250, 000. The value of the coverage to cartographers will be dictated by the accuracy of the digital image data, which is largely a function of satellite control...|$|E
40|$|ZiYuan 3 - 02 (ZY 3 - 02) is {{the first}} remote sensing {{satellite}} {{for the development of}} China’s civil space infrastructure (CCSI) and the second satellite in the ZiYuan 3 series; it was launched successfully on 30 May 2016, aboard the CZ- 4 B rocket at the Taiyuan Satellite Launch Center (TSLC) in China. Core payloads of ZY 3 - 02 include a triple <b>linear</b> <b>array</b> <b>camera</b> (TLC) and a multi-spectral camera, and this equipment will be used to acquire space geographic information with high-resolution and stereoscopic observations. Geometric quality is a key factor that affects the performance and potential of satellite imagery. For the purpose of evaluating comprehensively the geometric potential of ZY 3 - 02, this paper introduces the method used for geometric calibration of the TLC onboard the satellite and a model for sensor corrected (SC) products that serve as basic products delivered to users. Evaluation work was conducted by making a full assessment of the geometric performance. Furthermore, images of six regions and corresponding reference data were collected to implement the geometric calibration technique and evaluate the resulting geometric accuracy. Experimental results showed that the direct location performance and internal accuracy of SC products increased remarkably after calibration, and the planimetric and vertical accuracies with relatively few ground control points (GCPs) were demonstrated to be better than 2. 5 m and 2 m, respectively. Additionally, the derived digital surface model (DSM) accuracy was better than 3 m (RMSE) for flat terrain and 5 m (RMSE) for mountainous terrain. However, given that several variations such as changes in the thermal environment can alter the camera’s installation angle, geometric performance will vary with the geographical location and imaging time changes. Generally, ZY 3 - 02 can be used for 1 : 50, 000 stereo mapping and can produce (and update) larger-scale basic geographic information products...|$|E
40|$|This thesis {{addresses}} {{the problem of}} acquiring spatial data concerning points {{on the surface of}} structures such as underground tunnels and sewers. These data can usefully provide knowledge of deformation, shape, area, volume, and position of structures. Such data can be further analysed to give insight into clearances, deterioration, flow rates and in-fill volumes or can be used to give knowledge of the present state of structures and their position. Few systems address the problem of reliably acquiring this data {{in a manner that is}} fast and accurate while remaining flexible, adaptable and robust. This thesis considers a solution to the problem of fast and accurate spatial data acquisition concerning commonly found structures using the technique of optical triangulation with a <b>linear</b> <b>array</b> <b>camera</b> and diode laser light source. Optical triangulation is a technique that has not fully matured for medium range measurement with few systems having been developed and little research material produced. However, the research carried out for this thesis shows that providing all the factors that contribute errors of measurement are understood, then a fast, robust and high accuracy system can be developed. The development of the optical triangulation technique for use in surveying was addressed through a programme of prototype development, testing, and refinement. Three prototypes were built that demonstrated the reliability, accuracy, speed and robustness of this technique. The errors associated with the a triangulation measuring system when applied to surveying application is considered from the intrinsic errors which are the same for any triangulation system and the extrinsic errors which are particular to the use of this system in surveying situations. A calibration bench was constructed for consideration of the triangulation system which was automatic and used an interferometer to provide high accuracy measurement of the performance of the triangulation system. Calibration and interpolation trials were conducted and the results analysed. An analysis of the subpixel accuracy achieved with the discrete pixel CCD imagers has been performed and an analysis made. One of the main disadvantages of optical triangulation when applied to the range 0 - 5 metres is that of non-linearity. A method of correction has been developed and analysed which is believed to be novel and makes a significant improvement to the measuring system. The conclusion of this research is that an improved system of measurement has been produced which has a number of novel features. Trials show that the measuring system could be developed commercially to provide a solution to measurements of structures within the range of the device and with greater accuracy than comparable equipment designed for the same purpose...|$|E
50|$|Except {{for some}} <b>linear</b> <b>array</b> type of <b>cameras</b> at the highest-end and simple web cams at the lowest-end, a digital memory device (usually a memory card; floppy disks and CD-RWs are less common) {{is used for}} storing images, which may be {{transferred}} to a computer later.|$|R
40|$|Multi-camera {{systems such}} as <b>linear</b> <b>camera</b> <b>arrays</b> are {{commonly}} used to capture content for multi-baseline stereo estimation, view generation for auto-stereoscopic displays, or similar tasks. However, even after a careful mechanical alignment, residual vertical disparities and horizontal disparity offsets impair further processing steps. In consequence, the multicamera content needs to be rectified on a common baseline. The trifocal tensor represents the geometry between three cameras and hence is a helpful tool to calibrate a multi-camera system, and to derive rectifying homographies. Against this background we propose a new method for a robust estimation of the trifocal tensor specialized for <b>linear</b> <b>camera</b> <b>arrays</b> and subsequent rectifying homography computation based on feature point triplets...|$|R
40|$|We have {{developed}} a highly sensitive method for measuring thermal expansion, mechanical strain, and creep rates. We use the well-known technique of observing laser speckle {{with a pair of}} <b>linear</b> <b>array</b> <b>cameras,</b> but we employ a data-processing approach based on a two-dimensional transform of the speckle histories from each camera. This technique can effect large gauge sizes, which are important in the assessment of the spatial statistics of creep. Further, the algorithm provides simultaneous global estimates of the strain rates at both small- and large-scale sizes. This feature may be of value in the investigation of materials with different short- and long-range orders. General advantages of our technique are compact design, modest resolution requirements, insensitivity to slow surface microstructure changes (as seen with oxidation), and insensitivity to zero-mean-noise processes such as turbulence and vibration. Herein we detail the theory of our technique and the results of a number of experiments. Thesetests are intended to demonstrate the performance advantages and limitations of the transform method of processing speckle strain-rate data...|$|R
40|$|A laser speckle strain {{measurement}} system with two-dimensional measurement capabilities {{has been built}} and tested for high temperature applications. The 1 st and 2 nd principle strains at a point on a specimen are calculated from three components of one-dimensional strain. Strain components are detected by cross-correlating reference and shifted speckle patterns recorded before and after straining the specimen. Speckle patterns are recorded by a <b>linear</b> photodiode <b>array</b> <b>camera.</b> Accurate strains have been measured at temperatures up to 650 C. Stable speckle correlations and linear stress-strain relations have been demonstrated up to 750 C. The resolution {{of the system is}} 15 microstrains, with a gauge length less than 1 mm...|$|R
40|$|AbstractThe {{measurement}} of blood-plasma velocity distributions with high {{spatial and temporal}} resolution in vivo {{is important for the}} investigation of embryonic heart at its early stage development. Optical Coherence Tomography is a non-invasive imaging modality with high resolution (5 to 20 μm) that can provide flow velocity information by calculating the Doppler frequency shift. In this paper, a high speed spectral optical coherence tomography system was demonstrated. An achievable scanning speed of 92 k line/sec has been reached by using an ultra-high speed <b>linear</b> <b>array</b> CCD <b>camera.</b> The measurable flow velocity range is [- 24, 24]mm/s using this system. Early stage Chicken embryo heart blood flow was measured in vivo...|$|R
40|$|International audienceThis paper {{presents}} {{a novel approach}} to capture light field in <b>camera</b> <b>arrays</b> based on the compressive sensing framework. Light fields are captured by a <b>linear</b> <b>array</b> of <b>cameras</b> with overlapping field of view. In this work, we design a redundant dictionary to exploit cross-cameras correlated structures to sparsely represent cameras image. Our main contributions are threefold. First, we exploit {{the correlations between the}} set of views by making use of a specially designed redundant dictionary. We show experimentally that the projection of complex scenes onto this dictionary yields very sparse coefficients. Second, we propose an efficient compressive encoding scheme based on the random convolution framework. Finally, we develop a joint sparse recovery algorithm for decoding the compressed measurements and show a marked improvement over independent decoding of CS measurements...|$|R
50|$|In 1998 the {{tapestries}} were {{cleaned and}} restored. In the process, the linen backing was removed, the tapestries were bathed in water, {{and it was}} discovered that the colours on the back were in even better condition than those on the front (which are also quite vivid). A series of high resolution digital photographs were taken of both sides using a customised scanning device suspending a <b>linear</b> <b>array</b> scan <b>camera</b> and lighting over the delicate textile. The front and back of the tapestries were photographed in approximately three-foot square segments. The largest tapestry required up to 24 individual 5000 × 5000 pixel images. Merging the massive data stored in these photos required the efforts of two mathematicians, the Chudnovsky brothers.|$|R
40|$|We {{present a}} system and {{techniques}} for synthesizing views for three-dimensional video teleconferencing. Instead of performing complex 3 D scene acquisition, {{we decided to}} trade storage/hardware for computation, i. e., using more cameras. While it is expensive to directly capture a scene from all possible viewpoints, we observed that the participants’ viewpoints usually remain at a constant height (eye level) during video teleconferencing. Therefore we can restrict the possible viewpoint to be within a virtual plane without sacrificing much of the realism. Doing so significantly reduces the number of cameras required. We demonstrate a realtime system that uses a <b>linear</b> <b>array</b> of <b>cameras</b> to perform Light-Field style rendering. The simplicity and robustness of light fielding rendering, combined with the natural restrictions of limited view volume in video teleconferencing, allow us to synthesize photo-realistic views per user request at interactive rate...|$|R
40|$|This paper {{presents}} {{a system for}} capturing and rendering a dynamic image-based representation called the plenoptic videos. It is a simplified version of light fields for dynamic environment, where user viewpoints are constrained along the camera plane of a <b>linear</b> <b>array</b> of video <b>cameras.</b> The system consists of a <b>camera</b> <b>array</b> of 8 Sony CCX-Z 11 CCD cameras and eight Pentium 41. 8 GHz computers connected together through a 100 baseT LAN. Important issues such as multiple camera calibration, real-time compression, decompression and rendering are addressed. Experimental results demonstrated {{the usefulness of the}} proposed parallel processing based system in capturing and rendering high quality dynamic image-based representation using off-the-shelf equipment, and its potential applications in visualization and immersive television systems. published_or_final_versio...|$|R
40|$|We {{present a}} system and {{techniques}} for synthesizing views for many-to-many video teleconferencing. Instead of replicating oneto -one systems for {{each pair of}} users, or performing complex 3 D scene acquisition, we rely upon user tolerance for soft discontinuities for our rendering techniques. Furthermore, we observed that the participants' eyes usually remain at a constant height (sitting height) during video teleconferencing, thus we only {{need to be able}} to synthesize new views on a horizontal plane. We demonstrate a real-time system that uses a <b>linear</b> <b>array</b> of <b>cameras</b> to perform Light Field style rendering. The simplicity and robustness of Light Fielding rendering, combined with the natural restrictions of limited view volume in video teleconferencing, allow us to synthesize photo-realistic views for a group of participants at interactive rate. Categories and Subject Descriptors: H. 5. 1 [Multimedia Information System]: Video teleconferencing; General Terms: Design, Algorithms; Keywords: Group video teleconferencing, Light field rendering, Scene reconstruction...|$|R
40|$|This paper proposes an object-based {{approach}} to plenoptic videos, where the plenoptic video sequences are segmented into image-based rendering (IBR) objects {{each with its}} image sequence, depth map and other relevant information such as shape information. This allows desirable functionalities such as scalability of contents, error resilience, and interactivity with individual IBR objects to be supported. A portable capturing system consisting of two <b>linear</b> <b>camera</b> <b>arrays,</b> each hosting 6 JVC video cameras, was developed to verify the proposed approach. Rendering and compression results of real-world scenes demonstrate the usefulness and good quality of the proposed approach. © 2005 IEEE. published_or_final_versio...|$|R
40|$|We present {{techniques}} {{and a system}} for synthesizing views for video teleconferencing between small groups. In place of replicating one-to-one systems for each pair of users, we create a single unified display of the remote group. Instead of performing dense 3 D scene computation, we use more cameras and trade-off storage and hardware for computation. While it is expensive to directly capture a scene from all possible viewpoints, we have observed that the participants viewpoints usually remain at a constant height (eye level) during video teleconferencing. Therefore, we can restrict the possible viewpoint to be within a virtual plane without sacrificing much of the realism, and in cloning so we significantly {{reduce the number of}} required cameras. Based on this observation, we have developed a technique that uses light-field style rendering to guarantee the quality of the synthesized views, using a <b>linear</b> <b>array</b> of <b>cameras</b> with a life-sized, projected display. Our full-duplex prototype system between Sandia National Laboratories, California and the University of North Carolina at Chapel Hill has been able to synthesize photo-realistic views at interactive rates, and has been used to video conference during regular meetings between the sites...|$|R
30|$|Ga 1 [*]−[*]xAs {{metamorphic}} QD nanostructures {{are interesting}} nanostructures, which allow to have emission or photoresponsivity in the 1.3 - and 1.55 -μm IR ranges [1 – 7]. Furthermore, {{it was reported}} by us earlier that vertical InAs/In 0.15 Ga 0.75 As QD structures can maintain photosensitivity comparable to the GaAs-based ones [5]. However, such metamorphic structures are seldom studied in photoelectric measurements with a lateral geometry, where the photocurrent proceeds through in-plane transport of carriers across channels between two top contacts. Commonly, the QD layers along with the associated WL form these conductivity channels in the lateral geometry-designed GaAs-based structures [38]. Owing to this peculiar type of conductivity, QD photodetectors with the lateral transport {{are believed to have}} potential for a high photoresponsivity [39, 40]. An in-depth study of metamorphic InAs/InGaAs QD nanostructures in the lateral configuration can provide a fundamental knowledge about the photoconductivity (PC) mechanism and efficiency of the in-plain carrier transport. In our recent paper devoted to the defects in metamorphic QD structures [17], we reported lateral PC measurements at low temperatures, considering only the IR spectra edges originating from defects. However, we believe that a proper characterization and fundamental investigation of the structure at room temperature can give precious insights for further improvements of novel light-sensitive devices as near-IR photodetectors, <b>linear</b> <b>arrays,</b> and <b>camera</b> matrixes, by implementing metamorphic QDs.|$|R
40|$|This paper {{presents}} {{a system for}} capturing and rendering a dynamic image-based representation called the plenoptic video. It is a simplified light field for dynamic environments, where user viewpoints are constrained to the camera plane of a <b>linear</b> <b>array</b> of video <b>cameras.</b> Important issues such as multiple camera calibration, real-time compression, decompression and rendering are addressed. The system consists of a <b>camera</b> <b>array</b> of eight Sony CCX-Z 11 CCD cameras and eight Pentium 4 1. 8 -GHz computers connected together through a 100 Base-T local area network. It is possible to perform software-assisted real-time MPEG- 2 compression at a resolution of (720 × 480). Using selective transmission, {{we are able to}} stream continuously plenoptic video with (256 × 256) resolution at a rate of 15 f/s over the network. For rendering from raw data on the hard disk, real-time rendering can be achieved with a resolution of (720 × 480) and a rate of 15 f/s. A new compression algorithm using both temporal and spatial predictions is also proposed for the efficient compression of the plenoptic videos. Experimental results demonstrate the usefulness of the proposed parallel processing based system in capturing and rendering high-quality dynamic image-based representations using off-the-shelf equipment, and its potential applications in visualization and immersive television systems. © 2005 IEEE. published_or_final_versio...|$|R
40|$|<b>Linear</b> <b>array</b> CCD-based {{panoramic}} <b>cameras</b> have a {{high potential}} for measurement applications due to their design in acquiring 360 degree field of views and high information content with up to a Giga-pixel image data in one scan. The best possible accuracy of such a system {{can be obtained by}} a suitable sensor model and by establishing an optimal network following the concept of network design. The influence of different network configurations on the object point coordinates precision was shown in our previous studies with networks of panoramic cameras and panoramic and matrix <b>array</b> CCD <b>cameras.</b> In this paper, the influence of different network configurations onto the determination of Additional Parameters (APs) for self-calibration is demonstrated. The accuracy and precision values of object points and the correlations of APs with respect to the object point coordinates and the exterior orientation parameters are analyzed. By computer simulation and some sensor assumptions, networks of leveled and tilted panoramic camera stations, at the same and at different heights, are analyzed. The datum choice in all network cases is the “free network”, based on the concept of inner constraints. We show that by increasing the tilt of camera stations the correlations of parameters are decreasing, especially the correlations of APs with object space coordinates. Based on these results we suggest tilted panoramic camera stations for the purpose of self-calibration. 1...|$|R
40|$|Work at the University of Arizona and at Lawrence Berkeley Laboratory on the {{development}} of a far infrared <b>array</b> <b>camera</b> for the Multiband Imaging Photometer on the Space Infrared Telescope Facility (SIRTF) is discussed. The camera design uses stacked <b>linear</b> <b>arrays</b> of Ge:Ga photoconductors to make a full two-dimensional array. Initial results from a 1 x 16 array using a thermally isolated J-FET readout are presented. Dark currents below 300 electrons s(exp - 1) and readout noises of 60 electrons were attained. Operation of these types of detectors in an ionizing radiation environment are discussed. Results of radiation testing using both low energy gamma rays and protons are given. Work on advanced C-MOS cascode readouts that promise lower temperature operation and higher levels of performance than the current J-FET based devices is described...|$|R
30|$|A SPAD <b>array</b> <b>camera</b> with single-photon {{sensitivity}} and zero read-out noise {{allows for the}} detection of extremely weak signals at ultra-fast imaging speeds. With temporal resolution in the order of micro-seconds, a SPAD <b>array</b> <b>camera</b> offers great potential for live-cell imaging with super-resolution.|$|R
3000|$|... 2, 1 {{minimization}} [2], and Root-MUSIC [9]. Numerical simulations {{indicate that}} the proposed method outperforms aforementioned algorithms in terms of {{root mean square error}} (RMSE) and recovery rate. The proposed approach is applicable to both uniform <b>linear</b> <b>arrays</b> and nonuniform <b>linear</b> <b>arrays.</b> In addition, numerical simulations show the superiority of proposed method in both uniform <b>linear</b> <b>array</b> (ULA) and nonuniform <b>linear</b> <b>array</b> (NLA) scenarios.|$|R
5000|$|BOL - {{bolometer}} <b>array</b> <b>camera</b> for {{cloud temperature}} measurement ...|$|R
30|$|The {{intrinsic}} photon-counting SPAD <b>array</b> <b>camera</b> offers single-photon sensitivity. The photon counts {{are read}} as digital signals. Therefore, the read-out noise of such cameras is zero (Cova and Ghioni 2011). SPAD <b>array</b> <b>cameras</b> enable high-speed imaging {{up to several}} 1000 fps (Veerappan et al. 2011) which is a capability needed for live-cell optical nanoscopy.|$|R
40|$|A {{radiant energy}} angle sensor is {{provided}} wherein the sensitive portion thereof comprises {{a pair of}} <b>linear</b> <b>array</b> detectors with each detector mounted normal to the other to provide X and Y channels {{and a pair of}} slits spaced from the pair of <b>linear</b> <b>arrays</b> with each of the slits positioned normal to its associated <b>linear</b> <b>array.</b> There is also provided electrical circuit means connected to the pair of <b>linear</b> <b>array</b> detectors and to separate X and Y axes outputs...|$|R
5000|$|In {{some of the}} {{detectors}} of the Infrared <b>Array</b> <b>Camera</b> on the Spitzer Space Telescope ...|$|R
40|$|Abstract—The {{purpose of}} this paper is to {{introduce}} the concept of fractals and its use in antenna arrays for obtaining multiband property. One type of fractals namely, Cantor set is investigated. Cantor set is used in <b>linear</b> <b>array</b> antenna design. Therefore, this array know fractal Cantor <b>linear</b> <b>array</b> antenna. A comparison with conventional non-fractal <b>linear</b> <b>array</b> antenna is made regarding the beamwidth, directivity, and side lobe level. MATLAB programming language version 7. 2 (R 2006 a) is used to simulate the fractal and conventional non-fractal <b>linear</b> <b>array</b> antenna and their radiation pattern. 1...|$|R
40|$|In this paper, {{a uniform}} high {{frequency}} {{formulation of the}} Green’s Function for an arbitrarily contoured finite array of electric dipoles is presented. The planar array is thought as a sequence of parallel finite <b>linear</b> <b>arrays</b> and its field is obtained by numeical superposition of the dominant field contributions from each constituting <b>linear</b> <b>array.</b> The <b>linear</b> <b>array</b> is represented as the difference between two spatially shifted semi-infinite <b>linear</b> <b>arrays.</b> The radiation from each semi-infinite <b>linear</b> <b>array</b> is uniformly asymptotically evaluated, to yield a field representation in terms of truncated Floquet waves and their corresponding tip diffracted contributions. This procedure leads to desccribe the total radiated field as the sum of elementary field contributions arising from the actual rim of the planar array, plus elementary truncated Floquet waves...|$|R
40|$|This paper {{describes}} a new multiple view line-scan imaging technique utilising a single area <b>array</b> <b>camera.</b> This {{is based on}} the concept of treating the <b>camera's</b> area <b>array</b> sensor as a contiguous set of line-scan sensors. The resultant images are accumulated in digital memory whilst the object under inspection is translated through the field of view of the camera. In this way a number of perspective images can be produced. This imaging technique has potential to model a multiple view line-scan x-ray imaging system. 1. Introduction An area array imaging sensor can be considered as a set of contiguous vertical or horizontal <b>linear</b> <b>arrays.</b> In other words the area array imager in combination with a standard lens can be used to produce a `slit' field of view. This can be considered identical to that produced by a dedicated line-scan system [1][2][3]. This is illustrated in Figure 1. Thus by cyclically storing image information from a single predetermined column on the sensor surface a [...] ...|$|R
5000|$|For a <b>linear</b> <b>array</b> with n nodes bisection {{bandwidth}} is one link bandwidth. For <b>linear</b> <b>array</b> {{only one}} link {{needs to be}} broken to bisect the network into two partitions.|$|R
50|$|Whereas {{traditional}} CMMs use a probe {{that moves}} on three Cartesian axes to measure an object’s physical characteristics, portable CMMs use either articulated arms or, {{in the case}} of optical CMMs, arm-free scanning systems that use optical triangulation methods and enable total freedom of movement around the object. Portable CMMs with articulated arms have six or seven axes that are equipped with rotary encoders, instead of linear axes. Portable arms are lightweight (typically less than 20 pounds) and can be carried and used nearly anywhere. However, optical CMMs are increasingly being used in the industry. Designed with compact <b>linear</b> or matrix <b>array</b> <b>cameras</b> (like the Microsoft Kinect), optical CMMs are smaller than portable CMMs with arms, feature no wires, and enable users to easily take 3D measurements of all types of objects located almost anywhere. Certain nonrepetitive applications such as reverse engineering, rapid prototyping, and large-scale inspection of parts of all sizes are ideally suited for portable CMMs. The benefits of portable CMMs are multifold. Users have the flexibility in taking 3D measurements of all types of parts and in the most remote/difficult locations. They are easy to use and do not require a controlled environment to take accurate measurements. Moreover, portable CMMs tend to cost less than traditional CMMs. The inherent trade-offs of portable CMMs are manual operation (they always require a human to use them). In addition, their overall accuracy can be somewhat less accurate than that of a bridge type CMM and is less suitable for some applications.|$|R
