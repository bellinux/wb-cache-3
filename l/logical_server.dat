16|26|Public
50|$|Sharding {{goes beyond}} this: it {{partitions}} the problematic table(s) {{in the same}} way, but it does this across potentially multiple instances of the schema. The obvious advantage would be that search load for the large partitioned table can now be split across multiple servers (logical or physical), not just multiple indexes on the same <b>logical</b> <b>server.</b>|$|E
50|$|Oracle Clusterware is the {{software}} which enables the nodes {{to communicate with}} each other, allowing them to form the cluster of nodes which behaves as a single <b>logical</b> <b>server.</b> Oracle Clusterware is run by Cluster Ready Services (CRS) consisting of two key components: Oracle Cluster Registry (OCR), which records and maintains the cluster and node membership information; voting disk, which polls for consistent heartbeat information from all the nodes when the cluster is running, and acts as a tiebreaker during communication failures.|$|E
5000|$|This {{does not}} mean that there are only 13 {{physical}} servers; each operator uses redundant computer equipment to provide reliable service even if failure of hardware or software occurs. Additionally, all operate in multiple geographical locations using a routing technique called anycast addressing, providing increased performance and even more fault tolerance. An informational homepage exists for every <b>logical</b> <b>server</b> (except G-Root) under the Root Server Technical Operations Association domain with web address in the form [...] http://letter.root-servers.org/, where letter ranges from a to m.|$|E
5000|$|Virtual private server, {{in which}} {{virtualization}} technology is employed {{in order to}} allow multiple <b>logical</b> <b>servers</b> to run on a single physical server ...|$|R
5000|$|Scalable multiserver cluster: {{different}} {{physical and}} <b>logical</b> <b>servers</b> (web, mail, DNS etc.) are managed from one control panel. More servers {{can be added}} on the fly.|$|R
5000|$|... #Caption: A {{map of the}} {{thirteen}} <b>logical</b> name <b>servers,</b> including anycasted instances, {{at the end of}} 2006.|$|R
5000|$|A bare-metal server is a {{physical}} computer server with a single tenant, in contrast to modern forms of virtualization and cloud hosting. Bare-metal servers may run any amount of work for the customer, or may have multiple simultaneous users, but they are dedicated entirely to the customer who is renting them. Unlike many servers in a data centre, {{they are not being}} shared between multiple customers. Each <b>logical</b> <b>server</b> offered for rental is a distinct physical piece of hardware that is a functional server on its own. They are not virtual servers running in multiple on shared hardware.|$|E
50|$|While NetWare 3.x was current, Novell {{introduced}} its first high-availability clustering system, named NetWare SFT-III, which allowed a <b>logical</b> <b>server</b> {{to be completely}} mirrored to a separate physical machine. Implemented as a shared-nothing cluster, under SFT-III the OS was logically split into an interrupt-driven I/O engine and the event-driven OS core. The I/O engines serialized their interrupts (disk, network etc.) into a combined event stream that was fed to two identical copies of the system engine through a fast (typically 100 Mbit/s) inter-server link. Because of its non-preemptive nature, the OS core, stripped of non-deterministic I/O, behaves deterministically, like a large finite state machine. The outputs of the two system engines were compared to ensure proper operation, and two copies fed back to the I/O engines. Using the existing SFT-II software RAID functionality present in the core, disks could be mirrored between the two machines without special hardware. The two machines could be separated {{as far as the}} server-to-server link would permit. In case of a server or disk failure, the surviving server could take over client sessions transparently after a short pause since it had full state information. SFT-III was the first NetWare version able to make use of SMP hardware - the I/O engine could optionally be run on its own CPU. NetWare SFT-III, ahead of its time in several ways, was a mixed success.|$|E
40|$|Abstract—In {{order to}} offer {{backward}} and forward secrecy for multicast applications (i. e., {{a new member}} cannot decrypt the multicast data sent before its joining and a former member cannot decrypt the data sent after its leaving), the data encryption key has to be changed whenever a user joins or leaves the system. Such a change {{has to be made}} known to all the current users. The bandwidth used for such re-key messaging can be high when the user pool is large. In this paper, we propose a distributed servers approach to minimize the overall system bandwidth (and complexity) by splitting the user pool into multiple groups each served by a (<b>logical)</b> <b>server.</b> After presenting an analytic model for the system based on a hierarchical key tree, we show that there is an optimal number of servers to achieve minimum system bandwidth. As the underlying user traffic fluctuates, we propose a simple dynamic scheme with low overhead where a physical server adaptively splits and merges its traffic into multiple groups each served by a <b>logical</b> <b>server</b> so as to minimize its total bandwidth. Our results show that a distributed servers approach is able to substantially reduce the total bandwidth required as compared with the traditional singleserver approach, especially for those applications with a large user pool, short holding time, and relatively low bandwidth of a data stream, as in the Internet stock quote applications. Index Terms—Distributed servers approach, key tree, multicast security, re-key messaging, split-and-merge scheme. I...|$|E
40|$|This {{paper is}} to address the {{resource}} utilization problem for high-performance data processing applications in a large IDC (Internet Data Center) environment. On one hand, each application calls for a best-fit infrastructure with a specific compute-storage ratio, to achieve the highest resource utilization while meeting its performance requirement. And such a ratio varies among applications. On the other hand, IDCs have always been trying to unify the infrastructures for lower TCO (Total Cost of Ownership). Therefore, it’s {{getting harder and harder}} to adapt infrastructures to application needs. This issue results in significant waste of investment in large IDCs. Furthermore, the high-performance data processing applications always require the infrastructure to offer as high compute-storage performance as a DAS (Direct Attached Storage) server, which remains as a great challenge when addressing the resource utilization problem. This paper, as part of Baidu-Intel joint research program, first evaluates the state-of-the-art solutions, and then introduces a more practical infrastructure, the core of which is rack-scale storage fabric. This infrastructure disaggregates compute units and storage units by a SAS (Serial Attached SCSI) fabric, and allows to compose <b>logical</b> <b>servers</b> with arbitrary computer-storage ratios within a rack. And the experiments in Baidu’s research environment show that the <b>logical</b> <b>servers</b> exhibit the similar throughput/IOPS as DAS servers, and also their compute-storage ratios can best-fit the needs of different Hadoop applications...|$|R
40|$|In {{this paper}} we“reverse-engineer”the YouTube video {{delivery}} cloud {{by building a}} distributed measurement infrastructure. Through extensive data collection and analysis, we deduce the key design features underlying the YouTube video delivery cloud. The design of the YouTube video delivery cloud consists of three major components: a “flat ” video id space, multiple DNS namespaces reflecting a multi-layered logical organization of video servers, and a 3 -tier physical cache hierarchy. By mapping the video id space to the <b>logical</b> <b>servers</b> via consistent hashing and cleverly leveraging DNS and HTTP re-direction mechanisms, such a design leads to a scalable, robust and flexible content distribution system. Categories andSubject Descriptor...|$|R
40|$|In {{this paper}} {{we set out}} to “reverse-engineer ” the YouTube video {{delivery}} cloud by building a globally distributed active measurement infrastructure. Through careful and extensive data collection, analysis and experiments, we deduce the key design features underlying the YouTube video delivery cloud. The design of the YouTube video delivery cloud consists of three major components: a “flat ” video id space, multiple DNS namespaces reflecting a multi-layered logical organization of video servers, and a 3 -tier physical cache hierarchy. By mapping the video id space to the <b>logical</b> <b>servers</b> via a fixed hashing and cleverly leveraging DNS and HTTP redirection mechanisms, such a design leads to a scalable, robust and flexible content distribution system. 1...|$|R
40|$|In {{order to}} offer {{backward}} and forward secrecy for multicast applications (i. e., {{a new member}} cannot decrypt the multicast data sent before its joining and a former member cannot decrypt the data sent after its leaving), the data encryption key has to be changed whenever a user joins or leaves the system. Such a change {{has to be made}} known to all the current users. The bandwidth used for such re-key messaging can be high when the user pool is large. In this paper, we propose a distributed servers approach to minimize the overall system bandwidth (and complexity) by splitting the user pool into multiple groups each served by a (<b>logical)</b> <b>server.</b> After presenting an analytic model for the system based on a hierarchical key tree, we show that there is an optimal number of servers to achieve minimum system bandwidth. As the underlying user traffic fluctuates, we propose a simple dynamic scheme with low overhead where a physical server adaptively splits and merges its traffic into multiple groups each served by a <b>logical</b> <b>server</b> so as to minimize its total bandwidth. Our results show that a distributed servers approach is able to substantially reduce the total bandwidth required as compared with the traditional single-server approach, especially for those applications with a large user pool, short holding time, and relatively low bandwidth of a data stream, as in the Internet stock quote applications...|$|E
40|$|Your {{company has}} elected to deploy the SAS ® Enterprise BI Server as their {{enterprise}} wide analytical platform. Congratulations! You are the SAS Administrator. This {{is a good}} thing, but {{there is much more}} involved than loading software on a server. You will need to determine how to make SAS BI work with your company’s security, quality assurance, and application deployment guidelines. When we started this journey at Blue Cross and Blue Shield of Minnesota (BCBSM), we found a number of security and operational requirements to consider: • The system will support both internal & external customer groups with thousands of named users. • The system will support separate <b>logical</b> <b>server</b> instances for Development, Integration testing, Quality Assurance testing and Production. • The system will support multiple development teams...|$|E
40|$|Indiana University-Purdue University Indianapolis (IUPUI) In today's {{internet}} {{world with}} such a high traffic, it becomes inevitable to have multiple servers representing a single <b>logical</b> <b>server</b> to share enormous load. A very common network configuration consists of multiple servers behind a load balancer. The load balancer determines which server would service a clients request or incoming load from the client. Such a hardware is expensive, runs a fixed policy or algorithm and is a single point of failure. In this paper, we will implement and analyze an alternative load balancing architecture using OpenFlow. This architecture acquires flexibility in policy, costs less and {{has the potential to}} be more robust. This paper also discusses potential usage of OpenFlow based load balancing for media gateway selection in SIP-PSTN networks to improve VoIP performance...|$|E
40|$|Current {{web server}} farms have simple {{resource}} allocation models. One such model used is to dedicate a server {{or a group}} of servers for each client. Another model partitions physical <b>servers</b> into <b>logical</b> <b>servers</b> and assigns one to each client. However, both these approaches prevent resource sharing and reduce the ability to handle peak loads except at the cost of having to reserve resources that will lie idle most of the time. Yet another model allows clients to be served via multiple servers by using load-balancing techniques to distribute incoming requests. This implies that every client's application must reside and remain active on multiple servers. For complex applications, this approach uses up significant server resources...|$|R
30|$|The aim of policy-based {{approaches}} for resource management (such as within IEEE 1900.4 [8]) is to decouple the policy derivation and evaluation process from the policy enforcement point. In this manner, it becomes possible to devolve decision-making functions from one <b>logical</b> entity (<b>server)</b> to another (client terminals).|$|R
40|$|Virtual Machines {{have become}} a {{standard}} unit of resource allocation for cloud environment, and virtualization {{has been used for}} implementing cloud infrastructures because of its natural ability to decouple the physical hardware from <b>logical</b> <b>servers.</b> The increasing density of computational power allows packing a significant number of virtual machines on a single node, leading to an exponential growth of the number of <b>logical</b> <b>servers</b> to be manages by the cloud infrastructure. To investigate smart policies to administer such a large number of virtual machines we have developed Octopus, a lightweight system for scheduling virtual machines on a cluster of hypervisors. Octopus has been implemented using Microsoft Hyper-V in order to exploit the WMI interface to control the hypervisor programmatically with the F# programming language. The original goal was to design a system capable of moving virtual machines across different computing nodes in order to optimize the workload and pack computations to save energy by turning off nodes. Moreover, the VM creation is under control of the final user through a web page where it is possible to ask for specific requirements about computing cores, memory and the OS image to be provisioned. More recently, our investigation has focused on the possibility of using expert systems to express complex policies and to govern this ever-increasing set of virtual machines. For this reason, we have embedded the CLIPS expert system inside Octopus in order to rely on a full rule-based expert system engine to define the resource management policies: the Octopus code asserts facts about VMs in the CLIPS systems and rules access system primitives exposed as functions invokes by triggered rules. The well-known RETE algorithm ensures a fast execution of policy while ensuring the ability to define policies that may even contain conflicting rules...|$|R
40|$|This {{paper will}} {{introduce}} {{the reader to}} the design functionality of the Cisco 12000 GSR product line, {{with a focus on}} the Cisco 12016 GSR. With more than 400 customer backbones utilizing the Cisco 12000 GSR today, the full suite of NEBS, 1 :N clocking fully resilient switch fabric, online insertion and removal (OIR) of modules, and distributed forwarding information across multiple line cards ensure that the Cisco 12000 GSR delivers IP carrier-class operations. In addition, Layer 3 protection switching can be designed to achieve much higher reliability than the individual network elements. This system approach includes the design principles of high availability, topology, peering, <b>logical</b> <b>server</b> placement, and interconnection features such as SONET/SDH automatic protection switching/MSP (APS/MSP), IP load-sharing across parallel networks paths, and fast restoration features in MPLS network...|$|E
40|$|The {{problem of}} sharing logical {{resources}} in multiprocessor systems cannot be satisfactorily solved using solutions intended for single processor systems. Sharing logical resources on multiprocessors is intrinsically exposed to parallel contention, which {{gives rise to}} high time penalty owing to the necessary serialization of access. Partitioned scheduling approaches help reduce contention, but {{at the cost of}} collocating every shared resource together with all of its users, which makes the partitioning problem harder. The concept of <b>logical</b> <b>server</b> used in hybrid (semi-partitioned) multiprocessor scheduling, a self-sucient entity that can schedule a set of tasks and become a schedulable entity itself, may come handy instead. Logical servers can be used to isolate and reduce the parallelism of collaborative tasks without suffering from the limitations of partitioned approaches. In this work, we provide a generalization of RUN, an optimal multiprocessor scheduling algorithm based on logical servers, for handling shared resources. We implement and evaluate a simple spin-based locking protocol that leverages servers to group collaborative tasks and reduces parallel contention...|$|E
40|$|Abstract: This paper {{presents}} a developed cloud based network architecture that {{is driven by}} discrete event process algorithms with emphasis on Integrated Service OpenFlow Load Balancer, (ISOLB). Existing cloud based architectures still uses either three-layer or four-layer architecture which have inherent defects. We explored the various contributions in literature to evolve an Enterprise Energy Analytic Tracking Cloud portal (EEATCP) which must sit on a Two-tier cloud network for optimal service delivery. The proposed architecture offered fault tolerance, improved throughput, scalability, cost-effective, service-oriented, and services responsiveness. The system specifications were derived based on a previous study {{as well as some}} production Testbeds which had deficiencies. The functional subsystems of the architecture were described while implementing it on Riverbed Modeller Academic Edition 17. 7 environment. Using the algorithms, a performance evaluation in the context of ISOLB vis-à-vis the system‟s throughput, delay, resource utilization and fault tolerance is presented. We showed that with an access layer (User base) procedure, distributed gateway decoding with Infinite M/M/ 1 Queue model, ISOLB, Job Queue Messaging Protocol (JQMP), Server Infrastructure Job execution, <b>logical</b> <b>Server</b> Mapping with VLAN Recursive Construction Algorithms, and logical Isolation of DCCN Server Cluster Architecture, a robust two-tier high performance network is guarantee...|$|E
50|$|The area server is an {{abstraction}} {{that represents the}} server that provides services for a specification area (Availability Management Framework, Cluster Membership Service, Checkpoint Service, and so on). Each area has a separate <b>logical</b> area <b>server,</b> although the implementer is free to create a separate physical module for each area server or combine one or more area servers into a single physical module.|$|R
40|$|Organizations {{that employ}} the Software as a Service model {{are looking to}} reduce the overall costs in {{supporting}} their “customers. ” In the government sector, an Information Technology organization {{may be responsible for}} providing data integration, business intelligence, or analytic applications to several consuming agencies. A standardized set of hardware and software configurations as well a set of best practices allows an organization to realize the benefits of centrally managed Software as a Service. The popularity of logical partition and container capabilities provided by hardware and operating system vendors allows larger servers to be divided into many smaller “logical ” servers providing consistency and ease of maintenance across the deployed software and applications. Sharing software binaries across the <b>logical</b> <b>servers</b> allows for enterprise-wide management of hot fixes and upgrades. We will examine some of the advantages and benefits of deploying the platform for SAS ® Business Analytics in a centrally managed environment. A discussion of some best practices and past experiences will also be presented...|$|R
40|$|Fully virtualized systems offer {{significant}} commercial {{advantages in}} certain markets by allowing users to consolidate many small servers onto a single large system that offers lower total cost {{and more efficient}} and flexible use of computing resources. With full virtualization, {{which means that the}} underlying hypervisor virtualizes processor, memory and I/O resources, the system becomes even more flexible since there is no fixed assignment of resources to partitions and operating system images. However, with such systems, the costs associated with failures and power management are much higher. For example, if a virtualized system fails, rather than losing a single system, the customer loses all of the consolidated system images, often on the order of 10 or even 10 <b>logical</b> <b>servers.</b> Similarly, unless the larger system offers power-management features comparable to those found in smaller systems, its power costs and power-related problems such as heat and potential failures are higher than the systems that it replaces...|$|R
40|$|Abstract. This paper {{describes}} a novel technique for establishing a virtual file {{system that allows}} data to be transferred user-transparently and on-demand across computing and storage servers of a computational grid. Its implementation is based on extensions to the Network File System (NFS) that are encapsulated in software proxies. A key differentiator between this approach and previous work {{is the way in}} which file servers are partitioned: while conventional file systems share a single (<b>logical)</b> <b>server</b> across multiple users, the virtual file system employs multiple proxy servers that are created, customized and terminated dynamically, for the duration of a computing session, on a peruser basis. Furthermore, the solution does not require modifications to standard NFS clients and servers. The described approach has been deployed in the context of the PUNCH network-computing infrastructure, and is unique in its ability to integrate unmodified, interactive applications (even commercial ones) and existing computing infrastructure into a network computing environment. Experimental results show that: (1) the virtual file system performs well in comparison to native NFS in a local-area setup, with mean overheads of 1 and 18 %, for the single-client execution of the Andrew benchmark in two representative computing environments, (2) the average overhead for eight clients can be reduced to within 1 % of native NFS with the use of concurrent proxies, (3) the wide-area performance is within 1 % of the local-area performance for a typical compute-intensive PUNCH application (SimpleScalar), while for the I/O-intensive application Andrew the wide-area performance is 5. 5 times worse than the local-area performance...|$|E
40|$|Multicast is an {{efficient}} technique for delivering data {{to a large}} group of users in multimedia applications such as the Internet stock quote, Internet radio, audio/music delivery, video surveillance, etc. Many of these applications require data confidentiality. One of the critical problem of data confidentiality is key management for backward and forward secrecy (i. e., a new member cannot decrypt the multicast data sent before its joining and a former member cannot decrypt the data sent after its leaving). In order to offer backward and forward secrecy for some multicast applications, data encryption key has to be changed whenever a user joins or leaves the system, and made known to the current users. The bandwidth used for such re-key messaging can be high when the user pool is large and the group is highly dynamic. In this thesis, we propose a distributed server approach to minimize the overall system bandwidth (and hence complexity) by splitting the user pool into multiple groups each served by a (<b>logical)</b> <b>server.</b> After presenting a simple model for the system based on a hierarchical key tree, we show that there is an optimal number of servers to achieve minimum system bandwidth. As the underlying user traffic fluctuates, we propose a simple dynamic scheme with low overhead in which the servers adaptively split and merge user groups according to user traffic to maintain such an optimum. Our results show that a distributed server approach is able to substantially reduce the total bandwidth required (by more than 30 %) as compared to the traditional single-server approach, especially for those applications with large user pool and short holding time, relatively low bandwidth of a data stream, and widely fluctuating user traffic (e. g., an Internet stock quote application) ...|$|E
40|$|The {{objective}} of the novel statistical distribution independent transfer policy model (SDITPM) {{is to reduce the}} service roundtrip time (RTT) in object-based computing effectively. As a result it shortens program execution and helps time-critical applications meet predefined deadlines effectively. The SDITPM achieves its objective through load balancing, which is a natural outcome of guided object/agent mobility. Applications running on the Internet are object-based, and the cognate objects interact in a client/server relationship over logical TCP (Transmission Control Protocol) channels. Service RTT is the interval between the point when a client has made a service request and the moment when it has received the result correctly. If the host of a <b>logical</b> <b>server</b> is congested, the average service RTT would be prolonged by delays of various origins, including queuing, context switching, and retransmissions. If the server can migrate from its congested host to a less busy node, the service RTT is automatically reduced due to the effect of load balancing, provided that the object mobility is properly guided. The SDITPM guides the object migration process by leveraging primary metrics (e. g. server's queue length). From every primary metric the corresponding secondary ones are derived for the sake of proportional (P), derivative (D), and integral (I) controls. The statistical leveraging operation {{is the responsibility of the}} M 3 RT module in the SDITPM framework, which treats every primary metric as a waveform. The secondary metrics to be derived are: (a) the "current estimated mean over the last estimated value" ratio for proportional (P) control, (b) the "current estimated rate of change" for derivative (D) control, and (c) the P and D deviation errors (DE) for integral (I) control. In every transfer policy decision making cycle the SDITPM combines the P, I, and D controls selectively to compute the overall transfer probability (TP 0) that decides if object migration should occur. An affirmative transfer policy decision by SDITPM is the first guidance to a successful object migration, which must be substantiated later by the location policy that provides the necessary second guidance of pinpointing the suitable host destination. The motivation of the SDITPM is to effect sound transfer policy decisions. Department of Computin...|$|E
40|$|The article {{reviews the}} {{principle}} and methods {{for creating the}} models of distributed engineering knowledge as well as software implementation of such models {{in the form of}} the Information and <b>Logical</b> Table <b>Server</b> (SILT) resulted in development of a cluster based on the WCF (Windows Communication Foundation) technology and reciprocity of the SILT elements. The cluster might be entered by means of a special web application which is an element of SILT using every device with a web browser and access to the internet. </p...|$|R
40|$|Abstract: In today’s {{high-traffic}} internet, it {{is often}} desirable to have multiple servers representing a single <b>logical</b> destination <b>server</b> to share load. A typical configuration consists of multiple servers behind a load-balancer which would determine which server would service a client’s request. Such hardware is expensive, has a rigid policy set, and is a single point of failure. In this paper we implement and evaluate an alternative load-balancing architecture using an OpenFlow switch connected to a NOX controller, which gains flexibility in policy, costs less, and {{has the potential to}} be more robust to failure with future generations of switches...|$|R
5000|$|All Internet root nameservers are {{implemented}} as clusters of hosts using anycast addressing. All 13 root servers A-M exist in multiple locations, with 11 on multiple continents. (Root servers B and H exist in two U.S. locations. [...] ) The servers use anycast address announcements {{to provide a}} decentralized service. This has accelerated the deployment of physical (rather than <b>logical)</b> root <b>servers</b> outside the United States. [...] documents the use of anycast addressing to provide authoritative DNS services. Many commercial DNS providers have switched to an IP anycast environment to increase query performance, redundancy, and to implement load balancing.|$|R
40|$|Abstract—in {{high-traffic}} Internet today, it {{is often}} desirable to have multiple servers that represent a single <b>logical</b> destination <b>server</b> to share the load. A typical configuration comprises multiple servers behind a load balancer that would determine which server would serve the request of a client. Such equipment is expensive, has a rigid set of rules, and is a single point of failure. In this paper, I propose an idea and design for an alternative load-balancing architecture {{with the help of}} an OpenFlow switch connected to a NOX controller that gains political flexibility, less expensive, and has the potential to be more robust to failure with future generations of switches I...|$|R
3000|$|The Forensic Investigations Process Model in Cloud Environments (FIPMCE) [15] was {{introduced}} {{based on the}} DFPM model. Due {{to the evolution of}} cloud computing the processes were changed to apply basic forensic principles and processes. It consists of four processes including Determining the purpose of the forensics requirement, Identifying the types of cloud services (SaaS, IaaS, PaaS), Determining the type of background technology used, and Examining the various physical and <b>logical</b> locations, (client, <b>server</b> or developer sides). The FIPMCE is given as: [...]...|$|R
40|$|Abstract 1 Today, most of {{the popular}} web sites are served from single locations. This basic Web {{client-server}} model is easy to deploy and maintain and thus is very successful. It suffers, however, from several efficiency and availability problems. This paper presents Walrus, a low-latency, high-throughput Web service that addresses some of these problems. Under Walrus, a single <b>logical</b> Web <b>server</b> can be replicated to several clusters of identical servers where each cluster resides in {{a different part of}} the Internet. An important aspect of Walrus is its ability to transparently direct the web browser to the best replica without any changes to the web server, web client, and network infrastructure. “Best ” is a relative term, dependent on where the client is located on the network, the load on each replica, and more. Walrus deploys an elegant algorithm that balances these considerations. 1...|$|R
50|$|There are 13 <b>logical</b> root name <b>servers</b> specified, with <b>logical</b> {{names in}} the form letter.root-servers.net, where letter ranges from a to m. The choice of {{thirteen}} name servers was made because of limitations in the original DNS specification, which specifies a maximum packet size of 512 bytes when using the User Datagram Protocol (UDP). Technically however, fourteen name servers fit into an IPv4 packet. The addition of IPv6 addresses for the root name servers requires more than 512 bytes, which is facilitated by the EDNS0 extension to the DNS standard.|$|R
40|$|Today, {{practically}} {{all of the}} popular web sites are served from single locations. This basic Web client-server model is easy to deploy and maintain and thus is very successful. It suffers, however, from several efficiency and availability problems. This paper presents Walrus, a low-latency, high-throughput Web service that addresses some of these problems. Under Walrus, a single <b>logical</b> Web <b>server</b> can be replicated to several clusters of identical servers where each cluster resides in {{a different part of}} the Internet. An important aspect of Walrus is its ability to transparently direct the web browser to the best replica without any changes to the web server, web client, and network infrastructure. "Best" is a relative term, dependent on where the client is located on the network, the load on each replica, and more. Walrus deploys an elegant algorithm that balances these considerations. Implementing this architecture for popular Web sites will result in a better response time and a hi [...] ...|$|R
40|$|Remote Procedure Calls (RPC) {{provide a}} high level {{paradigm}} which allows distributed applications {{to communicate with each}} other. Current implementations of RPC are mostly transport dependent. This paper describes the architecture, design, and implementation of transport independent RPC. Transport independent RPC depends upon three main modules: Transport Layer Interface/STREAMS, Network Selection, and the Name to Address Translation Mechanism. These mechanisms provide a very powerful way to dynamically select suitable transports at run-time. With these mechanisms and with the assistance of an RPC registry service, rpcbind, transport independent RPC implements a <b>logical</b> client to <b>server</b> communication system designed specifically for the support of transport independent distributed applications...|$|R
