2|31|Public
40|$|D. Tech. Electrical Engineering. Discusses the {{objective}} {{of this study is to}} develop an embedded generator-pump set topology using a wound rotor induction machine using the doubly fed induction generator concept, and a synchronous machine electrically and mechanically coupled to it, powering its magnetisation circuit. An adjustable pitch pump is also coupled to the generating set on the same shaft to provide an embedded generating-pumping solution that can provide co-incident generating ans pumping functions. The research objectives are as follows: to develop an overall plant topology, to identify plant attributes necessary for proper functionality of the proposed plant, to identify a pumping/generation topology that meets the required electro-mechanical and overall topological <b>layout</b> <b>attribute</b> requirements, to develop a primitive mathematical model of the plant that provides insight into fundamental physical behaviour of the plant, to investigate the stability issues arising from the electromechanical coupling of the two machines used, to establish controllability of the proposed configuration, to identify influencing factors on the stable operation of the proposed plant, to develop an overall system model for simulation. This also entails developing a suitable mathematical model for the variable pitch pump and to simulate the system steady state and dynamic behaviour...|$|E
40|$|The flow of {{information}} is continually increasing due to the ubiquitous use {{of information}} technology. Information communication and distribution is highly automated {{to meet the demand}} for an unobtrusive information flow. Servers, systems and applications overtake the tasks of gathering, storing, distributing and selecting the diverse information requests, though the task of presenting the information in a communicatively effective manner is still a manual tasks handled by professionals. In certain contexts, such as the distribution of dynamically generated content by web servers, the manual tasks restrict the system from performing as efficiently as possible. At the moment there is a great potential for production efficacy gain by automating the workflow to some extent. The system utilization of a document layout generator may cover some designer functions and thereby reducing task redundancy related to the presentation of information. Document layout automation algorithms are typically implemented with weighting metrics for the purpose of quantifying layout characteristics, also called layout attributes, of generated layout in the perspective of visual quality. Several different weighting metrics and algorithms have been studied and implemented in the past years, but still they fail to produce layouts matching the general performance of a designer in practice. Literature suggests that many of the weighting metrics, despite their advanced mathematical implementation, lack the implementation of a persistent set of preference attributes. Weighting metrics based on an extended set of preference attributes related to designer preferences may further improve the performance of a document layout automation algorithm. The scope of this thesis is to establish knowledge about the utilization of document <b>layout</b> <b>attribute</b> operators in document layout automation. How do document layout automation system with an extended attribute operator set perform related to layout quality? The study presented in this thesis reveals that implementing operators for the layout characteristics alignment and equilibrium in layout generators doesn’t automatically enhance the visually perceived quality of the layout generated. A second scope of this thesis is to generalize the modeling paradigms derived from existing literature related to document layout automation systems. Several systems have been proposed for solving layout automation related problems, but still {{there is a lack of}} literature related to the generalization of layout automation system models. Is it possible to recognize modeling patterns can be generalized an utilized in a wide range of layout automation systems, applications and / or components? We propose a layout automation system modeling framework based on a grounded theory study of related literature. The presented proof of concept system LaG is modeled based on this modeling framework and provides an affirmation of the modeling framework flexibility regarding layout automation technology and architectural modeling patterns...|$|E
40|$|This study explores how the typographic {{layout of}} {{information}} influences readers' impressions of magazine contents pages. Thirteen descriptors {{were used in}} a paired comparison procedure that assessed whether participants' rhetorical impressions {{of a set of}} six controlled documents change in relation to variations in layout. The combinations of <b>layout</b> <b>attributes</b> tested were derived from the structural attributes associated with three patterns of typographic differentiation (high, moderate, and low) described in a previous study (see Moys, 2014). The content and the range of stylistic attributes applied to the test material were controlled in order to focus on <b>layout</b> <b>attributes.</b> Triangulation of the quantitative and qualitative data indicates that, even within the experimental confines of limited stylistic differentiation, the <b>layout</b> <b>attributes</b> associated with patterns of high, moderate, and low typographic differentiation do influence readers' rhetorical judgments. In addition, the findings emphasize the importance of considering inter-relationships between clusters of typographic attributes rather than testing isolated variables...|$|R
40|$|Today's {{autonomous}} vehicles rely {{extensively on}} high-definition 3 D maps {{to navigate the}} environment. While this approach works well when these maps are completely up-to-date, safe autonomous vehicles {{must be able to}} corroborate the map's information via a real time sensor-based system. Our goal in this work is to develop a model for road layout inference given imagery from on-board cameras, without any reliance on high-definition maps. However, no sufficient dataset for training such a model exists. Here, we leverage the availability of standard navigation maps and corresponding street view images to construct an automatically labeled, large-scale dataset for this complex scene understanding problem. By matching road vectors and metadata from navigation maps with Google Street View images, we can assign ground truth road <b>layout</b> <b>attributes</b> (e. g., distance to an intersection, one-way vs. two-way street) to the images. We then train deep convolutional networks to predict these road <b>layout</b> <b>attributes</b> given a single monocular RGB image. Experimental evaluation demonstrates that our model learns to correctly infer the road attributes using only panoramas captured by car-mounted cameras as input. Additionally, our results indicate that this method may be suitable to the novel application of recommending safety improvements to infrastructure (e. g., suggesting an alternative speed limit for a street) ...|$|R
40|$|The {{essential}} <b>layout</b> <b>attributes</b> of {{a visual}} table {{can be defined}} by the location of four critical grid cells. Although these critical cells can often be located by automated analysis, some means of human interaction is necessary for correcting residual errors. VeriClick is a macro-enabled spreadsheet interface that provides ground-truthing, confirmation, correction, and verification functions for CSV tables. All user actions are logged. Experimental results of seven subjects on one hundred tables suggest that VeriClick can provide a ten- to twenty-fold speedup over performing the same functions with standard spreadsheet editing commands...|$|R
50|$|Southern Cemetery {{originally}} occupied 40 acre of {{land that}} cost Manchester Corporation £38,340 in 1872. Its cemetery buildings were designed by architect H J Paull and its <b>layout</b> <b>attributed</b> to the city surveyor, James Gascoigne Lynde. The cemetery opened on 9 October 1879 and had mortuary chapels for Anglicans, Nonconformists, and Roman Catholics linked by an elliptical drive and a Jewish chapel at the west corner of the site. The original cemetery is registered by English Heritage in the Register of Historic Parks and Gardens for its historic interest and the mortuary chapels and other structures are listed buildings. It is the largest municipal cemetery in the United Kingdom.|$|R
40|$|AIDA {{is a set}} of {{software}} tools based upon the programming system MUMPS. The AIDA tools are used to build (medical) information systems with less programming effort than in conventional programming and with more user involvement. A lot of the system characteristics can interactively be defined by the end-user. He interacts with the package by means of a CRT to define screen <b>layouts,</b> <b>attributes,</b> relations and database transactions. AIDA contains tools for program development as well as for maintenance. It is an ideal tool for prototyping, but for operational systems special tools to optimize for speed have to be applied. In the demonstration a system will be defined to show the construction of a small information system with emphasis on data entry...|$|R
50|$|Manchester Southern Cemetery {{originally}} {{occupied a}} 100 acre plot of land, {{in what was}} then Withington, that cost Manchester Corporation £38,340 in 1872. Its cemetery buildings were designed by architect H. J. Paull and its <b>layout</b> <b>attributed</b> to the city surveyor, James Gascoigne Lynde. The cemetery opened on 9 October 1879 and had mortuary chapels for Anglicans, Nonconformists, and Roman Catholics linked by an elliptical drive and a Jewish chapel at the west corner of the site. The original cemetery is registered by English Heritage in the Register of Historic Parks and Gardens for its historic interest and the mortuary chapels and other structures are listed buildings. The site was expanded by the purchase of 90 acre {{on the opposite side of}} Nell Lane in 1926, the first section of which opened in 1943. Some of the 1926 purchase has been developed for housing and some is occupied by allotments.|$|R
40|$|As {{our human}} spaceflight {{missions}} change as we reach towards Mars, {{the risk of}} an adverse behavioral outcome increases, and requirements for crew health, safety, and performance, and the internal architecture, will need to change to accommodate unprecedented mission demands. Evidence shows that architectural arrangement and habitability elements impact behavior. Net habitable volume is the volume available to the crew after accounting for elements that decrease the functional volume of the spacecraft. Determination of minimum acceptable net habitable volume and associated architectural design elements, as mission duration and environment varies, is key to enabling, maintaining, andor enhancing human performance and psychological and behavioral health. Current NASA efforts to derive minimum acceptable net habitable volumes and study the interaction of covariates and stressors, such as sensory stimulation, communication, autonomy, and privacy, and application to internal architecture design <b>layouts,</b> <b>attributes,</b> and use of advanced accommodations will be presented. Furthermore, implications of crew adaptation to available volume as they transfer from Earth accommodations, to deep space travel, to planetary surface habitats, and return, will be discussed...|$|R
40|$|Researchers often {{summarize}} {{their work}} {{in the form of}} posters. Posters provide a coherent and efficient way to convey core ideas from scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including panel <b>layout</b> and <b>attributes</b> of each panel, are learned and inferred from data. Then, given inferred <b>layout</b> and <b>attributes,</b> composition of graphical elements within each panel is synthesized. To learn and validate our model, we collect and make public a Poster-Paper dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach. Comment: in Proceedings of the 30 th AAAI Conference on Artificial Intelligence (AAAI' 16), Phoenix, AZ, 201...|$|R
5000|$|Neoclassical Сhurch of Theotokos of Vladimir in Dolgoprudny (1772-1777), with {{an unusual}} {{triangular}} <b>layout,</b> has been <b>attributed</b> to either Kazakov or Bazhenov despite complete lack of written evidence. Layout {{of the church is}} most likely inspired by the Temple of War by Jean-Francois Nefforge.|$|R
40|$|The {{process of}} {{retrieving}} the exact data {{that is needed}} by the user is known as Data Extraction. Extracting the exact data from web pages is a complex problem because the data’s in the database present in a complex structure. Many techniques have been proposed {{but all of them}} have some limitations because they are web page programming language dependent. In this paper a vision based approach that is web page programming language independent is proposed. Here the visual features like web page <b>layout,</b> font <b>attributes</b> and image size are used to retrieve the data in an effective manner. Keywords-Data extraction, Visual feature...|$|R
5000|$|Barlow Respiratory Hospital {{is unique}} in that it {{maintains}} a 25-acre campus with cottages, a library, the main hospital, and a community hall. Old chicken coops are still standing {{on the outskirts of}} the campus. This unique <b>layout</b> can be <b>attributed</b> to the hospital's history as a tuberculosis sanatorium where patients lived for several years.|$|R
50|$|Visual {{representation}} of social networks {{is important to}} understand the network data and convey the result of the analysis. Numerous methods of visualization for data produced by social network analysis have been presented. Many of the analytic software have modules for network visualization. Exploration of the data is done through displaying nodes and ties in various <b>layouts,</b> and <b>attributing</b> colors, size and other advanced properties to nodes. Visual representations of networks may be a powerful method for conveying complex information, but care should be taken in interpreting node and graph properties from visual displays alone, as they may misrepresent structural properties better captured through quantitative analyses.|$|R
5000|$|I {{want to put}} {{on record}} that I'm not an old reprobate longing for a return of the good old days. I'm more of an old father who is {{disappointed}} that his kids are only reaching 98 percent of their potential and wants them to reach 101 percent. My message to young designers is this: Look kids, you can do better, {{but the only way}} to achieve your potential is to go back to - and understand - the basics. That sounds boring, but it's reality. The seminal work on the Gutenberg diagram (or Z pattern <b>layout)</b> is <b>attributed</b> to the typographer Edmund Arnold, who is said to have developed the concept in the 1950s.|$|R
40|$|International audienceNode-link {{infographics}} {{are visually}} very rich and can communicate messages effectively, {{but can be}} very difficult to create, often involving a painstaking and artisanal process. In this paper we present an investigation of node-link visualizations for communication and how to better support their creation. We begin by breaking down these images into their basic elements and analyzing how they are created. We then present a set of techniques aimed at improving the creation workflow by bringing more flexibility and power to users, letting them manipulate all aspects of a node-link diagram (<b>layout,</b> visual <b>attributes,</b> etc.) while taking into account the context in which it will appear. These techniques were implemented in a proof-of-concept prototype called GraphCoiffure, which was designed as an intermediary step between graph drawing/editing software and image authoring applications. We describe how GraphCoiffure improves the workflow and illustrate its benefits through practical examples. Categories and Subject Descriptors (according to ACM CCS) : H. 5. 2 [Information Interfaces and Presentation]: User Interfaces—Graphical user interfaces (GUI...|$|R
5000|$|A {{church at}} the site was first {{constructed}} for the nuns of an Order of St Clare in the 14th century by Robert of Anjou and his wife. His wife, once widowed, entered this monastery. The Blessed Costanza Starace also once resided here. The initial <b>layout</b> was <b>attributed</b> to Masuccio II, but further reconstructions and decorations proceeded in 1646 and 1750. The architects Bartolomeo Vecchione and Crescenzo Torchese {{were involved in the}} latter reconstructions, including the facade. Much of the artwork has been moved or lost. The ceiling canvases in the nave were attributed to Balducci and his disciples. Other works made for the church or chapels were attributed to Andrea Malinconico; Giovanni Battista Caracciolo; a Virgin with St Anthony of Padua and Elizabeth of Hungary by Antonio Stabile, pupil of Silvestro Bruno; and a Virgin of the Rosary with St Domenic, Rose, Gennaro, and Barbara by Giacinto Popoli, pupil of Massimo Stanzione.|$|R
40|$|A paper {{published}} by Maniya and Bhatt (2011) (An alternative multiple attribute decision making methodology for solving optimal facility layout design selection problems, Computers & Industrial Engineering, 61, 542 - 549) proposed an alternative multiple attribute decision making method named as “Preference Selection Index (PSI) method” for selection of an optimal facility layout design. The authors had {{claimed that the}} method was logical and more appropriate and the method gives directly the optimal solution without assigning the relative importance between the facility <b>layout</b> design selection <b>attributes.</b> This note discusses the mathematical validity and the shortcomings of the PSI method...|$|R
30|$|We use attributed, {{labelled}} and edge-directed graphs. Graph nodes represent building spaces, while edges {{correspond to}} accessibility relations between these spaces and therefore represent doors, openings and accessibility between storeys through stairs/lifts. Labels assigned to graph nodes store names of spaces, while node attributes store other properties of spaces, for example their sizes or types. Attributes of graph edges represent costs of moving between spaces. The proposed system creates a three dimensional visualization {{of the building}} graph. The information about the building <b>layout</b> together with <b>attributes</b> representing costs of moving between spaces and through them is used to compute optimal routes accessible for disabled people.|$|R
40|$|Automatic image {{synthesis}} {{research has}} been rapidly growing with deep networks {{getting more and more}} expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic <b>layout</b> and scene <b>attributes</b> integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e. g. day-night, sunny-foggy, with clear object boundaries...|$|R
40|$|The {{web browser}} is a CPU-intensive program. Especially on mobile devices, webpages load too slowly, {{expending}} significant time in processing a document’s appearance. Due to power constraints, most hardware-driven speedups {{will come in}} the form of parallel architectures. This is also true of mobile devices such as phones and e-books. In this paper, we introduce new algorithms for CSS selector matching, layout solving, and font rendering, which represent key components for a fast layout engine. Evaluation on popular sites shows speedups as high as 80 x. We also formulate the <b>layout</b> problem with <b>attribute</b> grammars, enabling us to not only parallelize our algorithm but prove that it computes in O(log) time and without reflow...|$|R
50|$|From 1710 to 1743 he was {{assigned}} to participate {{in the construction of the}} towers of the church Kościół Matki Bożej Łaskawej i św. Wojciecha in Łowicz. Due to his (young) age he could not be the author of the project. The façade had recently been designed by the Algirdas Zagorski architect, Kacper Bażanka and after his death in 1726, Jakub Fontana was hired to work for Józef Fontana in order to continue and complete the construction. Fontana would make detailed drawings of towers and undertake some adjustments of detail. These towers represent the type prevalent in Poland in the third decade of the 18th century. The architectural decoration of aisles formed in the pillar-column <b>layout</b> can be <b>attributed</b> to Fontana, who made them when he returned to Poland in 1737. However, in an earlier construction he could only be seen as a helper to his father.|$|R
40|$|Abstract. This study {{purpose was}} to {{investigate}} the customer interface combination preferences of tourist toward ecotourism destination website of Mt. Gede Pangrango National Park. Primary data resources obtained through the spread a questionnaire to youth tourists who several times visit this tourism destination and often observe its website. Data analysis techniques used is conjoint analysis. The results of this study show the customer interface attributes combination of preference base on 7 C framework are 1) Context of navigation, primary color, speed and <b>layout.</b> 2) Content <b>attributes</b> i. e. information, services, font, and picture. 3) Communication attribute are news letter, broadcast event, and contact placing. 4) Customization attribute of login and configuration. 5) Community group form. 6) Connection link to private management, government, NGO, private institution, community, related infrastructures. 7) Commerce attributes of registration consists of required preferences of user name, email address, and password combination...|$|R
40|$|This paper {{reviews the}} {{research}} on Japanese science textbooks. We first give an overview of work dealing with Japanese science textbooks used in junior-high and high schools {{from the viewpoint of}} textbook distribution and usage, function and content. We then focus on research dealing with the form of textbooks: their physical <b>attributes,</b> <b>layout</b> and representative elements. We found that (1) most of the physical attributes of textbooks are positively evaluated; (2) the layout of textbooks is criticized in that some constitutional elements, e. g.,the main text and marginal information, are not clearly related to each other; and (3) some linguistic and non-linguistic representative elements are criticized for not corresponding to the developmental stage of target readers or for lacking scientic accuracy. There is no previous research, however, that deals with the descriptive patterns of textbooks, despite the importance of this aspect when studying the form of textbooks...|$|R
40|$|The World Wide Web (WWW) has {{recently}} become {{the main source}} of digital information accessible everywhere and by everyone. Nevertheless, the inherent visual nature of Internet browsers makes the Web inaccessible to the visually impaired. To solve this problem, non-visual browsers have been developed. One of the new problems, however, with those non-visual browsers is that they often transform the visual content of HTML documents into textual information only, that can be restituted by a text-tospeech converter or a Braille device. The loss of spatial <b>layout,</b> and textual <b>attributes</b> such as boldface, italic, underline, color or even size should be avoided since they often bear visually important information. Moreover, typical non-visual Internet browsers do not allow visually impaired and sighted individuals to easily work together using the same environment. These new problems have to be solved with new alternative non-visual display techniques. This paper presents WebSound, a new gener [...] ...|$|R
40|$|A simple {{knowledge-based}} {{approach to}} the recognition of objects in man-made scenes is being developed. Specifically, the system under development is a proposed enhancement to a robot arm {{for use in the}} space station laboratory module. The system will take a request from a user to find a specific object, and locate that object by using its camera input and information from a knowledge base describing the scene <b>layout</b> and <b>attributes</b> of the object types included in the scene. In order to use realistic test images in developing the system, researchers are using photographs of actual NASA simulator panels, which provide similar types of scenes to those expected in the space station environment. Figure 1 shows one of these photographs. In traditional approaches to image analysis, the image is transformed step by step into a symbolic representation of the scene. Often the first steps of the transformation are done without any reference to knowledge of the scene or objects. Segmentation of an image into regions generally produces a counterintuitive result in which regions do not correspond to objects in the image. After segmentation, a merging procedure attempts to group regions into meaningful units that will more nearly correspond to objects. Here, researchers avoid segmenting the image as a whole, and instead use a knowledge-directed approach to locate objects in the scene. The knowledge-based approach to scene analysis is described and the categories of knowledge used in the system are discussed...|$|R
40|$|Theoretical thesis. Bibliography: pages 49 - 511. Introduction [...] 2. Related work [...] 3. Why CSS needs {{attribute}} grammars [...] 4. CSS <b>layout</b> with <b>attribute</b> grammars [...] 5. Evaluation [...] 6. Conclusion. The World Wide Web is {{a technology}} used daily by billions of people internationally. The core technologies {{that make up}} the Web are specified by the World Wide Web Consortium, and implemented independently by various organisations in the form of web browsers, which are tools to retrieve and view HTML/CSS documents. The task of retrieving, parsing, and rendering an HTML document is very important and very complex. The specification that defines how an HTML page should be rendered (CSS) is more than 150, 000 words long. It is well known that all web browsers render HTML differently in subtle ways, and we argue that the size and non-deterministic nature of the specification contributes to this problem. This project demonstrates a new method of a) describing the CSS specification, and b) implementing a CSS-compliant HTML renderer, that employs dynamic attribute grammars to achieve both tasks. Attribute grammars assist in specification by describing the relation-ship between the Document Object Model and its visual representation deterministically, and greatly reduce the gap between specification and implementation by writing the specification in an executable format. Furthermore, we show informally that this executable specification is human-readable, which we argue is because attribute grammars are the natural language to describe CSS. Mode of access: World wide web 1 online resource (viii, 51 pages...|$|R
40|$|We {{developed}} a multimedia presentation authoring system supporting {{a mechanism for}} conceptually representing the temporal relations of different media. Our authoring system represents media (such as images, videos, sounds, etc.) as objects and provides various editing functionalities for temporal compositions and spatial compositions. It {{is based on the}} SMIL (Synchronized Multimedia Integration Language). The system contains a temporal relation editor, a timeline editor, a <b>layout</b> editor, an <b>attribute</b> editor, a tag editor, a text editor, and a SMIL Object Manager. Among the many editors that make up our system, the temporal relation editor provides users with an intuitive mechanism for representing the conceptual flow of a presentation by simple and direct graphical manipulations. The SMIL Object Manager is responsible for the dynamic modification of each editor. The conceptual temporal relation editor and other editors of our system exchange their information in real-time and automatically generate SMIL codes through the SMIL Object Manager. Together they form an easy and efficient multimedia authoring environment. In this paper, we present the design and implementation of a SMIL-based multimedia authoring system and we also propose some ideas to extend the current SMIL specification concerning the reusability of SMIL code...|$|R
40|$|Previous {{attempts}} {{to measure the}} capacity of iconic memory in elderly adults have been unsuccessful, demonstrating in one case that 80 % of the elderly adults tested could not perform above chance level. The present experiments illustrated that the partial report paradigm designed by Sperling (1960) could be used with elderly adults to obtain such a measure. Both the young and older adults exhibited a strong partial report superiority that declined with delays in the cue interval. The effect of the perceived organization of an array on readout from iconic memory also was examined. Contrary to Merikle (1980), the differences due to the display <b>layout</b> were <b>attributed</b> to better acuity for one type of display and not to perceptual organization factors. Key Words: Perceptual organization, Partial report, Whole report, Acuity, Attention IN a landmark study, Sperling (1960) demon-strated that {{a large amount of}} information is available for readout shortly after the presentation of a visual display. The characteristics of this mem-ory store, termed iconic memory by Neisser (1967), have been studied extensively within a variety of paradigms. Coltheart (1980), in reviewing this lit-erature, determined that the partial report method devised independently by Sperling (1960) and Averbach and Coriell (1961) best defines iconic memory. Other paradigms developed to tap iconic memory are inappropriate because either they are based on incorrect assumptions or they tap phe-nomena that are secondary to iconic memory. For example, experimental methods that measure the degree of stimulus integration among temporally sequenced stimuli (e. g., Eriksen & Collins, 1967; Haber & Standing, 1969) measure a secondary phenomenon of persistence and not the capacity of the iconic store. Coltheart's (1980) review illustrates the neces-sity of definitional rigor when considering iconic memory. If this rigor is applied to the aging litera-ture, then it must be concluded that there has not been an effective measure of iconic memory in elderly adults. Much important knowledge has been gained about the persistence of the stimulus (Kline & Scheiber, 1982) and the integrity of th...|$|R
40|$|The {{thesis is}} aimed at {{developing}} effective techniques for reducing learner cognitive overload while using pinyin to learn Chinese language. Pinyin {{is one of the}} phonetic systems used to provide auditory information about Chinese characters. It was suggested that certain commonly used formats for presenting pinyin might impose high levels of cognitive load that exceed learner working memory capacity. In one experiment, learning effects of vertical and horizontal layouts of characters, pinyin, and English translations were compared. It was suggested that the horizontal presentation format might require learner to search and match characters and the corresponding components of pinyin that might unnecessarily consume cognitive resources making them unavailable for effective learning. Results indicated a significant difference favouring the vertical format. This advantage of the vertical <b>layout</b> was <b>attributed</b> to the reduced split attention by making the correspondence of instructional components obvious when parts of pinyin were placed exactly below the matching characters. Another type of extraneous cognitive load that may be caused by pinyin is associated with multimedia redundancy. It was suggested that simultaneously presenting written (pinyin) and spoken (corresponding pronunciation) text that contained identical information could be detrimental for learning. In this situation, additional cognitive activities involved in processing pinyin and cross-referencing them with corresponding pronunciation might impose a wasteful cognitive load. Learning effects of alternative presentation formats (full pinyin, partial pinyin, and no pinyin) were compared using learners with different levels of prior experience (pinyin reading skills and spoken Mandarin proficiency) and materials with different levels of complexity (modern Chinese vocabulary, modern Chinese sentences, and classical Chinese sentences). Results demonstrated that the removal of pinyin might enhance learning Chinese language. However, the effect depended on learners&# 146; prior language proficiency and experience with pinyin (an expertise reversal effect), {{as well as on the}} level of instructional support provided by pinyin. The effect might also apply only to certain aspect of language learning, for example, recognition of characters and pronunciation...|$|R
40|$|Memory for {{the layout}} of the ten digits 0 to 9 on the keypads of {{push-button}} telephones and calculators was investigated in five experiments. Experiments 1 and 2 revealed that, despite frequent usage of these devices, free recall of the numerical layouts is quite poor; and that the layout on calculators is even harder to recall than the telephone layout. Experiment 4 showed that {{the same is true for}} recognition of the layouts. Experiment 3 revealed that part of the recall advantage of the telephone <b>layout</b> can be <b>attributed</b> to its being more plausible and more similar to a schematic or prototypical layout of digits. Experiment 5 indicated that a single case of directing attention to the layouts can enhance recall significantly. The results are integrated into earlier research on memory for everyday objects, and concepts used in laboratory memory research such as interference, inference, and attention are used to explain memory for these everyday objects. Copyright # 1999 John Wiley & Sons, Ltd. Before you turn the page or look at one of the devices, please try to recall where the ten digits 0 to 9 are located on the keypads of your push-button telephone and your pocket calculator! Most probably, you will find it hard to recall the correct layouts of the digits, and you might be surprised to find that the way the digits are laid out o...|$|R
40|$|This {{incorporates}} the features and bug fixes from the Augsut 20, 2014 sprint {{and a few}} from the July 18 sprint. It was deployed to www. betydb. org on August 27, 2014. Changes Pertinent to PEcAn Users Administrators need to do database migrations! See "Database Changes" below. Creation and modification of ensembles, posteriors, runs, and workflows {{is no longer possible}} through the BetyDB web interface. This now must be done through PEcAn. Release Highlights The pages for adding and editing traits has been improved with a more logical <b>layout</b> of the <b>attribute</b> fields; and the panel for editing trait covariates has been moved to a side bar. Users now have the ability to edit information for variables associated with formats. New model types can be created in BETY and can be associated with PFTs as well as models. Model types can now have a list of required inputs needed to run the model. The "New" and "Edit" pages for ensembles, runs, and workflows have been removed as this functionality has been passed off to the PEcAn software. Summary of Changes Database Changes Administrators need to do database migrations! The database version for this release is 20140729045640. A new modeltypes table has been added, and the textual model_type columns in the models and the pfts tables have been replaced by references to rows of the new table. A new modeltypes_formats table has been added; this allows one to specify what file formats are needed for execution in PEcAn as well as what tags are used to specify these files as inputs in the pecan. xml file. Minor Changes and Improvements P and HSD have been removed from the list of stat names that may be selected when adding or editing traits or yields. The interface for associating variables with a format has been improved. Bug Fixes The Willow map legend has been fixed. The code that generates images for the priors listing now references the correct version of R. Status of RSpec Tests All tests now pass when run in the default environment and can be run using the command rspec Complete details for running the rspec tests are on the updated Wiki page at [URL]...|$|R
40|$|AbstractGraphs are well-known, well-understood, and {{frequently}} used means to depict networks of related items. They are successfully {{used as the}} underlying mathematical concept in various application domains. In all these domains tools are developed that store, retrieve, manipulate and display graphs as underlying data structures, despite {{of the fact that}} in most cases these graphs have a different name such as object diagrams, (meta) class diagrams, hyper documents, semantic webs etc. It is the purpose of this workshop to summarize {{the state of the art}} of graph-based tool development, bring together developers of graph-based tools in different application fields and to encourage new tool development cooperations. Motivation Graphs are an obvious means to describe structural aspects in various fields of computer science. They have been successfully used in application areas such as compiler compiler toolkits, constraint solving problems, generation of CASE tools, pattern recognition techniques, program analysis, software engineering, software evolution, software visualization and animation, and visual languages. In all these areas tools have been developed that use graphs as an important underlying data structure. Since graphs are a very general structure mechanism, it is a challenge to handle graphs in an effective way. Using graphs inside tools the following topics play an important role: efficient graph algorithms, empirical and experimental results on the scalability of graphs, reusable graph-manipulating software components, software architectures and frameworks for graph-based tools, standard data exchange formats for graphs, more general graph-based tool integration techniques, and meta CASE tools or generators for graph-based tools. The aim of the workshop on graph-based tools (GraBaTs) is to bring together developers of all kinds of graph-based tools in order to exchange their experiences, problems, and solutions concerning the efficient handling of graphs. The GraBaTs workshop is, therefore, of special relevance for the [URL] 1 st Intl. Conference on Graph Transformation (ICGT) which hosts GraBaTs as a satellite event: In many cases the application of graph transformation technology requires the existence of reliable, user-friendly and efficiently working graph transformation tools. These tools in turn have to be built on top of basic services or frameworks for graphs, which are the main topic of our workshop. Today, several graph transformation tool implementations have emerged which do not share any basic graph services (e. g. for graph pattern matching or graph layout purposes) and which implement rather different graph concepts and graph transformation approaches. Some of these tools - as a kind of survey of the state of the art - were presented in a special session, which is part of the main conference as well as of this satellite workshop. The presented tools are AGG, DiaGen, Fujaba, GenGED, and UPGRADE. The GraBaTs workshop was held for 				 1 					 12 				 days. Its schedule contained in addition to the afore-mentioned session on graph transformation tools, an invited talk by Tiziana Margaria (University of Dortmund, Germany) on ETI, an electronic tool integration platform where graph-based tools will play an important role. Apart from four sessions with presentations of 15 accepted papers (out of 19 submissions) on various graph-based tools and tool-relevant topics, a successful discussion ''Towards Standard Exchange Formats for Graph and Graph Transformation'' took place. Workshop Issues The workshop aims at bringing together tool developers from different fields, dealing with graphs from different perspectives. In the following, we give an overview on the most important perspectives. Meta-modeling by Graphs For a long time the syntax and static semantics of most visual modeling or programming languages was only defined by means of characteristic examples and informal descriptions. To improve this situation the visual language community invented grammar-based formalisms for the definition of the syntax of their languages, such as constraint grammars, graph grammars, relational grammars, etc. Unfortunately it turned out that the grammar-based definition of visual languages is rather complicated compared with the meta-modeling approach developed in parallel. The Meta-modeling approach for the definition of visual languages uses a combination of class diagrams (ER-diagrams, etc.) and predicate logic expressions (Z, OCL, etc.) to define the syntax and static semantics of visual languages. It became popular with the standardization of the OO-modeling language UML and is used by various meta-modeling (meta-CASE) tools which are able to generate domain-specific CASE tools. The so-called MOF approach (Meta-Object Facility) is one attempt to come up with a meta-modeling standard. Despite of its limited expressiveness (compared with ER diagrams or UML class diagrams) MOF builds the basis for the formal definition of UML and other visual languages. All meta-modeling approaches used nowadays have one common property: they offer graph-like diagrams for the definition of the structure (syntax) of graph-like diagram languages. Therefore, meta-modeling is in fact the formal definition of graph languages by means of graphs which are instances of “meta” graph languages. As a consequence, meta-CASE tools are a special class of graph-based tools, which need at least basic services for storing, visualizing, and analyzing graphs. Graph Visualization Facilities for visualizing graphs are needed by all kinds of graph-based tools, independent of the fact whether they are e. g. used for meta-modeling or rule-based programming purposes. Furthermore, graph visualization techniques are the most important means for visualizing various aspects of softwarearchitectures, the dynamic behavior of running systems, their evolution history, and so forth. Software components developed for these purposes usually have to deal with huge graphs and need services for making these graphs persistent, for introducing abstractions based on hierarchical graph models, for computing reasonable graph layouts (efficiently), and for displaying graphs effectively using “fish-eye-techniques” and the like. And last but not least, graph visualization techniques are often employed for teaching purposes in computer science courses on “data structures and (graph) algorithms”. To summarize, almost all kinds of graph-based tools urgently need efficiently and effectively working graph visualization services, whereas graph visualization tools may profit from research activities on graph query and graph transformation engines for the computation of graph abstractions or views. We, therefore, hope that this workshop encourages researchers to start new cooperations, such as adapting graph visualization tools to the needs of graph manipulation tools or exploiting graph manipulation and transformation techniques to compute sensible abstractions of huge graphs. Graph Queries and Graph Algorithms Most, if not all, graph-based tools use to a certain degree software components (libraries, subsystems, etc.) for executing graph queries and/or various kinds of standard graph algorithms. For example, graph transformation tools rely on rather sophisticated means for computing graph matches (rule occurrences) and graph-based reverse engineering tools need rather powerful query engines for determining critical substructures of software architectures. On the other hand, quite a number of database management systems have already been developed using graphs (networks of related objects) as the underlying data model and offering query languages based on graph path expressions or even graph transformations. Vice versa, graph transformation languages like PROGRES are not only used for specifying and visualizing graph algorithms, but incorporate many elements of database query languages such as means for the construction of indexes, the materialization and incremental update of views, etc. Therefore, we like to encourage tool developers again to start cooperating across the boundaries of research areas. Graph Transformation Graph transformation means the rule-based manipulation of graphs. Several graph transformation approaches have emerged which differ w. r. t. to the underlying kind of graphs as well as in the way how rules are applied to graphs, i. e. graph transformation takes place. The kind of graphs used by these tools include labeled, directed graphs, hypergraphs, and graph structures. Their rules, the basic means to manipulate graphs, differ w. r. t. to the formal definition of their semantics, the way how occurrences (matches) are searched for, and how matching rules are applied eventually. In tools, graph transformation is applied to visual languages, specification, code generation, verification, restructuring, evolution and programming of software systems, etc. Developers of graph transformation tools may profit from other workshop participants concerning more efficient realizations of basic functionality, while developers of other graph-based tools might find the graph transformation paradigm attractive to implement certain graph manipulations. The workshop may also provide insights to apply these tools to other application domains. Common Exchange Formats for Graphs and Graph Transformation To support interoperability between various graph-based tools, several initiatives on the development of common exchange formats for graphs have been founded. These formats are all based on the extensible markup language XML developed to interchange documents of arbitrary types. Preceding events like three subgroup meetings of the EU Working Group APPLIGRAPH, a Workshop on Standard Exchange Formats, and a satellite workshop of the 8 th Intl. Symposium on Graph Drawing (GD 2000) discussed various ideas which are currently converging to one format being GXL. During the GraBaTs workshop a further discussion round on this topic was organized focusing especially on graph <b>layout</b> and graph <b>attributes.</b> Another topic of interest for this discussion is an exchange format for graph transformation systems called GTXL, which is under development and which will be built on top of GXL. Workshop Organizers The Program Committee of the workshop consists of: 				Luciano Baresi (Italy) Giuseppe Di Battista (Italy) Ulrik Brandes (Germany) Scott Marshall (The Netherlands) Tom Mens (Belgium) (Co-chair) Andy Schürr (Germany) (Co-chair) Gabriele Taentzer (Germany) (Co-chair) Andreas Winter (Germany) Albert Zündorf (Germany) We are very grateful to Hartmut Ehrig for his help with the organization of the Workshop as satellite event of the 1 st Int. Conference on Graph Transformation (ICGT) and to Mike Mislove, one of the Managing Editors of the ENTCS series. Thanks are also due to Fernando Orejas and his local organizers at UPC in Barcelona who supplied preprints of this volume for all workshop participants...|$|R

