26|140|Public
40|$|Abstract—This paper {{proposes a}} new {{technique}} for face detection and <b>lip</b> <b>feature</b> extraction. A real-time field-programmable gate array (FPGA) implementation of the two proposed techniques is also presented. Face detection {{is based on a}} naive Bayes classifier that classifies an edge-extracted representation of an image. Using edge representation significantly reduces the model’s size to only 5184 B, which is 2417 times smaller than a comparable statistical modeling technique, while achieving an 86. 6 % correct detection rate under various lighting conditions. <b>Lip</b> <b>feature</b> extraction uses the contrast around the lip contour to extract the height and width of the mouth, metrics that are useful for speech filtering. The proposed FPGA system occupies only 15 050 logic cells, or about six times less than a current comparable FPGA face detection system. Index Terms—Edge information, face detection, fieldprogrammable gate array (FPGA), <b>lip</b> <b>feature</b> extraction, li...|$|E
40|$|Compared {{with some}} "static" {{biometrics}} such as human face and fingerprint, person authentication based on lip movement {{has the advantage}} of incorporating "dynamic" features which contain rich information indicating the speaker identity. This paper proposes a new <b>lip</b> <b>feature</b> representation and analyzes its discrimination power for person authentication. Since the original lip features are usually of high-dimension, the independent component analysis (ICA) is adopted for dimension- reduction and discriminative feature extraction. Hidden Markov model (HMM) is then employed as the classifier for its superiority in dealing with time-series data. Experiments are carried out on a database containing 40 speakers in our lab. By analyzing the experimental results, detailed evaluation for various <b>lip</b> <b>feature</b> representation is made and 98. 07 % accuracy rate in speaker recognition and 2. 31 % EER in speaker authentication is achieved using our <b>lip</b> <b>feature</b> representation. Full Tex...|$|E
40|$|In this paper, the <b>lip</b> <b>feature</b> {{that has}} the highest {{correlation}} with audio features is investigated. Audio features are selected as Mel Frequency Cepstral Coefficients (MFCC) of the audio signal. Three different lip features are considered for the visual lip information, where these features are 2 D DCT coefficients of the intensity based image and the optical flow vectors within the lip region, and the distances between pre-defined points on the lip contour which carries the lip shape information. In this study, we present two techniques based on class conditional probability analysis and canonical correlation analysis to estimate and compare the correlations between audio feature and each <b>lip</b> <b>feature.</b> The <b>lip</b> <b>feature,</b> which has the highest correlation to audio features, is identified among the above lip features. Isolation of lip features, which are highly correlated with audio signal, {{can be used for}} audio-visual speech recognition, audio-visual lip synchronization and estimation of lip shapes using audio signal for visual synthesis. 1...|$|E
50|$|Track 4 by The Flaming <b>Lips</b> <b>featuring</b> Henry Rollins & Peaches.|$|R
50|$|Tracks 5 and 6 by The Flaming <b>Lips</b> <b>featuring</b> Henry Rollins.|$|R
5000|$|GTA - Red <b>Lips</b> <b>featuring</b> Sam Bruno (Kayzo Remix of Skrillex's Remix) ...|$|R
40|$|As we all known, various {{speakers}} {{have their}} own talking styles. Hence, lip shape and its movement {{can be used as}} a new biometrics and infer the speaker’s identity. Compared with the traditional biometrics such as human face and fingerprint, person verification based on the <b>lip</b> <b>feature</b> has the advantage of containing both static and dynamic information. Many researchers have demonstrated that incorporating dynamic information such as lip movement help improve the verification performance. However, which is more discriminative, the static features or the dynamic features remained unsolved. In this paper, the discriminative power analysis of the static and dynamic lip features is performed. For the static lip features, a new kind of feature representation including the geometric features, contour descriptors and texture features is proposed and the Gaussian Mixture Model (GMM) is employed as the classifier. For the dynamic features, Hidden Markov Model (HMM) is employed as the classifier for its superiority in dealing with time-series data. Experiments are carried out on a database containing 40 speakers in our lab. Detailed evaluation for various static/dynamic <b>lip</b> <b>feature</b> representation is made along with a corresponding discussion on the discriminative ability. The experimental results disclose that the dynamic lip shape information and the static lip texture information contain much identity-relevant information. Index Terms — <b>lip</b> <b>feature,</b> feature analysis, speaker verification 1...|$|E
40|$|We {{present the}} first results from {{applying}} a recently proposed novel algorithm for the robust and reliable automatic extraction of <b>lip</b> <b>feature</b> {{points to an}} audio-video speech data corpus. This corpus comprises 10 native speakers uttering sequences that cover the range of phonemes and visemes in Australian English. The lip-tracking algorithm is based on stereo vision which {{has the advantage of}} measurements being in real-world (3 D) coordinates, instead of image (2 D) coordinates. Certain <b>lip</b> <b>feature</b> points on the inner lip contour such as the lip corners and the mid-points of upper and lower lip are automatically tracked. Parameters describing the shape of the mouth are derived from these points. The results obtained so far show that there is a correlation between width and height of the mouth opening as well as between the protrusion parameters of upper and lower lips...|$|E
40|$|The use {{of color}} {{information}} can significantly improve efficiency and robustness of <b>lip</b> <b>feature</b> extraction capa bility over purely grayscale-based methods. Edge infor mation provides another useful tool in characterizing lip boundaries. In this paper we present a method of integrating both types of information {{to address the problem}} of <b>lip</b> <b>feature</b> extraction for the purpose of speechreading. We first examine various color models and view hue as an effective descriptor to characterize the lips due to its invat·iance to luminance and human skin color, and its discriminative properties. We use prominent red hue as an indicator to locate the posi tion of the lips. Based on the identified lip area, we further refine the interior and exterior lip boundary using both color and spatial edge information, where those two are combined within a Markov random field (MRF) framework. Experimental results are presented to show the effectiveness of this method. ...|$|E
5000|$|... "Yoshimi Battles the Pink Robots, Pt. 1" [...] by The Flaming <b>Lips</b> <b>features</b> the [...] "Zarvox" [...] voice ...|$|R
40|$|Compared {{with other}} {{traditional}} biometric {{features such as}} face, finger print, or hand writing, <b>lip</b> biometric <b>features</b> contain both physiological and behavioral information. Physiologically, different people have different lips. On the other hand, people can usually be differentiated by their talking style. Current research on lip biometrics generally does not distinguish between the two kinds of information during feature extraction and classification and the interesting {{question of whether the}} physiological or the behavioral <b>lip</b> <b>features</b> are more discriminative has not been comprehensively studied. In this paper, different physiological and behavioral <b>lip</b> <b>features</b> are studied with respect to their discriminative power in speaker identification and verification. Our experimental results have shown that both the static <b>lip</b> texture <b>feature</b> and the dynamic shape deformation feature can achieve high identification accuracy (above 90 %) and low verification error rate (below 5 %). In addition, the lip rotation and centroid deformations, which are related to the speaker's talking mannerism, are found to be useful for speaker identification and verification. In contrast to previous studies, our results show that behavioral <b>lip</b> <b>features</b> are more discriminative in speaker identification and verification compared to physiological features. No Full Tex...|$|R
5000|$|The credits roll {{song was}} {{recorded}} by The Flaming <b>Lips</b> <b>featuring</b> Tobacco, titled [...] "Peace Sword", which was sold separately as an EP.|$|R
40|$|The {{need for}} an {{automatic}} lip-reading system is ever increasing. Infact, today, extraction and reliable analysis of facial movements make up {{an important part in}} many multimedia systems such as videoconference, low communication systems, lip-reading systems. In addition, visual information is imperative among people with special needs. We can imagine, for example, a dependent person ordering a machine with an easy lip movement or by a simple syllable pronunciation. Moreover, people with hearing problems compensate for their special needs by lip-reading as well as listening to the person with whome they are talking. We present in this paper a new approach to automatically localize <b>lip</b> <b>feature</b> points in a speaker’s face and to carry out a spatial-temporal tracking of these points. The extracted visual information is then classified in order to recognize the uttered viseme (visual phoneme). We have developed our Automatic <b>Lip</b> <b>Feature</b> Extraction prototype (ALiFE). Experiments revealed that our system recognizes 72. 73 % of French Vowels uttered by multiple speakers (female and male) under natural conditions. Keywords: visual information, lip-reading system, Human-Machine interaction, lip, spatial-temporal tracking...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceThe {{acoustic}} to articulatory {{inversion of}} speech {{which refers to}} the mapping from the acoustic signal to the articulatory, is an interesting problem. Given the acoustic signal, {{the recovery of the}} articulatory state is considered difficult. The reason is the "one-to-many" nature of the acoustic-to-articulatory inversion problem: a given articulatory state has always only one acoustic realization but an acoustic signal can be the outcome of more than one articulatory states. In order to solve the one-to-many problem of the inversion, visual information complementary to acoustic signal is used. Hence, a robust lip tracking system to provide visual information (such as the width and height of mouth) for the acoustic-to-articulatory inversion is developed in this paper. The proposed approach uses a combination of motion, color and structure information of the mouth area to track <b>lip</b> <b>feature</b> points. This technique is designed to be effective and robust. It has the advantages to detect the <b>lip</b> <b>feature</b> points automatically and recover the feature points lost during tracking process. Encouraging results have been obtained using the proposed approach...|$|E
40|$|Abstract. This paper {{proposes a}} novel {{algorithm}} used in extraction of <b>lip</b> <b>feature</b> extraction for to improved efficiency and robustness of lip-reading system. First, Lip Gray Energy Image(LGEI) {{is used to}} smooth noise, and improve noise resistance of the system. Second, Discrete Wavelet Analysis (DWT) is used to extract salient visual speech information from lip by decorrelating spectral information. Last, lip features are obtained by downsampling data from second step, the resample can effectively {{reduce the amount of}} computation. Experimental results show the performance of this method is exceedingly discriminative, accurate and computation efficient, the precision rate can rate 96 %...|$|E
40|$|In {{this paper}} we {{consider}} the problem of automatic extraction of the geometric <b>lip</b> <b>features</b> {{for the purposes of}} multi-modal speaker identification. The use of visual information from the mouth region can be of great importance for improving the speaker identification system performance in noisy conditions. We propose a novel method for automated <b>lip</b> <b>features</b> extraction that utilizes color space transformation and a fuzzy-based c-means clustering technique. Using the obtained visual cues closed-set audio-visual speaker identification experiments are performed on the CUAVE database, [1] showing promising results. 1...|$|R
5000|$|... "The W.A.N.D. (The Will Always Negates Defeat)" [...] (usually {{shortened}} to [...] "W.A.N.D.") {{is a song}} by The Flaming <b>Lips,</b> <b>featured</b> {{on their}} 2006 album At War with the Mystics.|$|R
40|$|This paper {{presents}} a novel technique for the tracking of moving lips {{for the purpose}} of speaker identification. In our system, a model of the lip contour is formed directly from chromatic information in the lip region. Iterative refinement of contour point estimates is not required. Colour features are extracted from the lips via concatenated profiles taken around the lip contour. Reduction of order in <b>lip</b> <b>features</b> is obtained via principal component analysis (PCA) followed by linear discriminant analysis (LDA). Statistical speaker models are built from the <b>lip</b> <b>features</b> based on the Gaussian mixture model (GMM). Identification experiments performed on the M 2 VTS 1 database, show encouraging result...|$|R
40|$|We {{present a}} novel {{algorithm}} for the robust and reliable automatic extraction of <b>lip</b> <b>feature</b> points for speechreading. The algorithm uses {{a combination of}} colour information in the image data and knowledge about {{the structure of the}} mouth area to find certain feature points on the inner lip contour. A new confidence measure quantifying how well the feature extraction process worked is introduced. A parameter set describing the shape of the mouth is derived from the positions of the feature points. Using a stereo camera system, measurements are in 3 D. Such a 3 D parameter set is of great value for automatic speech-reading systems...|$|E
40|$|We have {{recently}} proposed a new algorithm for the automatic extraction of <b>lip</b> <b>feature</b> points. Based on their positions, parameters describing {{the shape of}} the mouth are derived. Since the algorithm is based on a stereo vision face tracking system, all measurements are in real-world distances. In this paper, we evaluate the accuracy of the automatic feature extraction algorithm by comparing its results with a manual feature extraction process. The results show an average error of about 1 - 2 mm for the internal mouth width and height. In {{the second part of the}} paper, we present the design of an AV speech database for Australian English for future experiments on the correlation of audio and video speech signals...|$|E
40|$|International audienceThe phonetic {{translation}} of Cued Speech (CS) (Cornett [1]) gestures needs {{to mix the}} manual CS information together with the lips, {{taking into account the}} desynchronization delay (Attina et al. [2], Aboutabit et al. [3]) between these two flows of information. The automatic coding of CS hand positions and lip targets (Aboutabit et al. [3], Aboutabit et al. [4]) are thus a key factor in the mixing process. This contribution focuses on the identification of vowels by merging CS hand positions and vocalic lip information produced by a CS speaker. The hand flow is coded automatically as plateaus between transition phases. A plateau is defined as the interval during which the hand is maintained at a specific CS hand position. A transition is the interval during which the hand moves from a specific CS hand position to another one. The CS hand position is automatically obtained {{as the result of the}} hand 2 d coordinates Gaussian classification. The instants of reached hand targets are used as reference instants to define the interval inside which the lip target instant of the vowel is automatically detected. The lip parameters extracted at this instant are processed in a Gaussian classifier as to identify the vocalic <b>lip</b> <b>feature</b> of the vowel. The vowel is obtained as the result of the combination of the corresponding hand position and the <b>lip</b> <b>feature.</b> The global performance of the method attains 77. 6 % as correct identification score. This result does not take into account the CS coding errors. This result has to be compared with the global 83. 5 % score of speech reception by deaf people using CS (Nichols and Ling, 1982 [6]...|$|E
40|$|Emotion {{recognition}} and estimation from tracked <b>lip</b> <b>features</b> We transmit and display information through the visual image of our lips; information about our speech and our emotions and expressions. By tracking {{the movement of}} the lips, computers can make use of the visual part of the information. We focus on how to make use of tracked <b>lip</b> <b>features</b> for emotions. We have found that people are better at interpreting basic emotions displayed through an animation of lips than interpreting the same emotions displayed through a real video sequence that shows {{the lower part of the}} face. We have successfully transferred three basic emotions from visual information into information from another modality; touch, via vibrations. ...|$|R
40|$|Abstract—There {{have been}} several studies that jointly use audio, lip intensity, and lip {{geometry}} information for speaker identification and speech-reading applications. This paper proposes using explicit lip motion information, instead of or in addition to lip intensity and/or geometry information, for speaker identification and speech-reading within a unified feature selection and discrimination analysis framework, and addresses two important issues: 1) Is using explicit lip motion information useful, and, 2) if so, what are the best <b>lip</b> motion <b>features</b> for these two applications? The best <b>lip</b> motion <b>features</b> for speaker identification {{are considered to be}} those that result in the highest discrimination of individual speakers in a population, whereas for speech-reading, the best features are those providing the highest phoneme/word/phrase recognition rate. Several <b>lip</b> motion <b>feature</b> candidates have been considered including dense motion features within a bounding box about the lip, <b>lip</b> contour motion <b>features,</b> and combination of these with <b>lip</b> shape <b>features.</b> Furthermore, a novel two-stage, spatial, and temporal discrimination analysis is introduced to select the best <b>lip</b> motion <b>features</b> for speaker identification and speech-reading applications. Experimental results using an hidden-Markov-modelbased recognition system indicate that using explicit lip motion information provides additional performance gains in both applications, and <b>lip</b> motion <b>features</b> prove more valuable in the case of speech-reading application. Index Terms—Bayesian discriminative <b>feature</b> selection, <b>lip</b> motion, speaker identification, speech recognition, temporal discriminative feature selection. I...|$|R
5000|$|Late Night Tales: The Flaming Lips is a {{compilation}} album {{compiled by the}} members of The Flaming <b>Lips,</b> <b>featuring</b> songs by various artists. The album was released on March 7, 2005 and it features one new Flaming Lips recording, a cover of the White Stripes' [...] "Seven Nation Army".|$|R
40|$|ABSTRACT: We have {{recently}} proposed a new algorithm for the automatic extraction of <b>lip</b> <b>feature</b> points. Based on their positions, parameters describing {{the shape of}} the mouth are derived. Since the algorithm is based on a stereo vision face tracking system, all measurements are in real-world distances. In this paper, we evaluate the accuracy of the automatic feature extraction algorithm by comparing its results with a manual feature extraction process. The results show an average error of about 1 - 2 mm for the internal mouth width and height. In {{the second part of the}} paper, we present the design of an AV speech database for Australian English for future experiments on the correlation of audio and video speech signals...|$|E
40|$|In this work, {{we suggest}} a feature based facial image {{reconstruction}} technique. The encoding technique utilizes affine {{structure of the}} face to capture the global facial motion [5]. The structure information is transmitted only once at the bootstrapping stage. After that only the motion information for each frame is used to reconstruct the image at the decoder. An attempt is made to perform local reconstruction without any colour patching. For local reconstruction lips are tracked across frames using the snake model. An online database of facial features is created from distinct feature templates. The <b>lip</b> <b>feature</b> in each frame is locally reconstructed from existing templates using local warping. The eye templates for each frame are also recognized and pasted from the online database. 1...|$|E
40|$|The phonetic {{translation}} of Cued Speech (CS) (Cornett [1]) gestures needs {{to mix the}} manual CS information together with the lips, {{taking into account the}} desynchronization delay (Attina et al. [2], Aboutabit et al. [3]) between these two flows of information. The automatic coding of CS hand positions and lip targets (Aboutabit et al. [3], Aboutabit et al. [4]) are thus a key factor in the mixing process. This contribution focuses on the identification of vowels by merging CS hand positions and vocalic lip information produced by a CS speaker. The hand flow is coded automatically as plateaus between transition phases. A plateau is defined as the interval during which the hand is maintained at a specific CS hand position. A transition is the interval during which the hand moves from a specific CS hand position to another one. The CS hand position is automatically obtained {{as the result of the}} hand 2 d-coordinates Gaussian classification. The instants of reached hand targets are used as reference instants to define the interval inside which the lip target instant of the vowel is automatically detected. The lip parameters extracted at this instant are processed in a Gaussian classifier as to identify the vocalic <b>lip</b> <b>feature</b> of the vowel. The vowel is obtained as the result of the combination of the corresponding hand position and the <b>lip</b> <b>feature.</b> The global performance of the method attains 77. 6 % as correct identification score. This result does not take into account the CS coding errors. This result has to be compared with the global 83. 5 % score of speech perception by deaf people using CS (Nichols and Ling, 1982 [6]. Index Terms: Cued Speech production, lip target segmentation, vocalic lip classification, and CS gesture segmentation. 1...|$|E
40|$|This {{study is}} a {{contribution}} to the field of visual speech processing. It focuses on the automatic extraction of Speech <b>lip</b> <b>features</b> from natural <b>lips.</b> The method is based on the direct prediction of these features from predictors derived from an adequate transformation of the pixels of the lip region of interest. The transformation is made of a 2 -D Discrete Cosine Transform combined with a Principal Component Analysis applied to a subset of the DCT coefficients corresponding to about 1 % of the total DCTs. The results show the possibility to estimate the geometric <b>lip</b> <b>features</b> with a good accuracy (a root mean square of 1 to 1. 4 mm for the lip aperture and the lip width) using a reduce set of predictors derived from the PCA...|$|R
40|$|Abstract. This paper aims {{to give a}} {{solutions}} {{for the construction of}} chinese visual speech feature model based on HMM. We propose and discuss three kind representation model of the visual speech which are <b>lip</b> geometrical <b>features,</b> <b>lip</b> motion <b>features</b> and <b>lip</b> texture <b>features.</b> The model combines the advantages of the local LBP and global DCT texture information together, which shows better performance than the single feature. Equally the model combines the advantages of the local LBP and geometrical information together is better than single feature. By computing the recognition rate of the visemes from the model, the paper shows the HMM which describing the dynamic of speech, coupled with the combined feature for describing the global and local texture is the best model...|$|R
5000|$|Glass <b>Lips</b> (2007) (<b>feature</b> {{film version}} of the Blood of a Poet {{installation}} (2006)) ...|$|R
40|$|International audienceThis {{study is}} a {{contribution}} to the field of visual speech processing. It focuses on the automatic extraction of Speech lip features from natural lips. The method is based on the direct prediction of these features from predictors derived from an adequate transformation of the pixels of the lip region of interest. The transformation is made of a 2 -D Discrete Cosine Transform combined with a Principal Component Analysis applied to a subset of the DCT coefficients corresponding to about 1 % of the total DCTs. The results show the possibility to estimate the geometric <b>lip</b> <b>feature</b> with a good accuracy (a root mean square of 1 to 1. 4 mm for the lip aperture and the lip width) using a reduce set of predictors derived from the PCA...|$|E
40|$|International audienceThe aim of {{this chapter}} {{is to examine the}} {{possibility}} of extracting prosodic information from lip features. We used two measurement techniques enabling automatic <b>lip</b> <b>feature</b> extraction to evaluate the "lip pattern" of prosodic focus in French. Two corpora with Subject-Verb-Object (SVO) sentences were designed. Four focus conditions (S, V, O or neutral) were elicited in a natural dialogue situation. In a first set of experiments, we recorded two speakers of French with front and profile video cameras. The speakers wore blue make-up and facial markers. In a second set we recorded five speakers with a 3 D optical tracker. An analysis of the lip features showed that visible articulatory lip correlates of focus exist for all speakers. Two types of patterns were observed: absolute and differential. A potential outcome {{of this study is to}} provide criteria for automatic visual detection of prosodic focus from lip data...|$|E
40|$|International audienceIn this paper, {{we propose}} an {{extraction}} method of lip movement images from successive image frames and present {{the possibility to}} utilize lip movement images in the speech activity extraction process of speech recognition phase. The image frames are acquired from the PC image camera {{with the assumption that}} facial movement is limited during talking. First of all, one new lip movement image frame is generated with comparing two successive image frames each other. Second, the fine image noises are removed. Each fitness rate is calculated by comparing the <b>lip</b> <b>feature</b> data as objectly separated images. It is analyzed {{whether or not there is}} the lip movement image through verification to the objects and three images which have higher rates in their fitnesses. As a result of linking the speech & image processing system, the interworking rate shows 99. 3 % even in the various illumination environments. It was visually confirmed that lip movement images are tracked and can be utilized in speech activity extraction process...|$|E
5000|$|Fight Test is an EP by The Flaming <b>Lips</b> <b>featuring</b> {{the song}} of the same name, {{released}} on Warner Bros. Records in 2003. The single version of [...] "Fight Test" [...] was released on June 23, 2003, peaking at #28 in the UK Singles Chart. It is the third single to be picked from the album Yoshimi Battles the Pink Robots.|$|R
50|$|Les Vampires is {{referenced}} in the 1974 French film Celine and Julie Go Boating, {{where the}} title characters dress in costumes resembling Irma Vep's black bodysuit, and the 2009 war film Inglourious Basterds, where advertising posters {{can be seen}} in an office. The début self-titled album by American punk group Black <b>Lips</b> <b>features</b> an image of Irma Vep as the album cover. French electronic music duo Château Flight released a soundtrack to the film in 2006.|$|R
50|$|Rock was {{the host}} of On the Record with Mick Rock, a {{documentary}} series on Ovation. The series follows Rock as he travels the country and meets with musicians for a tour of their hometowns, highlighting the people, places and cultural institutions that have been integral {{in their lives and}} careers. Each episode features a performance. Guests in the first season include Josh Groban, The Flaming <b>Lips</b> (<b>featuring</b> Wayne Coyne and Steven Drozd), Kings of Leon, Patti LaBelle, and Mark Ronson.|$|R
