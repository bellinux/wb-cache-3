1500|874|Public
25|$|Dirac: {{a part of}} the {{specification}} of dirac covers <b>lossless</b> <b>compression.</b>|$|E
25|$|The {{most common}} general-purpose, <b>lossless</b> <b>compression</b> {{algorithm}} used with TIFF is Lempel–Ziv–Welch (LZW). This compression technique, {{also used in}} GIF, was covered by patents until 2003. TIFF also supports the compression algorithm PNG uses (i.e. Compression Tag 000816 'Adobe-style') with medium usage and support by applications. TIFF also offers special-purpose <b>lossless</b> <b>compression</b> algorithms like CCITT Group IV, which can compress bilevel images (e.g., faxes or black-and-white text) better than PNG's compression algorithm.|$|E
25|$|GIFs are {{suitable}} for sharp-edged line art (such as logos) with {{a limited number of}} colors. This takes advantage of the format's <b>lossless</b> <b>compression,</b> which favors flat areas of uniform color with well defined edges.|$|E
50|$|Monkey's Audio is an {{algorithm}} {{and file}} format for <b>lossless</b> audio data <b>compression.</b> <b>Lossless</b> data <b>compression</b> does not discard data {{during the process}} of encoding, unlike lossy compression methods such as AAC, MP3, Vorbis and Musepack.|$|R
40|$|Data Compression is {{a method}} to {{increase}} the storage capacity by eliminating redundancies that occur in most text files. It converts a string of characters into a new string which have the same data in small length. There {{are two types of}} data <b>compression,</b> <b>lossless</b> data <b>compression</b> and lossy data <b>compression.</b> <b>Lossless</b> data <b>compression</b> includes texts and lossy data compression includes image, audio and video etc. There are various techniques used for data compressio...|$|R
50|$|FELICS, {{which stands}} for Fast Efficient & <b>Lossless</b> Image <b>Compression</b> System, is a <b>lossless</b> image <b>compression</b> algorithmthat {{performs}} 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio.|$|R
25|$|Entropy {{effectively}} bounds {{the performance}} of the strongest <b>lossless</b> <b>compression</b> possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.|$|E
25|$|Lossless audio formats: These formats {{maintain}} the Hi-fi quality of every song or disc. These {{are the ones}} used by CDs, many people recommend the use of lossless audio formats to preserve the CD quality in audio files on a desktop. Some of them are: Apple Lossless (proprietary format) and FLAC (Royalties free) are increasingly popular formats for <b>lossless</b> <b>compression,</b> which {{maintain the}} Hi-fi quality.|$|E
25|$|HD Photo (previously {{known as}} Windows Media Photo) is a {{photographic}} still image file format, that is introduced with Windows Vista. It supports {{features such as}} high dynamic range imaging, lossy as well as <b>lossless</b> <b>compression,</b> up to 32-bpp fixed or floating point representation, transparency, RGB, CMYK and n-channel color spaces, Radiance RGBE, embedded ICC color profiles, multiple images per file and support for Exif and XMP metadata formats. It is the preferred image format for XPS documents.|$|E
5000|$|<b>Lossless</b> data <b>compression</b> {{algorithms}} cannot guarantee compression for all {{input data}} sets. In other words, for any <b>lossless</b> data <b>compression</b> algorithm, {{there will be}} an input data set that does not get smaller when processed by the algorithm, and for any <b>lossless</b> data <b>compression</b> algorithm that makes at least one file smaller, there will be at least one file that it makes larger. This is easily proven with elementary mathematics using a counting argument, as follows: ...|$|R
30|$|Because {{the entire}} {{provenance}} graph is encoded {{into a single}} packet, the <b>lossless</b> provenance <b>compression</b> schemes are more robust compared to the block provenance schemes and the distributed provenance schemes. Moreover, the <b>lossless</b> provenance <b>compression</b> schemes generate a moderate average provenance size.|$|R
2500|$|Portable Network Graphics (PNG [...] ) is a raster {{graphics}} file format that supports <b>lossless</b> data <b>compression.</b> PNG was created as an improved, non-patented replacement for Graphics Interchange Format (GIF), {{and is the}} most widely used <b>lossless</b> image <b>compression</b> format on the Internet.|$|R
25|$|JPEG's lossy {{compression}} also suffers from generation loss, where repeatedly decoding and re-encoding an image {{to save it}} again causes a loss of information each time, degrading the image. This does not happen with repeated viewing or copying, {{but only if the}} file is edited and saved over again. Because PNG is lossless, it is suitable for storing images to be edited. While PNG is reasonably efficient when compressing photographic images, there are <b>lossless</b> <b>compression</b> formats designed specifically for photographic images, lossless WebP and Adobe DNG (digital negative) for example. However these formats are either not widely supported, or are proprietary. An image can be stored losslessly and converted to JPEG format only for distribution, so that there is no generation loss.|$|E
25|$|If a {{compression}} {{scheme is}} lossless—that is, {{you can always}} recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a <b>lossless</b> <b>compression</b> scheme cannot compress messages, on average, {{to have more than}} one bit of information per bit of message, but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.|$|E
25|$|Shannon's theorem also {{implies that}} no <b>lossless</b> <b>compression</b> scheme can shorten all {{messages}}. If some messages come out shorter, {{at least one}} must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, AAC, PNG or JPEG will generally result in a ZIP file that is slightly larger than the source file(s).|$|E
40|$|This master's thesis {{deals with}} <b>lossless</b> video <b>compression.</b> This thesis {{includes}} basic concepts and techniques used in image representation. The reader can find {{an explanation of}} basic difference between <b>lossless</b> video <b>compression</b> and lossy video <b>compression</b> and <b>lossless</b> video <b>compression</b> limitations. There is also possible find {{a description of the}} basic blocks forming the video codec (every block is described in detail). In every block there are introduced its possible variants. Implemented lossless videocodec was compared with common lossless videocodecs...|$|R
40|$|A {{research}} of <b>lossless</b> network <b>compression</b> is carried out. To meet the different needs, two approaches of <b>lossless</b> network <b>compression</b> are proposed in this research. One approach, judging {{importance of the}} nodes according to their roles playing in the community composition, quantifies the importance of every node in communities, and achieves <b>lossless</b> network <b>compression</b> through layers; another approach, judging importance of the nodes according to the distances from the community representative nodes to them, differentiates the nodes with different distances, and achieves <b>lossless</b> network <b>compression</b> through compression ratio. Comparative experiments show that the two approaches not only can achieve perfect compression ratio, and retain {{the relationship between the}} communities, but also can reserve the important nodes or basic community structures during the compression process according to the needs...|$|R
40|$|We {{show that}} the optimal rate of <b>lossless</b> quantum data <b>compression</b> {{is closely related to}} Berthiaume, van Dam and Laplante's quantum Kolmogorov complexity. We show that: The {{expected}} quantum Kolmogorov complexity of a mixture is close to the optimal rate of <b>lossless</b> data <b>compression</b> of that mixture. If quantum Kolmogorov complexity obeys some inequality, then so does the optimal rate of <b>lossless</b> quantum data <b>compression...</b>|$|R
2500|$|The {{compression}} of FASTA files requires a specific compressor to handle both channels of information: identifiers and sequence. For improved compression results, these are mainly divided in two streams where the compression is made assuming independence. For example, the algorithm MFCompress [...] performs <b>lossless</b> <b>compression</b> of these files using context modelling and arithmetic encoding.|$|E
2500|$|The {{original}} OpenDocument format {{consists of}} an XML document that has [...] as its root element. OpenDocument files can also take the format of a ZIP compressed archive containing a number of files and directories; these can contain binary content and benefit from ZIP's <b>lossless</b> <b>compression</b> to reduce file size. OpenDocument benefits from separation of concerns by separating the content, styles, metadata, and application settings into four separate XML files.|$|E
2500|$|The {{principle}} {{can be used}} {{to prove}} that any <b>lossless</b> <b>compression</b> algorithm, provided it makes some inputs smaller (as the name compression suggests), will also make some other inputs larger. Otherwise, the set of all input sequences up to a given length [...] could be mapped to the (much) smaller set of all sequences of length less than [...] without collisions (because the compression is lossless), a possibility which the pigeonhole principle excludes.|$|E
50|$|WavPack is a {{free and}} {{open-source}} <b>lossless</b> audio <b>compression</b> format.|$|R
5000|$|Support for HEVC's lossy and <b>lossless</b> data <b>compression</b> is included.|$|R
5000|$|Ogg Squish - a <b>lossless</b> audio <b>compression</b> {{format and}} {{software}} (discontinued) ...|$|R
2500|$|Quality values {{account for}} {{about half of the}} {{required}} disk space in the FASTQ format (before compression), and therefore the compression of the quality values can significantly reduce storage requirements and speed up analysis and transmission of sequencing data. Both lossless and lossy compression are recently being considered in the literature. For example, the algorithm QualComp [...] performs lossy compression with a rate (number of bits per quality value) specified by the user. Based on rate-distortion theory results, it allocates the number of bits so as to minimize the MSE (mean squared error) between the original (uncompressed) and [...] the reconstructed (after compression) quality values. Other algorithms for compression of quality values include SCALCE [...] and Fastqz. Both are <b>lossless</b> <b>compression</b> algorithms that provide an optional controlled lossy transformation approach. For example, SCALCE reduces the alphabet size based on the observation that “neighboring” quality values are similar in general.|$|E
2500|$|The {{basic model}} of a data {{communication}} system is composed of three elements, a source of data, a channel, and a receiver, and – as expressed by Shannon, who essentially single-handedly created the field of information theory – the [...] "fundamental problem of communication" [...] is for the receiver {{to be able to}} identify what data was generated by the source, based on the signal it receives through the channel. The entropy provides an absolute limit on the shortest possible average length of a <b>lossless</b> <b>compression</b> encoding of the data produced by a source, and if the entropy of the source is less than the channel capacity of the communication channel, the data generated by the source can be reliably communicated to the receiver (at least in theory, possibly neglecting some practical considerations such as the complexity of the system needed to convey the data and the amount of time it may take for the data to be conveyed).|$|E
50|$|<b>Lossless</b> <b>compression</b> {{algorithms}} reduce {{file size}} while preserving a perfect {{copy of the}} original uncompressed image. <b>Lossless</b> <b>compression</b> generally, but not always, results in larger files than lossy compression. <b>Lossless</b> <b>compression</b> {{should be used to}} avoid accumulating stages of re-compression when editing images.|$|E
30|$|A new update-then-predict integer lifting wavelet {{family for}} <b>lossless</b> image <b>compression</b> is built and named in this paper. It {{is a perfect}} {{invertible}} update-then-predict structure and compared with the integer lifting structure of 5 / 3 wavelet, 9 / 7 -wavelet, and iDTT, IUPILW-(1, 5) results in the lower bit-rates for <b>lossless</b> image <b>compression.</b>|$|R
5000|$|Lempel-Ziv-Oberhumer (LZO) is a <b>lossless</b> data <b>compression</b> {{algorithm}} that {{is focused}} on decompression speed.|$|R
40|$|The project mainly aims to the {{development}} of applications for viewing <b>lossless</b> data <b>compression</b> algorithms. A theoretical part is devoted to description of theory from the area of <b>lossless</b> data <b>compression</b> algorithms and basic description of the language of HTML 5, JavaScript, and CSS that will be used during {{the development}}. A practical part is focused on proposal and implementation of particular applications...|$|R
50|$|<b>Lossless</b> <b>compression</b> methods may be {{categorized}} {{according to the}} type of data they are designed to compress. While, in principle, any general-purpose <b>lossless</b> <b>compression</b> algorithm (general-purpose meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. Many of the <b>lossless</b> <b>compression</b> techniques used for text also work reasonably well for indexed images.|$|E
50|$|Consumer {{video is}} {{generally}} compressed using lossy video codecs, since {{that results in}} significantly smaller files than <b>lossless</b> <b>compression.</b> While there are video coding formats designed explicitly for either lossy or <b>lossless</b> <b>compression,</b> some video coding formats such as Dirac and H.264 support both.|$|E
50|$|<b>Lossless</b> <b>{{compression}}</b> formats {{include the}} common FLAC, WavPack, Monkey's Audio, ALAC (Apple Lossless). They provide a compression ratio of about 2:1 (i.e. their files take {{up half the}} space of PCM). Development in <b>lossless</b> <b>compression</b> formats aims to reduce processing time while maintaining a good compression ratio.|$|E
50|$|<b>Lossless</b> {{predictive}} audio <b>compression</b> (LPAC) is {{an improved}} <b>lossless</b> audio <b>compression</b> algorithm developed by Tilman Liebchen, Marcus Purat and Peter Noll at Institute for Telecommunications, Technical University Berlin (TU Berlin), to compress PCM audio in a lossless manner, unlike conventional audio compression algorithms which are lossy.|$|R
5000|$|... ”An Efficient <b>Lossless</b> EEG <b>Compression</b> Engine,” NIH SBIR #1R43NS34211. 1995-1997 (phase 1); 1999-2003 (phase 2).|$|R
5000|$|... xz is a <b>lossless</b> data <b>compression</b> {{program and}} file format which {{incorporates}} the LZMA/LZMA2 compression algorithms.|$|R
