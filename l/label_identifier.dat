4|39|Public
50|$|ECI is a symbology-independent {{extension}} of the Global <b>Label</b> <b>Identifier</b> (GLI) system in the PDF417 barcode.|$|E
50|$|The California Direct Access Standards setting {{process in}} 1997 {{identified}} {{the need to}} standardize the electric meter <b>label</b> <b>identifier</b> so as to create a unique identifier for every electric meter in the United States. The AEP meter label format is a recommended solution for this need.|$|E
5000|$|StuffBak sells labels {{in varying}} {{shapes and sizes}} to consumers, which have a {{telephone}} number and a unique <b>label</b> <b>identifier</b> printed, along with the message [...] "Reward For Return". The labels are made in sizes ranging from a small sticker-dot for portable items, to a thick solid metal luggage tag. The owner registers their property and label numbers with the StuffBak web site.|$|E
50|$|Tape <b>labels</b> are <b>identifiers</b> {{given to}} volumes of {{magnetic}} tape.|$|R
50|$|Function labels {{consist of}} an identifier, {{followed}} by a colon. Each such label points to a statement in a function and its identifier must be unique within that function. Other functions may use the same name for a <b>label.</b> <b>Label</b> <b>identifiers</b> occupy their own namespace - one can have variables and functions with the same name as a label.|$|R
5000|$|In other {{languages}} such as C and Ada, a <b>label</b> is an <b>identifier,</b> usually appearing {{at the start}} of a line and immediately followed by a colon. For example, in C: ...|$|R
40|$|In the United States, Americans of African Descent {{have held}} many {{identity}} labels: African, Colored, Negro, Afro-American, Black, and African-American. In the 1960 s, {{there was a}} shift {{from the use of}} "Negro" to the use of "black" as a group identifier. In 1966 Stokely Carmichael shouted the phase "Black Power. " Three years later, in 1969, "Negro" was replaced by "black" as the dominant <b>label</b> <b>identifier.</b> This paper will how I measured when the shift occurred and will also set out three major explanations for why the shift happened relatively quickly. Understanding the shift to "black" may help with understanding why the identifier "African-American" has not completely replaced "black. ...|$|E
5000|$|In {{programming}} languages like PL/1 and Assembler used on IBM mainframe systems, {{as well as}} JCL (Job Control Language), the # symbol (along with $ and @) {{are used}} as additional letters in <b>identifiers,</b> <b>labels</b> and data set names.|$|R
40|$|We study {{deterministic}} gossiping in ad hoc radio networks {{with large}} node <b>labels.</b> The <b>labels</b> (<b>identifiers)</b> of the nodes {{come from a}} domain of size N which may be {{much larger than the}} size n of the network (the number of nodes). Most of the work on deterministic communication has been done for the model with small labels which assumes N = O(n). A notable exception is Peleg’s paper [32], where the problem of deterministic communication in ad hoc radio networks with large labels is raised and a deterministic broadcasting algorithm is proposed, which runs in O(n² log n) time for N polynomially large in n. The O(n log² n) -time deterministic broadcasting algorithm for networks with small labels given by Chrobak et al. [11] implies deterministic O(n log N log n) -time broadcasting and O(n² log² N log n) -time gossiping in networks with large labels. We propose two new deterministic gossiping algorithms for ad hoc radio networks with large labels, which are the first such algorithms with subquadratic time for polynomially large N. More specifically, we propose: – a deterministic O(n 3 / 2 log²N log n) -time gossiping algorithm for directed networks; and – a deterministic O(n log² N log² n) -time gossiping algorithm for undirected networks...|$|R
5000|$|... | company_id | EUI <b>label</b> | {{extension}} <b>identifier</b> | field [...] | AC | DE | 48 | FF | FE | 23 | 45 | 67 | hex 10101100 11011110 01001000 11111111 11111110 00100011 01000101 01100111 bits | | | | | most-significant-byte least-significant-byte | most-significant-bit least-significant-bit ...|$|R
2500|$|NaPTAN stop points have {{a number}} of text {{descriptor}} elements associated with them: not just a name, but also additional <b>labels</b> and distinguishing <b>identifiers</b> that will help users to recognise them. These elements can be combined in different ways to provide [...] presentations of names useful for [...] many different contexts, for example on maps, stop finders, timetables etc., and on mobile devices ...|$|R
30|$|The LPA (Label Propagation Algorithm) [48] is a near linear time {{algorithm}} {{for community}} detection. Its functioning is very simple, considered its computational efficiency. LPA uses only the network structure as its guide, is optimized for large-scale networks, {{does not follow}} any pre-defined objective function and does not require any prior information about the communities. <b>Labels</b> represent unique <b>identifiers,</b> assigned to each vertex of the network.|$|R
40|$|Abstract. Although ontologies were {{originally}} conceived as abstract knowledge representations understandable by computers, {{there is an}} ever increasing need of providing this knowledge to the user in a more friendly way. One method to facilitate this process is ontology localization, which has acquired great importance in research, in that it tries to present the ontology informa-tion in the user’s language. However, localizing ontologies is not a trivial task. In this {{paper we propose some}} ontology design patterns that can guide users in the process of assigning <b>labels</b> or <b>identifiers</b> to ontology entities. Based on our experience on localizing ontologies, we have developed some good practices as patterns following the Ontology Design Patterns initiative...|$|R
40|$|With {{increasing}} frequency, {{studies of}} biological samples include processing on two (or more) high-throughput platforms. Each platform produces a large set of features, each <b>labeled</b> by an <b>identifier.</b> It {{is one thing}} to merge the data by sample, simply combining the features on both platforms into a single data set. However, exploiting the full biological significance of the data depends on linking a feature from one platform with a feature on the other, where the pair of features ar...|$|R
40|$|In Chapter 1, {{we looked}} at {{distributed}} algorithms for coloring. In particular, we saw that rings and rooted trees can be colored with 3 colors in log ∗ n + O(1) rounds. In this chapter, we will reconsider the distributed coloring problem. We will look at a classic lower bound that shows that the result of Chapter 1 is tight: Coloring rings (and rooted trees) indeed requires Ω(log ∗ n) rounds. In particular, we will prove a lower bound for coloring in the following setting: • We consider deterministic, synchronous algorithms. • Message size and local computations are unbounded. • We assume that the network is a directed ring with n nodes. • Nodes have unique <b>labels</b> (<b>identifiers)</b> from 1 to n. Remarks: • A generalization of the lower bound to randomized algorithms is possible. • Except for restricting to deterministic algorithms, all the conditions above make a lower bound stronger: Any lower bound for synchronous algorithms certainly also holds for asynchronous ones. A lower bound that is true if message size and local computations are not restricted is clearly also valid if we require a bound on the maximal message size or the amount of local computations. Similarly also assuming that the ring is directed and that node labels are from 1 to n (instead of choosing IDs from a more general domain) strengthen the lower bound. • Instead of directly proving that 3 -coloring a ring needs Ω(log ∗ n) rounds, we will prove a slightly more general statement. We will consider deterministic algorithms with time complexity r (for arbitrary r) and derive a lower bound {{on the number of}} colors that are needed if we want to properly color an n-node ring with an r-round algorithm. A 3 -coloring lower bound can then be derived by taking the smallest r for which an r-round algorithm needs 3 or fewer colors...|$|R
30|$|Radiotracer {{injection}} {{quality was}} checked by SPECT. Both the SPECT and CT scanner units passed all manufacturer recommended QC procedures checked {{before and after}} the study. Directly after image acquisition and reconstruction, images were inspected to rule out technical issues like motion artefacts or poor injections. QC after image analysis included (1) inspection of all MIP movies and single slice images to confirm proper processing (e.g. coregistration and ROI placements) and image <b>labelling</b> by subject <b>identifiers</b> (comparing all images per mouse) and (2) automated detection of potential outliers in the uptake data.|$|R
5000|$|... | OUI | MAC <b>label</b> | {{extension}} <b>identifier</b> | field [...] | 1st | 2nd | 3rd | 4th | 5th | 6th | 7th | 8th | order | C A | E D | 8 4 | F F | F F | 3 2 | 5 4 | 7 6 | hex 00110101 01111011 00010010 11111111 11111111 11000100 10100100 11100110 bits | | | | | | | | | | | | | | | | lsb msb lsb msb lsb msb lsb msb lsb msb lsb msb lsb msb lsb msb ...|$|R
40|$|This {{document}} specifies {{an extension}} to the identifiers {{to be used}} in the Transport Profile of Multiprotocol <b>Label</b> Switching (MPLS-TP). <b>Identifiers</b> that follow IP/MPLS conventions have already been defined. This memo augments that set of identifiers for MPLS-TP management and Operations, Administration, and Maintenance (OAM) functions to include identifier information in a format typically used by the International Telecommunication Union Telecommunication Standardization Sector (ITU-T). Status of This Memo This is an Internet Standards Track document. This document is a product of the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|R
40|$|This paper {{describes}} the different strategies {{used to improve}} the results obtained by our off-line speaker diarisation tool with the Albayzin 2010 diarisation database. The errors made by the system have been analyzed and different strategies have been proposed to reduce each kind of error. Very short segments incorrectly labelled and different appearances of one speaker <b>labelled</b> with different <b>identifiers</b> {{are the most common}} errors. A post-processing module that refines the segmentation by retraining the GMM models of the speakers involved has been built to cope with these errors. This post-processing module has been tuned with the training dataset and improves the result of the diarisation system by 16. 4 % in the test dataset...|$|R
40|$|Abstract. We {{present a}} survey of the usage and style of <b>identifiers</b> and <b>labels</b> of named {{entities}} in a corpus of OWL ontologies. We investigated the frequency of use of both labels and meaningful or meaningless identifiers in those ontologies. We also surveyed common practices of lexical encoding styles for identifiers. We found that most ontologies do not use labels for named entities. When they do use labels, those labels are mostly meaningful and most ontologies also used meaningful identifiers. CamelCase style appears to be the most widely used style of lexical encoding for identifiers. We observed, however, {{that the majority of the}} ontologies use a mixture of two or more lexical encoding styles. The result of this survey is useful when considering strategies, for example, natural language generation from ontologies or converting artefacts, such as OWL ontologies, into languages like the Simple Knowledge Representation System (SKOS), where the notion of label is important. Given that labels are optional in OWL ontologies, what is the best way to handle the label selection when converting them into SKOS? Merging multiple entities may require selection from <b>labels</b> or <b>identifiers</b> assigned to these entities for skos:prefLabel and skos:altLabel...|$|R
40|$|In the past, Mandatory Access Control (MAC) {{systems have}} used very rigid {{policies}} that were implemented in particular protocols and platforms. As MAC systems become more widely deployed, additional flexibility in mechanism and policy will be required. While traditional trusted systems implemented Multi-Level Security (MLS) and integrity models, modern systems have {{expanded to include}} such technologies as type enforcement. Due to {{the wide range of}} policies and mechanisms that need to be accommodated, {{it is unlikely that the}} use of a single security label format and model will be viable. To allow multiple MAC mechanisms and label formats to co-exist in a network, this document creates a registry of label format specifications. This registry contains <b>label</b> format <b>identifiers</b> and provides for the association of each such identifier with a corresponding extensive document outlining the exact syntax and use of the particular label format. Status of This Memo This is an Internet Standards Track document. This document is a product of the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|R
40|$|We {{consider}} learning tree patterns from queries extending our preceding work [Amoth, Cull, & Tadepalli, 1998]. The {{instances in}} this paper are unordered trees with nodes <b>labeled</b> by constant <b>identifiers.</b> The concepts are tree patterns and unions of tree patterns (unordered forests) with leaves labeled with constants or variables. A tree pattern matches any tree with its variables replaced with constant subtrees. A negative result for learning with equivalence and membership/subset queries is shown for unordered trees where a successful match requires {{the number of children}} in the pattern and instance to be the same. Unordered trees and forests are shown to be learnable with an alternative matching semantics that allows an instance to have extra children at each node...|$|R
40|$|Region growing, {{often given}} as a {{classical}} {{example of the}} recursive control structures used in image processing which are often awkward to implement in hardware where the intent is the segmentation of an image at raster scan rates, is addressed {{in light of the}} postulate that any computation which can be performed recursively can be performed easily and efficiently by iteration coupled with association. Attention is given to an algorithm and hardware structure able to perform region labeling iteratively at scan rates. Every pixel is individually <b>labeled</b> with an <b>identifier</b> which signifies the region to which it belongs. Difficulties otherwise requiring recursion are handled by maintaining an equivalence table in hardware transparent to the computer, which reads the labeled pixels. A simulation of the associative memory has demonstrated its effectiveness...|$|R
40|$|We {{consider}} learning tree patterns from queries. The instances {{are ordered}} and unordered trees with nodes <b>labeled</b> by constant <b>identifiers.</b> The concepts are tree patterns and unions of tree patterns (forests) {{where all the}} internal nodes are labeled with constants and the leaves are labeled with constants or variables. A tree pattern matches any tree with its variables replaced with constant subtrees. We show that ordered trees, in which the children are matched in a strict left-to-right order, are exactly learnable from equivalence queries, while ordered forests are learnable from equivalence and membership queries. Unordered trees are exactly learnable from superset queries, and unordered forests are learnable from superset and equivalence queries. Negatively, we also show {{that each of the}} query types used is necessary for learning each concept class. 1 INTRODUCTION A large part of computational learning theory is devoted to learning concepts over instances represented as attribute v [...] ...|$|R
30|$|To {{overcome}} hidden or spurious {{links in}} observed networks, some network inference may assist investigators to reconstruct a crime network. At least three network inference tasks arise [53]. The {{first is to}} assign {{the name of a}} player to each <b>identifier</b> <b>label</b> in the data set – entity resolution. Next is to predict the set of edges in the true network – link prediction. Finally, the analyst predicts the role of each player in the network – collective classification. We focus on two approaches that allow analysts to reconstruct a network from an incomplete data set on pairs of linked players. The graph-based approach [54] employs only basic graph-theoretic measures to predict the presence of a link and does not attempt to model the network in any way. The model-based approach [46] assumes the network was generated by a probabilistic function. Under this assumption, analysts use the data to first estimate the model’s parameters, and then to predict link presence or absence from this fitted model.|$|R
40|$|Abstract Background Environmental {{sequence}} datasets {{are increasing}} at an exponential rate; however, {{the vast majority}} of them lack appropriate descriptors like sampling location, time and depth/altitude: generally referred to as metadata or contextual data. The consistent capture and structured submission of these data is crucial for integrated data analysis and ecosystems modeling. The application MetaBar has been developed, to support consistent contextual data acquisition. Results MetaBar is a spreadsheet and web-based software tool designed to assist users in the consistent acquisition, electronic storage, and submission of contextual data associated to their samples. A preconfigured Microsoft ® Excel ® spreadsheet is used to initiate structured contextual data storage in the field or laboratory. Each sample is given a unique identifier and at any stage the sheets can be uploaded to the MetaBar database server. To <b>label</b> samples, <b>identifiers</b> can be printed as barcodes. An intuitive web interface provides quick access to the contextual data in the MetaBar database as well as user and project management capabilities. Export functions facilitate contextual and sequence data submission to the International Nucleotide Sequence Database Collaboration (INSDC), comprising of the DNA DataBase of Japan (DDBJ), the European Molecular Biology Laboratory database (EMBL) and GenBank. MetaBar requests and stores contextual data in compliance to the Genomic Standards Consortium specifications. The MetaBar open source code base for local installation is available under the GNU General Public License version 3 (GNU GPL 3). Conclusion The MetaBar software supports the typical workflow from data acquisition and field-sampling to contextual data enriched sequence submission to an INSDC database. The integration with the megx. net marine Ecological Genomics database and portal facilitates georeferenced data integration and metadata-based comparisons of sampling sites as well as interactive data visualization. The ample export functionalities and the INSDC submission support enable exchange of data across disciplines and safeguarding contextual data. </p...|$|R
40|$|MirBot is a {{collaborative}} application for smartphones that {{allows users to}} perform object recognition. This app {{can be used to}} take a photograph of an object, select the region of interest and obtain the most likely class (dog, chair, etc.) by means of similarity search using features extracted from a convolutional neural network (CNN). The answers provided by the system can be validated by the user so as to improve the results for future queries. All the images are stored together with a series of metadata, thus enabling a multimodal incremental dataset <b>labeled</b> with synset <b>identifiers</b> from the WordNet ontology. This dataset grows continuously thanks to the users' feedback, and is publicly available for research. This work details the MirBot object recognition system, analyzes the statistics gathered after more than four years of usage, describes the image classification methodology, and performs an exhaustive evaluation using handcrafted features, convolutional neural codes and different transfer learning techniques. After comparing various models and transformation methods, the results show that the CNN features maintain the accuracy of MirBot constant over time, despite the increasing number of new classes. The app is freely available at the Apple and Google Play stores...|$|R
40|$|Abstract. XML query {{processing}} {{is one of}} {{the most}} active areas of database research. Although the main focus of past research has been the processing of structural XML queries, there are growing demands for a full-text search for XML documents. In this paper, we propose XICS (XML Indices for Content and Structural search), novel indices built on a B +-tree, for the fast processing of queries that involve structural and fulltext searches of XML documents. To represent the structural information of XML trees, each node in the XML tree is <b>labeled</b> with an <b>identifier.</b> The identifier contains an integer number representing the path information from the root node. XICS consist of two types of indices, the COB-tree (COntent B +-tree) and the STB-tree (STructure B +-tree). The search keys of the COB-tree are a pair of text fragments in the XML document and the identifiers of the leaf nodes that contain the text, whereas the search keys of the STB-tree are the node identifiers. By using a node identifier in the search keys, we can retrieve only the entries that match the path information in the query. Our experimental results show the efficiency of XICS in query processing. ...|$|R
40|$|Identity {{uncertainty}} is a pervasive problem in real-world data analysis. It arises whenever objects are not <b>labeled</b> with unique <b>identifiers</b> or when those identifiers {{may not be}} perceived perfectly. In such cases, two observations {{may or may not}} correspond to the same object. In this paper, we consider the problem in the context of citation matching [...] the problem of deciding which citations correspond to the same publication. Our approach is based on the use of a relational probability model to define a generative model for the domain, including models of author and title corruption and a probabilistic citation grammar. Identity {{uncertainty is}} handled by extending standard models to incorporate probabilities over the possible mappings between terms in the language and objects in the domain. Inference is based on Markov chain Monte Carlo, augmented with specific methods for generating efficient proposals when the domain contains many objects. Results on several citation data sets show that the method outperforms current algorithms for citation matching. The declarative, relational nature of the model also means that our algorithm can determine object characteristics such as author names by combining multiple citations of multiple papers...|$|R
40|$|Data are the {{foundation}} of empirical research, yet all too often the datasets underlying published papers are unavailable, incorrect, or poorly curated. This is a serious issue, because future researchers are then unable to validate published results or reuse data to explore new ideas and hypotheses. Even if data files are securely stored and accessible, they must also be accompanied by accurate <b>labels</b> and <b>identifiers.</b> To assess how often problems with metadata or data curation affect the reproducibility of published results, we attempted to reproduce Discriminant Function Analyses (DFAs) {{from the field of}} organismal biology. DFA is a commonly used statistical analysis that has changed little since its inception almost eight decades ago, and therefore provides an opportunity to test reproducibility among datasets of varying ages. Out of 100 papers we initially surveyed, fourteen were excluded because they did not present the common types of quantitative result from their DFA or gave insufficient details of their DFA. Of the remaining 86 datasets, there were 15 cases for which we were unable to confidently relate the dataset we received to the one used in the published analysis. The reasons ranged from incomprehensible or absent variable labels, the DFA being performed on an unspecified subset of the data, or the dataset we received being incomplete. We focused on reproducing three common summary statistics from DFAs: the percent variance explained, the percentage correctly assigned and the largest discriminant function coefficient. The reproducibility of the first two was fairly high (20 of 26, and 44 of 60 datasets, respectively), whereas our success rate with the discriminant function coefficients was lower (15 of 26 datasets). When considering all three summary statistics, we were able to completely reproduce 46 (65 %) of 71 datasets. While our results show that a majority of studies are reproducible, they highlight the fact that many studies still are not the carefully curated research that the scientific community and public expects...|$|R
40|$|InvertNet, one of {{the three}} Thematic Collection Networks (TCNs) funded {{in the first round of}} the U. S. National Science Foundation’s Advancing Digitization of Biological Collections (ADBC) program, is tasked with {{providing}} digital access to ~ 60 million specimens housed in 22 arthropod (primarily insect) collections at institutions distributed throughout the upper midwestern USA. The traditional workflow for insect collection digitization involves manually keying information from specimen labels into a database and attaching a unique <b>identifier</b> <b>label</b> to each specimen. This remains the dominant paradigm, despite some recent attempts to automate various steps in the process using more advanced technologies. InvertNet aims to develop improved semi-automated, high-throughput workflows for digitizing and providing access to invertebrate collections that balance the need for speed and cost-effectiveness with long-term preservation of specimens and accuracy of data capture. The proposed workflows build on recent methods for digitizing and providing access to high-quality images of multiple specimens (e. g., entire drawers of pinned insects) simultaneously. Limitations of previous approaches are discussed and possible solutions are proposed that incorporate advanced imaging and 3 -D reconstruction technologies. InvertNet couples efficient digitization workflows with a highly robust network infrastructure capable of managing massive amounts of image data and related metadata and delivering high-quality images, including interactive 3 -D reconstructions in real time via the Internet...|$|R
40|$|Face {{classification}} can {{be defined}} as the problem of assigning a predefined label to an image or subpart of an image that contains one or more faces. This definition comprises many sub disciplines in the visual pattern recognition field: (i) face detection, where the goal is to detect the presence of a face on an image, (ii) face recognition, where we assign an <b>identifier</b> <b>label</b> to the detected face, Face recognition is emerging as an active research area with numerous commercial and law enforcement applications. Although existing methods perform well under certain conditions, the illumination changes, occlusions and recognition time are still challenging problems. This work attempts to use Random Forests to deal with the above challenges in improvement in face classification. Random Forest is a tree based classifier that consists of many decision trees. Each tree gives a classification[11] and regression[11] and process further carry with this two techniques,here we are focusing main over the regression technique. The proposed algorithm first extracts features from the face images from a small dataset using the Gabor wavelet transform and then uses the Random Forest algorithm to classify the images based on the regression technique. The proposed algorithm makes use of a Random Forest regression that selects a small set of most discriminant Gabor wavelet features. Only this small set of features is now used to classify the images resulting in a fast face recognition technique. The proposed approaches are tested on a multiple image face databases and the results are found to be highly encouraging...|$|R
40|$|SUMMARY XML query {{processing}} {{is one of}} {{the most}} active areas of database research. Although the main focus of past research has been the processing of structural XML queries, there are growing demands for a fulltext search for XML documents. In this paper, we propose XICS (XML Indices for Content and Structural search), which aims at high-speed processing of both full-text and structural queries in XML documents. An important design principle of our indices is the use of a B +-tree. To represent the structural information of XML trees, each node in the XML tree is <b>labeled</b> with an <b>identifier.</b> The identifier contains an integer number representing the path information from the root node. XICS consist of two types of indices, the COB-tree (COntent B +-tree) and the STB-tree (STructure B +-tree). The search keys of the COB-tree are a pair of text fragments in the XML document and the identifiers of the leaf nodes that contain the text, whereas the search keys of the STB-tree are the node identifiers. By using a node identifier in the search keys, we can retrieve only the entries that match the path information in the query. The STB-tree can filter nodes using structural conditions in queries, while the COB-tree can filter nodes using text conditions. We have implemented a COB-tree and an STB-tree using GiST and examined index size and query processing time. Our experimental results show the efficiency of XICS in query processing. key words: XML query processing, full-text search, B +-tree, node labeling scheme 1...|$|R
40|$|In this thesis, a {{probabilistic}} {{approach to the}} problem of packet forwarding in information centric networks is analysed and further developed. This type of networks are based on information identifiers rather than on the traditional host addresses. The approach is compact forwarding where the Bloom filter is the key method for aggregating forwarding information that allows moving packets at line speed <b>labelled</b> with fiat <b>identifiers.</b> The Bloom filter reduces state at the nodes, simplifies multicast delivery and introduces new trade-offs in the traditional routing and forwarding design space. However) it is a lassy method which produces some potential bandwidth penalties, loops, packet storms, and security issues due to false positives. This thesis focuses on false posit ive control for the probabilistic in-packet forwarding method and proposes two approaches either to reduce false positives or to exploit them in a useful way. One approach consists of a mechanism to carefully select the number of hash functions to use to generate the Bloom filter, The mechanism on average offers the minimum false positive occurrences depending on the traffic along the links. The other approach is a variation of the Bloom filter, the optihash, that can give better performance with respect to the Bloom filter at a cost of slightly more processing. The optihash is constructed with a family of functions that allows an optimisation which can be performed according to different metrics. Two general metrics are proposed in detail and some other, appJicationspeCific, are explored for in-packet forwarding techniques in different types of networks. The time complexity/false positive trade-off is thoroughly investigated and the evaluation of the optihasb {{as an alternative to the}} Bloom filter is performed for in-packet compact forwarding. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Radio Frequency Identification (RFID) is a {{wireless}} technology {{that can be}} used to track inventory <b>labeled</b> with microchip-embedded <b>identifiers</b> communicating passively with scanners without operator involvement. This non-line-of-sight technology has the potential of dramatically increasing the level of visibility throughout the supply chain for many types of products, assisting in defect reduction, increased granularity in inventory tracking, and decreased direct labor. In recent years, developments in RFID technology have decreased the cost of RFID equipment, and several large U. S. retailers have started to use RFID to track consumer products. However, what is not clear is whether or not these RFID implementations have yielded economic returns. Although RFID promises higher read rates and increased accuracy, how the technology works in particular warehouse settings is not clear. The first step to determining the feasibility of RFID in any organization is the complete evaluation of RFID technology. This document discusses an evaluation strategy using the Six Sigma DMADV framework. The strategy was carried out at internet retailer Amazon. com. (cont.) The document discusses the various steps required for a complete implementation of the evaluation strategy and refers to the evaluation at Amazon. com as a case study. The purpose of this document is to recommend a complete evaluation strategy of RFID system components for any customer fulfillment center that is thinking of implementing this technology to replace existing tracking technologies such as bar code or other manual forms of tracking. by Howard H. Shen. Thesis (M. B. A.) [...] Massachusetts Institute of Technology, Sloan School of Management; and, (S. M.) [...] Massachusetts Institute of Technology, Engineering Systems Division; in conjunction with the Leaders for Manufacturing Program at MIT, 2006. Includes bibliographical references (leaves 66 - 67) ...|$|R
40|$|Abstract Background Advances in the {{high-throughput}} omic {{technologies have}} made it possible to profile cells in a large number of ways at the DNA, RNA, protein, chromosomal, functional, and pharmacological levels. A persistent problem is that some classes of molecular data are <b>labeled</b> with gene <b>identifiers,</b> others with transcript or protein identifiers, and still others with chromosomal locations. What has lagged behind is the ability to integrate the resulting data to uncover complex relationships and patterns. Those issues are reflected in full form by molecular profile data on the panel of 60 diverse human cancer cell lines (the NCI- 60) used since 1990 by the U. S. National Cancer Institute to screen compounds for anticancer activity. To our knowledge, CellMiner is the first online database resource for integration of the diverse molecular types of NCI- 60 and related meta data. Description CellMiner enables scientists to perform advanced querying of molecular information on NCI- 60 (and additional types) through a single web interface. CellMiner is a freely available tool that organizes and stores raw and normalized data that represent multiple types of molecular characterizations at the DNA, RNA, protein, and pharmacological levels. Annotations for each project, along with associated metadata on the samples and datasets, are stored in a MySQL database and linked to the molecular profile data. Data can be queried and downloaded along with comprehensive information on experimental and analytic methods for each data set. A Data Intersection tool allows selection of a list of genes (proteins) in common between two or more data sets and outputs the data for those genes (proteins) in the respective sets. In addition to its role as an integrative resource for the NCI- 60, the CellMiner package also serves as a shell for incorporation of molecular profile data on other cell or tissue sample types. Conclusion CellMiner is a relational database tool for storing, querying, integrating, and downloading molecular profile data on the NCI- 60 and other cancer cell types. More broadly, it provides a template to use in providing such functionality for other molecular profile data generated by academic institutions, public projects, or the private sector. CellMiner is available online at [URL]. </p...|$|R
