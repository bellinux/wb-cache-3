116|8|Public
5000|$|Usually {{the values}} that {{insurance}} adjusters receive are either left-truncated, right-censored, or both. For example, if policyholders are subject to a policy limit u, then any loss amounts that are actually above u are reported to the insurance company as being exactly u because u is the amount the insurance company pays. The insurer knows that the actual loss is greater than u but they don't know what it is. On the other hand, <b>left</b> <b>truncation</b> occurs when policyholders are subject to a deductible. If policyholders are subject to a deductible d, any loss amount that is less than d will not even be reported to the insurance company. If there is a claim on a policy limit of u and a deductible of d, any loss amount that is greater than u will be reported to the insurance company as a loss of [...] {{because that is the}} amount the insurance company has to pay. Therefore, insurance loss data is left-truncated because the insurance company doesn't know if there are values below the deductible d because policyholders won't make a claim. The insurance loss is also right-censored if the loss is greater than u because u is the most the insurance company will pay. Thus, it only knows that your claim is greater than u, not the exact claim amount.|$|E
40|$|In this study, we {{investigated}} the robustness {{of the methods}} that account for independent <b>left</b> <b>truncation</b> when applied to competing risks settings with dependent <b>left</b> <b>truncation.</b> We specifically focused on the methods for the proportional cause-specific hazards model and the Fine–Gray model. Simulation experiments showed that these methods are not in general robust against dependent <b>left</b> <b>truncation.</b> The magnitude of the bias was analogous {{to the strength of}} the association between <b>left</b> <b>truncation</b> and failure times, the effect of the covariate on the competing cause of failure, and the baseline hazard of <b>left</b> <b>truncation</b> time...|$|E
40|$|Lifetime {{data are}} usually assumed {{to stem from}} a {{continuous}} distribution supported on [0, b) for some b ≤ ∞. The continuity assumption implies that {{the support of the}} distribution does not have atom points, particularly not at 0. Accordingly, it seems reasonable that with an accurate measurement tool all data observations will be positive. This suggests that the true support may be truncated from the left. In this work we investigate the effects of adding a <b>left</b> <b>truncation</b> parameter to a continuous lifetime data statistical model. We consider two main settings: right truncation parametric models with possible <b>left</b> <b>truncation,</b> and exponential family models with possible <b>left</b> <b>truncation.</b> We analyze the performance of some optimal estimators constructed under the assumption of no <b>left</b> <b>truncation</b> when <b>left</b> <b>truncation</b> is present, and vice versa. We investigate both asymptotic and finite-sample behavior of the estimators. We show that when <b>left</b> <b>truncation</b> is not assumed but is, in fact present, the estimators have a constant bias term, and therefore will result in inaccurate and inefficient estimation. We also show that assuming <b>left</b> <b>truncation</b> where actually there is none, typically does not result in substantial inefficiency, and some estimators in this case are asymptotically unbiased and efficient...|$|E
40|$|Description A {{simple way}} of fitting {{detection}} functions to distance sampling data for both line and point transects. Adjustment term selection, <b>left</b> and right <b>truncation</b> {{as well as}} monotonicity constraints and binning are supported. Abundance and density estimates can also be calculated (via a Horvitz-Thompson-like estimator) if survey area information is provided. Version 0. 9. ...|$|R
30|$|Existing ISO {{segmented}} {{and continuous}} separation methods for differentiating {{the two components}} contained within a bi-Gaussian stratified surface were developed based on the fit of the probability material ratio curve. In the present study, because of the significant effect of the plateau component on tribological behavior such as asperity contact, wear and friction, a truncated separation method is proposed based on the truncation of the upper Gaussian component defined by zero skewness. The three separation methods are applied to real worn surfaces. Surface-separation and surface-reconstruction {{results show that the}} truncated method accurately captures the upper component identically to the ISO and continuous ones. The identification of the lower component characteristics requires performing a curve fit procedure on the data <b>left</b> after <b>truncation.</b> However, the truncated method fails in identifying the upper component when the material ratio of the transition is less than 9 %.|$|R
40|$|The {{amount of}} {{electronic}} information in Arabic and other non-English languages available, {{especially on the}} World Wide Web, is increasing. Searches for such information can be undertaken on engines developed with the English language in mind, but will these engines work as effectively in other languages? This article investigates the impact on retrieval of prefixes in Arabic, which are far more common than in English. Typically search engines such as AltaVista designed implicitly for English include right hand (suffix) but not <b>left</b> hand (prefix) <b>truncation.</b> A test collection of 27...|$|R
40|$|Necessary and {{sufficient}} conditions for consistency {{of a simple}} estimator of Kendall’s tau under bivariate censoring are presented. The results are extended to data subject to bivariate <b>left</b> <b>truncation</b> as well as right censoring. Some key words: Bivariate survival; Clayton’s model; Cross-ratio function; Frailty; <b>Left</b> <b>truncation.</b> 1...|$|E
30|$|In many applications, the {{available}} data come from a sampling scheme that causes loss of information in terms of <b>left</b> <b>truncation.</b> In some cases, in addition to <b>left</b> <b>truncation,</b> the data are weakly dependent. In this paper {{we are interested in}} deriving the asymptotic normality as well as a Berry-Esseen type bound for the kernel density estimator of left truncated and weakly dependent data.|$|E
40|$|In this paper, {{two types}} of kernel based estimators of hazard rate under <b>left</b> <b>truncation</b> and right {{censorship}} are considered. An asymptotic representation of the integrated squared error for both estimators is obtained. Also it is shown that the bandwidth selected by the data-based {{method of least squares}} cross-validation is asymptotically optimal in a compelling sense. <b>Left</b> <b>truncation</b> right censorship Hazard rate estimation Asymptotic representation Cross-validation Optimal bandwidth...|$|E
50|$|Despite AER’s successes on the {{international}} front, leadership transition issues and interference directed at its US affiliate- the Agribusiness Council (ABC)- traced to rival domestic agricultural interests- created sufficient doubt to enable opponents in AID to terminate {{a number of key}} grant contracts in mid-1992 (after reinstating ABC/AER following some initial confusion in early 1990). Congressional supporters of ABC/AER forced an inquiry which resulted in the abrupt termination/redirection of several AID/USDA careers, but after the smoke cleared several years later, AER watched its contract support dry up. The effect of this <b>truncation</b> <b>left</b> numerous AER organizational efforts and potentially supportive USAID missions in the lurch.|$|R
40|$|This {{paper is}} {{designed}} to give a complete overview of the literature that is available, {{as it relates to}} application of the Bayesian analysis model to investigate multiple group nonlinear structural equation models, also known as SEMs, including those having ordered categorical, dichotomous and categorical-dichotomous mixed variables. It will also work to summarize Bayesian multiple group nonlinear SEMs with nonlinear covariate variables, and latent variables in the structural model and both linear covariant and latent variable sin the measurement models. More specifically, it will be suggested that using hidden continuous normal distribution, including both right and <b>left</b> censoring and <b>truncation,</b> and interval censoring and truncation, can improve the Bayesian approach to multiple group nonlinear structural equation models when solving problems using ordered categorical and dichotomous data...|$|R
40|$|The use of {{truncated}} distributions arises {{often in}} a wide variety of scientific problems. In the literature, there are a lot of sampling schemes and proposals developed for various specific truncated distributions. So far, however, the study of the truncated multivariate t (TMVT) distribution is rarely discussed. In this paper, we first present general formulae for computing the first two moments of the TMVT distribution under the double truncation. We formulate the results as analytic matrix expressions, which can be directly computed in existing software. Results for the <b>left</b> and right <b>truncation</b> can be viewed as special cases. We then apply the slice sampling algorithm to generate random variates from the TMVT distribution by introducing auxiliary variables. This strategic approach can result in a series of full conditional densities that are of uniform distributions. Finally, several examples and practical applications are given to illustrate the effectiveness and importance of the proposed results. (C) 2011 Elsevier B. V. All rights reserved...|$|R
40|$|PURPOSE: To {{assess the}} impact of random <b>left</b> <b>truncation</b> of data on the {{estimation}} of time-dependent exposure effects. METHODS: A simulation study was conducted in which the relation between exposure and outcome was based on an immediate exposure effect, a first-time exposure effect, or a cumulative exposure effect. The individual probability of truncation, the moment of truncation, the exposure rate, and the incidence rate of the outcome were varied in different simulations. All observations before the moment of <b>left</b> <b>truncation</b> were omitted from the analysis. RESULTS: Random <b>left</b> <b>truncation</b> did not bias estimates of immediate exposure effects, but resulted in an overestimation of a cumulative exposure effect and underestimation of a first-time exposure effect. The magnitude of bias in estimation of cumulative exposure effects depends on a combination of exposure rate, probability of truncation, and proportion of follow-up time left truncated. CONCLUSIONS: In case of a cumulative or first-time exposure, <b>left</b> <b>truncation</b> can result in substantial bias in pharmacoepidemiologic studies. The potential for this bias likely differs between databases, which may lead to heterogeneity in estimated exposure effects between studies...|$|E
40|$|Observational {{epidemiological}} studies often include prevalent cases recruited {{at various times}} past diagnosis. This <b>left</b> <b>truncation</b> {{can be dealt with}} in non-parametric (Kaplan–Meier) and semi-parametric (Cox) time-to-event analyses, theoretically generating an unbiased hazard ratio (HR) when the proportional hazards (PH) assumption holds. However, concern remains that inclusion of prevalent cases in survival analysis results inevitably in HR bias. We used data on three well-established breast cancer prognosticators – clinical stage, histopathological grade and oestrogen receptor (ER) status – from the SEARCH study, a population-based study including 4470 invasive breast cancer cases (incident and prevalent), to evaluate empirically the effectiveness of allowing for <b>left</b> <b>truncation</b> in limiting HR bias. We found that HRs of prognostic factors changed over time and used extended Cox models incorporating time-dependent covariates. When comparing Cox models restricted to subjects ascertained within six months of diagnosis (incident cases) to models based on the full data set allowing for <b>left</b> <b>truncation,</b> we found no difference in parameter estimates (P= 0. 90, 0. 32 and 0. 95, for stage, grade and ER status respectively). Our results show that use of prevalent cases in an observational epidemiological study of breast cancer does not bias the HR in a <b>left</b> <b>truncation</b> Cox survival analysis, provided the PH assumption holds true...|$|E
40|$|Many {{of us are}} {{familiar}} with PROC LIFETEST {{as a tool for}} producing survival function estimates, but it is not widely known that PROC PHREG can also be used for this purpose. This is especially convenient in the case of left truncated data. <b>Left</b> <b>truncation</b> is present, for example, in studies of disease mortality where survival from the time of diagnosis is the outcome of interest even though patients may have been diagnosed many months or years prior to enrollment in the study. While PROC LIFETEST is not set up to handle this situation, PROC PHREG is, using the ENTRY = option to specify the <b>left</b> <b>truncation</b> time. Group differences in survival can be estimated with STRATA processing with a null model, which avoids the assumption of proportional hazards. We will provide examples from a disease registry that show why the issue of <b>left</b> <b>truncation</b> is important when analyzing survival data, and how ignoring it can get you into trouble...|$|E
40|$|We {{propose a}} method for fitting semiparametric models such as the {{proportional}} hazards (PH), additive risks (AR), and proportional odds (PO) models. Each of these semiparametric models implies that some transformation of the conditional cumulative hazard function (at each t) depends linearly on the covariates. The proposed method is based on nonparametric estimation of the conditional cumulative hazard function, forming a weighted average over a range of t-values, and subsequent use of least squares to estimate the parameters suggested by each model. An approximation to the optimal weight function is given. This allows semiparametric models to be fitted even in incomplete data cases where the partial likelihood fails (e. g., <b>left</b> censoring, right <b>truncation).</b> However, the main advantage of this method rests {{in the fact that}} neither the interpretation of the parameters nor the validity of the analysis depend on the appropriateness of the PH {{or any of the other}} semiparametric models. In fact, we propose an integrated method for data analysis where the role of the various semiparametric models is to suggest the best fitting transformation. A single continuous covariate and several categorical covariates (factors) are allowed. Simulation studies indicate that the test statistics and confidence intervals have good small-sample performance. A real data set is analyzed...|$|R
40|$|Abstract Background Use of rate {{adaptive}} atrioventricular (AV) delay remains {{controversial in}} patients with biventricular (Biv) pacing. We hypothesized that a shortened AV delay would provide optimal diastolic filling by allowing separation of early and late diastolic filling at increased heart rate (HR) in these patients. Methods 34 patients (75 ± 11 yrs, 24 M, LVEF 34 ± 12 %) with Biv and atrial pacing had optimal AV delay determined at baseline HR by Doppler echocardiography. Atrial pacing rate was then increased in 10 bpm increments {{to a maximum of}} 90 bpm. At each atrial pacing HR, optimal AV delay was determined by changing AV delay until best E and A wave separation was seen on mitral inflow pulsed wave (PW) Doppler (defined as increased atrial duration from baseline or prior pacemaker setting with minimal atrial <b>truncation).</b> <b>Left</b> ventricular (LV) systolic ejection time and velocity time integral (VTI) at fixed and optimal AV delay was also tested in 13 patients. Rate adaptive AV delay was then programmed according to the optimal AV delay at the highest HR tested and patients were followed for 1 month to assess change in NYHA class and Quality of Life Score as assessed by Minnesota Living with Heart Failure Questionnaire. Results 81 AV delays were evaluated at different atrial pacing rates. Optimal AV delay decreased as atrial paced HR increased (201 ms at 60 bpm, 187 ms at 70 bpm, 146 ms at 80 bpm and 123 ms at 90 bpm (ANOVA F-statistic = 15, p = 0. 0010). Diastolic filling time (P Conclusions Increased heart rate by atrial pacing {{in patients with}} Biv pacing causes compromise in diastolic filling time which can be improved by AV delay shortening. Aggressive AV delay shortening was required at heart rates in physiologic range to achieve optimal diastolic filling and was associated with an increase in LV ejection time during optimization. Functional class improved at 1 month post optimization using aggressive AV delay shortening algorithm derived from echo-guidance at the time of Biv pacemaker optimization. </p...|$|R
40|$|There is a {{surge in}} medical {{follow-up}} studies that include longitudinal covariates in the modeling of survival data. So far, the focus has been largely on right-censored survival data. We consider survival data that are subject to both <b>left</b> <b>truncation</b> and right censoring. <b>Left</b> <b>truncation</b> is well known to produce biased sample. The sampling bias issue has been resolved in the literature for the case which involves baseline or time-varying covariates that are observable. The problem remains open, however, for the important case where longitudinal covariates are present in survival models. A joint likelihood approach {{has been shown in}} the literature to provide an effective way to overcome those difficulties for right-censored data, but this approach faces substantial additional challenges in the presence of <b>left</b> <b>truncation.</b> Here we thus propose an alternative likelihood to overcome these difficulties and show that the regression coefficient in the survival component can be estimated unbiasedly and efficiently. Issues about the bias for the longitudinal component are discussed. The new approach is illustrated numerically through simulations and data from a multi-center AIDS cohort study. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Parametric {{proportional}} hazards {{fitting with}} <b>left</b> <b>truncation</b> and right censoring for common fami-lies of distributions,piecewise constant hazards, and discrete models. AFT regression for left truncated and right censored data. Binary and Poisson regression for clustered data, fixed and random effects with bootstrapping...|$|E
40|$|The cohort {{under study}} {{comprises}} A-bomb survivors residing in Hiroshima Prefecture since 1968. After this year, thousands of survivors were newly recognized every year. The {{aim of this}} study is to determine whether the survival experience of the late entrants to the cohort is significantly different from the registered population in 1968. Parametric models that account for <b>left</b> <b>truncation</b> and competing risks were developed by using sub-hazard functions. A Weibull distribution was used to determine the possible existence of a late entry effect in Hiroshima A-bomb survivors. The competing risks framework shows that there might be a late entry effect in the male and female groups. Our findings are congruent with previous studies analysing similar populations. competing risks, late entry effect, <b>left</b> <b>truncation,</b> sub-hazard function, Weibull distribution,...|$|E
40|$|Let X be the {{variable}} of interest with distribution function F, hazard function λ and Y be another independent random variable. In the random <b>left</b> <b>truncation</b> model one observes only those (X,Y) for which Y ≤ X. ^ In this study nonparametric kernel {{estimates for the}} hazard function λ and its derivatives are considered under the random <b>left</b> <b>truncation</b> model. The bias, variance, consistency and local asymptotic normality of the estimators are established. In particular, the local asymptotic normality is derived via the Hajek projection method. For the derivatives of the hazard rates kernel functions with compact support are employed and for the hazard rates the results are extended to a more general class of kernel functions. ^ The optimum bandwidth that minimizes the asymptotic MSE is derived and {{it depends on the}} true unknown distribution. An adaptive bandwidth based on the data is proposed, which provides a kernel estimator with the same limiting distribution as the kernel estimator employing the optimal bandwidth. The proof involves weak convergence of a bandwidth process. ^ The above results are extended to random <b>left</b> <b>truncation</b> and right censoring models. An alternative approach to estimating the hazard function is also presented. The procedures are applied to simulated samples and the real data example. ...|$|E
40|$|In {{longitudinal}} studies of developmental and disease processes, participants are followed prospectively with intermediate milestones identified as they occur. Frequently, studies enroll participants over {{a range of}} ages including ages at which some participants’ milestones have already passed. Ages at milestones that occur prior to study entry are left censored if individuals are enrolled in the study or left truncated if they are not. The authors examined the bias incurred by ignoring these issues when estimating the distribution of age at milestones or the time between 2 milestones. Methods that account for <b>left</b> <b>truncation</b> and censoring are considered. Data on the menopausal transition are used to illustrate the problem. Simulations show that bias can be substantial and that standard errors can be severely underestimated in naïve analyses that ignore <b>left</b> <b>truncation.</b> Bias can be reduced when analyses account for <b>left</b> <b>truncation,</b> although the results are unstable when the fraction truncated is high. Simulations suggest that a better solution, when possible, is to modify the study design so that information on current status (i. e., {{whether or not a}} milestone has passed) is collected on all potential participants, analyzing those who are past the milestone at the time of recruitment as left censored rather than excluding such individuals from the analysis...|$|E
40|$|In this note, {{we study}} the {{convergence}} of increments for cumulative hazard function based on data which subject to both right censoring and <b>left</b> <b>truncation.</b> As an example of its application, a rate of convergence theorem concerning the nearest-neighbor type estimators is formulated. Hazard estimates Censoring Survival analysis Truncation...|$|E
40|$|In {{survival}} or reliability studies, it {{is common}} to have truncated data due to the limited time span of the study or dropouts of the subjects for vari-ous reasons. The estimation of survivor function under <b>left</b> <b>truncation</b> was first discussed by Kaplan and Meier by extending the well known product-limit estimator of the survivor function. The focus of this paper is on the nonparametric estimation of the survivor function and the cause-specific sub-distribution functions in bivariate competing risks set up, when the obser-vations are subject to random <b>left</b> <b>truncation</b> and right censoring. Various asymptotic properties of the estimators are discussed. A simulation study discussing the empirical behaviour of the estimator is carried out. We illus-trate the procedure by a data set. AMS (2000) subject classification. Primary 62 G 05; 62 P 10...|$|E
40|$|The paper {{deals with}} the family of {{irreducible}} <b>left</b> <b>truncation</b> invariant bivariate copulas, which admit a nontrivial lower tail dependence function. Such copulas, similarly as the Archimedean ones, are characterized by a functional parameter, a generator being an increasing convex function. We provide a nonparametric, piece-wise linear estimator of such generators...|$|E
40|$|Spontaneous {{abortion}} {{studies that}} recruit pregnant women are left truncated because an unknown {{proportion of the}} source population experiences losses prior to enrollment. Unconditional logistic regression, commonly used in such studies, ignores <b>left</b> <b>truncation,</b> whereas survival analysis can accommodate <b>left</b> <b>truncation</b> and is therefore more appropriate. This study assessed the magnitude of bias introduced by fitting logistic versus Cox models using left-truncated data from a 1998 US pregnancy cohort study (n 5, 104) of trihalomethanes and spontaneous abortion. In addition, the conditions producing bias were explored by using simulated exposure data. The odds ratios and hazard ratios from the actual study differed by 10 % or less. However, when the exposed women entered observation earlier on average than those unexposed, the hazard ratio was closer to the null than the odds ratio, whereas the reverse was true when the exposed entered later. The simulation suggests that bias in the odds ratio will exceed 20 %when average gestational age at entry for the exposed versus the unexposed differs by 10 days or more, as has been observed regarding some socioeconomic factors, such as education and ethnicity. Cox regression can correct for <b>left</b> <b>truncation</b> and is no more difficult to perform than logistic regression. abortion, spontaneous; bias (epidemiology); logistic models; survival analysis; trihalomethanes Abbreviation: TTHM, total trihalomethane. Most studies of spontaneous abortion are left truncate...|$|E
40|$|IntroductionOccupational {{exposure}} to endotoxin, found in Gram-negative bacteria in organic material, {{has been associated}} predominantly with a reduced risk of lung cancer among workers. An inverse exposure 22 ̆ 0 ac 2 ̆ 01 cresponse gradient among women textile workers in Shanghai, China, has been reported previously. In this case 22 ̆ 0 ac 2 ̆ 01 ccohort study, we investigated the influence of <b>left</b> <b>truncation,</b> which can itself induce a downward trend, on the observed association. MethodsSubjects were enrolled between 1989 and 1991 and followed until 1998. The data were left-truncated as all subjects were hired before baseline. An analysis was performed with 3038 subcohort members and 602 cases of incident lung cancer. To evaluate <b>left</b> <b>truncation,</b> we compared lung cancer rates in those hired longer ago with those hired more recently among unexposed subjects. Cox proportional hazards modelling was used to estimate incident rate ratios (IRRs) and 95...|$|E
40|$|Cataloged from PDF {{version of}} article. Suppose that we observe bivariate data (X,. q) only when Y, < Xi (<b>left</b> <b>truncation).</b> Denote with F the {{marginal}} d. f. of the X’s In this paper we derive a Bahadur-type representation for the quantile {{function of the}} pertaining product-limit estimator of F. As an application we obtain confidence intervals and bands for quantiles of F...|$|E
40|$|In <b>left</b> <b>truncation</b> {{and right}} {{censoring}} models one observes i. i. d. {{samples from the}} triplet (T, Z, delta) only if T {{less than or equal}} to Z, where Z = min(Y, C) and delta is one if Z = Y and zero otherwise. Here, Y is the variable of interest, T is the truncating variable and C is the censoring variable. Recently, Gurler and Gijbels (1996) proposed a nonparametric estimator for the bivariate distribution function when one of the components is subject to <b>left</b> <b>truncation</b> and right censoring. An asymptotic representation of this estimator as a mean of i. i. d. random variables with a negligible remainder term has been developed. This result establishes the convergence to a two time parameter Gaussian process. The covariance structure of the limiting process is quite complicated however, and is derived in this paper. We also consider the special case of censoring only. In this case the general expression for the variance function reduces to a simpler formula. status: publishe...|$|E
40|$|We propose new {{procedures}} for estimating the univariate quantities {{of interest in}} both additive and multiplicative nonparametric marker dependent hazard models. We work with a full counting process framework that allows for <b>left</b> <b>truncation</b> and right censoring. Our procedures are based on kernels and {{on the idea of}} marginal integration. We provide a central limit theorem for our estimator. Additive model, censoring, kernel, proportional hazards, survival analysis...|$|E
40|$|Abstract: In many {{applications}} involving follow-up studies, individuals ' lifetimes may {{be subjected to}} <b>left</b> <b>truncation</b> and right censoring. Here we suppose the truncating variable can be observed. And we propose a semiparametric procedure to regress the median of the lifetime variable on the covariates. References [1] Zhou, Y. and Yip (1999). A Strong Representation of the Product-limit Estimator for the Left Truncated an...|$|E
40|$|In {{this paper}} we study uniform {{versions}} of two limit theorems in random <b>left</b> <b>truncation</b> model (RLTM). The law {{of large numbers}} (LLN) and the central limit theorem (CLT) have been obtained under the bracketing entropy conditions in this setting. The uniform LLN and the uniform CLT of the present paper extend the one dimensional LLN and the one dimensional CLT under RLTM respectively. Comment: 10 page...|$|E
40|$|The nonparametric {{monotone}} MLE is used {{to overcome}} the severe under-estimation of survival functions by the NPMLE for left truncated data when a monotone hazard assumption is appropriate. In this paper, we establish {{the consistency of the}} monotone MLE for interval-censored data with or without <b>left</b> <b>truncation</b> in two different ways corresponding to two different realistic conditions. Cumulative hazard function Monotone hazard Nonparametric maximum likelihood Vague convergence...|$|E
40|$|Suppose that {{we observe}} bivariate data (Xi, Yi) only when Yi [less-than-or-equals, slant] Xi (<b>left</b> <b>truncation).</b> Denote with F the {{marginal}} d. f. of the X's. In this paper we derive a Bahadur-type representation for the quantile {{function of the}} pertaining product-limit estimator of F. As an application we obtain confidence intervals and bands for quantiles of F. Truncated data Bahadur representation confidence interval product-limit estimator survival function...|$|E
40|$|The {{stepped wedge}} design {{is a unique}} {{clinical}} trial design that allows for a sequential introduction of an intervention. However, the statistical analysis is unclear when this design is applied in survival data. The time-dependent introduction of the intervention in combination with terminal endpoints and interval censoring makes the analysis more complicated. In this paper, a time-on-study scale discrete survival model was constructed. Simulations were conducted primarily to study the performance of our model for different settings of the stepped wedge design. Secondary, we compared our approach to continuous Cox proportional hazard model. The {{results show that the}} discrete survival model estimates the intervention effects unbiasedly. If the length of the censoring interval is increased, the precision of the estimates is decreased. Without <b>left</b> <b>truncation</b> and late entry, the number of steps improves the precision of the estimates, whereas in combination of <b>left</b> <b>truncation</b> and late entry, the number of steps decreases the precision. Given the same number of participants and clusters, a parallel group design has higher precision than a stepped wedge design. Copyright (c) 2016 John Wiley & Sons, Ltd...|$|E
40|$|Studies {{of chronic}} {{life-threatening}} diseases often involve both mortality and morbidity. In observational studies, the data {{may also be}} subject to administrative <b>left</b> <b>truncation</b> and right censoring. Since mortality and morbidity may be correlated and mortality may censor morbidity, the Lynden-Bell estimator for left truncated and right censored data may be biased for estimating the marginal survival function of the non-terminal event. We propose a semiparametric estimator for this survival function based on a joint model for the two time-to-event variables, which utilizes the gamma frailty specification {{in the region of}} the observable data. Firstly, we develop a novel estimator for the gamma frailty parameter under <b>left</b> <b>truncation.</b> Using this estimator, we then derive a closed form estimator for the marginal distribution of the non-terminal event. The large sample properties of the estimators are established via asymptotic theory. The methodology performs well with moderate sample sizes, both in simulations and in an analysis of data from a diabetes registry. Bivariate survival function, Concordance probability, Copula, Semi-competing risks, truncation,...|$|E
