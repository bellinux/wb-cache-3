11|39|Public
50|$|Receivers {{that are}} used for {{compliance}} testing have to fulfill the basic emc standard CISPR 16-1-1. CISPR 16-1-1 defines requirements for indication of CW Signals and pulses. The amplitude range where these requirements are met is called CISPR indication range. Within this range the receiver {{can be used for}} compliance tests. Usually EMI receivers have a CISPR indication range that starts about 6dB above the noise floor. The performance usually demonstrated by a linerity check for sinusoidal signals and broadband pulses. This <b>linearity</b> <b>check</b> is performed over the amplitude range starting from typical levels of 10dBuV. Some EMI receivers, even if called full compliant have a CISPR indication range that starts at higher levels e.g. 40dBuV. Typically for such a receivers only one level e.g. 60dBuV is presented. A demonstration of CISPR compliance at lower levels cannot be demonstrated.|$|E
40|$|Abstract. Gun pipe <b>linearity</b> <b>check</b> {{and measure}} belongs to {{difficult}} deep-hole space check and measure. After the deviation data is obtained using designed <b>linearity</b> <b>check</b> and measure instrument for gun pipe, it need carry out analysis and operation {{to get the}} error. The essence of using minimum envelope region method to resolve and evaluate gun pipe linearity error is one optimizing problem about envelopment column surface, wherefore, it built error solution model of space linearity based on genetic algorithm, and it developed one solution and analysis software system using VB program. Application shows that the algorithm {{can be used to}} resolve linearity error quickly and conveniently, and the precision is good...|$|E
30|$|Spearman’s rank {{correlation}} coefficient ρ was determined for {{the correlations between}} subject-specific WSS and WT, WT and diameter as well as WSS and diameter, using all pixels. A Fisher z-transformation was applied to normalize ρ, enabling a Student’s t-test {{to determine if the}} z-values averaged across subjects were significantly different from 0. p values lower than 0.05 were considered significant. Linear regression was performed as well and R 2 reported. A <b>linearity</b> <b>check</b> was additionally performed.|$|E
30|$|For <b>linearity</b> <b>checking</b> Stock {{solution}} (2 mg/mL) {{containing a}} mixture of saccharin, phenacetin, paracetamol and methaqualone was further diluted with mobile phase to give the final concentration of 0.1 μg/mL. And these solutions were injected into the HPLC system and the resultants peak areas of each component were recorded.|$|R
40|$|This note {{describes}} a resistor network that enables automated <b>linearity</b> <b>checks</b> of resistance-thermometry measurements. The network {{is made in}} such a manner that the voltages across any number of resistors in a resistor series are read to get separate four-terminal values interrelated by the formula for the series connection. Linearity tests of resistance bridges show that the network can resolve deviations from linearity down to +/- 20 mu ohm for resistance measurements from 32 to 284 ohm...|$|R
5000|$|In {{addition}} to providing ranking indices allow to <b>check</b> <b>linearity</b> and monotonicity of objective with respect to each input ...|$|R
40|$|As a {{step toward}} realizing the quantum current standard, single-electron pumping of {{parallel}} SINIS turnstiles was performed. Eleven SINIS turnstiles were fabricated on the same chip, {{and the number of}} operating SINIS turnstiles was increased one by one. As a result, 11 turnstiles were operated simultaneously up to 100 MHz, resulting in a pumped dc of 176. 2 pA. We also did a <b>linearity</b> <b>check</b> toward parallelization. This paper for increasing the pumped current by parallelization will lead to practical use of stable and accurate dc sources with tiny values of the output current and the metrology triangle experiment for SI redefinition...|$|E
40|$|This work {{provides}} a new method for the absolute amplitude calibration of the GEO[*] 600 interferometric gravitational wave detector. It {{is motivated by}} ongoing data analysis efforts {{carried out by the}} LIGO Scientific and Virgo Collaborations. Previously, arm length difference measurements of GEO[*] 600 have been calibrated against the length of the first mode cleaner. Here, we re-evaluate this process, providing new measurements, and add a much simpler and more elegant method which calibrates against the laser wavelength. Since our calibration measurements involve injections of signals at least five orders of magnitude larger than possible gravitational wave signals, we also carry out a <b>linearity</b> <b>check</b> of the electrostatic drive (ESD) actuators used to make the injections. We find that these actuators exhibit no more than a 0. 3 % deviation from linearity over nearly six orders of magnitude. The new absolute amplitude calibration method that uses the laser wavelength as a length standard is shown here to be superior to the old one. Using this method, we show that the ESD gain over a two-year period varies by less than 4 %...|$|E
40|$|The aim of {{text frame}} {{classification}} technique is to label a video frame as text or non-text before text detection and recognition. It {{is an essential}} step prior to text detection because text detection methods assume the input to be a text frame. Consequently, when a non-text frame is subjected to text detection, the precision of the text detection method decreases because of false positives. In this paper a new text frame classification approach based on component linearity is proposed. The method firstly obtains probable text clusters from the gradient values of the RGB images of an input video frame. The Sobel edges corresponding to the text cluster are then extracted and used for further processing. Next, the method proposes to eliminate false text components before undertaking a <b>linearity</b> <b>check</b> where the linearity of the text components is determined using their centroids in a piece-wise manner. If the components in a frame satisfy the defined linearity condition, then the frame is considered as a text frame; otherwise it is considered as a non-text frame. The proposed method has been tested on standard text and non-text datasets of different orientations to demonstrate that it is independent of orientation. A comparative study with the existing method shows that the proposed method is superior in terms of classification rate and processing time. (C) 2014 Elsevier Ltd. All rights reserved...|$|E
40|$|Solid State array sensors are {{ubiquitous}} nowadays {{for obtaining}} gross field images in numerous scientific and engineering applications including optical diagnostics and instrumentation. Linear responses of these sensors are often required as in interferometry, light scattering and attenuation measurements, and photometry. In most applications, the linearity is usually {{taken to be}} granted without thorough quantitative assessment or correction through calibration. Upper-grade CCD cameras of high price may offer better linearity: however, they also require <b>linearity</b> <b>checking</b> and correction if necessary. Intermediate- or low-grade CCD cameras {{are more likely to}} need calibration for linearity. Here, we present two very simple approaches: one for quickly <b>checking</b> camera <b>linearity</b> without any additional setup and one for precisely correcting nonlinear sensor responses. It is believed that after calibration, those sensors of intermediate or low grade can function as effectively as their expensive counterpart...|$|R
5000|$|If [...] and [...] {{are complex}} vector spaces, a {{function}} [...] is antilinear ifWith {{the use of}} the conjugate vector space , an antilinear map [...] can be regarded as an ordinary linear map of type [...] The <b>linearity</b> is <b>checked</b> by noting:Conversely, any linear map defined on [...] gives rise to an antilinear map on [...]|$|R
40|$|The study {{presents}} {{results of}} calibration experiments for four PMP compliant CPCs (3 TSI’s 3790 and a TSI’s 3010 operating at lower saturator-condenser temperature differences) tested against {{a range of}} calibration particles, including Poly(alpha) -Olefin (PAO), tetracontane (C 40), tetradecane (C 14) and hexadecane (C 16). The experimental {{data were analyzed using}} a numerical model developed by Giechaskiel et al. (Journal of Aerosol Science, 42 : 20 - 37) which was amended to investigate the effect of line tension, curvature dependence of the surface tension as well as condensational growth of activated particles. The study also investigated possibilities to verify the linearity of CPCs against an electrometer in the sub 2000 #/cm 3 concentration range. The use of elevated sample flowrates and multiply charged particles of well defined charge status, allowed for some <b>linearity</b> <b>checks</b> down to 300 #/cm 3 concentration levels. JRC. F. 8 -Sustainable Transpor...|$|R
40|$|Summary: Five {{different}} methods for {{the determination of}} 3 -methoxy- 4 -hydroxymandelic acid (vanilmandel-ic acid, VMA) in urine were compared: a GLC-FID catecholamine metabolite profiling method, an HPLC method with electrochemical detection, the method of Pisano et al. ((1962) Clin. Chim. Acta 7, 285 — 291), a one dimensional paper Chromatographie method with diazotized p-nitroaniJine staining and the commercially available Bio-Rad VMA by Column Test. The comparison consisted of an imprecision study, a <b>linearity</b> <b>check,</b> a recovery study, a split sample comparison and an interference study. The best results of the impreci-sion study (n = 8) were found with the Bio-Rad and the HPLC method (within-run imprecision had a coeffi-cient of Variation (CV) of 5. 1 % and 1. 4 %; between-days CV of 5. 9 % and 6. 0 % respectively for values of 32. 4 μπιοΐ/ΐ and 24. 5 μπιοΙ/ 1). The Pisano method had the poorest within-run CV (14. 6 %) and between-days CV (16. 8 %) for a value of 23. 2 μΐηοΐ/ΐ. All methods showed good linearity. The mean recovery of the HPLC method was 101. 3 %; the mean recovery of the other four methods ranged from 93. 9 %— 96. 0 %. The split sample comparison showed that {{the accuracy of the}} HPLC, the GLC and the Pisano method is comparable. The accuracy of the paper Chromatographie method and the Bio-Rad method had a positive bias compared with the HPLC method. Especially the positive bias of the Bio-Rad method can be very large. The HPLC method was not influenced by the compounds tested in the interference study, whereas the GLC method i...|$|E
40|$|Several {{methods have}} been {{proposed}} to reconstruct a High Dy-namic Range (HDR) image from differently exposed photographs that are either scanned films or firmware-processed outputs of digi-tal cameras [Debevec et al, 1997]. We use instead the unprocessed sensor data of digital cameras and show how this simplifies the HDR image reconstruction and more accurately reproduces color information from the original scene. 1 <b>Linearity</b> <b>check</b> Though sensors of standard digital cameras can hold up to 12 bits of digitized signal, in-camera processes scale it down to just 8 bits of tonal information. Using raw output from the camera sensors pro-vides a larger coverage of the dynamic range at a higher precision. It also enables a more accurate computation of the luminance value since {{the response of the}} sensors is considered to be linear relative to the input signal. To check this important property of linearity, we photographed an HDR scene under 36 exposures spaced by 1 / 3 EV using a Canon 1 D Mark II camera set to output raw data. We then plotted the raw data pixel values relative to exposure for four levels of luminance (figure 1). Figure 1 : Canon 1 D Mark II raw data tonal response. Having linear data at our disposal eliminates the need for calibra-tion. It also makes the HDR image calculation depend on hardware specification only and not on firmware processes that may differ across the exposure range and vary depending on the characteris-tics of the scene. 2 Color gamut Algorithms for constructing HDR images focus on the camera tonal rather than color response. An exception is the calibration tech-nique in [Goesele et al 2001] where ICC profiles are used. We take the same approach, except that we profile the camera sensor instead of the low dynamic range output resulting from in-camera process-ing. This has for advantage to preserve the full color gamut of th...|$|E
40|$|The present NCAR {{instrument}} for HO 2 /RO 2 measurements {{has been described}} previously. It {{is based on the}} reactions involving HO 2, RO 2, and HO radicals with CO and NO. Since (HO 2) + (RO 2) + (HO) is much greater than (HO) for most atmospheres, it is useful as a peroxy radical detector. Operation of the instrument depends on the creation of a chemical chain reaction which is initiated as HO 2 and RO 2 radicals in ambient air encounter added NO gas; this forms an NO 2 molecule and an HO or RO radical: HO 2 (RO 2) + NO yields HO(RO) + NO 2. RO radicals react relatively efficiently with O 2 to form an HO 2 radical, and subsequently an HO-radical, by reaction with NO. CO gas added to the reaction chamber during part of the operating cycle, recycles the HO to HO 2; HO + CO (+O 2) yields HO 2 + CO 2. The reaction sequence may form several hundred NO 2 molecules per HO 2 (RO 2) originally present, before chain termination occurs. The added CO is replaced by N 2 addition periodically so that the chain reaction is suppressed, and a 'blank' signal resulting from NO 2, O 3 and possibly other NO 2 -forming species (non-chain processes) in ambient air is recorded. The difference between the signal with and without CO is proportional to the peroxy radical concentration. The NO 2 produced is monitored using a sensitive luminol chemiluminescence detector system. In the NCAR instrument the length of the amplification chain is determined using a stable source of HO 2 radicals (H 2 O 2 thermal decomposition); the ratio of the signal seen with CO present to that with N 2 present gives the sensitivity of the instrument to HO 2 (molecules of NO 2 formed/peroxy radical). The instrument is automated to carry out in hourly repeated cycles: (1) chain length determination; (2) NO 2 calibration; and (3) <b>linearity</b> <b>check</b> on the response of signals. One minute averages of signals are normally recorded. The sensitivity of the instrument to detect peroxy radicals is in the pptv range. The present instrument has operated continuously (24 hr/day) in the field studies which extended {{over a period of several}} weeks. The major advantages of this instrument are as follows: (1) its relative simplicity; (2) low power requirements; and (3) its rapid response to all types of peroxy radicals [...] HO 2, CH 3 O 2 and the higher alkyl and acyl peroxy radicals; however not all RO 2 species generate HO 2 radicals with perfect efficiency and hence have somewhat lower response/molecule than HO 2 radicals...|$|E
40|$|In 1997, {{there were}} five fills with two or more {{energies}} measured by resonant depolarisation, RDP. These increased the RDP lever arm to 14 GeV, increased the maximum RDP energy to 55 GeV and allowed some <b>linearity</b> <b>checks.</b> The energy scale is extrapolated to physics energies using the 16 NMR probes. These are also crosscalibrated with the flux loop, which samples 97 per cent of the bending field. The status of {{the analysis of the}} NMR/RDP and NMR/flux-loop comparisons is presented, and the requests for measurements in 1998 are given. 1 INTRODUCTION The bulk of this paper discusses the problem of establishing the beam energy at LEP 2 physics energies by the method of magnetic extrapolation. The energy scale at lower (LEP 1) energies can be measured very precisely by resonant depolarisation. The extrapolation to physics energy depends on a comparison of these "polarisation" energies with the magnetic fields measured by NMR probes in the LEP dipoles, and on the relationship between the NMR [...] ...|$|R
50|$|A {{linear type}} system {{is similar to}} C++'s unique_ptr class, which behaves like a pointer but can only be moved (i.e. not copied) in an assignment. Although the <b>linearity</b> {{constraint}} is <b>checked</b> at compile time, dereferencing an invalidated unique_ptr causes undefined behavior at run-time.|$|R
40|$|The Eastman Kodak Ektachem 400 Analyzer was {{evaluated}} {{in terms of}} its precision, linearity, accuracy, and interferences for two colorimetric tests (neonatal bilirubin and albumin), a two point rate colorimetric test (amylase), and four potentiometric tests (sodium, potassium, chloride and carbon dioxide). The precision study results obtained were comparable to those of other laboratory instruments for five of the seven tests under consideration. The exceptions were carbon dioxide at the high range (30 mmol/L) and albumin at the low range (2 g/dl). <b>Linearity</b> <b>checks</b> were satisfactory for all seven tests but albumin, where a negative bias was observed in readings below 2 g/dl. Accuracy testing by comparison of patient results of the Ektachem technology and other laboratory methods was acceptable for all seven tests except albumin and amylase. Interference studies indicate that the Ektachem methodologies are less susceptible to elevated triglyceride and protein interference than comparable laboratory methods for sodium and potassium analysis. As a result of this evaluation (and other studies not presented here), the Ektachem 400 Analyzer was implemented in this laboratory for all tests except amylase, albumin and creatinine, i. e., glucose, urea nitrogen, sodium, potassium, chloride, carbon dioxide, calcium, uric acid, cholesterol, triglyceride, total protein, ammonia, and neonatal bilirubin were acceptable...|$|R
40|$|Remote {{infrared}} (IR) sensing {{provides a}} valuable {{method for detection}} and identification of materials associated with nuclear proliferation. Current challenges for remote sensors include minimizing the size, mass, and power requirements for cheaper, smaller, and more deployable instruments without affecting the measurement performance. One area that is often overlooked is sensor calibration design that is optimized to minimize the cost, size, weight, {{and power of the}} payload. Yet, an on-board calibration system is essential to account for changes in the detector response once the instrument has been removed from the laboratory. The Calibration Systems project at Pacific Northwest National Laboratory (PNNL) is aimed towards developing and demonstrating compact quantum cascade (QC) laser-based calibration systems for infrared sensor systems in order to provide both a spectral and radiometric calibration while minimizing the impact on the instrument payload. In FY 05, PNNL demonstrated a multi-level radiance scheme that provides six radiance levels for an enhanced <b>linearity</b> <b>check</b> compared to the currently accepted two-point scheme. PNNL began testing the repeatability of this scheme using a cryogenically cooled, single-mode quantum cascade laser (QCL). A cyclic variation in the power was observed that was attributed to the thermal cycling of the laser's dewar. In FY 06, PNNL continued testing this scheme and installed an auxiliary liquid nitrogen reservoir to limit the thermal cycling effects. Although better repeatability was achieved over a longer time period, power fluctuations were still observed due to the thermal cycling. Due to the limitations with the cryogenic system, PNNL began testing Fabry-Perot QCLs that operate continuous-wave (cw) or quasi-cw at room temperature (RT) in FY 06. PNNL demonstrated a multi-level scheme that provides five radiance levels in 105 seconds with excellent repeatability. We have continued testing this repeatability in FY 07. A burn-in effect appears in which the power increases over a certain time period. Repeatability better than 1 %, however, is demonstrated for most of the radiance levels after this initial burn-in. In FY 06, PNNL also began investigating a fiber-coupled RT QCL for a compact IR calibration source. PNNL demonstrated a uniform beam profile by measuring a time-averaged response and modulating the fiber optic with a motor to minimize the effects of speckle. In FY 07, PNNL examined the power stability of fiber-coupled QCLs. Feedback appears to degrade the stability so that anti-reflective coatings for fibers may be essential. In FY 07, PNNL continued to investigate the stability of room temperature QCLs as well as the measurement technique to provide a quantitative estimate for the measurement uncertainty. We designed and built a custom environmental enclosure to reduce the measurement uncertainty. After an initial burn-in, we have achieved uncertainties better than 0. 1 % for data collected over almost 100 hours of operation. We also built a bench-top system to demonstrate how the QC laser can be used to calibrate a microbolometer array and illustrated the importance of a multi-point calibration...|$|E
40|$|A predição dos fenômenos naturais e sociais sempre foi um dos tópicos de maior interesse dos pesquisadores. No {{marketing}} e na psicologia social, diversas teorias foram elaboradas não somente para entender e explicar, mas também para prever, o comportamento dos indivíduos. Esta tese tem por objetivos principais testar três teorias da ação: teoria da ação racional (FISHBEIN; AJZEN, 1975), teoria do comportamento planejado (AJZEN, 1985) e teoria da tentativa (BAGOZZI; WARSHAW, 1990); e comprovar se a teoria da tentativa possui maior poder de predição das intenções comportamentais e do comportamento do que as outras duas, como preconizado pelos seus criadores. O comportamento sob questão consiste na tentativa de perder peso nos próximos trinta dias. Esta é uma pesquisa descritiva, dividida em duas partes: a primeira de abordagem qualitativa e a segunda de abordagem quantitativa. Na primeira etapa, foram aplicados 170 roteiros com questões abertas considerando o comportamento de tentar perder peso, as quais abordavam, basicamente, as vantagens e desvantagens, o que facilitaria, o que dificultaria, pessoas que eram importantes neste tipo de comportamento e as emoções antecipadas relativas a esse comportamento. Em seguida, foi elaborado o questionário, o qual foi submetido a um pré-teste com 55 pessoas. A partir das considerações identificadas nesta fase, procedeu-se a pequenas mudanças no questionário e em sua aplicação. Foram obtidos 650 questionários, dos quais foram aproveitados 426. Esta amostra foi composta pelos alunos do Centro Universitário UNA de Belo Horizonte. O questionário, além de contar com os construtos elaborados pelos autores das teorias testadas nesta tese, acrescido de três construtos complementares, com o intuito de testar também extensões das teorias originais: nível de envolvimento com o comportamento (desenvolvida por Zaichkowsky em 1985 e 1990), força das atitudes (elaborada por Laville et al., em 1998) e emoções antecipadas (desenvolvido a partir dos resultados da pesquisa qualitativa). Procedeu-se ao exame dos dados, etapa que consistiu nas seguintes atividades: verificação da análise de conteúdo, análise dos dados faltantes, análise da normalidade, identificação das observações atípicas, verificação da linearidade, verificação da unidimensionalidade, verificação da confiabilidade das escalas, verificação da validade convergente e verificação da validade discriminante. Os objetivos desta pesquisa foram comprovados por meio da verificação da validade nomológica das teorias testadas e de suas extensões. O instrumento estatístico utilizado foi a modelagem de equações estruturais. Os resultados obtidos indicam que a teoria da ação racional possui completa validade nomológica. A teoria do comportamentamento planejado e teoria da tentativa possuem validade nomológica parcial em virtude de que alguns construtos não apresentam relações estatisticamente significativas. Em relação ao poder de predição das intenções comportamentais e do comportamento, a teoria da tentativa apresentou os maiores valores. A adição do construto nível de envolvimento aumentou substancialmente o poder de predição da teoria da tentativa e a adição do construto força das atitudes teve uma influência no poder de predição da teoria da ação racional e da teoria do comportamento planejado. The {{prediction of}} natural and social phenomena {{has always been one}} of the topics of greatest interest of the researchers. In marketing and social psychology, several theories have been developed to understand, explain, and to predict the individual´ behavior. This thesis has its objective to test three main action´s theories: theory of reasoned action (FISHBEIN; AJZEN, 1975), theory of planned behavior (AJZEN, 1985) and theory of trying (BAGOZZI; WARSHAW, 1990), and prove if the theory of trying has greater predictive power of behavioral intentions and of the behavior than the two others, as recommended by its creators. The behavior in question is an attempt to lose weight in the next thirty days. This is a descriptive research, divided into two parts: the first of a qualitative approach and the second of a quantitative approach. In the first step, 170 scripts were applied with open questions considering the behavior of trying to lose weight, which dealt basically the advantages and disadvantages, which would make it easy, which would make it difficult, people who were important in this type of behavior and the emotions in advance for that behavior. Then the questionnaire was drew up, which was submitted to a pretest with 55 people. From the considerations identified in this step, the small changes were maked in the questionnaire and its application. Returned 650 questionnaires, which 426 were recovered. This sample was composed of students of the UNA University Center in Belo Horizonte city. The questionnaire, besides having the constructs developed by the authors of the theories tested in this thesis, more three additional constructs, in order to also test extensions of the original theories: the level of involvement with the behavior (developed by Zaichkowsky in 1985 and 1990), strength of the attitudes (drawn by Laville et al. in 1998) and anticipated emotions (developed from the results of qualitative research). There has been an examination of the data, this step consisted of the following activities: verification of content analysis, analysis of missing data, analysis of the normality, identifying atypical observations, the <b>linearity</b> <b>check,</b> unidimensionality checking, checking the reliability of the scales, convergent validity checking and discriminant validity checking. The objectives of this research were supported by verifying of the nomological validity of the theories tested and its extensions. The statistical tool used was a structural equation modeling. The results indicate that the theory of reasoned action has complete nomological validity. The theory of planned behavior and the theory of trying have nomological validity in part because some constructs do not show statistically significant relationships. Regarding the predictive power of behavioral intentions and of the behavior, the theory of trying had the highest values. The addition of the construct level of involvement has substantially increased the predictive power of the theory of trying and adding the construct of attitude strength has an influence on the predictive power of the theory of reasoned action and the theory of planned behavior...|$|E
40|$|A dual {{ejector system}} with an {{intermediate}} Evaporating Tube was circulated at 11 laboratories {{to measure the}} Particle Concentration Reduction Factors (PCRF) at 30, 50 and 100 nm (as required by the legislation). In addition to this “Golden” Volatile Particle Remover (GVPR), a PALAS DNP 3000 graphite spark generator (Golden Aerosol Generator - GAG) and a TSI 3790 Condensation Particle Counter (Golden CPC – GCPC), were also circulated to compare {{the performance of the}} different aerosol generators (including CAST, sodium chloride and palladium) and CPCs employed at each laboratory. The study highlighted the importance of controlling and accounting for the pressures in the calibration setup. It also highlighted the difficulties associated with the measurement of the size distribution of the polydisperse aerosols produced by the generators that due to the high number concentrations are prone to significant coagulation. The study also provided evidence that the pre-treatment of sodium-chloride and CAST particles employed in most laboratories is not sufficient, and can lead to inaccuracies in the PCRF measurements at 30 nm if a CPC with a 50 % counting efficiency at 23 nm is employed. No significant linearity issues were identified in the 15 in total CPCs that were cross-checked against the GCPC. However, a change of the operating temperature of TSI 3790 CPCs to reduce the cut-off size can lead to significant linearity issues for some units, and therefore such modifications must be accompanied by <b>linearity</b> <b>checks.</b> JRC. F. 8 -Sustainable Transpor...|$|R
40|$|The modal {{identification}} {{is a common}} and necessary procedure performed on large aerospace structures. Aeronautical structures like prototype of new large aircraft have to be investigated within a ground vibration test (GVT) in order to validate the analytical Finite Element Model and to enable reliable flutter predictions for flight tests. Spacecraft structures like large satellites are investigated by so called modal survey tests. Here, the main purpose is also to validate the analytical model. With the validated model the severe dynamic loads which emerge during launch can be predicted. The dynamics of aeronautical structures and spacecraft are characterized by their modal parameters like natural frequencies, mode shapes, modal damping and generalized masses. These modal parameters can be obtained using two different philosophies resp. methods: Modes, which are important {{within the scope of}} the certification process can be identified using the phase resonance method. On the other hand, not all modes have to be known with the same precision and to the same extent, e. g. including <b>linearity</b> <b>checks.</b> Here, phase separation techniques, which are nowadays standard in the modal analysis of large aerospace structures, can save time compared with the phase resonance method. In this lecture the theory of modal identification methods is presented. After that we focuses on the performance of vibration tests. The test equipment is described in detail. In the last chapter of the lecture, examples from vibration tests which were recently performed on aerospace structures are shown...|$|R
40|$|We {{describe}} {{development and}} validation of a tangent linear {{model for the}} High-Order Method Modeling Environment, the default dynamical core in the Community Atmosphere Model and the Community Earth System Model that solves a primitive hydrostatic equation using a spectral element method. A tangent linear model is primarily intended to approximate the evolution of perturbations generated by a nonlinear model, provides a computationally efficient way to calculate a nonlinear model trajectory {{for a short time}} range, and serves as an intermediate step to write and test adjoint models, as the forward model in the incremental approach to four-dimensional variational data assimilation, and as a tool for stability analysis. Each module in the tangent linear model (version 1. 0) is linearized by hands-on derivations, and is validated by the Taylor–Lagrange formula. The <b>linearity</b> <b>checks</b> confirm all modules correctly developed, and the field results of the tangent linear modules converge to the difference field of two nonlinear modules as the magnitude of the initial perturbation is sequentially reduced. Also, experiments for stable integration of the tangent linear model (version 1. 0) show that the linear model is also suitable with an extended time step size compared to the time step of the nonlinear model without reducing spatial resolution, or increasing further computational cost. Although the scope of the current implementation leaves room for a set of natural extensions, the results and diagnostic tools presented here should provide guidance for further development of the next generation of the tangent linear model, the corresponding adjoint model, and four-dimensional variational data assimilation, with respect to resolution changes and improvements in linearized physics and dynamics...|$|R
40|$|A reversed-phase {{high-performance}} liquid chromatographic {{method was}} developed {{for the analysis of}} eight anticoagulant rodenticides in animal liver. Coumarinic anticoagulant rodenticides (brodifacoum, bromadiolone, coumachlor, coumatetralyl, difenacoum, and warfarin) were detected by using a gradient elution and a fluorimetric detection. Indanedione anticoagulant rodenticides (chlorophacinone and diphacinone) were detected by using an isocratic elution and an UV detection. Anticoagulants were extracted from liver with mixtures of acetone/diethylether and acetone/chloroform. Extracts were applied to solid-phase extraction cartridges. <b>Linearity</b> was <b>checked</b> over the concentration range 0. 1 - 0. 6 pg/g. Relative standard deviations of within-run and between-run variability were all between 5. 7 and 10. 3 %. Recoveries from spiked liver samples were between 51. 7 (difenacoum) and 78. 2 % (warfarin). Limits of detection were between 0. 01 (difenacoum and warfarin) and 0. 11 pg/g (chlorophacinone) ...|$|R
40|$|There is a {{constant}} urge from the European aircraft industry to reduce testing time of prototypes without loss of accuracy in the data. As a consequence, substantial changes concerning the testing strategy for modal survey tests, called ground vibration tests (GVTs) in connection to aircrafts, were proposed and applied during {{the last couple of}} years. Basic idea is to combine the benefits of the very reliable but time consuming phase resonance method (sine dwell) and the use of phase separation techniques on data stemming from swept-sine excitation. The idea of the current paper is to extend this concept introducing a combined approach also for <b>linearity</b> <b>checks.</b> Plotting modal parameters like the eigenfrequency {{as a function of the}} excitation level is the way to explore non-linear phenomena in GVTs for time being. Such plots are also called {em impedance plots}. Traditionally, these are produced using the phase resonance method by variation of the level of excitation energy, i. e. by harmonically exciting one single normal mode. The contemporary GVT strategy includes extensive broad-band excitation at various load introduction points. The authors recommend the use of swept-sine excitation which provide a high excitation level compared to random excitation and short testing time compared to stepped-sine excitation. The modes of the structure are identified from the resulting frequency response functions by means of phase separation techniques in parallel to the on-going measurement. The goal to extract as much information as possible from these data can be complemented by computing impedance plots for modes which have been identified at various excitation levels during the swept-sine testing depending on the excitation point and the exciter force. The paper explains in detail how the excitation level can be determined in case of swept-sine excitation. It gives examples of impedance plots from numerical simulations and complex large aerospace structures. The impedance plots are compared with curves stemming from phase resonance techniques. The achieved results are very encouraging...|$|R
40|$|Modal {{identification}} {{is a common}} and necessary procedure performed on large aerospace structures. Aeronautical structures like the prototype of new, large aircraft have to be investigated within a ground vibration test (GVT) in order to validate the analytical Finite Element Model and to enable reliable flutter predictions for flight tests. The dynamics of such aircraft are characterized by their modal parameters like natural frequencies, mode shapes, modal damping and generalized masses. These modal parameters can be obtained using two different philosophies. One {{of these is the}} phase resonance method in which modes, which are important within the scope of the certification process, can be identified. On the other hand, not all modes have to be known with the same precision and to the same extent, i. e., including <b>linearity</b> <b>checks.</b> This is where the second method, the phase separation technique, comes in. Nowadays, it is the standard method in modal analysis of large aerospace structures and can save time compared with the phase resonance method. As described above, modal identification within ground vibration testing on large aircraft is more or less standard in order to gather data (modal parameters) which are required within the aeroelastic certification process. This data is also used for the assistance of the flight tests or for the prediction issue with respect to structural modifications. Nevertheless, the scope of requirements of the aircraft manufacturers regarding the test program has increased in the last years in order to cover new issues like passenger comfort, fan-blade-off (wind-milling) or aero-servo-elasticity. Therefore, the test procedures, soft- and hardware and also the test strategy had to be enhanced. In this lecture the theory of modal identification methods is presented in brief. After that the lecture focuses on an advanced test strategy, which is used for the GVTs on large aircraft. Recent developments in terms of identification methods, soft- and hardware applications and a vision with respect to ground vibration testing in the future will complete the seminar. Examples from vibration tests that were recently performed on large aircraft (e. g. Airbus A 380) will be used to underline the applicability of the developed procedures...|$|R
40|$|The European {{aircraft}} industry is constantly {{calling for a}} reduction in the testing time of new aircraft without diminishing the accuracy of the data. As a consequence, substantial changes in the testing strategy of ground vibration tests (GVTs), were proposed and applied during {{the last couple of years}} (Fargette et al., Tasks for improvements in ground vibration testing of large aircraft, in: Proc. of IFASD, CASA/AIAE, 2001, pp. 121 - 133; G. Gloth, et al., Sound and Vibration 35 (11) (2001) 14 - 18). The basic idea is to combine the benefits of the very reliable but time-consuming phase resonance method (sine dwell) with the application of phase separation techniques to data stemming from broad-band excitation at various excitation points. The authors recommend the use of swept-sine excitation which provides a high excitation level compared to random excitation and a short testing time compared to stepped-sine excitation. The modes of the structure are identified from the resulting frequency response functions (FRFs) by means of phase separation techniques in parallel to the on-going measurement. In the practice of modal testing it is often observed that structures do not behave in a perfectly linear manner. This article investigates the influence of non-linear structural dynamics behaviour on the measurement results. An expansion of the test concept of swept-sine excitation is proposed which introduces a combined approach for <b>linearity</b> <b>checks</b> as well. The plotting of modal parameters like the eigenfrequency as a function of the excitation level is the manner in which non-linear phenomena in GVTs are presently explored. Traditionally these linearity functions are produced by harmonically exciting one single normal mode on different levels of excitation. The goal to extract as much information as possible from the FRF data can be complemented by computing plots of amplitude-dependent modal parameters for modes which have been identified at various excitation levels during the swept-sine testing, depending on the excitation point and the exciter force. The article expounds how the excitation level can be determined in the case of swept-sine excitation. It gives examples of linearity plots from numerical simulations and complex, large aerospace structures. The plots are compared with curves that stem from the phase resonance testing...|$|R
40|$|The {{tests were}} carried out on a carbon fibre {{reinforced}} structure with wires of different kind of installation and shielding. The peak value of the current impulses ranges from 5 kA up to 200 kA and the comparison shows, that a reasonable linearity exists for carbon fibre reinforced material and that the deviations are on the save side. The verification of the <b>linearity</b> has been <b>checked</b> by the low-level method. This method uses the transfer function of the whole test arrangement, with the current impulse as the excitation signal and the induced voltage on the wires as the response signal...|$|R
30|$|When {{measuring}} nuclear DNA content {{by means}} of flow cytometry, {{it is necessary to}} chop tissue from the plant of interest together with an internal standard. This standard must be {{as close as possible to}} the plants of interest and not overlap with the ploidy area of interest. If they are too close together the peak values interfere with each other. <b>Linearity</b> is <b>checked</b> by comparing the different ploidies as found within leaves and roots of many plants. In this way, variation in signal intensities due to staining kinetics, to light absorption and quenching by sample components, as well as to instrument and other variables, is reduced to a minimum. Agave americana was chosen as internal standard for Gagea. For Gagea minima and G. villosa, with 2 C-values that more or less coincided with Agave americana, Agave attenuata was used. Agave is available year-round, does not mind several weeks without water and, being a large plant, a single specimen can serve a lifetime, thereby further reducing variation in readings. It also has a low background in propidium iodide measurements, and show a single G 0 peak, almost lacking G 2 arrest.|$|R
40|$|AbstractArtificial {{neural network}} (ANN) model was {{developed}} and tested for estimating soil phosphorus (P) in Kouhin watershed area (1000 ha), Qazvin province, Iran using terrain analysis. Based on the soil distribution correlation, vegetation growth pattern across the topographically heterogeneous landscape, the topographic and vegetation attributes were used in addition to pedologic information {{for the development of}} ANN model in area for estimating of soil phosphorus. Totally, 85 samples were collected and tested for phosphorus contents and corresponding attributes were estimated by the digital elevation model (DEM). In order to develop the pedo-transfer functions, data <b>linearity</b> was <b>checked,</b> correlated and 80 % was used for modeling and ANN was tested using 20 % of collected data. Results indicate that 68 % of the variation in soil phosphorus could be explained by elevation and Band 1 data and significant correlation was observed between input variables and phosphorus contents. There was a significant correlation between soil P and terrain attributes which can be used to derive the pedo-transfer function for soil P estimation to manage nutrient deficiency. Results showed that P values can be calculated more accurately with the ANN-based pedo-transfer function with the input topographic variables along with the Band 1...|$|R
40|$|In {{the present}} work the author tries to analyse {{one of the}} {{fundamental}} concepts that underlie Kaplan's theory: his idea of “linearity”. Rather surprisingly, despite its importance, it is a construct that usually goes undefined in the literature. Different parameters of rhetorical organisation will be considered in this paper in order to clarify the essence of <b>linearity.</b> We shall <b>check</b> then Kaplan’s contention that English is a “linear” language whereas Spanish, a member of the Romance family, is characterised by a broken or non-linear structure. We shall also verify if there exist differences between English and Spanish in the discursive organisation of an expository text. Finally, we shall discuss which parameters appear to be more coincidental and more divergent within the rhetorical organisation of each language...|$|R
40|$|ABSTRACT Introduction: Autoverification is {{the release}} of {{laboratory}} test results from clinical instruments to hospital interface, or to patients' records, with no human intervention. Verification rules are inserted in the middleware and/or in the laboratory information system (LIS), based on criteria established by the laboratory. As a result, it ensures that every result is consistently reviewed in the same way, improving the entire verification process and patient safety. Objective: Describe the implementation of autoverification of clinical chemistry tests results at the core laboratory of Hospital das Clínicas da Universidade Federal de Minas Gerais (HC/UFMG), Brazil. Material and methods: Twenty-six automated chemistry assays were chosen. They were fully automated including internal quality control, interfaced with LIS, available 24 hours a day, seven days a week. Rules were {{set up in the}} middleware and in the LIS. Instrument flags, evaluation of sample integrity, test <b>linearity,</b> delta <b>check</b> and critical values were used to construct the verification algorithms. Results: An autoverification algorithm was constructed; delta check values were calculated and defined, as well as automatic verification ranges. The results retained for manual verification followed a flowchart prepared for this purpose. Conclusion: Autoverification implementation led to a more consistent reviewing process of test results, efficiency and improved patient safety...|$|R
40|$|Charles University Faculty of Pharmacy in Hradec Králové Department of Pharmacology and Toxicology Candidate: Daniel Herbolt, MSc. Supervisor: Assoc. Prof. Přemysl Mladěnka, Pharm. D., Ph. D. Title of {{rigorous}} thesis: Development {{of a novel}} {{spectrophotometric method}} for zinc ions chelatation screening Zinc {{is the only one}} trace metal, which is a cofactor for over 300 active enzymes. It is localized in all tissues and organs. Zinc deficiency can cause many diseases such as e. g. diabetes, disorders of hemostasis, and cancer. The toxicity of zinc is usually considered to be mild, but the imbalance of zinc, e. g. its presence in extracellular plaques by patients with Alzheimer's disease suggests a more complex relationship. The purpose of this work was to create a new spectrophotometric methodology for screening of chelation of zinc ions. The first step was to measure the spectra of the selected indicator dithizone and its complex with zinc and finding the ideal wavelengths for accurate determination of the linear concentration of zinc ions. Thereafter, the <b>linearity</b> was <b>checked</b> at two selected wavelengths. After that, the stability of dithizone and its complex with zinc was measured in relationship to time. The final phase was to validate the methodology on two known zinc chelators which were [...] ...|$|R
30|$|Because of {{the high}} {{sensitivity}} at specific low-energy radiation and radial isotropy (ability to measure backscatter), we used Automess SEQ- 6 R dosimeters (energy range from 18  keV to 3  MeV; dose range 0.01 – 2.00  mGy). All dosimeters were re-calibrated by separated calibration measurement for the radiation quality in use (30 kVp W-Rh target) to the air kerma with a RQM detector (IBA Dosimetry GmbH, Schwarzenbruck, Germany), with Dosimax plus unit, range 500 nGy – 9999  mGy). Energy dependence and <b>linearity</b> were <b>checked</b> by free air measurements with 20 kVp W-target. We compared the back scatter from muscle tissue according to ICRP 110 [15] and muscle tissue under a layer of 0.25  mm Pb (collar lead equivalent) and under a layer of 2.5  mm PMMA, irradiated with 28 and 35  keV W/Rh x-rays by MCS. Based on MCS, {{the effect of the}} thyroid collar to the backscatter was estimated to be <[*] 3 % and therefore, no separated backscatter correction for the collar was applied for patient measurements. It is assumed that the air kerma (in the investigated energy range, this corresponds to the absorbed dose in air and is in the following taken as measure for the entrance surface dose) at the surface in front of the thyroid was representative for the effect on the thyroid dose.|$|R
40|$|Carbamates are the {{pesticides}} {{most commonly}} found in dark colored foods and beverages in cases of accidental or intentional poisoning. In this work, the liquid-liquid extraction with low temperature partitioning (LLE-LTP) was optimized and validated for determination of the carbamates aldicarb, carbofuran and carbaryl in grape juice and chocolate milk beverages. This method involved extraction with acetonitrile, liquid-liquid partition at low temperature and the analysis by high performance liquid chromatography with ultraviolet detection (HPLC-UV). The method is rapid, efficient and of low-cost, employing small volumes of solvent per sample and requiring no cleanup of the extracts. The extraction methodology was selective and presented recovery percentages above 90 %. The premises related to the statistical <b>linearity</b> tests were <b>checked</b> and confirmed. The method of extraction and analysis was validated with satisfactory results, and may be applied in forensic and routine analysis...|$|R
40|$|An {{important}} {{problem for}} fitting local linear regression {{is the choice}} of the smoothing parameter. As the smoothing parameter becomes large, the estimator tends to a straight line, which is the least squares fit in the ordinary linear regression setting. This property may be used to assess the adequacy of a simple linear model. Motivated by Silverman's (1981) work for testing multimodality in kernel density estimation, a suitable test statistic for <b>checking</b> <b>linearity</b> is the critical smoothing parameter where the estimate changes from nonlinear to linear, determined by the approximate F-tests given in Hastie and Tibshirani (1990) for a prescribed type I error. To assess the significance, the "wild bootstrap" procedure is used to replicate the data and the proportion of bootstrap samples which give a nonlinear estimate when using the critical bandwidth is obtained as the p-value. Simulation results show that the critical smoothing test is useful in detecting a wide range of alternatives [...] . ...|$|R
