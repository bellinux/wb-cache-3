1230|5720|Public
5|$|By 2000, Juniper had {{developed}} five hardware systems and made seven new releases of its Junos operating system. That April, Juniper released {{the second generation}} of the internet processors embedded in its core routers. In April 2002, Juniper released {{the first of the}} T-series family (originally known under the code-name Gibson), which could perform four times as many route <b>lookups</b> per second as the M160. The first products of the TX Matrix family, which could be used to combine up to four T-series routers, was released in December 2004.|$|E
5|$|Along with {{quadratic}} probing {{and double}} hashing, linear probing {{is a form}} of open addressing. In these schemes, each cell of a hash table stores a single keyâ€“value pair. When the hash function causes a collision by mapping a new key to a cell of the hash table that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. <b>Lookups</b> are performed in the same way, by searching the table sequentially starting at the position given by the hash function, until finding a cell with a matching key or an empty cell.|$|E
5|$|At the time, {{the general}} method {{to compute the}} inverse square root was to {{calculate}} an approximation for , then revise that approximation via another method until it came within an acceptable error range of the actual result. Common software methods in the early 1990s drew approximations from a lookup table. The key of the fast inverse square root was to directly compute an approximation by utilizing the structure of floating-point numbers, proving faster than table <b>lookups.</b> The algorithm was approximately four times faster than computing the square root with another method and calculating the reciprocal via floating point division. The algorithm was designed with the IEEE 754-1985 32-bit floating point specification in mind, but investigation from Chris Lomont showed {{that it could be}} implemented in other floating point specifications.|$|E
40|$|<b>Lookup</b> of {{services}} {{is an important}} issues in many distributed systems. This paper deals with <b>lookup</b> in service-oriented architectures, such as Web services, P 2 P systems, GRIDs, or spontaneous networks. Service-oriented architectures impose specific requirements onto the <b>lookup</b> service, for instance regarding the runtime extensibility of <b>lookup</b> models, runtime extensibility of <b>lookup</b> queries, construction of complex <b>lookup</b> queries, scalability, and fault tolerance. These requirements are not well solved by existing <b>lookup</b> approaches. We propose a semantic <b>lookup</b> service using Semantic Web ontologies, expressed in RDF. Query scripts are sent from the client to the server and are interpreted at server side using the RDF repository. We also present a safe, scalable, and efficient architecture for defining and querying <b>lookup</b> information using this <b>lookup</b> service concept. ...|$|R
40|$|<b>Lookup</b> of {{services}} {{is an important}} issues in many distributed systems. This paper deals with <b>lookup</b> in service-oriented architectures, such as Web services, P 2 P systems, GRIDs, or spontaneous networks. Service-oriented architectures impose specific requirements onto the <b>lookup</b> service, for instance regarding the runtime extensibility of <b>lookup</b> models, runtime extensibility of <b>lookup</b> queries, construction of complex <b>lookup</b> queries, scalability, and fault tolerance...|$|R
50|$|Business {{service is}} located by <b>lookup</b> service {{which is used}} by the {{business}} delegate. the implementation details of business service <b>lookup</b> is encapsulated by <b>lookup</b> service.|$|R
5|$|Juniper Networks {{originally}} {{focused on}} core routers, {{which are used}} by internet service providers (ISPs) to perform IP address <b>lookups</b> and direct internet traffic. Through the acquisition of Unisphere in 2002, the company entered the market for edge routers, which are used by ISPs to route internet traffic to individual consumers. Juniper Networks entered the IT security market with its own JProtect security toolkit in 2003 before acquiring security company NetScreen Technologies the following year. It entered the enterprise segment in the early 2000s, which accounted for one-third of revenues by 2005. As of 2014, Juniper {{has been focused on}} developing new software-defined networking products. However, in 2016, the company encountered some controversy for allegedly putting backdoors into its ScreenOS products.|$|E
5|$|Integer sorting {{algorithms}} {{are usually}} {{designed to work}} in either the pointer machine or random access machine models of computing. The main {{difference between these two}} models is in how memory may be addressed. The random access machine allows any value that is stored in a register to be used as the address of memory read and write operations, with unit cost per operation. This ability allows certain complex operations on data to be implemented quickly using table <b>lookups.</b> In contrast, in the pointer machine model, read and write operations use addresses stored in pointers, and it is not allowed to perform arithmetic operations on these pointers. In both models, data values may be added, and bitwise Boolean operations and binary shift operations may typically also be performed on them, in unit time per operation. Different integer sorting algorithms make different assumptions, however, about whether integer multiplication is also allowed as a unit-time operation. Other more specialized models of computation such as the parallel random access machine have also been considered.|$|E
25|$|Only <b>lookups</b> are needed.|$|E
5000|$|Argument-dependent name <b>lookup</b> - another C++ name <b>lookup</b> rule ...|$|R
50|$|There {{are several}} {{commands}} involved with Nbtstat that allows several options such as: local cache <b>lookup,</b> WINS Server query, broadcast, LMHOSTS <b>lookup,</b> and Hosts <b>lookup.</b> It {{is not for}} DNS server query.|$|R
50|$|In the C++ {{programming}} language, argument-dependent <b>lookup</b> (ADL), or argument-dependent name <b>lookup,</b> {{applies to}} the <b>lookup</b> of an unqualified function name depending on the types of the arguments given to the function call. This behavior {{is also known as}} Koenig <b>lookup,</b> as it is often attributed to Andrew Koenig, though he is not its inventor.|$|R
25|$|This {{section is}} {{simplified}} {{to use a}} single bit; see the section accelerated <b>lookups</b> {{for more information on}} real routing tables.|$|E
25|$|Like {{most major}} web browsers, Chrome uses DNS {{prefetching}} {{to speed up}} website <b>lookups,</b> as do other browsers like Firefox, Safari, Internet Explorer (called DNS Pre-resolution), and in Opera as a UserScript (not built-in).|$|E
25|$|CoolWebSearch, a {{group of}} programs, takes {{advantage}} of Internet Explorer vulnerabilities. The package directs traffic to advertisements on Web sites including coolwebsearch.com. It displays pop-up ads, rewrites search engine results, and alters the infected computer's hosts file to direct DNS <b>lookups</b> to these sites.|$|E
40|$|The Forwarding Information Base (FIB) of {{backbone}} routers {{has been}} rapidly growing in size. An ideal IP <b>lookup</b> algo-rithm should achieve constant, yet small, IP <b>lookup</b> time and on-chip memory usage. However, no prior IP <b>lookup</b> algorithm achieves both requirements {{at the same}} time. In this paper, we first propose SAIL, a Splitting Approach to IP <b>Lookup.</b> One splitting is along the dimension of the <b>lookup</b> process, namely finding the prefix length and finding the next hop, and another splitting is along the dimension of prefix length, namely IP <b>lookup</b> on prefixes of length {{less than or equal}} to 24 and IP <b>lookup</b> on prefixes of length longer than 24. Second, we propose a suite of algorithms for IP <b>lookup</b> based on our SAIL framework. Third, we im-plemented our algorithms on four platforms: CPU, FPGA, GPU, and many-core. We conducted extensive experiments to evaluate our algorithms using real FIBs and real traffic from a major ISP in China. Experimental results show that our SAIL algorithms are several times or even two orders of magnitude faster than well known IP <b>lookup</b> algorithms...|$|R
40|$|Dictionary <b>lookup</b> is a {{computational}} {{activity that}} can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine Computer (CM). Several algorithms for parallel dictionary <b>lookup</b> are discussed, including one that allows the CM to <b>lookup</b> words at a rate 450 times that of <b>lookup</b> on a Symbolics 3600 Lisp Machine...|$|R
40|$|This paper {{describes}} an in-depth investigation {{and implementation of}} interleaved memory for pixel <b>lookup</b> operations in computer vision. Pixel <b>lookup,</b> mapping between coordinates and pixels, is a common operation in computer vision, but is also a potential bottleneck due to formidable bandwidth requirements for real-time operation. We focus on the acceleration of pixel <b>lookup</b> operations through parallelizing memory banks by interleaving. The key to applying interleaving for pixel <b>lookup</b> is 2 D block data partitioning and support for unaligned access. With this optimization of interleaving, pixel <b>lookup</b> operations can output a block of pixels at once without major overhead for unaligned access. An example implementation of our optimized interleaved memory for affine motion tracking shows that the pixel <b>lookup</b> operations can achieve 12. 8 Gbps for random <b>lookup</b> of a 4 x 4 size block of 8 -bit pixels under 100 MHz operation. Interleaving can be a cost-effective solution for fast pixel <b>lookup</b> in embedded computer vision. 1...|$|R
25|$|Some applications, such as web browsers, {{maintain}} an internal DNS cache to avoid repeated <b>lookups</b> via the network. This practice can add extra difficulty when debugging DNS issues, as it obscures {{the history of}} such data. These caches typically use very short caching times â€“ {{in the order of}} one minute.|$|E
25|$|Some {{implementations}} do {{support such}} data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually {{has a significant}} cost when compressed segments need to be split or merged. Some tradeoff {{has to be made}} between data compression and update speed. A typical strategy is to limit the range of global <b>lookups</b> for comparing the common branches in the sparse trie.|$|E
25|$|Node <b>lookups</b> {{can proceed}} asynchronously. The {{quantity}} of simultaneous <b>lookups</b> is denoted by Î± and is typically three. A node initiates a FIND_NODE request by querying to the Î± nodes {{in its own}} k-buckets that are the closest ones to the desired key. When these recipient nodes receive the request, they will look in their k-buckets and return the k closest nodes to the desired key that they know. The requester will update a results list with the results (node ID's) he receives, keeping the k best ones (the k nodes that are closer to the searched key) that respond to queries. Then the requester will select these k best results and issue the request to them, and iterate this process again and again. Because every node has a better knowledge of his own surroundings than any other node has, the received results will be other nodes that are every time {{closer and closer to}} the searched key. The iterations continue until no nodes are returned that are closer than the best previous results. When the iterations stop, the best k nodes in the results list are the ones in the whole network that are the closest to the desired key.|$|E
40|$|Abstractâ€”A {{major issue}} in router design for the next genera-tion Internet is the fast IP address <b>lookup</b> mechanism. The {{existing}} scheme by Huang et al. performs the IP address <b>lookup</b> in hard-ware in which the forwarding table can be compressed to fit into reasonable-size SRAM, and a <b>lookup</b> can be accomplished in three memory accesses. In this letter, we claim that with a little extra memory, {{it is able to}} further reduce the <b>lookup</b> time to two memory accesses. Index Termsâ€”Gigabit networking, Internet, IP address, <b>lookup.</b> I...|$|R
40|$|This paper {{treats the}} problem of {{designing}} an optimal size for a <b>lookup</b> table used for sensor linearization. In small embedded systems the <b>lookup</b> table must {{be reduced to a}} minimum {{in order to reduce the}} memory footprint and intermediate table values are estimated by linear interpolation. Since interpolation introduces an estimation uncertainty that increases with the sparseness of the <b>lookup</b> table there is a trade-off between <b>lookup</b> table size and estimation precision. This work will present a theory for finding the minimum allowed size of a <b>lookup</b> table that does not affect the overall precision, i. e. the overall precision is determined by the <b>lookup</b> table entriesâ€™ precision, not by the interpolation error...|$|R
40|$|Abstractâ€”In this paper, {{a weight}} method with using a reduced <b>lookup</b> table is {{developed}} to decode the three possible errors in (23, 12, 7) Golay code. The reduced <b>lookup</b> table consists of syndrome patterns and corresponding error patterns which {{only have one}} and two errors occurred in the message block of the received codeword. The useful proposed algorithm makes use of the properties of Cyclic codes, weight of syndrome, and reduced <b>lookup</b> table. It often results in {{a significant reduction in}} the memory requirements comparing to the traditional <b>lookup</b> table. This weight algorithm together with a reduced table <b>lookup</b> makes a fast and low complexity of the table <b>lookup</b> decoding algorithm. Moreover, a computer simulation shows that such a novel method is a much faster algorithm in software than the traditional full <b>lookup</b> table searching algorithm. I...|$|R
25|$|The XOR metric allows Kademlia {{to extend}} routing tables beyond single bits. Groups of bits {{can be placed}} in k-buckets. The group of bits are termed a prefix. For an m-bit prefix, there will be 2m-1 k-buckets. The missing k-bucket is a further {{extension}} of the routing tree that contains the node ID. An m-bit prefix reduces {{the maximum number of}} <b>lookups</b> from log2 n to log2m n. These are maximum values and the average value will be far less, increasing the chance of finding a node in a k-bucket that shares more bits than just the prefix with the target key.|$|E
25|$|The OpenType font format {{includes}} {{features for}} associating multiple glyphs {{to a single}} character, used for ligature substitution. Typesetting software {{may or may not}} implement this feature, even if it is explicitly present in the font's metadata. XeTeX is a TeX typesetting engine designed {{to make the most of}} such advanced features. This type of substitution used to be needed mainly for typesetting Arabic texts, but ligature <b>lookups</b> and substitutions are being put into all kinds of Western Latin OpenType fonts. In OpenType, there are standard liga, historical hlig, contextual clig, discretionary dlig and required rlig ligatures. These can be enabled or disabled in CSS3 using font-feature-settings.|$|E
25|$|Kademlia is a {{distributed}} {{hash table}} for decentralized peer-to-peer computer networks designed by Petar Maymounkov and David MaziÃ¨res in 2002. It specifies {{the structure of}} the network and the exchange of information through node <b>lookups.</b> Kademlia nodes communicate among themselves using UDP. A virtual or overlay network is formed by the participant nodes. Each node is identified by a number or node ID. The node ID serves not only as identification, but the Kademlia algorithm uses the node ID to locate values (usually file hashes or keywords). In fact, the node ID provides a direct map to file hashes and that node stores information on where to obtain the file or resource.|$|E
5000|$|When it {{receives}} a <b>Lookup</b> message {{for any other}} identifier, it forwards the <b>Lookup</b> message to E ...|$|R
5000|$|In the Asterisk CLI {{one can do}} a <b>lookup</b> by hand to test if a DUNDi {{configuration}} works. asterisk1*CLI> dundi <b>lookup</b> 301@priv bypass 1. 0 IAX2/priv:ByWFbOGKgGmZbM43BJHSZw@192.168.1.2/301 (EXISTS) from 00:0c:29:d2:d8:ec, {{expires in}} 3600 s DUNDi <b>lookup</b> completed in 113 msThe above DUNDi <b>lookup</b> tells the PBX to ask the known peers if {{they know how to}} reach extension 301 in the [...] "priv" [...] network.The answer consists of 6 parts: ...|$|R
40|$|Abstract â€” A Parallel {{algorithm}} and its hardware {{implementation of}} Inverse Halftone operation is proposed in this paper. The algorithm {{is based on}} <b>Lookup</b> Tables from which the inverse halftone value of a pixel is directly determined using a pattern of pixels. A method has been developed that allows accessing more than one value from the <b>lookup</b> table at any time. The <b>lookup</b> table is divided into smaller <b>lookup</b> tables, such that each pattern selected at any time goes to a separate smaller <b>lookup</b> table. The 15 -pixel parallel version of the algorithm was tested on sample images and a simple and effective method {{has been used to}} overcome quality degradation due to pixel loss in the proposed algorithm. It can provide at least 4 times decrease in <b>lookup</b> table size when compared with serial <b>lookup</b> table method implemented multiple times for same number of pixels. I...|$|R
25|$|The DNS {{resolver}} {{will almost}} invariably have a cache (see above) containing recent <b>lookups.</b> If the cache {{can provide the}} answer to the request, the resolver will return the value in the cache to the program that made the request. If the cache does not contain the answer, the resolver will send the request to one or more designated DNS servers. In the case of most home users, the Internet service provider to which the machine connects will usually supply this DNS server: such a user will either have configured that server's address manually or allowed DHCP to set it; however, where systems administrators have configured systems to use their own DNS servers, their DNS resolvers point to separately maintained name servers of the organization. In any event, the name server thus queried will follow the process outlined above, until it either successfully finds a result or does not. It then returns its results to the DNS resolver; assuming it has found a result, the resolver duly caches that result for future use, and hands the result back to the software which initiated the request.|$|E
500|$|For manual {{calculations}} {{that demand}} any appreciable precision, performing the <b>lookups</b> {{of the two}} logarithms, calculating their sum or difference, and looking up the antilogarithm is much faster than performing the multiplication by earlier methods such as prosthaphaeresis, which relies on trigonometric identities. Calculations of powers and roots are reduced to multiplications or divisions and look-ups by ...|$|E
500|$|... {{showed that}} in some cases the multiplications or table <b>lookups</b> {{required}} by some integer sorting algorithms could be replaced by customized operations that would be more easily implemented in hardware but that are not typically available on general-purpose computers. [...] improved on this by showing how to replace these special operations by the bit field manipulation instructions already available on Pentium processors.|$|E
40|$|Abstractâ€”Name-based route <b>lookup</b> {{is a key}} {{function}} for Named Data Networking (NDN). The NDN names are hierarchical and have variable and unbounded lengths, which are much longer than IPv 4 / 6 address, making fast name <b>lookup</b> a challenging issue. In this paper, we propose a parallel architecture for NDN name <b>lookup</b> called Parallel Name <b>Lookup</b> (PNL) which lever-ages hardware parallelism to achieve high <b>lookup</b> speedup while keeping a low and controllable memory redundancy. The core of PNL is an allocation algorithm that maps the logically tree-based structure to physically parallel modules, with low computational complexity. We evaluate the PNLâ€™s performance and show that PNL dramatically accelerates the name <b>lookup</b> process. Further-more, with certain knowledge of prior probability, the speedup can be significantly improved. I...|$|R
5000|$|... <b>lookup</b> - The <b>lookup</b> {{operation}} {{enables a}} requestor {{to obtain the}} XML that represents an object on a target.|$|R
40|$|VFS <b>lookup</b> code {{examines}} and translates path names {{one component}} at a time, checking for special {{cases such as}} mount points and symlinks. VFS calls the NFS <b>lookup</b> operation as necessary. NFS employs caching {{to reduce the number}} of <b>lookup</b> operations that go to the server. However, when part or all of a path is not cached, NFS <b>lookup</b> operations go back to the server. Although NFS's caching is effective, component-bycomponent translation of an uncached path is inefficient, enough so that <b>lookup</b> is typically the operation most commonly processed by servers. We study the effect of augmenting the VFS <b>lookup</b> algorithm and the NFS protocol so that a client can ask a server to translate an entire path in a single operation. The preconditions for a successful request are usually but not always satisfied, so the algorithm is optimistic. This small change can deliver substantial improvements in client latency and server load. 1 Introduction The NFS <b>lookup</b> operation frequently goes "over the wire [...] ...|$|R
