176|332|Public
50|$|Similar {{results were}} {{obtained}} in 1976 by Aaron D. Wyner and Jacob Ziv {{with regard to}} <b>lossy</b> <b>coding</b> of joint Gaussian sources.|$|E
50|$|This bound {{has been}} {{extended}} {{to the case of}} more than two correlated sources by Thomas M. Cover in 1975, and similar results were obtained in 1976 by Aaron D. Wyner and Jacob Ziv with regard to <b>lossy</b> <b>coding</b> of joint Gaussian sources.|$|E
5000|$|JPEG XR also {{supports}} lossless compression. The signal processing steps in JPEG XR {{are the same}} for both lossless and <b>lossy</b> <b>coding.</b> This makes the lossless mode simple to support and enables the [...] "trimming" [...] of some bits from a lossless compressed image to produce a lossy compressed image.|$|E
5000|$|... #Subtitle level 3: Wyner-Ziv <b>coding</b> - <b>lossy</b> {{distributed}} <b>coding</b> ...|$|R
40|$|This paper {{provides}} {{a necessary condition}} good rate-distortion codes must satisfy. Specifically, it is shown that as the blocklength increases, {{the distribution of the}} input given the output of a good <b>lossy</b> <b>code</b> converges to the distribution of the input given the output of the joint distribution achieving the rate-distortion function, in terms of the normalized conditional relative entropy. The result holds for stationary ergodic sources with subadditive distortion measures, both for fixed-length and variable-length compression. A similar necessary condition is given for <b>lossy</b> joint source-channel <b>coding...</b>|$|R
5000|$|Tarkin - an {{experimental}} <b>lossy</b> video <b>coding</b> format; no stable release (discontinued) ...|$|R
5000|$|For {{example the}} mapping [...] is not {{non-singular}} because both [...] "a" [...] and [...] "b" [...] {{map to the}} same bit string [...] "0" [...] any extension of this mapping will generate a lossy (non-lossless) coding. Such singular coding may still be useful when some loss of information is acceptable (for example when such code is used in audio or video compression, where a <b>lossy</b> <b>coding</b> becomes equivalent to source quantization).|$|E
40|$|In this {{correspondence}} {{we investigate}} {{the performance of}} the Lempel [...] Ziv incremental parsing scheme on nonstationary sources. We show that it achieves the best rate achievable by a finite-state block coder for the nonstationary source. We also show a similar result for a <b>lossy</b> <b>coding</b> scheme given by Yang and Kieffer which uses a Lempel [...] Ziv scheme to perform <b>lossy</b> <b>coding...</b>|$|E
3000|$|Excellent: {{the content}} in the video {{sequence}} may appear a bit blurred but no other artifacts are noticeable (i.e., only the <b>lossy</b> <b>coding</b> is present [...]).|$|E
40|$|Abstract—This paper shows new finite-blocklength {{converse}} bounds {{applicable to}} <b>lossy</b> source <b>coding</b> {{as well as}} joint sourcechannel coding, which are tight enough not only to prove the strong converse, but to find the rate-dispersion functions in both setups. In order to state the converses, we introduce the d-tilted information, a random variable whose expectation and variance (with respect to the source) are equal to the rate-distortion and rate-dispersion functions, respectively. Index Terms—Converse, finite blocklength regime, joint source-channel <b>coding,</b> <b>lossy</b> source <b>coding,</b> memoryless sources, rate-distortion theory, Shannon theory. I...|$|R
40|$|This article {{introduces}} {{a novel approach}} for scalable video coding based on an analysis-synthesis scheme. Active meshes are used to represent motion model, this permits to exploit temporal redundancy along motion trajectories in a video sequence using temporal wavelet transform. The use of 3 D wavelets in the coding strategy provides natural scalability functionalities to the video coder. Furthermore, the analysis-synthesis scheme allows to decouple motion and texture and to code these informations separately. Motion can then be <b>lossy</b> <b>coded,</b> bitrates gain {{can be reported to}} texture coding. Because motion is <b>lossy</b> <b>coded,</b> a new quality criterion measured in the texture domain is then proposed. Finally, the proposed analysis-synthesis video coding scheme overcomes some of the limitations of existing video coding schemes using 3 D wavelets, limitations due for the most part to the use of block-based motion model. Our video coding scheme performs as well as fully optimized H 26 Lv 8, while providing a scalable bitstream...|$|R
40|$|We {{study the}} moderate-deviations (MD) setting for <b>lossy</b> source <b>coding</b> of {{stationary}} memoryless sources. More specifically, we derive fundamental compression limits of source codes whose rates are R(D) ±ϵ_n, where R(D) is the rate-distortion function and ϵ_n is a sequence that dominates √(1 /n). This MD setting is complementary to the large-deviations and central limit settings and was studied by Altug and Wagner for the channel coding setting. We show, for finite alphabet and Gaussian sources, that {{as in the}} central limit-type results, the so-called dispersion for <b>lossy</b> source <b>coding</b> plays {{a fundamental role in}} the MD setting for the <b>lossy</b> source <b>coding</b> problem. Comment: To be presented at ISIT 2012 in Cambridge, M...|$|R
40|$|Abstract—For realtime {{teleoperation}} with haptic feedback, network-induced artifacts like delay, {{packet loss}} and <b>lossy</b> <b>coding</b> of haptic data deteriorate the operability of the system. We provide a systematic quantification {{of the limits}} within which, these network-induced degradations are tolerable for human task performance in executing a given haptic task. These limits are conservative {{in the sense that}} we do not account for any stabilizing approaches from control engineering to counter the impact of the network-induced degradations. Through experimental evaluation with a pursuit tracking task, we show that task performance is most affected by delay on the network and a strong <b>lossy</b> <b>coding</b> scheme. With statistical analysis we show that task performance is significantly decreased for one-way delay higher than 14 ms and strong <b>lossy</b> <b>coding</b> with a deadband parameter higher than 27 %. For the given task, packet loss is found not to affect task performance significantly. I...|$|E
30|$|For comparison, {{the coding}} gain of LiftLT [12] is 9.5378 (dB) which {{is higher than}} the {{proposed}} 8 [*]×[*] 16 IntFLTs because this is optimized for <b>lossy</b> <b>coding.</b>|$|E
30|$|The {{baseline}} scalable {{lossless coding}} method {{is simply an}} extension of the non-scalable <b>lossy</b> <b>coding</b> method. We now summarize this baseline method, as well as the problem considered in this paper.|$|E
40|$|Here {{we write}} in a unified fashion (using "R(P, Q, D) ") the random coding exponents in channel <b>coding</b> and <b>lossy</b> source <b>coding.</b> We derive their {{explicit}} forms and show, that, for a given random codebook distribution Q, the channel decoding error exponent {{can be viewed as}} an encoding success exponent in <b>lossy</b> source <b>coding,</b> and the channel correct-decoding exponent can be viewed as an encoding failure exponent in <b>lossy</b> source <b>coding.</b> We then extend the channel exponents to arbitrary D, which corresponds for D > 0 to erasure decoding and for D < 0 to list decoding. For comparison, we also derive the exact random coding exponent for Forney's optimum tradeoff decoder. Comment: This paper is self-contained, and serves also as an addendum to our paper "Exponential source/channel duality...|$|R
40|$|The {{audio quality}} in music {{has proved to}} be {{important}} by a broad audience. Studies show that 67, 1 % of young listeners (Olive, 2011) and 72 % of experienced listeners (Pras et al, 2009) can distinguish <b>lossy</b> <b>coded</b> audio from lossless coded audio. Ogg Vorbis and AAC are two common lossy codecs in music streaming services. Factors that impact the perceived audio quality are genre, bit rate, codec and instrument. An ABX test was performed with 20 trained listeners. The excerpts that were tested were three popular songs from 2015 and 2016 coded in AAC and Ogg Vorbis 96 kbit/s, 128 kbit/s and 192 kbit/s. The results showed that the subjects only could hear a difference between uncoded and <b>lossy</b> <b>coded</b> audio for one excerpt coded in AAC 96 kbit/s. This result does not exactly match those of earlier studies where subjects could hear the difference at higher bit rates, but there are still some similarities. This study is too small to come to any conclusions and more research has to be done on these codecs. The music streaming services that use these codecs already use higher bit rates than these that were tested so the quality is already good enough. Validerat; 20160531 (global_studentproject_submitter...|$|R
40|$|A subband {{decomposition}} based lossless {{image compression}} algorithm based on adaptive is described. The decomposition {{is achieved by}} a two-channel adaptive filter bank. The resulting coefficients are <b>lossy</b> <b>coded</b> first, and th en the residual error between the lossy and error free coefficients are compressed. The locations and the magnitudes of the nonzero coefficients are encoded separately by a hierarchical enumerative coding method. The locations of the nonzero coefficients in children bands are predicted {{from those in the}} parent band. The proposed compression algorithm, on the average, provides higher compression ratios than the state-of-the-art methods...|$|R
30|$|We first {{describe}} two floating-point data formats for HDR images. A non-scalable <b>lossy</b> <b>coding</b> method, {{which is}} extended to scalable lossless coding of HDR {{images in the}} next section, is also summarized.|$|E
40|$|The use of subband coding on {{image data}} is discussed. An {{overview}} of subband coding is given. Advantages of subbanding for browsing and progressive resolution are presented. Implementations for lossless and <b>lossy</b> <b>coding</b> are discussed. Algorithm considerations and simple implementations of subband are given...|$|E
40|$|Many {{approaches}} {{have been proposed}} to support lossless coding within video coding standards that are primarily designed for <b>lossy</b> <b>coding.</b> The simplest approach is to just skip transform and quantization and directly entropy code the prediction residual, which is used in HEVC version 1. However, this simple approach is inefficient for compression. More efficient approaches include processing the residual with DPCM prior to entropy coding. This paper explores an alternative approach based on processing the residual with integer-to-integer (i 2 i) transforms. I 2 i transforms map integers to integers, however, unlike the integer transforms used in HEVC for <b>lossy</b> <b>coding,</b> they do not increase the dynamic range at the output {{and can be used}} in lossless coding. Experiments with the HEVC reference software show competitive results...|$|E
40|$|The three-node multiterminal <b>lossy</b> source <b>coding</b> {{problem is}} investigated. We derive an inner {{bound to the}} general {{rate-distortion}} region of this problem which is {{a natural extension of}} the seminal work by Kaspi' 85 on the interactive two-terminal source coding problem. It is shown that this (rather involved) inner bound contains several rate-distortion regions of some relevant source coding settings. In this way, besides the non-trivial extension of the interactive two terminal problem, our results {{can be seen as a}} generalization and hence unification of several previous works in the field. Specializing to particular cases we obtain novel rate-distortion regions for several <b>lossy</b> source <b>coding</b> problems. We finish by describing some of the open problems and challenges. However, the general three-node multiterminal <b>lossy</b> source <b>coding</b> problem seems to offer a formidable mathematical complexity. Comment: New version with changes suggested by reviewers. Revised and resubmitted to IEEE Transactions on Information Theory. 92 pages, 11 figures, 1 tabl...|$|R
40|$|A {{lossless}} {{image compression}} algorithm based on adaptive subband decomposition is proposed. The subband decomposition {{is achieved by}} a two-channel LMS adaptive "lter bank. The resulting coe$cients are <b>lossy</b> <b>coded</b> "rst, and then the residual error between the lossy and error-free coe$cients is compressed. The locations and the magnitudes of the nonzero coe$cients are encoded separately by an hierarchical enumerative coding method. The locations of the nonzero coe$cients in children bands are predicted {{from those in the}} parent band. The proposed compression algorithm, on the average, provides higher compression ratios than the state-of-the-art methods. � 2001 Elsevier Science B. V. All rights reserved...|$|R
40|$|Abstract:- Vector {{quantization}} {{is known}} as the best <b>lossy</b> source <b>coding</b> among the fixed-to-fixed coding methods because of its satisfactory ability of expression. Although it can represent any fixed-to-fixed code and has optimization methods that guarantee local optimality, its encoding and optimization require the computation that grows exponential to the data length. We propose an optimization method for a product coding, which avoids this computational explosion problem by applying a reasonable restriction to the model architecture. The performance of the product coding is evaluated by a simple problem. Key-Words:- <b>lossy</b> source <b>code,</b> product code, companding function, optimization...|$|R
40|$|Existing {{distributed}} image coder Interpolation of disparity {{field to}} pixel resolution Edge-based noise estimation Shape-based disparity estimation Experimental results – Bit savings for lossless l coding di – PSNR improvements for <b>lossy</b> <b>coding</b> – Faster convergence of decoding algorithm – Removal of blocking artifacts Suggested future wor...|$|E
40|$|A multiterminal <b>lossy</b> <b>coding</b> problem, which {{includes}} various {{problems such as}} the Wyner-Ziv problem and the complementary delivery problem as special cases, is considered. It is shown that any point in the achievable rate-distortion region can be attained even if the source statistics are not known. Comment: 9 pages, 4 figure...|$|E
30|$|We now {{describe}} {{a series of}} experiments that tested nine HDR images, including five type A images and four type B images. For the <b>lossy</b> <b>coding</b> in the base layer and the lossless coding in the enhancement layer, we used the JPEG 2000 international standard [1] in lossy mode and lossless mode, respectively.|$|E
40|$|Cataloged from PDF {{version of}} article. A {{lossless}} image compression algorithm based on adaptive subband decomposition is proposed. The subband decomposition {{is achieved by}} a two-channel LMS adaptive filter bank. The resulting coefficients are <b>lossy</b> <b>coded</b> first, and then the residual error between the lossy and error-free coefficients is compressed. The locations and the magnitudes of the nonzero coefficients are encoded separately by an hierarchical enumerative coding method. The locations of the nonzero coefficients in children bands are predicted {{from those in the}} parent band. The proposed compression algorithm, on the average, provides higher compression ratios than the state-of-the-art methods. (C) 2001 Elsevier Science B. V. All rights reserved...|$|R
40|$|This paper {{shows the}} strong {{converse}} and the dispersion of memoryless channels with cost constraints and performs a refined {{analysis of the}} third-order term in the asymptotic expansion of the maximum achievable channel coding rate, showing that it is equal to (1 / 2) ((log n) /n) in most cases of interest. The analysis {{is based on a}} nonasymptotic converse bound expressed in terms of the distribution of a random variable termed the mathsf b -tilted information density, which plays a role {{similar to that of the}} mathsf d -tilted information in <b>lossy</b> source <b>coding.</b> We also analyze the fundamental limits of <b>lossy</b> joint-source-channel <b>coding</b> over channels with cost constraints...|$|R
40|$|We {{consider}} {{the problem of}} <b>lossy</b> source <b>coding</b> with a mismatched distortion measure. That is, we investigate what distortion guarantees {{can be made with}} respect to distortion measure ρ̃, for a source code designed such that it achieves distortion less than D with respect to distortion measure ρ. We find a single-letter characterization of this mismatch distortion and study properties of this quantity. These results give insight into the robustness of <b>lossy</b> source <b>coding</b> with respect to modeling errors in the distortion measure. They also provide guidelines on how to choose a good tractable approximation of an intractable distortion measure. Comment: 28 pages, 4 figure...|$|R
30|$|DVC is {{the result}} of the information-theoretic bounds {{established}} for distributed source coding (DSC) by Slepian and Wolf [4] for lossless coding, and by Wyner and Ziv [5] for <b>lossy</b> <b>coding</b> with SI at the decoder. Lossless DSC refers to two correlated random sources separately encoded and jointly decoded by exploiting the statistical dependencies.|$|E
3000|$|... {{exploiting}} {{their mutual}} knowledge; this coding paradigm {{is known as}} distributed source coding (DSC). While the Slepian-Wolf theorem deals with lossless coding (with a vanishing error probability), Wyner and Ziv studied the case of <b>lossy</b> <b>coding</b> with side information (SI) at the decoder. The Wyner-Ziv (WZ) theorem [2] states that when the SI (i.e., the correlated source [...]...|$|E
40|$|The {{order of}} letters {{is not always}} {{relevant}} in a communication task. This paper discusses the implications of order irrelevance on source coding, presenting results in several major branches of source coding theory: lossless coding, universal lossless coding, rate-distortion, high-rate quantization, and universal <b>lossy</b> <b>coding.</b> The main conclusions demonstrate {{that there is a}} significant rate savings when order is irrelevant. In particular, lossless coding of n letters from a finite alphabet requires Theta(log n) bits and universal lossless coding requires n + o(n) bits for many countable alphabet sources. However, there are no universal schemes that can drive a strong redundancy measure to zero. Results for <b>lossy</b> <b>coding</b> include distribution-free expressions for the rate savings from order irrelevance in various high-rate quantization schemes. Rate-distortion bounds are given, and it is shown that the analogue of the Shannon lower bound is loose at all finite rates. Comment: 35 page...|$|E
40|$|Abstract — We {{consider}} {{the problem of}} <b>lossy</b> source <b>coding</b> with a mismatched distortion measure. That is, we investigate what distortion guarantees {{can be made with}} respect to distortion measure ˜ρ, for a source code designed such that it achieves distortion less than D with respect to distortion measure ρ. We find a singleletter characterization of this mismatch distortion. We then study properties of this quantity and derive asymptotically tight bounds on it. These results give insight into the robustness of <b>lossy</b> source <b>coding</b> with respect to modelling errors in the distortion measure. They also provide guidelines on how to choose a good tractable approximation of an intractable distortion measure. A. Problem Formulation I...|$|R
40|$|Let X =(X 1, [...] .) be a {{stationary}} ergodic finite-alphabet source, X n denote its first n symbols, and Y n be the codeword assigned to X n by a <b>lossy</b> source <b>code.</b> The empirical kth-order joint distribution ˆ Q k [X n,Y n](x k,y k) {{is defined as}} the frequency of appearances of pairs of k-strings (x k,y k) alongthepair(X n,Y n). Our main interest is in the sample behavior of this (random) distribution. Letting I(Q k) denote the mutual information I(X k; Y k) when (X k,Y k) ∼ Q k we show that for any (sequence of) <b>lossy</b> source <b>code(s)</b> of rate ≤ R lim sup n→∞ 1 k I ˆQ k n...|$|R
40|$|Abstract—This paper shows new tight finite-blocklength bounds for {{the best}} {{achievable}} <b>lossy</b> joint source-channel <b>code</b> rate, and demonstrates that joint source-channel code design brings considerable performance advantage over a separate one in the non-asymptotic regime. A joint source-channel code maps ablockofk source symbols onto a length−n channel codeword, and the fidelity of reproduction at the receiver end {{is measured by the}} probability ɛ that the distortion exceeds a given threshold d. Formemorylesssourcesandchannels,itisdemonstratedthat the parameters of the best joint source-channel code must satisfy nC − kR(d) ≈ p nV + kV(d) Q − 1 (ɛ), whereC and V are the channel capacity and dispersion, respectively; R(d) and V(d) are the source rate-distortion and rate-dispersion functions; andQ is the standard Gaussian complementary cdf. Index Terms—Achievability, converse, finite blocklength regime, joint source-channel <b>coding,</b> <b>lossy</b> source <b>coding,</b> memoryless sources, rate-distortion theory, Shannon theory. I...|$|R
