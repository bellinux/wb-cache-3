31|249|Public
5000|$|The {{associated}} cone-adapted discrete shearlet system {{consists of}} three parts, each one corresponding {{to one of these}} frequency domains.It is generated by three functions [...] and a <b>lattice</b> <b>sampling</b> factor ...|$|E
40|$|Discrepancy is an {{important}} criterion for uniformity {{in the design of}} experiments. Basically, it measures the distance between two functions: the empirical distribution of the design points and the theoretical uniform distribution function. This paper studies the properties of uniformity when the factor level values are contaminated with errors. Specifically, our study focuses on the wrap-around L 2 -discrepancy. It is shown that uniform designs with errors are less uniform (in the average sense) than the original ones without errors. Related theorems are obtained. Furthermore, it can be shown that the <b>lattice</b> <b>sampling</b> outperforms Latin hypercube sampling. The latter {{can be viewed as a}} <b>lattice</b> <b>sampling</b> with error-in-level values. Discrepancy Robust Uniform design...|$|E
40|$|This paper {{presents}} a correction {{to the simple}} chi-squared test for spatial data. The paper commences with an outline of the temporal analogue before giving a proof for a deflating factor appropriate to a square <b>lattice</b> <b>sampling</b> scheme. The effects of varying degrees of spatial autocorrelation are illustrated, though the correlation is designed to require only minimum assumptions {{about the nature of}} the autocorrelation mechanism. ...|$|E
40|$|We {{report the}} {{observation}} of calorons with nontrivial holonomy and fractional topological charge objects in cooled <b>lattice</b> <b>samples</b> derived from SU(2) equilibrium ensembles at $T<T_c$. Comment: 3 pages, 3 figures, talk given at Confinement V, Gargnano, Italy, 10 - 14 Sep 2002. References and acknowlegements adde...|$|R
40|$|Interval-Lipschitz {{mappings}} between topological vector {{spaces are}} defined and {{compared with other}} Lipschitz-type operators. A theory of generalized gradients is presented when both spaces are locally convex and the range space is an order complete vector <b>lattice.</b> <b>Sample</b> applications {{to the theory of}} nonsmooth optimization are given...|$|R
40|$|We {{introduce}} {{an exact}} algorithm for the computation of spin correlation functions {{for the two}} dimensional +/-J Ising spin glass in the ground state. Unlike with the transfer matrix method, there is no particular restriction on {{the shape of the}} <b>lattice</b> <b>sample,</b> and unlike Monte Carlo based methods it avoids extrapolation from finite temperatures. The computational requirements depend only on the number and distribution of frustrated plaquettes. Comment: Latex file with 5 figures (postscipt). Submitted to PR...|$|R
40|$|Abstract: The Fourier {{transforms}} of Laguerre functions {{play the}} same canonical role in Wavelet analysis as do the Hermite functions in Gabor analysis. We will use them as mother wavelets {{in a similar way}} as the Hermite functions were recently used as windows in Gabor frames by Gröchenig and Lyubarskii. Using results due to K. Seip concerning <b>lattice</b> <b>sampling</b> sequences on weighted Bergman spaces, we find a sufficient condition for the discretization of the resulting wavelet transform to be a frame. As in Gröchenig-Lyubarskii theorem, the density increases with n, when considering frames generated by translations and dilations of the Fourier transform of the nth Laguerre function...|$|E
40|$|Gabor frames with Hermite {{functions}} are equivalent to sampling sequences in true Fock spaces of polyanalytic functions. In the L^ 2 -case, such an equivalence {{follows from the}} unitarity of the polyanalytic Bargmann transform. We will introduce Banach spaces of polyanalytic functions and investigate the mapping properties of the polyanalytic Bargmann transform on modulation spaces. By applying the theory of coorbit spaces and localized frames to the Fock representation of the Heisenberg group, we derive explicit polyanalytic sampling theorems which {{can be seen as}} a polyanalytic version of the <b>lattice</b> <b>sampling</b> theorem discussed by J. M. Whittaker in Chapter 5 of his book "Interpolatory Function Theory". Comment: 17 page...|$|E
40|$|The {{purpose of}} model-based {{experimental}} design is to maximise {{the information gathered}} for quantitative model identification. Instead of the commonly used optimal experimental design, robust experimental design aims to address parametric uncertainties in the design process. In this paper, the Bayesian robust experimental design is investigated, where both a Monte Carlo sampling strategy and local sensitivity evaluation at each sampling point are employed to achieve the robust solution. The link between global sensitivity analysis (GSA) and the Bayesian robust experimental design is established. It is revealed that a <b>lattice</b> <b>sampling</b> based GSA strategy, the Morris method, can be explicitly interpreted as the Bayesian A-optimal design for the uniform hypercube type uncertainties...|$|E
40|$|We {{show how}} to {{construct}} a variety of “trapdoor ” cryptographic tools assuming the worst-case hardness of standard lattice problems (such as approximating {{the length of the}} shortest nonzero vector to within certain polynomial factors). Our contributions include a new notion of preimage sampleable functions, simple and efficient “hash-and-sign ” digital signature schemes, and identity-based encryption. A core technical component of our constructions is an efficient algorithm that, given a basis of an arbitrary <b>lattice,</b> <b>samples</b> <b>lattice</b> points from a discrete Gaussian probability distribution whose standard deviation is essentially the length of the longest Gram-Schmidt vector of the basis. A crucial security property is that the output distribution of the algorithm is oblivious to the particular geometry of the given basis...|$|R
40|$|This article {{examines}} the design of Quadratic Fisher Discriminants (QFDs) that operate directly on image pixels, when image ensembles are taken to comprise all rotated and reflected versions of distinct sample images. A procedure based on group theory is devised to identify and discard QFD coefficients made redundant by symmetry, for arbitrary <b>sampling</b> <b>lattices.</b> This procedure introduces {{the concept of a}} degeneracy matrix. Tensor representations are established for the square lattice point group (8 -fold symmetry) and hexagonal lattice point group (12 -fold symmetry). The analysis is largely applicable to the symmetrisation of any quadratic filter, and generalises to higher order polynomial (Volterra) filters. Experiments on square <b>lattice</b> <b>sampled</b> synthetic aperture radar (SAR) imagery verify that symmetrisation of QFDs can improve their generalisation and discrimination ability. Comment: Accepted for publication in IEEE Transactions on Information Theor...|$|R
40|$|S. Caprari Abstract — This article {{examines}} the design of Quadratic Fisher Discriminants (QFDs) that operate directly on image pixels, when image ensembles are taken to comprise all rotated and reflected versions of distinct sample images. A procedure based on group theory is devised to identify and discard QFD coefficients made redundant by symmetry, for arbitrary <b>sampling</b> <b>lattices.</b> This procedure introduces {{the concept of a}} degeneracy matrix. Tensor representations are established for the square lattice point group (8 -fold symmetry) and hexagonal lattice point group (12 -fold symmetry). The analysis is largely applicable to the symmetrisation of any quadratic filter, and generalises to higher order polynomial (Volterra) filters. Experiments on square <b>lattice</b> <b>sampled</b> synthetic aperture radar (SAR) imagery verify that symmetrisation of QFDs can improve their generalisation and discrimination ability. Index Terms — pattern recognition; statistical target detection; image processing; lattice symmetry; group theory; dihedral groups. I...|$|R
40|$|In this thesis, we {{investigate}} {{the problem of}} decoding for wireless communications {{from the perspective of}} <b>lattice</b> <b>sampling.</b> In particular, computationally efficient <b>lattice</b> <b>sampling</b> algorithms are exploited to enhance the system performance, which enjoys the system tradeoff between performance and complexity through the sample size. Based on this idea, several novel <b>lattice</b> <b>sampling</b> algorithms are presented in this thesis. First of all, in order to address the inherent issues in the random sampling, derandomized sampling algorithm is proposed. Specifically, by setting a probability threshold to sample candidates, the whole sampling procedure becomes deterministic, leading to considerable performance improvement and complexity reduction over to the randomized sampling. According to the analysis and optimization, the correct decoding radius is given with the optimized parameter setting. Moreover, the upper bound on the sample size, which corresponds to near-maximum likelihood (ML) performance, is also derived. After that, the proposed derandomized sampling algorithm is introduced into the soft-output decoding of MIMO bit-interleaved coded modulation (BICM) systems to further improve the decoding performance. According to the demonstration, we show that the derandomized sampling algorithm is able to achieve the near-maximum a posteriori (MAP) performance in the soft-output decoding. We then extend the well-known Markov Chain Monte Carlo methods into the samplings from lattice Gaussian distribution, which has emerged as a common theme in lattice coding and decoding, cryptography, mathematics. We firstly show that the statistical Gibbs sampling is capable to perform the lattice Gaussian sampling. Then, a more efficient algorithm referred to as Gibbs-Klein sampling is proposed, which samples multiple variables block by block using Klein’s algorithm. After that, for the sake of convergence rate, we introduce the conventional statistical Metropolis-Hastings (MH) sampling into lattice Gaussian distributions and three MH-based sampling algorithms are then proposed. The first one, named as MH multivariate sampling algorithm, is demonstrated to have a faster convergence rate than Gibbs-Klein sampling. Next, the symmetrical distribution generated by Klein’s algorithm is taken as the proposal distribution, which offers an efficient way to perform the Metropolis sampling over high-dimensional models. Finally, the independent Metropolis-Hastings-Klein (MHK) algorithm is proposed, where the Markov chain arising from it is proved to converge to the stationary distribution exponentially fast. Furthermore, its convergence rate can be explicitly calculated in terms of the theta series, making it possible to predict the exact mixing time of the underlying Markov chain. Open Acces...|$|E
40|$|The Fourier {{transforms}} of Laguerre functions {{play the}} same canonical role in Wavelet analysis as do the Hermite functions in Gabor analysis. We will use them as mother wavelets {{in a similar way}} as the Hermite functions were recently used as windows in Gabor frames by Gr¨ochenig and Lyubarskii. Using results due to K. Seip concerning <b>lattice</b> <b>sampling</b> sequences on weighted Bergman spaces, we find a sufficient condition for the discretization of the resulting wavelet transform to be a frame. As in Gr¨ochenig-Lyubarskii theorem, the density increases with n, when considering frames generated by translations and dilations of the Fourier transform of the nth Laguerre function. Fundação Ciência e Tecnologia; Centro de Matemática da Universidade de Coimbr...|$|E
40|$|This paper {{presents}} a novel metamodel formulation for efficient function emulation based on Radial Basis Function (RBF) interpolation. <b>Lattice</b> <b>sampling</b> plans and {{the geometry of}} the Voronoi tessellation are exploited to localise the RBF interpolation {{with the aim of}} increasing efficiency for large scale modelling with minimum loss of global accuracy. The performance of the proposed technique is examined using standard test problems and compared to established global and compact RBF modelling techniques. The fitting and prediction times for the new method are shown to be linearly proportional to the number of sample points allowing large data sets to be handled without prohibitive growth in computational cost. Accuracy is shown to be comparable to that of the global RBF for problems of medium to low dimensionality. I...|$|E
40|$|We {{provide a}} method for {{constructing}} regular <b>sampling</b> <b>lattices</b> in arbitrary dimensions together with an integer dilation matrix. Subsampling using this dilation matrix leads to a similarity-transformed version of the lattice with a chosen density reduction. These lattices are interesting candidates for multidimensional wavelet constructions with {{a limited number of}} subbands. 1. Primer on <b>sampling</b> <b>lattices</b> and related work A <b>sampling</b> <b>lattice</b> is a set of points {Rk: k ∈ Z n}⊂R n that is closed under addition and inversion. The nonsingular generating matrix R ∈ R n×n contains basis vectors in its columns. Lattice points are uniquely indexed by k ∈ Z n and the neighbourhood around each samplin...|$|R
40|$|The {{well-known}} Whittaker-Kotel'nikov-Shannon sampling theorem for frequency-bandlimited {{functions of}} time is extended to functions of multidimensional arguments. It is shown that a function whose spectrum is restricted to a finite region of wave-number space may be reconstructed from its samples taken over a periodic lattice having suitably small repetition vectors. The most efficient lattice (i. e., requiring minimum sampling points per unit hypervolume) is not in general rectangular, nor is a unique reconstruction function associated with a given <b>sampling</b> <b>lattice.</b> The above results also apply to homogeneous wave-number-limited stochastic processes {{in the sense of}} a vanishing mean-square error. It is also found that, given a particular <b>sampling</b> <b>lattice,</b> the optimum (mean-square) presampling filter for nonwave-number-limited processes effects an ideal wave-number cutoff appropriate to the specified <b>sampling</b> <b>lattice.</b> Particular attention is paid to isotropic processes: minimum <b>sampling</b> <b>lattices</b> are specified up to eight-dimensional spaces, and a number of typical reconstruction functions are calculated...|$|R
40|$|Summary. In {{this paper}} we {{introduce}} reconstruction kernels for the 3 D optimal <b>sampling</b> <b>lattice</b> and demonstrate a practical realisation of a few. First, we review fundamentals of multidimensional sampling theory. We derive the optimal regular <b>sampling</b> <b>lattice</b> in 3 D, namely the Body Centered Cubic (BCC) lattice, {{based on a}} spectral sphere packing argument. With the introduction of this <b>sampling</b> <b>lattice,</b> we review some of its geometric properties and its dual lattice. We introduce the ideal reconstruction kernel {{in the space of}} bandlimited functions on this lattice. Furthermore, we introduce a family of box splines for reconstruction on this <b>sampling</b> <b>lattice.</b> We conclude the paper with some images and results of sampling on the BCC lattice and contrast it with equivalent samplings on the traditionally used Cartesian lattice. Our experimental results confirm the theory that BCC sampling yields a more accurate discrete representation of a signal comparing to the commonly used Cartesian sampling. ...|$|R
40|$|We give a {{complete}} characterization of all <b>lattice</b> <b>sampling</b> and inter-polating sequences in the Fock space of polyanalytic functions (poly-Fock spaces), displaying a ”Nyquist rate ” which increases with n, {{the degree of}} polyanaliticity of the space: A sequence of lattice points is sampling {{if and only if}} its density is strictly larger than n, and it is interpolating if and only if its density is strictly smaller than n. In our method of proof we introduce a unitary mapping between vector valued Hilbert spaces and poly-Fock spaces which allows the extension of Bargmann´s the-ory to polyanalytic spaces. Then we connect this mapping to Gabor transforms with Hermite windows and apply duality principles from time-frequency analysis {{in order to reduce the}} problem to a ”purely holomorphic” situation...|$|E
40|$|We give a {{complete}} characterization of all <b>lattice</b> <b>sampling</b> and interpolation sequences in the Fock space of polyanalytic functions (poly-Fock spaces), displaying a ”Nyquist rate” which increases with {{the degree of}} polyanaliticity. This is done introducing a unitary mapping between vector valued spaces and poly-Fock spaces, which generalizes the Bargmann transform and is connected to Gabor transforms with Hermite windows. Then we apply duality principles from time-frequency analysis {{in order to reduce}} the problems to ”purely holomorphic ” situations. Using our characterization we show that D(Λ) > n is a sharp density condition on the lattices generating Gabor Frames with the nth Hermite window, proving thus a conjecture made by Gröchenig and Lyubarskii. This shows, in a constructive way, that the critical density of Gabor frames can be arbitrarily large...|$|E
40|$|There {{are many}} good methods for {{sampling}} Markov chains via streams of independent U[0, 1] random variables. Recently some non-random and some random but dependent driving sequences {{have been shown}} to result in consistent Markov chain sampling, sometimes with considerably improved accuracy. The key to consistent sampling is for the driving sequence to be completely uniformly distributed (CUD) or weakly CUD. This paper gives some sufficient conditions for an infinite sequence to be (W) CUD. The earlier theory did not incorporate acceptance-rejection sampling. We show by a coupling argument that a strategy due to Liao (1998) for inserting IID points into a WCUD sequence leads to consistent sampling. We also introduce a notion of (W) CUD triangular arrays for finite samples, and show that a <b>lattice</b> <b>sampling</b> construction of Niederreiter (1977) produces CUD triangular arrays. ...|$|E
40|$|Rectangular <b>sampling</b> <b>lattices</b> bave been {{extensively}} applied to image coding. Sampling {{is a fundamental}} orera. tion m image coding systems as a proper selection of the <b>sampling</b> <b>lattice</b> minimizes the physica support of the informalion affecting the overall performance {{of the system and}} increasing the compression ratio. Non-rectangular <b>sampling</b> <b>lattices</b> have been occasionaly used in specific image systems but their application to coding schemes has not been reported in the literature. In this paper we present a characterization of multidimensional systems whose input and output are defined on arbitrary sampling structures. We also introduce the system response that eases the analysis of the sampling structure conversions. In {{the second part of the}} paper this theory is aplied to Laplacian Pyramid Coding. The result is the modelization of the algoríthms used in Pyramid Coding. In the third part of the paper this model is used to describe an efficient adaptive Pyramid image structure defined on arbitrary non-rectangular <b>sampling</b> <b>lattices.</b> Peer ReviewedPostprint (published version...|$|R
5000|$|Explicit {{construction}} of ideal low-pass filters (i.e., sinc functions) generalized to optimal lattices is possible {{by studying the}} geometric properties of Brillouin zones (i.e., [...] in above) of these lattices (which are zonotopes). This approach provides a closed-form explicit representation of [...] for general <b>lattices,</b> including optimal <b>sampling</b> <b>lattices.</b> This construction provides a generalization of the Lanczos filter in 1-D to the multidimensional setting for optimal lattices.|$|R
40|$|We {{describe}} a neural attention {{model with a}} learnable retinal <b>sampling</b> <b>lattice.</b> The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal <b>sampling</b> <b>lattice</b> after training. Specifically, we show that this lattice resembles the eccentricity dependent <b>sampling</b> <b>lattice</b> of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function. Comment: Published as a conference paper at ICLR 201...|$|R
40|$|International audienceMany lattice {{cryptographic}} primitives {{require an}} efficient algorithm to sample lattice points {{according to some}} Gaussian distribution. All algorithms known for this task require long-integer arithmetic at some point, which may be problematic in practice. We study how much <b>lattice</b> <b>sampling</b> can be sped up using floating-point arithmetic. First, we show that a direct floating-point implementation of these algorithms does not give any asymptotic speedup: the floating-point precision needs to be greater than the security parameter, leading to an overall complexity (n^ 3) where n is the lattice dimension. However, we introduce a laziness technique that can significantly speed up these algorithms. Namely, in certain cases such as NTRUsign lattices, laziness can decrease the complexity to (n^ 2) or even (n). Furthermore, our analysis is practical: for typical parameters, most of the floating-point operations only require the double-precision IEEE standard...|$|E
40|$|International audienceA {{procedure}} for sampling lattice vectors {{is at the}} heart of many lattice constructions, and the algorithm of Klein (SODA 2000) and Gentry, Peikert, Vaikuntanathan (STOC 2008) is currently the one that produces the shortest vectors. But due to the fact that its most time-efficient (quadratic-time) variant requires the storage of the Gram-Schmidt basis, the asymptotic space requirements of this algorithm are the same for general and ideal lattices. The main result of the current work is a series of algorithms that ultimately lead to a sampling procedure producing the same outputs as the Klein/GPV one, but requiring only linear-storage when working on lattices used in ideal-lattice cryptography. The reduced storage directly leads to a reduction in key-sizes by a factor of Ω(d), and makes cryptographic constructions requiring <b>lattice</b> <b>sampling</b> much more suitable for practical applications. At the core of our improvements is a new, faster algorithm for computing the Gram-Schmidt orthogonalization of a set of vectors that are related via a linear isometry. In particular, for a linear isometry r : R d → R d which is computable in time O(d) and a d-dimensional vector b, our algorithm for computing the orthogonalization of (b, r(b), r 2 (b),. [...] , r d− 1 (b)) uses O(d 2) floating point operations. This is in contrast to O(d 3) such operations that are required by the standard Gram-Schmidt algorithm. This improvement is directly applicable to bases that appear in ideal-lattice cryptography because those bases exhibit such " isometric structure ". The above-mentioned algorithm improves on a previous one of Gama, Howgrave-Graham, Nguyen (EUROCRYPT 2006) which used different techniques to achieve only a constant-factor speed-up for similar lattice bases. Interestingly, our present ideas can be combined with those from Gama et al. to achieve an even an larger practical speed-up. We next show how this new Gram-Schmidt algorithm can be applied towards <b>lattice</b> <b>sampling</b> in quadratic time using only linear space. The main idea is that rather than pre-computing and storing the Gram-Schmidt vectors, one can compute them " on-the-fly " while running th...|$|E
40|$|This paper {{examines}} the modeling accuracy of finite element interpolation, kriging, and polynomial regres-sion {{used in conjunction}} with the Progressive <b>Lattice</b> <b>Sampling</b> (PLS) incremental design-of-experiments approach. PLS is a paradigm for sampling a deterministic hypercubic parameter space by placing and incre-mentally adding samples in a manner intended to maximally reduce lack of knowledge in the parameter space. When combined with suitable interpolation methods, PLS is a formulation for progressive construc-tion of response surface approximations (RSA) in which the RSA are efficiently upgradable, and upon upgrading offer convergence information essential in estimating error introduced by the use of RSA in the problem. The three interpolation methods tried here are examined for perfomance in replicating an analytic test function as measured by several different indicators. The process described here provides a framework for future studies using other interpolation schemes, test functions, and measures of approximation quality...|$|E
40|$|The trapped {{magnetic}} field is examined in bulk high-temperature superconductors that are artificially drilled along their c-axis. The influence of the hole pattern on the magnetization is studied and compared by means of numerical models and Hall probe mapping techniques. To this aim, we consider two bulk YBCO samples with a rectangular cross-section that are drilled each by six holes arranged either on a rectangular <b>lattice</b> (<b>sample</b> I) or on a centered rectangular <b>lattice</b> (<b>sample</b> II). For the numerical analysis, three different models are considered for calculating the trapped flux: (i), a two-dimensional (2 D) Bean model neglecting demagnetizing effects and flux creep, (ii), a 2 D finite-element model neglecting demagnetizing effects but incorporating magnetic relaxation {{in the form of}} an E – J power law, and, (iii), a 3 D finite element analysis that takes into account both the finite height of the sample and flux creep effects. For the experimental analysis, the trapped magnetic flux density is measured above the sample surface by Hall probe mapping performed before and after the drilling process. The maximum trapped flux density in the drilled samples is found to be smaller than that in the plain samples. The smallest magnetization drop is found for sample II, with the centered rectangular lattice. This result is confirmed by the numerical models. In each sample, the relative drops that are calculated independently with the three different models are in good agreement. As observed experimentally, the magnetization drop calculated in the sample II is the smallest one and its relative value is comparable to the measured one. By contrast, the measured magnetization drop in sample (1) is much larger than that predicted by the simulations, most likely because of a change of the microstructure during the drilling process. Peer reviewe...|$|R
40|$|The multi-dimensional {{sampling}} theorem is explained. A mathematical {{description is}} used that makes possible the computation {{of the spectrum}} for regular and irregular (but periodic) <b>sampling</b> <b>lattices.</b> The N-dimensional conversion between two signals sampled with arbitrary <b>sampling</b> <b>lattices</b> is then presented in the frequency domain. The required ideal filters are given. After that the conversion is described as an N-dimensional time variant system. A structure for the realization is presented. An application of B= 2 is the television standards conversion...|$|R
40|$|The Cartesian Cubic lattice {{is known}} to be sub optimal when consideringband-limited signals but is still used as {{standard}} in three-dimensional medical magneticresonance imaging. The optimal <b>sampling</b> <b>lattices</b> are the body-centered cubic latticeand the face-centered cubic lattice. This report discusses the possible use of thesesampling lattices in MRI and presents verification of the non standard Fouriertransform method that is required for MR image creation for these <b>sampling</b> <b>lattices.</b> The results show that the Fourier transform is consistent with analytical models...|$|R
40|$|We give a {{complete}} characterization of all <b>lattice</b> <b>sampling</b> and interpolating sequences in the Fock space of polyanalytic functions, displaying a ”Nyquist rate” which increases with n, {{the degree of}} polyanaliticity of the space: a sequence of lattice points is sampling {{if and only if}} its density is strictly larger than n, and it is interpolating if and only if its density is strictly smaller than n. In our method of proof we introduce a unitary mapping between vector valued Hilbert spaces and polyanalytic Fock spaces, which allows to extend the theory of Valentine Bargmann to polyanalytic spaces. Then, we connect the problem to vector valued Gabor frames with Hermite windows, and apply duality principles from time-frequency analysis. This approach reveals a duality between sampling and interpolation in polyanalytic spaces, and multiple interpolation and sampling in analytic spaces, the latter being a ”purely holomorphic” problem...|$|E
40|$|Abstract. A {{procedure}} for sampling lattice vectors {{is at the}} heart of many lattice constructions, and the algorithm of Klein (SODA 2000) and Gentry, Peikert, Vaikuntanathan (STOC 2008) is currently the one that produces the shortest vectors. But due to the fact that its most time-efficient (quadratic-time) variant requires the storage of the Gram-Schmidt basis, the asymptotic space requirements of this algorithm are the same for general and ideal lattices. The main result of the current work is a series of algorithms that ultimately lead to a sampling proce-dure producing the same outputs as the Klein/GPV one, but requiring only linear-storage when working on lattices used in ideal-lattice cryp-tography. The reduced storage directly leads to a reduction in key-sizes by a factor of Ω(d), and makes cryptographic constructions requiring <b>lattice</b> <b>sampling</b> much more suitable for practical applications. At the core of our improvements is a new, faster algorithm for computing the Gram-Schmidt orthogonalization of a set of vectors that are relate...|$|E
40|$|We {{consider}} the approximate recovery of multivariate periodic functions from a discrete set of function values {{taken on a}} rank-s integration lattice. The main result {{is the fact that}} any (non-) linear reconstruction algorithm taking function values on a rank-s lattice of size M has a dimension-independent lower bound of 2 ^-(α+ 1) / 2 M^-α/ 2 when considering the optimal worst-case error with respect to function spaces of (hybrid) mixed smoothness α> 0 on the d-torus. We complement this lower bound with upper bounds that coincide up to logarithmic terms. These upper bounds are obtained by a detailed analysis of a rank- 1 <b>lattice</b> <b>sampling</b> strategy, where the rank- 1 lattices are constructed by a component-by-component (CBC) method. This improves on earlier results obtained in [25] and [27]. The lattice (group) structure allows for an efficient approximation of the underlying function from its sampled values using a single one-dimensional fast Fourier transform. This is one reason why these algorithms keep attracting significant interest. We compare our results to recent (almost) optimal methods based upon samples on sparse grids...|$|E
40|$|Abstract—This paper {{proposes a}} new family of bivariate, nonseparable splines, called hex-splines, {{especially}} designed for hexagonal lattices. The starting point of the construction is the indicator function of the Voronoi cell, {{which is used to}} define in a natural way the first-order hex-spline. Higher order hex-splines are obtained by successive convolutions. A mathematical analysis of this new bivariate spline family is presented. In particular, we derive a closed form for a hex-spline of arbitrary order. We also discuss important properties, such as their Fourier transform and the fact they form a Riesz basis. We also highlight the approximation order. For conventional rectangular lattices, hex-splines revert to classical separable tensor-product B-splines. Finally, some prototypical applications and experimental results demonstrate the usefulness of hex-splines for handling hexagonally sampled data. Index Terms—Approximation theory, bivariate splines, hexagonal <b>lattices,</b> <b>sampling</b> theory. I...|$|R
40|$|A {{custom-designed}} micro-digital {{image correlation}} system {{was used to}} track {{the evolution of the}} full-surface three-dimensional strain field of Ti 6 Al 4 V additively manufactured <b>lattice</b> <b>samples</b> under mechanical loading. The high-magnification capabilities of the method allowed to resolve the strain distribution down to the strut level and disclosed a highly heterogeneous mechanical response of the lattice structure with local strain concentrations well above the nominal global strain level. In particular, we quantified that strain heterogeneity appears at a very early stage of the deformation process and increases with load, showing a strain accumulation pattern with a clear correlation to the later onset of the fracture. The obtained results suggest that the unique opportunities offered by the proposed experimental method, in conjunction with analytical and computational models, could serve to provide novel important information for the rational design of additively manufactured porous biomaterials...|$|R
40|$|Abstract- In a {{spatially}} adaptive subsampling scheme, the subsampling lattice {{is adapted}} to the local spatial frequency content of an image sequence. In this paper, we use rate-distortion theory to show that spatially adaptive subsampling gives a better performance than subsampling with a fixed <b>sampling</b> <b>lattice.</b> A new algorithm that optimally assigns <b>sampling</b> <b>lattices</b> to {{different parts of the}} image is presented. The proposed spatially adaptive subsampling method can be applied within a motion-compensated coding scheme as well. Experiments show an increased perfor-mance over fixed lattice subsampling. I...|$|R
