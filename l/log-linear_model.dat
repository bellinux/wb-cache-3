624|1041|Public
25|$|This allows an {{exponentially}} growing {{variable to}} be modeled with a <b>log-linear</b> <b>model.</b> For example, if {{one wishes to}} empirically estimate the growth rate from intertemporal data on x, one can linearly regress log x on t.|$|E
50|$|A <b>log-linear</b> <b>model</b> is decomposable {{if it is}} {{graphical}} and if {{the corresponding}} graph is chordal.|$|E
5000|$|Each <b>log-linear</b> <b>model</b> can be {{represented}} as a log-linear equation. For example, with the three variables (A, B, C) the saturated model has the following log-linear equation: ...|$|E
40|$|This paper {{describes}} <b>log-linear</b> parsing <b>models</b> for Combinatory Categorial Grammar (CCG). <b>Log-linear</b> <b>models</b> {{can easily}} encode the long-range dependencies inherent in coordination and extraction phenomena, which CCG {{was designed to}} handle. <b>Log-linear</b> <b>models</b> have previously been applied to statistical parsing, {{under the assumption that}} all possible parses for a sentence can be enumerated. Enumerating al...|$|R
40|$|We {{present a}} {{framework}} for word alignment based on <b>log-linear</b> <b>models.</b> All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. <b>Log-linear</b> <b>models</b> allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that <b>log-linear</b> <b>models</b> significantly outperform IBM translation models. ...|$|R
40|$|Abstract—We present latent <b>log-linear</b> <b>models,</b> an {{extension}} of <b>log-linear</b> <b>models</b> incorporating latent variables and we propose two applications thereof: <b>log-linear</b> mixture <b>models</b> and image deformation-aware <b>log-linear</b> <b>models.</b> The resulting models are fully discriminative, can be trained efficiently, and the model complexity can be controlled. <b>Log-linear</b> mixture <b>models</b> offer additional flexibility within the <b>log-linear</b> <b>modeling</b> framework. Unlike previous approaches, the image deformation-aware model directly considers image deformations and allows for a discriminative training of the deformation parameters. Both are trained using alternating optimization. For certain variants convergence to a stationary point is guaranteed and in practice even variants without this guarantee converge and find models that perform well. We tune the methods on the USPS dataset and evaluate on the MNIST dataset demonstrating the generalization capabilities of our proposed models. Our models, although using significantly fewer parameters, are able to obtain competitive results with models proposed in the literature. Index Terms—Log-linear models, latent variables, conditional random fields, OCR, image classificatio...|$|R
5000|$|Any Markov random field (with a {{strictly}} positive density) {{can be written}} as <b>log-linear</b> <b>model</b> with feature functions [...] such that the full-joint distribution can be written as ...|$|E
50|$|This allows an {{exponentially}} growing {{variable to}} be modeled with a <b>log-linear</b> <b>model.</b> For example, if {{one wishes to}} empirically estimate the growth rate from intertemporal data on x, one can linearly regress log x on t.|$|E
5000|$|A <b>log-linear</b> <b>model</b> is a {{mathematical}} model that {{takes the form of}} a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form ...|$|E
40|$|Conventional speech {{recognition}} systems {{are based on}} Gaussian hidden Markov models (HMMs). Discriminative techniques such as <b>log-linear</b> <b>modeling</b> have been investigated in {{speech recognition}} only recently. This thesis establishes a <b>log-linear</b> <b>modeling</b> framework {{in the context of}} discriminative training criteria, with examples from continuous speech recognition, part-of-speech tagging, and handwriting recognition. The focus will be on the theoretical and experimental comparison of different training algorithms. Equivalence relations for Gaussian and <b>log-linear</b> <b>models</b> in speech recognition are derived. It is shown how to incorporate a margin term into conventional discriminative training criteria like for example minimum phone error (MPE). This permits to evaluate directly the utility of the margin concept for string recognition. The equivalence relations and the margin-based training criteria lead to a unified view of three major training paradigms, namely Gaussian HMMs, <b>log-linear</b> <b>models,</b> and support vector machines (SVMs). Generalized iterative scaling (GIS) is traditionally used for the optimization of <b>log-linear</b> <b>models</b> with the maximum mutual information (MMI) criterion. This thesis suggests an extension of GIS to <b>log-linear</b> <b>models</b> including hidden variables, and to other training criteria (e. g. MPE). Finally, investigations on convex optimization in speech recognition are presented. Experimental results are provided for a variety of tasks, including the European Parliament plenary sessions task and Mandarin broadcasts...|$|R
40|$|Non-response {{boundary}} solutions {{occur in}} <b>log-linear</b> <b>models</b> with non-ignorable non-response. We prove existence and uniqueness of maximum likelihood estimates under weak conditions, and their consistency and asymptotic normality. This contrasts with findings for boundary solutions in <b>log-linear</b> <b>models</b> for sparse tables. Boundary solutions Categorical data Consistency Informative non-response Normal approximation...|$|R
40|$|Several {{authors have}} {{previously}} discussed {{the use of}} <b>log-linear</b> <b>models,</b> often called maximum entropy models, for analyzing spike train data to detect synchrony. The usual <b>log-linear</b> <b>modeling</b> techniques, however, do not allow time-varying firing rates that typically appear in stimulus-driven (or action-driven) neurons, nor do they incorporate non-Poisson history effects or covariate effects. We generalize the usual approach, combining point-process regression models of individual neuron activ-ity with <b>log-linear</b> <b>models</b> of multiway synchronous interaction. The methods are illustrated with results found in spike trains recorded si-multaneously from primary visual cortex. We then assess the amount of data needed to reliably detect multiway spiking. ...|$|R
50|$|In {{more general}} {{mathematical}} settings, the Boltzmann distribution {{is also known}} as the Gibbs measure. In statistics and machine learning it is called a <b>log-linear</b> <b>model.</b> In deep learning the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine.|$|E
5000|$|The {{formulation}} of binary logistic regression as a <b>log-linear</b> <b>model</b> {{can be directly}} extended to multi-way regression. That is, we model the logarithm of the probability of seeing a given output using the linear predictor {{as well as an}} additional normalization factor, the logarithm of the partition function: ...|$|E
50|$|Because only {{differences}} of vectors of regression coefficients are used, adding an arbitrary constant to all coefficient vectors {{has no effect}} on the model. This means that, just as in the <b>log-linear</b> <b>model,</b> only K-1 of the coefficient vectors are identifiable, and the last one can be set to an arbitrary value (e.g. 0).|$|E
5000|$|Model Fit {{involving}} categorical predictors may {{be achieved}} by using <b>log-linear</b> <b>modeling.</b>|$|R
5000|$|... #Subtitle level 3: For {{datasets}} {{with a few}} variables - general <b>log-linear</b> <b>models</b> ...|$|R
40|$|This paper {{describes}} <b>log-linear</b> <b>models</b> for a general-purpose sentence realizer {{based on}} dependency structures. Unlike traditional realizers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps. First, the relative order between head and each dependent {{is determined by}} their dependency relation. Then the best linearizations compatible with the relative order are selected by <b>log-linear</b> <b>models.</b> The <b>log-linear</b> <b>models</b> incorporate three types of feature functions, including dependency relations, surface words and headwords. Our approach to sentence realization provides simplicity, efficiency and competitive accuracy. Trained on 8, 975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0. 8874. ...|$|R
5000|$|... {{while the}} {{generalized}} <b>log-linear</b> <b>model</b> for [...] and [...] according to Kuonen [...] and Lee [...] is defined as: [...] and , where [...] refers to attacking and defensive strengths and to home field advantage respectively. [...] and [...] are correction factors which represent {{the means of}} goals scored during the season by home and away teams.|$|E
50|$|A <b>log-linear</b> <b>model</b> is {{graphical}} if, {{whenever the}} model contains all two-factor terms {{generated by a}} higher-order interaction, the model also contains the higher-order interaction.As a direct-consequence, graphical models are hierarchical. Moreover, being completely determined by its two-factor terms, a graphical model can be represented by an undirected graph, where the vertices represent the variables and the edges represent the two-factor terms included in the model.|$|E
50|$|In statistics, Poisson {{regression}} is {{a generalized}} linear model form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a <b>log-linear</b> <b>model,</b> especially when used to model contingency tables.|$|E
40|$|This is {{yet another}} {{introduction}} to <b>log-linear</b> (“maximum entropy”) <b>models</b> for NLP practitioners, {{in the spirit of}} Berger (1996) and Ratnaparkhi (1997 b). The derivations here are similar to Berger’s, but more details are filled in and some errors are corrected. I do not address iterative scaling (Darroch and Ratcliff, 1972), but rather give derivations of the gradient and Hessian of the dual objective function (conditional likelihood). Note: This is a draft; please contact the author if you have comments, and do not cite or circulate this document. 1 <b>Log-linear</b> <b>Models</b> <b>Log-linear</b> <b>models</b> 1 have become a widely-used tool in NLP classification tasks (Berger et al., 1996; Ratnaparkhi, 1998). <b>Log-linear</b> <b>models</b> assign joint probabilities to observation/label pairs (x, y) ∈ X × Y as follows: Pr(x, y) ...|$|R
40|$|Inference in <b>log-linear</b> <b>models</b> scales linearly in {{the size}} of output space in the worst-case. This is often a {{bottleneck}} in natural language processing and computer vision tasks when the output space is feasibly enumerable but very large. We propose a method to perform inference in <b>log-linear</b> <b>models</b> with sublinear amortized cost. Our idea hinges on using Gumbel random variable perturbations and a pre-computed Maximum Inner Product Search data structure to access the most-likely elements in sublinear amortized time. Our method yields provable runtime and accuracy guarantees. Further, we present empirical experiments on ImageNet and Word Embeddings showing significant speedups for sampling, inference, and learning in <b>log-linear</b> <b>models.</b> Comment: In UAI proceeding...|$|R
40|$|<b>Log-linear</b> <b>models</b> have {{recently}} been used in acoustic modeling for speech recognition systems. This has been motivated by competitive results compared to systems based on Gaussian models, and a more direct parametrisation of the posterior model. To competitively use <b>log-linear</b> <b>models</b> for speech recognition, important methods, such as speaker adaptation, have to be reformulated in a log-linear framework. In this work, an approach to log-linear affine feature transforms for speaker adaptation is described. Experiments for both supervised and unsupervised adaptation are presented, showing improvements over a maximum likelihood baseline {{in the form of}} feature space maximum likelihood linear regression for the case of supervised adaptation. Index Terms: speech recognition, adaptation, <b>log-linear</b> <b>models</b> 1...|$|R
50|$|Log-linear {{models are}} {{especially}} convenient for their interpretation. A <b>log-linear</b> <b>model</b> {{can provide a}} much more compact representation for many distributions, especially when variables have large domains. They are convenient too because their negative log likelihoods are convex. Unfortunately, though {{the likelihood of a}} logistic Markov network is convex, evaluating the likelihood or gradient of the likelihood of a model requires inference in the model, which is generally computationally infeasible (see 'Inference' below).|$|E
5000|$|Given {{a two-way}} (I &times; J)-table of counts , where the cell values {{are assumed to}} be Poisson or multinomially distributed, we wish to {{estimate}} a decomposition [...] for all i and j such that [...] is the maximum likelihood estimate (MLE) of the expected values [...] leaving the marginals [...] and [...] fixed. The assumption that the table factorizes in such a manner is known as the model of independence (I-model). Written in terms of a <b>log-linear</b> <b>model,</b> we can write this assumption as , where , [...] and the interaction term vanishes, that is [...] for all i and j.|$|E
50|$|However, these {{assumptions}} are inappropriate for {{some types of}} response variables. For example, {{in cases where the}} response variable is expected to be always positive and varying over a wide range, constant input changes lead to geometrically varying, rather than constantly varying, output changes. As an example, a prediction model might predict that 10 degree temperature decrease would lead to 1,000 fewer people visiting the beach is unlikely to generalize well over both small beaches (e.g. those where the expected attendance was 50 at a particular temperature) and large beaches (e.g. those where the expected attendance was 10,000 at a low temperature). The problem with this kind of prediction model would imply a temperature drop of 10 degrees would lead to 1,000 fewer people visiting the beach, a beach whose expected attendance was 50 at a higher temperature would now be predicted to have the impossible attendance value of −950. Logically, a more realistic model would instead predict a constant rate of increased beach attendance (e.g. an increase in 10 degrees leads to a doubling in beach attendance, and a drop in 10 degrees leads to a halving in attendance). Such a model is termed an exponential-response model (or <b>log-linear</b> <b>model,</b> since the logarithm of the response is predicted to vary linearly).|$|E
40|$|Estimation of pure {{premiums}} for alternative rate classes using regression methods requires {{the choice of}} a functional form for the statistical model. Common choices include linear and <b>log-linear</b> <b>models.</b> This paper considers maximum likelihood estimation and testing for functional form using the power transformation sug-gested by Box and Cox. The linear and <b>log-linear</b> <b>models</b> are special cases of this transformation. Application of the procedure is illustrated using auto insur-ance claims data from the state of Massachusetts and from the United Kingdom. The predictive accuracy of the method compares favorably to that for the linear and <b>log-linear</b> <b>models</b> for both data sets. KEYWORDS Pure premium regression models, functional form, maximum likelihood estima-tion, power transformation. 1...|$|R
40|$|In {{the past}} decade the social {{sciences}} have seen an upsurge of interest in analysing multidimensional contingency tables using <b>log-linear</b> <b>models.</b> Two broad families of <b>log-linear</b> <b>models</b> may be distinguished: the family of conventional models and the family of unconventional models (that is, quasi-log-linear and hybrid models). In this paper {{a brief review of}} such models is presented and some linkage to the class of generalised linear models suggested by Nelder and Wedderburn is provided. The great potential of <b>log-linear</b> <b>models</b> for spatial analysis is illustrated in applying conventional and unconventional models in a migration context to identify intertemporal stability of migration patterns. The problem that the effective units migrating are households rather than individuals is coped with by postulating a compound Poisson sampling scheme. ...|$|R
40|$|<b>Log-linear</b> <b>models</b> {{are widely}} used for {{qualitative}} data in multidimensional contingency tables. Hierarchical <b>log-linear</b> <b>models</b> are models that include all lower-order terms composed from variables contained in a higher-order model term. The starting point is a saturated model, then homogenous associations, conditional independence and complete independence. There are several statistics that help to choose the best model. The first is the likelihood ratio approach, next is AIC and BIC information criteria. In R software there is loglm() function in MASS library and glm in stats library. The first approach is presented in this pape...|$|R
40|$|This paper {{discusses}} the <b>log-linear</b> <b>model</b> for multi-way contingency ta-ble, where the cell values represent the frequency counts that follow an ex-tended negative multinomial distribution. This {{is an extension}} of negative multinomial <b>log-linear</b> <b>model</b> described by Evans (1989). The parameters of the new model are estimated by maximum likelihood method. The like-lihood ratio test for the general log-linear hypothesis is also derived. A practical application of the <b>log-linear</b> <b>model</b> under the generalized inverse sampling scheme has also been demonstrated by an example. 1...|$|E
40|$|Orthographic {{similarities}} across languages {{provide a}} strong signal for probabilistic decipherment, especially for closely related language pairs. The existing decipherment models, however, are not well-suited for exploiting these orthographic similarities. We propose a <b>log-linear</b> <b>model</b> with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive {{for the proposed}} <b>log-linear</b> <b>model.</b> To address this challenge, we perform approximate inference via MCMC sampling and contrastive divergence. Our {{results show that the}} proposed <b>log-linear</b> <b>model</b> with contrastive divergence scales to large vocabularies and outperforms the existing generative decipherment models by exploiting the orthographic features...|$|E
40|$|AbstractIn {{this paper}} we {{consider}} categorical {{data that are}} distributed according to a multinomial, product-multinomial or Poisson distribution whose expected values follow a <b>log-linear</b> <b>model</b> and we study the inference problem of hypothesis testing in a <b>log-linear</b> <b>model</b> setting. The family of test statistics considered {{is based on the}} family of ϕ-divergence measures. The unknown parameters in the <b>log-linear</b> <b>model</b> under consideration are also estimated using ϕ-divergence measures: Minimum ϕ-divergence estimators. A simulation study is included to find test statistics that offer an attractive alternative to the Pearson chi-square and likelihood-ratio test statistics...|$|E
30|$|<b>Log-linear</b> <b>modeling</b> {{showed that}} RENEX was {{equivalent}} to any expert in rating kidneys, {{particularly in the}} obstructed and non-obstructed categories. This conclusion, {{which could not be}} derived from the original ROC and kappa analysis, emphasizes and illustrates the role and importance of <b>log-linear</b> <b>modeling</b> {{in the absence of a}} gold standard. The log-linear analysis also provides additional evidence that RENEX has the potential to assist in the interpretation of diuresis renography studies.|$|R
40|$|Abstract. <b>Log-linear</b> <b>models</b> {{represent}} nowadays the state-of-the-art in statistical machine translation. There, several {{models are}} combined altogether {{into a whole}} statistical approach. Finite-state transducers constitute a special type of statistical translation model whose interest has been proved in different translation tasks. The goal of this work is to introduce a finite-state framework for a <b>log-linear</b> <b>modelling</b> approach in statistical machine translation. Results for a French-English technical translation task show the convenience of the proposed methods. ...|$|R
40|$|The use of <b>log-linear</b> <b>models</b> for {{investigating}} differential item functioning (DIF) associated with examinee/respondent background characteristics was examined. The Likert-type items {{used in this}} study were drawn from a 36 -item self-report measure [...] the Suicide Probability Scale. Specifically, <b>log-linear</b> <b>models</b> were used to investigate whether contingency tables for ethnicity (55 African Americans, 186 Anglo Americans, and 189 Hispanic Americans) or gender, 332 males and 627 females) by item response by mental health status suggested evidence of an interaction between the background varlable and item response. The investigation focused on a set of 35 Likert-type items that measure subjective well- being and coping behavior. Several <b>log-linear</b> <b>models</b> were fit to the data, and rationale for the composition of the various models is discussed. Among tables where a statistically significan...|$|R
