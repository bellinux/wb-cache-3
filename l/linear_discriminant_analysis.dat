4343|10000|Public
25|$|Three Iris {{varieties}} {{are used in}} the Iris flower data set outlined by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of <b>linear</b> <b>discriminant</b> <b>analysis.</b>|$|E
25|$|The first BCI {{session with}} 100% {{accuracy}} (based on 80 right-hand and 80 left-hand movement imaginations) {{was recorded in}} 1998 by Christoph Guger. The BCI system used 27 electrodes overlaying the sensorimotor cortex, weighted the electrodes with Common Spatial Patterns, calculated the running variance and used a <b>linear</b> <b>discriminant</b> <b>analysis.</b>|$|E
5000|$|Classification: <b>linear</b> <b>discriminant</b> <b>analysis</b> (LDA), Basic perceptron, Elastic Net, {{logistic}} regression, (Kernel) Support Vector Machines (SVM), Diagonal <b>Linear</b> <b>Discriminant</b> <b>Analysis</b> (DLDA), Golub Classifier, Parzen-based, (kernel) Fisher Discriminant Classifier, k-nearest neighbor, Iterative RELIEF, Classification Tree, Maximum Likelihood Classifier ...|$|E
40|$|Tourmaline has an {{extensive}} range of solid solution that can potentially reflect {{the nature of}} its coexisting phases, including some minerals of economic interest. In this study, tourmaline compositions are considered in the framework of two hypothetical classes of mineral deposits to evaluate the use of tourmaline chemistry to assess economic potential. Tourmalines from 27 known mineralized and barren localities in Nova Scotia were analyzed by electron micro-probe for their major elements and by inductively coupled plasma mass spectrometry for their boron isotope compositions. <b>Linear</b> <b>discriminant</b> function <b>analysis</b> of these data on a two-group (mineralized and barren) basis correctly classified 85 to 89 percent of the tourmaline samples. Similarly, <b>linear</b> <b>discriminant</b> function <b>analysis</b> on a three-group (nongranite miner-alized, granite-related mineralized, granite-related barren) basis correctly classified 72 to 85 percent of the tourmaline samples. Quadratic <b>discriminant</b> function <b>analysis</b> resulted in even better discrimination. These preliminary results suggest that this type of geochemical-statistical approach is promising for mineral exploration...|$|R
30|$|Recently, many LDA-based {{methods have}} been {{proposed}} to embed manifold structure into the facial feature extraction process [6 – 13]. Park and Savvides [6] proposed a multifactor extension of LDA. Na et al. [7] proposed the <b>linear</b> boundary <b>discriminant</b> <b>analysis,</b> which increases class separability by reflecting different significances of nonboundary and boundary patterns.|$|R
30|$|The {{investigation}} {{was conducted on}} a series of 92 human skulls (61 males and 31 females). The orbital height (OH) and breadth (OB) were measured by using digital vernier calliper. Data analysis was performed by computing descriptive statistics like mean, standard deviation and range. <b>Linear</b> <b>discriminant</b> function <b>analysis</b> was carried out to derive a classification model.|$|R
50|$|<b>Linear</b> <b>discriminant</b> <b>analysis.</b>|$|E
5000|$|Logistic {{regression}} is {{an alternative}} to Fisher's 1936 method, <b>linear</b> <b>discriminant</b> <b>analysis.</b> [...] If the assumptions of <b>linear</b> <b>discriminant</b> <b>analysis</b> hold, the conditioning can be reversed to produce logistic regression. The converse is not true, however, because logistic regression {{does not require the}} multivariate normal assumption of discriminant analysis.|$|E
5000|$|<b>Linear</b> <b>discriminant</b> <b>analysis</b> is a {{generalization}} of Fishers linear discriminant ...|$|E
40|$|Description This package {{provides}} an efficient framework for high-dimensional <b>linear</b> and diagonal <b>discriminant</b> <b>analysis</b> with variable selection. The classifier is trained using James-Stein-type shrinkage estimators and predictor variables are ranked using CAT scores (correlation-adjusted t-scores). Variable selection error is controlled using false non-discovery rates or higher criticism scores...|$|R
40|$|This study {{proposed}} a novel way to calculate Basic Probability Assignment(BPA), which {{is crucial in}} the data fusion. The study mapped sensing signal with a <b>linear</b> <b>discriminant</b> function <b>analysis</b> and assessed the sizes with time. This was beneficial to get a clue for context inference with using the Dempster-Shafer Theory and to determine BPA based on the size changing of mapping data in time intervals. The study provides with the way of context inference for fast detecting a local event that affects the whole area...|$|R
40|$|Decision {{trees are}} used very {{successfully}} for the identification resp. classification task {{of objects in}} many domains like marketing (e. g. Decker, Temme (2001)) or medicine. Other procedures to classify objects are for instance the logistic regression, the logit- or probit analysis, the <b>linear</b> or squared <b>discriminant</b> <b>analysis,</b> the nearest neighbour procedure o...|$|R
5000|$|Risk Minimization (Support vector regression, support vector machine, <b>linear</b> <b>discriminant</b> <b>analysis)</b> ...|$|E
5000|$|<b>Linear</b> <b>Discriminant</b> <b>Analysis</b> (or Fisher's linear discriminant) (LDA)—assumes Gaussian {{conditional}} density models ...|$|E
5000|$|The softmax {{function}} {{is used in}} various multiclass classification methods, such as multinomial logistic regression, multiclass <b>linear</b> <b>discriminant</b> <b>analysis,</b> naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and <b>linear</b> <b>discriminant</b> <b>analysis,</b> the input to the {{function is}} the result of [...] distinct linear functions, and the predicted probability for the 'th class given a sample vector [...] and a weighting vector [...] is: ...|$|E
40|$|Friedman (1989) has {{proposed}} a regularization technique (RDA) of <b>discriminant</b> <b>analysis</b> in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between <b>linear</b> and quadratic <b>discriminant</b> <b>analysis.</b> In this paper, we propose an alternative approach to design classi cation rules which have also a median position between <b>linear</b> and quadratic <b>discriminant</b> <b>analysis.</b> Our approach {{is based on the}} reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k�Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA...|$|R
30|$|Univariate {{analysis}} of variance of 180 adult specimens showed {{significant differences between the}} means of the two studied groups for 15 standardized morphometric measurements out of 31 (P[*]<[*] 0.05). In morphometric trait <b>linear</b> <b>discriminant</b> function <b>analysis,</b> the overall assignments of individuals into their original groups in male and female specimens were 77.1 and 84.0  %, respectively. The <b>discriminant</b> <b>analysis</b> showed a morphological segregation of the studied populations based on the characters predorsal length, interdorsal, interorbital distance, tail length, and first dorsal fin length. The principal component analysis, scatter plot of individual component score between PC 1 and PC 2, showed the specimens grouped into two areas but with high and moderate overlap between two localities in males and females, respectively.|$|R
40|$|Among various {{statistical}} {{and data}} mining <b>discriminant</b> <b>analysis</b> proposed so far for group classification, <b>linear</b> programming <b>discriminant</b> <b>analysis</b> have recently attracted the researchers’ interest. This study evaluates multi-group <b>discriminant</b> <b>linear</b> programming (MDLP) for classification problems against well-known {{methods such as}} neural networks, support vector machine, and so on. MDLP is less complex compared to other methods and does not suffer from local optima. However, sometimes classification becomes infeasible due to insufficient data in databases {{such as in the}} case of an Internet Service Provider (ISP) small and medium-sized market considered in this research. This study proposes a fuzzy Delphi method to select and gather required data. The results show that the performance of MDLP is better than other methods with respect to correct classification, at least for small and medium-sized datasets...|$|R
50|$|The Mahalanobis {{distance}} used in Fisher's <b>linear</b> <b>discriminant</b> <b>analysis</b> is {{a particular}} case of the Bhattacharyya Distance.|$|E
5000|$|Multivariate analysis: Cluster analysis, Principal {{components}} analysis, <b>Linear</b> <b>discriminant</b> <b>analysis,</b> canonical analysis, Multidimensional scaling, Canonical {{correlation analysis}} ...|$|E
5000|$|<b>Linear</b> <b>discriminant</b> <b>analysis</b> (LDA) computes {{a linear}} {{predictor}} from {{two sets of}} normally distributed data to allow for classification of new observations.|$|E
40|$|<b>Linear</b> and Quadratic <b>Discriminant</b> <b>Analysis</b> {{have been}} used widely {{in many areas of}} data mining, machine learning, and bioinformatics. Friedman {{proposed}} a compromise between <b>Linear</b> and Quadratic <b>Discriminant</b> <b>Analysis,</b> called Regularized <b>Discriminant</b> <b>Analysis</b> (RDA), which {{has been shown to be}} more flexible in dealing with various class distributions. RDA applies the regularization techniques by employing two regularization parameters, which are chosen to jointly maximize the classification performance. The optimal pair of parameters is commonly estimated via crossvalidation from a set of candidate pairs. It is computationally prohibitive for high dimensional data, especially when the candidate set is large, which limits the applications of RDA to low dimensional data. In this paper, a novel algorithm for RDA is presented for high dimensional data. It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently. Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency, and also show that, for a properly chosen pair of regularization parameters, RDA performs favorably in classification, in comparison with other existing classification methods...|$|R
40|$|Optokinetic {{nystagmus}} (OKN) was {{studied in}} ten patients with vestibular neuritis, and in seventeen patients with unilateral and thirteen patients with bilateral infratentorial lesions and compared with OKN in fifty healthy subjects. Mean and maximum slow phase velocity of OKN was calculated {{as well as}} the asymmetry of responses. The efficiency of the different variables in OKN, in discriminating between different lesion sites, was tested in a <b>linear</b> <b>discriminant</b> function <b>analysis.</b> Slow phase velocity of OKN could satisfactorily separate patients with infratentorial lesions from healthy subjects or subjects with vestibular neuritis. Asymmetry of OKN did not contribute further to the correct diagnosis...|$|R
40|$|This paper studies {{recognition}} of human faces using wavelet transform, Eigen space mapping and <b>Linear</b> <b>Discriminant</b> Analysis/Fisher <b>Analysis</b> (LDA). Histogram Equalization is chosen as a preprocessing step {{to reduce the}} effect of variation in illumination on human faces. The preprocessed faces are then subjected to second level wavelet (Haar) decomposition for further calculation. Feature extraction is performed using Eigen space mapping followed by LDA on the second level approximation matrix (LL sub band). Manhattan distance {{is used as a}} classifier. The proposed scheme is tested on illumination and expression variant different face databases for performance evaluation...|$|R
50|$|The {{most widely}} used {{learning}} algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, <b>linear</b> <b>discriminant</b> <b>analysis,</b> decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).|$|E
5000|$|The {{weighted}} {{form of the}} GSVD {{is called}} as such because, with the correct selection of weights, it generalizes many techniques (such as multidimensional scaling and <b>linear</b> <b>discriminant</b> <b>analysis)</b> ...|$|E
50|$|Multilinear {{subspace}} learning algorithms are higher-order generalizations {{of linear}} subspace learning {{methods such as}} principal component analysis (PCA), independent component analysis (ICA), <b>linear</b> <b>discriminant</b> <b>analysis</b> (LDA) and canonical correlation analysis (CCA).|$|E
40|$|We {{provide a}} general {{theoretical}} framework to derive Bernstein-von Mises theorems for matrix functionals. The conditions on functionals and priors are explicit {{and easy to}} check. Results are obtained for various functionals including entries of covariance matrix, entries of precision matrix, quadratic forms, log-determinant, eigenvalues in the Bayesian Gaussian covariance/precision matrix estimation setting, {{as well as for}} Bayesian <b>linear</b> and quadratic <b>discriminant</b> <b>analysis...</b>|$|R
40|$|We {{propose a}} {{methodology}} that uses {{data envelopment analysis}} (DEA) for solving the inverse classification problem. An inverse classification problem involves finding out how predictor attributes of a case can be changed so that the case can be classified into a different and more desirable class. For a binary classification problem and non-negative decision-making attributes, we show that under the assumption of conditional monotonicity, and convexity of classes, DEA {{can be used for}} inverse classification problem. We illustrate the application of our proposed methodology on a hypothetical and a real-life bankruptcy prediction data. Classification Data envelopment <b>analysis</b> <b>Linear</b> programming <b>Discriminant</b> <b>analysis...</b>|$|R
40|$|Biological and {{environmental}} sources of morphological variation were evaluated in populations of Littorina littorea from two estuaries in New England. Intraspecific morphological groups were characterized using <b>linear</b> <b>discriminant</b> function <b>analysis</b> on {{a suite of}} sixteen morphometric variables relating to length, width, height, weight, and recent growth history. Subsequent characterizations {{were based on a}} multivariate generalization of the allometric growth model. The parameters of each model were derived from the eigenvectors resulting from principal component analyses using within-group covariance matrices of a log transformed subset of seven variables. ^ Three independent trends in morphological variation were identified that correspond to size, weight, and shape. These trends jointly explain more than 98...|$|R
50|$|Three Iris {{varieties}} {{are used in}} the Iris flower data set outlined by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of <b>linear</b> <b>discriminant</b> <b>analysis.</b>|$|E
50|$|Mahalanobis {{distance}} {{is widely used}} in cluster analysis and classification techniques. It {{is closely related to}} Hotelling's T-square distribution used for multivariate statistical testing and Fisher's <b>Linear</b> <b>Discriminant</b> <b>Analysis</b> that is used for supervised classification.|$|E
50|$|Popular {{recognition}} algorithms include {{principal component}} analysis using eigenfaces, <b>linear</b> <b>discriminant</b> <b>analysis,</b> elastic bunch graph matching using the Fisherface algorithm, the hidden Markov model, the multilinear subspace learning using tensor representation, and the neuronal motivated dynamic link matching.|$|E
40|$|In <b>linear</b> <b>discriminant</b> (LD) <b>analysis</b> high sample size/feature {{ratio is}} desirable. The linear {{programming}} procedure (LP) for LD identification handles {{the curse of}} dimensionality through simultaneous minimization of the L 1 norm of the classification errors and the LD weights. The sparseness of the solution 2 ̆ 013 the fraction of features retained 2 ̆ 013 can be controlled by a parameter in the objective function. By qualitatively analyzing the objective function and {{the constraints of the}} problem, we show why sparseness arises. In a sparse solution, large values of the LD weight vector reveal those individual features most important for the decision boundary. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|It is {{a common}} {{practice}} that a matrix, the de facto image representation, is first converted into a vector before fed into subspace analysis or kernel method; however, the conversion ruins the spatial structure of the pixels that defines the image. In this paper, we propose two kernel subspace methods that are directly based on the matrix representation, namely matrix-based kernel principal component analysis (matrix KPCA) and matrix-based kernel <b>linear</b> <b>discriminant</b> component <b>analysis</b> (matrix KLDA). We show that, through an extended Gram matrix, the two proposed matrix-based kernel subspace methods generalize their vector-based counterparts and contain richer information. Our experiments also confirm {{the advantages of the}} matrix-based kernel subspace methods over the vector-based ones...|$|R
40|$|King {{mackerel}} (Scomberomorus cavalla) are ecologically {{and economically}} important scombrids that inhabit U. S. {{waters of the}} Gulf of Mexico (GOM) and Atlantic Ocean (Atlantic). Separate migratory groups, or stocks, migrate from eastern GOM and southeastern U. S. Atlantic to south Florida waters where the stocks mix during winter. Currently, all winter landings from a management-defined south Florida mixing zone are attributed to the GOM stock. In this study, the stock composition of winter landings across three south Florida sampling zones was estimated by using stock-specific otolith morphological variables and Fourier harmonics. The mean accuracies of the jackknifed classifications from stepwise <b>linear</b> <b>discriminant</b> function <b>analysis</b> of otolith shape variables ranged from 66 − 76...|$|R
