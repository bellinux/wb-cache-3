386|773|Public
2500|$|Comparing two {{different}} arrays, or {{two different}} samples hybridized {{to the same}} array generally involves making adjustments for systematic errors introduced by differences in procedures and dye intensity effects. [...] Dye normalization for two color arrays is often achieved by <b>local</b> <b>regression.</b> [...] LIMMA provides a set of tools for background correction and scaling, {{as well as an}} option to average on-slide duplicate spots. A common method for evaluating how well normalized an array is, is to plot an MA plot of the data.|$|E
5000|$|Many {{types of}} models and {{techniques}} {{are subject to}} this formulation. A few examples are linear least squares, smoothing splines, regression splines, <b>local</b> <b>regression,</b> kernel regression, and linear filtering ...|$|E
50|$|An {{important}} concept {{associated with}} regression splines {{is that of}} a knot. Knot is where one <b>local</b> <b>regression</b> model gives way to another and thus is the point of intersection between two splines.|$|E
40|$|We {{study the}} problem of {{estimating}} σ from individuals chart data {{in the presence of}} out of control conditions. We introduce several new estimators which are based on <b>local</b> polynomial <b>regression.</b> Through a simulation study, we find that there is no advantage to using <b>local</b> linear <b>regression</b> over <b>local</b> constant <b>regression.</b> As far as we know, this is an unusual situation – in most circumstances until now, <b>local</b> linear <b>regression</b> has been found to outperform <b>local</b> constant <b>regression...</b>|$|R
40|$|This thesis {{examines}} <b>local</b> polynomial <b>regression.</b> <b>Local</b> polynomial <b>regression</b> {{is one of}} non-parametric {{approach of}} data fitting. This particular method is based on repetition of fitting data using weighted least squares estimate of {{the parameters of the}} polynomial model. The aim of this thesis is therefore revision of some properties of the weighted least squares estimate used in linear regression model and introduction of the non-robust method of <b>local</b> polynomial <b>regression.</b> Some statistical properties of the <b>local</b> polynomial <b>regression</b> estimate are derived. Conditional bias and conditional variance of the <b>local</b> polynomial <b>regression</b> estimate are then approximated using Monte Carlo method and compared with theoretical results. Powered by TCPDF (www. tcpdf. org...|$|R
5000|$|... #Caption: Evolution {{of voting}} {{intentions}} for the 2008 Quebec general election. Dots are individual poll results and trend lines are <b>local</b> <b>regressions</b> with 95% confidence interval.|$|R
50|$|Chronux is {{organized}} {{into a number}} of distinct toolboxes. These include the spectral analysis toolbox, the <b>local</b> <b>regression</b> and likelihood toolbox, and the spike-sorting toolbox. In addition, a number of domain-specific GUIs are part of the Chronux package and more are envisaged. Much of Chronux is written in MATLAB with certain intensive computations being coded in C with a MEX interface to MATLAB. The methods employed are state-of-the-art: For example, the spectral analysis toolbox implements the multitaper spectral estimation method and the <b>local</b> <b>regression</b> and Likelihood toolbox (Locfit) implements a set of highly flexible methods for fitting functions and probability distributions to data. Chronux provides robust estimates of the confidence intervals on computed quantities. Thus, the computation of a spectrum can be augmented by a computation of both asymptotic and jackknife based confidence intervals and {{the same is true of}} most quantities in the spectral analysis toolbox. Similarly, the <b>local</b> <b>regression</b> and likelihood toolbox is a MEX front-end to the Locfit package which provides a comprehensive set of tools for model testing and validation.|$|E
50|$|The term 'Computational statistics' {{may also}} be used to refer to {{computationally}} intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, <b>local</b> <b>regression,</b> kernel density estimation, artificial neural networks and generalized additive models.|$|E
50|$|William Swain Cleveland II (born 1943) is an American {{computer}} scientist and Professor of Statistics and Professor of Computer Science at Purdue University, {{known for his}} work on data visualization, particularly on nonparametric regression and <b>local</b> <b>regression.</b>|$|E
40|$|Key words: {{algorithmic}} stability; transductive learning; cost stability; <b>local</b> transductive <b>regression</b> algorithm; reproducing kernel Hilbert space; pseudo-target. Abstract. In this paper, {{the stability}} of <b>local</b> transductive <b>regression</b> algorithms is studied by adopting a strategy which adjusts the sample set by removing one or two elements from it. A sufficient condition for uniform stability is given. The result of our work shows that if a <b>local</b> transductive <b>regression</b> algorithm uses square loss, and if for any x, a kernel function K(x, x) has a limited upper bound, then the <b>local</b> transductive <b>regression</b> algorithm which minimizes the standard form will have good uniform stability...|$|R
40|$|<b>Local</b> {{polynomial}} <b>regression</b> is {{a useful}} non-parametric regression tool to explore fine data structures and has been widely used in practice. We propose a new non-parametric regression technique called "local composite quantile regression smoothing" to improve <b>local</b> polynomial <b>regression</b> further. Sampling properties of the estimation procedure proposed are studied. We derive the asymptotic bias, variance and normality of the estimate proposed. The asymptotic relative efficiency of the estimate with respect to <b>local</b> polynomial <b>regression</b> is investigated. It is shown that the estimate can be much more efficient than the <b>local</b> polynomial <b>regression</b> estimate for various non-normal errors, while being almost as efficient as the <b>local</b> polynomial <b>regression</b> estimate for normal errors. Simulation is conducted to examine {{the performance of the}} estimates proposed. The simulation results are consistent with our theoretical findings. A real data example is used to illustrate the method proposed. Copyright (c) 2010 Royal Statistical Society. ...|$|R
40|$|There {{has been}} much justifiable recent {{interest}} in <b>local</b> polynomial <b>regression,</b> and in particular in its local linear special case. <b>Local</b> linear <b>regression</b> has advantages in terms of desirable theoretical properties both in the interior and near {{the boundaries of the}} region of interest. For implementation, binning is useful. In this paper, we describe a variation on <b>local</b> linear <b>regression</b> which can be considered an alternative binning thereof. We show that existing and novel methods are almost indistinguishable. The point of the paper is not to extol the virtues of the new version over the old, but rather (i) to show that the good properties of <b>local</b> linear <b>regression</b> can be achieved in more than one way, and (ii) to elucidate close links between <b>local</b> linear <b>regression</b> and other kernel smoothing methods. The latter include, most closely, a boundary corrected ‘naive’ kernel estimator and a recent proposal of Wu and Chu (1992), as well as binned Nadaraya–Watson estimators and methods for binomial regression...|$|R
5000|$|... #Caption: Global mean surface-temperature {{change from}} 1880 to 2016, {{relative}} to the 1951-1980 mean. The black line is the global annual mean, and the red line is the five-year <b>local</b> <b>regression</b> line. The blue uncertainty bars show a 95% confidence interval.|$|E
50|$|Comparing two {{different}} arrays, or {{two different}} samples hybridized {{to the same}} array generally involves making adjustments for systematic errors introduced by differences in procedures and dye intensity effects. Dye normalization for two color arrays is often achieved by <b>local</b> <b>regression.</b> LIMMA provides a set of tools for background correction and scaling, {{as well as an}} option to average on-slide duplicate spots. A common method for evaluating how well normalized an array is, is to plot an MA plot of the data.|$|E
50|$|Chronux is an {{open-source}} {{software package}} {{developed for the}} loading, visualization and analysis {{of a variety of}} modalities / formats of neurobiological time series data. Usage of this tool enables neuroscientists to perform a variety of analysis on multichannel electrophysiological data such as LFP (local field potentials), EEG, MEG, Neuronal spike times and also on spatiotemporal data such as FMRI and dynamic optical imaging data. The software consists of a set of MATLAB routines interfaced with C libraries {{that can be used to}} perform the tasks that constitute a typical study of neurobiological data. These include <b>local</b> <b>regression</b> and smoothing, spike sorting and spectral analysis - including multitaper spectral analysis, a powerful nonparametric method to estimate power spectrum. The package also includes some GUIs for time series visualization and analysis. Chronux is GNU GPL v2 licensed (and MATLAB is proprietary).|$|E
40|$|<b>Local</b> linear <b>regression</b> is {{commonly}} used in practice because of its excellent numerical and theoretical properties. It involves fitting a straight line segment over a small region whose midpoint is the target point x, and the local linear estimate at x is the estimated intercept of that straight line segment. Local linear estimator has an asymptotic bias of order h 2 and variance of order (nh) − 1 with h the bandwidth. In this paper, we propose a new estimator, the double-smoothing local linear estimator, which is constructed by integrally combining all fitted values at x of local lines in its neighborhood with another round of smoothing. In contrast to using only an intercept in the conventional <b>local</b> linear <b>regression,</b> the proposed estimator attempts {{to make use of}} all information obtained from fitting local lines. Without changing the order of variance, the new estimator can reduce the bias to an order of h 4. Compared to <b>local</b> linear <b>regression,</b> the proposed estimator has better performance in situations with considerable bias effects; compared to <b>local</b> cubic <b>regression,</b> with the same convergence rate for the asymptotic bias and variance, the new estimator has less variability and more easily overcomes the sparse data problem because the design matrix is used only in the first step of smoothing. At boundary points, the proposed estimator is comparable to <b>local</b> linear <b>regression.</b> Simulation results and a real data i example further substantiate that double-smoothing <b>local</b> linear <b>regression</b> often has better overall performance than <b>local</b> linear <b>regression...</b>|$|R
40|$|A popular color {{management}} {{standard for}} controlling color reproduction is the ICC color profile. The {{core of the}} ICC profile is a look-up-table which maps a regular grid of deviceindependent colors to the printer colorspace. To estimate the look-up-table from sample input-output colors, <b>local</b> linear <b>regression</b> {{has been shown to}} work better than other methods. An open problem in <b>local</b> linear <b>regression</b> is how to define the locality or neighborhood for each of the <b>local</b> linear <b>regressions.</b> In this paper, new adaptive neighborhood definitions and regularized <b>local</b> linear <b>regression</b> are proposed to address this problem. The adaptive neighborhood definitions enclose the test sample, and are motivated by a result showing they yield bounded estimation variance. An experiment shows that both regularization and the proposed neighborhoods can lead to a significant reduction in error. Index Terms — color management, linear regression, regularization, printers 1...|$|R
40|$|Copyright 2002 IEEEIn this paper, {{we present}} a novel nonparametric {{algorithm}} for short term electricity demand forecasting. The algorithm is based on <b>local</b> linear <b>regression</b> using sliding window with variable length. The method for selecting optimal window length for each local fit offers close insight into trade-off between bias and standard deviation of <b>local</b> <b>regressions.</b> Optimal window length is selected for each value in the load time-series: large window for linear change of load to reduce variability and small window when load departs from linear function to control bias. In the presented algorithm <b>local</b> linear <b>regression</b> is used to estimate trend component of the load time series and to forecast trend component by extrapolating with the fitted local linear function. Some features of the algorithm are demonstrated in the paper using examples from the historic load data recorded in the Namibian Power Utility...|$|R
40|$|This paper gives a brief {{review of}} {{clustering}} and <b>local</b> <b>regression</b> techniques; we are mainly focused on its implementation to software engineering data and we present an example of the preliminary results using clustering and <b>local</b> <b>regression.</b> The clustering, and <b>local</b> <b>regression</b> are part of the data processing of a project called Analysis of Softwar...|$|E
30|$|Robust <b>Local</b> <b>Regression</b> (LRR): The <b>local</b> <b>regression</b> {{version was}} {{vulnerable}} to outliers {{that could be}} caused by heavy tailed distribution. In {{order to make a}} robust solution modification was proposed by adding the robust estimation method called bi-square which transformed LR onto an iterative method.|$|E
40|$|<b>Local</b> <b>regression</b> or loess {{has become}} a popular method for {{smoothing}} scatterplots and for nonparametric regression in general. The final result is a "smoothed" version of the data. In order to obtain {{the value of the}} smooth estimate associated with a given covariate, a polynomial, usually a line, is fitted locally using weighted least squares. In general the weighted least square estimates of the parameters that define the fitted polynomial are not used. In this paper we will present a version of <b>local</b> <b>regression,</b> that fits more general parametric functions. In certain cases, the fitted parameters may be interpreted in some way and we call them meaningful parameters. Examples are presented showing how this procedure is useful for physiological, epidemiological, signal processing, and financial data. KEY WORDS: <b>Local</b> <b>Regression,</b> Harmonic Model, Meaningful Parameters, Circadian Patterns, Sound Analysis. 1 Introduction <b>Local</b> <b>regression</b> estimation is a method for smoothing scatterplots, (x [...] ...|$|E
5000|$|... #Caption: Generated in R {{directly}} from {{data in the}} table below. Trendlines are <b>local</b> <b>regressions</b> calculated from the 25 nearest polls, weighted by proximity in time and margin of error. 95% confidence ribbons represent uncertainty about the regressions, not the likelihood that actual election results would fall within the intervals. -- Source code for plot generation is available here.|$|R
40|$|This paper studies {{improvements}} of multivariate <b>local</b> linear <b>regression.</b> Two intuitively appealing variance reduction {{techniques are}} proposed. They both yield estimators that retain the same asymptotic conditional bias as the multivariate local linear estimator and have smaller asymptotic conditional variances. The estimators are further examined in aspects of bandwidth selection, {{asymptotic relative efficiency}} and implementation. Their asymptotic relative efficiencies {{with respect to the}} multivariate local linear estimator are very attractive and increase exponentially as the number of covariates increases. Data-driven bandwidth selection procedures for the new estimators are straightforward given those for <b>local</b> linear <b>regression.</b> Since the proposed estimators each has a simple form, implementation is easy and requires much less or about the same amount of effort. In addition, boundary corrections are automatic as in the usual multivariate <b>local</b> linear <b>regression.</b> Bandwidth selection Kernel smoothing <b>Local</b> linear <b>regression</b> Multiple regression Nonparametric regression Variance reduction...|$|R
30|$|However, <b>local</b> linear <b>regression</b> {{methods are}} very {{sensitive}} to outliers, and individual outliers can lead to large changes in the results of statistical inference, therefore leading to irrational and even erroneous conclusions. Such statistical methods like the <b>local</b> linear <b>regression</b> approach are not strong enough to adapt the complex changing reality, in other words, the <b>local</b> linear <b>regression</b> method is not robust when it comes to outliers or heavy-tailed distributions. In recent years, various robust methods have been proposed for abnormal observation, which has become increasingly crucial and frequent in many research fields. Reference [20] defined maximum likelihood type robust estimates of regression and investigated the asymptotic properties. From then on, the maximum likelihood type robust estimation (M-estimation) has been discussed by many authors, for example, [21, 22] and the references therein. Meanwhile, some modified maximum likelihood type estimators were developed, such as the local maximum likelihood type estimator (local M-estimators), which is a combination of the <b>local</b> linear <b>regression</b> and the M-estimation regression, so the nice properties of local linear estimator and M-estimator persist. For instance, [23] constructed variable bandwidth local linear M-estimator for a regression function. Reference [24] proposed a nonparametric estimator of the regression function by combining <b>local</b> polynomial <b>regression</b> and M-estimation regression. Reference [25] developed robust version of <b>local</b> linear <b>regression</b> smoothers for stationary time series sequence. Reference [26] considered local M-estimation of the unknown drift and diffusion functions of integrated diffusion processes.|$|R
40|$|This paper {{describes}} {{the application of}} <b>local</b> <b>regression</b> trees to an environmental regression task. This task {{was part of the}} 3 rd Internation ERUDIT Competition. The technique described in this paper was announced as one of the three runner-up methods by the jury of the competition. We briefly describe RT, a system that is able to generate <b>local</b> <b>regression</b> trees. We emphasise the multi-strategy features of RT that we claim as being one of the causes for the obtained performance. We described the pre-processing steps that were taken in order to apply RT to the competition data, highlighting the weighed schema that was used to combine the predictions of the best RT variants. We conclude by reinforcing the idea that the combination of features of different data analysis techniques can be useful to obtain higher predictive accuracy. KEYWORDS: Regression trees, local modelling, hybrid methods, <b>local</b> <b>regression</b> trees. INTRODUCTION This paper describes an application of <b>local</b> <b>regression</b> [...] ...|$|E
40|$|Moving <b>local</b> <b>regression</b> is a nonparametric {{technique}} for smoothing, interpolating and forecasting {{by means of}} locally fitted regression models. The paper explores the "optimal" structure of the weight function, {{taking into account the}} location of supporting points and the suspected behaviour of the remainder term, and surveys results on choice of weight functions in traditional moving <b>local</b> <b>regression</b> approaches. (author's abstract) Series: Forschungsberichte / Institut für Statisti...|$|E
40|$|In {{this study}} we present two novel {{normalization}} schemes for cDNA microarrays. They are based on iterative <b>local</b> <b>regression</b> and optimization of model parameters by generalized cross-validation. Permutation tests assessing the efficiency of normalization demonstrated that the proposed schemes have an improved ability to remove systematic errors and to reduce variability in microarray data. The analysis also reveals that without parameter optimization <b>local</b> <b>regression</b> is frequently insufficient to remove systematic errors in microarray data...|$|E
40|$|Abstract—In general, {{irrelevant}} {{features of}} high-dimensional data will degrade {{the performance of}} an inference system, e. g., a clustering algorithm or a classifier. In this paper, we therefore present a <b>Local</b> Kernel <b>Regression</b> (LKR) scoring approach to evaluate the relevancy of features based on their capabilities of keeping the local configuration in {{a small patch of}} data. Accordingly, a score index featuring applicability to both of supervised learning and unsupervised learning is developed to identify the relevant features within the framework of <b>local</b> kernel <b>regression.</b> Experimental results show the efficacy of the proposed approach in comparison with the existing methods. Index Terms—Relevant features, feature selection, <b>local</b> kernel <b>regression</b> score, high-dimensional data. ...|$|R
40|$|The aim of {{this work}} is to {{introduce}} a new nonparametric regression technique {{in the context of}} functional covariate and scalar response. We propose a <b>local</b> linear <b>regression</b> estimator and study its asymptotic behaviour. Its finite-sample performance is compared with a Nadayara-Watson type kernel regression estimator and with the linear regression estimator via a Monte Carlo study and the analysis of two real data sets. In all the scenarios considered, the <b>local</b> linear <b>regression</b> estimator performs better than the kernel one, {{in the sense that the}} mean squared prediction error is lower. 62 G 08 62 G 30 Functional data Nonparametric smoothing <b>Local</b> linear <b>regression</b> Kernel regression Fourier expansion Cross-validation...|$|R
5000|$|... #Caption: Evolution {{of voting}} {{intentions}} in the pre-campaign {{period of the}} 43rd Canadian federal election. Trendlines are <b>local</b> <b>regressions</b> calculated from the 25 nearest polls, weighted by proximity in time and margin of error. 95% confidence ribbons represent uncertainty about the regressions, not the likelihood that actual election results would fall within the intervals. -- Source code for plot generation is available here.|$|R
40|$|This paper empirically {{demonstrates}} {{the relative merits}} of the optimal choice of the weight function in a moving <b>local</b> <b>regression</b> as suggested by Fedorov et al., (1993) over traditional weight functions which ignore the form of the local model. The discussion is based on a task that is imbedded into the smoothing methodology, namely the forecasting of business time series data {{with the help of a}} one-sided moving <b>local</b> <b>regression</b> model. (author's abstract) Series: Forschungsberichte / Institut für Statisti...|$|E
30|$|<b>Local</b> <b>Regression</b> (LR): LR {{builds a}} curve that approximates {{original}} data {{by setting up}} the sample data models to localized subset of data.|$|E
40|$|We {{describe}} and test three methods {{to estimate the}} remaining time between a series of microtexts (tweets) and the future event they refer to via a hashtag. Our system generates hourly forecasts. A linear and a local regression-based approach are applied to map hourly clusters of tweets directly onto time-to-event. To take changes over time into account, we develop a novel time series analysis approach that fi rst derives word frequency time series from sets of tweets and then performs <b>local</b> <b>regression</b> to predict time-to-event from nearest-neighbor time series. We train and test on a single type of event, Dutch premier league football matches. Our results indicate that in an 'early' stage, four days or more before the event, the time series analysis produces time-to-event predictions that are about one day off; closer to the event, <b>local</b> <b>regression</b> attains a similar accuracy. <b>Local</b> <b>regression</b> also outperforms both mean and median-based baselines, but on average none of the tested system has a consistently strong performance through time...|$|E
40|$|We {{consider}} local polynomial {{fitting for}} estimating a regression function and its derivatives nonparametrically. This method possesses many nice features, among which automatic {{adaptation to the}} boundary and adaptation to various designs. A first contribution {{of this paper is}} the derivation of an optimal kernel for <b>local</b> polynomial <b>regression,</b> revealing that there is a universal optimal weighting scheme. Fan (1993, Ann. Statist., 21, 196 - 216) showed that the univariate <b>local</b> linear <b>regression</b> estimator is the best linear smoother, meaning that it attains the asymptotic linear minimax risk. Moreover, this smoother has high minimax risk. We show that this property also holds for the multivariate <b>local</b> linear <b>regression</b> estimator. In the univariate case we investigate minimax efficiency of <b>local</b> polynomial <b>regression</b> estimators, and find that the asymptotic minimax efficiency for commonly-used orders of fit is 100 % among the class of all linear smoothers. Further, we quantify the loss in efficiency when going beyond this class. status: publishe...|$|R
40|$|This paper {{describes}} theoretical {{properties of}} {{a new type of}} model-assisted nonparametric regression estimator for the finite population total, based on local poly- <b>LOCAL</b> POLYNOMIAL <b>REGRESSION</b> ESTIMATORS 3 nomial smoothing. <b>Local</b> polynomial <b>regression</b> is a generalization of kernel regression. Cleveland (1979) and Cleveland and Devlin (1988) showed that these techniques are applicable {{to a wide range of}} problems. Theoretical work by Fan (1992, 1993) and Ruppert and Wand (1994) showed that it has many desirable theoretical properties, including adaptation to the design of the covariate(s), consistency and asymptotic unbiasedness. Wand and Jones (1995) provide a clear explanation of the asymptotic theory for kernel <b>regression</b> and <b>local</b> polynomial <b>regression.</b> The monograph by Fan and Gijbels (1996) explores a wide range of application areas of <b>local</b> polynomial <b>regression</b> techniques. However, the application of these techniques to model-assisted survey sampling is new. In Section 1. 2 we introduce the <b>local</b> polynomial <b>regression</b> estimator and in Section 1. 3 we state assumptions used in the theoretical derivations of Section 2, in which our main results are described. Section 2. 1 shows that the estimator is a weighted linear combination of study variables in which the weights are calibrated to known control totals. Section 2. 2 contains a proof that the estimator is asymptotically design unbiased and design consistent, and Section 2. 3 provides an approximation to its mean squared error and a consistent estimator of the mean squared error. Section 2. 4 provides su#cient conditions for asymptotic normality of the <b>local</b> polynomial <b>regression</b> estimator and establishes a central limit theorem in the case of simple random sampling. We show that the estimator is robust in the sense o [...] ...|$|R
40|$|This paper {{presents}} the bandwidth selection methods for <b>local</b> polynomial <b>regression</b> with Normal, Epanechnikov, and Uniform kernel function. The bandwidth selection methods are proposed by Histogram Bin Width method, Bandwidth for Kernel Density Estimation method, and Bandwidth for <b>Local</b> Linear <b>Regression</b> method {{to estimate the}} <b>local</b> polynomial <b>regression</b> estimator. Using Monte Carlo simulations, we compare the Mean Square Error (MSE) of the bandwidth selection methods. For simulation results, {{it can be seen}} that the MSE of Bandwidth for Kernel Density Estimation method provides the smallest in all situations. The bandwidth selection methods are applied to the Stock Exchange of Thailand (SET) index. The results show that the MSE of Kernel Density Estimation method with Normal kernel function is the smallest as the simulation study...|$|R
