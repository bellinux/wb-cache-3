12|2|Public
50|$|Always {{interested}} in how machines could better emulate human brain functions, he postulated Patom theory - the word representing a combination of pattern matching and atom. This reflected his belief that the brain simply stores, matches and uses hierarchical, bidirectional <b>linkset</b> patterns (sequences and sets) as sufficient to explain human capabilities. This he claimed was {{the approach of the}} human brain to language and vision and was first publicly aired in 2000, on Robyn Williams’ Okham’s Razor.|$|E
40|$|Abstract The <b>linkset</b> {{model is}} defined for parametrizing the general 2 ^n {{contingency}} table. The <b>linkset</b> parameters {{are designed to}} represent latent influences that promote the co-occurrences of binary events beyond that explained by chance. Linkages involving 2 through n binary variables are included in this parametrization. The intent {{of this process is}} to elucidate the patterns of linkage, no matter how complex they might be, rather than to fit simplifying models. The relationship between <b>linkset</b> parameters and the natural parameters for a 2 n table are derived, and large sample inference methods are provided. Examples are given from medical diagnostics, survival from the Titanic sinking, and employment discrimination in Chicago...|$|E
40|$|Linked data best {{practices}} are getting extremely popular: various companies and public institutions have started {{taking advantage of}} linked data principles for exposing their datasets, and for relating their datasets to those served by third parties. Such enthusiasm {{is due to the}} linked data promise of evolving into a Global Data Space. Linksets are sets of links relating datasets and they surely play a fundamental role in this promise. However, a stable and wellaccepted notion of <b>linkset</b> quality has not been yet defined. This paper contributes to overcome this lack by proposing a <b>linkset</b> quality measure. Among the different quality dimensions that can be addressed, the proposed measure focuses on completeness. The paper formally defines novel scoring functions and proposes an interpretation of these functions when maintaining and complementing third party datasets...|$|E
40|$|Abstract. Biological {{interactions}} such {{as those}} between genes and pro-teins are complex and require intricate OWL models. However, direct links between biological entities can support search and data integra-tion. In this paper we introduce <b>linksets</b> of convenience that capture these direct links. We show the provenance statements required to track the derivation of such linksets; linking {{them back to the}} full biological justification...|$|R
40|$|Abstract. The Web of Data {{is built}} upon two simple ideas: Employ the RDF data model to publish {{structured}} {{data on the}} Web and to create explicit data links between entities within different data sources. This pa-per presents the Silk – Linking Framework, a toolkit for discovering and maintaining data links between Web data sources. Silk consists of three components: 1. A link discovery engine, which computes links between data sources based on a declarative specification of the conditions that entities must fulfill {{in order to be}} interlinked; 2. A tool for evaluating the generated data links in order to fine-tune the linking specification; 3. A protocol for maintaining data links between continuously changing data sources. The protocol allows data sources to exchange both <b>linksets</b> as well as detailed change information and enables continuous link recom-putation. The interplay of all the components is demonstrated within a life science use case. Key words: Linked data, web of data, link discovery, link maintenance, record linkage, duplicate detection...|$|R
40|$|Abstract. Linked Data {{is largely}} adopted {{to share and}} make data more {{accessible}} on the web. A quite impressive number of datasets has been exposed and interlinked according to the Linked Data paradigm but the quality of these datasets is still a big challenge in the consuming process. Measures for quality of linked data datasets have been proposed, mainly by adapting concepts defined in the research field of information sys-tems. However, very limited {{attention has been dedicated}} to the quality of linksets, the result of which might be important as dataset’s quality in consuming data coming from distinct sources. In this paper, we address <b>linkset</b> quality proposing the <b>linkset</b> importing, a novel measure which estimates the completeness of dataset obtained by complementing SKOS thesauri with their skos:exactMatch-related information. We validate the proposed measure with an in-house developed synthetic benchmark: ex-periments demonstrate that our measure may be adopted as a predictor of the gain that is obtained when complementing thesauri. ...|$|E
40|$|Query {{processing}} is {{an important}} way of accessing data on the Semantic Web. Today, the Semantic Web is characterized as a web of interlinked datasets, and thus querying the web {{can be seen as}} dataset integration on the web. Also, this dataset integration must be transparent from the data consumer as if she is querying the whole web. To decide which datasets should be selected and integrated for a query, one requires a metadata of the web of data. In this paper, to enable this transparency, we introduce a federated query engine called WoDQA (Web of Data Query Analyzer) which discovers datasets relevant with a query in an automated manner using VOID documents as metadata. WoDQA focuses on powerful dataset elimination by analyzing query structure with respect to the metadata of datasets. Dataset and <b>linkset</b> descriptions in VOID documents are analyzed for a SPARQL query and a federated query is constructed. By means of <b>linkset</b> concept of VOID, links between datasets are incorporated into selection of federated data sources. Current version of WoDQA is available as a SPARQL endpoint. 1...|$|E
40|$|Abstract. The Linked Data {{initiative}} {{promotes the}} publication of previously isolated databases as interlinked RDF datasets, thereby creating a global scale data space. Datasets are frequently interlinked using some automated matching process that results in a materialized sameAs <b>linkset,</b> that is, a set of links of the form (s, owl:sameAs, o), which asserts that s denotes the same resource as o. This paper proposes strategies to reduce the cognitive overhead of creating ma-terialized sameAs linksets and to correctly maintain them. The paper also out-lines an architecture to improve the support for materialized sameAs linksets...|$|E
40|$|The {{main idea}} behind Linked Data is to connect data from {{different}} sources together, {{in order to}} develop a hub of shared and publicly accessible knowledge. While the benefit of sharing knowledge is universally recognised, what is less visible is how much results can be affected when the knowledge in one dataset and in the connected ones are not equally distributed. This lack of balance in information, or bias, generally assumed a priori, can actually be quantified {{to improve the quality of}} the results of applications and analytics relying on such linked data. In this paper, we propose a process to measure how much bias one dataset contains when compared to another one, by identifying the most affected RDF properties and values within the set of entities that those datasets have in common (defined as the <b>linkset).</b> This process was ran on a wide range of linksets from Linked Data, and in the experiment section we present the results as well as measures of its performance...|$|E
40|$|Discovering {{links between}} {{overlapping}} datasets on the Web is generally realised {{through the use}} of fuzzy similarity measures. Configuring such measures is often a non-trivial task that depends on the domain, ontological schemas, and formatting conventions in data. Existing solutions either rely on the user's knowledge of the data and the domain or on the use of machine learning to discover these parameters based on training data. In this paper, we present a novel approach to tackle the issue of data linking which relies on the unsupervised discovery of the required similarity parameters. Instead of using labeled data, the method takes into account several desired properties which the distribution of output similarity values should satisfy. The method includes these features into a fitness criterion used in a genetic algorithm to establish similarity parameters that maximise the quality of the resulting <b>linkset</b> according to the considered properties. We show in experiments using benchmarks as well as real-world datasets that such an unsupervised method can reach the same levels of performance as manually engineered methods, and how the different parameters of the genetic algorithm and the fitness criterion affect the results for different datasets...|$|E
40|$|The {{objective}} of this work is to perform simulation modeling of the Signaling System No. 7 (SS 7) network with particular emphasis on modeling of the Message Transfer Part Level 2. The basics of common channel signaling using Signaling System No. 7 is initially outlined and discussed {{with reference to the}} ITU-T Q. 7 xx-Series Recommendations. This includes the protocol stack, signaling points, signaling links and typical network structure. In particular, the functionality of the Message Transfer Part, which provides the main mechanism to convey signaling messages, is discussed in detail. Subsequently the modeling of the Message Transfer Part, in particular MTP level 2, using the simulation tool OPNET from MIL 3. Inc. is presented. The model uses a multi-layer modular approach, with each layer corresponding to the SS 7 layer it is modeling. The functional blocks within each layer are thought of as processes. With their buffers and processors, these processes form a complex interlinked queuing model that is complicated to analyze but is readily simulated. In order to illustrate the use of the simulation model, the basic <b>linkset</b> delay between two signaling points under a heavy traffic load is simulated and compared with analysis based on M/G/ 1 queuing models[URL] University of Singapore author (civilian) ...|$|E
40|$|Andriy Nikolov and Mathieu d’Aquin and Enrico MottaAbstract. As {{commonly}} accepted identifiers {{for data}} instances in semantic datasets (such as ISBN codes or DOI identifiers) {{are often not}} available, discovering links between overlapping datasets on the Web is generally realised {{through the use of}} fuzzy similarity measures. Configuring such measures, i. e. deciding which similarity function to apply to which data properties with which parameters, is often a non-trivial task that depends on the domain, ontological schemas, and formatting conventions in data. Existing solutions either rely on the user’s knowledge of the data and the domain or on the use of machine learning to discover these parameters based on training data. In this report, we present a novel approach to tackle the issue of data linking which relies on the unsupervised discovery of the required similarity parameters. Instead of using labeled training data, the method takes into account several desired properties which the distribution of output similarity values should satisfy. The method includes these features into a fitness criterion used in a genetic algorithm to establish similarity parameters that maximise the quality of the resulting <b>linkset</b> according to the considered properties. We show in experiments using benchmarks as well as real-world datasets that such an unsupervised method can reach the same levels of performance as manually engineered methods, and how the different parameters of the genetic algorithm and the fitness criterion affect the results for different datasets. ...|$|E
40|$|In current days, {{separate}} compilation plays a {{very important}} role in the practical program development. As the name “separate compilation ” indicates, it allows that the program is split into several source files. Each of them is self-contained in the sense it describes the import signature which are assumed to be provided by other code fragment and its own export declarations. Each unit can be compiled independently and then linked together as a whole. [1] proposes such an simple but elegant framework to describe the semantics of separate compilation and linking. Given a code fragment with the explicit import interface called <b>linkset,</b> it is compiled to a self-contained entity called intra-checking and then compatible modules can be safely linked together called inter-checking. As for the SML language, many SML compilers support the so-called incremental recompilation system such as the SML/NJ [2] and MLKit[3]. In these compiler systems, the compilation of the source files must be compiled incrementally. The interface of each source file is inferred automatically by the compiler without the interference of programmer. But a weak point of such strategy is that compiler always needs to know the implementation which imposes a really inconvenient situation in practical development. Imagine the following scenario: A and B cooperates to develop a software system and B depends on A. Without the implementation of A, B cannot do any real development. This is a serious obstacle to apply SML language into large scale software development. The ML dialect, Ocaml proposes a solution for cut-off separate compilation[4]. In Objective Caml, compilation units are regarded as a basic unit to be independently compiled and linked later. And each can be seen as special cases of structures and signatures, and the relationship between the units can be explained easily in terms of the ML module system. We use the example given in [4] to illustrate. A compilation unit A comprises two files: implementation file A. ml, consisting a sequence of definitions, analogous to the inside of a struct [...] . end construct; interface file A. mli, which contains a sequence of specifications, analogous to the inside of a sig [...] . end construct. Both files define a structure named A as if the following definition was entered at top-level as follows: module A: sig (* contents of file A. mli *) end...|$|E

