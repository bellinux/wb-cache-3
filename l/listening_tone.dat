0|65|Public
6000|$|Alfred <b>listened</b> for the <b>tone</b> of his mother's voice; he knew {{he should}} judge by that, even without {{catching}} the words--low, subdued, sad--he almost thought she began with 'Yes.' ...|$|R
50|$|Duffy {{was born}} in Birmingham {{and grew up in}} Rednal in the south of the city, {{attending}} St. Thomas Aquinas Catholic School in King's Norton, and growing up <b>listening</b> to 2 <b>Tone,</b> punk, The Beatles and Led Zeppelin.|$|R
50|$|Tappers Harker (Long Eaton, Nottingham): {{a railway}} worker who <b>listened</b> to the <b>tone</b> of a hammer being hit onto a railway wagon wheel, to check its soundness. Similar to the Wheeltappers and Shunters fictional pub of the 1970s show.|$|R
50|$|FMRIs {{have been}} used to measure the {{correlation}} between <b>listening</b> to alternating <b>tones</b> compared to single stream of tones. The posterior regions of the left auditory cortex were modulated by the alternating tones, indicating that there may be areas of the brains responsible for stream segregation.|$|R
6000|$|... "Why, in {{a minute}} we shall go in and join them. Mark the affection, almost maternal, that will well up in Aunt Mildred's eyes. <b>Listen</b> to the <b>tones</b> of Uncle Robert's voice when he says, 'Well, Chris, my boy?' Watch Mrs. Grantly melt, {{literally}} melt, like a dewdrop in the sun.|$|R
50|$|Gro Shetelig at The Norwegian Academy of Music {{is working}} on the {{development}} of a Microtonal Ear Training method for singers and has developed the software Micropalette, a tool for <b>listening</b> to microtonal <b>tones,</b> chords and intervals. Aaron Hunt at Hi Pi instruments has developed Xentone, another tool for microtonal ear training.|$|R
40|$|Includes bibliographical {{references}} (pages 77 - 79) Playing {{the piano}} successfully involves combining many different musical elements. A student needs to <b>listen</b> to the <b>tone</b> he produces, have the technical skills to project {{what is on}} the page, and be able to play from the heart, with emotion. (See more in text. ...|$|R
60|$|These were my own {{thoughts}} as I <b>listened</b> to the <b>tones</b> {{of the priest}} as they came, droningly, out of the door, while Nick was exchanging jokes in doubtful French with some half-breeds leaning against the palings. Then we heard benches scraping on the floor, and the congregation began to file out.|$|R
60|$|He {{understood}} it all {{now as he}} <b>listened</b> to the <b>tone</b> of her voice, {{and looked into the}} lines of her face. There was written there plainly enough that spretæ injuria formæ of which she herself was conscious, but only conscious. Even his eyes, blind as he had been, were opened,--and he knew that he had been a fool.|$|R
50|$|In {{pure tone}} audiometry, an {{audiometer}} {{is used to}} play a series of tones using headphones. The participants <b>listen</b> to the <b>tones</b> which will vary in pitch and loudness. The test will play with the volume controls and the participant is asked to signal {{when he or she}} can no longer hear the tone being played. The testing is completed after listening to a range of pitches. Each ear is tested individually.|$|R
40|$|The present paper {{investigates the}} {{influence}} of the bichord interval between two pure-tones on perceptual simultaneity. In the experiment, 30 participants (musicians and non musicians) <b>listened</b> to <b>tone</b> pairs (bichords: C-D; C-E flat; C-F sharp; C-G) with different degrees of asynchronization and indicated whether two tones had been simultaneous or successive. The method of constant stimuli was used. An ANOVA showed that bichords intervals were perceived with different degrees of simultaneity (p< 0. 0001). Tones that are less distant in frequency are not necessarily easier to perceive as simultaneous. This result suggests that judgement of simultaneity cannot be accounted for only in terms of frequency or temporal relationship between single stimuli at the physical level. The effect of a non temporal variable concerning the perceptual moment hypothesis is discussed...|$|R
40|$|Timing is {{essential}} to the execution of skilled movements, yet our knowledge of the neural systems underlying timekeeping operations is limited. Using whole-brain functional magnetic resonance imaging, subjects were imaged while tapping with their right index finger in synchrony with tones that were separated by constant intervals [Synchronization (S) ], followed by tapping without the benefit of an auditory cue [Continuation (C) ]. Two control conditions followed in which subjects <b>listened</b> to <b>tones</b> and then made pitch discriminations (D). Both the S and the C conditions produced equivalent activation within the left sensorimotor cortex, the right cerebellum (dorsal dentate nucleus), and the right superior temporal gyrus (STG). Only the C condition produced activation of a medial premotor system, including the caudal supplementary motor area (SMA), the left putamen, and the left ventrolateral thalamus. The C conditio...|$|R
40|$|Schizophrenia {{patients}} {{have been shown}} to exhibit subnormal levels of electrophysiological suppression to self-initiated, button press elicited sounds. These self-suppression deficits {{have been shown to}} improve following the imposition of a subsecond delay between the button press and the evoked sound. The current study aimed to investigate whether nonclinical individuals who scored highly on the personality dimension of schizotypy would exhibit similar patterns of self-suppression abnormalities to those exhibited in schizophrenia. Thirty-nine nonclinical individuals scoring above the median (High Schizotypy) and 41 individuals scoring below the median (Low Schizotypy) on the Schizotypal Personality Questionnaire (SPQ) underwent electroencephalographic recording. The amplitude of the N 1 -component was calculated while participants (1) <b>listened</b> to <b>tones</b> initiated by a willed button press and played back with varying delay periods between the button press and the tone (Active conditions) and (2) passively listened to a series of <b>tones</b> (<b>Listen</b> condition). N 1 -suppression was calculated by subtracting the amplitude of the N 1 -component of the auditory evoked potential in the Active condition from that of the Listen condition, while controlling for the activity evoked by the button press per se. The Low Schizotypy group exhibited significantly higher levels of N 1 -suppression to undelayed tones compared to the High Schizotypy group. Furthermore, while N 1 -suppression was found to decrease linearly with increasing delays between the button press and the tone in the Low Schizotypy group, {{this was not the case}} in the High Schizotypy group. The findings of this study suggest that nonclinical, highly schizotypal individuals exhibit subnormal levels of N 1 -suppression to undelayed self-initiated tones and an abnormal pattern of N 1 -suppression to delayed self-initiated tones. To the extent that these results are similar to those previously reported in patients with schizophrenia, these findings provide support for the existence of a neurophysiological "continuum of psychosis"...|$|R
40|$|Audibility of inharmonicity in {{realistic}} {{acoustic guitar}} tones was studied through formal <b>listening</b> experiments. Test <b>tones</b> were synthesized using an FZ-ARMA model of acoustic guitar sounds, which allows accurate {{control of the}} inharmonic partials. Inharmonicity was detected fairly easily for the lowest strings if the plucking transient was {{left out of the}} sound, but much harder for the full sound with transient. Detection thresholds were measured for two pitches at the low range of the guitar. Mean thresholds were close to, though above, typical amounts of inharmonicity in the guitar. Implications to digital sound synthesis are discussed. 1...|$|R
40|$|Neural {{response}} adaptation {{plays an}} important role in perception and cognition. Here, we used electroencephalography to investigate how aging affects the temporal dynamics of neural adaptation in human auditory cortex. Younger (18 – 31 years) and older (51 – 70 years) normal hearing adults <b>listened</b> to <b>tone</b> sequences with varying onset-to-onset intervals. Our results show long-lasting neural adaptation such that the response to a particular tone is a nonlinear function of the extended temporal history of sound events. Most important, aging is associated with multiple changes in auditory cortex; older adults exhibit larger and less variable response magnitudes, a larger dynamic response range, and a reduced sensitivity to temporal context. Computational modeling suggests that reduced adaptation recovery times underlie these changes in the aging auditory cortex and that the extended temporal stimulation has less influence on the neural response to the current sound in older compared with younger individuals. Our human electroencephalography results critically narrow the gap to animal electrophysiology work suggesting a compensatory release from cortical inhibition accompanying hearing loss and aging...|$|R
40|$|The {{sense of}} agency (SoA) refers to {{perceived}} causality of the self, i. e. {{the feeling of}} causing something to happen. The SoA has been probed {{using a variety of}} explicit and implicit measures. Explicit measures include rating scales and questionnaires. Implicit measures, which include sensory attenuation and temporal binding, use perceptual differences between self- and externally generated stimuli as measures of the SoA. In the present study, we investigated whether the different measures tap into the same self-attribution processes by determining whether individual differences on implicit and explicit measures of SoA are correlated. Participants performed tasks in which they triggered tones via key presses (operant condition) or passively <b>listened</b> to <b>tones</b> triggered by a computer (observational condition). We replicated previously reported effects of sensory attenuation and temporal binding. Surprisingly the two implicit measures of SoA were not significantly correlated with each other, nor did they correlate with the explicit measures of SoA. Our results suggest that some explicit and implicit measures of the SoA may tap into different processes...|$|R
5000|$|He has {{described}} his violin style as [...] "slightly playful", {{because of the}} enjoyment he has as he plays and listens to music, hoping to share the same with his audience. In 2012, he said because he covers many vocal songs, he tries to use his violin {{to bring out the}} same human feel and <b>tones,</b> <b>listening</b> to the song for a few days and playing by ear as he practices for the final recording. Rather than creating a [...] "perfect" [...] music, he finds it important to include his personal feelings and emotions to create a story with the music, that his audience identifies as his own.|$|R
5000|$|The game is {{extremely}} simple: <b>listen</b> for the <b>tone,</b> {{look for the}} light, hit the target. The game can be set at varying degrees of difficulty where the targets will stay on {{for as long as}} ten seconds down to as brief as 3/4ths of a second. The goal is for every player to have a positive experience with the game so that they will be encouraged to try again to beat their previous score.Patterns are random and the current, standard game will have only one target light on a tower at a time.The game scores accuracy (how many targets were hit before they [...] "timed out") and reaction time.|$|R
50|$|Fletcher and Munson first {{measured}} equal-loudness contours using headphones (1933). In their study, {{test subjects}} <b>listened</b> to pure <b>tones</b> at various frequencies and over 10 dB increments in stimulus intensity. For each frequency and intensity, the listener also {{listened to a}} reference tone at 1000 Hz. Fletcher and Munson adjusted the reference tone until the listener perceived {{that it was the}} same loudness as the test tone. Loudness, being a psychological quantity, is difficult to measure, so Fletcher and Munson averaged their results over many test subjects to derive reasonable averages. The lowest equal-loudness contour represents the quietest audible tone—the absolute threshold of hearing. The highest contour is the threshold of pain.|$|R
40|$|Consistent {{evidence}} suggests that pitch height may be represented in a spatial format, having both a vertical and an horizontal representation. The spatial representation of pitch height results into response compatibility effects for which high pitch tones are preferentially associated to up-right responses, and low pitch tones are preferentially associated to down-left responses (i. e., the SMARC effect), with the strength of these associations depending on individuals’ musical skills. In this study we investigated whether <b>listening</b> to <b>tones</b> of different pitch affects the representation of external space, as assessed in a visual and haptic line bisection paradigm, in musicians and non musicians. Low and high pitch tones affected the bisection performance in musicians differently, both when pitch was relevant and irrelevant for the task, and in both the visual and the haptic modality. No effect of pitch height was observed on the bisection performance of non musicians. Moreover, our data also show that musicians present a (supramodal) rightward bisection bias in both the visual and the haptic modality, extending previous findings limited to the visual modality, and {{consistent with the idea}} that intense practice with musical notation and bimanual instrument training affects hemispheric lateralization. <br/...|$|R
40|$|Triangular designs {{analyzed}} by contemporary nonmetric scaling procedures can yield estimated scales that are nonlinearly {{related to the}} "true " scale values. In nonmetric unidimensional scaling, observers rate the "difference, " "similarity, " or "dissimilarity" of stimulus pairs, and a subtractive model is used to estimate scale values that will reproduce the rank order of the judgments. For example, subjects could be asked to <b>listen</b> to two <b>tones</b> and judge the "dissimilarity" in loudness. If the subjects report that the "dissimilarity " of A to B exceeds the "dissimilarity" of C to D, then the absolute difference in scale values between A and B is assumed to exceed the difference in sensation between C and D. dij = M (I si- sj I), (2...|$|R
40|$|International audienceLaback et al. [(2011). J. Acoust. Soc. Am. 129, 888 897] {{investigated}} the additivity of nonsimulta- neous masking using short Gaussian-shaped tones as maskers and target. The present study involved Gaussian stimuli {{to measure the}} additivity of simultaneous masking for combinations of up to four spectrally separated maskers. According to most basilar membrane measurements, the maskers should be processed linearly at the characteristic frequency (CF) of the target. Assuming also compression of the target, all masker combinations should produce excess masking (exceeding linear additivity). The results {{for a pair of}} maskers flanking the target indeed showed excess masking. The amount of excess masking could be predicted by a model assuming summation of masker-evoked excitations in intensity units at the target CF and compression of the target, using compressive input/output functions derived from the nonsimultaneous masking study. However, the combinations of lower-frequency maskers showed much less excess masking than predicted by the model. This cannot easily be attributed to factors like off-frequency <b>listening,</b> combination <b>tone</b> perception, or between-masker suppression. It was better predicted, however, by assuming weighted intensity summation of masker excitations. The optimum weights for the lower-frequency maskers were smaller than one, consistent with partial masker compression as indicated by recent psychoacoustic data...|$|R
40|$|Musicians {{are highly}} trained motor experts with {{pronounced}} associations between musical actions {{and the corresponding}} auditory effects. However, the importance of auditory feedback for music performance is controversial, and it is unknown how feedback during music performance is processed. The present study investigated the neural mechanisms underlying the process-ing of auditory feedback manipulations in pianists. To disentangle effects of action-based and perception-based expectations, we compared feedback manipulations during performance to the mere perception of the same stimulus material. In two experi-ments, pianists performedbimanually sequences on a piano,while at random positions, the auditory feedback of single notes was manipulated, thereby creating a mismatch between an expected and actually perceived action effect (action condition). In addition, pianists <b>listened</b> to <b>tone</b> sequences containing the same ma-nipulations (perception condition). The manipulations in the perception condition were either task-relevant (Experiment 1) or task-irrelevant (Experiment 2). In action and perception condi-tions, event-related potentials elicited by manipulated tones showed an early fronto-central negativity around 200 msec, pre-sumably reflecting a feedback ERN/N 200, followed by a positive deflection (P 3 a). The early negativity was more pronounced dur-ing the action compared to the perception condition. This shows that during performance, the intention to produce specific audi-tory effects leads to stronger expectancies than the expectancies built up during music perception. ...|$|R
40|$|International audienceRecent {{evidence}} {{has reported that}} the motor system has a role in speech or emotional vocalization discrimination. In the present study we investigated {{the involvement of the}} larynx motor representation in singing perception. Twenty-one non-musicians <b>listened</b> to short <b>tones</b> sung by a human voice or played by a machine and performed a categorization task. Thereafter continuous theta-burst transcranial magnetic stimulation was applied over the right larynx premotor area or on the vertex and the test administered again. Overall, reaction times (RTs) were shorter after stimulation over both sites. Nonetheless and most importantly, RTs became longer for sung than for “machine” sounds after stimulation on the larynx area. This effect suggests that the right premotor region is functionally involved in singing perception and that sound humanness modulates motor resonance...|$|R
500|$|Some {{experiments}} give subjects a [...] "distractor" [...] task {{to ensure}} that subjects are not consciously {{paying attention to the}} experimental stimuli; this may be done to test whether a certain computation in the brain is carried out automatically, regardless of whether the subject devotes attentional resources to it. [...] For example, one study had subjects <b>listen</b> to non-linguistic <b>tones</b> (long beeps and buzzes) in one ear and speech in the other ear, and instructed subjects to press a button when they perceived a change in the tone; this supposedly caused subjects not to pay explicit attention to grammatical violations in the speech stimuli. [...] The subjects showed a mismatch response (MMN) anyway, suggesting that the processing of the grammatical errors was happening automatically, regardless of attentionor at least that subjects were unable to consciously separate their attention from the speech stimuli.|$|R
40|$|We {{recorded}} the auditory evoked magnetic ¢elds from children {{with and without}} absolute pitch under the following conditions: (a) hearing 1000 Hz pure tones inattentively, (b) hearing eight ran-dom tones inattentively and (c) <b>listening</b> to eightrandom <b>tones</b> and identifying each tone. We calculated the appearance rate ofN 100 m as theratio of the subjectswhohadN 100 m. Therewas a signi¢cant positive correlation between the appearance rate of N 100 m and age inboth groups. Therewas also a signi¢cantpositive correlation between the appearance rate of N 100 m {{and the kinds of}} the task only in childrenwithout absolute pitch. These results suggest that, in the children with absolute pitch, N 100 m was elicited equally in every session because of their automatically driven auditory atten-tion. No signi¢cant correlationwas foundbetween the appearance rate of N 100 m and the possession of absolute pitch. NeuroRepor...|$|R
5000|$|Some {{experiments}} give subjects a [...] "distractor" [...] task {{to ensure}} that subjects are not consciously {{paying attention to the}} experimental stimuli; this may be done to test whether a certain computation in the brain is carried out automatically, regardless of whether the subject devotes attentional resources to it. For example, one study had subjects <b>listen</b> to non-linguistic <b>tones</b> (long beeps and buzzes) in one ear and speech in the other ear, and instructed subjects to press a button when they perceived a change in the tone; this supposedly caused subjects not to pay explicit attention to grammatical violations in the speech stimuli. The subjects showed a mismatch response (MMN) anyway, suggesting that the processing of the grammatical errors was happening automatically, regardless of attention - or at least that subjects were unable to consciously separate their attention from the speech stimuli.|$|R
60|$|So {{much for}} their eyes. As to the conversation, {{it had been}} {{perfectly}} insignificant because naturally {{they had nothing to}} say to each other. Heyst had been interested by the girl's physiognomy. Its expression was neither simple nor yet very clear. It was not distinguished--that could not be expected--but the features had more fineness than those of any other feminine countenance he had ever had the opportunity to observe so closely. There was in it something indefinably audacious and infinitely miserable--because the temperament and the existence of that girl were reflected in it. But her voice! It seduced Heyst by its amazing quality. It was a voice fit to utter the most exquisite things, a voice which would have made silly chatter supportable and the roughest talk fascinating. Heyst drank in its charm as one <b>listens</b> to the <b>tone</b> of some instrument without heeding the tune.|$|R
40|$|The {{present study}} {{investigated}} how fast {{younger and older}} adults recovered from a distracted attentional state induced by rare, unpredictable sound events. The attentional state was characterized by the auditory N 1 event-related potential (ERP), which is enhanced for sound events in the focus of attention. Younger (19 - 26 years) and older (62 - 74 years) adults <b>listened</b> to continuous <b>tones</b> containing rare pitch changes (glides) and short gaps. Glides and gaps could be separated in 150 ms, 250 ms, 650 ms or longer and the task was gaps detection while ignoring glides. With longer glide-gap separations similar N 1 enhancements were observable in both groups suggesting that {{the duration of the}} distracted sensory state was not affected by aging. Older adults responded, however, slower at short glide-gap separations which indicated that distraction at subsequent levels of processing may have nonetheless more impact in older than in younger adults...|$|R
40|$|Lexical tones have {{presented}} great difficulties for second language learners whose native language is non-tonal. A {{number of recent}} studies suggest categorical-like perception of lexical tones by native Mandarin speakers. Can native speakers of non-tonal languages acquire categorical representations of lexical tones? Are there any differences between L 1 and L 2 tone perceptions? This study investigates brain responses to lexical tone categorization for three groups of adult listeners: 1) native English speakers who had no exposure to Mandarin before age 17, but took advanced Mandarin courses as adults; 2) naïve English speakers; and 3) native Mandarin speakers. Two tonal continua were derived from natural speech through interpolation within two tonal contrasts (Tone 1 /Tone 4; Tone 2 /Tone 3). Firstly, category boundaries were examined through classic identification and discrimination tasks. Secondly, high-density electroencephalography (EEG) was used to record brain responses while participants <b>listened</b> to <b>tones</b> in two oddball paradigms: across-category and within-category. If perception of lexical tones is categorical, cross-category deviants are expected to elicit larger ERP responses (specifically, mismatch negativity (MMN) and P 300) than within-category deviants. Both behavioral and ERP results indicate that lexical tones are perceived categorically by native Chinese speakers but not by inexperienced English speakers. Although English learners of Chinese demonstrated categorical perception in behavioral tasks, their ERP response did not differ between within- and across-category conditions, however, significantly greater P 300 responses were observed. Acoustic cues and characteristics of L 2 phonological learning in adulthood are discussed...|$|R
40|$|Background A {{wide array}} of {{experimental}} studies are supportive of a working memory explanation {{for the effects of}} eye movements in EMDR therapy. The working memory account predicts that, as a consequence of competition in working memory, traumatic memories lose their emotional charge. Method This study was aimed at investigating (1) the effects of taxing the working memory, as applied in EMDR, during recall of negative memories in 32 patients with posttraumatic stress disorder (PTSD), and 32 patients with other mental disorders, and (2) whether the results would differ between both groups. In a therapeutic session patients were asked to recollect a crucial upsetting memory while, in counterbalanced order (a) performing eye movements, (b) <b>listening</b> to <b>tones</b> and (c) watching a blank wall (‘recall only’), each episode lasting 6 min. Results Eye movements were found to be more effective in diminishing the emotionality of the memory than ‘recall only’. There was a trend showing that tones were less effective than eye movements, but more effective than ‘recall only’. The majority of patients (64 %) preferred tones to continue with. The effects of taxing working memory on disturbing memories did not differ between PTSD patients and those diagnosed with other conditions. Conclusions The findings provide further evidence for the value of employing eye movements in EMDR treatments. The results also support the notion that EMDR is a suitable option for resolving disturbing memories underlying a broader range of mental health problems than PTSD alone...|$|R
40|$|ABSTRACT. This chapter {{concerns}} the processes through which auditory sensory information is {{converted into a}} perceptual representation relevant for behavior. We describe a series of experiments where subjects <b>listened</b> to faint <b>tones</b> appearing amidst background noise, and were instructed to respond {{as fast as possible}} when they detect the tonal objects. Simultaneous psychophysical and brain imaging (MEG) measures were employed to study the mechanisms by which auditory objects are detected and separated from their surroundings. The results demonstrate a striking incongruence between behavioral responses and pre-attentive brain responses, and therefore dissociation between higher level mechanisms related to conscious detection of tones amidst the noise and the lower level, pre-attentive cortical mechanisms that sub-serve the physical extraction of the tonal targets. The implications of these data for the processes that underlie the creation of perceptual representations are discussed. These finding may also provide a methodological tool to study the heuristics humans employ in the course of conscious decision making about events in the world. ...|$|R
40|$|This study {{investigated}} the accuracy of musical pitch detection in children with autistic spectrum disorders as compared with typically developing children. Seventeen children on the autistic spectrum (Mage ¼ 9 : 34, SDage ¼ 1 : 12) and 13 typically developing, chronological age-matched children (Mage ¼ 9 : 13, SDage ¼ 1 : 68) {{took part in the}} current study. Children were required to <b>listen</b> to four <b>tones,</b> which were paired with four different pictures and asked to learn the combinations. The children were then assessed for their ability to identify the previously learned tones, when they were presented as single tones and when they were embedded in chords and discords. No significant group differences were found. However, after subdividing the clinical group according to their diagnosis of autism or Asperger’s syndrome, the results indicated a slightly superior disembedding ability in participants with Asperger’s syndrome. The findings are discussed in terms of the weak central coherence concept...|$|R
40|$|Research {{was carried}} out in {{collaboration}} with the Head and Neck Department and the Clinical Neurophysiological Unit at the University of Trieste (Italy) to assess the effects of resonance phenomena on the human body. We worked with volunteers who underwent examination by EEG while <b>listening</b> to <b>tones</b> between 90 Hz and 120 Hz, similar to the resonant sounds found at some Neolithic structures in Europe (England, Ireland, Italy, Malta). As in the study by Ian Cook at the University of California (UCLA, 2008), all of our volunteers were subjected to a "comfortable" volume of sound whilst in the absorbing sound room. This is used for audiometric tests at the Otorhinolaryngology Clinic and has been modified with suitable software and hardware. This type of room is also protected by a Faraday cage to shield from any possible external electromagnetic interference that could affect the results. After two minutes of silence to evaluate the resting brain rhythm, the volunteers were subjected to the tones of 90, 95, 100, 105, 110, 115, 120 Hz arranged in a random way for one minute each. At the end of every cycle they listened to a mantra of the same frequency for a period of two minutes. Technicians examined the EEGs to verify the data collected. They found there was a prevalence of frontal areas or occipital (posterior) areas with no predominance of one cerebral hemisphere (left of right) over the other during playing. Each volunteer had a different sensitivity to all the tones without one tone prevailing (i. e. 110 Hz), with each exhibiting a strong response to a subjective and personal tone (90 Hz, 105 Hz, 120 Hz [...] .) ...|$|R
30|$|Many {{ideas were}} {{proposed}} which aim {{to reduce the}} MAC overhead of CSMA-based wireless LANs, especially regarding the random backoff scheme of 802.11 DCF. WiFi-Nano [23] reduces slot time to 800 ns instead of 9 μs in current standards. Since backoff time is proportional to slot time, using short slot time can reduce idle time and thus improve system throughput. Back 2 F [24] removes temporal backoff using frequency domain backoff. Instead of waiting for a random number of slots, each node transmits a tone on the selected subcarrier. By <b>listening</b> to the <b>tones,</b> a node can determine if it has selected the smallest number which will win the channel. HiBo [25] uses hierarchical backoff {{in order to reduce}} the average backoff time. In HiBo, nodes pick random numbers from a small range which will result in multiple nodes selecting the same number. The nodes who selected the same number move to the next round and perform another contention. HiBo decreases both idle time and collision rate, thereby improving the system throughput.|$|R
40|$|OBJECTIVE: The {{goal was}} to assess {{auditory}} cortex activation evoked by pure-tone stimulus with func-tional MRI. METHODS: Five healthy children, aged 7 to 10 years, were studied. Hearing evaluation was performed by pure-tone audiometry in a sound-treated room and in the MRI scanner with the scanner noise in the background. Subjects were asked to <b>listen</b> to pure <b>tones</b> (500, 1000, 2000, and 4000 Hz) at thresholds determined in the MRI scanner. Functional image processing was performed with a cross-correlation technique with a correlation coefficient of 0. 5 (P < 0. 0001). Auditory cortex activation was assessed by observing activated pixels in functional images. RESULTS: Functional images of auditory cortex acti-vation were obtained in 3 children. All children showed activation in Heschl’s gyrus, middle tempo-ral gyrus, superior temporal gyrus, and planum temporale. The number of activated pixels in audi-tory cortexes ranged from 4 to 33. CONCLUSIONS: Functional images of auditory cor-tex activation evoked by pure-tone stimuli are obtained in healthy children with the functional MRI technique. (Otolaryngol Head Neck Surg 2000; 122...|$|R
