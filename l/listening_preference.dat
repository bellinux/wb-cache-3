21|47|Public
40|$|Spoken {{telephone}} numbers are prosodically structured. This is reflected on various levels, such as grouping, wording and accenting. Realisation strategies employed by German speakers {{are used to}} model the prosody of telephone number production. In a <b>listening</b> <b>preference</b> test using synthetic speech two strategies used by commercial inquiry systems proved to be less acceptable than the versions based on the proposed models. These models are proposed for use in speech-synthesis-based telephone number inquiry services. 1...|$|E
40|$|The Headturn Preference Procedure (HPP) is a {{frequently}} used method (e. g., Jusczyk & Aslin; and subsequent studies) to investigate linguistic abilities in infants. In this paradigm infants are usually first familiarised with words and then tested for a <b>listening</b> <b>preference</b> for passages containing those words {{in comparison to}} unrelated passages. <b>Listening</b> <b>preference</b> {{is defined as the}} time an infant spends attending to those passages with his or her head turned towards a flashing light and the speech stimuli. The knowledge and abilities inferred from the results of HPP studies have been used to reason about and formally model early linguistic skills and language acquisition. However, the actual cause of infants' behaviour in HPP experiments has been subject to numerous assumptions as there are no means to directly tap into cognitive processes. To make these assumptions explicit, and more crucially, to understand how infants' behaviour emerges if only general learning mechanisms are assumed, we introduce a computational model of the HPP. Simulations with the computational HPP model show that the difference in infant behaviour between familiarised and unfamiliar words in passages can be explained by a general learning mechanism and that many assumptions underlying the HPP are not necessarily warranted. We discuss the implications for conventional interpretations of the outcomes of HPP experiments...|$|E
40|$|This study {{investigated}} the psychological dimensions underlying auditory processing of monotonic and melodic-rhythmic patterns, and influences of musical experiences on the dimensionality of 38 university music students ' rhythmic processing. Apparent alterations in tempo, duration and pitch characteristics, melodic and rhythmic phrase patterning, and monotony were shown to be organizers of rhythmic processing. Both major performing instrument and classification of major performing instrument significantly affected the dimensionality of subjects ' rhythmic processing. Analyses of variance also showed slight effects of generic style music <b>listening</b> <b>preference,</b> music course experience, and hours music listening on rhythmic processing. The importance of each organizer for subjects depended {{in part on the}} objective ordering of the rhythmic and tonal information, and in part on past musical experiences...|$|E
40|$|We develop {{temporal}} embedding {{models for}} exploring how <b>listening</b> <b>preferences</b> {{of a population}} develop over time. In particular, we propose time-dynamic probabilistic embedding models that incorporate users and songs in a joint Euclidian space in which they gradually change position over time. Using large-scale Scrobbler data from Last. fm spanning a period of 8 years, our models generate trajectories of how user tastes changed over time, how artists developed, and how songs move in the embedded space. This ability to visualize and quantify <b>listening</b> <b>preferences</b> of a large population of people over a multi-year time period provides exciting opportunities for data-driven exploration of musicological trends and patterns. 1...|$|R
5000|$|Swell Radio was {{a mobile}} radio {{streaming}} application that learned user <b>listening</b> <b>preferences</b> based on <b>listening</b> behavior, community filtering, and a proprietary algorithm. Originally {{designed for use}} while commuting to and from work, the service focused on delivering spoken-word audio content to users. Major streaming partners included ABC News Radio, NPR, PRI, and TED ...|$|R
40|$|The {{purpose of}} this study was to explore the {{associative}} learning of statistically-frequent, hierarchically-derived melodic sequences in 14 -month-old infants, with the ultimate goal of comparing infant musical and linguistic knowledge. Using the Head-Turn Preference Procedure (Nelson et al., 1995), we assessed infant <b>listening</b> <b>preferences</b> for Western Tonal melodies with expected endings, and Western Tonal melodies with unexpected endings, specifically asking whether the infant participants would be able to discriminate the melodic sets. Successful discrimination would indicate knowledge of the statistically-frequent, expected nature of the expected-ending melodies. After comparing mean listening times for the two sets of stimuli, we found no significant difference, which would imply that – at 14 months – infants do not have expectations for the expected nature of our melodies. However, female and male participants demonstrated longer listening times for different stimuli, which may indicate that 14 -month-old infants can discriminate expected and unexpected melodies, but that the two genders possess opposing <b>listening</b> <b>preferences.</b> More participants would be needed to further assess this inference...|$|R
40|$|We did a paired {{comparison}} preference test where listeners rated their <b>listening</b> <b>preference</b> for four different pop musical pieces presented by WFS, stereo or surround. The musical pieces were all mixed {{by the same}} person {{in order to try}} to minimize the influence of the mix on the ratings, but still trying to get the best out of every system, see [1] for details. The mixes are available at [URL] Here, we provide the results of the 22 listeners that participated in the experiment together with an analysis which calculates a Bradley-Terry-Luce model after Wickelmayer et al. [2]. [1] Hold, C., Wierstorf, H., Raake, A. (2016), “The Difference Between Stereophony and Wave Field Synthesis in the Context of Popular Music,” 140 th AES Convention, Paper 9533 [2] [URL]...|$|E
40|$|The {{purpose of}} this study was to measure the effects of style, tempo, and {{performing}} medium onfifth-rade students ' expressed music <b>listening</b> <b>preference.</b> A listening test was administered to 107 students in four classes in central Michigan. Test reliability was evaluated in terms of common factor concentration and stability across time, and behavior observation was used to help interpret results. A preference hierarchy emerged in which the popular styles were most favored and correlation analysis indi-cated that style was most strongly related to preference. A three-way repeated measures analysis of variance disclosed a significant three-way interaction. An ex-amination of charted cell means indicated a strong effectfor style, which was notice-ably suppressed by performance in the instrumental medium. Across pooled styles there was a slight preference for faster tempos and the instrumental medium...|$|E
40|$|The {{purpose of}} this study was to examine the {{personality}} characteristics and developmental issues of 3 groups of adolescent music listeners: those preferring light qualities of music, those preferring heavy qualities of music, and those who had eclectic preferences for music qualities. One hundred sixty-four adolescents completed an age-appropriate personality inventory and a systematic measure of music <b>listening</b> <b>preference.</b> The findings indicate that each of the 3 music preference groups is inclined to demonstrate a unique profile of personality dimensions and developmental issues. Those preferring heavy or light music qualities indicated at least moderate difficulty in negotiating several distinct domains of personality and/or developmental issues; those with more eclectic music preferences did not indicate similar difficulty. Thus, there was considerable support for the general hypothesis that adolescents prefer listening to music that reflects specific personalities and the developmental issues with which they are dealing. KEY WORDS: music; personality; adolescent development...|$|E
40|$|This paper {{describes}} {{the operation of}} and research behind a networked application for the delivery of personalised streams of music at Trinity College Dublin. Smart Radio is a web based client-server application that uses streaming audio technology and recmnmendation techniques to allow users build, manage and share music programmes. While it is generally acknowledged that music distribution over the web will dramatically change how the music industry operates, there are few prototypes available to demonstrate how this could work in a regulated way. The Smart Radio approach is to have people manage their music resources by putting together personalised music programmes. These programmes can then be recommended to other listeners {{using a combination of}} collaborative and contentbased recommendation strategies. We describe how we use a novel two-stage approach to find recommendations that are pertinent to a listener's current <b>listening</b> <b>preferences,</b> something which collaborative techniques are insensitive to. We describe additional constraints required to provide a service personalized to each user' s <b>listening</b> <b>preferences.</b> The Smart Radio system currently runs within the Computer Science Intranet with permission from the National Music Rights Organisation It is a prototype system for an "always on" high bandwidth Internet connection such as ADSL...|$|R
40|$|The {{intelligibility}} {{of speech}} played back via audio reproduction systems is often impaired in noisy backgrounds. Ideally, algorithms enhancing speech intelligibility should be adaptive {{to the type}} and temporal variations of the noise, and also account for differences in individual <b>listening</b> <b>preferences.</b> While noise-adaptive algorithms have been investigated in several studies, individual preferences {{have not yet been}} addressed in this context. The current study investigated the inter-individual variability of normal-hearing subjects’ preferences with respect to intelligibility enhancement in noise for communication applications using the AdaptDRC algorithm [Schepker, Rennies & Doclo, Proc. of Interspeech, Lyon, France, Aug. 2013, pp. 3577 - 3581], which {{has been shown to be}} highly effective in various types of background noise. Originally, the algorithm uses estimations of the SII to control spectral shaping and compression characteristics of the speech signal. In this study subjects were asked to adjust the parameters themselves based on their personal preferences at different SNRs in three types of background noises. The data are discussed with respect to the relation between individual <b>listening</b> <b>preferences,</b> generic model- based parameters and the predictability of individually preferred parameter settings, which would allow a complete individualization of the algorithm...|$|R
40|$|TCD-CS- 2002 - 07 This paper {{describes}} {{the operation of}} and research behind a networked application for the delivery of personalised streams of music at Trinity College Dublin. Smart Radio is a web based client-server application that uses streaming audio technology and recommendation techniques to allow users build, manage and share music programmes. While it is generally acknowledged that music distribution over the web will dramatically change how the music industry operates, there are few prototypes available to demonstrate how this could work in a regulated way. The Smart Radio approach is to have people manage their music resources by putting together personalised music programmes. These programmes can then be recommended to other listeners {{using a combination of}} collaborative and contentbased recommendation strategies. We describe how we use a novel two-stage approach to find recommendations that are pertinent to a listener?s current <b>listening</b> <b>preferences,</b> something which collaborative techniques are insensitive to. We describe additional constraints required to provide a service personalized to each user?s <b>listening</b> <b>preferences.</b> The Smart Radio system currently runs within the Computer Science Intranet with permission from the National Music Rights Organisation It is a prototype system for an "always on" high bandwidth Internet connection such as ADSL...|$|R
40|$|The {{purpose of}} this study was to detemine whether musical selections, {{classical}} or popular, played before, during, and after a short one day surgical procedure contributed to the self-reported psychological well being of participating day stay surgery patients in a large general hospital. Subjects were 89 same day surgery patients between the ages of 20 and 60 receiving either local or general anesthesia. They were given their choice of listening to either a specially prepared classical or popular tape. Subjects were given two short questionnaires: (1) PreSurgery. To determine Behavior Type A or B; and (2) Post Surgery. To determine their reactions to listening to the music. ^ Results of this study showed a very high proportion of the population (76. 4 %) were considered definite Type A behavior personalities; 21. 3 % were borderline; and 2. 3 % were definite Type B as scored with the Framingham Type A Behavior Scale. The population as a whole responded highly favorably to finding the music calming, distracting from surroundings, pleasant, positively affecting recovery, a positive experience in the recovery room, enjoying the musical selections, ability to listen to the music, and overall reaction to the experience. Results of this study showed few significant differences between the music groups and behavior groups with respect to feelings. Classical listeners reported slightly more positive feelings towards the music with respect to pleasantness and the individual pieces of music on the tape. They were also more likely to chose the same kind of music again. Type A subjects who listened to popular music reported a more positive recovery effect. All behavior groups who did not get their normal musical <b>listening</b> <b>preference</b> found the music to be slightly disturbing. Other significant findings include: the more emotional the subject the less disturbing they found the music they were listening to that was NOT their preference; the higher the nervous disposition score the more positive the classical listeners thought the music helped their recovery, but the less opinion they had if they were listening to their normal <b>listening</b> <b>preference.</b> In addition, questions of imagery, and ability to listen to music were analyzed. ...|$|E
40|$|Infants {{preferentially}} {{discriminate between}} speech tokens that cross native category boundaries prior to acquiring a large receptive vocabulary, implying {{a major role}} for unsupervised distributional learning strategies in phoneme acquisition {{in the first year}} of life. Multiple sources of between-speaker variability contribute to children's language input and thus complicate the problem of distributional learning. Adults resolve this type of indexical variability by adjusting their speech processing for individual speakers. For infants to handle indexical variation in the same way, they must be sensitive to both linguistic and indexical cues. To assess infants' sensitivity to and relative weighting of indexical and linguistic cues, we familiarized 12 -month-old infants to tokens of a vowel produced by one speaker, and tested their <b>listening</b> <b>preference</b> to trials containing a vowel category change produced by the same speaker (linguistic information), and the same vowel category produced by another speaker of the same or a different accent (indexical information). Infants noticed linguistic and indexical differences, suggesting that both are salient in infant speech processing. Future research should explore how infants weight these cues in a distributional learning context that contains both phonetic and indexical variation...|$|E
40|$|Abstract. Contextual {{information}} of the listener is only slowly being integrated into music retrieval and recommendation systems. Given the enormous rise in mobile music consumption {{and the many}} sensors inte-grated into today’s smart-phones, at the same time, an unprecedented source for user context data of different kinds is becoming available. Equipped with a smart-phone application, which had been developed to monitor contextual aspects of users when listening to music, we collected contextual data of listening events for 48 users. About 100 different user features, in addition to music meta-data have been recorded. In this paper, we analyze the relationship between aspects of the user context and music <b>listening</b> <b>preference.</b> The goals are to assess (i) whether user context factors allow predicting the song, artist, mood, or genre of a listened track, and (ii) which contextual aspects are most promising for an accurate prediction. To this end, we investigate various classifiers to learn relations between user context aspects and music meta-data. We show that the user context allows to predict artist and genre to some extent, but can hardly be used for song or mood prediction. Our study further reveals {{that the level of}} listening activity has little influence on the accuracy of predictions. ...|$|E
40|$|Senior {{citizens}} are {{the fastest growing}} demographic today, and the digital music industry {{is one of the}} fastest growing aspects of our economy. This study explores the relationships and underlying correlations between these two facets. Three research methods were employed: field observation of a musician targeting his music to the elderly; data analysis of this musician’s sales records; and a questionnaire seeking the purchasing and <b>listening</b> <b>preferences</b> of baby boomers and senior citizens. The results reveal that, as age increases, senior citizens’ comfort purchasing and listening to music through digital methods decreases. A negative correlation between age, nostalgic marketing, and the Internet is explored as well. Suggestions are provided for the implementation of these results in the marketplace...|$|R
40|$|Previous {{research}} by Persaud (2013) found that infant listeners preferentially listen longer to sung stimuli over spoken stimuli, {{regardless of the}} age of the infant. The present study tests two age groups of infants to determine whether early language exposure affects infants 2 ̆ 7 <b>listening</b> <b>preferences</b> for song and speech. Six- to seven-month- old infants and eight- to ten-month-old infants from English speaking homes were presented with auditory stimuli of English-speaking women speaking or singing and tested in a head-turn preference task. Consistent with the findings from Persaud (2013), it was found that both age groups listened longer to the sung stimuli compared to the spoken stimuli. This suggests that song is inherently more attractive to infants, possibly because song stimuli are generally less acoustically variable compared to speech stimuli, and therefore ultimately easier for infants to cognitively process compared to speech stimuli. The results of this study support a processing-based account of infants 2 ̆ 7 preferences for ID-stimuli...|$|R
40|$|Although {{content is}} {{fundamental}} to our music <b>listening</b> <b>preferences,</b> the leading performance in music recommen-dation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user’s listening his-tory rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known “cold-start ” prob-lem, i. e., it is unable to work with new songs {{that no one has}} listened to. Efforts on incorporating content informa-tion into collaborative filtering methods have shown suc-cess in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model {{and use it as a}} prior in a collaborative fil-tering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evalu-ated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case. 1...|$|R
40|$|For years, {{the effects}} of sound {{pressure}} level measurements upon human hearing and psychology have been studied, and only in recent years scientists have seen a link between these measurements and headphones connected to portable listening devices. The Brigham Young University Educational Acoustics Research Study (E. A. R. S) is an interactive demonstration devoted to acquiring samples {{of the student body}} that connect findings between the use of headphones with portable listening devices, such as mp 3 players, and educating individuals in the process. This report will highlight the reasoning for conducting the study, describe the proper background in acoustics needed to understand the material, and analyze the results from the study, while concluding observations. The study will evaluate the overall sound pressure level due to a one second Leq, safe listening time according to non-occupational noise criterion, and demographics that define the individual, such as age, gender, <b>listening</b> <b>preference,</b> and mp 3 player. Also, the study will ask the individual about their listening habits and if they will change based around the feedback received from the E. A. R. S. This study will ultimately benefit science with information to understand connections between listening habits and safe practices of listener protection...|$|E
40|$|The most robust finding on infants ’ {{listening}} preferences {{has been}} widely charac-terized as a preference for baby talk (BT) over adult-directed speech (ADS). Although prosodic modifications characteristic of BT also convey positive affect, differences in affect across BT and ADS speech registers have not been controlled in previous studies. This set of experiments sought to elucidate the basis for 6 -month-olds ’ <b>listening</b> <b>preference</b> by independently manipulating affect and speech register. When affect was held constant, no preference for any speech regis-ter was observed. Moreover, when ADS stimuli presented more positive affect than BT stimuli, infants ’ preferences followed the positive affect. Higher and more vari-able pitch was neither necessary nor sufficient for determining infants ’ preferences, although pitch characteristics may modulate affect-based preferences. The BT preference is thus attributable to a more general preference for speech that imparts relatively positive affect, a preference perhaps ascribable to a preexisting general-purpose mechanism opportunistically exploited by language. From birth onward, infants display a range of preferences for auditory stimuli. As early as {{the first month of}} life, infants prefer to listen to speech rather than nonspeech (Cooper & Aslin, 1994), mothers’speech rather than other females’speech (DeCasper Supplementary materials to this article are available on the World Wide Web a...|$|E
40|$|The {{purpose of}} this project {{is to build a}} PIC (Peripheral Interface Controlled) {{controlled}} two-band stereo audio equalizer. The input audio from each stereo channel is separated into four different analog filters. The PIC has been programmed in BASIC and is used to route the audio signal to one of the four analog filters. Three of the filters have been preset to a certain frequency response (Jazz, Natural, and Bass), and the fourth filter has the unique ability to be controlled manually. Stereo potentiometers have been installed on the “Manual” filter to allow the user to change the frequency response of the circuit. The manual filter has been divided into two-bands, the “Bass” band, and the “Treble” band. By varying the stereo potentiometer on the manual filter, the manual filter circuit will provide a +/- 12 dB of gain for both bands. The “bass” band has a bandwidth of 20 - 500 Hz, while the “Treble” band has a bandwidth of 500 hz- 20 kHz. The PIC Controlled Two-Band Stereo Audio Equalizer is useful for cutting and boosting certain frequencies, allowing the user to adjust the music to their <b>listening</b> <b>preference.</b> The total material cost is around $ 150 per unit...|$|E
40|$|Directional Audio Coding (DirAC) is a well-proven {{technique}} for recording spatial sound and efficiently coding {{it into one}} or very few audio channels accompanied by parametric side information. Therefore, {{it is suited for}} teleconferencing featuring spatial rendering of distributed sources. In teleconferences with more than two attending parties, additional rendering capabilities are desired to (a) spatially distribute each party for better intelligibility of single speakers especially in situations of multiple active sources, (b) adjust levels of individual sources to the requirements of given <b>listening</b> <b>preferences,</b> and to (c) align acoustic with visual cues. MPEG Spatial Audio Object Coding (SAOC) provides this required functionality. Originally, SAOC was designed for having single separated audio objects, i. e., their signals as inputs. In this contribution we propose {DirAC} in acoustic front-end processing for SAOC, where directional filtering in DirAC's parameter domain is used to separate single sources from an acoustic mixture. The paper presents a close look at an efficient transcoding of the parameters of the two considered techniques...|$|R
40|$|This {{submission}} {{is comprised}} {{of a variety of}} compositions for young choirs. Composing for such groups is distinct from composing for other ensembles. because the nature of the voice. is not static: it changes markedly during childhood and adolescence. General patterns of vocal development in childhood and adolescence are reflected in these compositions, with the aim of utilising young voices fully but avoiding making potentially damaging demands on them. The range of styles and some significant characteristics of the compositions are informed by other developmental factors: changes in <b>listening</b> <b>preferences</b> and the evidence of developing musical understanding demonstrated in young people's compositions. When young people listen to music, their openness to style first narrows and later broadens; when they compose music, different musical characteristics appear to be primary focal points of different stages in learning. These psychological factors can be compared with (variable) formants - the composer's task is to create music that can "resonate" with groups of young singers. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|This five-family {{case study}} {{examined}} how five families in Miami-Dade County (Miami, FL), in which neither parent self-identified as a musician, talked about music. Through Internet-facilitated interviews and e-journals, {{participants responded to}} questions about music in their family and the listening guidelines that were communicated between members. Study participants included five mothers of similar ages, incomes, and educational backgrounds. Six children participated in the study; they {{ranged in age from}} six to thirteen years of age, and included both males and females. The goal {{of the study was to}} gain an understanding of the types of discourse around music listening in these families, and to examine the content of their shared family talk about and in response to music. The theoretical framework for this study was rooted in both sociolinguistics, through the investigation of the relationship between language and society, and family script theory (Byng-Hall, 1995, 1998), which provided guidance in analyzing the intergenerational transmission of music values. Narrative self-reports offered by participating parents and children demonstrated how discourse was used to describe, interpret, construct, and disseminate musical meaning within families. Listening guidelines and listenership roles, enacted and enforced in contextual and reactive ways within a body of shared discourse, were found to both facilitate and influence family members’ music listening exposures. The study yielded fruitful descriptive data regarding the nature of musical exposures and music-use in the participating families in addition to findings facilitated by sociolinguistic analysis. For each of the five participating families, their communal discourse—their family’s script—described situational music listening guidelines and roles which contended with certain “bad” words and attempted to limit exposure to questionable themes. Sociolinguistic terminology, including notions of age appropriateness and taboo, helped explain several ways in which discourse can link music exposure with the development of music <b>listening</b> <b>preferences.</b> The present study supports previous findings (Borthwick 2 ̆ 6 Davidson, 2002) regarding the important role family plays in shaping individual identities and music <b>listening</b> <b>preferences.</b> Further, this multiple-case study offers compelling evidence as to how intimate relationships, defined and negotiated through shared discourse, can influence individuals’ decisions about what music they value...|$|R
40|$|Several {{studies have}} {{demonstrated}} that infants are sensitive to prosodic cues from birth on and use this kind of information for their earliest steps into the acquisition of words and syntactic regularities of their target language. In the following we will report the results of four experiments with the headturn preference/discrimination paradigm we conducted with German infants between 4 and 6 months of age and with French infants at 6 months of age. Our study addresses the question of whether during this developmental period we can already find evidence that infants have determined the predominant trochaic pattern for bisyllabic words in their surrounding language. We presented 6 - and 4 -month-olds with trochaic or iambic bisyllabics in German and 6 month-olds with same stimuli in French. The German 6 -month-olds showed a <b>listening</b> <b>preference</b> for the trochaic pattern, but not the German 4 -month-olds nor the French 6 -month-olds. However, 6 -month-old French infants were able to discriminate trochaic from iambic bisyllabics. This suggests that the preference for the language dominant pattern of word stress arises between the age of 4 and 6 months in German. In French, in which there is little if any accentuation at the lexical level, French-learning 6 -month-olds do not show a preference for any stress pattern, but they are sensitive to acoustic differences between trochaic and iambic bisyllables...|$|E
40|$|This paper {{continues}} {{to discuss how}} a specific musical instrument impacts the design of practice rooms. In the previous paper six instruments (violin, clarinet, trumpet, xylophone, piano, and guitar) and a choir performance are assessed. The reverberation time (RT) is a starting acoustical consideration and the floor area is a starting architectural consideration. The rooms are simulated with the architectural variables. The anechoic recordings are used to prepare auralizations of each instrument and the choir performance for each separate room design. However, ordinary people are asked to select for the best rooms to listen/experience the rooms. In this paper, the same listening tests were done to investigate the ‘best ’ room preference of the professional musicians to practice. Then the musicians were also asked for their <b>listening</b> <b>preference.</b> The appropriate reverberation time and the most preferable rooms are demonstrated. One {{of the results of}} this experiment is that musicians make a clear distinction between the rooms for practicing and rooms for listening. For the practice low RT (0. 45 - 0. 9 sec) is preferred for violin, clarinet, trumpet, while high RT is preferred for choir (2 - 2. 5 sec). For the listening experience for most of the instruments mid RT values (1. 3 - 1. 6 sec) is preferred. For all calculations and the auralizations, a computational model is used: ODEON 7. 0...|$|E
40|$|In {{this paper}} {{we use a}} {{computational}} model to investigate four assumptions that are tacitly present in interpreting the results of studies on infants' speech processing abilities using the Headturn Preference Procedure (HPP) : (1) behavioral differences originate in different processing; (2) processing involves some form of recognition; (3) words are segmented from connected speech; and (4) differences between infants should not affect overall results. In addition, we investigate the impact of two potentially important aspects {{in the design and}} execution of the experiments: (a) the specific voices used in the two parts on HPP experiments (familiarization and test) and (b) the experimenter's criterion for what is a sufficient headturn angle. The model is designed to be maximize cognitive plausibility. It takes real speech as input, and it contains a module that converts the output of internal speech processing and recognition into headturns that can yield real-time <b>listening</b> <b>preference</b> measurements. Internal processing is based on distributed episodic representations in combination with a matching procedure based on the assumptions that complex episodes can be decomposed as positive weighted sums of simpler constituents. Model simulations show that the first assumptions hold under two different definitions of recognition. However, explicit segmentation is not necessary to simulate the behaviors observed in infant studies. Differences in attention span between infants can affect the outcomes of an experiment. The same holds for the experimenter's decision criterion. The speakers used in experiments affect outcomes in complex ways that require further investigation. The paper ends with recommendations for future studies using the HPP. - See more at: [URL]...|$|E
40|$|Includes bibliographical {{references}} (pages 82 - 83) KCSN-FM is a non-commercial {{public radio}} station located {{on the campus of}} California State University at Northridge. The station's prime coverage area is the San Fernando Valley. The Federal Communications Commission in granting a license to KCSN-FM made the provision that the station was obligated to make a continuing, diligent effort to determine the significant needs and problems of our service area. The need to provide management with a demographic profile and <b>listening</b> <b>preferences</b> was the incentive for this thesis. Three hundred and sixty five people, who contributed to the station during the period of November, 1975 through July, 1976 were mailed a pre-tested questionnaire. The responses to the questionnaire were coded, computerized and analyzed. The average KCSN-FM contributor listener is mainly over the age of thirty-six, married, well educated, lists occupation as business or professional, generally earning more than 12, 000 a year and tunes into the station at least once a week. The questionnaire indicated that the contributor listener is loyal and occupies a middle class position in the community...|$|R
40|$|TCD-CS- 2005 - 28 Typically, case-based {{recommender}} systems recommend single {{items to}} the on-line customer. In this paper we introduce {{the idea of}} recommending a user-defined collection of items where the user has implicitly encoded {{the relationships between the}} items. Automated collaborative filtering (ACF), a so-called `contentless? technique, has been widely used as a recommendation strategy for music items. However, its reliance on a global model of the user?s interests makes it unsuited to catering for the user?s local interests. We consider the context-sensitive task of building a compilation, a user-defined collection of music tracks. In our analysis, a collection is a case that captures a specific shortterm information/music need. In an offline evaluation, we demonstrate how a case-completion strategy that uses short-term representations is significantly more effective than the ACF technique. We then consider the problem of recommending a compilation according to the user?s most recent <b>listening</b> <b>preferences.</b> Using a novel on-line evaluation where two algorithms compete for the user?s attention, we demonstrate how a knowledge-light case-based reasoning strategy successfully addresses this problem...|$|R
50|$|Subjectivists {{believe that}} careful {{individual}} listening {{is an appropriate}} tool for discovering the true worth of a device or treatment, and will generally acquire equipment that suits their own <b>listening</b> or style <b>preferences</b> as opposed to measurable equipment performance.|$|R
40|$|Various {{measures}} of infant responsiveness {{have been shown}} to predict child outcomes. Despite this extensive research, there is no work examining links between infant responsiveness during caregiver-infant interactions with infants ' ability to perform basic linguistic tasks. One key task in early linguistic development is word segmentation, an achievement that allows infants to build their mental dictionaries. We hypothesized that infants ' responsiveness to caregiver facial expressions might be related to their word segmentation ability. In order to test this hypothesis, mothers came into the lab and were videotaped reading books containing target words to their 5 -month-old children. After the infants were read to, we tested their <b>listening</b> <b>preference</b> for words in the books, as well as novel words; this test yielded a preference score (preference for familiar vs. unfamiliar words). We also used the videotaped reading to code facial expressions for both infant and caregiver, and subsequently, we tabulated occasions where synchronous facial expressions occurred for each member of the dyad. We then examined possible correlations between our preference score and measures gleaned from the dyadic facial expression coding. Although neither the number of infant-led synchronous facial expressions nor the total number of facial expressions produced by either member was significantly correlated with preference score, our measure of synchronous facial expressions led by the caregiver was highly correlated with preference score. Thus, results support the hypothesis that infant responsiveness during caregiver-infant interaction, as indexed by synchronous facial expressions with caregivers, may be related to language learning ability...|$|E
40|$|Three {{experiments}} investigated possible acoustic {{determinants of}} the infant lis-tening preference for motherese speech found by Fernald (1985). To test {{the hypothesis that the}} intonation of motherese speech was sufficient to elicit this preference, it was necessary to eliminate lexical content and to isolate the three maior acoustic correlates of intonation: (1) fundamental frequency (Fo), or pitch: (2) amplitude, correlated with loudness; and (3) duration, related to speech rhythm. Three sets of auditory reinforcers were computer-synthesized, derived from the FO (Experiment 1). amplitude (Experiment 2). and durotion (Experiment 3) characteristics {{of the infant}}- and adult-directed natural speech samples used by Fernald (1985). Thus, each of these experiments focused on particular prosodic variables in the absence of segmental variation. Twenty 4 -month-old infants were tested in on operant ouditory preference procedure in each experiment. In-fonts showed a significant preference for the FO-patterns of motherese speech, but not for the amplitude or duration patterns of motherese. motherese intonation infant auditory preference fundamentol frequency language input auditory development The primary emphasis in research on the structure and functions of motherese has been on linguistic variables and language outcome measures. Much less at-tention has been given to the sound of mothers ’ speech, and to the possible role of the exaggerated intonation typical of motherese in communicating af-fect and regulating infant attention, particularly with prelinguistic infants. In an auditory preference study, Fernald (1985) found that 4 -month-old infants chose to listen more often to infant-directed speech than to adult-directed speech, a <b>listening</b> <b>preference</b> which could be accounted for by a number of perceptual as well as linguistic variables. The present study extends these find-This report is based on a dissertation submitted in partial fulfillment of the requirements fo...|$|E
40|$|Infants start {{learning}} the prosodic properties of {{their native language}} before 12 months, {{as shown by the}} emergence of a trochaic bias in English-learning infants between 6 and 9 months (Jusczyk et al., 1993), and in German-learning infants between 4 and 6 months (Höhle et al., 2009; Höhle et al., 2014), while French-learning infants do not show a bias at 6 months (Höhle et al., 2009). This language-specific emergence of a trochaic bias is supported by the fact that English and German are languages with trochaic predominance in their lexicons, while French is a language with phrase-final lengthening but lacking lexical stress. We explored the emergence of a trochaic bias in bilingual French/German infants, to study whether the developmental trajectory would be similar to monolingual infants and whether amount of relative exposure to the two languages has an impact on the emergence of the bias. Accordingly, we replicated Höhle et al. (2009) with 24 bilingual 6 -month-olds learning French and German simultaneously. All infants had been exposed to both languages for 30 to 70 % of the time from birth. Using the Head Preference Procedure (HPP), infants were presented with two lists of stimuli, one made up of several occurrences of the pseudoword /GAba/ with word-initial stress (trochaic pattern), the second one made up of several occurrences of the pseudoword /gaBA/ with word-final stress (iambic pattern). The stimuli were recorded by a native German female speaker. Results revealed that these French/German bilingual 6 -month-olds have a trochaic bias (as evidenced by a preference to listen to the trochaic pattern). Hence, their <b>listening</b> <b>preference</b> is comparable to that of monolingual German-learning 6 -month-olds, but differs from that of monolingual French-learning 6 -month-olds who did not show any preference (Höhle et al., 2009). Moreover, the size of the trochaic bias in the bilingual infants was not correlated with their amount of exposure to German. The present results thus establish that the development of a trochaic bias in simultaneous bilinguals is not delayed compared to monolingual German-learning infants (Höhle et al., 2009) and is rather independent of the amount of exposure to German relative to French...|$|E
40|$|This {{dissertation}} {{deals with}} the radio broadcasting in the Czech Republic that is focused {{on a group of}} listeners aged 30 - 50 years. It especially covers the public and private radio stations with statewide coverage. It also surveys what kind of formats of stations and broadcast programs dominate at target listeners and what the target listeners themselves expect from the broadcasting. The work is divided into the chapters and subchapters. The first chapter is concentrated {{on the development of the}} public and private radio broadcasting after the year 1989. It describes the demonopolization of Československý rozhlas and formation of competition of private stations, the ending of Československý rozhlas and origin of Český rozhlas, which serve as a statutory organization. The second chapter is aimed at the beginning or private broadcast stations with statewide significance. The third chapter focuses on both formats of radio stations and the formats of programs which are recently used in the radio broadcasting. The fourth chapter briefly describes the program of three favorite stations of the target audience. Last chapter summarizes research part of dissertation. The questionnaires investigated the <b>listening</b> <b>preferences</b> of target group, which stations the listeners prefere and which content of [...] ...|$|R
40|$|In {{the past}} few years the {{computational}} capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle – a prototype system for content-based music browsing and exploration that targets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning capabilities based on GPS. GeoShuffle adds location-based and time-based context to a user’s <b>listening</b> <b>preferences.</b> Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of interaction is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of textual information that can be displayed. We propose selforganizing tag clouds, a 2 D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can improve the quality of music recommendation and that selforganizing tag clouds provide faster browsing and are more engaging than text-based tag clouds. 1...|$|R
40|$|This {{data set}} {{contains}} stimuli and {{results from a}} <b>listening</b> paired-comparison <b>preference</b> test. The stimuli consisted always of the same acoustic scene with three individual sources, which had different degrees of impairments. All of the stimuli are created in order to listen binaural via headphones to them. They include already a headphone compensation filter for usage with AKG K 601 headphones. The stimuli were generated using a recording done by the FH Köln [1]. For details on the experiment see [2]. [1] [URL] [2] Raake, A., Wierstorf, H., Blauert J. (2014), “A case for TWO!EARS in audio quality assessment,” Forum Acusticu...|$|R
