7|22|Public
40|$|Massive and massless Gaussian free fields can be {{described}} as generalized Gaussian processes indexed by an appropriate space of functions. In this article we study various approaches to approximate these fields and look at the fractal properties of the thick points of their cut-offs. Under some sufficient conditions for a centered Gaussian process with <b>logarithmic</b> <b>variance</b> we study the set of thick points and derive their Hausdorff dimension. We prove that various cut-offs for Gaussian free fields satisfy these assumptions. We also give sufficient conditions for comparing thick points of different cut-offs. Comment: 23 pages. Major changes done in the lower bound result and some other inconsistencies correcte...|$|E
40|$|An {{analysis}} {{is made of}} the difference between the alpha particle and proton flow velocities in the solar wind as observed by the OGO 5 satellite. The alpha and proton velocities from each of 962 spectral scans are compared with the variance of 32 solar wind flux measurements made during the scans. The average velocity difference is plotted for each of 10 <b>logarithmic</b> <b>variance</b> intervals and is seen to decrease and approach zero when the variance is high. It is shown that such an anticorrelation {{may be due to the}} fact the wave/particle interactions provide the drag force between two streams of different velocity in a collisionless plasma...|$|E
40|$|This paper explores an {{alternative}} volatility estimation approach discovering the helical structure of Fourier coefficients of volatility wave. Volatility wave is calculated by using wavelet decomposition with consequent <b>logarithmic</b> <b>variance</b> indicator estimation for each decomposed {{part of the}} signal and subsequent volatility matrix transform in a specified way. Further, using discrete Fourier transform the Fourier image of obtained volatility wave is analyzed. The Fourier image coefficients of the transformed volatility indicator have a clear helical (spiral) structure that evolves in time. This brings {{a new understanding of}} volatility and its evolution process from signal theory (and wavelet theory) perspective. We have found some regularity in the volatility evolution process. The minimum total distance indicator between Fourier coefficients is proposed as a measure of such regularity. This indicator has a nature of volatility lower bound...|$|E
50|$|These <b>logarithmic</b> <b>variances</b> and {{covariance}} are {{the elements}} of the Fisher information matrix for the beta distribution. They are also a measure of the curvature of the log likelihood function (see section on Maximum likelihood estimation).|$|R
40|$|Abstract. The {{first order}} design problem in geodesy is {{generalized}} here, {{to seek the}} network configuration that optimizes the precision of geophysical parameters. An optimal network design that satisfies intuitively appropriate criteria corresponds to minimizing the sum of <b>logarithmic</b> <b>variances</b> of eigenparameters. This is equivalent to maximizing the determinant of the design matrix, allowing for closed-form analysis. An equivalent expression is also given specifically for square root information filtering, to facilitate numerical solution. Appropriate seeding of numerical solutions can be provided by exact analytical solutions to idealized models. For example, for an ideal transform fault, simultaneous resolution of both the locking depth D and location of the fault is optimized by placing stations at ± D 3 (~ 9 km) from the a priori fault plane. In a two-fault system, the resolution of slip partitioning is optimized by including a station midway between faults; however resolution is fundamentally limited for fault separation < 2 D (~ 30 km). 1...|$|R
40|$|In recent years, {{with the}} {{availability}} of high-frequency financial market data modeling realized volatility has become a new and innovative research direction. The construction of “observable” or realized volatility series from intra-day transaction data {{and the use of}} standard time-series techniques has lead to promising strategies for modeling and predicting (daily) volatility. In this article, we show that the residuals of commonly used time-series models for realized volatility and <b>logarithmic</b> realized <b>variance</b> exhibit non-Gaussianity and volatility clustering. We propose extensions to explicitly account for these properties and assess their relevance for modeling and forecasting realized volatility. In an empirical application for S&P 500 index futures we show that allowing for time-varying volatility of realized volatility and <b>logarithmic</b> realized <b>variance</b> substantially improves the fit as well as predictive performance. Furthermore, the distributional assumption for residuals plays a crucial role in density forecasting. Density forecasting, Finance, HAR-GARCH, Normal inverse Gaussian distribution, Realized quarticity, Realized volatility,...|$|R
40|$|Using intradaily {{high-frequency}} {{returns on}} the Dow Jones Industrial Average portfolio over the January 1993 to May 1998 period, we document {{the properties of}} interdaily `realized' volatility and fit a fractionally integrated model that accounts for the leverage e#ect directly to logarithmic realized variances. On the basis of ex ante one-day-ahead prediction criteria we find that this model yields unbiased and accurate variance, standard deviation and <b>logarithmic</b> <b>variance</b> predictions and that these predictions clearly improve upon the ones obtained by a GARCH, FIGARCH, EGARCH and FIEGARCH model. JEL Classification: C 22, C 53, G 14 Keywords: Volatility; High-Frequency Data; Long-Memory; ARCH Models; Prediction # I am grateful to the Johns Hopkins Milton S. Eisenhower Library for acquiring the used intraday data. For helpful comments I thank Pedro de Lima, Goncalo Fonseca, Louis Maccini and Robert Mo#tt. 1. Introduction Daily return volatility is indispensable to finance. Asset [...] ...|$|E
40|$|We study first-passage {{percolation}} in two dimensions, using measures mu on passage {{times with}} b:=inf supp(mu) > 0 and mu(b) =p ≥ p_c, the threshold for oriented percolation. We first show that for each such mu, {{the boundary of}} the limit shape for mu is differentiable at the endpoints of flat edges in the so-called percolation cone. We then conclude that the limit shape must be non-polygonal {{for all of these}} measures. Furthermore, the associated Richardson-type growth model admits infinite coexistence and if mu is not purely atomic the graph of infection has infinitely many ends. We go on to show that lower bounds for fluctuations of the passage time given by Newman-Piza extend to these measures. We establish a lower bound for the variance of the passage time to distance n of order log n in any direction outside the percolation cone under a condition of finite exponential moments for mu. This result confirms a prediction of Newman-Piza and Zhang. Under the assumption of finite radius of curvature for the limit shape in these directions, we obtain a power-law lower bound for the variance and an inequality between the exponents chi and xi. Comment: 32 pages, 3 figures. This is a revised version of the paper "Limit shapes outside the percolation cone. " We changed the title and included a new appendix which allows the moment assumption of the main <b>logarithmic</b> <b>variance</b> bound (Theorem 2. 5) to be reduced from 2 +beta moments (beta positive) to 2 moment...|$|E
40|$|The {{prediction}} {{and analysis}} {{of changes in the}} numbers of biological populations rest on mathematical formulations of demographic events (births and deaths) classified by the age of individuals. The development of demographic theory when birth and death rates vary statistically over time is the central theme of this work. A study of the standard Leslie model for the demographic dynamics of populations in variable environments is made. At each time interval a Leslie matrix of survival rates and fertilities of a population is chosen according to a Markov process and the population numbers in different age classes are computed. Analytical bounds are developed for the logarithmic growth rate and the age-structure of a population after long times. For a two dimensional case, it is shown analytically that a uniform distribution results for the age-structure if the survival rate from the first to the second age-class is a uniformly distributed random quantity with no serial auto correlation. Numerical studies are made which lead to similar conclusions when the survival rate obeys other distributions. It is found that the variance in the survival parameter is linearly related to the variance in the age structure. An efficient algorithm is developed for numerical simulations on a computer by considering a time sequence of births rather than whole populations. The algorithm is then applied to an example in three dimensions to calculate a sequence of births when the survival rate from the first to the second age-class is a random parameter. Numerical values for the logarithmic growth rate and the <b>logarithmic</b> <b>variance</b> for a population and the probability of extinction are obtained and then compared to the analytical results reported here and elsewhere...|$|E
40|$|This {{paper has}} {{classified}} the acceleration signals of different working states of gearbox {{based on the}} wavelet-fractal analysis. Considering the similarity of the power spectrums between bearing vibration signals and 1 /f processes signals, the principles based on wavelet-fractal analysis for gearbox fault diagnosis are explored. The improved approach mainly includes three following steps: the discrete wavelet transform (DWT) is first performed on vibration signals gathered by accelerometer from gearbox to achieve a series of detailed signals at different scales; the variances of multiscale detailed signals are then calculated; finally, the improved approach slope features are estimated from the slope of <b>logarithmic</b> <b>variances.</b> The presented features reveal an inherent structure within the power spectra of vibration signals. The effectiveness of the proposed feature was verified by experiment on gear wear diagnosis. Experimental {{results show that the}} improved approach features have the merits of high accuracy and stability in classifying different fault conditions of gearbox, and thus are valuable for machine fault diagnosis...|$|R
40|$|We {{formulate}} a theory {{to show that}} the statistics of OCT signal amplitude and intensity are highly dependent on the sample reflectivity strength, motion, and noise power. Our theoretical and experimental results depict the lack of speckle amplitude and intensity contrasts to differentiate regions of motion from static areas. Two logarithmic intensity-based contrasts, <b>logarithmic</b> intensity <b>variance</b> (LOGIV) and differential <b>logarithmic</b> intensity <b>variance</b> (DLOGIV), are proposed for serving as surrogate markers for motion with enhanced sensitivity. Our findings demonstrate a good agreement between the theoretical and experimental results for logarithmic intensity-based contrasts. Logarithmic intensity-based motion and speckle-based contrast methods are validated and compared for in vivo human retinal vasculature visualization using high-speed swept-source optical coherence tomography (SS-OCT) at 1060 nm. The vasculature was identified as regions of motion by creating LOGIV and DLOGIV tomograms: multiple B-scans were collected of individual slices through the retina and the <b>variance</b> of <b>logarithmic</b> intensities and differences of logarithmic intensities were calculated. Both methods captured the small vessels and the meshwork of capillaries associated with the inner retina in en face images over 4 mm 2 in a normal subject...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedIntensity scintillation in {{a laser beam}} at 0. 63 micrometers in the marine boundary layer has been studied over a 4. 3 kilometer horizontal path across Monterey Bay and also from shore to ship at San Nicolas Island. Optical and micrometeorological measurements of the refractive index structure constant, CN, agree to approximately one standard deviation giving values in the range 2. 0 x 10 - 8 to 5. 0 x 10 - 1 / 3 (meters). Optical and meteorological data correlation improved as mean wind speed increased. Evidence was found to support the reported existence of a saturation region for the laser beam <b>logarithmic</b> amplitude <b>variance.</b> [URL] United States Nav...|$|R
40|$|The neural {{activity}} {{of the human brain}} recorded non-invasively is sufficient to control the external machine, if an advanced method of signal analysis and feature extraction are used in combination with the machine learning techniques either supervised or unsupervised. The brain machine interface (BMI) pertains to manipulation or operation of the external machine as per thought of a user and such machine is called as thought controlled machine. Thus there is a direct communication at the level of thought for the action between both human and machine via a computer. The BMI is most commonly known as the brain computer interface (BCI) because there is a direct communication between the brain and the external machine via a computer, which analyze and interpret the incoming physiological signals (electroencephalograms), which contain the shadow of the mental activity and the different types of artifacts. A multi-channel recordings of the electromagnetic waves emerging from the neural currents in the brain generate a large amounts of the EEG data. A suitable feature extraction and classification methods are useful to generate a control command for controlling the external machine. The concept of thought generation in the human brain is a very complex phenomenon and has not been accessible yet directly as the biological processing and the format of the signal in the brain is not clearly understood. However, the levels of thoughts and types of thoughts have been broadly investigated as change in the rhythm and the patterns of the EEG signals capture at the surface of the skull. Although, the EEG signals are not the direct access to the levels of thoughts and thought related actions but it represents the shadow of it and hence to some extent thought related patterns can be inferred and used for a controlling action. However 100 % thought recognition will remain untouched for either of the statistical classifier using a single feature vector. For the real time control, the information transfer rate is not sufficient to control the external machine such as a robot, a electric wheelchair and a computer cursor. The objective of this thesis is to examine the areas of the BCI/BMI with respect to the classification accuracy and the information transfer rate. How can the classification accuracy and the information transfer rate be enhanced for the real time control of the external machine? How can the processing time for the thought recognition and a command generation be reduced for the real time control of the external machine? These are the fundamental problems and require some advanced signal analysis and feature extraction methods, which will efficiently solve these fundamental problems. In order to improve the classification accuracy, reduce the processing time and maximize the information transfer rate, the author studies and investigates a novel approach of signal processing for enhancing a recorded multi-channel EEG signals. Firstly, {{the author would like to}} clear that this work is organized into two situations. Firstly, a novel approach of signal processing is investigating to achieve perfect classification, which is based on the batch processing of the signals, where the entire duration of the EEG signals is applied in order to decode the brain activity from a three channel/more than a three channel EEG signals. Secondly, a novel feature extraction method is investigating to generate the biofeedback to a user, which is based on the sequential processing of the EEG signals for the continuous classification output, where the author has an aim to classify the incoming every time sample of the EEG signals into its classes. The continuous classification output is useful to provide the feedback to a user at the time of the data acquisition/ controlling the external machine. The aims and scope of our work is also in same context. Blind Signal Separation (BSS) based on the Independent component analysis (ICA) has emerged as a potential engineering solution for the biomedical signal analysis. This thesis also addresses the developments and application of the ICA/BSS based algorithm for the blind separation of the instantaneous mixing of the EEG signals captured by the three sensors and more than three sensors. This thesis presents an extension of the ICA/BSS methods in context with the averaged EEG/or a single trial EEG signal for preprocessing the recorded EEG signals before the feature extraction. Right and left hand movement imageries have many applications in the BCI system. Still the developed method does not claim 100 % classification accuracy. An accurate pattern extraction depends on the signal preprocessing methodology. Signal separation from a multi-channel signals have been the most interesting topic of research from a decade. The ICA is a tool of the BSS which exploits a higher order statistics for the signal separation from the mixed sensor signals. A general approach of blind signal separation is based on the standard ICA/BSS where the classical assumption of the ICA/BSS exist. In some application the general ICA/BSS model is relaxed where assumption of the mutual independence /temporally uncorrelated sources are violated. A blind signal separation approach based on the relaxed ICA/BSS model is called as the subband ICA/BSS where the classical ICA/BSS model is relaxed by considering the independent/dependent signal sources. Generally the standard ICA/BSS approach is the most common method for uncorrelating a recorded single trial multi-channel EEG signals based on a second order statistics. It is well known that the EEG signals contain information in a certain frequency bands called as rhythm. These subbands information can be easily obtained by applying many digital filters of the narrow frequency bands. The performance of the subband BSS is more significant where the information is contained in the subbands. Biomedical signals are generally contained information in the subbands of the narrow frequency bands. Either the standard BSS or the subband BSS approach is yet a key problem for a single trial EEG signals analysis for the signal separation. This thesis also presents a novel application of the subband BSS over the standard BSS for estimating the separating matrices for the signal separation from a single trial EEG signals and investigates why the subband BSS approach has an effective impact on the classification of the mental activities. The algorithms chosen for the standard/subband BSS approach are the AMUSE (Algorithm for Multiple Unknown Signals Extraction) and the SOBI (Second Order Blind Identification) and the SEONS (Second Order Non-stationary Source) algorithms based on a second order characviii teristics of the source signals. In the standard BSS one considers the measurement signals for estimating the separating matrices while in the subband BSS one considers many non-overlapping narrow frequency bands for estimating the separating matrices for all subbands. From these subbands separating matrices one create the global mixing-demixing matrices for estimating the blind performance index (BPI) to estimate the independent subbands. For every global matrix one estimates the value of the BPI, the global matrices corresponding to the minimum value of the BPI are selected. The corresponding subbands separating matrices are considered as the final separating matrices. The estimated separating matrices depend on the BPI in the subband BSS. The classical BPI is totally based on the blind concept for estimating the independent subbands. In this study author considers the two mental tasks either the imagination of the left hand or the imagination of the right hand movement. It is known that the human brain has an asymmetrical structure. Usually the ICA/BSS is based on the wide classes of the unsupervised learning algorithms. The objective of this thesis is to address the effect of asymmetry of the human cortex on the classification accuracy. Depending on the applications, one assumes the intrinsic property of the signals and accordingly one applies the appropriate ICA/BSS algorithms. The author assumes that the EEG signals are stationary over a recording duration and it is a time correlated. Before the feature extraction, the recorded EEG signals should be uncorrelated. Blind source separation algorithms based on the time structure of the signals are a good candidate for the temporal uncorrelation. The algorithms chosen for the standard ICA/BSS, are the AMUSE and the SOBI algorithms based on a second order characteristics of the source signals. The AMUSE is the first BSS algorithm which apply a single time delay covariance matrix in order to estimate the separating matrices while the SOBI apply many time delay covariance matrices and all delayed matrices are subjected to the simultaneous diagonalization by applying the joint approximate diagonalization (JAD) procedure. It is well known that the cerebrum consists of the left and right hemispheres. Each hemisphere of the brain is morphologically different. As per literature survey, 90 % people are right handed and only 10 % are the left handed. Right handed subject is habitual to used the right hand for many kinds of manual tasks. It is also well known that the left hemisphere of the brain controls the right body movement and vice versa. We are considering the primary motor cortex area of the cerebral hemisphere for recording the EEG signals. Basically the primary motor cortex area consists of crest of folded cortical tissues called as precentral gyrus and postcentral gyrus. Both gyri have been separated by a grooves that divide gyri from one another is called as central sulcus. These gyri are also functionally significant in that the precentral gyrus contains the primary motor cortex important for the control of movement and the postcentral gyrus contains the primary somatic sensory cortex which is important for the body senses. By comprehensive survey of the literature, it has been found that the left hemisp here central sulcus is larger and deeper compared to that of the right hemisphere for the right handed people. We infer from the above facts that the volumetric areas of the left hemisphere over primary motor cortex will be greater than that of the right hemisphere for the right handed people. A large numbers of dipole sources will be active in the left hemisphere compared to the right hemisphere in right hander for right hand movement imagination. That’s why the magnitude of the EEG signals in the left hemisphere for the right hand movement will be greater than that of the right hemisphere. The event related desynchronization (ERD) and the event related synchronization (ERS) is the fundamental phenomenon of the motor cortical neurons and there will be a magnitude difference between the mean power of left hemisphere (ERD) and the right hemisphere (ERS) for the right hand movement imagination. For the left hand movement imagination, there will be also a magnitude difference between the mean power of the left hemisphere (ERS) and the right hemisphere (ERD) over the primary motor cortex areas in certain frequency bands. The magnitude asymmetry is due to the structural/functional differences between both hemispheres of the cortex for the unilateral movement. This magnitude asymmetry is implicitly related to the estimated separating matrix. Firstly we have calculated the asymmetry coefficient between the separating matrix of the left and right classes based on the frobenius matrix norm. In this study, author considers an asymmetry of the human cortex along with both a stationary/or non-stationary as well as an independence/or an uncorrelatedness over the averaged EEG signals and the author proposes the hybrid approach of the signal preprocessing methods before the feature extraction under the supervised learning algorithm. A filter bank approach of the discrete wavelet transform (DWT) is used to exploit a non-stationary characteristics of the EEG signals and it decomposes the EOG corrected raw EEG signals into the subbands of different center frequencies are known as rhythms. A post processing of the selected subband by the AMUSE algorithm (a second order statistics based ICA/BSS algorithm) provides the separating matrix for each class of the movement imagery. In the subband domain the orthogonality as well as orthonormality criteria over the whitening matrix and the separating matrix do not come respectively. It has been observed that the ratio between the norms of the left and the right classes separating matrices should be different for better discrimination between these two classes. The alpha/beta band asymmetry coefficients between the separating matrices of the left and the right classes will provide the condition to select an appropriate multiplier. So we modify the estimated separating matrix by an appropriate multiplier in order to get the required asymmetry coefficient and extend the AMUSE algorithm in the subband domain. The desired subband is further subjected to the updated separating matrices to extract the subband subcomponents from each class. The extracted subband subcomponent sources are further subjected to the feature extraction (Power spectral density) step followed by the linear discriminant analysis (LDA). With modification of the separating matrix at the desired subband, the author ensures almost 100 % classification accuracy. In this thesis work, the author also addresses the performance of a common cpatial pattern (CSP) on the rhythmic band information selected from the spatio-temporally uncorrelated signals. For spatio-temporal uncorrelation, the author applies the spatial filter based on the standard BSS approach such as the AMUSE, the SOBI and the SEONS algorithm. The standard BSS approach are firstly applied to the EOG corrected raw EEG signals to make the EOG corrected raw EEG signals temporally uncorrelated. One can not apply the CSP method for the feature extraction from the temporally uncorrelated sources because each temporally uncorrelated source has a unity variance. It is well known among the BCI community that the brain signals contain information in a narrow frequency bands. A narrow band signals from each temporally uncorrelated source will be highly correlated. Any linear digital filter/or signal reconstruction algorithm can be used to select the frequency band information from each spatio-temporally uncorrelated source. The selected rhythmic band information is further processed by the CSP method for the feature extraction. The <b>logarithmic</b> <b>variance</b> of the first and last new time series EEG data over the rhythmic band signals of the temporally uncorrelated sources are subjected to the LDA. The reported results confirm that the proposed method outperforms over the conventional CSP method. In this thesis work, the author also exploits the effectiveness of the subband decomposition based ICA/BSS approach in context with the averaged EEG signals over the standard ICA/BSS approach. A filter bank (FB) approach of the discrete wavelet transform (DWT) is used to exploit a non-stationary characteristics of the EOG corrected raw EEG signals and it decomposes the EOG corrected raw EEG signals into the subbands of different center frequencies. The subbands are further subjected to a second order statistics based BSS algorithms, the AMUSE, the SOBI and the SEONS algorithms to estimate the separating matrix/or un-mixing matrix separately. The global matrices over the series of separating matrices are made to measure the band performance index (BPI). The estimated separating matrices, from the minimum value of the BPI over the global matrices, are applied to the EOG corrected raw EEG signals for the extraction of the independent/ or uncorrelated sources. The author proposes a novel method for the automatic selection of the independent/uncorrelated components among the multiple sources, which have a maximum discriminatory information related to the movement imagery. Recently the concept of biofeedback has been introduced at the time of the EEG measurements /or at the time of a real time control of the external machine using the thought of a user. The training of the user is necessary to enhance the classification accuracy before the real time control of the external machine. It requires the sequential (sample by sample) processing of the incoming physiological signals. The author has an aim to classify every incoming samples to its actual classes. This is also an very attractive, a very challenging and very much interesting research area. The objective of the BCI competition II was to provide the continuous classification output for the incoming samples of the EEG signals and find the time instant where there is a maximum mutual information. Several signal processing and feature extraction methods have been investigated to maximize the mutual information (MI), the Cohen’s kappa coefficient, the classification accuracy for the biofeedback. This thesis work presents a novel feature extraction method for the continuous classification of the incoming samples into the right and the left hand motor imageries to provide the biofeedback at the time of the EEG data acquisition or to train the user. A novel features temporal variations of relative spectral power (TRSP) is proposed in the motor rhythm and applied to the Graze dataset of the BCI competitions II. The autoregressive (AR) coefficients are also considered in this thesis work, where author apply Burg and YulearWalker methods for the AR coefficients estimation. The extracted features are subjected to the LDA, whose design is based on the Fisher criterion and the Mahanlanobis distance based approach, to classify every time samples of the right and the left hand movement imageries data. The obtained result has been compared with that of the BCI competitions II on the same data set in terms of the mutual information and the misclassification rate. The AR coefficients estimation with Yulear Walker method outperforms the winner of the BCI competitions II and achieves the maximum value of MI 0. 69 bits at 5. 21 s while the TRSP achieves the maximum value of MI 0. 54 bits at 5. 24 s. TRSP outperforms the second winner who has a maximum value of MI 0. 46 bits at 5. 05 s. The effectiveness of all proposed methods has been evaluated by the BCI performance evaluation parameters: the classification accuracy (ACC), the ERR rate, the Cohen’s Kappa coefficient () and the mutual information (MI). 電気通信大学 201...|$|E
40|$|We {{demonstrate}} an intensity-based motion sensitive method, called differential <b>logarithmic</b> intensity <b>variance</b> (DLOGIV), for 3 D microvasculature {{imaging and}} foveal avascular zone (FAZ) visualization {{in the in}} vivo human retina using swept source optical coherence tomog. (SS-OCT) at 1060 nm. A motion sensitive SS-OCT system was developed operating at 50, 000 A-lines/s with 5. 9 μm axial resoln., and used to collect 3 D images over 4 mm^ 2 in a normal subject eye. Multiple B-scans were acquired at each individual slice through the retina and the variance of differences of logarithmic intensities {{as well as the}} differential phase variances (DPV) was calcd. to identify regions of motion (microvasculature). En face DLOGIV image were capable of capturing the microvasculature through depth with an equal performance compared to the DPV...|$|R
40|$|Modeling of {{volatility}} {{has been}} felt {{one of the}} major academic contributions in Indian commodity futures market. We have selected black pepper as a commodity for estimating volatility and its spillover incorporating a series of models. We have employed models with their specifications, namely, GARCH (2, 2), EGARCH (2, 2), EGARCH (3, 3), CGARCH (1, 1), MGARCH (Diagonal VECH and BEKK) for both the spot and futures return-series of the commodity. Study reveals that bidirectional spillover is captured under GARCH (2, 2) model whereas unidirectional spillover is found under EGARCH (2, 2) model and results obtained through EGARCH (3, 3) are not impressive. News impact curve depicts the steeper movement on the <b>logarithmic</b> conditional <b>variance</b> of futures and spot-return series due to ‘positive shocks’ and rather than to ‘negative shock’. Conditional correlation is also found dynamic and the correlation between spot and futures returns of pepper changes temporally. volatility, spillover, asymmetric effect, news impact...|$|R
40|$|It was {{evaluated}} {{the heterogeneity of}} components of phenotypic variance {{and its effects on}} the heritability and repeatability estimates for milk yield in Holstein cattle. The herds were grouped according to their level of production (low, medium and high) and evaluated in the non-transformed, square-root and <b>logarithmic</b> scale. <b>Variance</b> components were estimated using a restricted maximum likelihood method based on an animal model that included fixed effects of herd-year-season, and as covariates the linear effect of lactation duration and the linear and quadratic effects of cow's age at calving and the random direct additive genetic, permanent environment and residual effects. In the non-transformed scale all the variance components were heterogeneous. on this scale, residual and phenotypic variance components were associated positively with the level of production while in logarithmic scale that association was negative. Estimates of heritability were more affected than the repeatability for the phenotypic variance heterogeneity and their components. The of selection process efficiency for milk production could be affected by the level of production which was considered for genetic parameters estimation...|$|R
40|$|Abstract. Quadtrees {{constitute}} a classical data structure for storing and accessing collections of points in multidimensional space. It is proved that, in any dimension, {{the cost of}} a random search in a randomly grown quadtree has <b>logarithmic</b> mean and <b>variance</b> and is asymptotically distributed as a normal variable. The limit distribution property extends to quadtrees of all dimensions a result only known so far to hold for binary search trees. The analysis is based on a technique of singularity perturbation that appears to be of some generality. For quadtrees, this technique is applied to linear differential equations satisfied by intervening bivariate generating functions 1...|$|R
40|$|The {{polarized}} piezoreflection {{spectra of}} the first singlet system of anthracene and their Kramers-Kronig transforms are reported. Exciton energy strain shifts are reported for the b-polarized and for the ac-polarized factor group components. These values are shown {{to be in good}} agreement with related experimental data and with theoretical calculations. The piezomodulation spectra significantly enhance structure and are also sensitive to the extent of crystal coupling. Deviations in the measured spectra are observed in the a-polarized spectra from that calculated for the <b>logarithmic</b> derivative. These <b>variances</b> are attributed to coupling with the second singlet of anthracene or the presence of the 1 Lb transition...|$|R
40|$|This article {{examines}} the impact of serial correlation in high frequency returns on the realized variance measure. In particular, it is shown that the realized variance measure yields a biased estimate of the conditional return variance when returns are serially correlated. Using 10 years of FTSE- 100 minute by minute data we demonstrate that a careful choice of sampling frequency is crucial in avoiding substantial biases. Moreover, {{we find that the}} autocovariance structure (magnitude and rate of decay) of FTSE- 100 returns at different sampling frequencies is consistent with that of an ARMA process under temporal aggregation. A simple autocovariance function based method is proposed for choosing the “optimal ” sampling frequency, that is, the highest available frequency at which the serial correlation of returns has a negligible impact on the realized variance measure. We find that the <b>logarithmic</b> realized <b>variance</b> series of the FTSE- 100 index, constructed using an optimal sampling frequency of 25 minutes, can be modelled as an ARFIMA process. Exogenous variables such as lagged returns and contemporaneous trading volume appear to be highly significant regressors and are able to explain {{a large portion of the}} variation in daily realized variance...|$|R
40|$|Quadtrees {{constitute}} a classical data structure for storing and accessing collections of points in multidimensional space. It is proved that, in any dimension, {{the cost of}} a random search in a randomly grown quadtree has <b>logarithmic</b> mean and <b>variance</b> and is asymptotically distributed as a normal variable. The limit distribution property extends to quadtrees of all dimensions a result only known so far to hold for binary search trees. The analysis is based on a technique of singularity perturbation that appears to be of some generality. For quadtrees, this technique is applied to linear differential equations satisfied by intervening bivariate generating functions 1 Introduction This work concerns itself with an analysis in distribution of the cost of retrieving data from a randomly grown quadtree structure based on a combination of complex asymptotic and analytic probabilistic methods. Quadtrees are a well known data structure for multidimensional retrieval problems discovered by Fink [...] ...|$|R
40|$|The density {{variance}} - Mach number {{relation of}} the turbulent interstellar medium is relevant for theoretical models of the star formation rate, efficiency, and the initial mass function of stars. Here we use high-resolution hydrodynamical simulations with grid resolutions of up to 1024 ^ 3 cells to model compressible turbulence in a regime similar to the observed interstellar medium. We use Fyris Alpha, a shock-capturing code employing a high-order Godunov scheme to track large density variations induced by shocks. We investigate the robustness of the standard relation between the <b>logarithmic</b> density <b>variance</b> (sigma_s^ 2) and the sonic Mach number (M) of isothermal interstellar turbulence, in the non-isothermal regime. Specifically, we test ideal gases with diatomic molecular (gamma = 7 / 5) and monatomic (gamma = 5 / 3) adiabatic indices. A periodic cube of gas is stirred with purely solenoidal forcing at low wavenumbers, leading to a fully-developed turbulent medium. We find that as the gas heats in adiabatic compressions, it evolves along the relationship in the density variance - Mach number plane, but deviates significantly from the standard expression for isothermal gases. Our main result is a new density variance - Mach number relation that takes the adiabatic index into account: sigma_s^ 2 = ln { 1 +b^ 2 *M^[(5 *gamma+ 1) / 3]} and provides good fits for b*M 1. We conclude that this new relation for adiabatic turbulence may introduce important corrections to the standard relation, if the gas is not isothermal. Comment: 11 pages, 9 figures, accepted for publication in MNRA...|$|R
40|$|A {{number of}} {{observations}} {{are made on}} Hofstadter’s integer sequence defined by Q(n) = Q(n − Q(n − 1)) + Q(n − Q(n − 2)), for n> 2, and Q(1) = Q(2) = 1. On short scales the sequence looks chaotic. It turns out, however, that the Q(n) can be grouped into a sequence of generations. The k-th generation has 2 k members which have “parents” mostly in generation k − 1, and a few from generation k − 2. In this sense the sequence becomes Fibonacci type on a <b>logarithmic</b> scale. The <b>variance</b> of S(n) = Q(n) − n/ 2, averaged over generations, is ≃ 2 α k, with exponent α = 0. 88 (1). The probability distribution p ∗ (x) of x = R(n) = S(n) /n α, n>> 1, is well defined and strongly non-Gaussian, with tails well described by the error function erfc. The probability distribution of xm = R(n) − R(n − m) is given by In his famous book Gödel, Escher, Bach: an Eternal Golden Brai...|$|R
40|$|In {{a recent}} paper, Gutiérrez et al. (Nonlinear Process Geophys 15 (1) : 109 - 114, 2008) {{introduced}} a new characterization of spatiotemporal error growth - the so called mean - <b>variance</b> <b>logarithmic</b> (MVL) diagram - and applied it to study ensemble prediction systems (EPS); in particular, they analyzed single-model ensembles obtained by perturbing the initial conditions. In the present work, the MVL diagram is applied to multi-model ensembles analyzing also the effect of model formulation differences. To this aim, the MVL diagram is systematically applied to the multi-model ensemble produced in the EU-funded DEMETER project. It is shown that the shared building blocks (atmospheric and ocean components) impose similar dynamics among different models and, thus, contribute to poorly sampling the model formulation uncertainty. This dynamical similarity {{should be taken into}} account, at least as a pre-screening process, before applying any objective weighting method. © Springer-Verlag 2008. This work was partially supported by the Spanish Ministry of Education and Science through grants CGL- 2007 - 64387 /CLI and CGL 2005 - 06966 -C 07 - 02 /CLI. J. F. is supported by the Spanish Ministry of Education and Science through the Juan de la Cierva program. Peer Reviewe...|$|R
40|$|This article {{examines}} the impact of serial correlation in high frequency returns on the realized variance measure. In particular, it is shown that the realized variance measure yields a biased estimate of the conditional return variance when returns are serially correlated. Using 10 years of FTSE- 100 minute by minute data we demonstrate that a careful choice of sampling frequency is crucial in avoiding substantial biases. Moreover, {{we find that the}} autocovariance structure (magnitude and rate of decay) of FTSE- 100 returns at different sampling frequencies is consistent with that of an ARMA process under temporal aggregation. A simple autocovariance function based method is proposed for choosing the “optimal” sampling frequency, that is, the highest available frequency at which the serial correlation of returns has a negligible impact on the realized variance measure. We find that the <b>logarithmic</b> realized <b>variance</b> series of the FTSE- 100 index, constructed using an optimal sampling frequency of 25 minutes, can be modelled as an ARFIMA process. Exogenous variables such as lagged returns and contemporaneous trading volume appear to be highly significant regressors and are able to explain {{a large portion of the}} variation in daily realized variance. [...] Dieser Artikel untersucht die Auswirkungen von autokorrelierten Erträgen auf das Maß der realisierten Varianz bei hochfrequenten Daten über die Erträge. Es wird gezeigt, dass die realisierte Varianz ein verzerrter Schätzer für die bedingte Varianz der Erträge bei Vorliegen von Autokorrelation ist. Unter Verwendung eines zehnjährigen Datensatzes von Minutendaten des FTSE- 100 wird dargestellt, dass eine sorgfältige Auswahl der Stichprobenfrequenz unabdingbar zur Vermeidung von Verzerrungen ist. Eine einfache Methode zur Bestimmung der optimalen Stichprobenfrequenz, basierend auf der Autokovarianzfunktion, wird vorgeschlagen. Diese ergibt sich als die höchste Frequenz, bei der die vorhandene Autokorrelation noch einen vernachlässigbaren Einfluss auf das Maß der realisierten Varianz hat. Für den betrachteten Datensatz ergibt sich eine optimale Frequenz von 25 Minuten. Unter Verwendung dieser Frequenz können die logarithmierten Erträge des FTSE- 100 als ARFIMA Prozess modelliert werden. High frequency data,realized return variance,market microstructure,temporal aggregation,long memory,bootstrap...|$|R
40|$|The {{atmospheric}} {{refractive index}} consists of both real and imaginary parts. The intensity of refractive index fluctuations is generally expressed as the refractive index structure parameter, {{with the real}} part reflecting the strength of atmospheric turbulence and the imaginary part reflecting absorption in the light path. A large aperture scintillometer (LAS) {{is often used to}} measure the structure parameter of the real part of the atmospheric refractive index, from which the sensible and latent heat fluxes can further be obtained, whereas the influence of the imaginary part is ignored or considered noise. In this theoretical analysis study, the relationship between <b>logarithmic</b> light intensity <b>variance</b> and the atmospheric refractive index structure parameter (ARISP), as well as that between the logarithmic light intensity structure function and the ARISP, is derived. Additionally, a simple expression for the imaginary part of the ARISP is obtained which can be conveniently used to determine the imaginary part of the ARISP from LAS measurements. Moreover, these relationships provide a new method for estimating the outer scale of turbulence. Light propagation experiments were performed in the urban surface layer, from which the imaginary part of the ARISP was calculated. The experimental results showed good agreement with the presented theory. The results also suggest that the imaginary part of the ARISP exhibits a different diurnal variation from that of the real part. For the wavelength of light used (0. 62 μm), the variation of the imaginary part of the ARISP is related to both the turbulent transport process and the spatial distribution characteristics of aerosols...|$|R
40|$|It {{has been}} deemed {{important}} to normalize flow-mediated dilation (FMD), {{a marker of}} endothelial function, for between-subject differences in the eliciting shear rate (SR) stimulus. Conventionally, FMD is divided by the area under {{the curve of the}} SR stimulus. In the context of a cross-sectional comparison across different age cohorts, we examined whether this ratio approach adhered to established statistical assumptions necessary for reliable normalization. To quantify brachial artery FMD and area under the curve of SR, forearm cuff inflation to suprasystolic pressure was administered for 5 min to 16 boys aged 10. 9 yr (SD 0. 3), 48 young men aged 25. 3 yr (SD 4. 2), and 15 older men aged 57. 5 yr (SD 4. 3). Mean differences between age groups were statistically significant (P < 0. 001) for nonnormalized FMD [children: 10. 4 % (SD 5. 4), young adults: 7. 5 % (SD 2. 9), older adults: 5. 6 % (SD 2. 0) ] but not for ratio-normalized FMD (P = 0. 10). Moreover, all assumptions necessary for reliable use of ratio-normalization were violated, including regression slopes between SR and FMD that had y-intercepts greater than zero (P < 0. 05), nonlinear and unstable relations between the normalized ratios and SR, skewed data distributions, and heteroscedastic <b>variance.</b> <b>Logarithmic</b> transformation of SR and FMD before ratio calculation improved adherence to these assumptions and resulted in age differences similar to the nonnormalized data (P = 0. 03). In conclusion, although ratio normalization of FMD altered findings about age differences in endothelial function, this could be explained by violation of statistical assumptions. We recommend that exploration of these assumptions should be routine in future research. If the relationship between SR and FMD is generally found to be weak or nonlinear or variable between samples, then ratio normalization should not be applied...|$|R
40|$|The plane {{parallel}} homogeneous (PPH) bias {{is defined}} as the difference between the plane parallel cloud albedo, calculated for homogeneous cloud optical depth distributions, and the independent pixel (IP) albedo, which allows for optical depth variability, but assumes that individual cloudy columns transfer solar radiation as plane parallel slabs (horizontal photon transport is neglected). Estimates of the PPH bias from extensive Advanced Very High Resolution Radiometer (AVHRR) observations are provided for areas similar in size to weather and climate model gridboxes. The goal is to investigate conditions and assumptions influencing the PPH bias, and to suggest methods to correct for it. Visible PPH biases vary from about 0. 02 to 0. 30, depending on area size, view/sun geometry, and other factors influencing optical depth retrievals and albedo calculations. Broadband PPH biases are slightly smaller than visible biases; broadband absorptance biases are about an order of magnitude smaller. Approximate estimates of the bias in broadband reflected flux at cloud top often exceed 30 Wm$ sp{- 2 }$ for near-nadir measurements, suggesting that the assumption of cloud homogeneity produces errors that cannot be ignored in climate studies. Solar geometry affects the PPH bias not only through the direct dependence of albedo on solar zenith angle, but also through systematic changes in the apparent mean and variance of optical depth arising from the neglect of 3 -d effects in satellite radiance inversions. PPH biases decrease with data resolution, increase when atmospheric radiative effects are accounted for in optical depth retrievals, and are only slightly affected by water cloud microphysics. PPH biases are also shown to differ substantially between the forward and backward scattering directions, and between large-scale models with and without provision for fractional cloudiness. A large fraction of the PPH bias is removable by: (1) adjusting regionally averaged optical depths with the reduction factor of Cahalan et al. (1994 a), and (2) fitting observed optical depth distributions with lognormal and (to a lesser degree) gamma distributions. These methods require the <b>logarithmic</b> mean and <b>variance</b> of optical depth, which are parameterized as a function of regional mean optical depth and cloud fraction, quantities routinely available in climate models...|$|R

