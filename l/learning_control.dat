1365|3217|Public
5000|$|... nD systems, {{iterative}} <b>learning</b> <b>control,</b> repetitive processes, ...|$|E
50|$|MLC comprises, for instance, {{neural network}} control, genetic {{algorithm}} based control, genetic programming control,reinforcement <b>learning</b> <b>control,</b> and has methodolical overlaps with other data-driven control,like artificial intelligence and robot control.|$|E
50|$|Machine <b>learning</b> <b>control</b> (MLC) is a {{subfield}} {{of machine}} learning and control theorywhich solves optimal control problems with methods of machine learning.Key applications are complex nonlinear systemsfor which linear control theory methods are not applicable.|$|E
50|$|These changes {{cannot be}} {{controlled}} by instructing a person to speak as they would in silence, though people can <b>learn</b> <b>control</b> with feedback.|$|R
5000|$|GamePro {{gave the}} arcade version a perfect score, citing easy to <b>learn</b> <b>controls,</b> the {{excitement}} added by the surround sound and [...] "rumble seat", and the detailed visuals. They commented that [...] "While T-MEKs graphics are not groundbreaking, they sizzle with depth and realism." ...|$|R
50|$|Tutorial: <b>Learn</b> the <b>controls</b> {{and how to}} play.|$|R
5000|$|Meanwhile, Willow is {{studying}} with Giles in Westbury, England. She studies magic and meditation with a coven of Wiccans that Giles knows. She is <b>learning</b> <b>control,</b> but feels frightened and distraught because she [...] "killed people".|$|E
50|$|Bernd Rainer Noack (born 17 February 1966, Korbach, Germany) is a German scientist. His {{research}} and teaching area is closed-loop flow control for transport systems. Focus is placed on machine <b>learning</b> <b>control</b> and model-based nonlinear control using reduced-order modelling and nonlinear (attractor) closures. Currently investigated configurations include wakes, mixing layers, jets, combustor mixing and aerodynamic flows around cars and airplanes. Another main area is thermodynamic formalisms for turbulence modeling.|$|E
5000|$|Iterative <b>Learning</b> <b>Control</b> (ILC) is {{a method}} of {{tracking}} control for systems that work in a repetitive mode. Examples of systems that operate in a repetitive manner include robot arm manipulators, chemical batch processes and reliability testing rigs. In each of these tasks the system is required to perform the same action {{over and over again}} with high precision. This action is represented by the objective of accurately tracking a chosen reference signal [...] on a finite time interval.|$|E
40|$|Many {{different}} {{machine learning}} techniques {{have been used}} to <b>learn</b> <b>control</b> knowledge for planning. However, subsymbolic techniques have not been widely used, in particular artificial neural networks. In this paper, a feedforward neural network (FNN) will be used to learn a heuristic function for improving planning efficiency. Th...|$|R
2500|$|... hèbôn enkratès, (as young men, <b>learn</b> to <b>control</b> the passions) ...|$|R
5000|$|<b>Learn</b> to <b>control</b> your {{ejaculation}} so it {{does not}} happen prematurely.|$|R
50|$|In studies {{investigating}} {{the effectiveness of}} mastery <b>learning,</b> <b>control</b> and experimental groups were not always valid. Experimental groups typically consisted of courses that were developed {{to adhere to the}} best principles of mastery. However, control groups were sometimes existing classes to use as a comparison. This poses a problem since {{there was no way to}} test the effectiveness of the control group to begin with - it could have been a poorly constructed course being compared against a strictly designed mastery course.|$|E
50|$|Repetitive Control is {{a control}} method {{developed}} {{by a group of}} Japanese scholars in 1980s. It is based on the Internal Model Principle and used specifically in dealing with periodic signals, for example, tracking periodic reference or rejecting periodic disturbances. The repetitive control system has been proven to be a very effective and practical method dealing with periodic signals. Repetitive control has some similarities with iterative <b>learning</b> <b>control.</b> The differences between these two methods can be found in Gao, and Doyle. 2009.|$|E
50|$|This {{project was}} born to fulfill {{the needs of a}} {{university}} group, mainly focused to have a flexible platform for <b>learning</b> <b>control</b> systems design, allowing the students to test their programs remotely, over the Internet. Leaving the first wishful thinking and going to the real implementation gave rise to the alpha version of RTAI-XML, that showed the potential impact of the basic idea of a net separation of hard and soft real-time tasks in the programmation logic. What was necessary to assure that students could not crash the RT process, is now becoming a new RTAI paradigm.|$|E
5000|$|Agency is <b>learning</b> to <b>control</b> both one's own {{activity}} and external events ...|$|R
5000|$|... "As children, learn good manners.:As young men, <b>learn</b> to <b>control</b> the passions.|$|R
60|$|ANNYS [She resents herself meekly. Apologising generally.] I must <b>learn</b> to <b>control</b> myself.|$|R
5000|$|Originally from South Africa, Grove began {{competing}} in rugby {{at a young}} age. Having attended eight different schools growing up, Grove was frequently bullied being the [...] "new kid," [...] and often got into fights as a result. In 1996, Grove moved to London, England where {{he worked as a}} personal trainer, sports therapist, and also worked as a bouncer at an Irish nightclub in South London. In 2000, after Grove and a fellow bouncer were involved in a massive brawl at the nightclub, the two men were forced to take Goju-ryu classes at Daigaku Karate Kai as a way of <b>learning</b> <b>control</b> and self-restraint. Grove soon developed a love for the martial art, which, as a combat-style karate, involves throws and submission wrestling. A year after receiving his black belt in 2005, Grove transitioned into Sambo, Brazilian jiu-jitsu and mixed martial arts.|$|E
5000|$|Borkar's {{researches}} {{were mainly}} {{in the fields of}} Stochastic control, <b>Learning</b> <b>control</b> theory and random processes and he is known to have introduced a new convex analytical paradigm based upon occupation measures. His work is reported to have assisted in bettering the understanding of Stochastic control issues and elucidated adaptive control schemes with regard to asymptotic optimality. He has worked on Distributed computation, Multiple timescales, Approximation and learning algorithms, Multiagent problems and Small noise limits and developed a protocol which used conditional version of importance sampling for the estimation of Markov chain averages; the scheme was later confirmed by a team of scientists from Massachusetts Institute of Technology. His researches have been documented in several peer-reviewed articles; and Google Scholar, an online article repository of scientific articles has listed 373 of them. Besides, he has published 5 books viz. Optimal control of diffusion processes, Probability Theory: An Advanced Course, Stochastic Approximation: A Dynamical Systems Viewpoint, Hamiltonian Cycle Problem and Markov Chains and Ergodic Control of Diffusion Processes. He has also contributed chapters to several books edited by others and has delivered a number of invited or keynote addresses including the address on Control and Optimization at the International Congress of Mathematicians held in Madrid in August 2006 and the Lecture on Probability and Stochastic Processes XI organized by Indian Statistical Institute, Delhi in November 2016.|$|E
5000|$|The term [...] "hyperheuristics" [...] {{was first}} coined in a 2000 {{publication}} by Cowling and Soubeiga, who {{used it to}} describe the idea of [...] "heuristics to choose heuristics". They used a [...] "choice function" [...] machine learning approach which trades off exploitation and exploration in choosing the next heuristic to use. Subsequently, Cowling, Soubeiga, Kendall, Han, Ross and other authors investigated and extended this idea {{in areas such as}} evolutionary algorithms, and pathological low level heuristics. The first journal article to use the term appeared in 2003. The origin of the idea (although not the term) {{can be traced back to}} the early 1960s and was independently re-discovered and extended several times during the 1990s. In the domain of Job Shop Scheduling, the pioneering work by Fisher and Thompson, hypothesized and experimentally proved, using probabilistic learning, that combining scheduling rules (also known as priority or dispatching rules) was superior than any of the rules taken separately. Although the term was not then in use, this was the first [...] "hyper-heuristic" [...] paper. Another root inspiring the concept of hyper-heuristics comes from the field of artificial intelligence. More specifically, it comes from work on automated planning systems, and its eventual focus towards the problem of <b>learning</b> <b>control</b> knowledge. The so-called COMPOSER system, developed by Gratch et al., was used for controlling satellite communication schedules involving a number of earth-orbiting satellites and three ground stations. The system can be characterized as a hill-climbing search in the space of possible control strategies.|$|E
40|$|Abstract. In this paper, {{we discuss}} the {{application}} of <b>learning</b> impedance <b>control</b> scheme to exoskeleton arm driven by pneumatic artificial muscles (PAM), for assisting in the rehabilitation of {{patients who suffer from}} debilitating illness. An iterative <b>learning</b> impedance <b>control</b> problem for robotic manipulators is analyzed, proposed and solved. The target impedance reference modifies a desired trajectory according to the force signals and position signals of the joint. The desired <b>control</b> input of <b>learning</b> impedance <b>control</b> was estimated by radial basis function (RBF) neural network incorporated experience database. The curves of experiment result on the experimental setup show that the algorithm is successful also in the application of exoskeleton arm...|$|R
40|$|AbstractOne {{method for}} {{reducing}} the time required for plan generation is to <b>learn</b> search <b>control</b> rules from experience. The most common approach to <b>learning</b> search <b>control</b> knowledge has been explanationbased learning. An alternative approach is to use inductive learning. An inductive approach {{does not require a}} complete and tractable domain theory and has the potential to create more effective rules by learning from more than one example at a time. In this paper we describe Grasshopper, an inductive system that <b>learns</b> search <b>control</b> rules for a classical plan generation system. We also provide an empirical evaluation of Grasshopper by comparing it with an existing explanation-based learning system...|$|R
50|$|Mental Adjustment: {{adjusting}} turbulence and buoyancy, <b>learning</b> breath <b>control,</b> cultivating {{confidence and}} good attitude.|$|R
40|$|Iterative <b>learning</b> <b>control</b> {{applied to}} a {{simplified}} model of a robot arm is studied. The iterative <b>learning</b> <b>control</b> input signal is used in combination with conventional feed-back and feed-forward control, and {{the aim is to}} let the <b>learning</b> <b>control</b> signal handle the effects of unmodeled dynamics and friction. Convergence and robustness aspects of the choice of filters in the updating scheme of the iterative <b>learning</b> <b>control</b> signal are studied...|$|E
40|$|Iterative <b>Learning</b> <b>Control</b> is {{presented}} briefly {{together with a}} review of some results on convergence and some new results on Iterative <b>Learning</b> <b>Control</b> with load disturbances and measurement disturbances. In the example presented, Iterative <b>Learning</b> <b>Control</b> is used in combination with conventional feed-back and feed-forward control, and it is shown that <b>learning</b> <b>control</b> is highly affected by the different disturbances. Some ideas on how the filters in the ILC algorithm should be chosen are also discussed...|$|E
40|$|This paper firstly makes a {{retrospective}} review of some iterative <b>learning</b> <b>control</b> techniques and results {{regarding to the}} initial state shift issue and the monotone convergence analysis. Secondly, the paper presents {{a review of the}} higher-order iterative <b>learning</b> <b>control</b> scheme including its convergence speed comparison and effectiveness. Then, the paper exhibits a review of iterative <b>learning</b> <b>control</b> mechanism for large-scale systems including repetitive systems and magnitude-varying industrial processes. Lastly, the paper gives a comment on prospective long-term <b>learning</b> <b>control</b> for the future...|$|E
40|$|In {{this paper}} we explore {{the ability of a}} recent model-based {{learning}} technique Receding Horizon Locally Weighted Regression (RH-LWR) useful for learning temporally dependent systems. In particular this paper investigates the application of RH-LWR to <b>learn</b> <b>control</b> of Multiple-input Multiple-output robot systems. RH-LWR is demonstrated through learning joint velocity and position control of a three Degree of Freedom (DoF) rigid body robot...|$|R
40|$|Abstract. Inductive Logic Programming (ilp) {{methods have}} proven to succesfully acquire {{knowledge}} in very different learning paradigms, such as supervised and unsupervised learning or relational reinforcement learning. However, very little {{has been done on}} General Problem Solving (gps). One of the ilp-based approaches applied to gps is hamlet. This method is able to <b>learn</b> <b>control</b> rules (heuristics) for a non linear planner, prodigy 4. 0, which is integrated into the ipss system; control rules are used as an effective guide when building the planning search tree. Other learning approaches applied to planning generate macro-operators, building high-level blocks of actions, but increasing the branching factor of the search tree. In this paper, we focus on integrating the two different learning approaches (hamlet and macro-operators learning), to improve a planning process. The goal is to <b>learn</b> <b>control</b> rules that decide when to use the macro-operators. This process is succesfully applied in several classical planning domains. ...|$|R
40|$|In {{this paper}} we {{advocate}} a learning method where a deductive and an inductive strategies are combined to efficiently <b>learn</b> <b>control</b> knowledge. The approach consists of initially bounding the explanation to a predetermined set {{of problem solving}} features. Since {{there is no proof}} that the set is sufficient to capture the correct and complete explanation for the decisions, the control rules acquired are then refined, if and when applied incorrectly to new examples. The method is especially significant as it applies directly to nonlinear problem solving, where the search space is complete. We present hamlet, a system where we implemented this learning method, {{within the context of the}} prodigy architecture. hamlet <b>learns</b> <b>control</b> rules for individual decisions corresponding to new learning opportunities offered by the nonlinear problem solver that go beyond the linear one. These opportunities involve, among other issues, completeness, quality of plans, and opportunistic decision making. Finally, we show empirical results illustrating hamlet's learning performance...|$|R
40|$|This article {{examines}} state-of-the-art <b>learning</b> <b>control</b> schemes, particularly in applications for robot control systems, based on {{artificial neural network}} architectures with supervised and unsupervised learning rules. Existing schemes are classified as belonging to two different approaches: inverse dynamics modeling robot <b>learning</b> <b>control</b> (IDMRLC) and sensorimotor coordination robot <b>learning</b> <b>control</b> (SMCRLC). First, some commonly used artificial neural network architectures and learning rules are briefly reviewed. The inverse dynamics modeling robot <b>learning</b> <b>control</b> approach and the sensorimotor coordination robot <b>learning</b> <b>control</b> approach are then presented. Finally, additional neural-based solution approaches for other robot control problems are discussed. Ongoing research is showing that <b>learning</b> <b>control</b> schemes based on artificial neural network architectures are evolving to become a powerful technique for building stable, adaptive, real-time, and robust controllers. In addition, in the meantime they can help validate more biologically inspired models...|$|E
40|$|This paper {{studies the}} {{applicability}} of a discrete-time <b>learning</b> <b>control</b> method to the slewing control of a large flexible panel. Among the issues discussed are feasibility of the desired trajectories and their specification schemes, <b>learning</b> <b>control</b> by linear feedback, limitation of the actuator device output, and robustness to parameter changes or modeling errors associated with the chosen <b>learning</b> <b>control</b> design. To demonstrate the effectiveness of <b>learning</b> <b>control,</b> the system is designed with a proportional controller of the base angle only. Then application of the <b>learning</b> <b>control</b> is shown {{to make the system}} learn quickly to achieve the desired slewing without residual vibration {{at the end of the}} maneuver. Simulation results are reported and discussed...|$|E
40|$|An {{approach}} to iterative <b>learning</b> <b>control</b> system design based on two-dimensional system theory is presented. A two-dimensional {{model for the}} iterative <b>learning</b> <b>control</b> system which reveals the connections between <b>learning</b> <b>control</b> systems and two-dimensional system theory is established. A <b>learning</b> <b>control</b> algorithm is proposed, and the convergence of learning using this algorithm is guaranteed by two-dimensional stability. The learning algorithm is applied successfully to the trajectory tracking control problem for a parallel link robot manipulator. The excellent performance of this learning algorithm is demonstrated by the computer simulation results...|$|E
50|$|Children {{are made}} to <b>learn</b> body <b>control</b> & {{management}} of body movement through physical play.|$|R
5000|$|The End of Protest: How Free Market Capitalism <b>Learned</b> to <b>Control</b> Dissent, {{published}} in 2013; ...|$|R
50|$|Turbulent Gliding: <b>learning</b> to <b>control</b> the {{position}} of the body while being moved by the instructor.|$|R
