5|2538|Public
5000|$|Analyze module: This module {{features}} three process representations (general and linear <b>logging</b> <b>file</b> and the s-notation of the text) {{and four}} aggregated levels of analysis (summary, pause, revision and source analyses). Additionally a process graph is produced.|$|E
5000|$|Initially Meebo's {{offering}} was Meebo Messenger - a browser-based {{instant messaging}} application which supported multiple IM services, including Yahoo!, MSN, AIM, ICQ, MySpaceIM, Facebook Chat and Google Talk. [...] Features of Meebo Messenger included invisible sign-on, simultaneous uniform access to multiple IM services and conversation <b>logging.</b> <b>File</b> transfer and videoconferencing features were subsequently {{added to the}} application. Notification system was also available for Windows users via a standalone installable notifier, which could also maintain user's presence. Registered Meebo users could save their login information, so that all IM connections would be automatically established upon logging into Meebo.|$|E
40|$|This paper aims {{to propose}} an {{effective}} mechanism dealing with reputation assessment of communities of web services (CWSs) {{that are known}} as societies composed {{by a number of}} functionally identical web services. The objective is to provide a general incentive for CWSs to act truthfully given that they are allowed to decide about their actions. The considered entities (web services, virtual organizations, etc.) are designed as software autonomous agents equipped with advanced communication and reasoning capabilities. User agents request CWSs for services and accordingly rate their satisfactions about the received quality and community responsiveness. The strategies taken by different parties are private to individual agents. The <b>logging</b> <b>file</b> that collects feedback is investigated by a controller agent. Furthermore, the accurate reputation assessment is achieved by maintaining a sound logging mechanism. To this end, the incentives for CWSs to act truthfully are investigated and analyzed, which allows the controller agent to keep the <b>logging</b> <b>file</b> accurate. The proposed framework defines the evaluation metrics involved in the reputation assessment of a community, and supervises the logging system in order to verify the validity and soundness of the feedback provided by the users. In this paper, the proposed framework is described, a theoretical analysis of its assessment and its implementation along with discussion of empirical results are provided. We also show how our model is efficient, particularly in very dynamic environments. KEY WORDS: Web services, communities, incentives, trust, reputation, autonomous agent...|$|E
40|$|World Wide Web gives large {{information}} to internet user; it {{is a huge}} repository of web pages and links. At which time user visit website his or her clicks recorded in web <b>log</b> <b>file.</b> When user accesses website, <b>log</b> <b>file</b> are created. <b>Log</b> <b>file</b> is simple plain text ASCII file. Tremendous uses of web, web <b>log</b> <b>files</b> are growing at faster rate. Display of <b>log</b> <b>file</b> data in different format like W 3 C Extended <b>log</b> <b>file</b> format, NCSA common <b>log</b> <b>file</b> format, IIS <b>log</b> <b>file</b> format. <b>Log</b> <b>file</b> contain noisy & ambiguous data which may affect result of mining process. To improve quality of data, <b>log</b> <b>file</b> should be preprocessed. One technique to detection of web attack is to analyze web server <b>log</b> <b>file.</b> This web attack means intrusion, Intrusion detection consists of procedures and systems created and operated to detect system intrusions.. It gives detailed discussion about web <b>log</b> <b>file,</b> web <b>log</b> <b>file</b> format, intrusion detection. In this paper we survey about data preprocessing of web <b>log</b> <b>file</b> for web intrusion detection...|$|R
50|$|Urchin {{software}} can be run {{in two different}} data collection modes: <b>log</b> <b>file</b> analyzer or hybrid. As a <b>log</b> <b>file</b> analyzer, Urchin processes web server <b>log</b> <b>files</b> {{in a variety of}} <b>log</b> <b>file</b> formats. Custom file formats can also be defined. As a hybrid, Urchin combines page tags with <b>log</b> <b>file</b> data to eradicate the limitations of each data collection method in isolation. The result is more accurate web visitor data.|$|R
50|$|The NetInsight Extract, transform, load {{process can}} read <b>log</b> <b>files</b> in {{virtually}} any format, including logs from web servers, proxy servers, streaming media servers and FTP servers. As well as processing normal server <b>log</b> <b>files</b> NetInsight can use <b>log</b> <b>files</b> derived from page tags to replace or augment <b>log</b> <b>file</b> data.|$|R
40|$|With {{a market}} share of 84. 82 % in 2016 Android is the most {{influential}} mobile oper-ating system on the world. In March 2017 users could find about 2. 8 million ap-plications in the official Playstore while the number applications from other sourcesis unknown]. Since mobile devices are a fundamental source for news, enter-tainment, social activities and more they are also used for mobile banking, healthtracking and other data sensitive tasks. Besides static analysis the approach of dy-namically analyzing applications is necessary to ensure integrity and security. In theinternet a plethora of dynamic analysis methods for Android can be found. Problem-atic for a software security tester is to keep an overview over the quickly changinglandscape of these approaches. In this thesis work relevant dynamic analysis methodswere grouped and evaluated on different criterion. Furthermore an implementationfor the <b>logging</b> <b>file</b> related system calls with LD_PRELOAD was implemented andinvestigated how API calls can be mapped and the data visualized...|$|E
40|$|Data {{acquisition}} systems, as {{the name}} implies, are products and/or processes used to collect information to document or analyze some phenomenon. In the simplest form, a technician logging the temperature of an oven {{on a piece of}} paper is performing data acquisition. As technology has progressed, this type of process has been simplified and made more accurate, versatile, and reliable through electronic equipment. Equipment ranges from simple recorders to sophisticated computer systems Data Acquisition system designed in this project is for Image Grabbing and then storing it in the Computer where image data can be analyzed and modified. The module designed and developed consists of 2 * 2 array of photo diodes used to collect the physical data i. e. light signal. The Analog signal generated from the photo diodes was then converted to digital signal with the help of an ADC and then stored into the memory. The card used NI-PCIe 6537 is a Digital VO card from National Instruments. It was used to generate control signals for the module developed. It has fast data transfer capability which was the basic requirement. Basically a DAQ system is used for converting physical signals to digital signals using various kinds of sensors and signal processing circuitry. The DAQ module developed had a fast conversion speed (8 MHz to be exact) and also everything had to be done in parallel. In order to work at this high speed a memory is required to store the data obtained from the ADC which can be later on transferred to the computer. An SRAM is used as a memory storage device because of its simplicity to operate. A VHDCI cable is used to connect the card NI-PCIe 6537 to the DAQ board. All the control signals for the circuit are generated from the card using LabVIEW. The address for the SRAM is also generated using Lab VIEW which is basically a Graphical programming environment and an interface between the user and the DIO (Digital 1 / 0) card. The DAQ (Data AcQuisition system) developed worked at the required speed of 8 MHz. The card PCie 6537 was configured (timing wise) in such a way that the whole system worked at a designated frequency of 8 MHz. The data obtained from the ADC was stored in PC in the form of a ???. tdms??? file which is basically a data <b>logging</b> <b>file...</b>|$|E
3000|$|... • The {{twenty four}} <b>log</b> <b>files</b> {{of the attack}} (six <b>log</b> <b>files</b> for all executions of each attack step and smurf attack) with the {{respective}} <b>log</b> <b>files</b> for normal system operation.|$|R
40|$|The Software {{applications}} are usually programmed to generate some auxiliary text files {{referred to as}} <b>log</b> <b>files.</b> Such files are used throughout various stages of the software development, primarily for debugging and identification of errors. Use of <b>log</b> <b>files</b> makes debugging easier during testing. It permits following {{the logic of the}} program, at high level, while not having to run it in debug mode. Nowadays, <b>log</b> <b>files</b> are usually used at commercial software installations for the aim of permanent software observation and finetuning. <b>Log</b> <b>files</b> became a typical part of software application and are essential in operating systems, networks and distributed systems. <b>Log</b> <b>files</b> are usually the only way to determine and find errors in a software application, because probe effect has no effect on <b>log</b> <b>file</b> analysis. <b>Log</b> <b>files</b> are usually massive and may have an intricate structure. Though the method of generating <b>log</b> <b>files</b> is sort of easy and simple, <b>log</b> <b>file</b> analysis may well be an incredible task that needs immense computing resources and complex procedures. ...|$|R
40|$|<b>Log</b> <b>files</b> contain {{information}} about User Name, IP Address, Time Stamp, Access Request, number of Bytes Transferred, Result Status, URL that Referred and User Agent. The <b>log</b> <b>files</b> are {{maintained by the}} web servers. By analysing these <b>log</b> <b>files</b> gives a neat idea about the user. This paper gives a detailed discussion about these <b>log</b> <b>files,</b> their formats, their creation, access procedures, their uses, various algorithms used and the additional parameters {{that can be used}} in the <b>log</b> <b>files</b> which in turn gives way to an effective mining. It also provides the idea of creating an extended <b>log</b> <b>file</b> and learning the user behaviour. Comment: 12 pages, 6 figures,CCSIT 201...|$|R
40|$|Large {{software}} systems typically keep <b>log</b> <b>files</b> {{of events}} {{for use in}} debugging or regression testing. A formal framework is proposed for analyzing these <b>log</b> <b>files</b> to verify that the associated system has the desired behaviour. Taking into account some common properties of <b>log</b> <b>files,</b> a <b>log</b> <b>file</b> analyzer is de ned {{as a set of}} possibly communicating state machines, which accept a <b>log</b> <b>file</b> if the file causes the machines to move through valid sequences of states. A prototype implementation consistent with the formal definition is described, and examples of its use are given. Suggestions are made as to how such <b>log</b> <b>file</b> analysis could be used in general software testing...|$|R
50|$|NetTracker {{processes}} <b>log</b> <b>files</b> {{in virtually}} any format using a custom log definition. Logs can be processed from web servers, proxy servers, streaming media servers and FTP servers. As well as processing <b>log</b> <b>files</b> NetTracker can use page tag data to augment <b>log</b> <b>file</b> data.|$|R
40|$|Abstract — There {{are various}} {{applications}} {{which have a}} huge database. All databases maintain <b>log</b> <b>files</b> that keep records of database changes. This can include tracking various user events. Apache Hadoop {{can be used for}} log processing at scale. <b>Log</b> <b>files</b> have become a standard part of large applications and are essential in operating systems, computer networks and distributed systems. <b>Log</b> <b>files</b> are often the only way to identify and locate an error in software, because <b>log</b> <b>file</b> analysis is not affected by any time-based issues known as probe effect. This is opposite to analysis of a running program, when the analytical process can interfere with time-critical or resource critical conditions within the analyzed program. <b>Log</b> <b>files</b> are often very large and can have complex structure. Although the process of generating <b>log</b> <b>files</b> is quite simple and straightforward, <b>log</b> <b>file</b> analysis could be a tremendous task that requires enormous computational resources, long time and sophisticated procedures. This often leads to a common situation, when <b>log</b> <b>files</b> are continuously generated and occupy valuable space on storage devices, but nobody uses them and utilizes enclosed information. The overall goal of this project is to design a generic log analyzer using hadoop map-reduce framework. This generic log analyzer can analyze different kinds of <b>log</b> <b>files</b> such as- Emai...|$|R
40|$|Abstract. In many {{application}} areas, systems reports occurring {{events in}} a kind of textual data called usually <b>log</b> <b>files.</b> <b>Log</b> <b>files</b> report the status of systems, products, or even causes of problems that can occur. The Information extracted from <b>log</b> <b>files</b> of computing systems can be considered one of the important resources of information systems. <b>Log</b> <b>files</b> are considered as a kind of “complex textual data”, i. e. the multi-source, heterogeneous, andmulti-format data. In this paper, we aim particularly at exploring the lexical structure of these <b>log</b> <b>files</b> in order to extract the terms used in <b>log</b> <b>files.</b> These terms will be used in the building of domain ontology and also in enrichment of features of <b>log</b> <b>files</b> corpus. According to features of such textual data, applying the classical methods of information extraction is not an easy task, more particularly for terminology extraction. Here, we introduce a new developed version of EXTERLOG, our approach to extract the terminology from <b>log</b> <b>files,</b> which is guided by Web to evaluate the extracted terms. We score the extracted terms by a Web and context based measure. We favor the more relevant terms of domain and emphasize the precision by filtering terms based on their scores. The experiments show that EXTERLOG is well-adapted terminology extraction approach from <b>log</b> <b>files.</b> ...|$|R
40|$|Abstract. This paper {{describes}} {{the implementation of}} Web usage mining for DSpace server of NIT Rourkela. The DSpace <b>log</b> <b>files</b> have been preprocessed to convert the data stored in them into a structured format. Thereafter, the general procedures for bot-removal and session-identification from a Web <b>log</b> <b>file</b> have been applied with certain modifications pertaining to the DSpace <b>log</b> <b>files.</b> Furthermore, analysis of these <b>log</b> <b>files</b> using a subjective interpretation of recently proposed algorithm EIN-WUM has also been conducted...|$|R
40|$|Nowadays <b>log</b> <b>file</b> plays {{vital role}} in web {{forensic}} as digital evidence. Hence security of <b>log</b> <b>file</b> is a major topic of apprehension. In this paper a model of image logging server having alteration detectable capability, is proposed. According to this approach we first convert a text <b>log</b> <b>file</b> into image <b>log</b> <b>file</b> {{with the help of}} bit encoding technique and tamper detection capability is achieved by self embedding fragile watermark scheme. If any alteration is done on image <b>log</b> <b>file</b> then due to nature of fragile watermark, one can easily locate that tampered region. Proposed model is also able to ensure all security requirements like Authenticity, Integrity and confidentiality...|$|R
40|$|ABSTRACT: In this paper, we have {{introduced}} {{a concept of}} capturing different web <b>log</b> <b>file,</b> while the user is accessing the Distance Education System website. Web <b>log</b> <b>file</b> can be further used in pattern discovery and pattern analysis process. Web <b>log</b> <b>file</b> is saved in text (. txt) format with “comma ” separated attributes. <b>Log</b> <b>files</b> can’t be directly used for pattern discovery process because it consists of irrelevant and inconsistent access information. Therefore there is need of Web log preprocessing which includes different techniques such as field extraction, data cleaning, data filtering, and data summarization. We have discussed different types of web <b>log</b> <b>files</b> and preprocessing techniques...|$|R
40|$|Automated {{software}} <b>log</b> <b>file</b> analysis {{holds an}} important position in software maintenance. Currently available analysis tools are not generic. They {{tend to focus}} on specific software or servers and their flexibilities are minimal. Furthermore, costs of commercially available log analysis tools are not affordable for small and medium scale firms. This has left a void in the market for generic, customizable and open source <b>log</b> <b>file</b> analysis tools. The impediment to such a tool emerging is the unavailability of a generic <b>log</b> <b>file</b> data extraction mechanism. A generic <b>log</b> <b>file</b> format definition language and an underlying persistent data storage system is a solution to this problem. <b>Log</b> <b>file</b> structures could be defined by the aforementioned language and the data extracted would be stored in the persistent storage. This methodology enables generic <b>log</b> <b>file</b> analysis on top of the extracted data. Through the research and implementations carried out, it was identified that a modified version of simple declarative language is suitable for the <b>log</b> <b>file</b> format definition language. II would have the capability of handling and defining all patterns of text based <b>log</b> <b>files.</b> Additionally. the results revealed that the appropriate storage mechanism would be an Extensible Markup Language (XML) database mainly because of the similarities between the hierarchical nature of XML and common <b>log</b> <b>file</b> structures...|$|R
5000|$|Coordinating {{the reading}} of the {{transaction}} logs and the archiving of <b>log</b> <b>files</b> (database management software typically archives <b>log</b> <b>files</b> off-line on a regular basis).|$|R
40|$|In today’s IT environments, {{there is}} an ever {{increasing}} demand for <b>log</b> <b>file</b> analysis solutions. <b>Log</b> <b>files</b> often con-tain important information about possible incidents, but in-specting the often large amounts of textual data is too time-consuming and tedious a task to perform manually. To ad-dress this issue, we propose a novel <b>log</b> <b>file</b> visualization technique called Histogram Matrix (HMAT). HMAT visual-izes the content of a <b>log</b> <b>file</b> in order to enable a security ad-ministrator to efficiently spot anomalies. The system uses a combination of graphical and statistical techniques and al-lows even non-experts to interactively search for anomalous log messages. Contrary to other approaches, our proposal does not only work on certain special kinds of <b>log</b> <b>files,</b> but instead works on almost every textual <b>log</b> <b>file.</b> Addition-ally, the system allows to automatically generate security events if an anomaly is detected, similar to anomaly-based intrusion detection systems. This paper introduces HMAT, demonstrates its functionality using <b>log</b> <b>files</b> {{from a variety of}} services in real environments, and identifies strengths and limitations of the technique. ...|$|R
40|$|Abstract. Contemporary {{information}} systems are replete with <b>log</b> <b>files,</b> created in multiple places (e. g., network servers, database management systems, user monitoring applications, system services and utilities) for multiple purposes (e. g., maintenance, security issues, traffic analysis, legal requirements, software debugging, customer management, user interface usability studies). <b>Log</b> <b>files</b> in complex systems may quickly grow to huge sizes. Often, {{they must be}} kept {{for long periods of}} time. For reasons of convenience and storage economy, <b>log</b> <b>files</b> should be compressed. However, most of the available <b>log</b> <b>file</b> compression tools use general-purpose algorithms (e. g., Deflate) which do not take advantage of redundancy specific for <b>log</b> <b>files.</b> In this paper a specialized <b>log</b> <b>file</b> compression scheme is described in five variants, differing in complexity and attained compression ratios. The proposed scheme introduces a <b>log</b> <b>file</b> transform whose output is much better compressible with general-purpose algorithms than original data. Using the fast Deflate algorithm, the transformed <b>log</b> <b>files</b> were, on average, 36. 6 % shorter than the original files compressed with gzip (employing the same algorithm). Using the slower PPMVC algorithm, the transformed files were 62 % shorter than the original files compressed with gzip, and 41 % shorter than the original files compressed with bzip 2...|$|R
50|$|The Webalizer can process CLF, Apache and W3C Extended <b>log</b> <b>files,</b> {{as well as}} HTTP proxy <b>log</b> <b>files</b> {{produced}} by Squid servers. Other <b>log</b> <b>file</b> formats are usually converted to CLF {{in order to be}} analyzed. In addition, logs compressed with either GZip (.gz) or BZip2 (.bz2) can be processed directly without the need to uncompress before use.|$|R
50|$|Log FilesAs {{stated earlier}} <b>log</b> <b>files</b> {{are a way}} to check events that have been {{happening}} on a computer or network. For a hacker, having the ability to change what the <b>log</b> <b>file</b> says can help him not to be noticed. There is code and directions on how to change some <b>log</b> <b>files</b> in the book.|$|R
40|$|<b>Log</b> <b>Files</b> {{are created}} for Traffic Analysis, Maintenance, Software debugging, {{customer}} management at multiple places like System Services, User Monitoring Applications, Network servers, database management systems {{which must be}} kept {{for long periods of}} time. These <b>Log</b> <b>files</b> may grow to huge sizes in this complex systems and environments. For storage and convenience <b>log</b> <b>files</b> must be compressed. Most of the existing algorithms do not take temporal redundancy specific <b>Log</b> <b>Files</b> into consideration. We propose a Non Linear based Classifier which introduces a multidimensional <b>log</b> <b>file</b> compression scheme described in eight variants, differing in complexity and attained compression ratios. The FELFCNCA scheme introduces a transformation for <b>log</b> <b>file</b> whose compressible output is far better than general purpose algorithms. This proposed method was found lossless and fully automatic. It does not impose any constraint on the size of log fileComment: International Journal on Communications (IJC) Volume 1 Issue 1, December 2012 [URL]...|$|R
40|$|Abstract—We {{describe}} and apply a lightweight formal method for checking test results. The method {{assumes that the}} software under test writes a text log file; this <b>log</b> <b>file</b> is then analyzed by a program {{to see if it}} reveals failures. We suggest a state-machine-based formalism for specifying the <b>log</b> <b>file</b> analyzer programs and describe a language and implementation based on that formalism. We report on empirical studies of the application of <b>log</b> <b>file</b> analysis to random testing of units. We describe the results of experiments done to compare the performance and effectiveness of random unit testing with coverage checking and <b>log</b> <b>file</b> analysis to other unit testing procedures. The experiments suggest that writing a formal <b>log</b> <b>file</b> analyzer and using random testing is competitive with other formal and informal methods for unit testing. Index Terms—Testing, specification, safety verification, lightweight formal methods, test oracles, unit testing, <b>log</b> <b>file</b> analysis æ...|$|R
40|$|In many domains, the <b>log</b> <b>files</b> {{generated}} by digital systems contain important {{information on the}} conditions and configurations of systems. Information Extraction from these <b>log</b> <b>files</b> is an essential phase in information systems, which manage the production line. In the case of Integrated Circuit designs, <b>log</b> <b>files</b> {{generated by}} design tools are not exhaustively exploited. Although these <b>log</b> <b>files</b> are written in English, they usually do not respect the grammar and the structures of natural language. Moreover, such logs have a heterogeneous and evolving structure. According to features of such textual data, applying the classical methods of information extraction {{is not an easy}} task, more particularly for terminology extraction. In this paper, we thus introduce our approach EXTERLOG to extract the terminology from such <b>log</b> <b>files.</b> We also aim at knowing if POS tagging of such <b>log</b> <b>files</b> is a relevant approach for terminology extraction. ...|$|R
40|$|This thesis {{investigates the}} {{implementation}} of secure <b>log</b> <b>file</b> handling mechanisms {{in the light of}} recent smart card improvements. Initially, we examine how smart cards evolved from single application cards into true multi-application cards. Additionally, we present the most recent architectures (client application interfaces) that enable client applications to interface with smart card applications. Previous proposals for maintaining <b>log</b> <b>files</b> in smart cards are very limited and mostly theoretical. We examine those most related to smart cards along with presenting the very few real world examples of <b>log</b> <b>files.</b> We go on to examine the new events that required logging along with the requirements of the entities involved. Subsequently, we describe an ideal event-logging model for a multi-application smart card environment. To meet the identified requirements, we describe the details of a smart card entity that is responsible for dynamically updating the smart card <b>log</b> <b>files.</b> In that context, along with providing adequate <b>log</b> <b>file</b> space management, we propose a possible standard <b>log</b> <b>file</b> format for smart cards. In the core part of the thesis we describe three different smart card <b>log</b> <b>file</b> download protocols, the selection of which depends on the requirements of the entities involved. These protocols download audit data to another entity that does not suffer from immediate storage restrictions. Finally, we describe implementation details and performance measurements of both the <b>log</b> <b>file</b> download protocol and the standard <b>log</b> <b>file</b> format in two of the most advanced multi application smart cards...|$|R
5000|$|This process writes redo <b>log</b> <b>files</b> to disk. <b>Log</b> <b>files</b> contain all {{information}} {{about changes in}} the database's data. They are used for fast transaction processing and restoration.|$|R
40|$|Process mining {{techniques}} try {{to discover}} and analyse business processes from recorded process data. These data have to be structured in so called computer <b>log</b> <b>files.</b> If processes are supported by different computer systems, merging the recorded data into one <b>log</b> <b>file</b> can be challenging. In this paper we present a computational algorithm, based on the Artificial Immune System algorithm, that we developed to automatically merge separate <b>log</b> <b>files</b> into one <b>log</b> <b>file.</b> We also describe our implementation of this technique, a proof of concept application and a real life test case with promising results...|$|R
40|$|Abstract. Process mining {{techniques}} {{are applied to}} single computer <b>log</b> <b>files.</b> But many processes are supported by different software tools and are by consequence recorded into multiple <b>log</b> <b>files.</b> Therefore {{it would be interesting}} to find a way to automatically combine such a set of <b>log</b> <b>files</b> for one process. In this paper we describe a technique for merging <b>log</b> <b>files</b> based on a genetic algorithm. We show with a generated test case that this technique works and we give an extended overview of which research is needed to optimise and validate this technique...|$|R
30|$|Client <b>log</b> <b>files</b> {{are based}} on browser plugins [6], java scripts, and java applets that are {{integrated}} with websites. When dealing with code running on the client side, the issue of privacy comes to the surface. Thus, {{it is recommended that}} the collection of data streams remains not directly related to users since malicious analysts could benefit from flowing traffic to violate users’ privacy [7, 8]. Concerning Proxy <b>log</b> <b>files,</b> they are generated by Proxy servers, which are commonly deployed by organizations to reduce Internet traffic usage. It may appear therefore that Proxy <b>log</b> <b>files</b> should be used along with web server <b>log</b> <b>files</b> to get a better understanding of Internet surfing. In this regard, we will demonstrate that our approach does not require getting hold of data stored on proxy servers, which could be regarded as a desirable feature. Finally, with regard to web server <b>log</b> <b>files,</b> they are automatically generated, and are the most commonly used <b>log</b> <b>files</b> for usage mining. These files do not contain entries of pages served by proxy servers to users, and consequently are not entered into web server <b>log</b> <b>files</b> [3].|$|R
40|$|Abstract — Event <b>logs</b> or <b>log</b> <b>files</b> form an {{essential}} part of any network management and administration setup. While <b>log</b> <b>files</b> are invaluable to a network administrator, the vast amount of data they sometimes contain can be overwhelming and can sometimes hinder rather than facilitate the tasks of a network administrator. For this reason several event clustering algorithms for <b>log</b> <b>files</b> have been proposed, one of which is the event clustering algorithm proposed by Risto Vaarandi, on which his Simple <b>Log</b> <b>file</b> Clustering Tool (SLCT) is based. The aim of this work is to develop a visualization tool {{that can be used to}} view <b>log</b> <b>files</b> based on the clusters produced by SLCT. The proposed visualization tool, which is called LogView, utilizes treemaps to visualize the hierarchical structure of the clusters produced by SLCT. Our results based on different application <b>log</b> <b>files</b> show that LogView can ease the summarization of vast amount of data contained in the <b>log</b> <b>files.</b> This in turn can help to speed up the analysis of event data in order to detect any security issues on a given application. I...|$|R
5000|$|...appendtrace: If true, {{then it will}} append {{the trace}} {{at the end of}} a <b>log</b> <b>file.</b> If false, then it will {{override}} the <b>log</b> <b>file</b> for the invocation of wsadmin.|$|R
5000|$|BES also {{produces}} {{a set of}} <b>log</b> <b>files</b> during operation, called the BES Event <b>Log.</b> The <b>log</b> <b>files</b> include (for a BES v4.0 and 4.1 system connecting to Microsoft Exchange): ...|$|R
50|$|A {{transaction}} <b>log</b> <b>file</b> records all transactions {{since the}} last checkpoint. Transaction <b>log</b> <b>files</b> may be preserved and archived for an indefinite time, providing a full, recoverable history of the database.|$|R
