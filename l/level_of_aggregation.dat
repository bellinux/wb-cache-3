658|10000|Public
25|$|Another {{advantage}} is aggregation. A well designed standardized test provides {{an assessment of}} an individual's mastery of a domain of knowledge or skill which at some <b>level</b> <b>of</b> <b>aggregation</b> will provide useful information. That is, while individual assessments may not be accurate enough for practical purposes, the mean scores of classes, schools, branches of a company, or other groups may well provide useful information because of the reduction of error accomplished by increasing the sample size.|$|E
2500|$|The {{results in}} bold-face {{indicate}} which technique is less expensive, showing reswitching. There {{is no simple}} (monotonic) relationship between the interest rate and the [...] "capital intensity" [...] or roundaboutness of production, either at the macro- or the microeconomic <b>level</b> <b>of</b> <b>aggregation.</b>|$|E
2500|$|In 1969, Milton Friedman, after {{examining}} {{the history of}} business cycles in the U.S., concluded that [...] "The Hayek-Mises explanation of the business cycle is contradicted by the evidence. It is, I believe, false." [...] He analyzed the issue using newer data in 1993, and again reached the same conclusion. Referring to Friedman's discussion of the business cycle, Austrian economist Roger Garrison argued that Friedman's empirical findings are [...] "broadly consistent with both Monetarist and Austrian views", {{and goes on to}} argue that although Friedman's model [...] "describes the economy's performance at the highest <b>level</b> <b>of</b> <b>aggregation,</b> Austrian theory offers an insightful account of the market process that might underlie those aggregates".|$|E
40|$|In this {{position}} paper {{we deal with}} the conception of heterogeneity as both the force and the result of evolutionary change. We ask, how this heterogeneity can be measured empirically and how we can get a measure which allows to get a broad comparable empirical account especially on several <b>levels</b> <b>of</b> <b>aggregation.</b> Based on this discussion we suggest that for several questions the measures of total factor productivity (TFP) and local changes of TFP seem to be acceptable candidates for measuring heterogeneity and its dynamics. Examples out of a number of empirical investigations applying this measures show how interesting empirical facts about evolutionary change on several <b>levels</b> <b>of</b> <b>aggregation</b> can be detected. The paper concludes by raising a number of unresolved issues mainly related to the question about the relationship between evolutionary dynamics on several <b>levels</b> <b>of</b> <b>aggregation.</b> ...|$|R
50|$|There {{are many}} {{definitions}} of internal control, as {{it affects the}} various constituencies (stakeholders) of an organization in various ways and at different <b>levels</b> <b>of</b> <b>aggregation.</b>|$|R
40|$|This {{research}} {{examines the}} properties of an estimation procedure frequently used because observations on some variables are available only at higher <b>levels</b> <b>of</b> <b>aggregation</b> than others. When this occurs, data are often stretched by repeating observations on variables at higher <b>levels</b> <b>of</b> <b>aggregation.</b> We show that this procedure results in biased estimators of coefficients and error variances. Under some circumstances the estimation based on stretched data has a smaller covariance matrix than that based on aggregated data. Comparisons of mean squared errors depend on unknown coefficients. Copyright 1989 by John Wiley & Sons, Ltd. ...|$|R
50|$|Traffic is {{represented}} {{at a high}} <b>level</b> <b>of</b> <b>aggregation,</b> thus not distinguishing vehicles individually.|$|E
50|$|Two {{prominent}} investigators, Lynn Margulis and, more fully, Leo Buss {{have developed}} {{a view of the}} evolved life structure as exhibiting tiered levels of (dynamic) aggregation of life units. In each <b>level</b> <b>of</b> <b>aggregation,</b> the component elements have mutually beneficial, or complementary, relationships.|$|E
5000|$|The {{results in}} bold-face {{indicate}} which technique is less expensive, showing reswitching. There {{is no simple}} (monotonic) relationship between the interest rate and the [...] "capital intensity" [...] or roundaboutness of production, either at the macro- or the microeconomic <b>level</b> <b>of</b> <b>aggregation.</b>|$|E
40|$|The {{reliability}} and correctness {{of the results}} of the assessment of research productivity in universities by means of bibliometric techniques depend on the <b>level</b> <b>of</b> data <b>aggregation</b> used for the analysis by disciplinary area. The variability among the university research fields, the varying prolificacy of the scientific disciplines and the different <b>levels</b> <b>of</b> representativeness by discipline of the journals covered in source databases are the main causes leading to distortions. Such effects can be reduced considerably by using a <b>level</b> <b>of</b> data <b>aggregation</b> by discipline which is as homogeneous and uniform as possible. This study compares the research productivity data of Italian universities at two different <b>levels</b> <b>of</b> <b>aggregation</b> <b>of</b> output and input data, thus showing and assessing the scale of the distortions that may result...|$|R
40|$|This paper {{proposes a}} simple method {{measuring}} spatial robustness of estimated coefficients and considers {{the role of}} administrative districts and regions' size. The procedure, dubbed "Grid and Shake", offers a solution for a practical empirical issue, when one compares a variables of interest across spatially aggregated units, such as regions. It may, for instance, be applied to investigate competition, agglomeration, spillover effects. The method offers to (i) have carry out estimations at various <b>levels</b> <b>of</b> <b>aggregation</b> and compare evidence, (ii) treat uneven and non-random distribution of administrative unit size, (iii) {{have the ability to}} compare results on administrative and artificial units, and (iv) be able to gouge statistical significance of differences. To illustrate the method, we use Hungarian data and compare estimates of agglomeration externalities at various <b>levels</b> <b>of</b> <b>aggregation.</b> We find that differences among estimated elasticities found at various <b>levels</b> <b>of</b> <b>aggregation</b> are broadly in the same range as those found in the literature employing various estimation method. Hence, the method <b>of</b> spatial <b>aggregation</b> seems to be of equal importance to modeling and econometric specification of the estimation...|$|R
3000|$|... b). The {{low energy}} band at 2.45 eV does not {{correspond}} to dipole mode interaction and can appear at higher <b>levels</b> <b>of</b> <b>aggregation,</b> {{as a result of}} high-order modes interaction (quadrupole, octupole, etc.).|$|R
5000|$|They {{count and}} code {{groups at the}} highest level within-country <b>level</b> <b>of</b> <b>aggregation</b> that is {{politically}} meaningful. For example, all Hispanics in the U.S. are profiled as a single group because they are usually regarded and treated by Anglo-Americans as one collectivity; and, ...|$|E
5000|$|Aggregation: The highest <b>level</b> <b>of</b> <b>aggregation</b> in {{a service}} {{provider}} network. The next {{level in the}} hierarchy under the core nodes is the distribution networks and then the edge networks. Customer-premises equipment (CPE) do not normally connect to the core networks of a large service provider.|$|E
50|$|At {{the highest}} <b>level</b> <b>of</b> <b>aggregation</b> in the Management Index, the {{overview}} provides important initial indications of which countries exhibit {{the best performance}} of governance overall and which countries show deficiencies. Background information in greater depth {{on the performance of}} a given country {{can be found in the}} country reports on the SGI website. These include substantiated, qualitative information right down to the level of individual indicators.|$|E
30|$|We {{recognize}} that measurement problems and high <b>levels</b> <b>of</b> <b>aggregation</b> potentially limit studies on age structure and productivity. These limits {{are far less}} severe in studies such as Boersch-Supan and Weiss (2007) that use insider econometrics.|$|R
3000|$|... 2 It is {{also true}} that natives may sort by area even at these higher <b>levels</b> <b>of</b> <b>aggregation,</b> and the effect of this sorting is later {{discussed}} in moderating the strength of results in this analysis.|$|R
40|$|Economists {{often have}} to use {{temporally}} aggregated data in causality tests. A number of theoretical studies {{have pointed out that}} temporal aggregation has distorting effects on causal inference. This paper examines the issue in detail by plugging in theoretical cross covariances into the limiting values of least squares estimates. An extensive Monte Carlo study is conducted to examine small sample results. An empirical example is also provided. It is observed that in general the most distorting causal inferences are likely at low <b>levels</b> <b>of</b> <b>aggregation</b> where the order <b>of</b> <b>aggregation</b> just exceeds the actual causal lag. At high <b>levels</b> <b>of</b> <b>aggregation,</b> causal information concentrates in contemporaneous correlations. At present, a data-based approach is not available to establish the direction of causality between contemporaneously correlated variables...|$|R
5000|$|Researchers {{are said}} to commit the {{ecological}} fallacy when they make untested inferences about individual-level relationships from aggregate data. It is called a fallacy because {{it is based on}} the problematic assumption that relationships at one <b>level</b> <b>of</b> <b>aggregation</b> also hold at another <b>level</b> <b>of</b> <b>aggregation.</b> To illustrate, consider the fact that George Wallace, a four-term governor of Alabama and well-known segregationist who ran as a third-party candidate well in the 1968 US Presidential election, received a higher share of votes in regions with higher percentages of blacks. [...] From this one might erroneously conclude that blacks were disproportionately inclined to vote for Wallace (post-election surveys showed that, while one in eight whites voted for Wallace, virtually no blacks did).Firebaugh has contributed to this literature by delineating theoretical conditions or rules under which it is possible to infer individual-level relationships from aggregate data. These conditions are important because researchers are subject to the ecological fallacy in virtually all the social and behavioral sciences - from history to political science to epidemiology - since individual-level data often are unavailable.|$|E
50|$|Postcodes {{have been}} adopted {{for a wide range}} of {{purposes}} in addition to aiding the sorting of the mail: for calculating insurance premiums, designating destinations in route planning software and as the lowest <b>level</b> <b>of</b> <b>aggregation</b> in census enumeration. The boundaries of each postcode unit and within these the full address data of currently about 29 million addresses (delivery points) are stored, maintained and periodically updated in the Postcode Address File database.|$|E
50|$|Another {{advantage}} is aggregation. A well designed standardized test provides {{an assessment of}} an individual's mastery of a domain of knowledge or skill which at some <b>level</b> <b>of</b> <b>aggregation</b> will provide useful information. That is, while individual assessments may not be accurate enough for practical purposes, the mean scores of classes, schools, branches of a company, or other groups may well provide useful information because of the reduction of error accomplished by increasing the sample size.|$|E
40|$|This paper {{summarizes}} the interrelationships among within-aggregate, between-aggregate, and total-group cor-relation coefficients, with artificial and "real-data " ex-amples. It also discusses {{the relevance of}} correlation analyses at various <b>levels</b> <b>of</b> <b>aggregation</b> and some <b>of</b> the difficulties encountered in cross-level inference. I...|$|R
40|$|This report {{describes}} {{methods of}} internal consistency in population projections at multiple <b>levels</b> <b>of</b> <b>aggregation.</b> The {{first step in}} the process is to make the initial assumptions used in the projections at different <b>levels</b> <b>of</b> <b>aggregation</b> consistent. This input-level consistency can be further enhanced by output-level analysis. Comparing the results at the relevant <b>levels</b> <b>of</b> <b>aggregation</b> ensures internal consistency at the output level. Thus, we are able to compare the differences in the age and sex distribution of the population and specific demographic indicators (such as the old age dependency ratio) over various regional levels. In PLUREL, the national projections will be carried out using the probabilistic method while the regional projections will use deterministic or variant methods. The results at these two <b>levels</b> <b>of</b> <b>aggregation</b> cannot be directly compared one-to-one as there is no simple correspondence between the output variants and the probabilistic range. To avoid any problems arising from this, we develop an index representing the differences in the size and distribution of the population from the variant method to a given percentile in the probabilistic population estimate. The report discusses various population projection techniques together with their strengths and weaknesses. The relative advantage of specific models for different purposes is discussed forming a selection of models to be used for the population projections in PLUREL: National (NUTS 1 - 0), Regional (NUTS- 2) and Case Study Projections (NUTS- 5). We conclude that stochastic projections are best suited for national projections, while classic or multiregional cohort-component model projections are likely to be the best choice for the regional projections and for the detailed case study projections...|$|R
30|$|To {{assess the}} <b>level</b> <b>of</b> {{instability}} farmers are facing, it is preferred {{to work on}} farm-level time-series because at higher <b>levels</b> <b>of</b> <b>aggregation,</b> poor income in some farms are offset by good income in others. Thus, aggregated data can severely underestimate farm level risk (Kimura et al. 2010; OECD 2009).|$|R
5000|$|With {{comparison}} to more conventional toxicology studies, the nanotoxicology field is however {{suffering from a}} lack of easy characterisation of the potential contaminants, the [...] "nano" [...] scale being a scale difficult to comprehend. The biological systems are themselves still not completely known at this scale. Ultimate Atomic visualisation methods such as Electron microscopy (SEM and TEM) and Atomic force microscopy (AFM) analysis allow visualisation of the nano world. Further nanotoxicology studies will require precise characterisation of the specificities of a given nano-element : size, chemical composition, detailed shape, <b>level</b> <b>of</b> <b>aggregation,</b> combination with other vectors, etc. Above all, these properties would have to be determined not only on the nanocomponent before its introduction in the living environment but also in the (mostly aqueous) biological environment.|$|E
5000|$|Additionally, {{national}} statistical offices {{may also}} publish SNA-type data series. More detailed data at a lower <b>level</b> <b>of</b> <b>aggregation</b> is often available on request. Because national accounts data is notoriously prone to revision (because it involves {{a very large number}} of different data sources, entries and estimation procedures impacting on the totals), there are often discrepancies between the totals cited for the same accounting period in different publications issued in different years. The [...] "first final figures" [...] may in fact be retrospectively revised several times because of new sources, methods or conceptual changes. The yearly revisions may be quantitatively slight, but cumulatively across e.g. ten years they may alter a trend significantly. This is something the researcher should bear in mind in seeking to obtain a consistent data set.|$|E
5000|$|Fact tables {{provide the}} (usually) {{additive}} values {{that act as}} independent variables by which dimensional attributes are analyzed. Fact tables are often defined by their grain. The grain of a fact table represents the most atomic level by which the facts may be defined. The grain of a SALES fact table might be stated as [...] "Sales volume by Day by Product by Store". Each record in this fact table is therefore uniquely defined by a day, product and store. Other dimensions might be members of this fact table (such as location/region) but these add nothing to {{the uniqueness of the}} fact records. These [...] "affiliate dimensions" [...] allow for additional slices of the independent facts but generally provide insights at a higher <b>level</b> <b>of</b> <b>aggregation</b> (a region contains many stores).|$|E
40|$|The non-additive {{nature of}} VaR results {{presents}} special problems in reporting. This paper describes {{the mechanics of}} VaR calculation {{and the maintenance of}} elemental correlations across all <b>levels</b> <b>of</b> <b>aggregation,</b> such as enterprisewide, business division and instrument type. Examples of graphs used in reporting are also given...|$|R
50|$|These CII {{categories}} and subcategories {{account for a}} high percentage of the total CII use. Some of these building/establishment types are identified at some <b>levels</b> <b>of</b> <b>aggregation</b> <b>of</b> the North American Industry Classification System (NAICS) and former Standard Industrial Classification (SIC) codes. Also, the Energy Information Administration (EIA) identified 85 types of commercial buildings and facilities which were grouped into 16 general categories. Many of these overlap with the list of 14 groupings listed above.|$|R
30|$|Scientific {{logistical}} inquiry may {{refer to}} different sections and different <b>levels</b> <b>of</b> <b>aggregation</b> <b>of</b> economic systems. The approach of scientific logistics {{is open to}} a wide range of issues. The network model is generic because of its property of self-similarity. Any logistical issue can be interpreted as a networks of flows, which may be part of a higher-level network.|$|R
5000|$|Automotive Manufacturers spend a {{substantial}} amount of their marketing budgets on dealer advertising, which may not be accurately measurable if not modeled at the right <b>level</b> <b>of</b> <b>aggregation.</b> If modeled at the national level or even the market or DMA level, these effects may be lost in aggregation bias. On the other hand going {{all the way down to}} dealer-level may overestimate marketing effectiveness as it would ignore consumer switching between dealers in the same area. The correct albeit rigorous approach would be to determine what dealers to combine into 'addable' common groups based on overlapping 'trade-areas' determined by consumer zip codes and cross-shopping information. At the very least 'Common Dealer Areas' can be determined by clustering dealers based on geographical distance between dealers and share of county sales. Marketing-mix models built by 'pooling' monthly sales for these dealer clusters will be effectively used to measure the impact of dealer advertising effectively.|$|E
5000|$|TFP {{is often}} {{interpreted}} as a rough average measure of productivity, more specifically the contribution to economic growth made by factors such as technical and organisational innovation. (OECD 2008,11). The most famous description is that of Solow’s (1957): ”I am using the phrase ’technical change’ as a shorthand expression {{for any kind of}} shift in the production function. Thus slowdowns, speed ups, improvements in the education of the labor force and all sorts of things will appear as ’technical change’ ”. The original MFP model (Solow 1957) involves several assumptions: that there is a stable functional relation between inputs and output at the economy-wide <b>level</b> <b>of</b> <b>aggregation,</b> that this function has neoclassical smoothness and curvature properties, that inputs are paid the value of their marginal product, that the function exhibits constant returns to scale, and that technical change has the Hicks’n neutral form (Hulten, 2009,5). In practice, TFP is [...] "a measure of our ignorance", as Abramovitz (1956) put it, precisely because it is a residual. This ignorance covers many components, some wanted (like the effects of technical and organizational innovation), others unwanted (measurement error, omitted variables, aggregation bias, model misspecification) (Hulten 2000,11). Hence the relationship between TFP and productivity remains unclear.|$|E
50|$|The second axiom (FP6), Value is cocreated by {{multiple}} actors, always including the beneficiary, contradicts the traditional worldview, in which firms {{are seen as}} the sole creator of value. Rather, it suggests that value {{is something that is}} always cocreated through the interaction of actors, either directly or indirectly (e.g., through goods). This axiom also enables one to see more clearly that the service-oriented view is inherently relational, because value does not arise prior to exchange transaction, but rather following it, {{in the use of the}} exchanged resources, in a particular context and in conjunction with resources provided by other service providers. This value creation is seen as unfolding, over time, with a consequence of continuing social and economic exchange, implicit contracts, and relational norms. The original scope for this axiom was intended to shift the primary locus of value creation from the firm's sphere to the customer's and from the primacy of value-in-exchange, toward the primacy of value-in-use. More recently, S-D logic has begun to use to term value-in-context to capture the notion that value must be understood in the context of the beneficiary's world and the associated resources and other actors (Vargo et al. 2009). This collaborative nature of value creation is best viewed from a higher <b>level</b> <b>of</b> <b>aggregation</b> than the dyad (e.g., meso- or macro levels) (Chandler and Vargo, 2011). That is, value co-creation through service-for-service exchange is at the very heart of society. It is also important to distinguish between co-production and the co-creation of value (Lusch and Vargo, 2006). Co-production refers to the customer's participation in the creation of the value-proposition (the firms offering), such as through co-design, customer-assembly, self-service, etc. Co-production is thus relatively optional and its advisability depends on a host of firm and customer conditions. This is different from co-creation of value, which is intended to capture the essential nature of value creation: it always involves the beneficiary's participation (through use, integration with other resources, etc.,) in some manner.|$|E
40|$|Within {{the areas}} of Computational Organisation Theory and Artificial Intelligence, {{techniques}} {{have been developed to}} simulate and analyse dynamics within organisations in society. Usually these modelling techniques are applied to factories and to the internal organisation of their process flows, thus obtaining models of complex organisations at various <b>levels</b> <b>of</b> <b>aggregation.</b> The dynamics in living cells are often interpreted in terms of well-organised processes, a bacterium being considered a (micro) factory. This suggests that organisation modelling techniques may also benefit their analysis. Using the example of Escherichia coli it is shown how indeed agent-based organisational modelling techniques can be used to simulate and analyse E. coli’s intracellular dynamics. Exploiting the abstraction levels entailed by this perspective, a concise model is obtained that is readily simulated and analysed at the various <b>levels</b> <b>of</b> <b>aggregation,</b> yet shows the cell’s essential dynamic patterns...|$|R
40|$|Individual- and household-based {{aggregate}} {{measures of}} joblessness can offer conflicting signals about labour market performance if work is unequally distributed. The paper introduces an index {{that can be}} used to measure both the extent and the sources of divergence between non-employment rates calculated at the two <b>levels</b> <b>of</b> <b>aggregation.</b> Built around a comparison of the actual household joblessness rate with that which would occur if work were equally distributed, the index conforms to basic axioms of consistency and can be decomposed and applied to any binary variable of interest measured at two different <b>levels</b> <b>of</b> <b>aggregation.</b> Applying these measures to data for Britain, we show that rising inequality in the distribution of work is mostly within group, largely unrelated to changes in size of household or the principal characteristics that are associated with individual joblessness. Copyright (c) 2008 Royal Statistical Society. ...|$|R
40|$|We study a newly {{constructed}} panel {{data set}} of relative prices {{of a large}} number of consumer goods among 31 European countries. We find that there is a substantial and nondiminishing deviation from PPP at all <b>levels</b> <b>of</b> <b>aggregation,</b> even among euro zone members. However, real exchange rates are very closely tied to relative GDP per capita within Europe, both across countries and over time. This relationship is highly robust at all <b>levels</b> <b>of</b> <b>aggregation.</b> We construct a simple two-sector endowment economy model of real exchange rate determination. Simulating the model using the historical relative GDP per capita for each country, we find that for most (but not all) countries there is a very close fit between the actual and simulated real exchange rate. Foreign exchange rates; Prices; Econometric models; Gross domestic product; International trade; Purchasing power parity...|$|R
