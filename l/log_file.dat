915|1679|Public
5|$|The {{project has}} also ported the NSA's FLASK/TE {{implementation}} from SELinux to FreeBSD. Other work includes {{the development of}} OpenBSM, an open source implementation of Sun's Basic Security Module (BSM) API and audit <b>log</b> <b>file</b> format, which supports an extensive security audit system. This was shipped as part of FreeBSD 6.2. Other infrastructure work in FreeBSD performed {{as part of the}} TrustedBSD Project has included GEOM and OpenPAM.|$|E
25|$|In {{addition}} to the aforementioned three components, the black box audit <b>log</b> <b>file</b> {{is also an important}} part of continuous auditing. This file can be viewed {{as an extension of the}} existing practice of documenting audit activities in manual or automated work papers. A black box <b>log</b> <b>file</b> is a read-only, third-party controlled record of the actions of auditors. The objective of black box logging is to protect a continuous auditing system against auditor and management manipulations.|$|E
25|$|Debugging tactics {{can involve}} {{interactive}} debugging, control flow analysis, unit testing, integration testing, <b>log</b> <b>file</b> analysis, monitoring at the application or system level, memory dumps, and profiling.|$|E
3000|$|... • The {{twenty four}} <b>log</b> <b>files</b> {{of the attack}} (six <b>log</b> <b>files</b> for all executions of each attack step and smurf attack) with the {{respective}} <b>log</b> <b>files</b> for normal system operation.|$|R
40|$|Abstract. In many {{application}} areas, systems reports occurring {{events in}} a kind of textual data called usually <b>log</b> <b>files.</b> <b>Log</b> <b>files</b> report the status of systems, products, or even causes of problems that can occur. The Information extracted from <b>log</b> <b>files</b> of computing systems can be considered one of the important resources of information systems. <b>Log</b> <b>files</b> are considered as a kind of “complex textual data”, i. e. the multi-source, heterogeneous, andmulti-format data. In this paper, we aim particularly at exploring the lexical structure of these <b>log</b> <b>files</b> in order to extract the terms used in <b>log</b> <b>files.</b> These terms will be used in the building of domain ontology and also in enrichment of features of <b>log</b> <b>files</b> corpus. According to features of such textual data, applying the classical methods of information extraction is not an easy task, more particularly for terminology extraction. Here, we introduce a new developed version of EXTERLOG, our approach to extract the terminology from <b>log</b> <b>files,</b> which is guided by Web to evaluate the extracted terms. We score the extracted terms by a Web and context based measure. We favor the more relevant terms of domain and emphasize the precision by filtering terms based on their scores. The experiments show that EXTERLOG is well-adapted terminology extraction approach from <b>log</b> <b>files.</b> ...|$|R
5000|$|Coordinating {{the reading}} of the {{transaction}} logs and the archiving of <b>log</b> <b>files</b> (database management software typically archives <b>log</b> <b>files</b> off-line on a regular basis).|$|R
25|$|Common <b>Log</b> <b>File</b> System (CLFS) API {{provides}} a high-performance, general-purpose log-file subsystem that dedicated user-mode and kernel-mode client applications can use and multiple clients can share to optimize log access and for data and event management.|$|E
25|$|FieldShield is a CoSort {{spin-off}} {{designed to}} protect data privacy. The software protects personally identifiable information and other private data at the field or record level within database tables, files and other sources subject to data spill. Privacy functions include AES encryption, data masking, and pseudonymization. Job details can be audited from a <b>log</b> <b>file</b> in XML format.|$|E
25|$|Facebook uses a {{combination}} platform based on HBase to store data across distributed machines. Using a tailing architecture, new events {{are stored in}} log files, and the logs are tailed. The system rolls these events up and writes them into storage. The user interface then pulls the data out and displays it to users. Facebook handles requests as AJAX behavior. These requests are written to a <b>log</b> <b>file</b> using Scribe (developed by Facebook).|$|E
40|$|In many domains, the <b>log</b> <b>files</b> {{generated}} by digital systems contain important {{information on the}} conditions and configurations of systems. Information Extraction from these <b>log</b> <b>files</b> is an essential phase in information systems, which manage the production line. In the case of Integrated Circuit designs, <b>log</b> <b>files</b> {{generated by}} design tools are not exhaustively exploited. Although these <b>log</b> <b>files</b> are written in English, they usually do not respect the grammar and the structures of natural language. Moreover, such logs have a heterogeneous and evolving structure. According to features of such textual data, applying the classical methods of information extraction {{is not an easy}} task, more particularly for terminology extraction. In this paper, we thus introduce our approach EXTERLOG to extract the terminology from such <b>log</b> <b>files.</b> We also aim at knowing if POS tagging of such <b>log</b> <b>files</b> is a relevant approach for terminology extraction. ...|$|R
5000|$|This process writes redo <b>log</b> <b>files</b> to disk. <b>Log</b> <b>files</b> contain all {{information}} {{about changes in}} the database's data. They are used for fast transaction processing and restoration.|$|R
40|$|Abstract. Process mining {{techniques}} {{are applied to}} single computer <b>log</b> <b>files.</b> But many processes are supported by different software tools and are by consequence recorded into multiple <b>log</b> <b>files.</b> Therefore {{it would be interesting}} to find a way to automatically combine such a set of <b>log</b> <b>files</b> for one process. In this paper we describe a technique for merging <b>log</b> <b>files</b> based on a genetic algorithm. We show with a generated test case that this technique works and we give an extended overview of which research is needed to optimise and validate this technique...|$|R
25|$|Usually, {{the most}} {{difficult}} part of debugging is finding the bug. Once it is found, correcting it is usually relatively easy. Programs known as debuggers help programmers locate bugs by executing code line by line, watching variable values, and other features to observe program behavior. Without a debugger, code may be added so that messages or values may be written to a console or to a window or <b>log</b> <b>file</b> to trace program execution or show values.|$|E
25|$|Web {{traffic is}} {{measured}} {{to see the}} popularity of web sites and individual pages or sections within a site. This {{can be done by}} viewing the traffic statistics found in the web server <b>log</b> <b>file,</b> an automatically generated list of all the pages served. A hit is generated when any file is served. The page itself is considered a file, but images are also files, thus a page with 5 images could generate 6 hits (the 5 images and the page itself). A page view is generated when a visitor requests any page within the web site– a visitor will always generate at least one page view (the main page) but could generate many more.|$|E
2500|$|... filename.out – The <b>log</b> <b>file</b> for the CHARMM run, {{containing}} echoed commands, {{and various}} amounts of command output. The output print level {{may be increased}} or decreased in general, and procedures such as minimization and dynamics have printout frequency specifications. The values for temperature, energy pressure, etc. are output at that frequency.|$|E
30|$|Client <b>log</b> <b>files</b> {{are based}} on browser plugins [6], java scripts, and java applets that are {{integrated}} with websites. When dealing with code running on the client side, the issue of privacy comes to the surface. Thus, {{it is recommended that}} the collection of data streams remains not directly related to users since malicious analysts could benefit from flowing traffic to violate users’ privacy [7, 8]. Concerning Proxy <b>log</b> <b>files,</b> they are generated by Proxy servers, which are commonly deployed by organizations to reduce Internet traffic usage. It may appear therefore that Proxy <b>log</b> <b>files</b> should be used along with web server <b>log</b> <b>files</b> to get a better understanding of Internet surfing. In this regard, we will demonstrate that our approach does not require getting hold of data stored on proxy servers, which could be regarded as a desirable feature. Finally, with regard to web server <b>log</b> <b>files,</b> they are automatically generated, and are the most commonly used <b>log</b> <b>files</b> for usage mining. These files do not contain entries of pages served by proxy servers to users, and consequently are not entered into web server <b>log</b> <b>files</b> [3].|$|R
5000|$|BES also {{produces}} {{a set of}} <b>log</b> <b>files</b> during operation, called the BES Event <b>Log.</b> The <b>log</b> <b>files</b> include (for a BES v4.0 and 4.1 system connecting to Microsoft Exchange): ...|$|R
40|$|Abstract- In this paper, a novel {{algorithm}} is proposed to protect the integrity of <b>log</b> <b>files.</b> Unlike other existing schemes, the proposed algorithm can detect and locate any malicious modifications made to the <b>log</b> <b>files.</b> Furthermore, massive deletion of continuous data can be classified and identified. Security analysis shows that the algorithm can detect modifications with high probability which is verified by the experimental results. In real application, the proposed algorithm can be built into the generation procedure of the <b>log</b> <b>files,</b> so no extra process is needed to embed the watermark for the <b>log</b> <b>files...</b>|$|R
2500|$|Pervasive.SQL 7 was {{released}} in March, 1998, and included Scalable SQL 4 and Btrieve 7.0. Btrieve 7.0 ran on the same platforms as Btrieve 6.x: Windows 95, Windows NT 3.51 & 4, Netware and DOS. However, the company changed to a component-based architecture called SmartComponents to resolve compatibility issues with upgrades. This used a component identification scheme both embedded into the file and encoded into the file name, along with dynamic binding of [...] "glue files" [...] (DLLs loaded into memory only when needed). The dynamic binding of components was done using a new [...] "Abstract OS Services DLL" [...] that looked for {{the latest version of}} the appropriate needed component via the file name encoding. This [...] "glue module" [...] is then loaded into memory and used. The old <b>log</b> <b>file</b> format of Btrieve 6.x was also replaced with a new centralised log called PVSW.LOG and that had a unified and enhanced <b>log</b> <b>file</b> format. They also improved their error messages and error message reporting mechanisms.|$|E
50|$|Urchin {{software}} can be run {{in two different}} data collection modes: <b>log</b> <b>file</b> analyzer or hybrid. As a <b>log</b> <b>file</b> analyzer, Urchin processes web server log files {{in a variety of}} <b>log</b> <b>file</b> formats. Custom file formats can also be defined. As a hybrid, Urchin combines page tags with <b>log</b> <b>file</b> data to eradicate the limitations of each data collection method in isolation. The result is more accurate web visitor data.|$|E
5000|$|...appendtrace: If true, {{then it will}} append {{the trace}} {{at the end of}} a <b>log</b> <b>file.</b> If false, then it will {{override}} the <b>log</b> <b>file</b> for the invocation of wsadmin.|$|E
40|$|<b>Log</b> <b>files</b> are {{commonly}} inspected by system administra-tors and developers to detect suspicious behaviors and di-agnose failure causes. Since size of <b>log</b> <b>files</b> grows fast, thus making manual analysis impractical, different automatic techniques {{have been proposed}} to analyze <b>log</b> <b>files.</b> Unfor-tunately, accuracy and effectiveness of these techniques are often limited by the unstructured nature of logged messages and the variety of data that can be logged. This paper presents a technique to automatically analyze <b>log</b> <b>files</b> and retrieve important information to identify fail-ure causes. The technique automatically identifies depen-dencies between events and values in logs corresponding to legal executions, generates models of legal behaviors and compares <b>log</b> <b>files</b> collected during failing executions with the generated models to detect anomalous event sequences that are presented to users. Experimental results show {{the effectiveness of the}} technique in supporting developers and testers to identify failure causes. ...|$|R
40|$|International audienceThe <b>log</b> <b>files</b> {{generated}} by digital {{systems can be}} used in management information systems as the source of important information on the condition of systems. However, <b>log</b> <b>files</b> are not exhaustively exploited in order to extract information. The classical methods of information extraction such as terminology extraction methods are irrelevant to this context because of the specific characteristics of <b>log</b> <b>files</b> like their heterogeneous structure, the special vocabulary {{and the fact that they}} do not respect a natural language grammar. In this paper, we introduce our approach Exterlog to extract the terminology from <b>log</b> <b>files.</b> We detail how it deals with the particularity of such textual data...|$|R
40|$|Abstract- For any {{organization}} which depending on sensitive data processing {{it is very}} important to maintain information about every event occurring within the organization’s system or network, these event details are called as <b>log</b> <b>files.</b> The <b>log</b> <b>files</b> will able to record user activities, troubleshooting problems and any policy violations. As this <b>log</b> <b>files</b> plays vital role and also contains sensitive information, it should be maintained very securely. The capital expenses will be very immense to maintain log data for m{{any organization}}s over long period. The alternative economic solution is to maintaining <b>log</b> <b>files</b> over a cloud database. <b>Log</b> <b>files</b> with sensitive information over a cloud environment will leads to challenges about confidentiality and privacy. In this paper, we propose effective secure cloud-based log management and also the use of homomorphic encryption as a solution for dealing the issues to access a cloud based data storage. 1...|$|R
50|$|This is {{a simple}} inetd service, written in C. It expects a command line {{argument}} containing a filename for a <b>log</b> <b>file,</b> and then it logs all strings sent through the socket to the <b>log</b> <b>file.</b> Note {{that this is a}} very insecure example program.|$|E
5000|$|...tracefile assigns <b>log</b> <b>file</b> {{name and}} {{location}} for the log output.|$|E
50|$|This command asks WEPCrack to {{determine}} the key from the <b>log</b> <b>file.</b>|$|E
40|$|The paper {{discusses}} {{our research}} in development of general and systematic methods for intrusion prevention. The key {{idea is to}} use data mining techniques to discover repeated patterns of system features that describe program and user behavior. Server systems customarily write comprehensive activity logs whose value is useful in detecting intrusion. Unfortunately, production volumes overwhelm the capacity and manageability of traditional approach. This paper discusses the issues involving largescale log processing that helps to analyze log records. Here, we propose to analyze intersections of firewall <b>log</b> <b>files</b> with application <b>log</b> <b>files</b> installed on one computer, as well as intersections resulting from firewall <b>log</b> <b>files</b> with application <b>log</b> <b>files</b> coming from different computers. Intersections of <b>log</b> <b>files</b> are substantially shorter than full logs and consist of records that indicate abnormalities in accessing single computer or set of computers. The paper concludes with some lessons we learned in building the system...|$|R
40|$|Abstract- In current trend, {{most of the}} {{businesses}} are running through online web applications such as banking, shopping, and several other e-commerce applications. Hence, securing the web sites is becomes must do task {{in order to secure}} sensitive information of end users as well as organizations. Web <b>log</b> <b>files</b> are generated for each user whenever he/she navigates through such e-commerce websites, users every click is recorded into such web <b>log</b> <b>files.</b> The analysis of such web <b>log</b> <b>files</b> now a day’s done using concepts of data mining. Further results of this data mining techniques are used in many applications. Most important use of such mining of web logs is in web intrusion detection. To improve the efficiency of intrusion detection on web, we must have efficient web mining technique which will process web <b>log</b> <b>files.</b> In this project, our first aim is to present the efficient web mining technique, in which we will present how various web <b>log</b> <b>files</b> in different format will combined together in one XML format to further mine and detect web attacks. And because <b>log</b> <b>files</b> usually contain noisy and ambiguous data this project will show how data will be preprocessed before applying mining process in order to detect attacks. Hence mining process includes two parts, web <b>log</b> <b>files</b> preprocessing in order to remove the noise or ambiguous data mining process to detect the web attacks...|$|R
5000|$|Redo <b>log</b> <b>files,</b> {{recording}} all {{changes to}} the database - used to recover from an instance failure. Often, a database stores these files multiple times for extra security in case of disk failure. Identical redo <b>log</b> <b>files</b> are associated in a [...] "group".|$|R
5000|$|In most {{database}} management systems durable database transactions are supported through a <b>log</b> <b>file.</b> However, multiple writes {{to the same}} page of that file can produce a slim chance of data loss. Assuming for simplicity that the <b>log</b> <b>file</b> is organized in pages whose size matches the block size of its underlying medium, the following problem can occur: ...|$|E
5000|$|The main {{advantages}} of <b>log</b> <b>file</b> analysis over page tagging are as follows: ...|$|E
50|$|Texmaker {{automatically}} locates {{errors and}} warnings {{detected in the}} <b>log</b> <b>file</b> after a compilation.|$|E
40|$|In my Kickstart paper I covered basic Unix <b>log</b> <b>files</b> with a {{configuration}} file that gathered everything. I {{would like to}} expand on that and now cover messages found in those <b>log</b> <b>files</b> that would cause concern and require further investigation. My selection to continue on this subject lies in my inability to find comprehensive information that provides direction to administrators, particularly those in federal government, on what messages in <b>log</b> <b>files</b> could require critical attention and reporting. Copyright SANS Institut...|$|R
40|$|As {{has been}} {{described}} else where, web <b>log</b> <b>files</b> are a useful {{source of information about}} visitor site use, navigation behaviour, and, to some extent, demographics. But <b>log</b> <b>files</b> can also reveal the existence of both web pages and search engine queries that are sources of new visitors. This study extracts such information from a single web <b>log</b> <b>files</b> and uses it to illustrate its value, not only to th site owner but also to those interested in investigating the online behaviour of web users...|$|R
40|$|In many {{real-world}} applications, {{sensitive information}} {{must be kept}} in <b>log</b> <b>files</b> on an untrusted machine. In the event that an attacker captures this machine, {{we would like to}} guarantee that he will gain little or no information from the <b>log</b> <b>files</b> and to limit his ability to corrupt the <b>log</b> <b>files.</b> We describe a computationally cheap method for making all log entries generated prior to the logging machine's compromise impossible for the attacker to read, and also impossible to undetectably modify or destroy...|$|R
