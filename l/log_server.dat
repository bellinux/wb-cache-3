24|780|Public
5000|$|DXA: The Real-time Online Radio <b>Log</b> <b>Server</b> (Cordell Expeditions, 2013) ...|$|E
5000|$|... accesslog: <b>log</b> <b>server</b> {{activity}} in another LDAP database, for LDAP-accessible logging ...|$|E
50|$|Clipperton Island: Schmieder {{organized}} {{and led the}} March, 2013, expedition to Clipperton Island. The team of 29 made the first discovery of foraminifera on Clipperton, and set a record {{for the number of}} radio contacts from there (113,601). This expedition and the 2005 expedition to Kure (above) are described in his book DXA. The Real-time Online Radio <b>Log</b> <b>Server.</b>|$|E
40|$|In {{the online}} world, {{increasing}} amounts of personal data of users are processed by service providers and other third party data processors. The privacy principle of transparency means that users should be informed about what data about them are processed by whom and how. This report describes a log service that enables users to reconstruct a log trail of these actions, by retrieving log entries from <b>log</b> <b>servers.</b> The information that links log entries into a trail is logged — together with the encrypted action data — to dedicated <b>log</b> <b>servers</b> by the data processors. In the proposed service, log entries of one trail can be spread over different <b>log</b> <b>servers,</b> possibly generated by different data processors. The fact that certain information is logged about a user can already leak information, and adding linking information only elevates this privacy risk. Encrypting the logged information does not suffice to protect against this. In our system, linking information across log databases is cryptographically protected and is only meaningful to the user to whom the log entries relate. In the report, we also {{consider the fact that}} <b>log</b> <b>servers</b> and data processors can become compromised. The scheme is therefore auditable, meaning that a third party can validate that <b>log</b> <b>servers</b> cannot make any changes to log entries without being detected, even when they collude with other <b>log</b> <b>servers</b> or data processors. EU/FP 7 PrimeLifeIBBT/Inde...|$|R
30|$|The hosts {{that produce}} these logs can have Variety such as {{end-user}} computer workstations, computer infrastructure servers, devices, appliances, virtualization hosts, or even cloud-based hosts. The types of logs being generated are heterogeneous and {{can vary from}} Operating System events {{to a wide variety}} of application events such as antivirus software, firewall logs, honeypot activity, web <b>server</b> <b>logs,</b> ftp <b>server</b> <b>logs,</b> email <b>server</b> <b>logs,</b> domain controller logs, web proxy logs, VPN <b>logs,</b> DHCP <b>server</b> <b>logs,</b> etc. While this is not a comprehensive listing of the various types of logs one can encounter in the typical organization, it illustrates that the various types of logs can pose Big Variety challenges in having to correlate security events across a wide range of heterogeneous log types.|$|R
40|$|The AFS servers at the Center for Information Technology Integration {{have been}} modiﬁed to trace and <b>log</b> file <b>server</b> activity. This report {{discusses}} the AFS modiﬁcations {{and the structure}} of the trace files and data. We also describe three large datasets collected from the <b>logging</b> <b>servers,</b> available to other researchers...|$|R
50|$|OP5 Monitor is a {{software}} product for server, Network {{monitoring and management}} based on the Open Source project Nagios, is further developed and supported by OP5 AB. OP5 Monitor displays the status, health and performance of the IT network being monitored and has an integrated <b>log</b> <b>server,</b> OP5 Logger. The company sells downloadable software that monitor, visualize and troubleshoot IT environments and collect information both from hardware, software, virtual and/or cloud based services.|$|E
50|$|Administrators {{are allowed}} to view and clear the log (there {{is no way to}} {{separate}} the rights to view and clear the log). In addition, an Administrator can use Winzapper to delete specific events from the log. For this reason, once the Administrator account has been compromised, the event history as contained in the Security Log is unreliable. A defense against this is to set up a remote <b>log</b> <b>server</b> with all services shut off, allowing only console access.|$|E
50|$|As Benton points out, {{one way of}} {{preventing}} successful attacks is security through obscurity. Keeping the IT department's security systems and practices confidential helps prevent users from formulating ways to cover their tracks. If users are aware that the log is copied over to the remote <b>log</b> <b>server</b> at :00 of every hour, for instance, they may take measures to defeat that system by attacking at :10 and then deleting the relevant log events before {{the top of the}} next hour.|$|E
5000|$|Computer data logging: <b>logging</b> APIs, <b>server</b> <b>logs</b> & syslog, web logging & web {{counters}} ...|$|R
5000|$|The SID {{is stored}} in many places (browser history <b>log,</b> web <b>server</b> <b>log,</b> proxy logs, ...) ...|$|R
50|$|Auditors also act as {{clients to}} the <b>log</b> <b>servers.</b> Certificate Transparency {{auditors}} use partial {{information about a}} log to verify the log against other partial information they have.|$|R
40|$|Keeping logs {{secure and}} untampered with {{has always been}} a problem. The first things a cracker goes after once a system has been {{compromised}} are the logs[9]. They do this in order to cover their tracks and to hide their presence. The preferred current solution the use of remote logging [14, p 372]. Even though this increases security of your log files, there is still the chance that a skilled cracker will be able to break into your <b>log</b> <b>server,</b> and tamper with your logs. A simple way to effectively reduce the chances that a computer can be cracked is by cutting the transmit wires on the communication medium. This effectively prohibits TCP connections and reduces the device to a receiving only station. This further reduces the chances of this computer being hacked simply because {{there is no way to}} get feedback from it, or even detect it. Private networks, with no routing into or out of the network, are an effective way of establish-ing secure and reliable communication between a set of stations. In order to gain access to the network a station on the perimeter has to be compromised before access to the private network can be gained. This paper proposes an alternative manner in which to perform logging. It makes use of a silent <b>log</b> <b>server,</b> preferably inside a private network. A dummy <b>log</b> <b>server</b> can also be used inside this private network to heighten security by hiding the fact that you are using a silent logger. ...|$|E
40|$|A {{large user}} base relies on {{software}} updates provided through package managers. This provides a unique lever {{for improving the}} security of the software update process. We propose a transparency system for software updates and implement it for a widely deployed Linux package manager, namely APT. Our system is capable of detecting targeted backdoors without producing overhead for maintainers. In addition, in our system, the availability of source code is ensured, the binding between source and binary code is verified using reproducible builds, and the maintainer responsible for distributing a specific package can be identified. We describe a novel "hidden version" attack against current software transparency systems and propose as well as integrate a suitable defense. To address equivocation attacks by the transparency <b>log</b> <b>server,</b> we introduce tree root cross logging, where the log's Merkle tree root is submitted into a separately operated <b>log</b> <b>server.</b> This significantly relaxes the inter-operator cooperation requirements compared to other systems. Our implementation is evaluated by replaying over 3000 updates of the Debian operating system over the course of two years, demonstrating its viability and identifying numerous irregularities...|$|E
40|$|This thesis {{focuses on}} hash-chain based {{protocols}} for time-stamping and secure logging. Any electronic service should offer transparency to data subjects in how their personal data is gathered, stored and processed by data processors. Secure logging as {{presented in this}} thesis {{can be used as}} a transparency-enhancing tool to achieve this service. One of the enabling technologies that we use to implement our transparency-enhancing tool is linked time-stamping. This technique allows binding digital information to time, using hash chains, establishing a one-way dependency between the issued time-stamps. Inserting or changing time-stamps is therefore unfeasible, even for the time-stamp issuer. One way to facilitate the adoption of linked time-stamping technology is to standardise the format of the issued time-stamps. This has been done for binary formats, but not yet for modern data standards such as XML. In the first part of this thesis, we propose several elements that facilitate the XML-standardisation of linked time-stamp tokens. We also propose an actual integration of our work into the OASIS DSS standard, which can already issue non-linked XML time-stamp tokens. In the second part, we present the definition of two protocols for auditable, secure, distributed, and privacy-preserving logging with log trail reconstruction by the data subject. The log trail reconstruction turns these protocols into transparency-enhancing tools. Hash-chains are used to link log entries that are related to the same data subject. In the first version of the protocol the log servers are marginally trusted, mainly to keep their stored logs safe: stored log events within the same <b>log</b> <b>server</b> are trivially linkable. The second version of the protocol -the bulk of the contribution- assumes less trust in the <b>log</b> <b>server,</b> and builds hash-chains through the logged events, for integrity checking and identification. This makes stored log entries within the same <b>log</b> <b>server</b> unlinkable. status: publishe...|$|E
40|$|In {{this paper}} {{we present a}} scheme for {{building}} a logging-trail for processes related to eGovernment services. A cit-izen can reconstruct the trail of such a process and verify its status if he {{is the subject of}} that process. Reconstruc-tion is based on hand-overs, special types of log events, that link data stored by multiple <b>logging</b> <b>servers,</b> which are not necessarily trusted. Our scheme is privacy-friendly in the sense that only the authorised subject, i. e. the citizen, can link the different log entries related to one specific process. The scheme is also auditable; it allows <b>logging</b> <b>servers</b> to show that they behave according to a certain policy. 1...|$|R
40|$|ABSTRACT – User clicks on Link or {{gone through}} any {{web site at}} that time {{particular}} user’s browsing data or browsing actions are captured by <b>server</b> on <b>server</b> <b>log</b> files. In this paper, it shown how browing détails stores in web logs and also mentions different types of fields in web logs. By knowing this kind of field we can easily work with our web usage mining applications. By extraction méthods we can use only those fields which is being used in our applications. Now {{we are going to}} understand what actually in to the <b>server</b> <b>log</b> files or web logs. We understand all the different fields which stores in the <b>server</b> <b>log</b> files by user single click. Let’s see one example of <b>server</b> <b>log</b> data. And see how actually web <b>logs</b> or <b>server</b> <b>log</b> files look like...|$|R
50|$|Monitors act as {{clients to}} the <b>log</b> <b>servers.</b> Monitors check <b>logs</b> {{to make sure}} they are {{behaving}} correctly. An inconsistency is used to prove that a log has not behaved correctly, and the signatures on the log's data structure (the Merkle tree) prevent the log from denying that misbehavior.|$|R
40|$|We {{contribute}} to the MIS education literature by empirically examining Web <b>log</b> <b>server</b> data generated by undergraduate students enrolled in multiple sections of a MIS course where an online Learning Management System (LMS) was used to complement a traditional classroom environment. We identify online learning styles by investigating differences in LMS usage pat-terns, finding four distinct usage patterns as well as differences in the level and variation of LMS usage by male and female students. We suggest that online learning styles are important considerations for instructors using instructional technologies {{as well as for}} researchers...|$|E
40|$|Logging {{is often}} a {{forgotten}} security friend for system administrators until a security breach has occurred. The security administrator then goes {{to look at the}} logs only to find that there are no logs, the logs are incomplete, or that the logs have been modified by the attacker himself to cover his tracks. To prevent this from happening, a system administrator should be prepared to have a good local logging system in place and perhaps even a central <b>log</b> <b>server</b> for archiving logs. In the case of a security breach or at [...] . Copyright SANS Institut...|$|E
40|$|With {{the rapid}} growth of the World Wide Web, finding useful {{information}} from the Internet has become a critical issue. Automatic classification of user navigation patterns provides a useful tool to solve these problems. In this paper, we propose an approach for classification of users’ navigation patterns and prediction of users’ future requests. Users’ profiles are constructed based on Web <b>log</b> <b>server</b> files and one of clustering methods is implemented to users’ profiles for assigning navigation patterns. Finally, using neural network, recommender engine produces a relevant recommendation list of web pages to the active user. The preliminary results indicate that the proposed approach has high accuracy and coverage in prediction of users’ future requests...|$|E
40|$|When using primary-backup replication, one checkpoints the primary’s {{state to}} reduce the {{failover}} time to a backup, upon failure of the primary. A trade-off is involved: by frequent checkpointing the response time for requests during “no-failure ” intervals is increased. On the other hand, when checkpointing frequency is low, failover takes longer. In this paper ¢ we provide a theoretical ground for computing the optimal checkpointing interval that minimizes the average response time of requests, given parameters such as load, failure rate and service rates. We use queuing theory for modelling the support for failover in a system where a <b>logging</b> <b>server</b> records client update operations. The novelty of the model is inclusion of the backlog processing time for requests accumulated during failover. Also, our model considers the waiting times in the queue of the <b>logging</b> <b>server</b> {{as well as the}} application server. ...|$|R
50|$|A typical {{example is}} a web <b>server</b> <b>log</b> which {{maintains}} a history of page requests. The W3C maintains a standard format (the Common Log Format) for web <b>server</b> <b>log</b> files, but other proprietary formats exist. More recent entries are typically appended {{to the end of}} the file. Information about the request, including client IP address, request date/time, page requested, HTTP code, bytes served, user agent, and referrer are typically added. This data can be combined into a single file, or separated into distinct logs, such as an access log, error log, or referrer <b>log.</b> However, <b>server</b> <b>logs</b> typically do not collect user-specific information.|$|R
5000|$|AWStats {{supports}} {{most major}} web <b>server</b> <b>log</b> file formats including Apache (NCSA combined/XLF/ELF log format or Common Log Format (CLF)), WebStar, IIS (W3C log format), {{and many other}} common web <b>server</b> <b>log</b> formats.|$|R
40|$|When {{it comes}} to the {{security}} of the IT system, event logs play a crucial role. Today, many applications, op-erating systems, network devices and other system components are capable of writing se-curity related event messages to log files. The BSD syslog protocol is an event logging stand-ard supported by majority of OS and network equipment vendors, which allows one to set up a central <b>log</b> <b>server</b> for receiving and stor-ing event messages from the whole IT system. There also exist several flexible and powerful syslog server implementations that are suitable for use at the central <b>log</b> <b>server,</b> most notably Syslog-ng. Since event logging is a widely ac-cepted and well-standardized practice, there is a high chance that after a security incident has occurred in an IT system, there is (are) also event log message(s) for it in some log file(s). Because in most cases event messages are appended to event logs in real-time as they are emitted by system components, event logs are an excellent source of infor-mation for monitoring the system, including security conditions that arise in it. Over the past 10 - 15 years, a number of open-source tools have been developed for monitoring event logs in real-time, e. g., Swatch and Log-surfer. However, majority of these tools can accomplish simple tasks only, e. g., raise an alarm immediately after a certain message has been appended to a log file. On the other hand, many essential event processing tasks involve event correlation – a conceptual inter-pretation procedure where new meaning is assigned to a set of events that happen within Simple Event Correlator for real-time security log monitorin...|$|E
40|$|Abstract—Hidden {{services}} are anonymously hosted services {{that can be}} accessed over Tor, an anonymity network. In this paper we present an attack that allows an entity to prove, once a machine suspect to host a hidden server has been confiscated, that such machine has in fact hosted a particular content. Our solution is based on leaving a timing channel fingerprint in the confiscated machine’s log file. In {{order to be able}} to fingerprint the <b>log</b> <b>server</b> through Tor we first study the noise sources: the delay introduced by Tor and the log entries due to other users. We then describe our fingerprint method, and analytically determine the detection probability and the rate of false positives. Finally, we empirically validate our results. I...|$|E
40|$|One {{issue of}} real {{interest}} {{in the area of}} web data mining is to capture users’ activities during connection and extract behavior patterns that help define their preferences in order to improve the design of future pages adapting websites interfaces to individual users. This research is intended to provide, first of all, a presentation of the methodological foundations of the use of probabilistic languages to identify relevant or most visited websites. Secondly, the web sessions are represented by graphs and probabilistic context-free grammars so that the sessions that have the highest probabilities are considered the most visited and most preferred, therefore, the most important in relation to a particular topic. It aims to develop a tool for processing web sessions obtained from a <b>log</b> <b>server</b> represented by probabilistic context-free grammars...|$|E
50|$|The CI is {{designed}} to initiate an interconnection across two CDNs and bootstrap the other CDNI interfaces. For example, the control interface {{can be used to}} provide the address of the <b>logging</b> <b>server</b> in order to bootstrap the logging interface, or {{it can be used to}} establish security associations for other interfaces. It can also allow an uCDN to preposition, revalidate or purge metadata and content on a dCDN.|$|R
40|$|Web {{has been}} growing as a {{dominant}} platform for retrieving information and discovering knowledge from web data. Web data is stored in web <b>server</b> <b>log</b> files. Web usage analysis or web usage mining or web log mining or click stream analysis {{is the process of}} extracting useful knowledge from web <b>server</b> <b>logs,</b> database logs, user queries, client side cookies and user profiles in order to analyze web users’ behavior. Web usage analysis requires data abstraction for pattern discovery. This data abstraction can be achieved through data preprocessing. This paper presents different formats of web <b>server</b> <b>log</b> files and how web <b>server</b> <b>log</b> data is preprocesses for web usage analysis...|$|R
30|$|<b>Server</b> <b>logs.</b> In {{our work}} we use <b>server</b> <b>logs</b> {{as a proxy}} for users’ online food preferences. In {{previous}} work, logs of search engine use have been successfully used to identify temporal trends (cf. [34]), geographic differences (cf. [35]) and to predict real world medical phenomena (cf. [36]). However, to our best knowledge this is the first work which analyzes <b>server</b> <b>log</b> data from recipe platforms to analyze the evolution of online food preferences.|$|R
40|$|The World Wide Web is an {{important}} medium for communication, data transaction and retrieving. Data mining {{is the process of}} extracting interesting patterns from a set of data sources. Web mining is the application of data mining techniques to extract useful patterns from web data. Web Mining can be divided into three categories, web usage mining, web content mining, and web structure mining. Web usage mining or web log mining is the extraction of interesting patterns from web <b>log</b> <b>server</b> entries. Those patterns are used to study user behavior and interests, facilitate support and services introduced to the website user, improve the structure of the website, and facilitate personalization and adaptive websites. This paper aims to explore various research issues in web usage mining and its application in the field of adaptive, and personalized websites. ...|$|E
40|$|Collection, {{storage and}} {{analysis}} of multiple hosts' audit trails in a distributed manner are known as a major requirement, {{as well as a}} major challenge for enterprise-scale computing environments. To ease these tasks, and to provide a central management facility, a software-suit, named as "LogHunter " has been developed. Log-Hunter is a secure distributed <b>log</b> <b>server</b> system which involves log collection and consolidation in a large-scale environment having multiple hosts that keeps at least one audit trail. This architecture also eases the inspection and monitoring of the audit trails generated on multiple hosts. By consolidating all the audit trails on a centralized server, it significantly reduces the manpower requirement, and also provides secure log storage for inspection of log entries as it becomes necessary. This paper presents the functional specifications, architecture and some preliminary performance results of the Log-Hunter...|$|E
40|$|Persistent {{stores have}} been {{implemented}} {{using a variety of}} storage technologies including shadow paging, log-based and log-structured approaches. Here we compare these approaches and advocate the use of log-structuring. The advantages of such a technique include efficient support for large (64 bit) address spaces, scalability and fast snapshot processing. We describe the architecture of a new log-structured persistent store and how it has been used to support resilient persistent processes {{in the context of the}} Grasshopper operating system. This store is based on the use of a <b>log</b> <b>server</b> which provides clients with private logical logs. Keywords persistent stores, log-structured stores, recoverable virtual memory. 1 Introduction The concept of persistence may be defined as the attribute of data which specifies its period of existence. Stemming from this simple definition, a wide range of systems supporting various forms of persistence have emerged. In systems that support orthogonal [...] ...|$|E
40|$|Fine-grained file update logging, which logs every {{update to}} a file system, is an {{enabling}} technology to protect file systems against malicious attacks and/or user mistakes, {{because it allows}} each file update to be undoable. This paper explores the performance tradeoffs of several user-level approaches to fine-grained file update logging. In particular, it examines four file update logging schemes for NFS servers: FUL-O, FUL-A, FUL-I and FUL-I+, which all work {{on top of the}} NFS protocol but entail different performance overhead, implementation complexity and hardware cost. FUL-O takes a naive update-in-place strategy and requires a separate <b>logging</b> <b>server.</b> FUL-A is based on FUL-O but takes a nonoverwrite strategy to reduce the disk I/O overhead. FUL-I is based on FUL-A but eliminates the need of a separate <b>logging</b> <b>server.</b> FUL-I+ is based on FUL-I but incorporates a kernel-level performance optimization mechanism to reduce the context switch and memory copy overhead associated with user-level file update logging. Measurements from running synthetic benchmarks and real-world file access traces demonstrate that the performance of an NFS server protected by these user-level file update logging schemes is actually comparable to that of an unprotected NFS server...|$|R
40|$|Security becomes o crucial port {{with the}} {{enhancements}} of Science and Technology. Physical Access Control {{plays a major}} role in leading security systems all around the world. Our target is to build a very secure Physical Access Control System considering accuracy, reliability and high speed operation. The product includes a Hand held device which is impossible to replicate, a wall console and a report <b>logging</b> <b>server</b> with <b>logging</b> databases. Scalability, redundancy and failsafe capabilities of the system are implemented and tested...|$|R
30|$|Instead of {{embedding}} blocking SQL statements {{into the}} application, we provided the applications with a UDP based {{access to the}} SQL logging. The SQL based <b>logging</b> <b>server</b> runs outside of the simulation environment as a separate process. It receives performance data regarding to simulation from the application {{in the form of}} UDP messages and commits them into the database. The benefit of this approach is two fold. First, SQL coding is decoupled from the geonetworking layer, second geonetworking layer does not block on time consuming SQL transactions.|$|R
