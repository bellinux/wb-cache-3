20|7|Public
30|$|By {{specifying the}} message {{transport}} priority or its <b>latency</b> <b>budget,</b> the transport layer of micROS-drt can schedule its network resources accordingly and decide which {{one should be}} sent firstly. Transport priority/latency budget is the most useful real-time parameter in many real-time distributed robotic systems.|$|E
30|$|CPRI over Ethernet: The use of Ethernet for {{transport}} in the fronthaul seems beneficial {{due to the}} maturity of the technology and its wide adoption. Statistical multiplexing and packet-based transport make CPRI over Ethernet (CoE) [77] efficient and cost effective. However, there are implementation challenges as packetization which introduces overhead and delay to the, already strict, <b>latency</b> <b>budget.</b> Furthermore, Ethernet communication is asynchronous, while fronthaul is characterized by strict synchronization requirements.|$|E
40|$|In 3 GPP the {{architecture}} of a New Radio Access Network (New RAN) has been defined where the evolved NodeB (eNB) functions can be split between a Distributed Unit (DU) and Central Unit (CU). Furthermore, in the virtual RAN (VRAN) approach, such functions can be virtualized (e. g., in simple terms, deployed in virtual machines). Based on the split type, different performance in terms of capacity and latency are requested to the network (i. e., fronthaul) connecting DU and CU. This study experimentally evaluates, in the 5 G segment of the Advanced Research on NetwOrking (ARNO) testbed (ARNO- 5 G), the fronthaul latency requirements specified by Standard Developing Organizations (SDO) (3 GPP in this specific case). Moreover it evaluates how much virtualization impacts the fronthaul <b>latency</b> <b>budget</b> for the the Option 7 - 1 functional split. The obtained results show that, in the considered Option 7 - 1 functional split, the fronthaul latency requirements are about 250 us but they depend on the radio channel bandwidth {{and the number of}} the connected UEs. Finally virtualization further decreases the <b>latency</b> <b>budget.</b> Comment: publication rights achieved typos fixe...|$|E
30|$|In {{contrast}} to other message-oriented communication standards such as Advanced Message Queuing Protocol (AMQP) [7], DDS has {{the following two}} prominent features: (1) Quality of Service (QoS) support. DDS supports the fine control over various QoS parameters related to time constraints, including deadline, <b>latency</b> <b>budgets,</b> delivery order and priority and (2) Scalability. DDS adopts a peer-to-peer model with a fully decentralized architecture unlike many message-oriented middleware, which has a centralized broker. And the support of UDP/IP multicast also makes it scalable {{when the number of}} subscribers in a topic is more than one. Moreover, DDS is a mature and industrialized standard. Both commercial and open-source DDS middleware have widely been applied into many real production systems, such as aerospace, defense, industrial automation and cloud computing systems [3, 8].|$|R
40|$|In {{data center}} applications, {{predictability}} in service time and controlled latency, especially tail latency, {{are essential for}} building performant applications. This {{is especially true for}} applications or services built by accessing data across thousands of servers to generate a user response. Current practice has been to run such services at low utilization to rein in latency outliers, which decreases efficiency and limits the number of service invocations developers can issue while still meeting tight <b>latency</b> <b>budgets.</b> In this paper, we analyze three data center applications, Memcached, OpenFlow, and Web search, to measure the effect of 1) kernel socket handling, NIC interaction, and the network stack, 2) application locks contested in the kernel, and 3) application-layer queueing due to requests being stalled behind straggler threads on tail latency. We propose Chronos, a framework to deliver predictable, low latency in data center applications. Chronos uses a combination of existing and new techniques to achieve this end, for example by supporting Memcached at 200, 000 requests per second per server at mean latency of 10 µs with a 99 th percentile latency of only 30 µs, a factor of 20 lower than baseline Memcached...|$|R
30|$|While {{directing}} their attentions on architecting {{a planning}} model for {{transmission of data}} to the cloud that is cost-effective, Cho et al. (2010) have designed Pandora. Later, these authors {{have gone on to}} propose a solution to curb transmission <b>latency</b> under <b>budget</b> constraints (Cho and Gupta 2011). Their study differs from ours in that they lay emphasis on fixed volume data transfer which is static, rather than data that is produced dynamically. Further, they have considered a single-cloud measure, whereas our study involves more than one data hub.|$|R
40|$|Contemporary {{ubiquitous}} {{information systems}} combine efficiently context-aware and peer-to-peer communication concepts. This paper introduces a novel mobile peer-to-peer (P 2 P) navigation application, NaviP 2 P. In addition to basic navigation functionalities such as GPS positioning, showing the user {{location on the}} map, scrolling and zooming of the map view, the application leverages P 2 P group communication to portray peer group members ’ availability information on the map for starting supported application sessions. In this feasibility study, {{the current status of}} the application prototype is presented with a quantitative latency and cost analysis. End-to-end <b>latency</b> <b>budget</b> is outlined and different map storage and distribution solutions are compared to analyse communications and implementation feasibility. I...|$|E
40|$|Abstract. Streaming media {{applications}} {{under the}} mobile IP networks suffer from playback disruptions resulting from handoff blackout period {{as well as}} fluctuation in the available bandwidth. To overcome possible shortage of buffer, pre-buffering techniques can be adopted where the client stores sufficient part of the stream in advance. However, under the mobile IP handoff situation that may take up to several seconds, {{it is extremely difficult}} to sustain seamless playback. Inaccurate and conservative choice on the required buffering margin can waste limited <b>latency</b> <b>budget,</b> resulting in overall quality degradation. Thus, in this paper, we introduce a scheme that helps estimate the required pre-buffering size more accurately by considering both handoff duration and transient packet losses. Experiments and network simulation results show that the proposed scheme can provide an appropriate guidelines on buffer parameters and thus facilitate seamless streaming over the mobile IP network. ...|$|E
40|$|Abstract — In this paper, {{we review}} the {{requirements}} for forward error correction (FEC) decoding for next generation wireless modems – mobile Worldwide Interoperability for Microwave Access (WiMAX) and third generation partnership project long term evolution (3 GPP LTE). FEC decoder consists of mainly three components: control channel decoder, data channel decoder, and hybrid automatic repeat query (HARQ) combining. Control channel decoder is constrained by <b>latency</b> <b>budget</b> which impacts buffering as well as power management of modem signal processing chains. For WiMAX, both Viterbi and Turbo decoders are required to receive control channel while for LTE, only Viterbi decoder is required. For data-channel, a highthroughput Turbo decoder is required to support high data rate. HARQ combining is mainly dominated by memory size and bandwidth requirements given the maximum data rate, maximum number of HARQ processes and re-transmission formats. We analyze the requirements and discuss possible candidate architectures for three components...|$|E
40|$|The {{state-of-the-art}} mobile {{edge applications}} are generating intense traffic and posing rigorous latency requirements to service providers. While resource sharing across multiple service providers {{can be a}} way to maximize the utilization of limited resources at the network edge, it requires a centralized repository maintained by all parties for service providers to share status. Moreover, service providers have to trust each other for resource allocation fairness, which is difficult because of potential conflicts of interest. We propose EdgeChain, a blockchain-based architecture to make mobile edge application placement decisions for multiple service providers. We first formulate a stochastic programming problem minimizing the placement cost for mobile edge application placement scenarios. Based on our model, we present a heuristic mobile edge application placement algorithm. As a decentralized public ledger, the blockchain then takes the logic of our algorithm as the smart contract, with the consideration of resources from all mobile edge hosts participating in the system. The algorithm is agreed by all parties and the results will only be accepted by majority of the mining nodes on the blockchain. When a placement decision is made, an edge host meeting the consumer's <b>latency</b> and <b>budget</b> requirements will be selected at the lowest cost. All placement transactions are stored on the blockchain and are traceable by every mobile edge service provider and application vendor who consumes resources at the mobile edge. Comment: 9 pages, 7 figures, conferenc...|$|R
40|$|Existing {{opportunities}} in advanced interceptor, satellite guidance and aircraft navigation technologies, requiring higher signal processing speeds and lower noise environments, are demanding Ring Laser Gyro (RLG) based Inertial Systems to reduce initialization and operational data latency {{as well as}} correlated noise magnitudes. Existing signal processing algorithms are often less than optimal when considering these requirements. Advancements in micro-electronic processes have made Application Specific Integrated Circuits (ASIC) a fundamental building block for system implementation when considering higher-level signal processing algorithms. Research of real time adaptive signal processing algorithms embedded in ASICs for use in RLG based inertial systems will help to understand the trade-off in finite register length effects to correlated noise magnitude, organizational complexity, computational efficiency, rate of convergence, and numerical stability. Adaptive filter structures selected will directly affect meeting inertial system performance requirements for data <b>latency,</b> residual noise <b>budgets</b> and real time processing throughput. Research in this area will help to target specific adaptive noise cancellation algorithms for RLG based inertial systems {{in a variety of}} military and commercial space applications. Of particular significance is an attempt to identify an algorithm embedded in an ASIC that will reduce the correlated noise components to the theoretical limit of the RLG sensor itself. This would support a variety of applications for the low noise space environments that the RLG based inertial systems are beginning to find promise for such as advanced military interceptor technology and commercial space satellite navigation, guidance and control systems...|$|R
40|$|As CMOS {{technology}} improves, {{the trend}} of processor designs has gone towards multi-core architectures. Networks-on-Chips (NoCs) have become popular on-chip interconnect fabrics that connect the ever-increasing cores because {{of their ability to}} provide high-bandwidth. However, as the number of cores keeps increasing, the endto- end packet latency and the total network power begin to pose tight constraints on NoC designs. In this thesis, we studied architecture proposals designed to tackle this <b>latency</b> and power <b>budget</b> issue. We also studied the impact of applying advanced circuit techniques to these architecture proposals and how to implement these techniques while realizing a NoC design. The thesis begins with an evaluation of physical express topologies and the virtual express topologies that enable the bypassing of intermediate router pipelines. The bypassing of pipeline stages help reduce both end-to-end latency and power consumption since fewer resources are used. We observed that both topologies have similar low-traffic-load latencies and that virtual express topologies result in higher throughput and are more robust across traffic patterns. Physical express topologies, however, deliver a better throughput/watt and can leverage the low-swing link circuits to lower the latency and increase the throughput. Next, then we identified that crossbars, in addition to links, can obtain benefit from the low-swing circuit techniques. We thus developed a layout generation tool for low-swing crossbars and links due to the inability of the existing tools for physical designs to generate these low-swing circuits automatically. The generated crossbars and links using our tool showed 50 % energy saving compared to the full-swing synthesized counterpart. We also demonstrated a case study with a router synthesized with the generated crossbar and links. by Chia-Hsin Chen. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2012. Cataloged from PDF version of thesis. Includes bibliographical references (p. 57 - 60) ...|$|R
30|$|This paper {{presents}} {{the design and}} implementation of micROS-drt, a software infrastructure that supports real-time and scalable data distribution in distributed robotic systems. A loosely coupled data publish-subscribe model for robots is proposed firstly. In this model, two kinds of message topics are defined: general topics without real-time assurance and real-time topics which support the fine definition of transport priority, <b>latency</b> <b>budget,</b> time-based filter and other real-time parameters. To reify this model, a mature data distribution standard, Object Management Group (OMG)’s data distribution service for real-time systems (DDS) [6], is adopted {{as the foundation of}} the transport layer of micROS-drt. We adapt and encapsulate the capability of the underlying DDS middleware to meet the requirements in robotic data distribution we mentioned earlier. Evaluation results in terms of scalability, latency jitter and transport priority on a test bed, as well as the experiment on real robots, show that micROS-drt can disseminate data scalably with real-time constraints in distributed robotic systems.|$|E
40|$|LHC Run 2 {{presents}} challenging {{high rate}} conditions for data analysis and processing within the ATLAS trigger systems. The ATLAS Inner Detector (ID) trigger implements the algorithms used for identification of tracks {{in nearly all}} physics signatures within the ATLAS trigger. The ID trigger was updated and redesigned during the 2013 - 2015 long shutdown to meet the challenging conditions of Run 2. As well, for Run 2 a new pixel detector layer was added in very {{close proximity to the}} beam pipe, which enhances the ID Trigger performance. The redesigned ID trigger algorithms for Run 2 are described, illustrating the significant improvements gained by the new tracking strategies adopted to deal with the increased rate. Performance of the ID trigger in Run 2 is shown in terms of algorithm timing, efficiency and resolution, using data collected by ATLAS in Run 2. The ID trigger continues to show excellent performance, with efficiencies greater than 99 %, and track reconstruction times well within the required <b>latency</b> <b>budget...</b>|$|E
40|$|Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, locality-oblivious blob store. Using a novel {{combination}} of full bisection bandwidth networks, data and metadata striping, and flow control, FDS multiplexes an application’s large-scale I/O across the available throughput and <b>latency</b> <b>budget</b> of every disk in a cluster. FDS therefore makes many optimizations around data locality unnecessary. Disks also {{communicate with each}} other at their full bandwidth, making recovery from disk failures extremely fast. FDS is designed for datacenter scale, fully distributing metadata operations that might otherwise become a bottleneck. FDS applications achieve single-process read and write performance of more than 2 GB/s. We measure recovery of 92 GB data lost to disk failure in 6. 2 s and recovery from a total machine failure with 655 GB of data in 33. 7 s. Application performance is also high: we describe our FDS-based sort application which set the 2012 world record for disk-to-disk sorting. ...|$|E
40|$|The {{upgrades}} of the LHC accelerator and {{the experiments}} in 2019 / 20 and 2023 / 24 will allow to in-crease the luminosity to 2 × 1034 cm− 2 s− 1 and 5 - 7 × 1034 cm− 2 s− 1, respectively. For the HL-LHC phase, the expected {{mean number of}} interactions per bunch crossing will be 55 at 2 × 1034 cm− 2 s− 1 and ~ 140 at 5 × 1034 cm− 2 s− 1. This increase drastically impacts the ATLAS trigger and trigger rates. For the ATLAS Muon Spectrometer, a replacement of the innermost endcap stations, the so-called “Small Wheels” operating in a magnetic field, is therefore planned for 2019 / 20 {{to be able to}} maintain a low pT threshold for single muon and excellent tracking capability in the HL-LHC regime. The New Small Wheels will feature two new detector technologies: Resistive Micromegas and small strip Thin Gap Chambers comprising a system of ~ 2. 4 million readout channels. Both detector technologies will provide trigger and tracking primitives fully compliant with the post- 2024 HL-LHC operation. To al-low for some safety margin, the design studies assume a maximum instantaneous luminosity of 7 × 1034 cm− 2 s− 1, 200 pile-up events, trigger rates of 1 MHz at Level- 0 and 400 KHz at Level- 1. A radia-tion dose of ~ 1700 Gy (inner radius) is expected. The electronics design of such a system will be implemented in some 8000 on-detector boards including the design of four different custom ASICs. Among them the 64 -channel VMM, a common frontend mixed-signal ASIC for both detector tech-nologies and charge-interpolating trackers, provides amplitude and timing measurements, direct output of trigger primitives and Level- 0 trigger buffering. The candidate selection is required to be within a <b>budget</b> <b>latency</b> of 1 us, and 6 us after 2024. Moreover, the design integrates the GBTx (a radiation hard 5 Gigabit transceiver) and a Slow Control ASICs developed at CERN. The custom GBTx data flow links are aggregated onto an industry standard high speed network to which standard PCs perform data acquisition, configuration, and monitoring. The large number of readout channels, high speed output data rate, harsh radiation and magnetic environment, small available space, poor access and low power consumption all impose great challenges on the system design. The overall design and first results from integration of the electronics in a vertical slice test will be presented...|$|R
40|$|The {{operation}} {{experience with}} ATLAS ALFA detectors in the LHC environment during the Run 1 period has shown significant beam-induced heating. Subsequent comprehensive studies revealed that heating effects could be disastrous {{in the case}} of the larger beam intensities foreseen for higher luminosities in the LHC Run 2. During the first LHC long shutdown (LS 1) all ALFA detectors have been removed from the LHC tunnel and their covers - Roman Pots - underwent a geometry upgrade to minimize the impedance losses. It will be shown that this modification together with a system improving the internal heat transfer and an air cooling system, significantly shifted the temperatures of ALFA detectors away from the critical limits throughout the LHC Run 2. Also ALFA trigger system was considerably upgraded to keep measured data safely inside the Run 2 ATLAS <b>latency</b> <b>budget</b> and to minimize dead time. The needed hardware changes of the trigger system are also describe...|$|E
30|$|In the past, much {{research}} has been performed in Publish/Subscribe (Pub/Sub) [6 – 10], {{but only a few}} support large-scale mobile networks and simultaneously offer QoS (Quality of Service) guarantees for the mobile communications, especially the aforementioned reliability and low-latency message delivery [11 – 13]. On the other hand, the OMG’s Data Distribution Service for Real-time Systems (DDS) standard [14, 15] offers high-performance communication capabilities and is currently used for several real-world distributed mission-critical applications. DDS specifies a decentralised (Peer-to-Peer) scalable middleware architecture for asynchronous, Publish-Subscribe-like data distribution, supporting several QoS policies (e.g., best effort or reliable communication, data persistency, data flow prioritisation, and several other message delivery optimisations). Unlike traditional Publish-Subscribe middleware, DDS can explicitly control the latency and efficient use of network resources through fine-tuning of its network services, which are critical for soft real-time applications (e.g., its QoS policies deadline, <b>latency</b> <b>budget</b> or transport priority) [16]. Moreover, because Publish-Subscribe communication is widely acknowledged as being one of the most suitable paradigms for mobile systems, we were sure that DDS would be very appropriate for large-scale mobile applications.|$|E
40|$|International audienceApplication-Level Forward Erasure Correction (AL-FEC) codes {{have become}} a key {{component}} of communication systems in order to recover from packet losses. This work analyzes the benefits of the AL-FEC codes based on a sliding encoding window (A. K. A. convolutional codes) for the reliable broadcast of real-time flows to a potentially large number of receivers over a constant bit rate channel. It first details the initialization of both sliding window codes and traditional block codes {{in order to keep the}} maximum AL-FEC decoding latency below a target <b>latency</b> <b>budget.</b> Then it presents detailed performance analyzes using official 3 GPP mobility traces, representative of our use case which involves mobile receivers. This work highlights the major benefits of RLC codes, representative of sliding window codes, that outperform any block code, from Raptor codes (that are part of 3 GPP MBMS standard) up to ideal MDS codes, both in terms of reduced added latency and improved robustness. It also demonstrates that our RLC codec features decoding speeds that are an order of magnitude higher than that of Raptor codes...|$|E
40|$|The LHCb {{software}} trigger has two levels: a high-speed trigger running at 1 MHz with strictly limited latency {{and a second}} level running below 40 kHz without latency limitations. The trigger strategy requires full flexibility {{in the distribution of}} the installed CPU power to the two {{software trigger}} levels because of the unknown background levels and event topology distribution at the time the LHC accelerator will start its operation. This requirement suggests using a common CPU farm for both trigger levels fed by a common data acquisition (DAQ) infrastructure. The limited <b>latency</b> <b>budget</b> of the first level of software trigger has an impact on the organization of the CPU farm performing the trigger function for optimal usage of the installed CPU power. We will present the architecture and the design of the hardware infrastructure for the entire LHCb software triggering system based on Ethernet as link technology that fulfills these requirements. The performance of the event-building of the combined traffic of both software trigger levels, as well as the expected scale of the system will be presented. (9 refs) ...|$|E
40|$|The ATLAS Level- 1 Calorimeter Trigger is a {{hardware-based}} {{system designed}} to identify high-pT jets, elec- tron/photon and tau candidates, and to measure total and missing ET in the ATLAS Liquid Argon and Tile calorimeters. It is a pipelined processor system, with {{a new set of}} inputs being evaluated every 25 ns. The overall trigger decision has a <b>latency</b> <b>budget</b> of 2 µs, including all transmission delays. The calorimeter trigger uses about 7200 reduced granularity analogue signals, which are ﬁrst digitized at the 40 MHz LHC bunch-crossing frequency, before being passed to a digital Finite Impulse Re- sponse (FIR) ﬁlter. Due to latency and chip real-estate constraints, only a simple 5 -element ﬁlter with limited precision can be used. Nevertheless, this ﬁlter achieves a signiﬁcant reduction in noise, along with improving the bunch-crossing assignment and energy resolution for small signals. The context in which digital ﬁlters are used for the ATLAS Level- 1 Calorimeter Trigger is presented, before describing the methods used to determine the best ﬁlter coefﬁcients for each detector element. The performance of these ﬁlters is investigated with commissioning data and cross-checks of the calibration with initial beam data from ATLAS are shown...|$|E
40|$|The Compact Muon Solenoid (CMS) {{experiment}} at the Large Hadron Collider (LHC) studies proton-proton collisions at a centre-of-mass {{energy of}} 13 TeV. With the LHC colliding proton bunches every 25 nanoseconds, {{the volume and}} rate of raw data produced by the detector are much larger than what can be read out, recorded, and reconstructed. Therefore, an efficient trigger system is required to identify events of interest in real time and to reduce the rate of events to a manageable level for later software reconstruction. The CMS trigger system consists of two processing stages, a level- 1 (L 1) hardware trigger and a high level software trigger. The current L 1 trigger decision relies solely on calorimetric and muon system information. During the High Luminosity LHC (HL-LHC) era, the instantaneous luminosity of the collider {{is expected to increase}} by approximately an order of magnitude, resulting in a significantly larger number of collisions per bunch crossing than observed in the current run. In order to preserve physics performance under such highly challenging conditions, the L 1 trigger system must be upgraded to accommodate the use of silicon tracker data. A new CMS L 1 Tracking Trigger will reconstruct tracks with transverse momentum above 2 GeV/c at the LHC bunch crossing rate and within a tight <b>latency</b> <b>budget</b> of approximately 4 μs. In this article we provide an overview of the three architectures currently studied by the CMS collaboration for the future L 1 Track Trigger system...|$|E
40|$|The {{search for}} {{transient}} phenomena at low radio frequencies is now {{coming of age}} {{with the development of}} radio sky monitors with a large field of view, which are made feasible by new developments in calibration algorithms and computing. However, accurate calibration of such arrays is challenging, especially within the latency requirements of near real-time transient monitors, and is the main cause of limiting their sensitivities. This paper describes a strategy for real-time, wide-field direction-dependent calibration of the Amsterdam-ASTRON Radio Transients Facility and Analysis Center (AARTFAAC) array, which is a sensitive, continuously available all-sky monitor based on the LOw Frequency ARray (LOFAR). The monitor operates in a zenith pointing, snapshot imaging mode for image plane detection of bright radio transients. We show that a tracking calibration approach with solution propagation satisfies our latency, computing, and calibration accuracy constraints. We characterize the instrument and verify the calibration strategy under a variety of observing conditions. This brings out several phenomena, which can bias the calibration. The real-time nature of the application further imposes strict latency and computational constraints. We find that although ionosphere-induced phase errors present a major impediment to accurate calibration, these can be corrected {{in the direction of the}} brightest few sources to significantly improve image quality. Our real-time calibration pipeline implementation processes a single spectral channel of a snapshot observation in ~ 0. 2 s on test hardware, which is well within its <b>latency</b> <b>budget.</b> Autonomously calibrating and imaging one second snapshots, our approach leads to a typical image noise of ~ 10 Jy for a ~ 90 kHz channel, reaching dynamic ranges of ~ 2000 : 1. We also show that difference imaging allows thermal-noise limited transient detection, despite the instrument being confusion-noise limited...|$|E
40|$|The {{primary purpose}} of the BaBar {{experiment}} is the systematic study of CP asymmetries in the decays of neutral B mesons. BaBar's calorimeter provides high efficiency neutral pion detection and is the main subdetector for the separation of pions and electrons. It comprises 6660 CsI(Tl) crystals covering the solid angle - 141 deg. #<=# #theta#_l_a_b #<=# 16 deg. The crystals are assembled into a barrel and a forward endcap and each crystal is aligned to point approximately towards the interaction point. The Level 1 trigger executes in hardware, providing an output rate of up to 2 kHz to the Level 3 trigger which executes in software. Continuous energy deposition data from the calorimeter are processed by BaBar's Level 1 Electromagnetic Calorimeter Trigger (EMT) Energy Selection Processors (ESP). The design, testing and evaluation of the ESP firmware is the main subject of this Thesis. The processors receive digitised calorimeter #phi#-tower energy data, each tower being nominally three crystals in #phi# by eight crystals in #theta#, at a 59. 5 MHz bit rate. The digital signals are processed in parallel on forty Xilinx Field Programmable Gate Array (FPGA) chips. They deliver selected calorimeter map objects to the Global Trigger, at a 7. 4 MHz bit rate, within a <b>latency</b> <b>budget</b> of 5 #mu#s. The processor logic determines the event time by locating the peak of the energy-deposit pulse with a Finite Impulse Response filter, utilising a Distributed Arithmetic technique. Furthermore, following the receipt of Level 1 Accept and Read Event signals from BaBar's Fast Control, key data are output to DAQ. A spying network has been incorporated to enable external verification and validation of the logic. The design has been tested in simulation and on test-stands, using simple, complex and realistic test patterns. The trigger firmware is now working successfully on the experiment, where it is taking and processing real data. Analysis of colliding-beam data has shown that for above-threshold energies, within statistics, the trigger is fully efficient. (author) SIGLEAvailable from British Library Document Supply Centre-DSC:DXN 036203 / BLDSC - British Library Document Supply CentreGBUnited Kingdo...|$|E
40|$|The 'Phase-I' upgrade of the Large Hadron Collider (LHC), {{scheduled}} {{to be completed in}} 2021, will lead to an enhanced collision luminosity of 2. 5 × 10 ^ 34 cm^- 2 s^- 1. To cope with the new and challenging accelerator conditions, all the CERN experiments have planned a major detector upgrade to be installed during the associated experimental shutdown period. One of the physics goals of the ATLAS experiment is to maintain sensitivity to electroweak processes despite the increased number of interactions per LHC bunch crossing. To this end, the component of the first level hardware trigger based on calorimeter data will be upgraded to exploit fine-granularity readout using a new system of Feature EXtractors (FEXs), which each uses different physics objects for trigger selection. There will be three FEX systems in total, with this contribution focusing on the first prototype of the jet FEX (jFEX). This system identifies jets and large area tau candidates while also calculating global variables such as transverse energy sums and missing transverse energy. The jFEX prototype is characterised by four large Xilinx Ultrascale Field Programmable Gate Arrays (FPGAs), XCVU 190 FLGA 2577, so far the largest available on the market, capable of handling a data volume of more than 3 TB/s of input bandwidth. The choice of such large devices was driven by the requirement for large input bandwidth and processing power. This comes from the need to exploit high granularity calorimeter information and also run several jet identification algorithms within the few hundred nanoseconds <b>latency</b> <b>budget</b> (≈ 350 ns). This presentation will report on the hardware design challenges and adopted solutions to preserve signal integrity within a densely populated high signal speed ATCA board. The parallel simulation activity that supported and validated the board design will also be presented. Particular emphasis will be given to the large FPGA power consumption effects on the boards. This was assessed via dedicated thermal simulation and cross-checked with a campaign of measurements. Preliminary results will also be presented from tests both at CERN and Mainz, based on the first jFEX prototype from December 2016...|$|E
40|$|ASML {{is a world}} leading {{supplier}} of complex lithography machines for the semiconductor industry. A lithography machine consists of many subsystems, e. g., Light Source, Lens, Reticle handler, Reticle stage, Wafer handler and Wafer stage, which synchronize together to make the machine work. The Reticle stage holds the circuit pattern, also known as reticle and the Wafer stage module holds the wafer. The UV light from the light source is projected on the circuit pattern, which is then passed through the lens to imprint the pattern on the wafer. Since the circuit pattern has to be imprinted on the wafer, {{the movement of the}} modules; Reticle stage and Wafer stage should be synchronized in six degrees of freedom (DoF) with nanometer accuracy. To employ the movement of the subsystems, motion controllers are used in ASML, and Long Stroke and Short Stroke controllers are responsible for the movement of a part of the Wafer stage subsystem. It has been envisioned that future lithography machines, because of its high precision mechatronic requirements, will need motion control algorithms, that run at higher sampling frequencies with a severely reduced IO <b>latency</b> <b>budget.</b> Current hardware architectures {{will not be able to}} meet the demands of these future motion control algorithms. In this thesis, we propose an architecture, that uses a multi-ASIP in FPGA as an accelerator in conjunction with a CPU, which acts as a master to run the motion control applications. The proposal of using multi-ASIP FPGA in conjunction with CPU is based on the analysis carried out previously in ASML. It was observed that a sampling frequency exceeding 100 KHz can be obtained after deploying the Long Stroke controller and Short Stroke controller on a multi-ASIP platform in FPGA. However, this work considered only the data flow and not the supervisory control. After carrying out detailed analysis, we could predict that a sampling frequency of 40 KHz could be achieved by offloading the compute intensive blocks present in the Long Stroke and Short Stroke controller from the CPU to FPGA. The sampling frequency of 40 KHz can be achieved by considering, both the data flow and supervisory control, and the communication between the CPU and FPGA. Finally, after offloading the compute intensive blocks from the CPU on the multi-ASIP FPGA, and after implementing the data flow and supervisory control and communication mechanism between the CPU and FPGA, we can justify that the sampling frequency of 40 KHz can be achieved. Computer EngineeringElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|ENUM is a {{protocol}} standard {{developed by the}} Internet Engineering Task Force (IETF) for translating the E. 164 phone numbers into Internet Universal Resource Identifiers (URIs). It plays an increasingly important role as the bridge between Internet and traditional telecommunications services. ENUM {{is based on the}} Domain Name System (DNS), but places unique performance requirements on DNS server. In particular, ENUM server needs to host a huge number of records, provide high query throughput for both existing and non-existing records in the server, maintain high query performance under update load, and answer queries within a tight <b>latency</b> <b>budget.</b> In this report, we evaluate and compare performance of serving ENUM queries by three servers, namely BIND, PDNS and Navitas. Our objective is to answer whether and how these servers can meet the unique performance requirements of ENUM. Test results show that the ENUM query response time on our platform has always been on the order of a few milliseconds or less, so this is likely not a concern. Throughput then becomes the key. The throughput of BIND degrades linearly as the record set size grows, so BIND is not suitable for ENUM. PDNS delivers higher performance than BIND in most cases, while the commercial Navitas server presents even better ENUM performance than PDNS. Under our 5 M-record set test, Navitas server with its default configuration consumes one tenth to one sixth the memory of PDNS, achieves six times higher throughput for existing records and an order of two magnitudes higher throughput for non-existing records than the bottom line PDNS server without caching. The throughput of Navitas is also the highest among the tested servers when the database is being updated in the background. We investigated ways to improve PDNS performance. For example, doubling CPU processing power by putting PDNS and its backend database in two separate machines can increase PDNS throughput for existing records by 45 % and that for nonexisting records by 40 %. Since PDNS is open source, we also instrumented the source code to obtain a detailed profile of contributions of various systems components to the overall latency. We found that when the server is within its normal load range, the main component of server processing latency is caused by backend database lookup operations. Excessive number of backend database lookups is the reason that makes PDNS throughput for non-existing records its key weakness. We studied using PDNS caching {{to reduce the number of}} database lookups. With a full packet cache and a modified cache maintenance mechanism, the PDNS throughput for existing records can be improved by 100 %. This brings the value to one third of its Navitas counterpart. After enabling the PDNS negative query cache, we improved PDNS throughput for non-existing records to the level comparable to its throughput for existing records, but this result is still an order of magnitude lower than the corresponding value in Navitas. Further improvements of PDNS throughput for non-existing records will require optimization of related processing mechanism in its implementation...|$|E

