3|11|Public
40|$|Multiword Expressions (MWEs), a known {{nuisance}} {{for both}} linguistics and NLP, blur {{the lines between}} syntax and semantics. The semantic of a MWE cannot be expressed after combining the semantic of its constituents. In this study, we propose a novel approach called “semantic clustering ” as an instrument for extracting the MWEs especially for resource constraint languages like Bengali. At the beginning, it tries to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potential candidate using a vector space model. Finally the judgment for the suitability of this phrase to be a MWE is carried out based on a predefined threshold. In this experiment, we apply the semantic clustering approach only for noun-noun bigram MWEs; however {{we believe that it}} can be extended to any types of MWEs. We compare our approach with the state-of-the-art statistical approach. The evaluation results show that the semantic clustering outperforms all other competing methods. As a byproduct of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali <b>linguistic</b> <b>thesaurus.</b> Povzetek: V prispevku je predstavljena metoda za semantično gručenje večbesednih izrazov. ...|$|E
40|$|Abstract: One of the {{key issues}} in both natural {{language}} understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the 372 Chakraborty et al. semantic clustering approach outperforms all other competing statistical models. As a byproduct of this experiment, we have started developing a standard lexicon in Bengali {{that serves as a}} productive Bengali <b>linguistic</b> <b>thesaurus...</b>|$|E
40|$|One of the {{key issues}} in both natural {{language}} understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models. As a by-product of this experiment, we have started developing a standard lexicon in Bengali {{that serves as a}} productive Bengali <b>linguistic</b> <b>thesaurus.</b> Comment: 25 pages, 3 figures, 5 tables, International Journal of Linguistics and Language Resources (Lingvisticæ Investigationes), 201...|$|E
40|$|We {{propose a}} new method that enhances {{automatic}} keyphrase extraction by using semantic information on terms and phrases gleaned from a domain-specific thesaurus. We evaluate the results against keyphrase sets assigned by a state-of-the-art keyphrase extraction system and those assigned by six professional indexers. Categories and Subject Descriptors H. 3. 1 [Content Analysis and Indexing]: Indexing methods, <b>linguistic</b> processing, <b>thesauruses...</b>|$|R
40|$|In this papewe propose an {{analysis}} and an upgrade WordNet's top-lezH snse 7 taxonom. We brieq rekOPB WordNej and ideHkq its main sen HkFO limitations. Some principles from a forthcoming OlndNTl, meqjjH) "P are applieH to the ontological anal sis of WordNeqF A re 7 jj 7 H top-le) "" taxonom is proposeH) which is meHkP to bemore conceq uall rigorous, cognitive transparecognitiveF 7 PHe eansparecognitiveF 7 PHe" 7 HO"BHe C at egor es & De sc r ptors H 3. 1 n o m a t i o n S t o r a g e a n d R e t ie v] C o n te n t A n s and Ind e xing [...] -Indexing methods, <b>Linguistic</b> processing, <b>Thesauruses...</b>|$|R
40|$|When {{querying}} a news video archive, {{the users}} {{are interested in}} retrieving precise answers {{in the form of}} a summary that best answers the query. However, current video retrieval systems, including the search engines on the web, are designed to retrieve documents instead of precise answers. This research explores the use of question answering (QA) techniques to support personalized news video retrieval. Users interact with our system, VideoQA, using short natural language questions with implicit constraints on contents, context, duration, and genre of expected videos. VideoQA returns short precise news video summaries as answers. The main contributions of this research are: (a) the extension of QA technology to support QA in news video; and (b) the use of multi-modal features, including visual, audio, textual, and external resources, to help correct speech recognition errors and to perform precise question answering. The system has been tested on 7 days of news video and has been found to be effective. Categories and Subject Descriptor H. 3. 1 [Content Analysis and Indexing]: <b>linguistic</b> processing, <b>thesaurus...</b>|$|R
40|$|AbstractWe {{study the}} lobby index (l-index for short) {{as a local}} node {{centrality}} measure for complex networks. The l-index is compared with degree (a local measure), betweenness and Eigenvector centralities (two global measures) {{in the case of}} a biological network (Yeast interaction protein–protein network) and a <b>linguistic</b> network (Moby <b>Thesaurus</b> II). In both networks, the l-index has a poor correlation with betweenness but correlates with degree and Eigenvector centralities. Although being local, the l-index carries more information about its neighbors than degree centrality. Also, it requires much less time to compute when compared with Eigenvector centrality. Results show that the l-index produces better results than degree and Eigenvector centrality for ranking purposes...|$|R
40|$|We {{study the}} h Hirsch index {{as a local}} node {{centrality}} measure for complex networks in general. The h index is compared with the Degree centrality (a local measure), the Betweenness and Eigenvector centralities (two non-local measures) {{in the case of}} a biological network (Yeast interaction protein-protein network) and a <b>linguistic</b> network (Moby <b>Thesaurus</b> II) as test environments. In both networks, the Hirsch index has poor correlation with Betweenness centrality but correlates well with Eigenvector centrality, specially for the more important nodes that are relevant for ranking purposes, say in Search Machine Optimization. In the thesaurus network, the h index seems even to outperform the Eigenvector centrality measure as evaluated by simple linguistic criteria. Comment: 8 pages, 4 figures, typos and references corrected, table I correcte...|$|R
40|$|Tyt. z nagłówka. Bibliogr. s. 162 - 163. The paper {{presents}} {{a proposal to}} take into account the language comprehension paradigm in an attempt to improve the design methodology of human-computer dialogues. It is assumed that the user treats a system as a subject, i. e. perceives communication with the system as a kind of discourse and intuitively treats it as cognition of the world through a fixed image of objective linguistic signs. Due to the fact that every end-user of a computer system has an individual <b>linguistic</b> competence, <b>thesaurus,</b> habits, previous experiences, one can never guarantee that identical semantic references arise in a variety of audiences. The situation becomes more stable if one considers the communication aspect of comprehension on the basis of practice (familiarity of things). It may be expected that a qualified professional user will commit fewer errors, caused by a poor dialogue understanding while interacting with the system. Therefore, the main task of a designer is to speed up and facilitate the learning process, which can be achieved by running the process of constructive rational understanding. Dostępny również w formie drukowanej. KEYWORDS: information system, human-computer interaction, phenomenon of understanding, language comprehension paradigm, quality of a dialogue...|$|R
40|$|We {{study the}} lobby index (l-index for short) {{as a local}} node {{centrality}} measure for complex networks. The l-inde is compared with degree (a local measure), betweenness and Eigenvector centralities (two global measures) {{in the case of}} biological network (Yeast interaction protein-protein network) and a <b>linguistic</b> network (Moby <b>Thesaurus</b> II). In both networks, the l-index has poor correlation with betweenness but correlates with degree and Eigenvector. Being a local measure, one can take advantage by using the l-index because it carries more information about its neighbors when compared with degree centrality, indeed it requires less time to compute when compared with Eigenvector centrality. Results suggests that l-index produces better results than degree and Eigenvector measures for ranking purposes, becoming suitable as a tool to perform this task. Comment: 11 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv: 1005. 480...|$|R
40|$|This report {{presents}} {{a new approach}} to index a web site using ontologies and natural language techniques for Internet information retrieval. Ontologies are used to index a web site and, as a result to represent the web pages content. First, a <b>linguistic</b> ontology (a <b>thesaurus)</b> is used to disambiguate the label of ontology concepts. This disambiguation process uses several hierarchical heuristics that take advantage of the "isa" relationship on both the ontology and the thesaurus. Natural language techniques based on extraction of well-formed terms are also presented. They used a web page particularity: HTML markers. Then, from the previous extracted terms, associated concepts are generated using a linguistic ontology and a semantic similarity measure. At each candidate concept is associated a couple of coefficient: the convenience coefficient and the weighted frequency coefficient. The match between web page concepts and ontology concepts is then presented. Moreover, results about different web sites on different domains are presented. They point out the good accuracy of our ontology indexation process...|$|R
40|$|The first {{international}} study (N= 684) we conducted within our research project on online dictionary use included very general questions on that topic. In this chapter, we present the corresponding results on questions like {{the use of}} both printed and online dictionaries {{as well as on}} the types of dictionaries used, devices used to access online dictionaries and some information regarding the willingness to pay for premium content. The data collected by us, show that our respondents both use printed and online dictionaries and, according to their self-report, many different kinds of dictionaries. In this context, our results revealed some clear cultural differences: in German-speaking areas spelling dictionaries are more common than in other <b>linguistic</b> areas, where <b>thesauruses</b> are widespread. Only a minority of our respondents is willing to pay for premium content, but most of the respondents are prepared to accept advertising. Our results also demonstrate that our respondents mainly tend to use dictionaries on big-screen devices, e. g. desktop computers or laptops...|$|R
40|$|Authorship Verification is an {{important}} sub discipline of digital text forensics. Its goal is to decide, if two texts are written by the same author or not. We present an efficient Authorship Verification scheme based on an ensemble of K-Nearest Neighbor classifiers, where each classifier generates a decision regarding a feature category. Our scheme provides many benefits such as, for instance, the independence of <b>linguistic</b> resources like <b>thesauruses</b> or language models. Furthermore, it can handle different Indo-European languages as for instance English, German, Spanish, Greek, Dutch, Swedish or French. Another benefit is the low runtime, {{due to the fact}} that deep linguistic processing (tagging, chunking, parsing, etc.) is not taken into account. Moreover, our scheme can easily be modified for example by replacing the involved distance function, the acceptance criterion or the used features including their parameters. The proposed scheme is evaluated against the publicly available PAN- 2013 Author Identification (AI) test corpus, where it was ranked as the second-best in the top ten list, as well as against five other test corpora, compiled by our own. We show in our experiments that it is possible to achieve promising results, even when using a fixed setting of parameters and features across seven different languages...|$|R
40|$|Ontology {{has been}} {{developed}} to offer a commonly agreed understanding of a domain that is required for knowledge representation, knowledge exchange and reuse across domains. Therefore, ontology organizes information into taxonomies of terms (i. e., concepts, attributes) and shows the relationships between them. In fact, it {{is considered to be}} helpful in reducing conceptual confusion for users who need to share applications of different kinds, so it is widely used to capture and organize knowledge in a given domain. Although ontologies are considered to provide a solution to data heterogeneity, from another point of view, the available ontologies could themselves introduce heterogeneity problems. In order to deal with these problems, ontologies must be available for sharing or reusing; therefore, semantic heterogeneity and structural differences need to be resolved among ontologies. This can be done, in some cases, by aligning or matching heterogeneous ontologies. Thus, establishing the relationships between terms in the different ontologies is needed throughout ontology alignment. Semantic interoperability can be established in ontology reconciliation. The original problem is called the ―ontology alignment‖. The alignment of ontologies is concerned with the identification of the semantic relationships (subsumption, equivalence, etc.) that hold between the constituent entities (which can be classes, properties, etc.) of two ontologies. In this thesis, an ontology alignment technique {{has been developed}} in order to facilitate communication and build a bridge between ontologies. An efficient mechanism has been developed in order to align entities from ontologies in different description languages (e. g. OWL, RDF) or in the same language. This approach tries to use all the features of ontologies (concept, attributes, relations, structure, etc.) in order to obtain efficiency and high quality results. For this purpose, several matching techniques have been used such as string, structure, heuristic and <b>linguistic</b> matchingtechniques with <b>thesaurus</b> support, as well as human intervention in certain cases, to obtain high quality results. The main aim of the work is to introduce a method for finding semantic correspondences among heterogeneous ontologies, with the intention of supporting interoperability over given domains. The approach brings together techniques in modelling, string matching, computation linguistics, structure matching and heuristic matching, in order to provide a semi-automatic alignment framework and prototype alignment system to support the procedure of ontology alignment in order to improve semantic interoperability in heterogeneous systems. This technique integrates some important features in matching in order to achieve high quality results, which will help when searching and exchanging information between ontologies. Moreover, an ontology alignment system illustrates the solving of the key issues related to heterogeneous ontologies, which uses combination-matching strategies to execute the ontology-matching task. Therefore, {{it can be used to}} discover the matching between ontologies. This thesis also describes a prototype implementation of this approach in many real-world case studies extracted from various Web resources. Evaluating our system is done throughout the experiments provided by the Ontology Alignment Evaluation Initiative. The system successfully achieved 93 % accuracy for ontology matching. Finally, a comparison between our system and well-known tools is achieved so that our system can be evaluated...|$|R

