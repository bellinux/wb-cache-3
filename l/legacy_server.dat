10|35|Public
50|$|Itanium {{processors}} released {{prior to}} 2006 had hardware {{support for the}} IA-32 architecture to permit support for <b>legacy</b> <b>server</b> applications, but performance for IA-32 code was much worse than for native code and also worse than the performance of contemporaneous x86 processors. In 2005, Intel developed the IA-32 Execution Layer (IA-32 EL), a software emulator that provides better performance. With Montecito, Intel therefore eliminated hardware support for IA-32 code.|$|E
5000|$|Before {{migrating}} to the Chromium code base, Opera {{was the most}} widely used web browser that did not have the User-Agent string with [...] "Mozilla" [...] (instead beginning it with [...] "Opera"). Since July 15, 2013, Opera's User-Agent string begins with [...] "Mozilla/5.0" [...] and, to avoid encountering <b>legacy</b> <b>server</b> rules, no longer includes the word [...] "Opera" [...] (instead using the string [...] "OPR" [...] to denote the Opera version).|$|E
40|$|Abstract. In this paper, {{we propose}} a hybrid {{peer-to-peer}} streaming architecture for large scale media streaming service. The proposed ar-chitecture combines peer-to-peer system and centralized server to ex-ploit {{advantages of the}} two models. Then, we deal with streaming load allocation problem under the proposed scheme. Given a set of central-ized servers {{and a set of}} unreliable supplying peers with heterogeneous bandwidth offers, we show how to assign streaming load to a subset of supplying peers, while considering unreliable performance of each peers. Our experimental results show that the proposed scheme, compared with <b>legacy</b> <b>server</b> with similar capacity, increases the number of clients about 67 % on the average. ...|$|E
50|$|One of {{the biggest}} discussions on <b>legacy</b> <b>servers</b> {{came out of the}} entire Nostalrius issue. A {{petition}} on change.org has received over 200,000 signatures pleading Blizzard to hear the voices. Many famous Youtubers have made videos on this very topic discussing {{the pros and cons of}} <b>Legacy</b> <b>servers,</b> and it managed to get the attention of one of the original designers of World of Warcraft, Mark Kern, who supports the idea of <b>Legacy</b> <b>servers.</b> Blizzard Entertainment was silent on the issue until April 25, where the lead game developer responded to the discussion.|$|R
40|$|Researchers {{have argued}} {{that the best way to}} {{construct}} a secure system is to proactively integrate security into the design of the system. However, this tenet is rarely followed because of economic and practical considerations. Instead, security mechanisms are added as the need arises, by retrofitting legacy code. Existing techniques to do so are manual and ad hoc, and often result in security holes. We present program analysis techniques to assist the process of retrofitting legacy code for authorization policy enforcement. These techniques can be used to retrofit <b>legacy</b> <b>servers,</b> such as X window, web, proxy, and cache servers. Because such servers manage multiple clients simultaneously, and offer shared resources to clients, they must have the ability to enforce authorization policies. A developer can use our techniques to identify security-sensitive locations in <b>legacy</b> <b>servers,</b> and place reference monitor calls to mediate these locations. We demonstrate our techniques by retrofitting the X 11 server to enforce authorization policies on its X clients. 1...|$|R
50|$|KioskNet is a system, {{developed}} at the University of Waterloo, to provide very low cost Internet access to rural villages in developing countries, based {{on the concept of}} delay-tolerant networking. It uses vehicles, such as buses, to ferry data between village kiosks and Internet gateways in nearby urban centers. The data is re-assembled at a Proxy Server for interaction with <b>legacy</b> <b>servers.</b> The system is free and open source.|$|R
40|$|Presents a flexible, CORBA-compliant middle-tier server {{architecture}} {{which is}} capable of adding dependability (i. e. reliability, availability and performability) to an existing service. The architecture provides a flexible and cost-effective framework for building fault-tolerant applications via straightforward integration of legacy software. In a typical scenario, the service would be originally provided by some <b>legacy</b> <b>server,</b> which is integrated in the new system with no changes being made to it. This has two main advantages. First, the development effort is minimized, since full reuse of existing software is achieved. Second, backward compatibility is preserved since {{it is possible to}} integrate new clients with existing applications and databases, protecting the investment in legacy systems. The architecture we propose is novel, although it builds upon redundancy techniques which have already been implemented in a massive number of designs for traditional fault-tolerant application...|$|E
40|$|This work {{presents}} a flexible, CORBA compliant Middle-Tier Server architecture which {{is capable of}} adding dependability (namely, reliability, availability, and performability) to an existing service. The architecture provides a flexible and cost-effective framework for building fault-tolerant applications via straightforward integration of legacy software. In the typical scenario, the service would be originally provided by some <b>legacy</b> <b>server,</b> which is integrated in the new system with no changes being made to it. This has two main advantages. First, the development effort is minimized, since full reuse of existing software is achieved. Second, backward compatibility is preserved, since {{it is possible to}} integrate new clients with existing applications and databases, protecting the investment in legacy systems. To the best of our knowledge, the architecture we propose is novel, although it builds upon redundancy techniques which have already been implemented in a massive number of desi [...] ...|$|E
40|$|This work {{presents}} a CORBA-based multi-tier architecture which {{is capable of}} adding security to an existing service. We assume the legacy application is available as a compiled program, consisting of a client and a server module. Under these assumptions, we show {{how to build a}} new system, which reintegrates the original service, and secures it. The architecture we propose is quite flexible and re{{presents a}} framework which can be adopted -with minor changes- for improving the security level of a wide class of legacy systems. A system prototype has been developed and its performance evaluated. The prototype uses digital certificates which can be provided by virtually any Certification Authority. A fundamental advantage of the proposed approach is that the <b>legacy</b> <b>server</b> is integrated in the secure system with no changes being made to it. This min- imizes the development effort, since full reuse of existing software is achieved. Furthermore, backward compatibility is preserved, since it is possible to integrate the new clients with the preexisting applications, protecting the investment in legacy systems...|$|E
40|$|AbstractResearchers {{have long}} {{argued that the}} best way to {{construct}} a secure system is to proactively integrate security into the design of the system. However, this tenet is rarely followed because of economic and practical consid-erations. Instead, security mechanisms are added as the need arises, by retrofitting legacy code. Unfortunately, existing techniques to do so are manual and adhoc, and often result in security holes in the retrofitted code. We show that program analysis techniques can be used to securely, and largely automatically, retrofit legacy code for authorization policy enforcement. Our techniques are applicable to a large class of <b>legacy</b> <b>servers,</b> namelythose that simultaneously manage multiple clients, possibly with di fferent security labels. It is important for suchservers to ensure that client interaction is governed by an authorization policy. We demonstrate our ideas using two program analysis tools we built, Aid and Alpen, which work togetherto automate the process of retrofitting <b>legacy</b> <b>servers</b> with mechanisms for authorization policy enforcement. We show that an X server retrofitted using these tools securely enforces authorization policies on its X clients...|$|R
40|$|Researchers {{have long}} {{argued that the}} best way to {{construct}} a secure system is to proactively integrate security into the design of the system. However, this tenet is rarely followed because of economic and practical considerations. Instead, security mechanisms are added as the need arises, by retrofitting legacy code. Unfortunately, existing techniques to do so are manual and adhoc, and often result in security holes in the retrofitted code. We show that program analysis techniques can be used to securely, and largely automatically, retrofit legacy code for authorization policy enforcement. Our techniques are applicable to a large class of <b>legacy</b> <b>servers,</b> namely those that simultaneously manage multiple clients, possibly with different security labels. It is important for such servers to ensure that client interaction is governed by an authorization policy. We demonstrate our ideas using two program analysis tools we built, Aid and Alpen, which work together to automate the process of retrofitting <b>legacy</b> <b>servers</b> with mechanisms for authorization policy enforcement. We show that an X server retrofitted using these tools securely enforces authorization policies on its X clients. NOTE: This report is superseded by our paper that appears i...|$|R
50|$|A <b>legacy</b> Archie <b>server</b> {{is still}} {{maintained}} active for historic purposes in Poland at University of Warsaw's Interdisciplinary Centre for Mathematical and Computational Modelling.|$|R
40|$|Users of {{a network}} system often require access to legacy resources. Providing this access is a {{difficult}} task for system administrators because the access protocols for those resources are typically insecure. A common approach {{is to develop a}} custom wrapper or proxy that securely processes user requests before forwarding them to the <b>legacy</b> <b>server.</b> The problem with this approach is that administrators must develop a custom solution for every resource. We believe that there are common requirements for managing these resources that can be addressed from a more centralized model. The userspace queuing extensions of the Netfilter firewall modules provide a generic environment in which protocol-aware deep packet filters can be constructed to enhance the security of resource access protocols. We employ this environment to strengthen two commonly used legacy protocols, and compare their requirements. We show {{that it is possible to}} secure legacy resources with minimal degradation in performance. We also discuss considerations for development of a deep packet filter toolkit to aid system administrators in securely managing legacy network resources...|$|E
40|$|Server {{applications}} augmented with behavioural adaptation logic {{can react}} to environmental changes, creating self-managing server applications with improved {{quality of service}} at runtime. However, developing adaptive server applications is challenging due {{to the complexity of}} the underlying server technologies and highly dynamic application environments. This paper presents an architecture framework, the Adaptive Server Framework (ASF) to facilitate the development of adaptive behaviour for <b>legacy</b> <b>server</b> applications. ASF provides a clear separation between the implementation of adaptive behavior and the business logic of the server application. This means a server application can be extended with programmable adaptive features through the definition and implementation of control components defined in ASF. Furthermore, ASF is a lightweight architecture in that it incurs low CPU overhead and memory usage. We demonstrate the effectiveness of ASF through a case study, in which a server application dynamically determines the resolution and quality to scale an image based on the load of the server and network connection speed. The experimental evaluation demonstrates the performance gains possible by adaptive behavior and the low overhead introduced by ASF...|$|E
40|$|Most {{research}} on QoS-aware computing considers systems where code is generally partitioned into separately schedulable tasks with associated timing constraints. In {{sharp contrast to}} such systems is a myriad of mainstream off-the-shelf applications and services such as Web servers, caches, mail servers, and content distribution proxies where QoS guarantees may be needed, yet the software follows a best-effort one-size-serves-all model. In this model, different traffic classes are not mapped to different schedulable entities (tasks), {{making it impossible to}} use real-time scheduling meaningfully to satisfy application QoS. This paper presents a kernel-level {{solution to the problem of}} retrofitting such best-effort systems with QoS support without changing application code. The solution has been implemented in Linux. By downloading a few kernel patches and configuring the patched kernel appropriately, a system administrator can endow a best-effort service with QoS assurances transparently to the <b>legacy</b> <b>server.</b> An extensible library is provided in a separate QoS manager that allows implementing different types of QoS guarantees within the extended service. The performance of the resulting system is evaluated on the implemented Linux-based prototype. It is shown that QoS-sensitive behavior is successfully achieved. ...|$|E
40|$|This paper {{presents}} {{an approach to}} statically retrofit <b>legacy</b> <b>servers</b> with mechanisms for authorization policy enforcement. The approach {{is based upon the}} observation that security-sensitive operations performed by a server are characterized by idiomatic resource manipulations, called fingerprints. Candidate fingerprints are automatically mined by clustering resource manipulations using concept analysis. These fingerprints are then used to identify security-sensitive operations performed by the server. Case studies with three real-world servers show that the approach can be used to identify security-sensitive operations with a few hours of manual effort and modest domain knowledge. ...|$|R
40|$|We {{present an}} {{approach}} based on concept analysis to retrofit <b>legacy</b> <b>servers</b> with mechanisms for authorization policy enforcement. Our approach {{is based upon}} the obser-vation that security-sensitive operations are characterized by idiomatic resource manipulations, called fingerprints. We statically mine fingerprints using concept analysis and then use them to identify security-sensitive operations and locate where they are performed by the server. Case stud-ies with three real-world servers show that our approach is affordable and effective. We were able to identify security-sensitive operations for each of these servers with a few hours of manual effort and modest domain knowledge. ...|$|R
40|$|Abstract—The {{exponential}} {{increase in}} mobile data demand, coupled with growing user expectation {{to be connected}} in all places at all times, have introduced novel challenges for researchers to address. Fortunately, the wide spread deployment of various network technologies and the increased adoption of multi-interface enabled devices have enabled researchers to develop solutions for those challenges. Such solutions aim to exploit available interfaces on such devices in both solitary and collaborative forms. These solutions, however, have faced a steep deployment barrier. In this paper, we present OSCAR, a multi-objective, incentive-based, collaborative, and deployable bandwidth aggregation system. We present the OSCAR architecture that does not introduce any intermediate hardware nor require changes to current applications or <b>legacy</b> <b>servers.</b> The OSCAR architec-ture is designed to automatically estimate the system’s context, dynamically schedule various connections and/or packets to different interfaces, be backwards compatible with the current Internet architecture, and provide the user with incentives for collaboration. We also formulate the OSCAR scheduler as a multi-objective, multi-modal scheduler that maximizes system throughput while minimizing energy consumption or financial cost. We evaluate OSCAR via implementation on Linux, as well as via simulation, and compare our results to the current opti-mal achievable throughput, cost, and energy consumption. Our evaluation shows that, in the throughput maximization mode, we provide up to 150 % enhancement in throughput compared to current operating systems, without any changes to <b>legacy</b> <b>servers.</b> Moreover, this performance gain further increases with the availability of connection resume-supporting, or OSCAR-enabled servers, reaching the maximum achievable upper-bound throughput. I...|$|R
40|$|Abstract- Distributed Cloud Computing Network is an {{important}} subsystem in an on going research referred to as Smart Green Energy Management System (SGEMS). In this paper, we propose a dynamic service availability architecture using two server types (virtualized and <b>legacy</b> <b>server</b> types) and their QoS metrics (particularly service delay time and resource availability) for detailed analysis. The work is aimed at determining the effect of virtualization {{in the context of}} resource allocation/scheduling in a cloud based server domain. This was carried out to facilitate the deployment of a previous work on Enterprise Energy Tracking Analytic Cloud Portal (EETACP) service proposed for SGEMS. Basically, the model demonstrates a technique for allocating resources from multiple resource pool (domain servers) in the cloud Data Center, while showing how the effect of resource allocation using virtualization can affect server performance. From the literature survey, little research has been done in this area, as such; this work seeks to use its findings to consolidate on the network performance of the SGEMS whose support platform is DCCN. Various mathematical models were derived to address performance constraints in a cloud based resource pool while using a Branch-and-Bound heuristic procedure to find good metric values for the evaluation study. From MATLAB Simevent environment, it was shown that that the server that have higher resource pool (virtualized type) offered an average delay of 21. 76 %, with average service availability 64. 42 %, hence having more job completion times. On the other hand, the server with lower resource pool (legacy type) offered an average delay of 78. 24 %, with average servic...|$|E
40|$|A {{reliable}} and useful {{storage area network}} (SAN) was created using two <b>legacy</b> <b>servers</b> and low-cost commodity hardware. Care was taken to create a “reference ” implementation for education purposes by using color coded cables, careful cable management, labels, and diagrams. The SAN is intended to support a scalable number of drive-less servers that will host virtual machines. The SAN servers boot from USB to Open Solaris, leaving the hard drives dedicated to SAN use. Comstar iSCSI was used to communicate between the servers, and the Zetta File System (ZFS) with RAIDZ 2 was used to protect against drive failure. IOMeter performance measures indicate that the SAN performs {{as well as a}} standard direct-attached hard drive, and approaches some local SSDs. The end result is redundant, fast storage using decommissioned and commodity hardware...|$|R
50|$|On many <b>legacy</b> IBM Intel-based <b>servers</b> the BMC is {{standard}} with the RSA II or RSA II Slimline as an Option device.|$|R
40|$|Part 2 : Middleboxes and AddressingInternational audienceNowadays, mobile {{devices are}} {{equipped}} with multiple radio interfaces, data centers provide redundant routing paths, and multihoming is the new tendency in existing, extensive server farms. Meanwhile, the unending growth rate of Internet traffic generation raises difficulties in meeting end user demands regarding bandwidth availability and Quality of Service standards, while TCP itself persists as a single-path transport protocol. Multipath TCP, {{as a set of}} extensions to legacy TCP, permits the simultaneous utilization of the available interfaces on a multihomed host, while preserving the standard TCP socket API. Consequently, smart terminals possess the distinct capability of leveraging path diversity in order to provide robust data transfers and enhance the overall connection performance. However, the implementation of Multipath TCP is still at a premature state. Ergo, we propose and evaluate a Multipath TCP Proxy as a mechanism towards the incremental adaptation of the extended protocol by service delivery platforms. Particularly, we examine the use of an HTTP Proxy as a protocol converter that will allow MPTCP-enabled clients to benefit from Multipath TCP even when communicating with <b>legacy</b> <b>servers...</b>|$|R
40|$|This paper {{summarizes}} {{the results of}} the BARWAN project, which focused on enabling truly useful mobile networking across an extremely wide variety of real-world networks and mobile devices. We present the overall architecture, summarize key results, and discuss four broad lessons learned along the way. The architecture enables seamless roaming in a single logical overlay network composed of many heterogeneous (mostly wireless) physical networks, and provides significantly better TCP performance for these networks. It also provides complex scalable and highly available services to enable powerful capabilities across a very wide range of mobile devices, and mechanisms for automated discovery and configuration of localized services. Four broad themes arose from the project: 1) the power of dynamic adaptation as a generic solution to heterogeneity, 2) the importance of cross-layer information, such as the exploitation of TCP semantics in the link layer, 3) the use of agents in the infrastructure to enable new abilities and to hide new problems from <b>legacy</b> <b>servers</b> and protocol stacks, and 4) the importance of soft state for such agents for simplicity, ease of fault recovery, and scalability...|$|R
40|$|With recent {{advances}} in virtualization and a growing concern regarding the administration and cooling costs associated with managing <b>legacy</b> <b>servers,</b> organisations are moving towards server and desktop virtualisation. Virtualisation provides the ability to provision a servers resources efficiently thus increasing hardware utilisation and reducing costs. While server virtualization provides clear advantages with regard to system management, higher system availability and lower recovery times, desktop virtualization is often complicated by the issue of determining the number of concurrent virtual desktops capable of running on a single server while providing acceptable performance to each desktop user. Determining the number of virtualised desktops capable of running on a virtual server is a non-trivial issue, and within most environments this is determined by trial and error. The objective of our experiments was to identify {{the maximum number of}} virtual desktop instances within our experimental environment. Each virtual desktop (guest operating system) was configured to automatically perform a specific tasks designed to strain the host servers resources to determine the breaking point of VirtualBox and to identify the maximum number of concurrent virtual desktops possible under 4 specific workloads...|$|R
40|$|Abstract—The {{explosive}} {{increase in}} data demand {{coupled with the}} rapid deployment of various wireless access technologies {{have led to the}} increase of number of multi-homed or multi-interface enabled devices. Fully exploiting these interfaces has motivated researchers to propose numerous solutions that aggregate their available bandwidths to increase overall throughput and satisfy the end-user’s growing data demand. These solutions, however, have faced a steep deployment barrier that we attempt to overcome in this paper. We propose a Deployable Bandwidth Aggregation System (DBAS) for multi-interface enabled devices. Our system does not introduce any intermediate hardware, mod-ify current operating systems, modify socket implementations, nor require changes to current applications or <b>legacy</b> <b>servers.</b> The DBAS architecture is designed to automatically estimate the characteristics of applications and dynamically schedule various connections or packets to different interfaces. Since our main focus is deployability, we fully implement DBAS on the Windows operating system and evaluate various modes of operation. Our implementation and simulation results show that DBAS achieves throughput gains up to 193 % compared to current operating systems, while operating as an out-of-the-box standard Windows executable, highlighting its deployability and ease of use. I...|$|R
40|$|The Jericho forum is a {{consortium}} of large corporations that have proposed a new architecture for network protection, a de-perimeterised architecture where organisations {{no longer have to}} hide behind a firewall. In this paper, we describe the design of a distributed network architecture where the need for conventional firewalls diminishes and where services can be offered to users regardless of their physical location. In this architecture, all systems should be able to protect themselves against network threats while security functions such as authentication and authorisation are handled at a global level. The result is that each individual server does not have to implement these functions and from a user point of view, functionality such as single sign-on becomes a possibility. The use of open protocols and standards is important and therefore technologies like Kerberos, IPSec, SSL and SSH will be used. Furthermore, the architecture must support older applications and application servers as well, e. g. <b>legacy</b> <b>servers</b> that cannot be modified to implement the new functionality. These should still work either with of some kind of decreased functionality or with full functionality provided by additional modules or front-end hardware that implements the new security functions...|$|R
40|$|Abstract—The {{exponential}} {{increase in}} mobile data demand, coupled with growing user expectation {{to be connected}} in all places at all times, have introduced novel challenges for researchers to address. Fortunately, the wide spread deployment of various network technologies and the increased adoption of multi-interface-enabled devices allow researchers to develop solutions for those challenges. Such solutions exploit available interfaces on these devices in both local and collaborative forms. These solutions, however, have faced a formidable deployment barrier. Therefore, in this paper, we present OSCAR, a multi-objective, incentive-based, collaborative, and deployable band-width aggregation system, designed to exploit multiple network interfaces on modern mobile devices. OSCAR’s architecture does not introduce any intermediate hardware nor require changes to current applications or <b>legacy</b> <b>servers.</b> This architecture estimates the interfaces characteristics and application require-ments, schedules various connections and/or packets to different interfaces, and provides users with incentives for collaboration and bandwidth sharing. We formulate the OSCAR scheduler as a multi-objective scheduler that maximizes system throughput while achieving user-defined efficiency goals for both cost and energy consumption. We implement a small scale prototype of our OSCAR system, which we use to evaluate its performance. Our evaluation shows that we provide up to 150 % enhancement in the throughput compared to current operating systems with only minor updates to the client devices. I...|$|R
50|$|Since {{the release}} of Windows 2000, the use of WINS for name {{resolution}} has been deprecated by Microsoft, with hierarchical Dynamic DNS now configured as the default name resolution protocol for all Windows operating systems. Resolution of (short) NETBIOS names by DNS requires that a DNS client expand short names, usually by appending a connection-specific DNS suffix to its DNS lookup queries. WINS can still be configured on clients as a secondary name resolution protocol for interoperability with legacy Windows environments and applications. Further, Microsoft DNS servers can forward name resolution requests to <b>legacy</b> WINS <b>servers</b> {{in order to support}} name resolution integration with legacy (pre-Windows 2000) environments that do not support DNS.|$|R
50|$|The Google Talk App for Android and the Google Chat tool in Gmail were {{discontinued}} on June 26, 2017, and {{no longer}} function. Users may still continue to use 3rd party XMPP clients to connect to the <b>legacy</b> Google talk <b>server,</b> but only for 1-on-1 chat with hangouts users, though support for XMPP federation was dropped, {{and there is no}} groupchat support with Hangouts users in this mode.|$|R
40|$|This project brings {{together}} five distributed university-based participants, each contributing unique models and architectures of client/server implementations that support {{one or more}} design phases in a distributed collaborative environment. ffl MIT: A Web-based CAD collaborative framework CollabTop, built on WebTop, supporting schematic design entry, simulation, and verification. ffl MSU: Custom JavaCADD client/server, based on Java remote method invocation, which offers sophisticated data packing mechanism for passing complex objects between the client/server with minimum programming effort. ffl NCSU: Collaborative user-configurable workflow clients, under the joint control of a synchronous group server (SGS) and an asynchronous group server (AGS). Distributed data is managed by AGS under Concurrent Version System (CVS). Each client can independently visit any number of web-based or <b>legacy</b> tool <b>servers,</b> modeled as OmniExpress nodes in the workflow. ffl Stanford U. : A network com [...] ...|$|R
40|$|Abstract A {{distributed}} resource environment (DRE) allows distributed compo-nents (i. e., <b>servers,</b> <b>legacy</b> systems, databases, COTs, printers, scan-ners, etc.) to {{be treated}} akin to OS resources, where each component (resource) can publish services (an API), that are then available for use by clients and resources alike. DREs have lagged in support of secu-rity. To address this deciency, this paper concentrates on proposing a technique for seamlessly integrating a role-based security model, au-thorization, authentication, and enforcement into a DRE, including our prototyping with the JINI DRE...|$|R
40|$|The {{advent of}} social markup {{languages}} and lightweight public data access methods {{has created an}} opportunity to share the social, documentary and system information locked in most servers as a mashup. Whereas solutions already exists for creating and managing mashups from network sources, we propose here a mashup framework whose primary information sources are the applications and user files of a server. This enables us to use <b>server</b> <b>legacy</b> data sources that are already maintained as part of basic administration to semantically link user documents and accounts using social web constructs...|$|R
5000|$|UTF-7, {{although}} sometimes considered deprecated, has {{an advantage}} over other Unicode encodings in {{that it does not}} require a transfer encoding to fit within the seven-bit limits of many <b>legacy</b> Internet mail <b>servers.</b> On the other hand, UTF-16 must be transfer encoded to fit SMTP data format. Although not strictly required, UTF-8 is usually also transfer encoded to avoid problems across seven-bit mail servers. MIME transfer encoding of UTF-8 makes it either unreadable as a plain text (in the case of base64) or, for some languages and types of text, heavily size inefficient (in the case of quoted-printable).|$|R
50|$|The QNX kernel {{contains}} only CPU scheduling, interprocess communication, interrupt redirection and timers. Everything else runs as a user process, including a special {{process known as}} proc which performs process creation and memory management by operating {{in conjunction with the}} microkernel. This is made possible by two key mechanisms — subroutine-call type interprocess communication, and a boot loader which can load an image containing not only the kernel but any desired collection of user programs and shared libraries. There are no device drivers in the kernel. The network stack is based on NetBSD code. Along with its support for its own, native, device drivers, QNX supports its <b>legacy,</b> io-net manager <b>server,</b> and the network drivers ported from NetBSD.|$|R
40|$|The use of ad hoc {{networking}} technologies {{is emerging}} as a viable and cost-effective solution to extend the range of traditional wireless local area networks (WLANs). In these networks, mobile client traffic reaches the access points through multi-hop wireless paths that are established by using an ad hoc routing protocol. However, several technical challenges have to be faced in order to construct such an extended WLAN. For instance, traditional autoconfiguration protocols commonly used in infrastructure-based WLANs, such as DHCP or Zeroconf, are not directly applicable in multi-hop wireless networks. To address this problem, {{in this paper we}} propose extensions to DHCP to enable the dynamic allocation of globally routable IPv 4 addresses to mobile stations in hybrid ad hoc networks, which transparently integrate conventional wired technologies with wireless ad hoc networking technologies. Some of the attractive features of our solution are its ability to cope with node mobility, the introduction of negligible protocol overheads, and the use of <b>legacy</b> DHCP <b>servers.</b> We have implemented a prototype of our scheme, and tested its functionalities considering various topology layouts, network loads and mobility conditions. The experimental results show that our solution ensures short address configuration delays and low protocol overheads...|$|R
50|$|In December 2010, Brocade began {{shipping}} the Brocade VDX 6720 Switch {{as part of}} {{its product}} family for Ethernet fabric environments based on Brocade VCS Fabric technology designed for highly scalable virtualized and cloud computing environments. In August 2011, Brocade introduced two additional products for this family. The Brocade VDX 6730 Switch is a 10 GbE switch which can also use FCoE to bridge VCS Fabrics with Fibre Channel SAN fabrics. The Brocade VDX 6710 Switch is an entry-level 1/10 GbE switch which enables <b>legacy</b> 1 GbE <b>servers</b> to connect to VCS Fabrics as well as traditional LANs. In September 2012, Brocade announced a modular switch as part of this portfolio. The Brocade VDX 8770 Switch supports single VCS Fabrics as large as 8000+ switch ports supporting up to 384,000 VMs attached to a single VCS Fabric. The VDX 8770 provides port-to-port latency at 3.5 μs across 1, 10, and 40 GbE ports.|$|R
