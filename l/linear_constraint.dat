442|3686|Public
5000|$|Enaml's {{layout engine}} {{is built on}} top of the Cassowary <b>linear</b> <b>constraint</b> optimizer.|$|E
5000|$|This form has 64 {{coefficients}} {{requiring the}} function {{to have a}} given value or given directional derivative at a point places one <b>linear</b> <b>constraint</b> on the 64 coefficients.|$|E
5000|$|If the {{parameter}} space [...] is finite (consisting of finitely many elements), then this robust optimization problem {{itself is a}} linear programming problem: for each [...] there is a <b>linear</b> <b>constraint</b> [...]|$|E
40|$|In {{this paper}} we {{consider}} Datalog queries with <b>linear</b> <b>constraints.</b> We identify several syntactical subcases of Datalog queries with <b>linear</b> <b>constraints,</b> called safe queries, {{and show that}} the least model of safe Datalog queries with <b>linear</b> <b>constraints</b> can be evaluated bottomup in closed-form. These subcases include Datalog with only positive and upper-bound or only negative and lower bound constraints or only half-addition, upper and lower bound constraints. We also study other subcases where the recognition problem is decidable...|$|R
30|$|Restricting the <b>constraints</b> set to <b>linear</b> <b>constraints</b> {{does not}} pose a problem in MRIO reconciliation, as most {{conditions}} that are posed on the MRIO table elements by superior data (such as balancing, adhered to sector totals, matching an aggregated table) {{can be interpreted as}} <b>linear</b> <b>constraints.</b>|$|R
40|$|Abstract. <b>Linear</b> integer <b>constraints</b> {{are one of}} {{the most}} {{important}} constraints in combinatorial problems since they are commonly found in many practical applications. Typically, encoding <b>linear</b> <b>constraints</b> to SAT performs poorly in problems with these constraints in comparison to constraint programming (CP) or mixed integer programming (MIP) solvers. But some problems contain a mix of combinatoric <b>constraints</b> and <b>linear</b> <b>constraints,</b> where encoding to SAT is highly effective. In this paper we define new approaches to encoding <b>linear</b> <b>constraints</b> into SAT, by extending encoding methods for pseudo-Boolean constraints. Experimental results show that these methods are not only better than the state-of-the-art SAT encodings, but also improve on MIP and CP solvers on appropriate problems. Combining the new encoding with lazy decomposition, which during runtime only encodes constraints that are important to the solving process that occurs, gives a robust approach to many highly combinatorial problems involving <b>linear</b> <b>constraints.</b> ...|$|R
50|$|An {{alternative}} constraint method, LINCS (<b>Linear</b> <b>Constraint</b> Solver) {{was developed}} in 1997 by Hess, Bekker, Berendsen and Fraaije, and {{was based on the}} 1986 method of Edberg, Evans and Morriss (EEM), and a modification thereof by Baranyai and Evans (BE).|$|E
5000|$|Algodoo ( [...] ) is a physics-based 2D sandbox {{freeware}} from Algoryx Simulation AB (marketed {{as simply}} Algoryx) as {{the successor to}} the popular physics application Phun. Algodoo was released on September 1, 2009 and is presented as an learning tool, an open ended computer game, an animation tool, and/or an engineering tool. The software has functionality with desktop and laptop computers, touch screen tablets such as the Intel Classmate PC and the iPad, and interactive white board systems such as SMART Boards. The physics engine in Algodoo utilizes the SPOOK <b>linear</b> <b>constraint</b> solver by Claude Lacoursière and {{a modified version of}} Smoothed-Particle Hydrodynamics (SPH) computational method.|$|E
50|$|The method {{proceeds}} {{by first}} dropping {{the requirement that}} the xi be integers and solving the associated linear programming problem to obtain a basic feasible solution. Geometrically, this solution will be a vertex of the convex polytope consisting of all feasible points. If this vertex is not an integer point then the method finds a hyperplane with the vertex {{on one side and}} all feasible integer points on the other. This is then added as an additional <b>linear</b> <b>constraint</b> to exclude the vertex found, creating a modified linear program. The new program is then solved and the process is repeated until an integer solution is found.|$|E
40|$|We {{present a}} method which computes {{optimized}} representations for non-convex polyhedra. Our method detects so-called redundant <b>linear</b> <b>constraints</b> in these representations {{by using an}} incremental SMT solver and then removes the redundant constraints based on Craig interpolation. The approach is evaluated both for formulas from the model checking context including boolean combinations of <b>linear</b> <b>constraints</b> and boolean variables and for random trees composed of quantifiers, AND-, OR-, NOT-operators, and <b>linear</b> <b>constraints</b> produced by a generator. The results clearly show the advantages of our approach in comparison to state-of-the-art solvers...|$|R
5000|$|... #Subtitle level 3: Other {{optimization}} {{problems with}} <b>linear</b> <b>constraints</b> ...|$|R
40|$|This work {{presents}} the formal approach for identifying continuous transfer {{functions of the}} vertical dynamics of a high-speed ship as a nonlinear optimization problem with <b>linear</b> <b>constraints.</b> The proposed solution is described with a hybrid optimization method (genetic algorithm + nonlinear optimization algorithm with <b>linear</b> <b>constraints</b> from the Matlab toolbox) ...|$|R
50|$|Inequality-constrained {{minimization}} of {{a function}} that is zero everywhere {{corresponds to the}} problem of simply identifying any feasible point. It turns out that any linear programming problem can be reduced to a linear feasibility problem (e.g. minimize the zero function subject to some linear inequality and equality constraints). One {{way to do this is}} by combining the primal and dual linear programs together into one program, and adding the additional (<b>linear)</b> <b>constraint</b> that the value of the primal solution is no worse than the value of the dual solution. Another way is to treat the objective of the linear program as an additional constraint, and use binary search to find the optimum value.|$|E
50|$|Shapley-Shubik use a numeraire {{and trading}} posts for goods. The {{relative}} price of each good {{in terms of}} the numeraire is determined as the ratio of the amount of the numeraire brought at each post, to the quantity of goods offered for sale at that post. In this way, every agent is allocated goods in proportion to his bids, so that posts always clear.Pradeep Dubey and John Geanakoplos show that such a game can be a strategic foundation of the Walras equilibrium. A key ingredient of such approaches is to have very large numbers of players, such that for each player the action appears to him as a <b>linear</b> <b>constraint</b> that he cannot influence.|$|E
50|$|This {{is shown}} in the graph below, which {{illustrates}} the trade-off between allocating time between leisure activities and income-generating activities. The <b>linear</b> <b>constraint</b> indicates that every additional hour of leisure undertaken requires the loss of an hour of labour and thus of the fixed amount of goods that that labour's income could purchase. Individuals must choose how much time to allocate to leisure activities and how much to working. This allocation decision is informed by the indifference curve labelled IC1. The curve indicates the combinations of leisure and work that will give the individual a specific level of utility. The point where the highest indifference curve is just tangent to the constraint line (point A), illustrates the optimum for this supplier of labour services.|$|E
40|$|AbstractWe develop {{canonical}} correlation analysis by imposing <b>linear</b> <b>constraints</b> upon parameters corresponding to {{two sets of}} variables. The results of our method, which we call canolc, are shown in terms of projection operators both orthogonal and oblique. Further, calc (correspondence analysis with <b>linear</b> <b>constraints)</b> {{turns out to be}} a special case of canolc...|$|R
40|$|In {{this paper}} we propose an {{efficient}} Gibbs sampler for simulation of a multivariate normal random vector subject to inequality <b>linear</b> <b>constraints.</b> Inference in a Bayesian linear model, where the regression parameters are subject to inequality <b>linear</b> <b>constraints,</b> is the primary motivation behind this research. In the literature, implementations of the Gibbs sampler for the multivariate normal distribution subject to inequality <b>linear</b> <b>constraints</b> and for the multiple linear regression with inequality constraints often exhibit poor mixing and slow convergence. This paper overcomes these limitations ∗ Gabriel Rodriguez-Yam is postdoctoral fellow and Richard Davis is Professor and chair...|$|R
40|$|Abstract. We giveanAC 0 {{upper bound}} on the {{complexity}} of rst-oder queries over (in nite) databases de ned by restricted <b>linear</b> <b>constraints.</b> This result enables us to deduce the non-expressibility ofvarious usual queries, such as the parity of the cardinality of a set or the connectivity of a graph in rst-order logic with <b>linear</b> <b>constraints.</b> ...|$|R
5000|$|The cutting-plane {{method for}} solving 0-1 integer programs, first {{introduced}} for the traveling salesman problem by [...] and generalized to other integer programs by , {{takes advantage of}} this multiplicity of possible relaxations by finding a sequence of relaxations that more tightly constrain the solution space until eventually an integer solution is obtained. This method starts from any relaxation of the given program, and finds an optimal solution using a linear programming solver. If the solution assigns integer values to all variables, {{it is also the}} optimal solution to the unrelaxed problem. Otherwise, an additional <b>linear</b> <b>constraint</b> (a cutting plane or cut) is found that separates the resulting fractional solution from the convex hull of the integer solutions, and the method repeats on this new more tightly constrained problem.|$|E
50|$|As {{a result}} of these operations, the {{addition}} of new constraints may change the old ones. It is essential that the interpreter is able to undo these changes when it backtracks. The simplest case method is for the interpreter to save the complete state of the store every time it makes a choice (it chooses a clause to rewrite a goal). More efficient methods for allowing the constraint store to return to a previous state exist. In particular, one may just save the changes to the constraint store made between two points of choice, including the changes made to the old constraints. This can be done by simply saving the old value of the constraints that have been modified; this method is called trailing. A more advanced method is to save the changes that have been done on the modified constraints. For example, a <b>linear</b> <b>constraint</b> is changed by modifying its coefficient: saving the difference between the old and new coefficient allows reverting a change. This second method is called semantic backtracking,because the semantics of the change is saved rather than the old version of the constraints only.|$|E
40|$|In the paper, we {{investigate}} a <b>linear</b> <b>constraint</b> optimization reformulation to {{a more general}} form of the l_ 1 regularization problem and give some good properties of it.  We first show that the equivalence between the <b>linear</b> <b>constraint</b> optimization problem and the l_ 1 regularization problem.  Second, the KKT point of the <b>linear</b> <b>constraint</b> problem always exists since the constraints are linear; we show that the half constraints must be active at any KKT point. In addition, we show that the KKT points of the <b>linear</b> <b>constraint</b> problem are {{the same as the}} stationary points of the l_ 1 regularization problem.  Based on the <b>linear</b> <b>constraint</b> optimization problem, we propose a nonomotone spectral gradient method and establish its global convergence.  Numerical experiments with compressive sense problems show that our approach is competitive with several known methods for standard l_ 2 -l_ 1 problem. </p...|$|E
50|$|For <b>linear</b> <b>constraints,</b> {{a fuller}} picture is {{provided}} by the following table.|$|R
40|$|We give an AC 0 {{upper bound}} on the {{complexity}} of first-oder queries over (infinite) databases defined by restricted <b>linear</b> <b>constraints.</b> This result enables us to deduce the non-expressibility of various usual queries, such as the parity of the cardinality of a set or the connectivity of a graph in first-order logic with <b>linear</b> <b>constraints</b> over the reals...|$|R
5000|$|... #Subtitle level 2: General {{solution}} for the maximum entropy distribution with <b>linear</b> <b>constraints</b> ...|$|R
40|$|We make a {{comparative}} study of chiral-boson theories in the Florenani-Jackiw (FJ) and <b>linear</b> <b>constraint</b> formulations. A special attention {{is given to the}} case with an improved way of implementing the <b>linear</b> <b>constraint.</b> We show that it has the same spectrum of the FJ formulation. Comment: 11 pages, Late...|$|E
40|$|Abstract — The {{purpose of}} this note is to prove, for real frames, that signal {{reconstruction}} from the absolute value of the frame coefficients is equivalent to solution of a sparse signal optimization problem, namely a minimum `p (quasi) norm over a <b>linear</b> <b>constraint.</b> This <b>linear</b> <b>constraint</b> reflects the coefficients relationship {{within the range of}} the analysis operator. Index Terms — frames, nonlinear processing, sparse represen-tation I...|$|E
40|$|In a {{previous}} paper we determined one dimensional distributions of a stationary field with linear regressions and quadratic conditional variances under a <b>linear</b> <b>constraint</b> on the coefficients of the quadratic expression. In this paper {{we show that}} for stationary Markov chains with linear regressions and quadratic conditional variances the coefficients of the quadratic expression are indeed tied by a <b>linear</b> <b>constraint</b> which can take {{only one of the}} two alternative forms...|$|E
40|$|Abstract. <b>Linear</b> <b>constraints</b> occur {{naturally}} in many reasoning {{problems and the}} information that they represent is often uncertain. There is a difficulty in applying many AI uncertainty formalisms to this situation, as their representation of the underlying logic, either as a mutually exclusive and exhaustive set of possibilities, or with a propositional or a predicate logic, is inappropriate (or at least unhelpful). To overcome this, we express reasoning with <b>linear</b> <b>constraints</b> as a logic, and develop the formalisms based on this different underlying logic. We focus in particular on a possibilistic logic representation of uncertain <b>linear</b> <b>constraints,</b> a lattice-valued possibilistic logic, and a Dempster-Shafer representation. ...|$|R
40|$|This paper {{deals with}} linear state space {{modelling}} subject to general <b>linear</b> <b>constraints</b> {{on the state}} vector. The discussion concentrates on four topics: the constrained Kalman filtering versus the recursive restricted least squares estimator; a new proof of the constrained Kalman filtering under a conditional expectation framework; <b>linear</b> <b>constraints</b> under a reduced state space modelling; and state vector prediction under <b>linear</b> <b>constraints.</b> The techniques proposed are illustrated in two real problems. The first problem is related to investment analysis under a dynamic factor model, whereas the second is about making constrained predictions within a GDP benchmarking estimation. Copyright (c) 2010 The Author. Journal compilation (c) 2010 International Statistical Institute. ...|$|R
40|$|We {{introduce}} a parallel machine scheduling problem {{in which the}} processing times of jobs are not given in advance but are determined by a system of <b>linear</b> <b>constraints.</b> The objective is to minimize the makespan, i. e., the maximum job completion time among all feasible choices. This novel problem is motivated by various real-world application scenarios. We discuss the computational complexity and algorithms for various settings of this problem. In particular, we show {{that if there is}} only one machine with an arbitrary number of <b>linear</b> <b>constraints,</b> or there is an arbitrary number of machines with no more than two <b>linear</b> <b>constraints,</b> or both the number of machines and the number of <b>linear</b> <b>constraints</b> are fixed constants, then the problem is polynomial-time solvable via solving a series of linear programming problems. If both the number of machines and the number of constraints are inputs of the problem instance, then the problem is NP-Hard. We further propose several approximation algorithms for the latter case. Comment: 21 page...|$|R
40|$|AbstractIn Bryc (Ann. Probab., (1998), to appear), we {{determined}} one-dimensional distributions of a stationary field with linear regressions (1) and quadratic conditional variances (2) under a <b>linear</b> <b>constraint</b> (7) on the coefficients of the quadratic expression (3). In this paper, {{we show that}} for stationary Markov chains with linear regressions and quadratic conditional variances the coefficients of the quadratic expression are indeed tied by a <b>linear</b> <b>constraint</b> which can take {{only one of the}} two alternative forms (7), or (8) ...|$|E
40|$|Abstract. <b>Linear</b> <b>constraint</b> {{systems are}} simple {{deductive}} systems {{based on the}} main underlying idea of linear logic: hypotheses represent physical resources which are consumed by the entailment relation. For such systems, we define back-and-forth translations into a class of high-level Petri nets. Using the specific properties of that class of nets, and previous results about {{the complexity of the}} reachability problem for such nets, we examine the complexity of the entailment problem for finitely generated <b>linear</b> <b>constraint</b> systems and we show that it is NP-complete. ...|$|E
40|$|In Bryc (Ann. Probab., (1998), to appear), we {{determined}} one-dimensional distributions of a stationary field with linear regressions (1) and quadratic conditional variances (2) under a <b>linear</b> <b>constraint</b> (7) on the coefficients of the quadratic expression (3). In this paper, {{we show that}} for stationary Markov chains with linear regressions and quadratic conditional variances the coefficients of the quadratic expression are indeed tied by a <b>linear</b> <b>constraint</b> which can take {{only one of the}} two alternative forms (7), or (8). Conditional moments Polynomial regression Linear regression...|$|E
40|$|Linear {{representations}} for a subclass of boolean symmetric functions {{selected by}} a parity condition are shown {{to constitute a}} generalization of the <b>linear</b> <b>constraints</b> on probabilities introduced by Boole. These <b>linear</b> <b>constraints</b> are necessary to compute probabilities of events with relations between the. arbitrarily specified with propositional calculus boolean formulas. Comment: Appears in Proceedings of the Second Conference on Uncertainty in Artificial Intelligence (UAI 1986...|$|R
40|$|The mixed {{estimator}} {{that has}} been widely used in linear regression models {{is applied to the}} general covariance structure model. This application provides a simple alternative to the Bayesian estimation, constrained estimation and inequality constrained estimation methods that have been proposed in the covariance structure model literature. covariance structures cross-validation exact <b>linear</b> <b>constraints</b> generalized least squares inequality constraints mixed estimator stochastic <b>linear</b> <b>constraints</b> Wald test...|$|R
40|$|We {{revisit the}} {{classical}} dual ascent algorithm for minimization of convex functionals {{in the presence}} of <b>linear</b> <b>constraints,</b> and give convergence results which apply even for non-convex functionals. We describe limit points in terms of the convex envelope. We also introduce a new augmented version, which is shown to have superior convergence properties, and provide new results even for convex but non-differentiable objective functionals (as well as non-convex). The results are applied to low rank approximation of a given matrix, subject to <b>linear</b> <b>constraints.</b> In particular, letting the <b>linear</b> <b>constraints</b> enforce Hankel structure of the respective matrices, the algorithms can be applied to complex frequency estimation. We provide numerical tests in this setting...|$|R
