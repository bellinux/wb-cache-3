135|1048|Public
50|$|The European Vital Computer (EVC) is {{the heart}} of <b>local</b> <b>computing</b> {{capabilities}} in the driving vehicle. It is connected with external data communication, internal controls to speed regulation of the loko, location sensors and all cab devices of the driver.|$|E
50|$|Prof. Joshi is {{a member}} of various {{national}} and International bodies i.e. International Association of Mass Communication Research (IAMCR), International Communication Association (ICA),International Pragmatic Association (IPrA), Global Initiative for <b>Local</b> <b>Computing</b> (GILC), Ireland, World Association of Community Radio Broadcasters (AMARC).|$|E
5000|$|... #Caption: Aerial view of {{the site}} of the Virgo {{experiment}} showing the central building, the Mode-Cleaner building, the full 3 km-long west arm {{and the beginning of the}} north arm (on the right). The other buildings include offices, workshops, the <b>local</b> <b>computing</b> center and the interferometer control room. When this picture was shot, the building hosting the project management and the canteen had not been built yet.|$|E
30|$|Community {{computation}} {{out from}} the set of <b>local</b> communities <b>computed</b> in the previous step.|$|R
40|$|We {{describe}} {{the development of}} a scientific cloud computing (SCC) platform that offers high performance computation capability. The platform consists of a scientific virtual machine prototype containing a UNIX operating system and several materials science codes, together with essential interface tools (an SCC toolset) that offers functionality comparable to <b>local</b> <b>compute</b> clusters. In particular, our SCC toolset provides automatic creation of virtual clusters for parallel computing, including tools for execution and monitoring performance, as well as efficient I/O utilities that enable seamless connections to and from the cloud. Our SCC platform is optimized for the Amazon Elastic Compute Cloud (EC 2). We present benchmarks for prototypical scientific applications and demonstrate performance comparable to <b>local</b> <b>compute</b> clusters. To facilitate code execution and provide user-friendly access, we have also integrated cloud computing capability in a JAVA-based GUI. Our SCC platform may be an alternative to traditional HPC resources for materials science or quantum chemistry applications...|$|R
3000|$|... is {{the average}} <b>local</b> cost <b>computed</b> within the image. Furthermore, we do not use the costless {{propagation}} here [...]...|$|R
50|$|The {{book was}} written after the book The Soul of a New Machine by Tracy Kidder, which was highly {{influential}} in <b>local</b> <b>computing</b> circles. That book documents {{the competition between}} Data General and DEC to create a 32-bit minicomputer. Both companies missed the opportunity to launch successful micro-computers {{and by the time}} the book was published, the IBM PC had already become a de facto standard. The year 1988 heralded a financial crisis that hit both companies hard, and started a downward slide in sales from which they never recovered.|$|E
5000|$|Privacy mode or [...] "private {{browsing}}" [...] or [...] "incognito mode" [...] is a privacy {{feature in}} some web browsers to disable browsing {{history and the}} web cache. This allows a person to browse the Web without storing local data that could be retrieved at a later date. Privacy mode will also disable the storage of data in cookies and Flash cookies. This privacy protection is only on the <b>local</b> <b>computing</b> device as {{it is still possible}} to identify frequented websites by associating the IP address at the web server.|$|E
50|$|Saucier {{described}} {{the use of}} meteorogram charts to analyze surface weather data with respect to time. It is a simple extension to project these variables into the future {{using data from the}} forecast models and produce the modern meteogram. So, meteograms have evolved to mean any group of meteorological variables graphed with respect to time, whether observed or forecast, usually confined to surface variables, though some include time-height charts. Meteogram use increased in the 1990s with the growth of computing power. Now meteograms are created from various models and made available to forecasters through the Internet. In addition, researchers and operational forecasters have the capability to make meteograms for their local use on their <b>local</b> <b>computing</b> systems.|$|E
40|$|Abstract. Setting up a {{symbolic}} algebraic {{system is the}} first step in mathematics mechanization of any branch of mathematics. In this paper, we establish a compact symbolic algebraic framework for <b>local</b> geometric <b>computing</b> in intrinsic differential geometry, by choosing only the Lie derivative and the covariant derivative as basic local differential operators. In this framework, not only geometric entities such as the curvature and torsion of an affine connection have elegant representations, but their involved <b>local</b> geometric <b>computing</b> can be simplified...|$|R
40|$|In {{a modern}} {{computing}} environment UNIX workstations {{become more and}} more important. They provide the user with <b>local</b> <b>compute</b> power and an easy to use X window based interface. On the other hand workstation administration under UNIX is expensive and requires different skills, knowledge, and experience than users normally have. This paper presents our experience with a project to support a workstation group based on RS/ 6000 s under AIX. It will cover functions, tools, and resources to manage workstation groups in a production environment...|$|R
2500|$|Enter any date, time, location, <b>local</b> conditions; <b>compute</b> [...] "UV dose rate" [...] of type [...] "Skin burn"; {{and divide}} result by 25 {{to obtain the}} UV Index.|$|R
5000|$|Frankel {{published}} {{a number of}} scientific papers throughout his career. Some of them explored the use of statistical sampling techniques and machine driven solutions. In a 1947 paper in Physical Review, he and Metropolis predicted the utility of computers in replacing manual integration with iterative summation as a problem solving technique. As head of a new Caltech digital computing group he worked with PhD candidate Berni Alder in 1949-1950 to develop what {{is now known as}} called Monte Carlo analysis. They used techniques that Enrico Fermi had pioneered in the 1930s. Due to a lack of <b>local</b> <b>computing</b> resources, Frankel travelled to England in 1950 to run Alder's project on the Manchester Mark 1 computer. Unfortunately, Alder's thesis advisor was unimpressed, so Alder and Frankel delayed publication of their results until 1955, in the Journal of Chemical Physics. This left the major credit for the technique to a parallel project by a team including Teller and Metropolis who published similar work in the same journal in 1953.|$|E
30|$|Layer 1 Node-level {{real-time}} {{is concerned}} with scheduling the <b>local</b> <b>computing</b> resources inside a single robotic computing node.|$|E
3000|$|... 0 {{with the}} {{measurements}} given in [24] for energy and frequency characteristics of <b>local</b> <b>computing</b> in commercial mobile handsets, {{as well as}} computation of data ratios in practical applications. As in an urban area, we have considered users with random velocity from 3 to 120  km/h.|$|E
30|$|The {{first three}} {{versions}} of GATK are implemented in Java and optimized {{for use on}} <b>local</b> <b>compute</b> infrastructures. Version 4 of GATK (at the time of writing in Beta) uses Spark to improve I/O performance and scalability ([URL] It uses GenomicsDB ([URL] for efficiently storing, querying, and accessing (sparse matrix) variant data. GenomicsDB is built on top of Intel’s TileDB ([URL] which is designed for scalable storage and processing of sparse matrices. To support tertiary (downstream) {{analysis of the data}} produced by GATK, the Hail framework ([URL] provides interactive analyses. It optimizes storage and access of variant data (sparse matrices) and provides built-in analysis functions. Hail is implemented using Spark and Parquet.|$|R
5000|$|... #Caption: A PLATO V {{terminal}} in 1981 displaying RankTrek application, one of {{the first}} to combine simultaneous <b>local</b> microprocessor-based <b>computing</b> with remote mainframe computing. The monochromatic plasma display's characteristic orange glow is illustrated. Infrared sensors mounted around the display watch for a user's touch screen input.|$|R
5000|$|Step {{one can be}} seen a {{simplified}} version of Personalized PageRank. Step two sums up the vector similarity of each iteration. Both, matrix and <b>local</b> representation, <b>compute</b> the same similarity score. CoSimRank {{can also be used}} to compute the similarity of sets of nodes, by modifying [...]|$|R
40|$|Mobile edge {{computing}} (MEC) has {{been regarded as}} a promising technique to enhance the computation capabilities of wireless devices, by enabling them to offload computation-intensive tasks to base stations (BSs) at the network edge. This paper studies a new multiuser MEC system by exploiting the multi-antenna non-orthogonal multiple access (NOMA) -based computation offloading. In this system, multiple users can simultaneously offload their computation tasks to one multi-antenna BS over the same time/frequency resources for remote execution, and the BS can employ successive interference cancellation (SIC) for information decoding. We consider the partial offloading case, such that each user can partition the computation task into two parts for <b>local</b> <b>computing</b> and offloading, respectively. Under this setup, we minimize the weighted sum of the energy consumption at all users subject to their computation latency constraints, by jointly optimizing each user's task partition, local central processing unit (CPU) frequency for <b>local</b> <b>computing,</b> and transmit power and rate for offloading, {{as well as the}} BS's decoding order for SIC. We present an efficient algorithm to obtain the globally optimal solution to this problem by applying the Lagrange dual method. Numerical results show that the proposed NOMA-based partial offloading design significantly improves the energy efficiency of the multiuser MEC system, as compared to benchmark schemes with orthogonal multiple access (OMA) -based partial offloading, <b>local</b> <b>computing</b> only, and full offloading only. Comment: 7 pages and 4 figure...|$|E
40|$|We {{present a}} CGM/BSP {{algorithm}} for computing an alignment (or string editing) between two strings A and C, with jAj = m and jCj = n. The algorithm requires O(p) communication rounds and O(nm p) <b>local</b> <b>computing</b> time, on a distributed memory parallel computer of p processors each with O(nm=p) memory. We also present implementation results obtained on Beowulf machine with 64 nodes...|$|E
40|$|One {{complication}} {{you probably}} {{have no control over}} is your <b>local</b> <b>computing</b> environ-ment. But even if it's horrible, as many are, you don't have to suffer stoically. Even a modest improvement of frequently used parts, like your programming and job control lan-guages, is well worthwhile, and there's no excuse for not trying to conceal the worst aspects. [...] Kernighan and Plauger, Software Tools 1...|$|E
40|$|Cloud {{computing}} {{works on}} the basis of multiple server based resources via a digital network over the web. The cloud service providers allow users to charter computing resources from the extensive data centers. Resource sharing is the major part in the cloud computing. Sharing the data in the cloud depends on the network performance of the data centers. While data centers provide many mechanisms to schedule <b>local</b> <b>compute,</b> memory, disk resources and bandwidth allocation for apportioning network resources. The Data centers which resides at different location to compute the resources needed by the cloud service consumer. Bandwidth allocation places a major role in sharing the resources towards the data center networks. This paper presents with the analysis on bandwidth allocation mechanisms, comparison between the different cloud vendors and their cost analysis...|$|R
40|$|CP 2 K is an {{important}} European program for atomistic simulation for many users of the PRACE Research Infrastructure as well as national and <b>local</b> <b>compute</b> resources. In {{the context of a}} PRACE Preparatory Access Type C project, we have parallelised several routines in CP 2 K to allow the code to gain better performance on the Intel Xeon Phi for a materials science application. We have obtained a 50 % speedup in the maximum performance of the code on the Xeon Phi, but {{have not been able to}} demonstrate better performance than running the same calculation on a Sandy Bridge 16 -core CPU node. We present details of the developments made to CP 2 K, and discuss several lessons, which will be of wider interest to developers considering porting their codes to Xeon Phi...|$|R
30|$|<b>Compute</b> <b>local</b> {{variance}} {{value for}} all pixels. Find the threshold {{value of the}} local variance values which is {{able to meet the}} embedding capacity.|$|R
40|$|Cracker attack plans {{often include}} {{attempts}} {{to gain access}} to target <b>local</b> <b>computing</b> and networking resources via war dialed entry around firewall-protected gateways. This paper describes two methods for organizations to reduce this risk in environments where removal of unknown modems is not feasible. In particular, we outline a workstation-based war dial trap scheme as well as a war dial signature-based Private Branch Exchange (PBX) audit processing method. ...|$|E
40|$|Abstract—Experiments for {{the online}} {{closed-loop}} control of neural prosthetics require feedback within 100 ms. In a typical neurophysiology laboratory with <b>local</b> <b>computing</b> machines, {{a majority of}} this time is spent on acquiring and analyzing the neural signals and a minority (i. e. less than a millisecond) is actual data transfer among machines on local- or campus-area networks. However, the <b>local</b> <b>computing</b> machines may not offer the computational resources necessary for running complex algorithms or scenarios that have been recently proposed. While scientists {{can take advantage of}} remote computing resource providers, wide-area networks present much larger latencies that can affect an online experiment. This work presents a split modeling approach that allows the execution of a controller on the neurophysiology resource and the execution of computationally intensive modeling and adaptation algorithms on a remote datacenter, even with the inevitable network latency. Simulation results are presented to quantify how the accuracy of the controller is affected by the split modeling approach in the presence of delays, and to demonstrate that scientists can take advantage of remotely available massive resources. I...|$|E
40|$|Abstract. We {{present a}} CGM/BSP {{algorithm}} for computing an align-ment (or string editing) between two strings A and C, with jAj = m and jCj = n. The algorithm requires O(p) communication rounds and O(nm p <b>local</b> <b>computing</b> time, on a distributed memory parallel computer of p processors each with O(nm=p) memory. We also present implementation results obtained on Beowulf machine with 64 nodes. Topic of interest: Algorithms and applications in Bioinformatics...|$|E
40|$|This paper {{presents}} a general method for {{the diagnosis of}} large systems, such as telecommunication networks. Because {{of the size of}} the system, the model we use is decentralized. In order to increase the efficiency of the diagnosis, the method combines two basic techniques of diagnosis: diagnosers and simulation-based techniques. We propose the construction of diagnosers based on <b>local</b> behaviors to <b>compute</b> <b>local</b> diagnoses. Then, we propose a coordination of local diagnoses based on a strategy which minimizes the computation for the coordination...|$|R
40|$|In this {{communication}} we introduce {{an efficient}} implementation of adaptive biasing that greatly improves {{the speed of}} free energy computation in molecular dynamics simulations. We investigated the use of accelerated simulations to inform on compound design using a recently reported and clinically relevant inhibitor of the chromatin regulator BRD 4. Benchmarking on our <b>local</b> <b>compute</b> cluster, our implementation achieves up to 2. 5 times more force calls per day than plumed 2. Results of five 1 μsecond-long simulations are presented, which reveal a conformational switch in the BRD 4 inhibitor between a binding competent and incompetent state. Stabilization of the switch led to a - 3 kcal/mol improvement of absolute binding free energy. These studies suggest an unexplored ligand design principle and offer new actionable hypotheses for medicinal chemistry efforts against this druggable epigenetic target class...|$|R
40|$|We {{introduce}} a new method to approximate algebraic space curves. The algorithm combines a subdivision technique with local approximation of piecewise regular algebraic curve segments. The <b>local</b> technique <b>computes</b> pairs of polynomials with modified Taylor expansions and generates approximating circular arcs. We analyze {{the connection between the}} generated approximating arcs and the osculating circles of the algebraic curve...|$|R
30|$|Finally, {{for medical}} personnel, patients, or other {{authorized}} units, the hash {{value of the}} plaintext EHR obtained by the <b>local</b> <b>computing</b> decryption gets the message authentication code (MAC)= H(E)to determine if the convergence key kE is equal: if equal, then the medical personnel, patients or other authorized units receive the correct and complete EHR document, to certify that an EHR received by a medical person, patient, or other authorized entity is a malformed EHR that has been maliciously altered.|$|E
30|$|The MAX IV Laboratory is {{currently}} the synchrotron X-ray source with the beam of highest brilliance. Four imaging beamlines are in construction or in the project phase. Their common characteristic will be the high acquisition rates of phase-enhanced images. This high data flow will be managed at the <b>local</b> <b>computing</b> cluster jointly with the Swedish National Computing Infrastructure. A common image reconstruction and analysis platform is being designed to offer reliable quantification of the multidimensional images acquired at all the imaging beamlines at MAX IV.|$|E
40|$|The BIOMOL {{grant was}} for 'Local System Support for PDB Biological Unit Search and Display' to augment Rasmol's [Bernstein 2000] [Sayle, Milner-White 1995] {{existing}} macromolecular display functions with new capabilities by {{taking advantage of}} recent increases in <b>local</b> <b>computing</b> power in order to move functionality that is now scattered among various local and remote systems into one local package. Work included new algorithms for molecular surface display, an extended format for Protein Data Bank Entries, work on issues relating to the integration of multiple diffraction images formats...|$|E
5000|$|Themes it {{has also}} focused on include IT-for-public health, Free/Libre and Open Source Software (FLOSS), ICT and human rights, {{emerging}} ICT technologies, community radio concerns, ICT for poverty alleviation, ICT for mass education, the knowledge society, <b>local</b> language <b>computing</b> initiatives, the [...] "digital divide" [...] generally, ongoing conferences and seminars in the region, and e-governance issues.|$|R
40|$|We have {{developed}} and tested a prototype VO-Role management system using the Community Authorization Service (CAS) from the Globus project. CAS {{allows for a}} flexible definition of resources. In this prototype we define a role as a resource within the CAS database and assign individuals in the VO access to that resource to indicate their ability to assert the role. The access of an individual to this VO-Role resource is then an annotation of the user's CAS proxy certificate. This annotation is then used by the local resource managers to authorize access to <b>local</b> <b>compute</b> and storage resources at a granularity {{that is based on}} neither VOs nor individuals. We report here on the configuration details for the CAS database and the Globus Gatekeeper and on how this general approch could be formalized and extended to meet the clear needs of LHC experiments using the Grid. 1...|$|R
40|$|Analytical {{technique}} combines ultrasonic scanning {{measurements of}} local velocity of sound in specimen of material with x-ray computed tomographic measurements of local mass density to <b>compute</b> <b>local</b> stiffness of material. Stiffnesses at various locations in specimen then used in finite-element mathematical model of elastic behavior of specimen to <b>compute</b> <b>local</b> stresses, local strains, and overall deformations. Technique enhances value of quantitative nondestructive x-ray and ultrasonic measurements. Especially useful in characterization of carbon/carbon composites and other advanced materials not homogeneous {{and for which}} customary simplifying assumption of constant density and/or constant stiffness not valid...|$|R
