1|15|Public
40|$|DE 1004019950 A UPAB: 20051216 NOVELTY - The {{conveyor}} for transporting semiconductor wafers {{through high}} temperature zones comprises a chain made from ceramic links (5). These are directly connected, a cylindrical head (6) {{on one end}} of a <b>link</b> <b>fitting</b> into a groove (7) on the opposite end of an adjacent link. USE - Transporting semiconductor wafers through high temperature zones. ADVANTAGE - Contamination of the wafers is reduced...|$|E
40|$|Abstract—The electron-proton {{collider}} HERA {{is being}} upgraded to provide higher luminosity {{from the end}} of the year 2001. In order to enhance the selectivity on exclusive processes a Fast Track Trigger (FTT) with high momentum resolution is being built for the H 1 Collaboration. The FTT will perform a 3 -dimensional reconstruction of curved tracks in a magnetic field of 1. 1 Tesla down to 100 MeV in transverse momentum. It is able to reconstruct up to 48 tracks within 23 µs in a high track multiplicity environment. The FTT consists of two hardware levels L 1, L 2 and a third software level. Analog signals of 450 wires are digitized at the first level stage followed by a quick lookup of valid track segment patterns. For the main processing tasks at the second level such as <b>linking,</b> <b>fitting</b> and deciding, a multifunctional processing board has been developed by the ETH Zürich in collaboration with Supercomputing Systems (Zürich). It integrates a high-density FPGA (Altera APEX 20 K 600 E) and four floating point DSPs (Texas Instruments TMS 320 C 6701). This presentation will mainly concentrate on second trigger level hardware aspects and on the implementation of the algorithms used for <b>linking</b> and <b>fitting.</b> Emphasis is especially put on the integrated CAM (content addressable memory) functionality of the FPGA, which is ideally suited for implementing fast search tasks like track segment linking...|$|R
40|$|The electron-proton {{collider}} HERA {{is being}} upgraded to provide higher luminosity {{from the end}} of the year 2001. In order to enhance the selectivity on exclusive processes a Fast Track Trigger (FTT) with high momentum resolution is being built for the H 1 Collaboration. The FTT will perform a 3 -dimensional reconstruction of curved tracks in a magnetic field of 1. 1 Tesla down to 100 MeV in transverse momentum. It is able to reconstruct up to 48 tracks within 23 mus in a high track multiplicity environment. The FTT consists of two hardware levels L 1, L 2 and a third software level. Analog signals of 450 wires are digitized at the first level stage followed by a quick lookup of valid track segment patterns. For the main processing tasks at the second level such as <b>linking,</b> <b>fitting</b> and deciding, a multifunctional processing board has been developed by the ETH Zurich in collaboration with Supercomputing Systems (Zurich). It integrates a high-density FPGA (Altera APEX 20 K 600 E) and four floating point DSPs (Texas Instruments TMS 320 C 6701). This presentation will mainly concentrate on second trigger level hardware aspects and on the implementation of the algorithms used for <b>linking</b> and <b>fitting.</b> Emphasis is especially put on the integrated CAM (content addressable memory) functionality of the FPGA, which is ideally suited for implementing fast search tasks like track segment linking. Comment: 6 pages, 4 figures, submitted to TN...|$|R
40|$|Abstract — The electron-proton {{collider}} HERA {{is being}} upgraded to provide higher luminosity {{from the end}} of the year 2001. In order to enhance the selectivity on exclusive processes a Fast Track Trigger (FTT) with high momentum resolution is being built for the H 1 Collaboration. The FTT will perform a 3 -dimensional reconstruction of curved tracks in a magnetic field of 1. 1 Tesla down to 100 MeV in transverse momentum. It is able to reconstruct up to 48 tracks within 23 µs in a high track multiplicity environment. The FTT consists of two hardware levels L 1, L 2 and a third software level. Analog signals of 450 wires are digitized at the first level stage followed by a quick lookup of valid track segment patterns. For the main processing tasks at the second level such as <b>linking,</b> <b>fitting</b> and deciding, a multifunctional processing board has been developed by the ETH Zürich in collaboration with Supercomputing Systems (Zürich). It integrates a high-density FPGA (Altera APEX 20 K 600 E) and four floatin...|$|R
40|$|We {{develop a}} fully Bayesian method {{to analyze the}} single index models, {{including}} variable selection, the index vector estimation and the <b>link</b> function <b>fitting</b> with free-knot splines. The proposed method is implemented {{by means of the}} reversible jump Markov chain Monte Carlo technique. We treat the marginal posterior of all the unknown quantities except the spline coefficients and error variance as the target distribution to reduce the dimension of the parameters and to obtain a rapid algorithm. We design a new random walk Metropolis sampler to sample from the conditional posterior distribution of the index vector. The proposed method is verified by simulation studies, and is applied to analyze two real data sets. ...|$|R
40|$|Natural Science Foundation of Fujian Province of China [S 0750018]We {{develop a}} fully Bayesian method {{to analyze the}} single index models, {{including}} variable selection, the index vector estimation and the <b>link</b> function <b>fitting</b> with free-knot splines. The proposed method is implemented {{by means of the}} reversible jump Markov chain Monte Carlo technique. We treat the marginal posterior of all the unknown quantities except the spline coefficients and error variance as the target distribution to reduce the dimension of the parameters and to obtain a rapid algorithm. We design a new random walk Metropolis sampler to sample from the conditional posterior distribution of the index vector. The proposed method is verified by simulation studies, and is applied to analyze two real data sets. (C) 2009 Elsevier B. V. All rights reserved...|$|R
40|$|Abstract. Knots {{and links}} in 3 -manifolds are studied by {{applying}} intersection invariants to singular concordances. The resulting link invariants generalize the Arf invariant, the mod 2 Sato-Levine invariants, and Milnor’s triple <b>linking</b> numbers. Besides <b>fitting</b> into a {{general theory of}} Whitney towers, these invariants provide obstructions {{to the existence of}} a singular concordance which can be homotoped to an embedding after stabilization by connected sums with S 2 × S 2. Results include classifications of stably slice links in orientable 3 -manifolds, stable knot concordance in products of an orientable surface with the circle, and stable link concordance for many links of null-homotopic knots in orientable 3 -manifolds. 1...|$|R
40|$|Droop schemes {{are usually}} {{applied to the}} control of {{distributed}} generators (DGs) in microgrids (MGs) to realize proportional power sharing. The objective might, however, not suit MGs well for economic reasons. Addressing that issue, this paper proposes an alternative droop scheme for reducing the total active generation costs (TAGC). Optimal economic operation, DGs’ capacity limitations and system stability are fully considered basing on DGs’ generation costs. The proposed scheme utilizes the frequency as a carrier to realize the decentralized economic operation of MGs without communication <b>links.</b> Moreover, a <b>fitting</b> method is applied to balance DGs’ synchronous operation and economy. The effectiveness and performance of the proposed scheme are verified through simulations and experiments...|$|R
40|$|Knots {{and links}} in 3 –manifolds are studied by {{applying}} intersection invariants to singular concordances. The resulting link invariants generalize the Arf invariant, the mod 2 Sato–Levine invariants and Milnor’s triple <b>linking</b> numbers. Besides <b>fitting</b> into a {{general theory of}} Whitney towers, these invariants provide obstructions {{to the existence of}} a singular concordance which can be homotoped to an embedding after stabilization by connected sums with S 2 S 2. Results include classifications of stably slice links in orientable 3 –manifolds, stable knot concordance in products of an orientable surface with the circle and stable link concordance for many links of null-homotopic knots in orientable 3 –manifolds. 57 M 27; 57 M 99...|$|R
40|$|We {{propose a}} {{generalized}} stochastic block model {{to explore the}} mesoscopic structures in signed networks by grouping vertices that exhibit similar positive and negative connection profiles into the same cluster. In this model, the group memberships are viewed as hidden or unobserved quantities, and the connection patterns between groups are explicitly characterized by two block matrices, one for positive links {{and the other for}} negative <b>links.</b> By <b>fitting</b> the model to the observed network, we can not only extract various structural patterns existing in the network without prior knowledge, but also recognize what specific structures we obtained. Furthermore, the model parameters provide vital clues about the probabilities that each vertex belongs to different groups and the centrality of each vertex in its corresponding group. This information sheds light on the discovery of the networks' overlapping structures and the identification of two types of important vertices, which serve as the cores of each group and the bridges between different groups, respectively. Experiments on a series of synthetic and real-life networks show the effectiveness as well as the superiority of our model. Comment: 12 pages, 8 figures, 2 tables. arXiv admin note: text overlap with arXiv: 1110. 1976 by other author...|$|R
40|$|Objectives: To {{investigate}} {{the validity of}} a common depression metric in independent samples. Study Design and Setting: We applied a common metrics approach based on item-response theory for measuring depression to four German-speaking samples that completed the Patient Health Questionnaire (PHQ- 9). We compared the PHQ item parameters reported for this common metric to reestimated item parameters that derived from fitting a generalized partial credit model solely to the PHQ- 9 items. We calibrated the new model on the same scale as the common metric using two approaches (estimation with shifted prior and StockingeLord <b>linking).</b> By <b>fitting</b> a mixed-effects model and using BlandeAltman plots, we investigated the agreement between latent depression scores resulting from the different estimation models. Results: We found different item parameters across samples and estimation methods. Although differences in latent depression scores between different estimation methods were statistically significant, these were clinically irrelevant. Conclusion: Our findings provide evidence {{that it is possible}} to estimate latent depression scores by using the item parameters from a common metric instead of reestimating and linking a model. The use of common metric parameters is simple, for example, using a Web application ([URL] and offers a long-term perspective to improve the comparability of patient-reported outcome measures...|$|R
40|$|How can complex {{relationship}} structures be managed? When collaborating in networks, {{the diversity of}} stakeholder relationships is increasing. Most times, this leads to interoperability issues {{that need to be}} addressed. In this chapter, the authors show how Anything Relationship Management (xRM) can increase interoperability in many-to-many (n:n) relationships. Building upon relationship management theory, they firstly categorize different types of relationships and <b>link</b> them with <b>fitting</b> IT solutions. The authors then give a brief introduction to the xRM concept. Following and more specifically, they present the EU research project GloNet 1 and propose three technical xRM approaches (collaboration spaces, integration of external services, and synchronization framework) in order to improve social network interoperability, services interoperability, and data interoperability. The chapter closes with a conclusion, an example of application, and a research outlook...|$|R
40|$|ITC/USA 2012 Conference Proceedings / The Forty-Eighth Annual International Telemetering Conference and Technical Exhibition / October 22 - 25, 2012 / Town and Country Resort & Convention Center, San Diego, CaliforniaThe next {{generation}} Landsat satellite, Landsat 8 (L 8), {{also known as}} the Landsat Data Continuity Mission (LDCM), uses a highly spectrally efficient modulation and data formatting approach to provide large amounts of downlink (D/L) bandwidth in a limited X-Band spectrum allocation. In addition to purely data throughput and bandwidth considerations, {{there were a number of}} additional constraints based on operational considerations for prevention of interference with the NASA Deep-Space Network (DSN) band just above the L 8 D/L band, minimization of jitter contributions to prevent impacts to instrument performance, and the need to provide an interface to the Landsat International Cooperator (IC) community. A series of trade studies were conducted to consider either X- or Ka-Band, modulation type, and antenna coverage type, prior to the release of the request for proposal (RFP) for the spacecraft. Through use of the spectrally efficient rate- 7 / 8 Low-Density Parity-Check error-correction coding and novel filtering, an XBand frequency plan was developed that balances all the constraints and considerations, while providing world-class <b>link</b> performance, <b>fitting</b> 384 Mbits/sec of data into the 375 MHz X-Band allocation with bit-error rates better than 10 ⁻¹² using an earth-coverage antenna...|$|R
40|$|It is {{commonly}} stated {{that we have}} entered the era of precision cosmology in which {{a number of important}} observations have reached a degree of precision, and a level of agreement with theory, that is comparable with many Earth-based physics experiments. One of the consequences is the need to examine at what point our usual, well-worn assumption of homogeneity associated to the use of perturbation theory begins to compromise the accuracy of our models. It is now a widely accepted fact that the effect of the inhomogeneities observed in the Universe cannot be ignored when one wants to construct an accurate cosmological model. Well-established physics can explain several of the observed phenomena without introducing highly speculative elements, like dark matter, dark energy, exponential expansion at densities never attained in any experiment (i. e. inflation), and the like. Two main classes of methods are currently used to deal with these issues. Averaging, sometimes <b>linked</b> to <b>fitting</b> procedures a la Stoegger and Ellis, provide us with one promising way of solving the problem. Another approach is the use of exact inhomogeneous solutions of General Relativity. This will be developed here. Comment: 13 pages, 6 figures, to be published in the proceedings of the Invisible Universe International Conference, held in Paris, 29 June - 3 July 2009, eds. J. -M. Alimi, A. Fuzfa and P. -S. Corasanit...|$|R
40|$|A study {{concerning}} the deep {{structure of the}} Almazán basin {{has been carried out}} by using seismic, gravity and geological data. A new interpretation of previous seismic reflection lines has provided an isopach map for the top Albian unit showing an overall simple flat-bottomed syncline geometry with a 4500 m thickness depocenter placed near the northeastern boundary of the basin. A gravity survey provided a gravity map of the basin and surrounding areas, depicting relative gravity highs and lows linked to sedimentary infill variations and lithological changes in the basement. A map for the theoretical gravity due to the sedimentary infill has been done by integrating the data derived fram the isopach map with the function which describes the density increasing with depth. Substracting this map to the observed gravity map, a new one depicting the gravity response for the basement has been obtained. Spectral analysis of this new map shows the occurrence of a regional source located at 11 km. Integrating all the aforementioned data, three 2 + 1 / 2 D gravity models have been obtained. These models display an acceptable <b>fitting</b> <b>linking</b> surface geological structure with bottom basin geometry, as well as different basement bodies and the geometry ofthe boundary between the basement and middle crus...|$|R
40|$|Discrete {{survival}} data are routinely encountered in many fields of study. There are two common types of discrete {{survival data}}. The first type is derived discrete, which is originally continuous but recorded in a discrete version by grouping or rounding into a discrete time. The second type is intrinsically discrete. The dissertation research {{is motivated by}} two types of discrete survival data in clinical trials. We develop a class of proportional exponentiated link transformed hazards (ELTH) models and a class of proportional exponentiated link transformed survival (ELTS) models. We examine the role of <b>links</b> in <b>fitting</b> discrete survival data and estimating regression coefficients. We also characterize the conditions for improper survival functions and the conditions for existence of the maximum likelihood estimates under the proposed ELTH models. An extensive simulation study is conducted to examine the empirical performance of the parameter estimates under the Cox proportional hazards model by treating discrete survival times as continuous survival times, and the model comparison criteria, AIC and BIC, in determining links and baseline hazards. A SEER breast cancer dataset is analyzed in details to further demonstrate the proposed methodology. Previous {{research has shown that}} outcome misclassification can bias estimation of the survival function under standard survival methods. We develop methods to accurately estimate the survival function when the diagnostic tool used to measure the outcome of disease is not perfectly sensitive and specific. Since the diagnostic tool used to measure disease outcome is not the gold standard, the true outcomes cannot be observed. Our method uses the negative predictive value (NPV) and the positive predictive values (PPV) to construct a bridge between the mismeasured outcomes and the true outcomes. We formulate an exact relationship between the true and the observed survival functions as a formulation of time-varying NPV and PPV. We specify models for the NPV and PPV that depend only on parameters that can be easily estimated from a fraction of the observed data. Furthermore, we extend and conduct an extensive study to accurately estimate the latent survival function {{based on the assumption that}} the underlying disease process follow a stochastic process. We further examine the performance of our method by applying it to the VIRAHEP-C data...|$|R

