0|173|Public
30|$|The {{results in}} Table  3 quantitatively {{illustrate}} {{that the proposed}} method has a good <b>fitting</b> <b>precision.</b> Taking the pressure at node 5 as an example, the value of ARMS reaches 0.062 %, which indicates the <b>fitting</b> <b>precision</b> is very high. Comparing the results of case 1 and case 2, {{it can also be}} concluded that as the random factors have a wider range, the <b>fitting</b> <b>precision</b> will degenerate as reflected by the values of ARMS.|$|R
40|$|This {{thesis is}} {{concerned}} with the Beagle 2 X-ray spectrometer (XRS) for Mars. The scientific goals of the XRS were to perform (i) geochemical analyses of Martian rocks and soils and (ii) in situ 40 K > 40 Ar dating of rocks. The major aims of this study were the development of the XRS to achieve its scientific goals and to inform the design of future versions of the instrument. The XRS is described and compared with the previous X-ray Spectrometers that have been successfully deployed on Mars. A characterisation of the XRS investigated the fundamental behaviour of the instrument in terms of its spectral features (gain, resolution and artefacts), gain variation with temperature of its components, deadtime and quantum efficiency. The XRS was calibrated to convert elemental intensities from its analyses into concentrations. The concentrations of several major and trace elements of interest in reference materials were linearly correlated with the certified concentration. The analytical performance of the XRS was evaluated in comparison with a terrestrial portable X-ray spectrometer (PXRF) and a wavelength dispersive X-ray spectrometer (WD XRF). The performance was characterised in terms of accuracy, detection <b>limit</b> and <b>fitting</b> <b>precision.</b> This study showed the importance of energy resolution to the analytical performance of the XRS. The operational performance of the XRS was evaluated. The geochemical composition of several basalts analysed by the XRS agreed with complementary analyses by the PXRF and WD XRF. The 40 K > 40 Ar radiometric ages for two basalts were determined using the K content in the basalts in conjunction with their 40 Ar isotope content (analysed by a laboratory version of the Beagle 2 Gas Analysis Package). The 40 K 40 Ar ages were found to differ to the Ar-Ar ages because of various effects associated with inhomogeneity of the K content in the rock and radiogenic 40 Ar loss...|$|R
30|$|By {{applying}} the method illustrated above, the <b>fitting</b> <b>precision</b> {{of the proposed}} method can be well improved.|$|R
30|$|As {{mentioned}} above, {{the comprehensive}} compensation model had higher <b>fitting</b> <b>precision.</b> Then {{in order to}} verify the applicability of this compensation model, this comprehensive compensation model was used to fit other thermal experimental data.|$|R
40|$|Abstract. An {{approach}} of regions segmentation algorithm in triangle meshes based on threshold {{angle of the}} normal vectors is proposed. In this paper, a set of threshold angles is used to identify features and segment different surfaces, and then the non-feature regions based on least squares method are fitted and the corresponding <b>fitting</b> <b>precision</b> are obtained. Finally, the best threshold angle of each region is determined according to the comparison the above <b>fitting</b> <b>precisions</b> and each non-feature region points are obtained exactly. This method can improve the precision of feature extraction and region segmentation in some hybrid blend region. Examples are given to prove the effectiveness and the processing flexibility and stability of the proposed approach...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with the conditional bias and variance of local quadratic regression to the multivariate predictor variables. Data sharpening methods of nonparametric regression were first proposed by Choi, Hall, Roussion. Recently, a data sharpening estimator of local linear regression was discussed by Naito and Yoshizaki. In this paper, to improve mainly the <b>fitting</b> <b>precision,</b> we extend their results on the asymptotic bias and variance. Using the data sharpening estimator of multivariate local quadratic regression, we are able to derive higher <b>fitting</b> <b>precision.</b> In particular, our approach is simple to implement, since it has an explicit form, and is convenient when analyzing the asymptotic conditional bias and variance of the estimator at the interior and boundary points of the support of the density function...|$|R
40|$|There are a {{large number}} of {{engineering}} optimization problems in real world, whose input-output relationships are vague and indistinct. Here, they are called black box function optimization problem (BBFOP). Then, inspired by the mechanism of neuroendocrine system regulating immune system, BP neural network modified immune optimization algorithm (NN-MIA) is proposed. NN-MIA consists of two phases: the first phase is training BP neural network with expected precision to confirm input-output relationship and the other phase is immune optimization phase, whose aim is to search global optima. BP neural network fitting without expected <b>fitting</b> <b>precision</b> could be replaced with polynomial fitting or other fitting methods within expected <b>fitting</b> <b>precision.</b> Experimental simulation confirms global optimization capability of MIA and the practical application of BBFOP optimization method...|$|R
40|$|Abstract. Make {{natural forest}} birch at Da Qingshan nature {{reserves}} in Inner Mongolia as the research object. The data {{is from the}} National Second-Class investigation data in 2006 by Inner Mongolia survey and design institute of forestry in 2006. Take 8 forest centre as study areas. All these datas would be sifted, and chosen the datas which the varieties of trees is white birch {{and the formation of}} the tree species is pure forest classes. The total of data is 4785. Use of Matlab software log-the sigmoid type function (logsig) and linear function (purelin) for the role of neurons. Based on the function of the concept of stand growth model, we choose age requirement (A), status level (N) and crown density (S) as input variables and the forest accumulation per hectare (M) as output variables to build and ttrain the stand growth BP artificial neural network model. And test the model <b>fitting</b> <b>precision</b> and inspection accuracy, the model <b>fitting</b> <b>precision</b> is 99. 93 %, inspection accuracy is 97. 79 %, these show that neural network modeling has better <b>fitting</b> <b>precision</b> and adaptability for the stand growth, and has good prediction ability. Foreword Stand growth has characteristics that process is complexity, is controlled by multiple factors and is complex to describe the structure and the relationship between characteristic factor and the chang...|$|R
40|$|Abstract: The {{paper is}} {{committed}} to overcome the influence of gross error on the small quantity data of forest fire grey modeling. According to the quantity of the modeling data, Grey judgment of gross error and robust estimation theory is used separately for finding the gross error exit whether or not from the modeling data. And robust estimation theory and LIR algorithm {{can be used to}} process the gross error. From the examples, A quarter of <b>fitting</b> <b>precision</b> of robust estimation is less than 1 %, and 75 % is 1 ～ 5 %; and half of <b>fitting</b> <b>precision</b> of LIR algorithm is less than 1 %, and half is 1 ～ 5 %. That is to say LIR algorithm provides a rapid, simple and practical way to build model of data which contains gross error or which contain missing data. 1...|$|R
30|$|The {{correlation}} coefficients of the nonlinear fitting formula {{of the three}} welded time the pretreated experimental groups were 94.77, 93.93, and 94.52 %, respectively, which indicated that the <b>fitting</b> <b>precision</b> of the nonlinear fitting surface was high. The regression relationship among temperature and depth, time is also {{in accordance with the}} general formula (4).|$|R
40|$|Keywords:GPS height conversion; terrain correction; {{singular}} integra; {{fitting accuracy}} Abstract. In this paper, {{through the surface}} of polynomial <b>fitting</b> <b>precision</b> analysis, obtains <b>fitting</b> after converting <b>precision</b> for normal height is mainly affected by {{the rate of change}} vertical deflection and higher order terms—also influenced by local terrain correction. The undulating topography is deduced and the calculation formula of cause height anomaly, this method is applicable to the complex terrain of area, will dividend increase formula one of the smaller constant, the calculating process of the intermediate results are returning to normal range, effectively avoid the central region of the singular integral. In the terrain change large region, attend to the influence of terrain correction, the surface fitting method for elevation conversion, can effective improve GPS elevation <b>fitting</b> the <b>precision</b> of the conversion...|$|R
40|$|Key words: Remove-restore; {{least square}} collocation; {{covariance}} function; GPS elevation fitting; fitting accuracy Abstract: The common method to determine Quasi-Geoids is GPS leveling however the Quasi-Geoid {{of this method}} determined {{is a kind of}} trend surface which not take the physical property of geoid into consideration, and the fitting method is surface fitting which only consider the surveying error, lead to inaccurate fitting result. In allusion to these problems, Remove-restore method is used to remove the long wave information of earth gravity field model to get more smooth residual gravity height anomaly, then compared the influence of different covariance function to the fitting result of least square collocation which take surveying error and model error into account. The results show that Gaussian and resemble Gaussian function can achieve higher <b>fitting</b> <b>precision</b> to the large area with height anomaly value changes significance; the Remove-restore method can effectively improve the <b>fitting</b> <b>precision</b> to least square collocation method which depend on the covariance value of each points...|$|R
40|$|The Beagle 2 X-ray Spectrometer (B 2 XRS) {{instrument}} {{was part of}} the Beagle 2 Mars lander payload and intended to perform in situ geochemical analyses of geological materials on Mars. The analytical performance of a spare version of the B 2 XRS was compared with (1) a portable X-ray fluorescence (PXRF) spectrometer designed to perform terrestrial fieldwork and (2) a laboratory-based wavelength-dispersive (WD-XRF) system used to produce high quality geochemical data. The criteria used to assess the performance were based on <b>fitting</b> <b>precision,</b> accuracy and detection limit, determined from the analysis of international geochemical reference materials. The <b>fitting</b> <b>precision</b> of the B 2 XRS and PXRF was found to be in agreement with the Horwitz function (a benchmark relating the analysed concentration of an analyte to its uncertainty) over 4 orders of magnitude of concentration range from 10 - 1 to 10 - 5 g/g. The PXRF generally had a better <b>fitting</b> <b>precision</b> than the B 2 XRS because of its better resolution. In order of improving accuracy, the spectrometers generally are ranked B 2 XRS, PXRF and WD-XRF for various major and trace elements. A limiting factor in the accuracy of the B 2 XRS was the application of the algorithm used for its quantitative analysis. The detection limits for the spectrometers ranked in the same order as the accuracy as a result of improving signal-to-noise ratio (SNR) of elemental peaks, which is a direct consequence of improving resolution between these spectrometers. Overall, the B 2 XRS was found to have a favourable analytical performance compared to the benchmark spectrometers, despite having met considerable design constraints and qualification tests as a planetary instrument...|$|R
30|$|The {{comprehensive}} compensating {{model of}} the thermal error of the wear-depth detecting system is built with MLR and time series analysis. The fitting curve of the comprehensive compensation model is more similar to the real detecting curve of the thermal errors, and the maximum residual of the two curves is only 1  μm. Finally, according to the experimental verification, the comprehensive compensation model has higher <b>fitting</b> <b>precision</b> and better applicability.|$|R
30|$|Finally, to {{investigate}} the influence of sampling frequency on the fit parameters and <b>fitting</b> <b>precision</b> (parameter standard deviations), IF sampling intervals were prolonged from the experimental 1  s to 30  s and 60  s, respectively, by deleting the data between these time points from both the experimental and above simulated noise-containing IFs. Kinetic modeling was performed with the identical simulated noise-containing TACs as used for the complete IF datasets.|$|R
40|$|Abstract. With market-oriented {{opening in}} the China's civil {{aviation}} industry and the rapid development of China's economy, China's civil aviation transportation fuel consumption has grown significantly in nearly past three decades. Therefore, it’s a very important strategic significance of the prediction of China's civil aviation transportation fuel consumption. In this paper, by using gray system and neural network approach, combined with China's civil aviation industry 1980 - 2010 total traffic volume of the data, we establish gray system GM (1, 1) model and BP neural network model for civil aviation transport volume. Training and simulation of the back propagation neutral network model and the gray system GM(1, 1) are used by MATLAB. BP neural network modeling takes into account three factors: the number of aircraft aviation industry, flight hours and total turnover. The <b>fitting</b> <b>precision</b> of the gray system GM(1, 1) model is 64. 2 % while the <b>fitting</b> <b>precision</b> of the back propagation neutral network model is 90. 16 %. Thus, the back propagation neutral network model is better for estimating civil aviation fuel consumption...|$|R
30|$|As {{shown in}} Figure  17 and Figure  18, the {{comprehensive}} compensation model curve {{is similar to}} the detecting thermal error curve, and the maximum residual error is about 9  μm. The maximum thermal error of the wear-depth detecting system is 0.02  mm, and the comprehensive compensation model could offset the thermal errors about 0.01  mm. Depending on the above verification, the comprehensive compensation model, built through MLR and time series analysis method, had the higher <b>fitting</b> <b>precision</b> and better applicability.|$|R
40|$|Key words：ER algorithm; robust estimation; gray {{modeling}} Abstract: To {{improve the}} precision of gray modeling in forest fire and {{solve the problem of}} small date modeling, ER algorithm is proposed. Based on the senior introduced the robust estimation to gray modeling, this method interpolate the modeling date again. The method can achieve small date (3 dates) modeling. This research compared with three calculation methods: least squares method, least squares interpolation method and ER algorithm. According to the <b>fitting</b> <b>precision,</b> least squares method is 10. 21 %, least squares interpolation method is 1. 08 % and ER algorithm is 0. 00 %. That can be obtained by calculating ER algorithm has a good fitting effect. 1. Preface. Usually people use the least squares method in gray modeling] 3][2][1 [. But this method has balanced the error, if the modeling date set contains gross error or no observed date is available, the <b>fitting</b> <b>precision</b> of the least squares method will be questioned] 4 [. Therefore, the thesis introduces robust estimation into gray modeling. Feng Li, etc. introduced robust estimation into the paper “Robust grey model RGM (1, 1) and application of it’s to forecast ” and achieved better forecasting precision than the original gray modeling. Selecting the minimum onetime norm(...|$|R
40|$|Abstract. Roundness error {{evaluation}} software is developed based on two-dimensional circle fitting with least-squares method based on nonlinear optimization with constraints. The local derivative-free optimization algorithms of NLopt can solve nonlinear constraint problems by combining with augmented Lagrangian algorithm. The <b>fitting</b> <b>precision</b> and convergence time of each algorithm are analyzed by calculating the fitting results with same test data {{to find its}} advantages and disadvantages. It is shown that each algorithm has different behaviors from others on performance and stability. This work provides a good basis for choosing the appropriate algorithm for roundness error evaluation...|$|R
40|$|The proton elastic {{scattering}} data on 4, 6, 8 He and 6, 7, 9, 11 Li nuclei at energies below 160 MeV/nucleon are analyzed using the optical model. The optical potential (OP) is taken microscopically, with few and <b>limited</b> <b>fitting</b> parameters, using the single folding {{model for the}} real part and high-energy approximation (HEA) for the imaginary one. Clear dependencies of the volume integrals on energy and rms radii are obtained from the results. The calculated differential and the reaction cross sections are in good agreement with the available experimental data. In general, this OP with few and <b>limited</b> <b>fitting</b> parameters, which have a systematic behavior with incident energy and matter radii, successfully describes the proton {{elastic scattering}} data with stable and exotic light nuclei at energies up to 160 MeV/nucleon...|$|R
30|$|In {{the above}} ten {{detecting}} points, not every point {{has the same}} influence on the thermal deformation of the wear-depth detecting system. They may have certain correlations among them, in other words, there is coupling among the ten detecting points. If the mathematical model is directly bulit based on all these detecting-point data, {{the complexity of the}} modeling process will be increased while the <b>fitting</b> <b>precision</b> of the model will be reduced. Therefore, {{in order to get the}} optimal temperature detecting points for the compensation model of the thermal errors, the ten detecting points on the spherical plain bearing test bench were selected.|$|R
40|$|Abstract. In {{order to}} predict {{accurately}} the total government revenue in Guizhou, two improved grey models were presented and have higher forecasting accuracy than original grey model, then combined with BP {{neural network model}} and higher precision is achieved; besides, the improved grey models integrated with Markov chain were used to forecast public finance budget revenue in Guizhou and provide obviously better accuracy than grey model. Finally, the Grey-Markov models were combined and then further improve the <b>fitting</b> <b>precision.</b> The {{results show that the}} methods of grey combination models are feasible and effective in forecasting the revenue...|$|R
40|$|Abstract. Ozone (O 3) {{profiles}} {{are derived}} from backscattered radiances in the ultraviolet spectra (290 - 340 nm) measured by the nadir-viewing Global Ozone Monitoring Experiment (GOME), with particular emphasis on improving tropospheric O 3 retrieval using optimal estimation. To optimize the retrieval and improve the <b>fitting</b> <b>precision</b> needed for tropospheric O 3, we perform extensive wavelength and radiometric calibrations and improve forward model inputs. Retrieved O 3 profiles compare well with coincident ozonesonde measurements at Hohenpeibenberg and Lauder, and the integrated total O 3 compares very well with Earth Probe TOMS data. Comparisons with retrievals using Phillips-Tikhonov regularization are also presented. 1...|$|R
40|$|An {{obstacle}} {{encountered in}} applying orthogonal-polynomials fitting {{is how to}} select out the proper fitting expression. By adding a Laplace term to the error expression and introducing the concept of overfitting degree, a regularization and corresponding cross validation scheme is proposed for two-variable polynomials fitting. While the Fortran implementation of above scheme is applied to magnetization data, a satisfactory <b>fitting</b> <b>precision</b> is reached, and overfitting problem can be quantitatively assessed, which therefore offers the quite reliable base for future comprehensive investigations of magnetocaloric and phase-transition properties of magnetic functional materials. Comment: 10 pages, 1 figure, 3 tables; comments are welcom...|$|R
40|$|Abstract. Support Vector Machine Regression is a {{nonlinear}} {{modeling method}} {{with a simple}} structure and shows excellent performance compared with other nonlinear-linear regression methods. Unfortunately, most users always select the SVMR parameters by rule of thumb, so they frequently fail to get the optimal model. This paper propose to use the immune genetic algorithm to adjust the SVMR parameters and use the RMSE of the cross validation as the fitness of IGA. At last, this method was applied to modeling satellite attitude control system to detect the faults of the system. Simulation shows the high <b>fitting</b> <b>precision</b> to the system models which insures the correctness of the fault detection...|$|R
40|$|Abstract—One {{improved}} {{approach of}} grey derivative in the direct GM (1, 1) {{is presented in}} this paper, which raises the modeling precision once again. The new model has been proven strictly to have the property of exponent, coefficient and translation constants superposition. The results of data simulation and model comparison show that the improved model in this paper raises the accuracy of background value, the <b>fitting</b> <b>precision</b> and forecasting precision. Moreover, {{it is not only}} suitable for the low growth sequence, but also suitable for the high growth sequence. What’s more, it is suitable for the nonhomogeneous exponential sequence. Index Terms—GM (1, 1), grey derivative, optimization, exponential model I...|$|R
40|$|Abstract. The {{parameterized}} {{design system}} of disc cams with translating roller followers was studied. The secondary development of AutoCAD {{was chosen as}} the developing tool. According to the design theories of the disc cam, the solution steps of the best offset and base circle radius were proposed in this work. All the solution steps meet the requirements of the motion parameters of the push bar. The method of identifying the reasonable step angle for the given <b>fitting</b> <b>precision</b> was developed. The profile curve of the disc cam can be drawn automatically and saved as the ‘dwg’ format file. Finally, an example was used to verify the feasibility of the system...|$|R
40|$|Abstract. The time-course of lipid {{oxidation}} {{determined by}} peroxide value and acid value of hazel nut packed at different temperatures were evaluated, and the separate kinetic models of peroxide value and acid value {{with respect to}} storage time at different storage temperatures were established based on Arrhenius equation, so as to predict and control the quality of hazel nut during storage. The {{results show that the}} peroxide value and acid value of hazelnut increased with extension of storage time, besides, it will increase rapidly with the storage temperature increased. The Acid value and peroxide value have a high <b>fitting</b> <b>precision</b> with the chemical reaction model and the Arrhenius equation...|$|R
40|$|Taking {{the total}} volume of import and export trade of Ningbo port in 2004 - 2011 as the {{original}} data, a mathematical model of gray forecasting is established for {{the total volume}} of import and export trade of port. The paper predicts the total volume of import and export trade of Ningbo port in 2012 - 2015 during the 12 th Five-Year Plan {{in accordance with the}} posterior variance testing. The analysis results indicate that the <b>fitting</b> <b>precision</b> of forecasting model of gray system is satisfactory, offering a good idea for the development and reform of the Ningbo port and the port construction through the sustainable  development overall in the future...|$|R
40|$|This paper {{presents}} a novel methodology-based discrete wavelet transform (DWT) {{and the choice}} of the optimal wavelet pairs to adaptively process tunable diode laser absorption spectroscopy (TDLAS) spectra for quantitative analysis, such as molecular spectroscopy and trace gas detection. The proposed methodology aims to construct an optimal calibration model for a TDLAS spectrum, regardless of its background structural characteristics, thus facilitating the application of TDLAS as a powerful tool for analytical chemistry. The performance of the proposed method is verified using analysis of both synthetic and observed signals, characterized with different noise levels and baseline drift. In terms of <b>fitting</b> <b>precision</b> and signal-to-noise ratio, both have been improved significantly using the proposed method...|$|R
40|$|Abstract Algebraic {{curve fitting}} {{based on the}} {{algebraic}} dis-tance is simple, but it has the disadvantage of inclining to a trivial solution. Researchers therefore introduce some con-straints into the objective function {{in order to avoid}} the triv-ial solution. However, this often causes additional branches. Fitting based on geometric distance can avoid additional branches, but it does not offer sufficient <b>fitting</b> <b>precision.</b> In this paper we present a novel algebraic B-spline curve fitting method which combines both geometric distance and alge-braic distance. The method first generates an initial curve by a distance field fitting that takes geometric distance as the objective function. Then local topology-preserving calibra-tions based on algebraic distance are performed so that each calibration does not produce any additional branches. In this way, we obtain an additional branch free <b>fitting</b> result whose <b>precision</b> is close to or even better than that produced by purely algebraic distance based methods. The adopted pre-cision criterion is the geometric distance error rather than the algebraic one. In addition, we find a calibration fatigue phenomenon about calibrating strategy and propose a hybrid mode to solve it...|$|R
30|$|In {{order to}} further improve the <b>fitting</b> <b>precision</b> of the thermal error {{model of the}} wear-depth {{detecting}} system, {{on the basis of}} multi-element fitting, time series analysis method was introduced for the residual errors δ (t), which improved the curve similarity between the multi-element fitting thermal errors and the detected thermal errors. The time series analysis, processing the dynamic data, is a parameterized analytical method, and it could fit the applicable time series model {{on the basis of the}} random data arranged according to the time sequence. The above model was used to analyze the data system to learn the inner structure and the dynamic property of the random data. Thus the data trend could be predict with the existing data [20].|$|R
40|$|Ozone {{profiles}} {{are derived}} from backscattered radiances in the ultraviolet spectra (290 - 340 nm) measured by the nadirviewing Global Ozone Monitoring Experiment using optimal estimation. Tropospheric O 3 is directly retrieved with the tropopause {{as one of the}} retrieval levels. To optimize the retrieval and improve the <b>fitting</b> <b>precision</b> needed for tropospheric O 3, we perform extensive wavelength and radiometric calibrations and improve forward model inputs. Retrieved O 3 profiles and tropospheric O 3 agree well with coincident ozonesonde measurements, and the integrated total O 3 agrees very well with Earth Probe TOMS and Dobson/Brewer total O 3. The global distribution of tropospheric O 3 clearly shows the influences of biomass burning, convection, and air pollution, and is generally consistent wiht our current understanding. 1...|$|R
40|$|Abstract—A new {{coagulant}} of polymerized-organic-Al-Zn-Fe (POAZF) {{was prepared}} using a galvanized aluminum slag and polyacrylamide (PAM) as raw materials. Sludge reduction of POAZF was probed {{compared to that}} of poly-Al-Zn-Fe (PAZF) and poly-aluminum-chloride (PAC). The results showed that the settling speed of flocs of POAZF was much greater than that both PAZF and PAC. The volume reduction ratio of wet sludge (VRRWS) of POAZF was fitted in polynomial mode to predict VRRWS. The <b>fitting</b> <b>precision</b> of VRRWS achieved by POAZF between the test data and fitting data at both higher dosage and lower or medium dosage within the settling time of 15 min was high, which is significant for sludge production and VRRWS in the actual wastewater treatment processes. Keywords-Fe-Zn-Al; PAM; sludge reduction; prediction. I...|$|R
30|$|Intelligent robots are {{expected}} {{to take an active}} role in the joining job, which comprises as large a part of the machine industry as the machining job. The intelligent robot can perform highly accurate assembly jobs, picking up a workpiece from randomly piled workpieces on a tray, assembling it with <b>fitting</b> <b>precision</b> of 10  μm or less clearance with its force sensors, and high-speed resistant spot arc welding in automotive welding and painting. However, the industrial intelligent robots still have tasks in which they cannot compete with skilled workers, though they have a high level of skills, as has been explained so far. Such as assembling flexible objects like a wire harness, there are several ongoing research and development activities in the world to solve these challenges (Nof 2009).|$|R
30|$|It is {{indicated}} in Table  1 {{that when the}} time period for inter-satellite two-way clock offset measurement is basically symmetric compared with the moment at the inter-satellite distance change rate of 0, the <b>fitting</b> <b>precision</b> measured for clock offset fit polynomial is higher with small fitting error and high clock offset measuring precision, and the minimum error between the calibrated inter-satellite clock offset and supposed clock offset including the simulation error condition is 0.287340679374  ns. The precision for measured clock offset fit polynomial is lower with large fitting error and reduced clock offset measuring precision when the time period for inter-satellite two-way clock offset measurement is asymmetric compared with the moment at the inter-satellite distance change rate of 0. The above {{results show that the}} proposed correction method for inter-satellite clock offset is correct.|$|R
