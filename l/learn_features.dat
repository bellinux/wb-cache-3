176|4662|Public
25|$|These {{can be used}} {{to output}} object {{bounding}} boxes {{in the form of a}} binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can <b>learn</b> <b>features</b> that capture geometric information, in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.|$|E
5000|$|King Stur-Gav Hi Fi Lee Unlimited (1983) Live & <b>Learn,</b> <b>features</b> [...] "Special Request" ...|$|E
5000|$|The affective filter hypothesis. This {{states that}} {{learners}} must be relaxed {{and open to}} learning in order for language to be acquired. Learners who are nervous or distressed may not <b>learn</b> <b>features</b> in the input that more relaxed learners would pick up with little effort.|$|E
40|$|This thesis compares hand-designed <b>features</b> with <b>features</b> <b>learned</b> by <b>feature</b> <b>learning</b> {{methods in}} video classification. The <b>features</b> <b>learned</b> by Principal Component Analysis whitening, Independent {{subspace}} analysis and Sparse Autoencoders {{were tested in}} a standard Bag of Visual Word classification paradigm replacing hand-designed features (e. g. SIFT, HOG, HOF). The classification performance was measured on Human Motion DataBase and YouTube Action Data Set. <b>Learned</b> <b>features</b> showed better performance than the hand-desined features. The combination of hand-designed <b>features</b> and <b>learned</b> <b>features</b> by Multiple Kernel Learning method showed even better performance, including cases when hand-designed <b>features</b> and <b>learned</b> <b>features</b> achieved not so good performance separately...|$|R
50|$|Supervised <b>feature</b> <b>learning</b> is <b>learning</b> <b>features</b> from labeled data. Approaches include.|$|R
40|$|We {{present a}} training/test {{framework}} for audio classification using <b>learned</b> <b>feature</b> representations. Commonly used audio features in audio classification, such as MFCC and chroma, {{have been developed}} based on acoustic knowledge. As an alternative, there is increasing interest in <b>learning</b> <b>features</b> from data using unsupervised learning algorithms. In this work, we apply sparse Restricted Boltzmann Machine to audio data, particularly focusing on <b>learning</b> high-dimensional sparse <b>feature</b> representation. Our evaluation results on two music genre datasets show that the <b>learned</b> <b>feature</b> representations achieve high accuracy. 1...|$|R
5000|$|These {{can be used}} {{to output}} object {{bounding}} boxes {{in the form of a}} binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can <b>learn</b> <b>features</b> that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.|$|E
5000|$|Supervised text {{summarization}} is {{very much}} like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can <b>learn</b> <b>features</b> of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as [...] "in summary" [...] or [...] "not in summary". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so {{it would be difficult to}} assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.|$|E
30|$|The main {{contribution}} {{of this paper}} relates {{to the fact that}} while existing works rely on features that are defined and crafted to fit certain characteristics of sounds, deep learning is powerful enough to <b>learn</b> <b>features</b> by itself when the appropriate architectures are in place. This work is not novel in terms of using deep learning for acoustic event detection as this is already familiar in ASR [11], and we are starting to see it in acoustic event detection as well [12]. What these studies do not do is truly exploit the feature learning ability of deep learning and use custom crafted features downsample and focus on specific properties of certain sounds. Here, we show how, with the appropriate architectures, deep learning models can <b>learn</b> <b>features</b> directly from a naive feature (i.e., the log power spectrogram in this work).|$|E
50|$|Unsupervised <b>feature</b> <b>learning</b> is <b>learning</b> <b>features</b> from unlabeled data. The goal of {{unsupervised}} <b>feature</b> <b>learning</b> {{is often}} to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the <b>feature</b> <b>learning</b> is performed in an unsupervised way, it enables {{a form of}} semisupervised <b>learning</b> where <b>features</b> <b>learned</b> from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following.|$|R
40|$|International audienceThis paper {{addresses}} {{the problem of}} image features selection for pedestrian gender recognition. Hand-crafted features (such as HOG) are compared with <b>learned</b> <b>features</b> which are obtained by training convolutional neural networks. The comparison is performed on the recently created collection of versatile pedestrian datasets which allows us to evaluate the impact of dataset properties {{on the performance of}} features. The study shows that hand-crafted and <b>learned</b> <b>features</b> perform equally well on small-sized homogeneous datasets. However, <b>learned</b> <b>features</b> significantly outperform hand-crafted ones in the case of heterogeneous and unfamiliar (unseen) datasets. Our best model which is based on <b>learned</b> <b>features</b> obtains 79 % average recognition rate on completely unseen datasets. We also show that a relatively small convolutional neural network is able to produce competitive features even with little training data...|$|R
40|$|We {{develop an}} {{approach}} for automatically <b>learning</b> the optimal <b>feature</b> transformation {{for a given}} classification problem. The approach is based on extending the principle of risk minimization (RM), commonly used for learning classifiers, to <b>learning</b> <b>feature</b> transformations that admit classifiers with minimum risk. This allows feature extraction and classification to proceed in a unified framework, both guided by the RM principle. The framework is applied to derive new algorithms for <b>learning</b> <b>feature</b> transformations. Our experiments demonstrate {{the ability of the}} resulting algorithms to <b>learn</b> good <b>features</b> for a variety of classification problems. 1...|$|R
40|$|Models of {{category}} learning often {{assume that}} exemplar features are learned {{in proportion to}} how much they reduce classification error. In contrast, experimental evidence suggests that people continue to <b>learn</b> <b>features</b> even when classification is perfect. We present three experiments that test explanations for how people might <b>learn</b> <b>features</b> {{in the absence of}} error. In Experiment 1, we varied the type of feedback participants received. In Experiment 2, we introduced a secondary task during the feedback phase, and in Experiment 3, we restricted the response window and varied the feedback. In all cases, we found that participants learn many more features than they need to classify the exemplars. Our results suggest that participants learn the internal correlations between features, rather than directly forming associations between features and the category label. This finding places restrictions on the types of categorisation models that can satisfactorily explain learning in the absence of classification error...|$|E
40|$|While {{there is}} an {{enormous}} amount of music data available, the eld of music analysis almost exclusively uses manually designed features. In this work we <b>learn</b> <b>features</b> from music data in a completely unsupervised way and evaluate them on a musical genre classi- cation task. We achieve results very close to state-of-the-art performance which relies on highly hand-tuned feature extractors...|$|E
30|$|Deep {{learning}} was investigated by researchers for machine fault diagnosis. Shao et al. developed a deep learning approach based on deep belief networks, {{in order to}} <b>learn</b> <b>features</b> from frequency distribution of vibration signals {{for the purpose of}} health condition evaluation of induction motors. Wang et al. investigated a motor fault diagnosis method based on short-time Fourier transform and convolutional neural network; its effectiveness was validated using experimental data.|$|E
25|$|The Cathedral of <b>Learning</b> <b>features</b> in Chris Kuzneski's 2009 novel, The Prophecy.|$|R
5000|$|Chamilo - an {{open-source}} learning {{management system}} incorporating a social <b>learning</b> <b>features</b> set ...|$|R
5000|$|Tanoshii Japanese, {{features}} EDICT-based dictionary, Text to Speech, KANJIDIC, KanjiVG and <b>learning</b> <b>features</b> ...|$|R
40|$|We {{consider}} {{the problem of}} predicting brain activation in response to arbitrary words in English. Whereas previous computational models have encoded words using predefined sets of features, we formulate a model that can automatically <b>learn</b> <b>features</b> directly from data. We show that our model reduces to a simultaneous sparse approximation problem and show two examples where learned features give insight about how the brain represents meanings of words. 1...|$|E
3000|$|... a. This {{ensures that}} the input feature is {{embedded}} with enough time-frequency detail. But, meaningful features {{still need to be}} obtained from such a high-dimensional input. Restricted Boltzmann machines (RBMs) [15] provide a useful paradigm for accomplishing this, since they are unsupervised generative models with great high-dimensional modeling capabilities, and allow the model to <b>learn</b> <b>features</b> from data. Moreover, RBMs form the basis of current state-of-the-art deep neural networks (DNNs) [16], allowing seamless integration into the DNN framework.|$|E
30|$|This {{research}} {{addresses the}} aforementioned challenges {{by developing a}} novel and complete computer-aided diagnosis (CAD) system for tumour detection and localization from MRIs. In the tumour detection phase, the system combines a CNN, which is used for feature extraction due to its ability to <b>learn</b> <b>features</b> from raw data, with an error-correcting output codes support vector machine (ECOC-SVM), which is used for feature classification. The system is considered a two-phase multi-model artefact due to its detection and localization abilities using different CNN models.|$|E
40|$|We {{present a}} feature {{extraction}} method for RGB-D data based on k-means clustering that builds on recent work by Coates et al. Using unsupervised learning methods {{we are able}} to automatically <b>learn</b> <b>feature</b> responses that combine all available information (color and depth) into one, concise representation. We show that depth information can substantially increase the recognition performance and that the <b>learned</b> <b>features</b> are competitive with previously published results on the RGB-D Object Dataset. For object recognition in high resolution RGB-D images we propose to embed the <b>learned</b> <b>features</b> into a local image descriptor, the convolutional k-means descriptor. Using this approach recognition accuracy can be increased even further. ...|$|R
50|$|In machine <b>learning,</b> <b>feature</b> <b>learning</b> or {{representation}} {{learning is}} a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both <b>learn</b> the <b>features</b> and use them to perform a specific task.|$|R
30|$|Instead of {{training}} networks on full images, {{we can use}} convolutional networks to reduce the computational cost of <b>learning</b> <b>features</b> from large-size images with autoencoders. First, small-size image patches are sampled randomly from the training set and trained with autoencoders. Then, the <b>learned</b> <b>features</b> should be convolved with larger images and different feature activations at each location can be obtained [11].|$|R
30|$|To {{overcome}} {{the problems of}} handcrafted features for scene classification, unsupervised feature learning is reckoned as the potential strategy. It can automatically <b>learn</b> <b>features</b> from unlabeled input data and has made astonishing progress in remote sensing scene classification [16 – 18]. The unsupervised-learning-based features are more discriminative and better suited to the classification problem. PCA, K-means clustering, sparse coding, and autoencoder are typical unsupervised learning methods. These methods and their variants have achieved great success in the scene classification field.|$|E
40|$|We present {{our system}} for the CAp 2017 NER {{challenge}} which is about named entity recognition on French tweets. Our system leverages unsupervised learning on a larger dataset of French tweets to <b>learn</b> <b>features</b> feeding a CRF model. It was ranked first without using any gazetteer or structured external data, with an F-measure of 58. 89 %. To {{the best of our}} knowledge, it is the first system to use fasttext embeddings (which include subword representations) and an embedding-based sentence representation for NER...|$|E
30|$|Without frame-by-frame spatial {{continuity}} as {{in single}} camera tracking, target re-identification across views {{is a difficult}} problem. To this end, we first learn target-specific discriminative appearance models to discriminate visually very similar targets. Positive and negative samples of the target are collected online from single camera tracking. For the target we aim to track, we <b>learn</b> <b>features</b> which are most discriminative in appearance matching using the collected training samples by Adaboost algorithm [1, 32]. The appearance affinity is then computed by the weighted feature similarity measurements [1].|$|E
3000|$|... a novel sensor {{placement}} methodology that {{stems from}} a machine <b>learning</b> <b>feature</b> selection procedure; and [...]...|$|R
40|$|Abstract. The recent {{progress}} in sparse coding and deep learning has made unsupervised <b>feature</b> <b>learning</b> methods a strong competitor to hand-crafted descriptors. In computer vision, success stories of <b>learned</b> <b>features</b> have been predominantly reported for object recognition tasks. In this paper, we investigate if and how <b>feature</b> <b>learning</b> {{can be used}} for material recognition. We propose two strategies to incorporate scale information into the learning procedure resulting in a novel multi-scale coding procedure. Our results show that our <b>learned</b> <b>features</b> for mate-rial recognition outperform hand-crafted descriptors on the FMD and the KTH-TIPS 2 material classification benchmarks. ...|$|R
40|$|Identifying {{suitable}} {{image features}} {{is a central}} challenge in computer vision, ranging from representations for lowlevel to high-level vision. Due {{to the difficulty of}} this task, techniques for <b>learning</b> <b>features</b> directly from example data have recently gained attention. Despite significant benefits, these <b>learned</b> <b>features</b> often have many fewer of the desired invariances or equivariances than their hand-crafted counterparts. While translation in-/equivariance has been addressed, the issue of learning rotation-invariant or equivariant representations is hardly explored. In this paper we describe a general framework for incorporating invariance to linear image transformations into product models for <b>feature</b> <b>learning.</b> A particular benefit is that our approach induces transformation-aware <b>feature</b> <b>learning,</b> i. e. it yields features that have a notion with which specific image transformation they are used. We focus our study on rotation in-/equivariance and show the advantages of our approach in learning rotation-invariant image priors and in building rotation-equivariant and invariant descriptors of <b>learned</b> <b>features,</b> which result in state-of-the-art performance for rotation-invariant object detection. 1...|$|R
40|$|Deep Autoencoder has the {{powerful}} ability to <b>learn</b> <b>features</b> from {{large number of}} unlabeled samples and {{a small number of}} labeled samples. In this work, we have improved the network structure of the general deep autoencoder and applied it to the disease auxiliary diagnosis. We have achieved a network by entering the specific indicators and predicting whether suffering from liver disease, the network using real physical examination data for training and verification. Compared with the traditional semi-supervised machine learning algorithm, deep autoencoder will get higher accuracy...|$|E
40|$|This paper {{defines a}} {{systematic}} approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity {{and the type}} of comment and (ii) proposing ro-bust shallow syntactic structures for im-proving model adaptability. We rely on the tree kernel technology to automatically ex-tract and <b>learn</b> <b>features</b> with better gener-alization power than bag-of-words. An ex-tensive empirical evaluation on our manu-ally annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural mod-els in a cross-domain setting. ...|$|E
40|$|We {{develop a}} {{hierarchical}} generative model to <b>learn</b> <b>features</b> of simple and complex {{cells in the}} primary visual cortex via multiplicative interactions and {{address the problem of}} learning invariant visual representation from natural image sequences. Generally, the overall objective of a generative sparse coding model (Olshausen and Field, Nature, 1996) is to reconstruct the input *x* via a linear combination of feature bases *A = [A~ 1 ~,A~ 2 ~, [...] .,A~M~]*, such that *x = As*, where s is the latent representation with a sparse constraint. |$|E
5000|$|After <b>learning</b> <b>features,</b> {{there should}} be some {{evaluation}} algorithms to evaluate the learning algorithms. D. Roth applied two learning algorithms: ...|$|R
40|$|Communication about {{requirements}} {{is often}} handled in issue tracking systems, {{especially in a}} distributed setting. As issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. This paper investigates natural language processing and machine <b>learning</b> <b>features</b> to detect software feature requests in natural language data of issue tracking systems. It compares traditional linguistic machine <b>learning</b> <b>features,</b> such as "bag of words", with more advanced features, such as subject-action-object, and evaluates combinations of machine <b>learning</b> <b>features</b> derived from the natural language and features taken from the issue tracking system meta-data. Our investigation shows that some combinations of machine <b>learning</b> <b>features</b> derived from natural language and the issue tracking system meta-data outperform traditional approaches. We show that issues or data fields (e. g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. Finally, we show that the choice of machine learning algorithms should depend on the goal, e. g. maximization of the detection rate or balance between detection rate and precision. In addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic...|$|R
30|$|It {{is obvious}} from Figures  5 and 6 that {{whitening}} transformation does affect image patches and corresponding <b>features</b> <b>learned</b> by sparse autoencoders. For example, more edge <b>features</b> can be <b>learned</b> when whitening processing is adopted. In addition, {{the value of}} ε also {{has an effect on}} the image patches and <b>learned</b> <b>features</b> when whitening pre-processing is used. As shown in Figure  6, the <b>learned</b> <b>features</b> look rather noisy if the value of ε is set to 0.01 and the data will be blurred if ε is set to 1. The best value of ε is 0.1 in our experiments.|$|R
