894|19|Public
25|$|JPEG (Joint Photographic Experts Group) format {{can produce}} a smaller file than PNG for {{photographic}} (and photo-like) images, since JPEG uses a lossy encoding method specifically designed for photographic image data, which is typically dominated by soft, <b>low-contrast</b> transitions, and an amount of noise or similar irregular structures. Using PNG instead of a high-quality JPEG for such images {{would result in a}} large increase in filesize with negligible gain in quality. In comparison, when storing images that contain text, line art, or graphics – images with sharp transitions and large areas of solid color – the PNG format can compress image data more than JPEG can. Additionally, PNG is lossless, while JPEG produces noticeable visual artifacts around high-contrast areas. Where an image contains both sharp transitions and photographic parts, a choice must be made between the two effects. JPEG does not support transparency.|$|E
500|$|Existing {{ground-based}} telescopes, {{and various}} proposed Extremely Large Telescopes, can exceed the HST {{in terms of}} sheer light-gathering power and diffraction limit due to larger mirrors, but other factors affect telescopes. In some cases, {{they may be able}} to match or beat Hubble in resolution by using adaptive optics. However, AO on large ground-based reflectors will not make Hubble and other space telescopes obsolete. Most AO systems sharpen the view over a very narrow field—Lucky Cam, for example, produces crisp images just 10" [...] to 20" [...] wide, whereas Hubble's cameras produce crisp images across a 2½' (150") field. Furthermore, space telescopes can study the universe across the entire electromagnetic spectrum, most of which is blocked by Earth's atmosphere. Finally, the background sky is darker in space than on the ground, because air absorbs solar energy during the day and then releases it at night, producing a faint—but nevertheless discernible—airglow that washes out <b>low-contrast</b> astronomical objects.|$|E
500|$|The cinematographer for Loves of a Blonde was Miroslav Ondříček, like Forman a {{graduate}} of the national Czech film school. [...] Ondříček and Forman had met at Czechoslovakia's Barrandov Studios in the early 1960s when Ondříček was just learning to shoot feature films and Forman was struggling to create his own early projects. Ondříček was in full accord with Forman's reliance on non-professional actors, adopting a cinéma-vérité style compatible with orthodox socialist realism. Ondříček was schooled in documentary filmmaking and, as a result, he insisted on performing all photographic tasks, like focus, lighting and composing, personally. while refusing access to the set by all film editors, saying: [...] "The editor can only edit what I am filming. He can't use a master shot if he doesn't have it. He can't make a close-up if he doesn't have it. This is our [...] job." [...] Critics have often admired Ondříček's success in sustaining <b>low-contrast</b> lighting throughout the film, rendering [...] "a sweetly mysterious softness, as if a principle of compassion existed in the world alongside its cruelty".|$|E
50|$|It {{has been}} {{developed}} by Cootes, Taylor et al. and became a standard in computer vision for the statistical study of shape and for segmentation of medical images where shape priors really help interpretation of noisy and <b>low-contrasted</b> pixels/voxels. The latter point leads to active shape models (ASM) and active appearance models (AAM).|$|R
40|$|A new {{framework}} {{dealing with}} motion estimation in transparent images is presented. It {{relies on a}} block-oriented estimation involving an efficient multiresolution minimization. A downhill simplex method provides an appropriate initialization to this scheme. The estimated velocity vectors are greatly improved by an original postprocessing stage which performs a single motion estimation on differences of warped images. Finally, a regularization step is carried out. It is demonstrated on a large set of simulations that a quarter-pixel accuracy can be attained on noise-free images. The case of noisy images is also addressed and provides satisfactory results, even {{in the case of}} <b>low-contrasted</b> medical images. An example on real clinical images is also reported with promising results. 1...|$|R
40|$|International audienceAmong {{the issues}} raised by Computer Vision, the {{tracking}} of one or several regions of interest is a major consideration. Achieving accuracy and robustness is critical in Visual Servoing and more generally in vision-based applications because of the sensibility of the algorithms. To address those issues in the specific case of MIS heart surgery, we developed a texture-based approach that provide good results on experimental beating heart images. Our approach consists in integrating textural characterization of the region of interest to reinforce the tracking procedure based on classical approaches such as correlation. We show that an appropriate combination of image descriptors can improve the efficiency of the tracking, especially on <b>low-contrasted</b> medical images...|$|R
500|$|The filming {{schedule}} was not {{organized in the}} episode sequence, which made managing the logistics much more challenging. The entire season was shot on 35 mm film, which the production staff chose to achieve a certain texture, {{as well as a}} [...] "nostalgic" [...] quality. The season was filmed using a Panavision Millennium XL2 camera, and the choice of lens corresponded to the period when a scene took place. Scenes set in 1995 and 2002 were captured with Panavision PVintage lenses, which produced a softer image because they were made of recycled, <b>low-contrast</b> glass. As these scenes were written as a reflection of Cohle and Hart's memory, production sought to make them as cinematic as possible, to reflect what Arkapaw called [...] "the fragmentation of their lucid imaginations back through their past." [...] To achieve this, they relied on wider lenses to exaggerate composition. The 2012 scenes were shot with Panavision Primo lenses: the visual palette in comparison was sharper and had much more contrast, lending a [...] "modern, crisp feeling" [...] to the images, and, according to Arkapaw, pulling [...] "characters out from their environments to hopefully help audiences get inside their heads." ...|$|E
500|$|The {{body weight}} {{of this species}} is {{typically}} {{in the range of}} , although weights of up to [...] have been recorded. [...] The body length averages , and its skull length ranges between , roughly intermediate in size between the smaller pygmy slow loris and the larger Sunda slow loris. [...] The slow lorises of Borneo are among the smallest of its genus, but this species can be distinguished from the others by its pale golden to red fur, <b>low-contrast</b> markings on its face and head, and the consistent lack of a second upper incisor. The rings around the eyes are either rounded or diffused-edged on top, while the bottom occasionally extends down below the zygomatic arch. [...] The stripe between its eyes is narrow, the ears usually lack fur, the patch {{on the top of the}} head is mostly diffused, and the band of fur in front of the ears varies in width. In comparison to the other three species of slow lorises on Borneo, both N.menagensis and N.kayan have a pale body coloration, but this species has pale, very lightly-contrasting facial markings, with markedly less contrast than the dark, high-contrast face mask of N.kayan. Additionally, this species has short, unfluffed body hair, in contrast to the longer, fluffier body hair of N.kayan.|$|E
500|$|The G2's {{rear-facing}} {{camera was}} considered good for its class, with its processor contributing to quicker HDR photo processing than its competitors. The Verge remarked that despite LG having [...] "practically stole" [...] Samsung's camera design and modes, the G2's camera interface {{were among the}} better implementations of Android camera software due to its available options. However, its low-light photos {{and some of its}} other modes were panned for not being as good as those of other devices such as the Nokia Lumia 920 and HTC One. In a photography-focused review by Digital Photography Review, the optical image stabilization system was praised for helping maintain good levels of exposure, and well-lit photos were found to have a decent level of detail, noting that its lens was [...] "sharp pretty much all across the frame and free of chromatic aberrations." [...] However, it was noted that [...] "as the light gets dimmer and in the ISO starts to increase", the device began to suffer from [...] "very heavy-handed noise reduction which results in visible softness", and further noted that [...] " [...] detail starts to suffer as soon as you go higher than base ISO and by ISO 400 most <b>low-contrast</b> detail is gone." [...] However, in a December 2013 comparison against other recent phones such as the One, Galaxy S4 Zoom, Xperia Z1, iPhone 5S, and Lumia 1020 by TechRadar, the G2 was named the best cameraphone of the six for [...] " [...] very well in terms of picture quality, ease of use and functionality, as well as post processing", although it was panned for not having as many options as its competitors, and for the probability of fingers accidentally getting into landscape shots due to the positioning of the lens.|$|E
40|$|International audienceReflectance {{confocal}} microscopy (RCM) is {{a powerful}} tool to visualize the skin layers at cellular resolution. The dermal-epidermal junction (DEJ) is a thin complex 3 D structure. It appears as a <b>low-contrasted</b> structure in confocal en-face sections, which is difficult to recognize visually, leading to uncertainty in the classification. In this article, we propose an automated method for segmenting the DEJ with reduced uncertainty. The proposed approach relies on a 3 D Conditional Random Field to model the skin biological properties and impose regularization constraints. We improve the restitution of the epidermal and dermal labels while reducing {{the thickness of the}} uncertainty area in a coherent biological way from 16. 9 µm (ground-truth) to 10. 3 µm...|$|R
40|$|This paper {{presents}} an automated fuzziness-driven algorithm for image enhancement. A class of parametric indices of fuzziness is introduced, {{which serves as}} the optimization criterion of the contrast-enhancement procedure. We show that the parametric class of indices constitutes a one-parameter generalization of the linear index of fuzziness of a set. The modification of the membership values of image pixels in the fuzzy plane is performed by finding the optimal S-function, which maximizes the parametric index of fuzziness (PIF). The first-order fuzzy moment of the image is used for tuning the PIF. Experimental results demonstrate {{the efficiency of the}} proposed framework in enhancing even highly <b>low-contrasted</b> images and also its ability to improve existing contrast-enhancing algorithms...|$|R
40|$|This {{paper is}} {{concerned}} with motion estimation in transparent X-Ray image sequences. Most of these medical images {{can be divided into}} areas containing at most two moving transparent layers. We will call it bi-distributed transparency. The first contribution of this paper is a motion estimation framework for the two-layer transparency case, able to handle noisy and <b>low-contrasted</b> X-Ray image sequences. It involves three steps: block-matching, affine fit and gradient-based parametric estimation. This estimation scheme is then extended to the bi-distributed transparency case. The second step is now formulated as a joint motion segmentation-estimation problem solved by the iterative minimization of a MRF-based energy function. This framework has been applied to synthetic and real image sequences with quite satisfactory results. 1...|$|R
5000|$|They {{are very}} <b>low-contrast</b> and {{therefore}} help to preserve shadow detail.|$|E
5000|$|... #Caption: This large Amritsar rug, {{with its}} <b>low-contrast,</b> classically {{decorated}} field, is a representative {{piece of this}} style.|$|E
50|$|Film type: For example, <b>low-contrast</b> {{print film}} has greater dynamic range than slide film's low dynamic range and higher contrast.|$|E
40|$|International audienceThis paper evaluates a {{nonlinear}} registration {{method for}} warping a 3 D histological atlas of the basal ganglia into patient data for {{deep brain stimulation}} (DBS) planning. The power of the method is the possibility to combine iconic registration with geometric constraints un-der a unified diffeomorphic framework. This combination aims to ensure robust and accurate atlas-to-patient warping and anatomy-preserving de-formations of stimulation target nuclei. A comparison of the method with a state-of-the-art diffeomorphic registration algorithm reveals how each approach deforms <b>low-contrasted</b> image regions where DBS target nuclei often lie. The technique is applied to T 1 -weighted magnetic resonance images from a cohort of Parkinsonian subjects, including subjects with standard-size and large ventricles. Results illustrate the effects of iconic or geometric registration alone, {{as well as how}} both constraints can be integrated in order to contribute for registration precision enhancement...|$|R
40|$|International audienceKey {{structures}} extraction like points, short-lines or regions extraction is a {{big issue}} in computer vision. Many fields of application need large image acquisition and fast extraction of fine structures. Several methods have been proposed with different accuracies and execution times. In this study, we focus on situations where existing local feature extractors give not enough satisfying results concerning both accuracy and time processing. Especially, we focus on short-line extraction in local <b>low-contrasted</b> images. To this end, we propose a new Fast Local Analysis by threSHolding (FLASH) designed to process large images under hard time constraints. We apply FLASH on the field of concrete infrastructure monitoring where robots and UAVs(Unmanned Aerial Vehicles) are more and more used for automated defect detection (like cracks). For large concrete surfaces, there are several hard constraints such as the computational time and the reliability. Results show that the computations are faster than several existing algorithms without learning stage, and lead to an automated monitoring of infrastructures...|$|R
40|$|National audienceThe {{efficiency}} of the Non-Local means (NLM) image denoising algorithm relies on the identification of similar original pixels from noisy similar patches. Hence fine details and <b>low-contrasted</b> structures are badly recovered after the application of NLM. But as these structures tend to correspond to redundant ones in the Fourier domain, NLM filtering in this domain allows one to better denoise them. A mixed space-frequency approach improves the denoising performances of NLM because it ensures that the information is redundant enough, in the spatial domain or in the frequency domain. Our approach is simple : it consists in running two times the NLM algorithm (firstly in the frequency domain and secondly in the spatial domain). For fine textures and isolated points we get a better visual reconstruction than with the original NLM. In terms of PSNR, the improvement can be over 1 dB. Our approach gives intermediate results between the original NLM and state-of-the-art methods {{while at the same}} time having moderate complexity and leading to few visual artifacts...|$|R
5000|$|Sensia: a <b>low-contrast</b> {{consumer}} slide film; {{the current}} emulsion {{is considered to}} be identical or near-identical to Astia in the professional line.|$|E
50|$|Its {{headline}} {{features were}} near-silent film winding, input of EOS barcode programs, integral auto-zoom flash, twin input dials, an autofocus auxiliary light for <b>low-contrast</b> subjects and five fully automatic modes.|$|E
5000|$|Do {{you have}} <b>low-contrast</b> sensitivity? For example, {{do you have}} trouble seeing a gray car at dusk, a black car at night, or a white car on a snowy roadway? ...|$|E
40|$|International audience—Image guided {{interventions}} {{have seen}} {{growing interest in}} recent years. The use of X-rays for the procedure impels limiting the dose over time. Image sequences obtained thereby exhibit high levels of noise and very low contrasts. Hence, the development of efficient methods to enable optimal visualization of these sequences is crucial. We propose an original denoising method based on the curvelet transform. First, we apply a recursive temporal filter to the curvelet coefficients. As some residual noise remains, a spatial filtering is performed in the second step, which uses a magnitude-based classification and a contextual comparison of curvelet coefficients. This procedure allows to denoise the sequence while preserving <b>low-contrasted</b> structures, but does not improve their contrast. Finally, a third step is carried out to enhance the features of interest. For this, we propose a line enhancement technique in the curvelet domain. Indeed, thin structures are sparsely represented in that domain, allowing a fast and efficient detection. Quantitative and qualitative evaluations performed on synthetic and real low-dose sequences demonstrate that the proposed method enables a 50 % dose reduction...|$|R
40|$|International audienceWe address, in this paper, a new {{adaptive}} binarization method on images {{captured by}} smartphones. This work {{is part of}} an application for visually impaired people assistance, that aims at making text information accessible to people who cannot read it. The main advantage of the proposed method is that the windows underlying the local thresh-olding process are automatically adapted to the image content. This avoids the problematic parameter setting of local thresholding approaches, difficult to adapt to a heterogeneous database. The adaptive windows are extracted based on ultimate opening (a morphological operator) and then used as thresholding windows to perform a local Otsu's algorithm. Our method is evaluated and compared with the Niblack, Sauvola, Wolf, TMMS and MSER methods on a new challenging database introduced by us. Our database is acquired by visually impaired people in real conditions. It contains 4000 annotated characters (available online for research purposes). Experiments show that the proposed method outperforms classical binarization methods for degraded images such as <b>low-contrasted</b> or blurred images, very common in our application...|$|R
40|$|The {{efficiency}} of the Non-Local means (NLM) image denoising algorithm relies on the identification of similar original pixels from noisy similar patches. Hence fine details and <b>low-contrasted</b> structures are badly recovered after the application of NLM. But as these structures tend to correspond to redundant ones in the Fourier domain, NLM filtering in this domain allows one to better denoise them. A mixed space-frequency approach improves the denoising performances of NLM because it ensures that the information is redundant enough, in the spatial domain or in the frequency domain. Our approach is simple : it consists in running two times the NLM algorithm (firstly in the frequency domain and secondly in the spatial domain). For fine textures and isolated points we get a better visual reconstruction than with the original NLM. In terms of PSNR, the improvement can be over 1 dB. Our approach gives intermediate results between the original NLM and state-of-the-art methods {{while at the same}} time having moderate complexity and leading to few visual artifacts...|$|R
5000|$|Artifacts - {{software}} (especially {{operations performed}} during RAW conversion) can cause significant visual artifacts, including data compression and transmission losses (e.g. Low quality JPEG), oversharpening [...] "halos" [...] {{and loss of}} fine, <b>low-contrast</b> detail.|$|E
50|$|An {{internegative}} is {{a motion}} picture film duplicate. It {{is the color}} counterpart to an interpositive, in which a <b>low-contrast</b> color image is used as the positive between an original camera negative and a duplicate negative.|$|E
50|$|A study {{comparing}} film/screen {{and digital}} imaging demonstrates that a digital system with high DQE can improve one's {{ability to detect}} small, <b>low-contrast</b> objects - even though the digital system may have substantially lower Limiting Spatial Resolution (LSR) than film.|$|E
40|$|International audienceThis article {{presents}} {{a method for}} crop row structure characterization that is adapted to phenotyping-related issues. In the proposed method, a crop row 3 D model is built {{and serves as a}} basis for retrieving plant structural parameters. This model is computed using Structure from Motion with RGB images acquired by translating a single camera along the row. Then, to estimate plant height and leaf area, plant and background are discriminated by a robust method that uses both color and height information in order to handle <b>low-contrasted</b> regions. The 3 D model is scaled and the plant surface is finally approximated using a triangular mesh. The efficacy of our method was assessed with two data sets collected under outdoor conditions. We also evaluated its robustness against various plant structures, sensors, acquisition techniques and lighting conditions. The crop row 3 D models were accurate and led to satisfactory height estimation results, since both the average error and reference measurement error were similar. Strong correlations and low errors were also obtained for leaf area estimation. Thanks to its ease of use, estimation accuracy and robustness under outdoor conditions, our method provides an operational tool for phenotyping applications...|$|R
40|$|International audienceImage {{denoising}} is {{a central}} problem in image processing and it is often a necessary step prior to higher level analysis such as segmentation, reconstruction or super-resolution. The non-local means (NL-means) perform denoising by exploiting the natural redundancy of patterns inside an image; they perform a weighted average of pixels whose neighborhoods (patches) are close to each other. This reduces significantly the noise while preserving most of the image content. While it performs well on flat areas and textures, it suffers from two opposite drawbacks: it might over-smooth <b>low-contrasted</b> areas or leave a residual noise around edges and singular structures. Denoising can also be performed by total variation minimization [...] the ROF model [...] which leads to restore regular images, but it is prone to over-smooth textures, staircasing effects, and contrast losses. We introduce in this paper a variational approach that corrects the over-smoothing and reduces the residual noise of the NL-means by adaptively regularizing non-local methods with the total variation. The proposed regularized NL-means algorithm combines these methods and reduces both of their respective defaults by minimizing an adaptive total variation with a non-local data fidelity term. Besides, this model adapts to different noise statistics and a fast solution can be obtained in the general case of the exponential family. We develop this model for image denoising and we adapt it to video denoising with 3 D patches...|$|R
40|$|International audienceVisual {{analysis}} of real-life scenes {{starts with the}} parallel extraction of different visual elementary features at different spatial frequencies. The global shape of the scene is mainly contained in low spatial frequencies (LSF), and the edges and borders of objects are mainly contained in high spatial frequencies (HSF). The present fMRI study investigates the effect of age on the spatial frequency processing in scenes. Young and elderly participants performed a categorization task (indoor vs. outdoor) on LSF and HSF scenes. Behavioral results revealed performance degradation for elderly participants only when categorizing HSF scenes. At the cortical level, young participants exhibited retinotopic organization of spatial frequency processing, characterized by medial activation in the anterior part of the occipital lobe for LSF scenes (compared to HSF), and the lateral activation in the posterior part of the occipital lobe for HSF scenes (compared to LSF). Elderly participants showed activation only in the anterior part of the occipital lobe for LSF scenes (compared to HSF), but not significant activation for HSF (compared to LSF). Furthermore, a ROI analysis revealed that the parahippocampal place area, a scene-selective region, was less activated for HSF than LSF for elderly participants only. Comparison between groups revealed greater activation of the right inferior occipital gyrus in young participants than in elderly participants for HSF. Activation of temporo-parietal regions was greater in elderly participants irrespective of spatial frequencies. The present findings indicate a specific <b>low-contrasted</b> HSF deficit for normal elderly people, in association with an occipito-temporal cortex dysfunction, and a functional reorganization of the categorization of filtered scenes...|$|R
5000|$|Eastman Kodak: Eastman Fine Grain Sound Recording Film, Type 5373 (<b>low-contrast)</b> for {{negative}} stock where other prints would be made. Fine Grain Release Positive Film, Type 7302 (high-contrast) for direct positive recordings and single system sound recordings using variable area sound.|$|E
50|$|Kodachrome Commercial has a <b>low-contrast</b> {{characteristic}} which {{complements the}} various duplication films {{with which it}} is intended to be used: silver separation negatives for 35 mm (controlled exclusively by Technicolor) and reversal duplicating and printing stocks for 16 mm (controlled exclusively by Eastman Kodak).|$|E
50|$|Technical Pan is a {{microfilm}} emulsion {{that was}} made panchromatic through the addition of sensitizing dyes, {{as is the case}} with all panchromatic films. If developed in a general-purpose developer such as D-76, Tech Pan displays extreme contrast. It becomes a pictorial film when developed in a very <b>low-contrast</b> developer.|$|E
30|$|Singh and Kapoor [16] {{presented}} an exposure-based sub-image histogram equalization method for contrast enhancement in low exposure grayscale images. Their method obtains thresholds, which are computed {{in order to}} split the original image into sub-images of various intensity levels. To control the enhancement rate, the histogram is clipped utilizing a threshold value as an average number of gray-level occurrences. Their method performed better than other conventional histogram equalization approaches. However, their approach does not adjust the level of enhancement, thereby resulting in darker or brighter enhanced images. Zhuang and Guan [17] used the mean and variance-based sub-image histogram equalization method to increase the contrast of the input image with brightness while retaining important features. However, because some IETs produce over-enhanced images and artifacts, Hussain et al. [8] proposed a dark image enhancement approach where local transformation of the image pixels is considered. The experiments in [8] showed that their method improved satisfactorily the quality of images. However, artifacts {{were present in the}} images. Reddy et al. [12] presented the dynamic clipped histogram equalization for enhancing low contrast images. Their approach selects a clipped level at the occupied bins and conducts histogram equalization on the clipped histogram to produce the output image. Reddy’s method uses three variants of the occupied bin space to enhance the <b>low-contrasted</b> dark, bright, and gray images. Shi et al [2] presented a dual channel prior-based method for nighttime low illumination image enhancement using a single image that is based on two existing image priors i.e., bright and dark priors. They used the bright channel prior to obtain the initial transmission estimate and used the dark prior as a complementary channel to adjust any wrong transmission estimate produced by the bright channel prior.|$|R
40|$|Reliable {{estimates}} of dispersal rates between habitat patches (i. e. functional connectivity) {{are critical for}} predicting long-term effects of habitat fragmentation on population persistence. Connectivity measures are frequently derived from least cost path or graph-based approaches, {{despite the fact that}} these methods make biologically unrealistic assumptions. Individual-based models (IBMs) have been proposed as an alternative as they allow modelling movement behaviour in response to landscape resistance. However, IBMs typically require excessive data to be useful for management. Here, we test the extent to which an IBM requiring only an uncomplicated set of movement rules [the 'stochastic movement simulator' (SMS) ] can predict animal movement behaviour in real-world landscapes. Movement behaviour of two forest birds, the Cabanis's greenbul Phyllastrephus cabanisi (a forest specialist) and the white-starred robin Pogonocichla stellata (a habitat generalist), across an Afrotropical matrix was simulated using SMS. Predictions from SMS were evaluated against a set of detailed movement paths collected by radiotracking homing individuals. SMS was capable of generating credible predictions of bird movement, although simulations were sensitive to the cost values and the movement rules specified. Model performance was generally highest when movement was simulated across <b>low-contrasting</b> cost surfaces and when virtual individuals were assigned low directional persistence and limited perceptual range. SMS better predicted movements of the habitat specialist than the habitat generalist, which highlights its potential to model functional connectivity when species movements are affected by the matrix. Synthesis and applications. Modelling the dispersal process with greater biological realism is likely to be critical for improving our predictive capability regarding functional connectivity and population persistence. For more realistic models to be widely applied, it is vital that their application is not overly complicated or data demanding. Here, we show that given relatively basic understanding of a species' dispersal ecology, the stochastic movement simulator represents a promising tool for estimating connectivity, which can help improve the design of functional ecological networks aimed at successful species conservation...|$|R
40|$|In this thesis, we {{tackle the}} problem of video object {{segmentation}} {{where we have to}} classify every pixel of every frame in a video sequence into background and foreground classes. Our algorithms fall in the semi-supervised category, i. e., they start with the object of interest annotated in the first frame and then they track and segment that object in the following frames. The first algorithm that we have implemented describes the object of interest in terms of a set of points distributed on the object and then tracks them in the following frames. To make the tracking robust, we impose that the spatial distribution of these points is stable along the frames. To do so, we place a mesh on top of the mask of the object, whose vertices are the interest points to track, and the edges define the spatial structure within them. We then compute a descriptor of the appearance of each of the points and look for the displacements that bring those points in the following frame to a point with a similar descriptor. We enforce that the displacements of neighboring points are similar, which favors coherent deformations of the object. This algorithm may experience difficulties at the contours of the objects as the point descriptors might be influenced by the background. To overcome this problem, our second algorithm is based on the idea of tracking the contour of the object by imposing smooth deformations between frames. Starting from a polygonal representation of the contour of the object,we look for the locations at the following frame that have a strong response of an edge detector while minimizing the deformation of the shape. Specifically, we build a multiscale pyramid of segments of the contour polygon and look for the displacement of every segment that matches the edge response while being coherent with the rest of elements of the pyramid. This second algorithm can be understood as complementary to the first one, since it might fail in object with <b>low-contrasted</b> contours or with cluttered background. As an overall trade off, we propose {{a combination of the two}} algorithms that tries to make the most out of each of them and compensate their weaknesses. In order to validate our approaches, we perform an extensive validation on a recently-published database called DAVIS that provides fifty sequences with the ground truth annotated in each of their frames. We sweep all the different parameters of the algorithms in order to achieve the best performance in this database. The results show that the contour algorithm outperforms the mesh algorithm, so the weaknesses presented in the previous paragraph are more prominent in the mesh algorithm. Once we combine both of them, although we have not been able to do a full search in the parameter space, the results obtained are promising and an increase in the parameter space search suggests that we would outperform any of the standalone methods. We also perform a comparison against six state-of-the-art algorithms which shows that although we are still behind the better-performing ones, our approach might be competitive with further tuning and experimentation...|$|R
