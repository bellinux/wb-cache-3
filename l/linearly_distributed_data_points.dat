0|10000|Public
25|$|The prior {{predictive}} {{distribution and}} posterior predictive distribution {{of a new}} normally <b>distributed</b> <b>data</b> <b>point</b> when a series of independent identically <b>distributed</b> normally <b>distributed</b> <b>data</b> <b>points</b> have been observed, with prior mean and variance as in the above model.|$|R
40|$|This paper {{considers}} the interpolation for multi-dimensional data using Voronoi diagrams. Sibson's interpolant is well-known as an interpolation method using Voronoi diagrams for discretely <b>distributed</b> <b>data,</b> {{and it is}} extended to continuously <b>distributed</b> <b>data</b> by Gross. On the other hand, the authors studied another interpolation method using Voronoi diagrams recently. This paper outlines the authors' interpolant brie#y, and extends the author's interpolant to <b>linearly</b> <b>distributed</b> <b>data</b> based upon the discussion using integrations...|$|R
5000|$|Generate a set [...] of [...] uniformly {{randomly}} <b>distributed</b> <b>data</b> <b>points.</b>|$|R
2500|$|For {{a set of}} i.i.d. {{normally}} <b>distributed</b> <b>data</b> <b>points</b> X of size n {{where each}} individual point x follows [...] with known variance σ2, the conjugate prior distribution is also normally distributed.|$|R
40|$|Computation of the {{diffraction}} {{field from}} a given set of arbitrarily <b>distributed</b> <b>data</b> <b>points</b> in space {{is an important}} signal processing problem arising in digital holographic 3 D displays. The field arising from such <b>distributed</b> <b>data</b> <b>points</b> has to be solved simultaneously by considering all mutual couplings to get correct results. In our approach, the discrete form of the plane wave decomposition {{is used to calculate}} the diffraction field. Two approaches, based on matrix inversion and on projections on to convex sets (POCS), are studied. Both approaches are able to obtain the desired field when the number of given <b>data</b> <b>points</b> is larger than the number of <b>data</b> <b>points</b> on a transverse cross-section of the space. The POCS-based algorithm outperforms the matrix-inversion-based algorithm when the number of known <b>data</b> <b>points</b> is large. © 2006 Elsevier B. V. All rights reserved...|$|R
2500|$|For {{a set of}} i.i.d. {{normally}} <b>distributed</b> <b>data</b> <b>points</b> X of size n {{where each}} individual point x follows [...] with unknown mean μ and unknown variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.|$|R
40|$|Cataloged from PDF {{version of}} article. Computation of the {{diffraction}} field from a given set of arbitrarily <b>distributed</b> <b>data</b> <b>points</b> in space {{is an important}} signal processing problem arising in digital holographic 3 D displays. The field arising from such <b>distributed</b> <b>data</b> <b>points</b> has to be solved simultaneously by considering all mutual couplings to get correct results. In our approach, the discrete form of the plane wave decomposition {{is used to calculate}} the diffraction field. Two approaches, based on matrix inversion and on projections on to convex sets (POCS), are studied. Both approaches are able to obtain the desired field when the number of given <b>data</b> <b>points</b> is larger than the number of <b>data</b> <b>points</b> on a transverse cross-section of the space. The POCS-based algorithm outperforms the matrix-inversion-based algorithm when the number of known <b>data</b> <b>points</b> is large. (C) 2006 Elsevier B. V. All rights reserved...|$|R
5000|$|For {{a set of}} i.i.d. {{normally}} <b>distributed</b> <b>data</b> <b>points</b> X of size n {{where each}} individual point x follows [...] with unknown mean μ and unknown variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.Logically, this originates as follows: ...|$|R
5000|$|Given {{a set of}} {{independent}} identically <b>distributed</b> <b>data</b> <b>points</b> [...] where [...] according to some probability distribution parameterized by θ, where θ itself is a random variable described by a distribution, i.e. [...] the marginal likelihood in general asks what the probability [...] is, where θ has been marginalized out (integrated out): ...|$|R
30|$|After {{creating}} the even grid, the subsequent {{step is to}} <b>distribute</b> all the <b>data</b> <b>points</b> into the grid. This procedure can be naturally parallelized since the <b>distributing</b> of each <b>data</b> <b>point</b> can be performed independently. Assuming there are m <b>data</b> <b>points,</b> we allocate m GPU threads to <b>distribute</b> all the <b>data</b> <b>points.</b> Each thread is responsible for calculating the position of one <b>data</b> <b>point</b> locating in the grid, i.e., to determine the index of the cell where the <b>data</b> <b>point</b> locates.|$|R
40|$|AbstractTo better {{approximate}} nearly singular functions with meshless methods, {{we propose}} a <b>data</b> <b>points</b> redistribution method {{extended from the}} well-known one-dimensional equidistribution principle. With properly <b>distributed</b> <b>data</b> <b>points,</b> nearly singular functions can be well approximated by linear combinations of global radial basis functions. The proposed method is coupled with an adaptive trial subspace selection algorithm {{in order to reduce}} computational cost. In our numerical examples, clear exponential convergence (with respect to the numbers of <b>data</b> <b>points)</b> can be observed...|$|R
5000|$|There are {{multiple}} formulations of the Hopkins Statistic. A typical one is as follows. Let [...] be {{the set of}} [...] <b>data</b> <b>points</b> in [...] dimensional space. Consider a random sample (without replacement) of [...] <b>data</b> <b>points</b> with members [...] Also generate a set [...] of [...] uniformly randomly <b>distributed</b> <b>data</b> <b>points.</b> Now define two distance measures, [...] to be the distance of [...] from its nearest neighbor in X and [...] to be the distance of [...] from its nearest neighbor in X. We then define the Hopkins statistic as: ...|$|R
2500|$|For {{a set of}} i.i.d. {{normally}} <b>distributed</b> <b>data</b> <b>points</b> X of size n {{where each}} individual point x follows [...] with known mean μ, the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution. [...] The two are equivalent except for having different parameterizations. [...] Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared {{for the sake of}} convenience. [...] The prior for σ2 is as follows: ...|$|R
40|$|For the Lego {{discrepancy}} with M bins, {{which is}} equivalent with a chi^ 2 -statistic with M bins, {{we present a}} procedure to calculate the moment generating function of the probability distribution perturbatively if M and N, the number of uniformly and randomly <b>distributed</b> <b>data</b> <b>points,</b> become large. Furthermore, we present a phase diagram for various limits of the probability distribution {{in terms of the}} standardized variable if M and N become infinite. Comment: 16 page...|$|R
5000|$|For {{a set of}} i.i.d. {{normally}} <b>distributed</b> <b>data</b> <b>points</b> X of size n {{where each}} individual point x follows [...] with known mean μ, the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution. The two are equivalent except for having different parameterizations. Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared {{for the sake of}} convenience. The prior for σ2 is as follows: ...|$|R
5000|$|... (In some instances, frequentist {{statistics}} {{can work}} around this problem. For example, confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance {{are constructed using}} a Student's t-distribution. This correctly estimates the variance, {{due to the fact}} that (1) the average of normally distributed random variables is also normally distributed; (2) the predictive distribution of a normally <b>distributed</b> <b>data</b> <b>point</b> with unknown mean and variance, using conjugate or uninformative priors, has a student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least, to an arbitrary level of precision, when numerical methods are used.) ...|$|R
40|$|AbstractThe present paper {{concerns}} {{fitting the}} range residuals of the Satellite Laser Ranging (SLR) data using the overlap technique. The range residuals {{are characterized by}} its randomly <b>distributed</b> <b>data</b> <b>points.</b> The method used for fitting is represented. The results are compared with those obtained when fitting the whole data using Chebyshev polynomial and Least Squares Method. The later is already applied in Helwan SLR station. We {{have found that the}} overlap technique allows us to use low degree polynomials in order to fit large number of <b>data</b> <b>points</b> and, at the same time, provides better standard deviations, which may be significant, when one wants to reach best fitting. Therefore, the results obtained have approved the choice of the overlap technique in dealing with the SLR range residuals, in comparable with fitting the whole data using either Chebyshev polynomial or the Least Squares Method...|$|R
40|$|Abstract- Mining or extracting the {{knowledge}} {{from the large}} amount of data is known as data mining. Here, the collection of data increases exponentially so that for extracting the efficient data we need good methods in data mining. Data mining analyzes several methods for extracting the data. Clustering is one of the methods for extracting the data from large amount of data. Multiple clustering algorithms were developed for clustering. Multiple clustering can be combined so that the final partitioning of data provides better clustering. Efficient density based k-means clustering algorithm has been proposed to overcome the drawbacks of dbscan and k-means clustering algorithms. The algorithm performs better than dbscan while handling clusters of circularly <b>distributed</b> <b>data</b> <b>points</b> and slightly overlapped clustering. Index Terms: clustering analysis, k-means, and dbscan. I...|$|R
40|$|Data mining {{refers to}} extracting or “mining” {{knowledge}} from {{large amounts of}} data. Clustering {{is one of the}} most important research areas in the field of data mining. Clustering means creating groups of objects based on their features in such a way that the objects belonging to the same groups are similar and those belonging in different groups are dissimilar. In this paper, the most representative partition based clustering algorithms are described and categorized based on their basic approach. The best algorithm is found out based on their performance. Two of the clustering algorithms, namely, Centroid based k-means, Representative object based k-medoids are implemented by using JAVA and their performance is analyzed based on their clustering quality. The randomly <b>distributed</b> <b>data</b> <b>points</b> are taken as input to these algorithms and clusters are found out for each algorithm. The algorithm’s performance is analyzed by different runs on the input <b>data</b> <b>points.</b> The experimental results are given as both graphical as well as tabular representation...|$|R
40|$|The {{problem of}} {{scattered}} data interpolation is the fitting of a smooth surface (or, more generally, a manifold) through {{a set of}} non-uniformly <b>distributed</b> <b>data</b> <b>points</b> that extends to all positions in a domain. Common sources of scattered data include experiments, physical measurements, and computational values. Scattered data interpolation assists in interpreting such data through the calculation of values at arbitrary positions in the domain. Despite much attention in the literature, scattered data interpolation remains a difficult and computationally expensive problem to solve. BSPLND is a software package that solves this problem. It uses the scattered data interpolation technique presented in [1] (hereafter, LWS). This technique is fast and produces a C 2 -continuous interpolation function for any set of scattered data using a hierarchical set of cubic B-splines. BSPLND extends the technique to work with data having an arbitrary number of dimensions for both its domain and range...|$|R
40|$|The {{complexation}} of Th-alpha-isosaccharinate {{has been}} examined using liquid-liquidextraction technique. An organic phase consisting of 0. 025 M acetylacetone in toluene was {{used with a}} 1. 0 M NaClO 4 aqueous phase kept at pH 8 in a thermostated AKUFVE unit. Two temperatures, 15 degrees C and 35 degrees C, were investigated and the stability constants for the complexation reaction Th 4 + + pISA Th(ISA(-)) (p) ((4 -p) +) were determined by curve fitting of a distribution ratio equation to experimental data employing a chi(2) method to estimate the standard deviation. By incorporating data from a previous study at 25 degrees C, Delta H degrees and AS' for the reaction were also obtained, using randomly generated, binomially <b>distributed,</b> <b>data</b> <b>points</b> around the stability constant values with one standard deviation in two dimensions. Three alpha-isosaccharinate ligands {{were found to be}} involved in the complexation with thorium...|$|R
40|$|There {{have been}} many {{interpolation}} methods developed over the years, each with their own problems. One of the biggest limitations in many applications is the non-correspondence of the surface with the objects used to support it- usually a set of arbitrarily <b>distributed</b> <b>data</b> <b>points,</b> This {{is due to the}} metric methods used to define both the zone of influence of a <b>data</b> <b>point</b> and the set of <b>data</b> <b>points</b> used to estimate surface properties at intermediate locations. Most methods are coordinate system oriented, not object oriented. We describe here an object-otiented approach, {{in the sense that the}} data objects themselves form the spatial structure used for interpolation. This has the primary benefit that the surface is described in terms of this structure, permitting easy correspondence between the surface and data values. In addition, the spatial objects used are not restricted to points, but may include more complex objects- currently composed of line segments. The structure used is the Voronoi diagram, which provides easily understood spatial relationships between map objects, as well as the local coordinate systems needed for point/surface correspondence. The methods described here use the Voronoi diagram to generate surfaces which pass precisel...|$|R
30|$|To {{visualize}} geographical {{patterns in}} SRR diversity of all species, we carried out grid-based spatial analyses at {{a resolution of}} 30 arc sec (~ 1  km at the equator). To obtain sufficient and more evenly <b>distributed</b> <b>data</b> <b>points</b> for constructing high resolution maps, we constructed circular neighborhoods of 5 arc minutes diameter (~ 10  km at the equator), following (Thomas et al. 2012). For each species we calculated values of allelic richness and Shannon diversity of individual raster cells based on all individuals included in the circular area of 5 arc minutes diameter. Calculation of parameters for tetraploid and diploid species was carried out with functions from the Polysat package and custom-made code for R (Thomas et al. 2012), respectively. The circular neighborhood diameter was chosen to allow comparison of the genetic profile of adjacent populations and hence detect spatial patterns at landscape level across the species distributions. All maps were edited in ArcMap v 10 (ESRI).|$|R
40|$|One of {{the main}} focus of {{scattered}} data interpolation is fitting a smooth surface to a set of non-uniformly <b>distributed</b> <b>data</b> <b>points</b> which extends to all positions in a prescribed domain. In this paper, given a set of scattered data V = {(xi, yi), i= 1, [...] .,n} ∈ R 2 over a polygonal domain and a corresponding set of real numbers {zi}i= 1 n, we wish to construct a surface S which has continuous varying tangent plane everywhere (G 1) such that S(xi, yi) = zi. Specifically, the polynomial being considered belong to G 1 quartic Bezier functions over a triangulated domain. In order to construct the surface, we need to construct the triangular mesh spanning over the unorganized set of points, V which will then have to be covered with Bezier patches with coefficients satisfying the G 1 continuity between patches and the minimized sum of squares of principal curvatures. Examples are also presented to show the effectiveness of our proposed method...|$|R
40|$|AbstractClustering plays a {{very vital}} role in {{exploring}} data, creating predictions and to overcome the anomalies in the data. Clusters that contain collateral, identical characteristics in a dataset are grouped using reiterative techniques. As the data in real world is growing day by day so very large datasets {{with little or no}} background knowledge can be identified into interesting patterns with clustering. So, in this paper the two most popular clustering algorithms K-Means and K-Medoids are evaluated on dataset transaction 10 k of KEEL. The input to these algorithms are randomly <b>distributed</b> <b>data</b> <b>points</b> and based on their similarity clusters has been generated. The comparison results show that time taken in cluster head selection and space complexity of overlapping of cluster is much better in K-Medoids than K-Means. Also K-Medoids is better in terms of execution time, non sensitive to outliers and reduces noise as compared to K-Means as it minimizes the sum of dissimilarities of data objects...|$|R
40|$|This paper {{identifies}} {{a previously}} undiscovered behavior of uniformly <b>distributed</b> <b>data</b> <b>points</b> or vectors in high dimensional ellipsoidal models. Such models give near normal distributions {{for each of}} its dimensions. Converse of this may also be true; that is, for a normal-like distribution of an observed variable, {{it is possible that}} the distribution is a result of uniform distribution of <b>data</b> <b>points</b> in a high dimensional ellipsoidal model, to which the observed variable belongs. Given the currently held notion of normal distributions, this new behavior raises many interesting questions. This paper also attempts to answer some of those questions. We cover both volume based (filled) and surface based (shell) ellipsoidal models. The phenomenon is demonstrated using statistical as well as mathematical approaches. We also show that the dimensionality of the latent model, that is, the number of hidden variables in a system, can be calculated from the observed distribution. We call the new distribution “Tanazur” and show through experiments that it is at least observed in one real world scenario, that of the motion of particles in an ideal gas. We show that the Maxwell-Boltzmann distribution of particle speeds can be explained on the basis of Tanazur distributions...|$|R
40|$|It {{is shown}} by earlier results that the minimax {{expected}} (test) distortion redundancy of empirical vector quantizers with three or more levels designed from n independent and identically <b>distributed</b> <b>data</b> <b>points</b> is at least Omega(1 / sqrt{n}) for the class of distributions on a bounded set. In this paper, a much simpler construction and proof for this are given with much better constants. There are similar bounds for the training distortion of the empirical optimal vector quantizer with three or more levels. These rates, however, do not hold for a one-level quantizer. Here the two-level quantizer case is clarified, showing that it already shares {{the behavior of the}} general case. Given that the minimax bounds are proved using a construction that involves discrete distributions, one suspects that for the class of distributions with uniformly bounded continuous densities, the expected distortion redundancy might decrease as o(1 / sqrt{n}) uniformly. It is shown as well that this is not so, proving that the lower bound for the expected test distortion remains true for these subclasses...|$|R
40|$|Abstract: Mining {{knowledge}} from {{large amounts of}} spatial data is known as spatial data mining. It becomes a highly demanding field because huge amounts of spatial data have been collected in various applications ranging from geo-spatial data to bio-medical knowledge. The amount of spatial data being collected is increasing exponentially. So, it far exceeded human’s ability to analyze. Recently, clustering has been recognized as a primary data mining method for knowledge discovery in spatial database. The database can be clustered in many ways depending on the clustering algorithm employed, parameter settings used, and other factors. Multiple clustering can be combined so that the final partitioning of data provides better clustering. In this paper, a novel density based k-means clustering algorithm has been proposed to overcome the drawbacks of DBSCAN and kmeans clustering algorithms. The result will be an improved version of k-means clustering algorithm. This algorithm will perform better than DBSCAN while handling clusters of circularly <b>distributed</b> <b>data</b> <b>points</b> and slightly overlapped clusters. Keywords: Clustering, DBSCAN, k-means, DBkmeans. I...|$|R
40|$|Efficient {{computation}} of scalar optical diffraction field due to {{an object}} is an essential issue in holographic 3 D television systems. The {{first step in the}} computation process is to construct an object. As a solution for this step, we assume that an object can be represented by a set of <b>distributed</b> <b>data</b> <b>points</b> over a space. The second step is to determine which algorithm provides better performance. The source model whose performance is investigated is based on superposition of the diffraction fields emanated from the hypothetical light sources located at the given sample points. Its performance is evaluated according to visual quality of the reconstructed field and its algorithmic complexity. Source model provides acceptable reconstructed patterns when the region in which the samples are given has a narrow depth along the longitudinal direction and a wide extent along the transversal directions. Also, the source model gives good results when the cumulative field at the location of each point due to all other sources tends to be independent of that location. © 2008 IEEE...|$|R
40|$|The {{level of}} the ground water has been {{constantly}} lowering and the urbanization has been rapidly developing during the last decades due to the strong groundwater extraction {{has led to the}} subsidence of some areas in the Ho Chi Minh City. Land deformation at the rate of few centimetres per year can be measured at the heavy ground water pumping stations. Most existing techniques for monitoring ground subsidence base on using methods of precise leveling, and more recently the GPS. These methods are generally expensive and inefficient for monitoring large areas. Besides, sparsely <b>distributed</b> <b>data</b> <b>points</b> are often insufficient to provide information on every localized ground subsidence. Recent advances in the SAR interferometry, especially with the Permanent Scatterer InSAR (PSInSAR) is an appropriate remote sensing technique for measuring ground subsidence in urban areas at high accuracy and low costs. Results demonstrate the effectiveness of employing the PSInSARTechnique for land subsidence monitoring at Ho Chi Minh city and PSInSAR has enabled a long-term study of vertical land movements using SAR images...|$|R
40|$|We {{propose the}} split-diffuse (SD) {{algorithm}} {{that takes the}} output of an existing dimension reduction algorithm, and <b>distributes</b> the <b>data</b> <b>points</b> uniformly across the visualization space. The result, called the topic grids, {{is a set of}} grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data. Topical analysis, comparison and interaction can be performed on the topic grids in a more perceivable way...|$|R
40|$|Problem statement: Clustering {{is one of}} {{the most}} {{important}} research areas in the field of data mining. Clustering means creating groups of objects based on their features in such a way that the objects belonging to the same groups are similar and those belonging to different groups are dissimilar. Clustering is an unsupervised learning technique. The main advantage of clustering is that interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. Clustering algorithms can be applied in many domains. Approach: In this research, the most representative algorithms K-Means and K-Medoids were examined and analyzed based on their basic approach. The best algorithm in each category was found out based on their performance. The input <b>data</b> <b>points</b> are generated by two ways, one by using normal distribution and another by applying uniform distribution. Results: The randomly <b>distributed</b> <b>data</b> <b>points</b> were taken as input to these algorithms and clusters are found out for each algorithm. The algorithms were implemented using JAVA language and the performance was analyzed based on their clustering quality. The execution time for the algorithms in each category was compared for different runs. The accuracy of the algorithm was investigated during different execution of the program on the input <b>data</b> <b>points.</b> Conclusion: The average time taken by K-Means algorithm is greater than the time taken by K-Medoids algorithm for both the case of normal and uniform distributions. The results proved to be satisfactory...|$|R
40|$|Reconstruction and {{visualization}} of the three-dimensional process models has been recently advanced underpinned {{with the rapid}} advances in computer technology and numerical computation algorithms. These modeling approaches has proved promising in predicting geologic, tectonic, and hydrologic processes needed for environmental planning of land-use, hazard mitigation, and sustainable resource management. However, these models require densely <b>distributed</b> <b>data</b> <b>points</b> that are often sparse and scarce. The present work introduces prototype 3 D geologic modeling tools developed to assist in constructing 3 D models from traditional geologic maps stored as GIS layers taking information depicted on these maps or from drilled-well charts as the source base data. The process-oriented algorithm integrates a successive sequence of geologic events and a logical model. The logical model describes the inter-relationships between the geologic units and its defining boundaries and their 3 D surface data represented by the topography. The tools implemented can produce and visualize models ranging from the simple layered sedimentation/erosion to the complexly structured faulted geologic units, based on scattered data, surfaces, 3 D grids, and the geologic entity/boundary hierarchy. The tools were encoded into a Visual Basic (VB...|$|R
40|$|There are {{number of}} {{techniques}} proposed byseveral researchers {{to analyze the}} performance ofclustering algorithms in data mining. All thesetechniques are not suggesting good results for thechosen data sets and for the algorithms in particular. Some of the clustering algorithms are suit for somekind of input data. This research work usesarbitrarily <b>distributed</b> input <b>data</b> <b>points</b> to evaluatethe clustering quality and performance of two of thepartition based clustering algorithms namely k-Means and k-Medoids. To evaluate the clusteringquality, the distance between two <b>data</b> <b>points</b> aretaken for analysis. The computational time iscalculated for each algorithm {{in order to measure}} theperformance of the algorithms. The experimentalresults show that the k-Means algorithm yields thebest results compared with k-Medoids algorithm...|$|R
40|$|Abstract: Problem statement: Clustering {{is one of}} {{the most}} {{important}} research areas in the field of data mining. Clustering means creating groups of objects based on their features in such a way that the objects belonging to the same groups are similar and those belonging to different groups are dissimilar. Clustering is an unsupervised learning technique. The main advantage of clustering is that interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. Clustering algorithms can be applied in many domains. Approach: In this research, the most representative algorithms K-Means and K-Medoids were examined and analyzed based on their basic approach. The best algorithm in each category was found out based on their performance. The input <b>data</b> <b>points</b> are generated by two ways, one by using normal distribution and another by applying uniform distribution. Results: The randomly <b>distributed</b> <b>data</b> <b>points</b> were taken as input to these algorithms and clusters are found out for each algorithm. The algorithms were implemented using JAVA language and the performance was analyzed based on their clustering quality. The execution time for the algorithms in each category was compared for different runs. The accuracy of the algorithm was investigated during different execution of the program on the input <b>data</b> <b>points.</b> Conclusion: The average time taken by K-Means algorithm is greater than the time taken by K-Medoids algorithm for both the case of normal and uniform distributions. The results proved to be satisfactory. Key words: K-Means clustering, K-Medoids clustering, data clustering, cluster analysi...|$|R
40|$|Calculation of near-neighbor {{interactions}} among high dimensional, irregularly <b>distributed</b> <b>data</b> <b>points</b> {{is a fundamental}} task to many graph-based or kernel-based machine learning algorithms and applications. Such calculations, involving large, sparse interaction matrices, expose the limitation of conventional data-and-computation reordering techniques for improving space and time locality on modern computer memory hierarchies. We introduce a novel method for obtaining a matrix permutation that renders a desirable sparsity profile. The method is distinguished by the guiding principle to obtain a profile that is block-sparse with dense blocks. Our profile model and measure capture the essential properties affecting space and time locality, and permit variation in sparsity profile without imposing a restriction to a fixed pattern. The second distinction lies in an efficient algorithm for obtaining a desirable profile, via exploring and exploiting multi-scale cluster structure hidden in but intrinsic to the data. The algorithm accomplishes its task with key components for lower-dimensional embedding with data-specific principal feature axes, hierarchical data clustering, multi-level matrix compression storage, and multi-level interaction computations. We provide experimental results from case studies with two important data analysis algorithms. The resulting performance is remarkably comparable to the BLAS performance for the best-case interaction governed by a regularly banded matrix with the same sparsity...|$|R
