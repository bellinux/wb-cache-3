0|10000|Public
40|$|Water {{quality and}} {{nutrient}} management {{research at the}} University of Kentucky related to beef cattle interaction with streams involved the collection and analysis of GPS <b>position</b> <b>data.</b> Typical data collection periods consisted of a <b>position</b> <b>data</b> <b>point</b> for each of 16 collars every five minutes for 18 days. Because {{of the volume of}} <b>data</b> <b>points,</b> multiple geodatabases were designed to organize and store the spatial data. Custom scripts and analysis techniques were developed to determine distance and location relationships using ArcGIS. The data were formatted and made available for research extension purposes using ArcPublisher...|$|R
3000|$|Create an even grid to {{partition}} the planar {{region that}} encloses the projected <b>positions</b> of all <b>data</b> <b>points</b> and interpolated points; [...]...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe track keeping characteristics of an autonomous ocean {{vehicle in the}} presence of a realistic time lag on the vehicle's position information is examined in two ways. A Hopf bifurcation analysis is applied to better predict vehicle behavior within its region of linearized classical stability. Additionally, the effects on guidance /control stability by the use of a single stage memory model incorporating the two previous vehicle positions in place of a single <b>position</b> <b>data</b> <b>point</b> is investigated. Results are presented using a dynamic model of the Naval Postgraduate School Autonomous Underwater Vehicle II (NPS AUV) [URL] Commander, United States Coast Guar...|$|R
30|$|After {{creating}} the even grid, the subsequent {{step is to}} distribute all the <b>data</b> <b>points</b> into the grid. This procedure can be naturally parallelized since the distributing of each <b>data</b> <b>point</b> can be performed independently. Assuming there are m <b>data</b> <b>points,</b> we allocate m GPU threads to distribute all the <b>data</b> <b>points.</b> Each thread is responsible for calculating the <b>position</b> of one <b>data</b> <b>point</b> locating in the grid, i.e., to determine the index of the cell where the <b>data</b> <b>point</b> locates.|$|R
40|$|We propose an {{algorithm}} for dynamically updating point-region (PR) quadtrees. Our {{algorithm is}} optimized for simultaneous update of <b>data</b> <b>points</b> comprising a quadtree. The intended application area focuses on simulating continuum phenomena, such as crowds, fluids, and smoke. We minimize {{the number of}} tree updates by making use of small changes in the <b>positions</b> of <b>data</b> <b>points.</b> We compare {{the efficiency of the}} proposed algorithm with two other approaches for updating a quadtree. One of these techniques creates the tree from scratch at each time-step. The second technique subsequently deletes a <b>data</b> <b>point</b> from the tree and reinserts it in its updated position. We achieve significant performance gains with our method in both cases. © 2012 Elsevier Inc. All rights reserved...|$|R
40|$|Cataloged from PDF {{version of}} article. We propose an {{algorithm}} for dynamically updating point-region (PR) quadtrees. Our algorithm is optimized for simultaneous update of <b>data</b> <b>points</b> comprising a quadtree. The intended application area focuses on simulating continuum phenomena, such as crowds, fluids, and smoke. We minimize {{the number of}} tree updates by making use of small changes in the <b>positions</b> of <b>data</b> <b>points.</b> We compare {{the efficiency of the}} proposed algorithm with two other approaches for updating a quadtree. One of these techniques creates the tree from scratch at each time-step. The second technique subsequently deletes a <b>data</b> <b>point</b> from the tree and reinserts it in its updated position. We achieve significant performance gains with our method in both cases. (C) 2012 Elsevier Inc. All rights reserved...|$|R
40|$|Principal {{components}} analysis (PCA) is {{a multivariate}} ordination technique used to display patterns in multivariate data. It aims to graphically display the relative <b>positions</b> of <b>data</b> <b>points</b> in fewer dimensions while retaining {{as much information}} as possible, and explore relationships between dependent variables. It is a hypothesis-generating technique that is intended to describe patterns in a data table, rather than test formal statistical hypotheses. PCA assumes linear responses of variables, and works best over short ecological gradients, with few zeroes in the data. It has a range of applications other than data display including multiple regression, and variable reduction...|$|R
3000|$|... 2 {{position}} of the 1, 2, 3 -thiadiazole thioacetanilides ring strongly support the above statement. The negative values of electrostatic descriptors suggested the requirement of electronegative group like NO 2, SO 2 R, CN, SO 2 Ar, COOH, F, Cl, Br, I, COOR, OR, OH at the <b>position</b> of generated <b>data</b> <b>point</b> E_ 587 (− 0.026282, – 0.019153) around 1, 2, 3 -thiadiazole thioacetanilides pharmacophore for maximum activity.|$|R
30|$|To {{perform the}} analysis, we {{organize}} relevant {{data in the}} form of a table: variables are laid out in columns, while rows correspond to <b>data</b> <b>points.</b> The 12 C# constructs (Table 5) are the variables. The <b>data</b> <b>points</b> correspond to every buggy file effectively located. The values for each variable are the summands that compose scoreS, the similarity score attributed by AmaLgam’s structure component (Section 2.3). scoreS is the summation of the similarities of each pair of file and bug parts (Equation 1). Thus, for each construct, there is a term from scoreS that reflects its specific contribution to the structural similarity score. These are the variable values used as input for the PCA. These values were taken from the best performing mode of construct mapping (i.e., the Complete mode, Section 3.6). After applying the selection criteria for effective instances (i.e., buggy files ranked among the top 10 <b>positions),</b> 363 <b>data</b> <b>points</b> were selected to compose the PCA input.|$|R
3000|$|... 1 {{position}} of the 1, 2, 3 -thiadiazole thioacetanilides ring which indicates that more hydrophobic substituents are favorable on this site and presence of more hydrophobic substituents increases the anti-HIV activity of 1, 2, 3 -thiadiazole thioacetanilides compounds. Therefore, more hydrophobic substituents such as –C 6 H 11, –C 6 H 5, –C(CH 3) 3, –CF 3,–CH 3, etc., were preferred at the <b>position</b> of generated <b>data</b> <b>point</b> H_ 171 around 1, 2, 3 -thiadiazole thioacetanilides pharmacophore.|$|R
30|$|An even grid is {{composed}} {{of a group of}} grid cells, and in this work, each grid cell is a square. The creation of an even grid is in fact to determine the position of the grid, the size of the cell, and the distribution layout of the cells. In our algorithm, an even planar grid is created to cover the planar region in which the projected <b>positions</b> of all <b>data</b> <b>points</b> and interpolated points locate.|$|R
40|$|People {{aggregate}} {{at different}} areas in {{different times of the}} day, thus forming different activity centers. The identification of activity centers faces the uncertain geographic context problem (UGCoP) because people go to different places to conduct different activities, and also go to the same place for carrying out different activities in different times of the day. In this paper, we employ two kinds of novel dynamic data, namely mobile phone <b>positioning</b> <b>data</b> and <b>Point</b> of Interest (POI) data to identify the activity centers in a city in China. Then mobile phone <b>positioning</b> <b>data</b> is utilized to identify the activity centers in different times of a working day, and POI data are used to show the activity density variations at these activity centers to explain the temporal dynamics of geographic context. We find that mobile phone <b>positioning</b> <b>data</b> and POI data as two kinds of spatial-temporal data demonstrate people’s activity patterns from different perspectives. Mobile phone <b>positioning</b> <b>data</b> provide a proxy to represent the activity density variations. POI data can be used to identify activity centers of different categories. These two kinds of data can be integrated to identify the activity centers and clarify the UGCoP...|$|R
30|$|We {{have also}} showed that the {{movement}} characteristics of speed, direction and distance of travel are very sensitive from a user privacy perspective: access to a user record {{of any of these}} movement characteristics, e.g., compass recordings, should be handled with a similar level of privacy as precise <b>positioning</b> <b>data.</b> Mobility <b>points</b> which have been removed from a trace should also be treated with great care. Our study of previously unseen points showed that, in the majority of cases, given a limited number of GPS spatio-temporal points it is possible to identify the traces from which the points originated, thus allowing a potential attacker to transfer the identity information from a non-anonymized dataset of trajectories to an anonymized set of points.|$|R
40|$|This report {{discusses}} two new indices {{for comparing}} clusterings {{of a set}} of points. The motivation for looking at new ways for comparing clusterings {{stems from the fact that}} the existing clustering indices are based on set cardinality alone and do not consider the <b>positions</b> of <b>data</b> <b>points.</b> The new indices, namely, the Random Walk index (RWI) and Variation of Information with Neighbors (VIN), are both inspired by the clustering metric Variation of Information (VI). VI possesses some interesting theoretical properties which are also desirable in a metric for comparing clusterings. We define our indices and discuss some of their explored properties which appear relevant for a clustering index. We also include the results of these indices on clusterings of some example data sets...|$|R
3000|$|At this stage, it is {{important}} to point out that the unconventional scan patterns used here induce a paradigm shift in how image data are considered. In a traditional scanning mode, the data are essentially stored as an array of intensities, which are assigned to elements within a 2 D matrix. However, for more complicated scan patterns, it is also necessary to specify the (nominal) <b>position</b> where each <b>data</b> <b>point</b> was acquired. A simple interpolation algorithm (herein called reconstruction) is used to map each <b>data</b> <b>point</b> to an element of the displayed or printed image. Thus, rather than a simple list of intensities (I [...]...|$|R
40|$|As {{described}} in the previous paper (Park et al. 2013), the detector subsystem of optical wide-field patrol (OWL) provides many observational <b>data</b> <b>points</b> of a single artificial satellite or space debris {{in the form of}} small streaks, using a chopper system and a time tagger. The position and the corresponding time data are matched assuming that the length of a streak on the CCD frame is proportional to the time duration of the exposure during which the chopper blades do not obscure the CCD window. In the previous study, however, the length was measured using the diagonal of the rectangle of the image area containing the streak; the results were quite ambiguous and inaccurate, allowing possible matching error of <b>positions</b> and time <b>data.</b> Furthermore, because only one (<b>position,</b> time) <b>data</b> <b>point</b> is created from one streak, the efficiency of the observation decreases. To define the length of a streak correctly, it is important to locate the endpoints of a streak. In this paper, a method using a differential convolution mask pattern is tested. This method can be used to obtain the positions where the pixel values are changed sharply. These endpoints can be regarded as directly detected positional data, and the number of <b>data</b> <b>points</b> is doubled by this result...|$|R
40|$|For location-based {{services}} it {{is often}} essential to efficiently process proximity relations among mobile objects, such as to establish whether {{a group of friends}} or family members are within a given distance of each other. A severe limitation in accurately establishing such relations is the inaccuracy of dynamically obtained <b>position</b> <b>data,</b> the <b>point</b> in time, and the frequency with which the <b>position</b> <b>data</b> is collected. In this paper, we use the common model of interpreting the unknown position of an object by a probability distribution centered around the last know position of the object. While this approach is straight forward, it poses severe difficulties for establishing the truth or falsehood of the proximity relation. To address this problem, we analytically quantify the lower and upper bounds {{of the size of the}} smallest circle that covers the mobile objects involved in the proximity relation. Based on this result we propose two novel algorithms that closely monitor the relation at low location update cost. Furthermore, we develop a costeffective estimation technique to determine the probability of match for a given proximity relation. ...|$|R
40|$|Pattern {{detection}} {{is one of}} {{the essential}} challenges in crime mapping and analysis. Data mining can be used to explore crime detection problems. A cluster technique is an effective method for determining areas with high concentrations of localized events. Conversely, it remains a particularly demanding task to detect hotspots with mapping methods in view of the vulnerability connected with the suitable number of groups to create and additionally securing significance of individual clusters identified. Fuzzy clustering means algorithm was used for identifying hotspots of Chicago police department’s citizen law enforcement analysis and reporting system data. In fuzzy clustering, a membership value to each data is assigned, which indicate the strength of relationship between that <b>data</b> <b>points</b> and a specific cluster. In this study each cluster represented the group of global <b>positioning</b> system <b>data</b> <b>points</b> having latitude and longitude as their co-ordinates. The findings from this study were expected to aware the public about crime hotspots. Law enforcement agencies can take prior steps to prevent crime with the use of detected crime hotspots...|$|R
40|$|We {{introduce}} a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each <b>data</b> <b>point</b> in a dataset is {{considered as a}} particle which can move at random in space according to the preset rules in the modified model. Further, this <b>data</b> <b>point</b> may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector {{in terms of the}} feedbacks of all <b>data</b> <b>points,</b> and then its transition direction is identified by an event-generating function. Finally, the <b>positions</b> of all <b>data</b> <b>points</b> are updated. As they move in space, <b>data</b> <b>points</b> collect gradually and some separating parts emerge among them automatically. As a consequence, <b>data</b> <b>points</b> that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that <b>data</b> <b>points</b> in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms. Comment: 21 pages, 13 figure...|$|R
40|$|The {{literature}} on skyline algorithms {{has so far}} dealt mainly with queries of static query points over static datasets. With {{the increasing number of}} mobile service applications and users, however, the need for continuous skyline query processing has become more pressing. A continuous skyline query involves not only static dimensions but also the dynamic one. In this paper, we examine the spatiotemporal coherence of the problem and propose a continuous skyline query processing strategy for moving query points. First, we distinguish the <b>data</b> <b>points</b> that are permanently in the skyline and use them to derive a search bound. Second, we investigate the connection between the spatial <b>positions</b> of <b>data</b> <b>points</b> and their dominance relationship, which provides an indication of where to find changes in the skyline and how to maintain the skyline continuously. Based on the analysis, we propose a kinetic-based data structure and an efficient skyline query processing algorithm. We concisely analyze the space and time costs of the proposed method and conduct an extensive experiment to evaluate the method. To the best of our knowledge, this is the first work on continuous skyline query processing...|$|R
40|$|ORBLIB is a Software {{package that}} {{provides}} {{an easy way}} to produce the navigation files required for photogrammetric restitution of spaceborne stereoscopic images. To this extent <b>position</b> <b>data</b> of the spacecraft and attitude data of the camera are transferred along with associated partial derivatives to the files ORBINPBA and ATTINPBA (1) for inclusion in the photogrammetric processing. As a results of the restitution, corrections to the <b>position</b> and attitude <b>data</b> are obtained that reflect the additional navigational information content of the stereo images. Therefore, the ORBLIB package also provides tools for reading the photogrammetric out- put files ORBOUTBA and ATTOUTBA and generates improved <b>position</b> and <b>pointing</b> <b>data.</b> This report describes in detail the concept and operations of the programs and its components in addition to a format description of the interface files...|$|R
40|$|The University of Wisconsin volume {{imaging lidar}} {{has been used}} to portray images of the {{three-dimensional}} structure of clear air convective plumes in the atmosphere surrounding the flight path of the instrumented Twin Otter aircraft operated by the National Aeronautical Establishment of Canada. Lidar images provide a context for interpretation of the aircraft measurements. The <b>position</b> of <b>data</b> <b>points</b> within a convective element can be determined and the temporal development of the plume can be observed to time the observation with respect to the life cycle of the plume. Plots of the vertical flux of water vapor, superimposed on lidar images clearly demonstrate the well-known sampling difficulties encountered when attempting to measure fluxes {{near the top of the}} convective layer. When loran was used to determine average aircraft velocity, flight-leg-averaged horizontal winds measured by the aircraft and area-averaged winds measured by lidar agree to within 0. 2 m/s in speed and 1 deg in direction...|$|R
40|$|Abstract — The {{literature}} on skyline algorithms {{has so far}} dealt mainly with queries of static query points over static datasets. With {{the increasing number of}} mobile service applications and users, however, the need for continuous skyline query processing has become more pressing. A continuous skyline query involves not only static dimensions but also the dynamic one. In this paper, we examine the spatiotemporal coherence of the problem and propose a continuous skyline query processing strategy for moving query points. First, we distinguish the <b>data</b> <b>points</b> that are permanently in the skyline and use them to derive a search bound. Second, we investigate the connection between the spatial <b>positions</b> of <b>data</b> <b>points</b> and their dominance relationship, which provides an indication of where to find changes in the skyline and how to maintain the skyline continuously. Based on the analysis, we propose a kinetic-based data structure and an efficient skyline query processing algorithm. We concisely analyze the space and time costs of the proposed method and conduct an extensive experiment to evaluate the method. To the best of our knowledge, this is the first work on continuous skyline query processing. Index Terms — Skyline, continuous query processing, moving object databases. I...|$|R
40|$|Isoeffect {{analysis}} of complete dose response curves {{can be used}} to study the interaction of agents in combined modality protocols. When such an analysis is applied to data from in vivo tumour model systems, the effects of the agents on factors such as tumour vasculature, growth or reoxygenation pattern also need to be considered. In this study the change in tumour size, which can occur during a long time-interval between agents, was used {{as an example of a}} factor which may influence the <b>position</b> of <b>data</b> <b>points</b> on an isoeffect plot. Assays of in vivo tumour growth delay and in vitro clonogenic survival were performed to demonstrate that the radiation response curves of EMT- 6 /Ro tumours were size dependent. These curves were used to illustrate that <b>data</b> <b>points</b> obtained from a combined modality treatment may fall outside of the envelope of additivity of an isoeffect plot, as a direct consequence of tumour growth. This finding indicates that it may not be possible to interpret the results from isoeffect analyses of in vivo data on the basis of cellular interactions between agents, and suggests that instead isoeffect analyses be applied primarily to assess the overall response of the tumour system...|$|R
40|$|We study distribution-dependent, data-dependent, {{learning}} in the limit with adversarial disturbance. We consider an optimization-based approach to learning binary classifiers from data under worst-case assumptions on the disturbance. The learning process is modeled as a decision-maker who seeks to minimize generalization error, given access only to possibly maliciously corrupted data. Two models for {{the nature of the}} disturbance are considered: disturbance in the labels of a certain fraction of the data, and disturbance that also affects the <b>position</b> of the <b>data</b> <b>points.</b> We provide distributiondependent bounds on the amount of error {{as a function of the}} noise level for the two models, and describe the optimal strategy of the decision-maker, as well as the worst-case disturbance. ...|$|R
40|$|We use the {{domination}} number of a parametrized random digraph family called proportional-edge proximity catch digraphs (PCDs) for testing multivariate spatial point patterns. This digraph family {{is based on}} relative <b>positions</b> of <b>data</b> <b>points</b> from various classes. We extend the results {{on the distribution of}} {{the domination}} number of proportional-edge PCDs, and use the domination number as a statistic for testing segregation and association against complete spatial randomness. We demonstrate that the domination number of the PCD has binomial distribution when size of one class is fixed while the size of the other (whose points constitute the vertices of the digraph) tends to infinity and asymptotic normality when sizes of both classes tend to infinity. We evaluate the finite sample performance of the test by Monte Carlo simulations, prove the consistency of the test under the alternatives, and suggest corrections for the support restriction on the class of points of interest and for small samples. We find the optimal parameters for testing each of the segregation and association alternatives. Furthermore, the methodology discussed in this article is valid for data in higher dimensions also. Comment: 30 pages, 7 table, and 19 figure...|$|R
40|$|We {{present the}} results of a {{controlled}} experiment to investigate the performance of different temporal glyph designs in a small multiple setting. Analyzing many time series at once is a common yet difficult task in many domains, for example in network monitoring. Several visualization techniques have, thus, been proposed in the literature. Among these, iconic displays or glyphs are an appropriate choice because of their expressiveness and effective use of screen space. Through a controlled experiment, we compare the performance of four glyphs that use different combinations of visual variables to encode two properties of temporal data: a) the <b>position</b> of a <b>data</b> <b>point</b> in time and b) the quantitative value of this <b>data</b> <b>point.</b> Our results show that depending on tasks and data density, the chosen glyphs performed differently. Line Glyphs are generally a good choice for peak and trend detection tasks but radial encodings are more effective for reading values at specific temporal locations. From our qualitative analysis we also contribute implications for designing temporal glyphs for small multiple settings. Author Keywords Glyphs; time series; evaluation; small multiples; informatio...|$|R
40|$|International audienceWe {{present the}} results of a {{controlled}} experiment to investigate the performance of different temporal glyph designs in a small multiple setting. Analyzing many time series at once is a common yet difficult task in many domains, for example in network monitoring. Several visualization techniques have, thus, been proposed in the literature. Among these, iconic displays or glyphs are an appropriate choice because of their expressiveness and effective use of screen space. Through a controlled experiment, we compare the performance of four glyphs that use different combinations of visual variables to encode two properties of temporal data: a) the <b>position</b> of a <b>data</b> <b>point</b> in time and b) the quantitative value of this <b>data</b> <b>point.</b> Our results show that depending on tasks and data density, the chosen glyphs performed differently. Line Glyphs are generally a good choice for peak and trend detection tasks but radial encodings are more effective for reading values at specific temporal locations. From our qualitative analysis we also contribute implications for designing temporal glyphs for small multiple settings...|$|R
40|$|We {{reason the}} {{circumstances}} around the three-dimensional vertex {{from the information}} about the intersection point in the two-dimensional image data inputted from a camera on the premise that we use this method as the eye ot the robot. In this method, we use the Vertex-Dictionary. We make the Vertex-Dictionary by calculation from CAD data of object figure (these data are already known) and the <b>position</b> <b>data</b> of the <b>point</b> of view. This dictionary includes the <b>data</b> of <b>position</b> and relations of connect surface etc. about a vertex. We get the data of three-dimensional vertex by comparison the data of two-dimensional intersection <b>point</b> in image <b>data</b> and Vertex-Dictionary. And we get the three-dimensional object by reasoning about the information of circumstances of all vertexes. Then we can recognize the three-dimensional object from image data. In this report, we explain the process to calculate the Vertex-Dictionary and some examples about this method...|$|R
40|$|Recent {{likelihood}} theory produces $p$-values {{that have}} remarkable accuracy and wide applicability. The calculations use familiar {{tools such as}} maximum likelihood values (MLEs), observed information and parameter rescaling. The usual evaluation of such $p$-values is by simulations, and such simulations do verify that the global distribution of the $p$-values is uniform(0, 1), to high accuracy in repeated sampling. The derivation of the $p$-values, however, asserts a stronger statement, {{that they have a}} uniform(0, 1) distribution conditionally, given identified precision information provided by the data. We take a simple regression example that involves exact precision information and use large sample techniques to extract highly accurate information as to the statistical <b>position</b> of the <b>data</b> <b>point</b> with respect to the parameter: specifically, we examine various $p$-values and Bayesian posterior survivor $s$-values for validity. With observed data we numerically evaluate the various $p$-values and $s$-values, and we also record the related general formulas. We then assess the numerical values for accuracy using Markov chain Monte Carlo (McMC) methods. We also propose some third-order likelihood-based procedures for obtaining means and variances of Bayesian posterior distributions, again followed by McMC assessment. Finally we propose some adaptive McMC methods to improve the simulation acceptance rates. All these methods are based on asymptotic analysis that derives from the effect of additional data. And the methods use simple calculations based on familiar maximizing values and related informations. The example illustrates the general formulas and the ease of calculations, while the McMC assessments demonstrate the numerical validity of the $p$-values as percentage <b>position</b> of a <b>data</b> <b>point.</b> The example, however, is very simple and transparent, and thus gives little indication that in a wide generality of models the formulas do accurately separate information for almost any parameter of interest, and then do give accurate $p$-value determinations from that information. As illustration an enigmatic problem in the literature is discussed and simulations are recorded; various examples in the literature are cited. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Numerous {{methods have}} been used for upward continuation, but most of them require data on a regular grid. Gridding can {{introduce}} errors that affect the continued data in an unpredictable way. To avoid this problem, we design a continuation operator used for the direct continuation of scattered data on a 3 -D basis. In this approach a harmonic function, satisfying the constraints imposed by the measured data, is developed. The continuation is written {{in the form of a}} linear combination of the measured data, but it depends on the arbitrary choice of the topographic zero level. However, the coefficients of the linear combination depend only on the <b>position</b> of the <b>data</b> <b>points.</b> This allows the zero level to be estimated on the basis of the continuation of synthetic anomalies calculated between the starting and ending surface. An important feature of the method is its local character, which allows the reduction of computation time. Also, the stability of the method for noisy data is reasonably good. The method is applied to both synthetic and real cases. Synthetic examples show how gridding‐related errors may affect the continuation when an irregular distribution of <b>data</b> <b>points</b> and a variable topography are considered...|$|R
40|$|Statistical pattern {{classification}} methods {{based on}} data-random graphs were introduced recently. In this approach, a random directed graph is constructed {{from the data}} using the relative <b>positions</b> of the <b>data</b> <b>points</b> from various classes. Different random graphs result from different definitions of the proximity region associated with each <b>data</b> <b>point</b> and different graph statistics can be employed for data reduction. The approach used {{in this article is}} based on a parameterized family of proximity maps determining an associated family of data-random digraphs. The relative arc density of the digraph is used as the summary statistic, providing an alternative to the domination number employed previously. An important advantage of the relative arc density is that, properly re-scaled, it is a $U$-statistic, facilitating analytic study of its asymptotic distribution using standard $U$-statistic central limit theory. The approach is illustrated with an application to the testing of spatial patterns of segregation and association. Knowledge of the asymptotic distribution allows evaluation of the Pitman and Hodges-Lehmann asymptotic efficacies, and selection of the proximity map parameter to optimize efficiency. Furthermore the approach presented here also has the advantage of validity for data in any dimension. Comment: 29 pages, 21 figure...|$|R
40|$|The {{theme of}} this thesis was a {{detailed}} planimetric survey and altimetric surfy {{in a part of}} the basin of Jenín as a map base to monitor long-term landscape changes. Location of this area was performed near the town of Jenín. Jenín is located in the cadastre unit of Jenín, the town of Dolní Dvořiště, Český Krumlov region. The extent of the mapped area is 40 ha. The aim of this study was to make a planimetric map and a topographic map on a scale of 1 : 1000. Terrain reconnaissance and current minor control reconnaissance anticipated the map execution as well as the point monumentation proposal and location of detailed survey points, computational process and cartographic works. The area was located by tacheometry. Location of detailed survey points was settled by polar method, heights were measured trigonometrically. <b>Position</b> <b>data</b> of all <b>points</b> are given in the Uniform Trigonometric Cadastral Network and hypsographic Baltic Vertical system...|$|R
5000|$|Each [...] "view" [...] (i.e., frame) of the {{animation}} is an orthogonal {{projection of the}} data set onto a 2-dimensional subspace of the Euclidean space Rp where the data resides. The subspaces are selected by taking small steps along a continuous curve, parametrized by time, {{in the space of}} all 2-dimensional subspaces of Rp, known as the Grassmannian G(2,p). To display these views on a computer screen, it is necessary to pick one particular rotated position of each view (in the plane of the computer screen) for display. This causes the <b>positions</b> of the <b>data</b> <b>points</b> on the computer screen to appear to vary continuously. Asimov showed that these subspaces can be selected so as to make the set of them (up to time t) increasingly close to all points in G(2,p), so that if the grand tour movie were allowed to run indefinitely, the set of displayed subspaces would correspond to a dense subset of G(2,p).|$|R
30|$|For {{the sake}} of {{simplicity}} and for demonstration purposes, we next include {{the implementation of the}} m-DKLS into a WSN with five motes <b>linearly</b> <b>positioned</b> and equally spaced, and in which each mote communicates with its nearest nodes. The distance between each two motes is 1 m.|$|R
40|$|We {{consider}} two parametrized random digraph families, namely, proportional-edge {{and central}} similarity proximity catch digraphs (PCDs) {{and compare the}} performance of these two PCD families in testing spatial point patterns. These PCD families are based on relative <b>positions</b> of <b>data</b> <b>points</b> from two classes and the relative density of the PCDs {{is used as a}} statistic for testing segregation and association against complete spatial randomness. When scaled properly, the relative density of a PCD is a U-statistic. We extend the distribution of the relative density of central similarity PCDs for expansion parameter being larger than one. We compare the asymptotic distribution of the statistic for the two PCD families, using the standard central limit theory of U-statistics. We compare finite sample performance of the tests by Monte Carlo simulations and prove the consistency of the tests under the alternatives. The asymptotic performance of the tests under the alternatives is assessed by Pitman's asymptotic efficiency. We find the optimal expansion parameters of the PCDs for testing each of the segregation and association alternatives in finite samples and in the limit. We demonstrate that in terms of empirical power (i. e., for finite samples) relative density of central similarity PCD has better performance (which occurs for expansion parameter values larger than one) under segregation alternative, while relative density of proportional-edge PCD has better performance under association alternative. The methods are illustrated in a real-life example from plant ecology. Comment: 56 pages, 42 figure...|$|R
