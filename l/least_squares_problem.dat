840|10000|Public
25|$|These {{differences}} must {{be considered}} whenever the solution to a nonlinear <b>least</b> <b>squares</b> <b>problem</b> is being sought.|$|E
25|$|G. H. Golub and C. F. Van Loan, An {{analysis}} of the total <b>least</b> <b>squares</b> <b>problem.</b> SIAM J. on Numer. Anal., 17, 1980, pp.883–893.|$|E
25|$|R. D. DeGroat and E. M. Dowling, The data <b>least</b> <b>squares</b> <b>problem</b> {{and channel}} equalization. IEEE Trans. Signal Processing, vol. 41, no. 1, pp.407–411, Jan. 1993.|$|E
40|$|The {{method of}} <b>Least</b> <b>Squares</b> {{is due to}} Carl Friedrich Gauss. The Gram-Schmidt {{orthogonalization}} method is of much younger date. A method for solving <b>Least</b> <b>Squares</b> <b>Problems</b> is developed which automatically results in {{the appearance of the}} Gram-Schmidt orthogonalizers. Given these orthogonalizers an induction-proof is available for solving <b>Least</b> <b>Squares</b> <b>Problems...</b>|$|R
40|$|AbstractA {{variant of}} the preconditioned {{conjugate}} gradient method to solve generalized <b>least</b> <b>squares</b> <b>problems</b> is presented. If the problem is min (Ax − b) TW− 1 (Ax − b) with A ∈ Rm×n and W ∈ Rm×m symmetric and positive definite, the method needs only a preconditioner A 1 ∈ Rn×n, but not the inverse of matrix W or of any of its submatrices. Freund's comparison result for regular <b>least</b> <b>squares</b> <b>problems</b> is extended to generalized <b>least</b> <b>squares</b> <b>problems.</b> An error bound is also given...|$|R
40|$|In this paper, {{we present}} a local {{convergence}} analysis of inexact Gauss-Newton like methods for solving nonlinear <b>least</b> <b>squares</b> <b>problems.</b> Under {{the hypothesis that the}} derivative of the function associated with the <b>least</b> <b>square</b> <b>problem</b> satisfies a majorant condition, we obtain that the method is well-defined and converges. Our analysis provides a clear relationship between the majorant function and the function associated with the <b>least</b> <b>square</b> <b>problem.</b> It also allows us to obtain an estimate of convergence ball for inexact Gauss-Newton like methods and some important, special cases...|$|R
25|$|Fitting {{of linear}} models by least squares often, but not always, {{arise in the}} context of {{statistical}} analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear <b>least</b> <b>squares</b> <b>problem.</b>|$|E
2500|$|Often {{it is of}} {{interest}} to solve a linear <b>least</b> <b>squares</b> <b>problem</b> with an additional constraint on the solution. With constrained linear least squares, the original equation ...|$|E
2500|$|M. Plešinger, The Total <b>Least</b> <b>Squares</b> <b>Problem</b> and Reduction of Data in AX ≈ B. Doctoral Thesis, TU of Liberec and Institute of Computer Science, AS CR Prague, 2008.|$|E
40|$|<b>Least</b> <b>squares</b> estimations {{have been}} used {{extensively}} in many applications system identification and signal prediction. These applications, the <b>least</b> <b>squares</b> estimators can usually be found by solving Toeplitz <b>least</b> <b>squares</b> <b>problems.</b> We present fast algorithms for solving the Toeplitz <b>least</b> <b>squares</b> <b>problems.</b> The algorithm is derived by using the displacement representation of the normal equations matrix. Numerical experiments show that these algorithms are efficient. published_or_final_versio...|$|R
40|$|This paper {{introduces}} tensor {{methods for}} solving large, sparse nonlinear <b>least</b> <b>squares</b> <b>problems</b> where the Jacobian either is analytically available or {{is computed by}} finite difference approximations. Tensor methods {{have been shown to}} have very good computational performance for small to medium-sized, dense nonlinear <b>least</b> <b>squares</b> <b>problems.</b> In this paper we consider the application of tensor methods to large, sparse nonlinear <b>least</b> <b>squares</b> <b>problems.</b> This involves an entirely new way of solving the tensor model that is efficient for sparse problems. A number of interesting linear algebraic implementation issues are addressed. The test results of the tensor method applied to a set of sparse nonlinear <b>least</b> <b>squares</b> <b>problems</b> compared with those of the standard Gauss-Newton method reveal that the tensor method is significantly more robust and efficient than the standard Gauss-Newton method. Key Words. tensor methods, sparse problems, large-scale optimization, rank-deficient matrices AMS(M [...] ...|$|R
40|$|AbstractThe Gauss–Newton {{method for}} solving {{nonlinear}} <b>least</b> <b>squares</b> <b>problems</b> is studied in this paper. Under {{the hypothesis that}} the derivative of the function associated with the <b>least</b> <b>square</b> <b>problem</b> satisfies a majorant condition, a local convergence analysis is presented. This analysis allows us to obtain the optimal convergence radius and the biggest range for the uniqueness of stationary point, and to unify two previous and unrelated results...|$|R
2500|$|A total <b>least</b> <b>squares</b> <b>problem</b> {{refers to}} {{determining}} the vector [...] which minimizes the 2-norm of a vector [...] under the constraint [...] The solution {{turns out to}} be the right-singular vector of [...] corresponding to the smallest singular value.|$|E
2500|$|I. Hnětynková, M. Plešinger, D. M. Sima, Z. Strakoš, and S. Van Huffel, The total <b>least</b> <b>squares</b> <b>problem</b> in AX ≈ B. A new {{classification}} {{with the}} relationship to the classical works. SIMAX vol. 32 issue 3 (2011), pp.748–770. Available [...] as a [...]|$|E
2500|$|Depending on how {{the error}} related to each {{constraint}} is measured, {{it is possible to}} determine or estimate an essential matrix which optimally satisfies the constraints for a given set of corresponding image points. [...] The most straightforward approach is to set up a total <b>least</b> <b>squares</b> <b>problem,</b> commonly known as the eight-point algorithm.|$|E
40|$|Abstract — Compressive Sensing {{approach}} allows {{reconstruction of}} under-sampled sparse signals, by using different optimization techniques. These techniques solve undetermined systems of equations {{which may be}} recast as <b>least</b> <b>square</b> <b>problems.</b> Since {{there is a growing}} need for real-time hardware implementations of the reconstruction methods, it is important for these methods to be fast enough and not be computationally demanding. Here, we will focus on QR decomposition based approach for solving <b>least</b> <b>square</b> <b>problems.</b> <b>Least</b> <b>square</b> <b>problem</b> solution is defined in such way that does not require Q matrix, obtained as a result of QR decomposition of the measurement matrix, to be used in calculation and leads to the lower computational complexity...|$|R
40|$|AbstractIn this paper, we {{deal with}} {{conjugate}} gradient methods for solving nonlinear <b>least</b> <b>squares</b> <b>problems.</b> Several Newton-like methods have been studied for solving nonlinear <b>least</b> <b>squares</b> <b>problems,</b> which include the Gauss–Newton method, the Levenberg–Marquardt method and the structured quasi-Newton methods. On the other hand, conjugate gradient methods are appealing for general large-scale nonlinear optimization problems. By combining the structured secant condition {{and the idea of}} Dai and Liao (2001)  [20], the present paper proposes conjugate gradient methods that make use {{of the structure of the}} Hessian of the objective function of nonlinear <b>least</b> <b>squares</b> <b>problems.</b> The proposed methods are shown to be globally convergent under some assumptions. Finally, some numerical results are given...|$|R
40|$|Important {{problems}} in many scientific computational areas are <b>least</b> <b>squares</b> <b>problems.</b> The <b>problem</b> of constraint <b>least</b> <b>squares</b> with full column weight matrix is {{a class of}} these problems. In this presentation, we {{are concerned with the}} connection between the condition numbers and the rounding error in the solution of the problem of constrained and weighted linear <b>least</b> <b>squares.</b> The fact that this problem is an intrinsic feature of <b>least</b> <b>squares</b> <b>problems</b> makes it necessary to study the characteristics of its solution. Investigation of the theoretical characteristics of the solution of our problem is based on perturbing the problem and driving bounds for the relative error. Explicit expressions for the inverse and Moore-Penrose inverse are used to estimate these bounds. Moreover, the effects of weights are presented. AMS classification: Primary 65 F 15; Secondary 65 G 05 Key words and phrases: <b>least</b> <b>squares</b> <b>problems,</b> perturbation, weights...|$|R
2500|$|There is, in some cases, a closed-form {{solution}} to a non-linear <b>least</b> <b>squares</b> <b>problem</b> – but in general there is not. In [...] the case of no {{closed-form solution}}, numerical algorithms are used to find {{the value of the}} parameters [...] that minimizes the objective. [...] Most algorithms involve choosing initial values for the parameters. [...] Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation: ...|$|E
2500|$|The linear <b>least</b> <b>squares</b> <b>problem</b> is to {{find the}} [...] that {{minimizes}} , which is equivalent to projecting [...] to the subspace spanned by the columns of [...] Assuming the columns of [...] (and hence [...] ) are independent, the projection solution is found from [...] Now [...] is square (...) and invertible, and also equal to [...] But the lower rows of zeros in [...] are superfluous in the product, which is thus already in lower-triangular upper-triangular factored form, as in Gaussian elimination (Cholesky decomposition). Here orthogonality is important not only for reducing [...] to , but also for allowing solution without magnifying numerical problems.|$|E
50|$|Computationally, the Cholesky {{decomposition}} is used {{to invert}} the working weight matrices and to convert the overall generalized <b>least</b> <b>squares</b> <b>problem</b> into an ordinary <b>least</b> <b>squares</b> <b>problem.</b>|$|E
40|$|Abstract. Computing the {{solution}} to <b>Least</b> <b>Squares</b> <b>Problems</b> is of great importance {{in a wide range}} of fields ranging from numerical linear algebra to econometrics and optimization. This paper aims to present numerically stable and computationally efficient algorithms for computing {{the solution}} to <b>Least</b> <b>Squares</b> <b>Problems.</b> In order to evaluate and compare the stability and efficiency of our proposed algorithms, the theoretical complexities and numerical results have been analyzed...|$|R
3000|$|... needs 3 JN flops. Solving (L + 1) <b>least</b> <b>square</b> <b>problems</b> using QR {{factorization}} ([22], p. 254) requires [...]...|$|R
30|$|This set of {{algorithms}} represents is the set {{of parallel}} method based on MapReduce to solve the <b>least</b> <b>square</b> <b>problem.</b>|$|R
5000|$|The {{recursive}} {{least squares}} algorithm considers an online {{approach to the}} <b>least</b> <b>squares</b> <b>problem.</b> It can be shown that by initialising [...] and , {{the solution of the}} linear <b>least</b> <b>squares</b> <b>problem</b> given in the previous section can be computed by the following iteration: ...|$|E
50|$|These {{equations}} {{form the}} basis for the Gauss-Newton algorithm for a non-linear <b>least</b> <b>squares</b> <b>problem.</b>|$|E
50|$|These {{differences}} must {{be considered}} whenever the solution to a nonlinear <b>least</b> <b>squares</b> <b>problem</b> is being sought.|$|E
40|$|In this paper, a new {{special class}} of {{splitting}} iterations for solving linear <b>least</b> <b>squares</b> <b>problems</b> in finite dimensions is defined and their main properties of strong global convergence to any problem solution are derived. The investigation results prove the new splitting iterations to be a generalization of the approximating splitting iterations for solving linear <b>least</b> <b>squares</b> <b>problems</b> in finite dimensions, suggesting their suitability for the robust approximate solution of such problems...|$|R
50|$|Mathematically, linear <b>least</b> <b>squares</b> is the <b>problem</b> of {{approximately}} solving an overdetermined system of linear equations, {{where the best}} approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear <b>least</b> <b>squares</b> since the assumed function is linear in the parameters to be estimated. Linear <b>least</b> <b>squares</b> <b>problems</b> are convex and have a closed-form solution that is unique, provided {{that the number of}} data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear <b>least</b> <b>squares</b> <b>problems</b> generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator. In statistics, linear <b>least</b> <b>squares</b> <b>problems</b> correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary <b>least</b> <b>squares</b> model. The present article concentrates on the mathematical aspects of linear <b>least</b> <b>squares</b> <b>problems,</b> with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.|$|R
40|$|An {{effective}} hybrid artificial {{bee colony}} {{algorithm is proposed}} in this paper for nonnegative linear <b>least</b> <b>squares</b> <b>problems.</b> To further improve the performance of algorithm, orthogonal initialization method is employed to generate the initial swarm. Furthermore, to balance the exploration and exploitation abilities, a new search mechanism is designed. The performance of this algorithm is verified by using 27 benchmark functions and 5 nonnegative linear <b>least</b> <b>squares</b> test <b>problems.</b> And the comparison analyses are given between the proposed algorithm and other swarm intelligence algorithms. Numerical results demonstrate that the proposed algorithm displays a high performance compared with other algorithms for global optimization problems and nonnegative linear <b>least</b> <b>squares</b> <b>problems...</b>|$|R
5000|$|... by an {{iterative}} method {{in which each}} step involves solving a weighted <b>least</b> <b>squares</b> <b>problem</b> of the form: ...|$|E
5000|$|In a {{standard}} <b>least</b> <b>squares</b> <b>problem,</b> the estimated parameter values, &beta;, are defined {{to be those}} values that minimise the objective function, S(&beta;), of squared residuals ...|$|E
5000|$|Often {{it is of}} {{interest}} to solve a linear <b>least</b> <b>squares</b> <b>problem</b> with an additional constraint on the solution. With constrained linear least squares, the original equation ...|$|E
25|$|The {{gradient}} equations {{apply to}} all <b>least</b> <b>squares</b> <b>problems.</b> Each particular problem requires particular expressions for the model and its partial derivatives.|$|R
40|$|Robust {{optimization}} is {{a rapidly}} developing methodology for handling optimization problems affected by non-stochastic uncertain-but-bounded data perturbations. In this paper, {{we consider the}} weighted <b>least</b> <b>squares</b> <b>problems</b> where the coefficient matrices and vector belong to different uncertain bounded sets. We introduce the robust counterparts of these problems and reformulate them as the tractable convex optimization problems. Two kinds of approaches for solving the robust counterpart of weighted <b>least</b> <b>squares</b> <b>problems</b> with ellipsoid uncertainty sets are also given...|$|R
25|$|In statistics, linear <b>least</b> <b>squares</b> <b>problems</b> {{correspond}} to a particularly important type of statistical model called linear regression which arises as a {{particular form of}} regression analysis. One basic form of such a model is an ordinary <b>least</b> <b>squares</b> model. The present article concentrates on the mathematical aspects of linear <b>least</b> <b>squares</b> <b>problems,</b> with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being {{dealt with in the}} articles just mentioned. See outline of regression analysis for an outline of the topic.|$|R
