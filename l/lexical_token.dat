8|46|Public
5000|$|... {{might be}} {{converted}} into the following <b>lexical</b> <b>token</b> stream; whitespace is suppressed and special characters have no value: IDENTIFIER net_worth_future EQUALS OPEN_PARENTHESIS IDENTIFIER assets MINUS IDENTIFIER liabilities CLOSE_PARENTHESIS SEMICOLON ...|$|E
50|$|A token or <b>lexical</b> <b>token</b> is a {{structure}} representing a lexeme that explicitly indicates its categorization {{for the purpose}} of parsing. A category of tokens is what might be termed a part-of-speech in linguistics. Examples of token categories may include identifier and integer literal, although the set of token categories differ in different programming languages. The process of forming tokens from an input stream of characters is called tokenization.|$|E
40|$|In {{spite of}} {{considerable}} converging {{evidence of the}} role of inflectional paradigms in word acquisition and processing, little efforts have been put so far into providing detailed, algorithmic models of the interaction between <b>lexical</b> <b>token</b> frequency, paradigm frequency, paradigm regularity. We propose a neurocomputational account of this interaction, and discuss some theoretical implications of preliminary experimental results...|$|E
50|$|In {{computer}} science, identifiers (IDs) are <b>lexical</b> <b>tokens</b> {{that name}} entities. Identifiers are used extensively {{in virtually all}} information processing systems. Identifying entities {{makes it possible to}} refer to them, which is essential for any kind of symbolic processing.|$|R
50|$|A {{language}} construct is a syntactically allowable part of {{a program}} that may be formed from one or more <b>lexical</b> <b>tokens</b> {{in accordance with the}} rules of a programming language. In simpler terms, it is the syntax/way a programming language is written.|$|R
5000|$|Preprocessing. Some languages, e.g., C, {{require a}} {{preprocessing}} phase which supports macro substitution and conditional compilation. Typically the preprocessing phase occurs before syntactic or semantic analysis; e.g. {{in the case}} of C, the preprocessor manipulates <b>lexical</b> <b>tokens</b> rather than syntactic forms. However, some languages such as Scheme support macro substitutions based on syntactic forms.|$|R
40|$|Even {{though there}} are various source code {{plagiarism}} detection approaches, only a few works which are focused on low-level representation for deducting similarity. Most of them are only focused on <b>lexical</b> <b>token</b> sequence extracted from source code. In our point of view, low-level representation is more beneficial than <b>lexical</b> <b>token</b> since its form is more compact than the source code itself. It only considers semantic-preserving instructions and ignores many source code delimiter tokens. This paper proposes a source code plagiarism detection which rely on low-level representation. For a case study, we focus our work on. NET programming languages with Common Intermediate Language as its low-level representation. In addition, we also incorporate Adaptive Local Alignment for detecting similarity. According to Lim et al, this algorithm outperforms code similarity state-of-the-art algorithm (i. e. Greedy String Tiling) in term of effectiveness. According to our evaluation which involves various plagiarism attacks, our approach is more effective and efficient when compared with standard lexical-token approach...|$|E
40|$|Automatic {{mapping of}} key {{concepts}} from clinical notes to a terminology {{is an important}} task to achieve for extraction of the clinical information locked in clinical notes and patient reports. The present paper describes a system that automatically maps free text into a medical reference terminology. The algorithm utilises Natural Language Processing (NLP) techniques to enhance a <b>lexical</b> <b>token</b> matcher. In addition, this algorithm is able to identify negative concepts as well as performing term qualification. The algorithm has been implemented as a web based service running at a hospital to process real-time data and demonstrated that it worked within acceptable time limits and accuracy limits for them. However broader acceptability of the algorithm will require comprehensive evaluations. ...|$|E
40|$|One of the {{ultimate}} goals of natural language processing (NLP) systems is understanding the meaning {{of what is being}} transmitted, irrespective of the medium (e. g., written versus spoken) or the form (e. g., static documents versus dynamic dialogues). Although much work has been done in traditional language domains such as speech and static written text, little has yet been done in the newer communication domains enabled by the Internet, e. g., online chat and instant messaging. This is in part {{due to the fact that}} there are no annotated chat corpora available to the broader research community. The purpose of this research is to build a chat corpus, tagged with <b>lexical</b> (<b>token</b> part-of-speech labels), syntactic (post parse tree), and discourse (post classification) information. Such a corpus can then be used to develop more complex, statistical-based NLP applications that perform tasks such as author profiling, entity identification, and social network analysis...|$|E
5000|$|... = {{the number}} of <b>lexical</b> word <b>tokens</b> (nouns, adjectives, verbs, adverbs) in the {{analysed}} text ...|$|R
40|$|Text mining {{is about}} looking for {{patterns}} in natural language text, {{and may be}} defined as the process of analyzing text to extract information from it for particular purposes. In previous work, we claimed that compression is a key technology for text mining, and backed this up with a study that showed how particular kinds of <b>lexical</b> <b>tokens</b> - names, dates, locations, etc. - can be identified and located in running text, using compression models to provide the leverage necessary to distinguish different token types...|$|R
50|$|This {{stage of}} a multi-pass {{compiler}} is to remove irrelevant information from the source program that syntax analysis {{will not be able}} to use or interpret. Irrelevant information could include things like comments and white space. In addition to removing the irrelevant information, the lexical analysis determines the <b>lexical</b> <b>tokens</b> of the language. This step means that forward declaration is generally not necessary if a multi-pass compiler is used.This phase is focused on breaking a sequence of characters into tokens with attributes such as kind, type, value, and potentially others, as well.|$|R
40|$|We {{describe}} {{a model for}} the lexical analysis of Arabic text, using the lists of alternatives supplied by a broad-coverage morphological analyzer, SAMA, which include stable lemma IDs that correspond to combinations of broad word sense categories and POS tags. We break down each of {{the hundreds of thousands of}} possible lexical labels into its constituent elements, including lemma ID and part-of-speech. Features are computed for each <b>lexical</b> <b>token</b> based on its local and document-level context and used in a novel, simple, and highly efficient two-stage supervised machine learning algorithm that overcomes the extreme sparsity of label distribution in the training data. The resulting system achieves accuracy of 90. 6 % for its first choice, and 96. 2 % for its top two choices, in selecting among the alternatives provided by the SAMA lexical analyzer. We have successfully used this system in applications such as an online reading helper for intermediate learners of the Arabic language, and a tool for improving the productivity of Arabic Treebank annotators. ...|$|E
40|$|Considerable {{evidence}} has accrued {{on the role}} of paradigms as both theoretical and cognitive structures regimenting the way words are processed and acquired. The evidence supports a view of the lexicon as an emergent integrative system, where word forms are concurrently and competitively stored as repeatedly successful processing patterns, and on-line processing crucially depends on the internal organisation of stored patterns. In spite of converging evidence in this direction, little efforts have been put so far into providing detailed, algorithmic models of the interaction between <b>lexical</b> <b>token</b> frequency, paradigm frequency, and paradigm regularity in word processing and acquisition. Here we propose a neuro-computational account of the frequency/regularity interaction, and discuss some of its theoretical implications by analysing experimental results in the computational framework of Temporal Self-Organising Maps. Detailed quantitative analysis shows that the model provides a unitary explanatory framework bringing together insights from neighbour family effects on word recognition and production, evidence from family size effects in serial lexical access and paradigm-based dynamics in lexical acquisition...|$|E
40|$|Lexical pattern {{matching}} and text extraction {{is an essential}} component of many Natural Language Processing applications. Following the language hierarchy first conceived by Chomsky, it is commonly accepted that simple phrasal patterns should he categorised under the class of Regular Language (RL). There are 3 operations in RL- Union, Concatenation and Kleene Closure- which are applied to a finite lexicon. The machinery that recognises RL is the Finite State Machine (FSM). This paper discusses and postulates that the degree with which a class of patterns exercise the aspects of RL operators, is directly proportional to the richness in morphology of <b>lexical</b> <b>tokens.</b> ...|$|R
40|$|Abstract This article {{introduces}} the configurable {{integrated development environment}} ACIDE, which is an ongoing development project currently in alpha status. It is cross-platform, open-source, and free, and will be distributed under GPL. Although targeted to any programming language environment, including compilers, interpreters, and database systems, in particular it is well-suited to tasks required for LATEX and TEX document preparation systems. It manages projects, is useful in dealing with multifile documents, allows configurable menus, has a toolbar for executing commands and <b>lexical</b> <b>tokens</b> for syntax colouring, and even grammars to identify programming (syntactical) errors on the fly...|$|R
40|$|The {{preprocessor}} {{incorporated into}} the C language {{is often used to}} substitute in-line code for function calls using preprocessor macros. However, the semantics of macro calls are different to those for function calls {{and there are a number}} of common pitfalls. This paper examines the most common errors and presents a number of algorithms whereby the compiler can detect these errors at compile-time. The algorithms vary in complexity from a simple analysis of <b>lexical</b> <b>tokens</b> in the preprocessor to examination of the parse tree within the parser. Most of the algorithms have been implemented in a prototype C macro checker called Check. 1...|$|R
5000|$|Lex (and Flex <b>lexical</b> analyser), a <b>token</b> parser {{commonly}} {{used in conjunction with}} Yacc (and Bison).|$|R
40|$|Abstract. This paper {{presents}} a supervised approach for relation extraction. We apply Support Vector Machines {{to detect and}} classify the relations in Automatic Content Extraction (ACE) corpus. We use a set of features including <b>lexical</b> <b>tokens,</b> syntactic structures, and semantic entity types for relation detection and classification problem. Besides these linguistic features, we successfully utilize the distance between two entities to improve the performance. In relation detection, we filter out the negative relation candidates using entity distance threshold. In relation classification, we use the entity distance as a feature for Support Vector Classifier. The system is evaluated in terms of recall, precision, and F-measure, and errors of the system are analyzed with proposed solution. ...|$|R
40|$|Taiwan Mandarin {{syllable}} contraction is a lenition {{process which}} involves the elision of the intervocalic segments and {{the merger of}} the tonal elements of two syllables. In this present study, it is shown that syllable contraction is optional with the trough depth distribution as the evidence. Trough depth is also employed as the measure for gradience. In the perception experiment, listeners were asked to do a forced-choice identification task and the accuracy was generally high. The results from the production experiment not only verifies that syllable contraction is optional and gradient, but also show that durations and F 0 range are the acoustic cues {{that contribute to the}} distinction between the fullycontracted <b>tokens</b> and the <b>lexical</b> <b>tokens...</b>|$|R
50|$|If the XML {{document}} type declaration includes any SYSTEM identifier for the external subset, it can't be safely processed as standalone: the URI should be retrieved, otherwise {{there may be}} unknown named character entities whose definition {{may be needed to}} correctly parse the effective XML syntax in the internal subset or in the document body (the XML syntax parsing is normally performed after the substitution of all named entities, excluding the five entities that are predefined in XML and that are implicitly substituted after parsing the XML document into <b>lexical</b> <b>tokens).</b> If it just includes any PUBLIC identifier, it may be processed as standalone, if the XML processor knows this PUBLIC identifier in its local catalog from where it can retrieve an associated DTD entity.|$|R
40|$|We {{introduce}} {{a notion of}} ordered context-free grammars (OCFGs) with datatype tags to concisely specify grammars of programming languages. Our work {{is an extension of}} syntax definition formalism (SDF) and concrete datatypes that automate scanning, parsing, and syntax tree construction. But OCFGs also capture associativity and precedence at the level of production rules instead of <b>lexical</b> <b>tokens</b> such that a concrete syntax grammar is succinct enough be an abstract syntax definition. By expanding and re-indexing grammar symbols, OCFGs can be translated to grammars for standard lex and yacc such that existing and efficient parsing infrastructures can be reused. We have implemented a Java 5 compiler frontend with OCFGs. The complete grammar for such a realistic language fits comfortably in two pages of this paper, showing the practicality of our formalism. ...|$|R
40|$|The START system {{responds}} to natural language queries with answers in text, pictures, and other media. START’s sentence-level natural language parsing {{relies on a}} number of mechanisms to help it process the huge, diverse resources available on the World Wide Web. Blitz, a hybrid heuristicand corpus-based natural language preprocessor, enables START to integrate a large and ever-changing lexicon of proper names, by using heuristic rules and precompiled tables of symbols to preprocess various highly regular and fixed expressions into <b>lexical</b> <b>tokens.</b> LaMeTH, a contentbased system for extracting information from HTML documents, assists START by providing a uniform method of accessing information on the Web in real time. These mechanisms have considerably improved START’s ability to analyze real-world sentences and answer queries through expansion of its lexicon and integration of Web resources. 1...|$|R
50|$|Macro systems—such as the C {{preprocessor}} described earlier—that work at {{the level}} of <b>lexical</b> <b>tokens</b> cannot preserve the lexical structure reliably.Syntactic macro systems work instead {{at the level of}} abstract syntax trees, and preserve the lexical structure of the original program. The most widely used implementations of syntactic macro systems are found in Lisp-like languages such as Common Lisp, Clojure, Scheme, ISLISP and Racket. These languages are especially suited for this style of macro due to their uniform, parenthesized syntax (known as S-expressions). In particular, uniform syntax makes it easier to determine the invocations of macros. Lisp macros transform the program structure itself, with the full language available to express such transformations. While syntactic macros are often found in Lisp-like languages, they are also available in other languages such as Prolog, Dylan, Scala, Nemerle, Rust, Elixir, Haxe, and Julia. They are also available as third-party extensions to JavaScript, C# and Python..|$|R
3000|$|..., {{they are}} {{properly}} delimited such that this message may be parsed {{even in the}} presence of <b>lexical</b> ambiguity between <b>tokens</b> of the host message and tokens of the encapsulated message.|$|R
40|$|AbstractStructured {{documents}} are usually processed by tree-based document transformers, which transform the document tree representing {{the structure of}} the input document into another tree structure. Event-based document transformers, by contrast, recognize the input as a stream of parsing events, i. e., <b>lexical</b> <b>tokens,</b> and process the events one by one in an event-driven manner. Event-based document transformers have advantages that they need less memory space and that they are more tolerant of large inputs, compared to tree-based transformers, which construct the intermediate tree representation. This paper proposes an algorithm which derives an event-based transformer from a given specification of a document transformation over a tree structure. The derivation of an event-based transformer is carried out in the framework of attribute grammars. We first obtain an attribute grammar which processes a stream of parsing events, by applying a deforestation method; We then derive an attribute evaluation scheme relevant to the event-based transformation. Using this algorithm, one can develop event-based document transformers in a more declarative style than directly programming over the stream of parsing events...|$|R
40|$|Code clones {{are similar}} code {{fragments}} that occur at multiple locations in a software system. Detection of code clones provides useful information for maintenance, reengineering, program understanding and reuse. Several techniques {{have been proposed}} to detect code clones. These techniques differ in the code representation used for analysis of clones, ranging from plain text to parse trees and program dependence graphs. Clone detection based on <b>lexical</b> <b>tokens</b> involves minimal code transformation and gives good results, but is computationally expensive {{because of the large}} number of tokens that need to be compared. We explored string algorithms to find suitable data structures and algorithms for efficient token based clone detection and implemented them in our tool Repeated Tokens Finder (RTF). Instead of using suffix tree for string matching, we use more memory efficient suffix array. RTF incorporates a suffix array based linear time algorithm to detect string matches. It also provides a simple and customizable tokenization mechanism. Initial analysis and experiments show that our clone detection is simple, scalable, and performs better than the previous wellknown tools...|$|R
40|$|Structured {{documents}} are usually processed by tree-based document transformers, which transform the document tree representing {{the structure of}} the input document into another tree structure. Event-based document transformers, by contrast, recognize the input as a stream of parsing events, i. e. <b>lexical</b> <b>tokens,</b> and process the events one by one in an event-driven manner. Event-based document transformers have advantages that they need less memory space and that they are more tolerant of large inputs, compared to tree-based transformers, which construct the intermediate tree representation. This paper proposes an algorithm which derives an event-based transformer from a given specification of a document transformation over a tree structure. The derivation of an event-based transformer is carried out in the framework of attribute grammars. We first obtain an attribute grammar which processes a stream of parsing events, by applying a deforestation method; We then derive an attribute evaluation scheme relevant to the event-based transformation. Using this algorithm, one can develop event-based document transformers in a more declarative style than directly programming over the stream of parsing events...|$|R
50|$|Each node in {{the tree}} is either a root node, a branch node, or a leaf node. A root node is a node that doesn't have any {{branches}} on top of it. Within a sentence, there is only ever one root node. A branch node is a mother node that connects to two or more daughter nodes. A leaf node, however, is a terminal node that does not dominate other nodes {{in the tree}}. S is the root node, NP and VP are branch nodes, and John (N), hit (V), the (D), and ball (N) are all leaf nodes. The leaves are the <b>lexical</b> <b>tokens</b> of the sentence. A mother node {{is one that has}} at least one other node linked by a branch under it. In the example, S is a parent of both N and VP. A daughter node is one that has at least one node directly above it to which it is linked by a branch of a tree. From the example, hit is a daughter node of V. The terms parent and child are also sometimes used for this relationship.|$|R
40|$|As phonological {{processing}} difficulties {{appear to}} be a central feature related to dyslexia, researchers have postulated that one force at work may be an underlying phonological representation that is ‘fuzzy’ or incomplete (Fowler, 1998). Children 2 ̆ 7 s ability to use complex constructions (e. g. consonant clusters), as well as their ability to employ a repertoire of consonants in a variety of phonological neighborhoods may reflect increasing differentiated phonological repertoire at 30 months of age compared with normally reading age-matched controls. In addition, the present study extended Scarborough 2 ̆ 7 s (1990) findings by examining the intelligibility of dyslexic children 2 ̆ 7 s productions and by measuring specific phonological features of the consonants produced in words. The design was retrospective, in that children identified as dyslexic in the early school years were compared with normally reading control children on speech samples obtained when the children were 30 months of age. Findings indicated that the children who were later identified as dyslexic produced a reduced diversity of <b>lexical</b> <b>tokens,</b> fewer multisyllabic words, increased unintelligibility and curtailed contextual use of phonemes. These results support the claim that phonological representations in this group are impaired. ...|$|R
40|$|Example {{programs}} {{are well known}} as an important tool to learn computer programming. Realizing the significance of example programs, this study has been conducted with a goal to measure and evaluate the quality of examples used in academia. We {{make a distinction between}} “good ” and “bad ” examples, as badly designed examples may prove harmful for novice learners. In general, students differ from expert programmers in their approach to read and comprehend a program. How do students understand example programs is explored in the light of classical theories and models of program comprehension. Key factors that impact program quality and comprehension are identified. To evaluate as well as improve the quality of examples, a set of quality attributes is proposed. Relationship between program complexity and quality is examined. We rate readability as a prime quality attribute and hypothesize that example programs with low readability are difficult to understand. Software Reading Ease Score (SRES), a program readability metric proposed by Börstler et al. [5], is implemented to provide a readability measurement tool. SRES is based on <b>lexical</b> <b>tokens</b> and is easy to compute using static code analysis techniques. To validate SRES metric...|$|R
40|$|The {{software}} lexicon is {{an important}} source of information during program comprehension activities and {{it has been in the}} focus of several recent case studies. Identifiers and comments, which constitute a lexicon in software, encode domain concepts and design decisions made by programmers. The paper presents an exploratory study that investigates regularities in the software lexicons of open-source projects by analyzing distributions of tokens in diverse software artifacts. The study examined source code of 142 systems from different domains, written in 12 different programming languages, as well as bug reports and external documentation. We discover that distributions of <b>lexical</b> <b>tokens</b> in studied artifacts follow the Zipf-Mandelbrot law, which is an empirical law in statistical natural language processing. Furthermore, the study reveals that the Zipf-Mandelbrot law is not confined to program lexicons in object-oriented languages, as shown in the previous studies, but also emerges in source code written using procedural, functional and markup languages, as well as other software artifacts. Our study also indicates that a previously devised software science equation does not hold for describing the program vocabulary–length relationship and more studies are necessary. 1...|$|R
50|$|Traditionally, a <b>lexical</b> {{analyzer}} represents <b>tokens</b> (the {{small units}} of indivisible character values) as discrete string objects. This approach is designated extractive parsing. In contrast, non-extractive tokenization mandates that one keeps the source text intact, and uses offsets and lengths to describe those tokens.|$|R
40|$|Abstract. Speakers of {{widespread}} languages may encounter problems in information retrieval and document understanding when they access {{documents in the}} same language from another country. The work described here focuses {{on the development of}} resources to support improved document retrieval and understanding by users of Modern Standard Arabic (MSA). The lexicon of an Egyptian Arabic speaker and the lexicon of an Algerian Arabic speaker overlap, but there are many <b>lexical</b> <b>tokens</b> which are not shared, or which mean different things to the two speakers. These differences give us a context for information retrieval which can improve retrieval performance and also enhance document understanding after retrieval. The availability of a suitable corpus is a key for much objective research. In this paper we present the results of experiments in building a corpus for Modern Standard Arabic (MSA) using data available on the World Wide Web. We selected samples of online published newspapers from different Arabic countries. We demonstrate the completeness and the representativeness of this corpus using standard metrics and show its suitability for Language engineering experiments. The results of the experiments show that is possible to link an Arabic document to a specific region based on information induced from its vocabulary. ...|$|R
40|$|Resources of {{interest}} in digital networks originate {{from a wide variety}} of sources, and may carry identifiers from different established public schemes, official standards, de facto schemes, or private cataloguing numbering. A key step in facilitating preservation, re-use and exchange of information is to enable users to re-use these identifiers (and their associated data) across different applications. Such interoperability of identifiers encompasses not only technical aspects of interoperability but consideration of the purpose and community of use of the identifiers. Interoperability Interoperability is the ability of independent systems to exchange meaningful information and initiate actions from each other, in order to operate together to mutual benefit. In particular, it envisages the ability for loosely-coupled independent systems to be able to collaborate and communicate. Identifiers are <b>lexical</b> <b>tokens</b> that denote things participating in these systems; a referent is the thing that is identified by an identifier. A resource can be part of more than one domain, and can be identified by different systems, so it is necessary to guarantee interoperability between different identification systems as well as implementations based on the same namespace. Identifier interoperability is necessary for purposes such as: • Metadata interoperability (since metadata is a relationship which somebody claims to exist between two referents) ...|$|R
40|$|Modern French society {{continues}} {{to struggle with}} the impacts of multicultural immigration, particularly from former French colonies throughout the twentieth century. November 2005 saw a significant flashpoint in the long-running history of tension between such minority groups and those in power in France. Two teenagers, allegedly while hiding from the police, were electrocuted in a Parisian electrical substation; this was the catalyst for the spread of violent riots in African and Muslim communities across banlieues or urban French districts lasting {{for a number of}} weeks. This paper examines French newspaper representations of the banlieues, focusing on the particularly traumatic events of November 2005. Recognising the active role of the media in constituting and shaping reality, my interest lies not merely in identifying instances of prejudicial discourse in French newspaper texts, but also highlighting how largely covert structures of power and dominance can be disguised in ostensibly neutral discourse. Employing a critical discourse analytical methodology, and incorporating elements of Van Leeuwenʼs (1996) ʻSocial Actor Analysisʼ, this paper critically analyses a corpus of newspaper discourse relating to these events. First a general characterization of the corpus is introduced, and the discourse is examined for instances of othering or ʻthemʼ versus ʻusʼ. Second, a number of articles are examined for the <b>lexical</b> <b>tokens,</b> grammatical structures, quotation patterns and use of metaphor etc. in their constructions of social actors present in the corpus. Observations are drawn with regard to the representation of minorities in the French press in particular, but more generally the role played by the media in constituting and replicating power structures within a society...|$|R
