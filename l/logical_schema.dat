107|63|Public
5000|$|Step 1 - Define the {{appropriate}} <b>logical</b> <b>schema</b> {{for a given}} application ...|$|E
50|$|The <b>logical</b> <b>schema</b> {{was the way}} {{data were}} {{represented}} {{to conform to the}} constraints of a particular approach to database management. At that time the choices were hierarchical and network. Describing the <b>logical</b> <b>schema,</b> however, still did not describe how physically data would be stored on disk drives. That is the domain of the physical schema. Now logical schemas describe data in terms of relational tables and columns, object-oriented classes, and XML tags.|$|E
5000|$|Logical schema: {{describes}} {{the structure of}} some domain of information. This consists of descriptions of (for example) tables, columns, object-oriented classes, and XML tags. The <b>logical</b> <b>schema</b> and conceptual schema are sometimes implemented as one and the same.|$|E
50|$|Data Design. The data expert designs the {{structural}} model, possibly by reverse-engineering the existing <b>logical</b> <b>schemas</b> of legacy data sources.|$|R
5000|$|... #Subtitle level 5: Schema to {{read the}} <b>schemas</b> (<b>Logical</b> Data <b>Schema</b> (LDS)) ...|$|R
5000|$|The {{definition}} of the <b>logical</b> data <b>schema</b> or LDS (the schema to read schemata) ...|$|R
50|$|In the VBS model, each unique {{value in}} the raw data is stored only once; therefore, the data is always {{normalized}} {{at the level of}} unique values. This eliminates the need to normalize data sets in the <b>logical</b> <b>schema.</b>|$|E
50|$|This is {{also why}} {{sharding}} {{is related to}} a shared nothing architecture—once sharded, each shard can live in a totally separate <b>logical</b> <b>schema</b> instance / physical database server / data center / continent. There is no ongoing need to retain shared access (from between shards) to the other unpartitioned tables in other shards.|$|E
5000|$|Physical data independence: The {{ability to}} change the {{physical}} schema without changing the <b>logical</b> <b>schema</b> is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.|$|E
50|$|Jean-Michel Berthelot's {{epistemological}} work {{combined the}} philosophy {{and history of}} science {{in the study of}} sociological theories to understand the logic of construction and justification of sociological knowledge. Berthelot created a typology of sociological explanations, constituted by six <b>logical</b> <b>schemas</b> of intelligibility: causal, actancial, hermeneutic, structural, functionalist and dialectic. These types of explanation were the result of formalization of theory and arguments in the history of sociology.|$|R
30|$|The GWML 2 {{conceptual}} and <b>logical</b> <b>schemas</b> {{are expressed in}} UML, and the physical schema is represented as a GML-XML schema accompanied by associated rules and examples. All three schemas {{are available in the}} GWML 2 SVN repository [11], and the physical schema is also available online from OGC [30]. The three schemas are summarized below, and their full descriptions can be found in the OGC standards specification [4].|$|R
40|$|This paper {{challenges}} the currently popular “Data Warehouse is a Special Animal” philosophy and advocates that practitioners {{adopt a more}} conservative “Data Warehouse = Database ” philosophy. The primary focus is the relevancy of Multi-Dimensional <b>logical</b> <b>schemas.</b> After enumerating the advantages of such schemas, a number of caveats to the presumed advantages are identified. The paper concludes with guidelines and commentary on implications for data warehouse design methodologies. 1...|$|R
50|$|Metadata helps {{perform the}} {{sleight of hand}} that lets users {{interact}} with the system {{in terms of the}} <b>logical</b> <b>schema</b> rather than the physical: the software continually consults the metadata for various operations such as data presentation, interactive validation, bulk data extraction and ad hoc query. The metadata can actually be used to customize the behavior of the system.|$|E
50|$|The data {{dictionary}} contains typical metadata plus additional statistical {{data about the}} tables, columns and occurrences of values in the <b>logical</b> <b>schema.</b> It also maintains information about {{the relationships between the}} logical tables. The index and linking storage includes all of the data used to locate the contents of a record from the ordered values in the data store.|$|E
50|$|A logical {{data model}} or <b>logical</b> <b>schema</b> is a data {{model of a}} {{specific}} problem domain expressed independently of a particular database management product or storage technology (physical data model) {{but in terms of}} data structures such as relational tables and columns, object-oriented classes, or XML tags. This is as opposed to a conceptual data model, which describes the semantics of an organization without reference to technology.|$|E
40|$|The Data Warehouse (DW) is {{considered}} as a repository that contains data collected from different sources. Its design {{is one of many}} issues treated in the literature. It {{is considered}} as the most important since it influences the quality of DW projects. Despite the number of works that have done, the design still suffers from many problems such as the lack of a consistent methodology that assists the user. In this work, we present a survey. It contains a presentation of the conceptual design models and the different <b>logical</b> <b>schemas</b> that exist...|$|R
30|$|The {{correspondence}} between CML and <b>logical</b> <b>schemas</b> is {{partly due to}} the fact that each entity in a CML schema instantiates an entity from the OGC/ISO General Feature Model (GFM; [23]) resulting in both conceptual and technological implications. Conceptually, the GFM describes a spatial meta-type (called FeatureType) and implies its instances (e.g. WaterWell type) are likely to have spatial properties such as location and shape – it thus implies a limited geographical ontology. Technologically, some instances of GFM entities can best be seen as technical artefacts that have more to do with the representation of the entity than about the entity itself. These include mandatory role names for relations, a signature for each operation, and a stereotype for each entity: roles do not necessarily exist within domains, operation signatures are computational necessities, and though stereotypes can be seen as denoting the instantiation of a meta-type, their inclusion is also essential to certain OGC technologies, such as a Web Feature Service operating only on feature types. In fact, some CML schemas are developed around implementation targets and are therefore heavily shaped by technological concerns (e.g. GeoSCiML Lite; [9]). Consequently, the instantiation of representational entities as well as the overall influence of related technologies results in CML schemas with varying technological flavour, making them in this respect quite similar to <b>logical</b> <b>schemas.</b>|$|R
40|$|We analyze inter-model {{transformations of}} {{database}} schemas from a conceptual point of view. A central question {{for us is}} not just whether the information capacity of the transformed schema is sufficient, but rather its suitability for a given task. For this, we re-quire criteria beyond the resulting degree of normal-isation which we subsume under the term “concep-tual justification”. To illustrate our point, we {{take a closer look}} at a class of conceptual requirements that frequently cause practitioners to manually de-normalise the <b>logical</b> schema: layered <b>schemas,</b> where the natural layering of the data clashes with the dom-inant access patterns and negatively impacts perfor-mance. We show how such requirements can directly influence the transformation process and give rise to conceptually justified <b>logical</b> <b>schemas.</b> We include an example which is based on the translation from the higher-order entity-relationship model to the re-lational model...|$|R
50|$|When ANSI first {{laid out}} {{the idea of a}} <b>logical</b> <b>schema</b> in 1975, the choices were {{hierarchical}} and network. The relational model - where data is described in terms of tables and columns - had just been recognized as a data organization theory but no software existed to support that approach. Since that time, an object-oriented approach to data modelling - where data is described in terms of classes, attributes, and associations - has also been introduced.|$|E
5000|$|Although criticised by the Buddhists as eel-wrigglers, in the Pali canon, Buddha is {{depicted}} negating various the four-fold logical alternatives (catuskoti) when posed with metaphysical questions, {{which is similar}} to the logic employed by the Ajñanins. However, all four schools of Ajnanins arrange their logic as five-fold formula, which seems {{to be based on the}} acceptance of the four-fold formula. This may mean that such <b>logical</b> <b>schema</b> was a common feature of the pre-Buddhist era. Alternatively, since there is no known school of Indian thinkers apart from the Buddhist who adopted a four-fold <b>logical</b> <b>schema,</b> and since all Sceptical schools are depicted to have the five-fold formula of denial, which seems to be based on the acceptance of a four-fold form of predication, this may suggest the four-fold schema to be the Buddhist invention. Indeed, two of the foremost disciples of Buddha, Sariputta and Moggallāna, were initially the students of Sanjaya; and a strong element of skepticism is found in early Buddhism, most particularly in the Aṭṭhakavagga sutra. The catuskoti was later used as a tool by Nagarjuna formulate his influential brand of Madhyamaka philosophy.|$|E
5000|$|The {{consistency}} of a relational database is enforced, not by rules {{built into the}} applications that use it, but rather by constraints, declared {{as part of the}} <b>logical</b> <b>schema</b> and enforced by the DBMS for all applications. In general, constraints are expressed using relational comparison operators, of which just one, [...] "is subset of" [...] (⊆), is theoretically sufficient. In practice, several useful shorthands are expected to be available, of which the most important are candidate key (really, superkey) and foreign key constraints.|$|E
40|$|In the {{implementation}} of hosted business services, multiple tenants are often consolidated into the same database to reduce total cost of ownership. Common practice is to map multiple single-tenant <b>logical</b> <b>schemas</b> in the application to one multi-tenant physical schema in the database. Such mappings are challenging to create because enterprise applications allow tenants to extend the base schema, e. g., for vertical industries or geographic regions. Assuming the workload stays within bounds, the fundamental limitation on scalability for this approach {{is the number of}} tables the database can handle. To get good consolidation, certain tables must be shared among tenants and certain tables must be mapped into fixed generic structures such as Universal and Pivot Tables, which can degrade performance. This paper describes a new schema-mapping technique for multi-tenancy called Chunk Folding, where the logical tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. The database’s “meta-data budget ” is divided between application-specific conventional tables and a large fixed set of generic structures called Chunk Tables. Good performance is obtained by mapping the most heavily-utilized parts of the <b>logical</b> <b>schemas</b> into the conventional tables and the remaining parts into Chunk Tables that match their structure as closely as possible. We present the results of several experiments designed to measure the efficacy of Chunk Folding and describe the multi-tenant database testbed in which these experiments were performed...|$|R
50|$|Semantic {{reconciliation}} is {{a process}} cycle constituted of four subsequent activities: scope, create, reﬁne, and articulate. First, the community is scoped: user roles and affordances are appointed. Next, relevant facts are collected from documentation such as, e.g., natural language descriptions, (legacy) <b>logical</b> <b>schemas,</b> or other metadata and consequently decomposing this scope in elicitation contexts. The deliverable of scoping is an initial upper common ontology that organizes the key upper common patterns that are shared and accepted by the community. These upper common patterns deﬁne the current semantic interoperability requirements of the community. Once the community is scoped, all stakeholders syntactically reﬁne and semantically articulate these upper common patterns.|$|R
30|$|Implementation and {{evaluation}} then ensure the developed schemas are implementable in a satisfactory way. Implementation can {{vary according to}} the targeted schema: conceptual and <b>logical</b> <b>schemas</b> are likely implemented in Semantic Web frameworks, e.g. to mediate in data interoperability scenarios or annotate web pages, while physical schemas are likely implemented in data storage or data transfer environments where they are populated with data and used. In all cases evaluation can involve criteria such as deployability, completeness, and usability. Deployability refers to relatively uncomplicated and efficient use within a broad range of information systems. Completeness refers to the degree of implementation of the conceptual schema, as well as to its fit to reference datasets – problems arise if a <b>logical</b> or physical <b>schema</b> does not fully or adequately cover the conceptual schema, or if any schema does not fully or adequately cover the entities identified in the vocabulary or the key contents of target datasets. Usability refers to satisfaction of the usage scenarios and can include a cost/benefit analysis.|$|R
50|$|A {{white pages}} schema is a data model, {{specifically}} a <b>logical</b> <b>schema,</b> for organizing the data contained in entries in a directory service, database, or application, {{such as an}} address book. In a white pages directory, each entry typically represents an individual person that makes use of network resources, such as by receiving email or having an account to log into a system.In some environments, the schema may also include the representation of organizational divisions, roles, groups, and devices. The term {{is derived from the}} white pages, the listing of individuals in a telephone directory, typically sorted by the individual's home location (e.g. city) and then bytheir name.|$|E
50|$|The {{relational}} model of data permits the database designer {{to create a}} consistent, logical representation of information. Consistency is achieved by including declared constraints in the database design, which is usually {{referred to as the}} <b>logical</b> <b>schema.</b> The theory includes a process of database normalization whereby a design with certain desirable properties can be selected from a set of logically equivalent alternatives. The access plans and other implementation and operation details are handled by the DBMS engine, and are not reflected in the logical model. This contrasts with common practice for SQL DBMSs in which performance tuning often requires changes to the logical model.|$|E
5000|$|Logical data {{independence}} {{is the ability}} to modify the <b>logical</b> <b>schema</b> without causing application program to be rewritten. Modifications at the logical level are necessary whenever the logical structure of the database is altered (for example, when money-market accounts are added to banking system). Logical Data independence means if we add some new columns or remove some columns from table then the user view and programs should not change. For example: consider two users A & B. Both are selecting the fields [...] "EmployeeNumber" [...] and [...] "EmployeeName". If user B adds a new column (e.g. salary) to his table, it will not effect the external view for user A, though the internal schema of the database has been changed for both users A & B.|$|E
40|$|Entity Relationship (ER) schemas include {{cardinality}} constraints, that {{restrict the}} dependencies among entities within a relationship type. The cardinality constraints have {{direct impact on}} application transactions, since insertions or deletions of entities or relationships might affect related entities. Application transactions can be strengthened to preserve the consistency of a database {{with respect to the}} cardinality constraints in a schema. Yet, once an ER schema is translated into a <b>logical</b> database <b>schema,</b> the direct correlation between the cardinality constraints and application transaction is lost, since the components of the ER schema might be decomposed among those of the <b>logical</b> database <b>schema.</b> We suggest to extend the Enhanced ER (EER) data model with integrity methods that take the cardinality constraints into account. The integrity methods can be fully defined by the cardinality constraints, using a small number of primitive update methods, and are automatically created for a given EER diagram. A translation of an EER <b>schema</b> into a <b>logical</b> database <b>schema</b> can create integrity routines by translating the primitive update methods alone. These integrity routines may be implemented as database procedures, if a relational DBMS is utilized, or as class methods, if an objectoriented DBMS is utilized...|$|R
40|$|Abstract. The paper {{presents}} a network model {{that can be}} used to produce conceptual and <b>logical</b> <b>schemas</b> for Information Retrieval applications. The model has interesting adaptability characteristics and can be instantiated in various e ectiveways. The paper also reports the results of an experimental investigation into the e ectiveness of implementing associative and adaptive retrieval on the proposed model by means of Neural Networks. The implementation makes use of the learning and generalisation capabilities of the Backpropagation learning algorithm to build up and use application domain knowledge in a sub-symbolic form. The knowledge is acquired from examples of queries and relevant documents. Three di erent learning strategies are introduced, their performance is analysed and compared with the performance of a traditional Information Retrieval system...|$|R
5000|$|Logical data independence: The {{ability to}} change the <b>logical</b> (conceptual) <b>schema</b> without {{changing}} the External schema (User View) is called logical data independence. For example, the addition or removal of new entities, attributes, or relationships to the conceptual schema sternal schemas or having to rewrite existing application programs.|$|R
50|$|The <b>logical</b> <b>schema</b> and its mapping {{with the}} {{physical}} schema is represented as an Entity Data Model (EDM), specified as an XML file. ADO.NET Entity Framework uses the EDM to actually perform the mapping letting the application work with the entities, while internally abstracting the use of ADO.NET constructs like DataSet and RecordSet. ADO.NET Entity Framework performs the joins necessary to have entity reference information from multiple tables, or when a relationship is traversed. When an entity is updated, it traces back which table the information came from and issues SQL update statements to update the tables in which some data has been updated. ADO.NET Entity Framework uses eSQL, a derivative of SQL, to perform queries, set-theoretic operations, and updates on entities and their relationships. Queries in eSQL, if required, are then translated to the native SQL flavor of the underlying database.|$|E
30|$|The <b>logical</b> <b>schema</b> {{implements}} {{some or all}} of {{the conceptual}} schema in a particular technological framework, taking into account information concerns such as data availability. For OGC data standards this follows the rules for application schema development, which highlight alignment with the GFM [22, 23]. Of note is assignment of a GFM meta-type to each domain type (via an UML stereotype such as <<FeatureType>>), and possibly the specialization of each domain type from some standard schema such as Observations and Measurements [7], or from some domain application schema such as GeoSciML. While the selected content of the conceptual schema must be fully incorporated into the <b>logical</b> <b>schema,</b> with original meanings intact, the structural correspondence between these schemas is not necessarily isomorphic as technological needs framing the <b>logical</b> <b>schema</b> might lead to some structural divergence. Documentation and definitions are cascaded to the <b>logical</b> <b>schema</b> from the conceptual schema, and development of the <b>logical</b> <b>schema</b> is also optional, i.e. if the goal of the standard is to develop a conceptual schema only, then the <b>logical</b> <b>schema</b> is obviously unnecessary. The <b>logical</b> <b>schema</b> is represented using a conceptual modelling formalism.|$|E
40|$|Abstract. This paper {{presents}} an architecture for managing database evolution {{when all the}} components of the database (conceptual schema, <b>logical</b> <b>schema</b> and extension) are available. The strategy of evolution in which our architecture is based is that of ‘forward database maintenance’, that is, changes are applied to the conceptual schema and propagated automatically down to the <b>logical</b> <b>schema</b> and to the extension. In order to put into practice this strategy, each component of a database is seen under this architecture as the information base of an information sys-tem. Furthermore, a translation information system is considered in or-der to manage the translation of conceptual elements into <b>logical</b> <b>schema</b> elements. A current Oracle implementation of this architecture is also presented...|$|E
40|$|XML Schema Definition (XSD) is the <b>logical</b> <b>schemas</b> of an XML model, {{but there}} is no {{standard}} format for the conceptual schema of an XML model. Therefore, we propose an XML Tree Model (XTM) as an XML conceptual schema for representing data semantics in a diagram, and also as an XML data model validator for confirming the data semantics required by users. An XTM consists of hierarchical nodes representing all the elements, and the data relationships among elements within the XSD. A rule-based algorithm and an information capacity with pre-and post-conditions are developed as the methodology for reverse engineering. The proposed algorithm consists of two rules: General Information Transformation and Data Semantic Recovering to construct an XTM [...] Users can draw an XTM with data relationships among elements {{as a result of the}} reverse engineering...|$|R
30|$|Fully {{articulated}} design {{methods for}} geospatial data standards are rare, with the most prominent belonging to OGC/ISO and INSPIRE [14, 23]. The OGC/ISO method includes development of an informal or semi-formal description of the domain, establishment of a schema, and integration with other schemas. The INSPIRE method refines these steps to include specification of use-cases, requirements, a data analysis, a <b>logical</b> or physical <b>schema,</b> and testing as well as evaluation guidelines. In each, application schemas that are domain-specific (e.g. for geology) are distinguished from standard schemas that are relatively domain-neutral (e.g. for geometry), with the key difference being that standard schemas are applicable to many domains. Both kinds of schema can be expressed in a conceptual modelling language (CML) such as UML or OWL [22, 24], or a data language (DL) such as GML-XML [32]; additional rules are also established for conversion amongst them [24, 32]. A schema following OGC/ISO protocols and expressed in a CML {{is referred to as}} a conceptual schema by OGC/ISO, but to avoid confusion we henceforth refer to such schemas as CML schemas, and use conceptual schema for the more general data modelling notion. We also suggest CML <b>schemas</b> correspond to <b>logical</b> <b>schemas</b> in data modelling, and DL schemas correspond to physical schemas.|$|R
40|$|The Anticipatory Optimization Generation (AOG) {{system is}} built to {{generate}} custom programs using largescale, programmatic transformations (versus pure pattern-based transformations) that operate on Abstract Syntax Trees (ASTs). Because {{of the scale}} of the AOG transformations and the often-complex program reorganizations that they enable, the transformations must operate on the low-level physical formats of the ASTs and simultaneously accommodate high degrees of variation in the ASTs. However, both the physical details of and variations within an AST are irrelevant to the AOG system in the same sense that both the physical details of and variations within a database format are irrelevant to a database management system. In both cases, only the logical structure is relevant. Database management systems introduce <b>logical</b> <b>schemas</b> to hide the details and variations in the format of their databases. Similarly, the AOG pattern matcher introduces a pattern notation that performs the analogous [...] ...|$|R
