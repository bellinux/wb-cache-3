1292|10000|Public
25|$|Although {{it is true}} that {{analyzing}} {{what has}} been learned by an artificial neural network is difficult, {{it is much easier to}} do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a <b>learning</b> <b>machine</b> to be successful. For example, local vs non-local learning and shallow vs deep architecture.|$|E
2500|$|GPU <b>{{learning}}</b> <b>machine</b> {{learning and}} data mining computations, e.g., with software BIDMach ...|$|E
2500|$|Minsky's inventions {{include the}} first head-mounted {{graphical}} display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). [...] He developed, with Seymour Papert, the first Logo [...] "turtle". [...] Minsky also built, in 1951, the first randomly wired neural network <b>learning</b> <b>machine,</b> SNARC.|$|E
50|$|<b>Machine</b> <b>learning</b> {{can make}} it {{possible}} to recognize the shared characteristics of promotional events and identify their effect on normal sales. <b>Learning</b> <b>machines</b> use simpler versions of nonlinear functions to model complex nonlinear phenomena. <b>Learning</b> <b>machines</b> process sets of input and output data and develop a model of their relationship. Based on this model, <b>learning</b> <b>machines</b> forecast outputs associated with new sets of input data.|$|R
40|$|AbstractGeneralization {{performance}} {{is the main}} purpose of <b>machine</b> <b>learning</b> theoretical research. It has been shown previously by Vapnik, Cucker and Smale that the empirical risks based on an i. i. d.  sequence must uniformly converge on their expected risks for <b>learning</b> <b>machines</b> as the number of samples approaches infinity. In order to study the generalization performance of <b>learning</b> <b>machines</b> under the condition of dependent input sequences, this paper extends these results to the case where the i. i. d.  sequence is replaced by exponentially strongly mixing sequence. We obtain the bound on the rate of uniform convergence for <b>learning</b> <b>machines</b> by using Bernstein’s inequality for exponentially strongly mixing sequences, and establishing the bound on the rate of relative uniform convergence for <b>learning</b> <b>machines</b> based on exponentially strongly mixing sequence. In the end, we compare these bounds with previous results...|$|R
5000|$|<b>Machine</b> <b>Learning</b> - <b>Machine</b> <b>learning</b> {{techniques}} {{can be applied}} also to the decision process.|$|R
50|$|Logic <b>Learning</b> <b>Machine</b> (LLM) is {{a machine}} {{learning}} method {{based on the}} generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, Senior Researcher at the Italian National Research Council CNR-IEIIT in Genoa.Logic <b>Learning</b> <b>Machine</b> is implemented in the Rulex suite.|$|E
5000|$|GPU <b>{{learning}}</b> <b>machine</b> {{learning and}} data mining computations, e.g., with software BIDMach ...|$|E
5000|$|According to {{the output}} type, {{different}} versions of Logic <b>Learning</b> <b>Machine</b> have been developed: ...|$|E
30|$|The Sum-of-Squares {{error is}} optimal for {{training}} supervised <b>learning</b> <b>machines</b> {{in order to}} detect or to classify Gaussian signals. If non-Gaussian interference is assumed in the radar, probably there exists some other error functions which give rise to better results, motivating the study {{to know if they}} fulfil the sufficient condition established in[1, 33]. In this article, one more error function is considered, the Cross-Entropy error. The study demonstrates that the Cross-Entropy error is also suitable to be used for training supervised <b>learning</b> <b>machines</b> in order to approximate the NP detector, even improving the performance of <b>learning</b> <b>machines</b> trained with the Sum-of-Squares error.|$|R
5000|$|S. Haykin, Neural Networks and <b>Learning</b> <b>Machines</b> (3rd Edition), Prentice Hall, 2009 ...|$|R
40|$|This paper {{concerns}} using <b>learning</b> <b>machines</b> for intrusion detection. Two {{classes of}} <b>learning</b> <b>machines</b> are studied: Artificial Neural Networks (ANNs) and Support Vector Machines (SVMs). We show that SVMs {{are superior to}} ANNs for intrusion detection in three critical respects: SVMs train, and run, {{an order of magnitude}} faster; SVMs scale much better; and SVMs give higher classification accuracy...|$|R
5000|$|Logic <b>Learning</b> <b>Machine</b> for regression, {{when the}} output is an integer or real number.|$|E
5000|$|Logic <b>Learning</b> <b>Machine</b> for classification, {{when the}} output is a {{categorical}} variable, which can assume values in a finite set ...|$|E
50|$|God and Golem {{presents}} Wiener's {{ideas on}} machine <b>learning,</b> <b>machine</b> reproduction, {{and the place}} of machines in society, with some religious context.|$|E
5000|$|... 1957: Applied Physics Laboratory AIS {{begins with}} focus on <b>learning</b> <b>machines</b> and {{self-organizing}} systems.|$|R
25|$|Alternatives to {{backpropagation}} include extreme <b>learning</b> <b>machines,</b> no-prop networks, training without backtracking, weightless networks. and non-connectionist neural networks.|$|R
5000|$|... 1986. (with Pat Langley) Editorial: <b>Machine</b> <b>Learning</b> and Discovery. <b>Machine</b> <b>Learning</b> 1: 363-366, 1986. Kluwer Academic Publishers, Boston.|$|R
50|$|In {{the final}} section of the paper Turing details his {{thoughts}} about the <b>Learning</b> <b>Machine</b> that could play the imitation game successfully.|$|E
50|$|Nicholson is {{frequently}} quoted and speaks publicly on {{such topics as}} artificial intelligence, deep <b>learning,</b> <b>machine</b> learning trends in technology, and public relations.|$|E
50|$|Suran Goonatilake, OBE is a Sri Lankan born British academic, {{entrepreneur}} and producer. He is {{a visiting}} professor at the Department of Computer Science at University College London (UCL), with research activities in 'deep tech': artificial intelligence, machine <b>learning,</b> <b>machine</b> vision, robotics, and body-scanning.|$|E
40|$|Abstract. The {{main idea}} of a priori <b>machine</b> <b>learning</b> is to apply a <b>machine</b> <b>learning</b> method on a <b>machine</b> <b>learning</b> problem itself. We call it “a priori” because the {{processed}} data set does not originate from any measurement or other observation. <b>Machine</b> <b>learning</b> which deals with any observation is called “posterior”. The paper describes how posterior <b>machine</b> <b>learning</b> can be modified by a priori <b>machine</b> <b>learning.</b> A priori and posterior <b>machine</b> <b>learning</b> algorithms are proposed for artificial neural network training and are tested in the task of audio-visual phoneme classification. ...|$|R
40|$|We {{conduct an}} {{empirical}} study of <b>machine</b> <b>learning</b> functionalities provided by major cloud service providers, {{which we call}} <b>machine</b> <b>learning</b> clouds. <b>Machine</b> <b>learning</b> clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying {{how to run a}} <b>machine</b> <b>learning</b> task, users only specify what <b>machine</b> <b>learning</b> task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current <b>machine</b> <b>learning</b> clouds on real-world <b>machine</b> <b>learning</b> workloads? We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running <b>machine</b> <b>learning</b> clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing <b>machine</b> <b>learning</b> clouds and points out potential future directions for improvement...|$|R
5000|$|Alternatives to {{backpropagation}} include Extreme <b>Learning</b> <b>Machines,</b> [...] "No-prop" [...] networks, training without backtracking, [...] "weightless" [...] networks." [...] and non-connectionist neural networks.|$|R
5000|$|The {{school at}} Someplace Else {{is ready to}} stage their field trip through the Neighborhood of Make-Believe. But a major {{stumbling}} block emerges when James Michael Jones shows off his <b>learning</b> <b>machine</b> on his head. Rogers talks to blind jazz saxophonist Eric Kloss at the neighborhood library.|$|E
50|$|In this problem, the <b>learning</b> <b>machine</b> {{is given}} pairs of {{examples}} {{that are considered}} similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.|$|E
5000|$|Minsky's inventions {{include the}} first head-mounted {{graphical}} display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). He developed, with Seymour Papert, the first Logo [...] "turtle". Minsky also built, in 1951, the first randomly wired neural network <b>learning</b> <b>machine,</b> SNARC.|$|E
40|$|AbstractContextEnsembles of <b>learning</b> <b>machines</b> and {{locality}} {{are considered}} two important topics {{for the next}} research frontier on Software Effort Estimation (SEE). ObjectivesWe aim at (1) evaluating whether existing automated ensembles of <b>learning</b> <b>machines</b> generally improve SEEs given by single <b>learning</b> <b>machines</b> and which {{of them would be}} more useful; (2) analysing the adequacy of different locality approaches; and getting insight on (3) how to improve SEE and (4) how to evaluate/choose <b>machine</b> <b>learning</b> (ML) models for SEE. MethodA principled experimental framework is used for the analysis and to provide insights that are not based simply on intuition or speculation. A comprehensive experimental study of several automated ensembles, single <b>learning</b> <b>machines</b> and locality approaches, which present features potentially beneficial for SEE, is performed. Additionally, an analysis of feature selection and regression trees (RTs), and an investigation of two tailored forms of combining ensembles and locality are performed to provide further insight on improving SEE. ResultsBagging ensembles of RTs show to perform well, being highly ranked in terms of performance across different data sets, being frequently among the best approaches for each data set and rarely performing considerably worse than the best approach for any data set. They are recommended over other <b>learning</b> <b>machines</b> should an organisation have no resources to perform experiments to chose a model. Even though RTs {{have been shown to be}} more reliable locality approaches, other approaches such as k-Means and k-Nearest Neighbours can also perform well, in particular for more heterogeneous data sets. ConclusionCombining the power of automated ensembles and locality can lead to competitive results in SEE. By analysing such approaches, we provide several insights that can be used by future research in the area...|$|R
40|$|<b>Machine</b> <b>learning</b> is a {{subfield}} {{of artificial}} intelligence concerned with techniques that allow computers {{to improve their}} outputs based on previous experiences (stored as data). What is <b>Machine</b> <b>Learning</b> ◮ <b>Machine</b> <b>learning</b> is a subfield of artificial intelligence concerned with techniques that allow computers to improve their outputs based on previous experiences (stored as data). ◮ Closely related to data mining and often uses techniques from statistics, probability theory, pattern recognition, {{and a host of}} other areas. What is <b>Machine</b> <b>Learning</b> ◮ <b>Machine</b> <b>learning</b> is a subfield of artificial intelligence concerned with techniques that allow computers to improve their outputs based on previous experiences (stored as data). ◮ Closely related to data mining and often uses techniques from statistics, probability theory, pattern recognition, {{and a host of other}} areas. ◮ “More data usually beats better algorithms. ” Anand Rajaraman. What is <b>Machine</b> <b>Learning</b> ◮ <b>Machine</b> <b>learning</b> is a subfield of artificial intelligence concerned with techniques that allow computers to improve their outputs based on previous experiences (stored as data). ◮ Closely related to data mining and often uses techniques from statistics, probability theory, pattern recognition, and a host of other areas. ◮ “More data usually beats better algorithms. ” Anand Rajaraman. ◮ Sample uses of <b>machine</b> <b>learning...</b>|$|R
3000|$|Finally, {{an aspect}} {{that should be}} taken into account in using <b>learning</b> <b>machines</b> to support visual quality {{assessment}} is that the loss function [...]...|$|R
50|$|Stephen M. Omohundro (born 1959) is an American {{scientist}} {{known for}} his research on Hamiltonian physics, dynamical systems, programming languages, machine <b>learning,</b> <b>machine</b> vision, and the social implications of artificial intelligence. His current work uses rational economics to develop safe and beneficial intelligent technologies for better collaborative modeling, understanding, innovation, and decision making.|$|E
5000|$|Omohundro {{started the}} [...] "Vision and Learning Group" [...] at the University of Illinois which {{produced}} 4 Masters and 2 Ph.D. theses. He developed {{a number of}} efficient geometric algorithms for speeding up neural network, machine <b>learning,</b> <b>machine</b> vision, and graphics tasks, several of which are widely used. Omohundro created numerous algorithms based on k-d trees, including the balltree, boxtree, and bumptree geometric data structures.|$|E
50|$|Although {{it is true}} that {{analyzing}} {{what has}} been learned by an artificial neural network is difficult, {{it is much easier to}} do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a <b>learning</b> <b>machine</b> to be successful. For example, local vs non-local learning and shallow vs deep architecture.|$|E
50|$|He {{has served}} as the chair of International Conference on <b>Machine</b> <b>Learning,</b> Intelligent Systems for Molecular Biology and Uncertainty in Artificial Intelligence. He is the Editor-in-Chief for Computational Intelligence (journal), and serves as a member of {{editorial}} board for Journal of <b>Machine</b> <b>Learning</b> Research, <b>Machine</b> <b>Learning</b> (journal) and Journal of Artificial Intelligence Research.|$|R
40|$|Abstract. We {{study the}} leave-one-out and {{generalization}} errors of voting combinations of <b>learning</b> <b>machines.</b> A special case considered is {{a variant of}} bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds {{can be used to}} formally show that, for example, bagging increases the stability of unstable <b>learning</b> <b>machines.</b> We report experiments supporting the theoretical findings...|$|R
40|$|International audienceThe {{success of}} <b>machine</b> <b>learning</b> in many domains crucially relies on human <b>machine</b> <b>learning</b> experts, who select {{appropriate}} features, workflows, <b>machine</b> <b>learning</b> paradigms, algorithms, and their hyperparameters. The {{rapid growth of}} <b>machine</b> <b>learning</b> applications has created a demand for off-the-shelf <b>machine</b> <b>learning</b> methods {{that can be used}} easily and without expert knowledge. We call the resulting research area that targets progressive automation of <b>machine</b> <b>learning</b> AutoML. For example, a recent instantiation of AutoML we’ll discuss is the ongoing ChaLearn AutoML challenge ([URL]...|$|R
