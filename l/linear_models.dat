10000|10000|Public
5|$|SAS was re-designed in SAS 76 {{with an open}} {{architecture}} that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general <b>linear</b> <b>models</b> was also added as was the FORMAT procedure, which allowed developers to customize the appearance of data. In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.|$|E
25|$|The {{rapid and}} {{sustained}} increases in computing power {{starting from the}} second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of <b>linear</b> <b>models,</b> but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized <b>linear</b> <b>models</b> and multilevel models.|$|E
25|$|Techniques from {{linear algebra}} {{are also used}} in {{analytic}} geometry, engineering, physics, natural sciences, computer science, computer animation, advanced facial recognition algorithms {{and the social sciences}} (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by <b>linear</b> <b>models.</b>|$|E
5000|$|The general <b>linear</b> <b>model</b> is a {{statistical}} <b>linear</b> <b>model.</b> It may be written as ...|$|R
50|$|In statistics, {{a proper}} <b>linear</b> <b>model</b> is a <b>linear</b> {{regression}} <b>model</b> {{in which the}} weights given to the predictor variables are chosen {{in such a way}} as to optimize the relationship between the prediction and the criterion. Simple regression analysis is the most common example of a proper <b>linear</b> <b>model.</b> Unit-weighted regression is the most common example of an improper <b>linear</b> <b>model.</b>|$|R
40|$|Abstract – This paper {{describes}} {{a method to}} construct a <b>linear</b> <b>model</b> of a Thyristor-Controlled Series Capacitor (TCSC) by analyzing its frequency response obtained from time-domain simulations of a detailed model of the device. Such <b>linear</b> <b>model</b> represents the low frequency behavior of the device as needed in power system stability studies. The <b>linear</b> <b>model</b> is validated comparing the time-domain simulations obtained using the original detailed TCSC-model, the developed <b>linear</b> <b>model</b> and a previously obtained <b>linear</b> <b>model.</b> The latter one has been built by disturbing the TCSC with two events and identified with Matlab’s System Identifi-cation ToolBox from time-domain simulations. By using a <b>linear</b> <b>model,</b> the computing time can be re-duced significantly compared to simulations with a detailed TCSC-model, maintaining dominant behavior of the TCSC. All simulations are done with the power system simulation software Simpow...|$|R
25|$|Usually a {{constant}} is included {{as one of}} the regressors. For example, we can take x'i1=1 for i=1,...,n. The corresponding element of β is called the intercept. Many statistical inference procedures for <b>linear</b> <b>models</b> require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.|$|E
25|$|Fitting of <b>linear</b> <b>models</b> by {{least squares}} often, but not always, {{arise in the}} context of {{statistical}} analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.|$|E
25|$|Hierarchical <b>linear</b> <b>models</b> (or {{multilevel}} regression) organizes {{the data}} into {{a hierarchy of}} regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.|$|E
40|$|We {{propose a}} new {{parametric}} regression <b>model,</b> Hybrid <b>Linear</b> Regression <b>Model</b> (or simply HLRM), which has partially additive and multiplicative covariate structure. In an ordinary <b>linear</b> regression <b>model</b> or generalized <b>linear</b> <b>model,</b> {{it is assumed}} that the covariates have either an additive or multiplicative effect on the response. A family of HLRM includes an ordinary <b>linear</b> regression <b>model,</b> logarithmic <b>linear</b> <b>model</b> and generalized <b>linear</b> <b>model</b> with normal errors as special cases. In analysis of HLRM, estimating unknown parameters or searching for the best fitting optimal model, we assume the log-normal distribution. Some illustrative analyses applying HLRM to actual data sets are also demonstrated...|$|R
30|$|Many authors {{assume that}} the {{covariance}} matrix is nonsingular in their analysis of this classic <b>linear</b> <b>model.</b> But the number of characteristics that could {{be included in the}} model may be clearly limited by this assumption of nonsingularity. A few authors relax the condition of nonsingularity and consider a singular <b>linear</b> <b>model.</b> For example, Liski et al. [10] and Liu [4] make efficiency comparisons between the OLSE and BLUE in a singular <b>linear</b> <b>model.</b> In the present paper, the singular <b>linear</b> <b>model</b> is further studied.|$|R
40|$|Parameter {{tracking}} {{method was}} used to estimate mathematically the physical parameters of food. Five mathematicalmodels of the food properties were studied: <b>linear</b> <b>model</b> with one parameter, <b>linear</b> <b>model</b> with two parameters, non <b>linear</b> <b>model</b> with two parameters, exponential model with two parameters and non <b>linear</b> <b>model</b> with three parameters. The parameters of the first four mathematical models can be estimated using the parameter tracking method. Analytical solution of the fifth model that indicates whether the parameters will converge to their real values can not be found...|$|R
25|$|In linear regression, the {{relationships}} are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called <b>linear</b> <b>models.</b> Most commonly, the conditional mean of y given {{the value of}} X {{is assumed to be}} an affine function of X; less commonly, the median or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.|$|E
25|$|The {{basic model}} assumes the {{observed}} HDR is the predicted HDR scaled by the weights for each event and then added, with noise mixed in. This generates {{a set of}} linear equations with more equations than unknowns. A linear equation has an exact solution, under most conditions, when equations and unknowns match. Hence one could choose any subset of the equations, with the number equal {{to the number of}} variables, and solve them. But, when these solutions are plugged into the left-out equations, there will be a mismatch between the right and left sides, the error. The GLM model attempts to find the scaling weights that minimize the sum of the squares of the error. This method is provably optimal if the error were distributed as a bell curve, and if the scaling-and-summing model were accurate. For a more mathematical description of the GLM model, see generalized <b>linear</b> <b>models.</b>|$|E
2500|$|Generalized <b>linear</b> <b>models</b> (GLMs) are a {{framework}} for modeling a response variable y that is bounded or discrete. [...] This is used, for example: ...|$|E
40|$|Abstract A <b>linear</b> <b>model</b> {{parameter}} estimation method is proposed based on Bayesian treatment {{in addition to}} the linear least squares,The detail algorithm of one order <b>linear</b> <b>model</b> {{parameter estimation}} was introduced in the paper, it {{can also be used to}} the parameter estimation of multi-order <b>linear</b> <b>model.</b> we estimate the <b>linear</b> <b>model</b> parameter of image edge use this method. experiment result show that it can finish the parameters fast estimation and the detection of image edge depend on few observed data with a high detect precision. Keywords:linear model; parameter estimation; edge detection...|$|R
40|$|In this article, {{estimation}} {{methods of the}} semiparametric generalized <b>linear</b> <b>model</b> known as the generalized partial <b>linear</b> <b>model</b> (GPLM) are reviewed. These methods are based on using kernel smoothing functions in the estimation of the nonparametric component of the model. We derive the algorithms for the estimation process and develop these algorithms for the generalized partial <b>linear</b> <b>model</b> (GPLM) with a binary response...|$|R
30|$|Since {{partially}} <b>linear</b> <b>model</b> has parametric and nonparametric components, {{and it is}} {{more flexible}} than <b>linear</b> <b>model,</b> many authors have been studied it, such as Ahn and Powell (1993), Wang et al. (2007).|$|R
2500|$|Generalized <b>linear</b> <b>models</b> {{allow for}} an {{arbitrary}} link function g that relates {{the mean of}} the response variable to the predictors, i.e. E(y) = g(β′x). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the [...] range of the linear predictor and the range of the response variable.|$|E
2500|$|In {{negative}} binomial regression, {{the distribution}} is specified {{in terms of}} its mean, , which is then related to explanatory variables as in linear regression or other generalized <b>linear</b> <b>models.</b> [...] From the expression for the mean m, one can derive [...] and [...] [...] Then, substituting these expressions in the one for the probability mass function when r is real-valued, yields this parametrization of the probability mass function in terms ofnbsp&m: ...|$|E
2500|$|The general {{linear model}} {{considers}} the situation when the response variable Y {{is not a}} scalar but a vector. Conditional linearity of E(y|x)=Bx is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of Ordinary Least-Squares (OLS) and Generalized Least-Squares (GLS) have been developed. [...] "General linear models" [...] are also called [...] "multivariate linear models". These {{are not the same}} as multivariable <b>linear</b> <b>models</b> (also called [...] "multiple linear models").|$|E
40|$|This article {{provides}} an introduction into {{the statistical analysis}} of neuroimaging data using the general <b>linear</b> <b>model.</b> The analysis allows a flexible use of various models offering {{a wide range of}} statistical tests for the analysis of typical neuroimaging experiments. A short introduction to the general <b>linear</b> <b>model</b> is provided using simple examples. We will focus on matrix formulations of the general <b>linear</b> <b>model</b> to clarify the nature of parameter estimation and ensuing statistical inference, using the extra sum-of-squares principle. The article concludes with a discussion of widely used statistical tests {{within the context of the}} general <b>linear</b> <b>model...</b>|$|R
40|$|Fairly general {{sufficient}} {{conditions are}} given to guarantee that invariant tests about means in the multivariate <b>linear</b> <b>model</b> and the repeated measures model have the correct asymptotic size when the normal assumption under which the tests are derived is relaxed. These conditions {{are the same as}} Huber's condition which guarantees asymptotic validity {{of the size of the}} F-test for the univariate <b>linear</b> <b>model.</b> multivariate <b>linear</b> <b>model</b> repeated measures model asymptotic validity...|$|R
40|$|This paper {{describes}} a discrete <b>linear</b> <b>model</b> for the course-changing manoeuvres of a ship. A non-linear mathematical model of three {{degrees of freedom}} is used. The <b>linear</b> <b>model</b> has been obtained from a parametric model. The coefficients are obtained by means of identification. In order to validate the <b>linear</b> <b>model,</b> a comparison {{has been made of}} the responses of the non-linear model and the identified model for various course-changing manoeuvres...|$|R
2500|$|Lack {{of perfect}} {{multicollinearity}} in the predictors. [...] For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as perfect multicollinearity in the predictor variables. [...] This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. [...] At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. [...] Methods for fitting <b>linear</b> <b>models</b> with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized <b>linear</b> <b>models,</b> do not suffer from this problem.|$|E
2500|$|Linear {{regression}} models are often fitted using the least squares approach, {{but they may}} also be fitted in other ways, such as by minimizing the [...] "lack of fit" [...] in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares loss function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not <b>linear</b> <b>models.</b> Thus, although the terms [...] "least squares" [...] and [...] "linear model" [...] are closely linked, they are not synonymous.|$|E
5000|$|Model {{relationships}} between variables by linear or nonlinear regression, generalized <b>linear</b> <b>models,</b> generalized additive models, generalized linear mixed models or hierarchical generalized <b>linear</b> <b>models,</b> Logistics regression, Multinomial regression; ...|$|E
50|$|In {{standard}} multiple regression, each predictor is {{multiplied by}} a number that is called the beta weight. The prediction is obtained by adding these products along with a constant. When the weights are chosen to give the best prediction by some criterion, the model {{referred to as a}} proper <b>linear</b> <b>model.</b> Therefore, multiple regression is a proper <b>linear</b> <b>model.</b> By contrast, unit-weighted regression is called an improper <b>linear</b> <b>model.</b>|$|R
40|$|The aim of {{this paper}} is to develop an {{accurate}} model of an electro-hydrostatic actuator (EHSA) and to estimate the model parameters. The physical behavior of each components has been analyzed and a <b>linear</b> <b>model</b> has been obtained. Then, the cylinder friction phenomenon and the cylinder chambers volume variations have been considered and a non <b>linear</b> <b>model</b> has been developed. An EHSA prototype has been produced and used to estimate the parameters and to validate the models. The comparisons between simulation results and experimental data show that the <b>linear</b> <b>model</b> well describes the main dynamics of the EHSA and the non <b>linear</b> <b>model</b> fits the data with an high accuracy...|$|R
40|$|This {{thesis is}} intent on using {{mixtures}} of probability distributions in generalized <b>linear</b> <b>model.</b> The theoretical part {{is divided into two}} parts. In the first chapter a generalized <b>linear</b> <b>model</b> (GLM) is defined {{as an alternative to the}} classical <b>linear</b> regression <b>model.</b> The second chapter describes the mixture of probability distributions and estimate of their parameters. At the end of the second chapter, the previous theories are connected into the finite mixture generalized <b>linear</b> <b>model.</b> The last third part is practical and shows concrete examples of these models...|$|R
5000|$|McCullagh is the {{coauthor}} with John Nelder of Generalized <b>Linear</b> <b>Models</b> (1983, Chapman and Hall - {{second edition}} 1989), a seminal {{text on the}} subject of generalized <b>linear</b> <b>models</b> (GLMs) with more than 23,000 citations.|$|E
5000|$|... #Article: Comparison {{of general}} and {{generalized}} <b>linear</b> <b>models</b> ...|$|E
5000|$|... #Subtitle level 3: Reduced-rank vector {{generalized}} <b>linear</b> <b>models</b> ...|$|E
30|$|Hierarchical <b>linear</b> <b>modeling.</b>|$|R
50|$|The general <b>linear</b> <b>model</b> {{incorporates}} {{a number of}} different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test. The general <b>linear</b> <b>model</b> is a generalization of multiple <b>linear</b> regression <b>model</b> to the case of more than one dependent variable. If Y, B, and U were column vectors, the matrix equation above would represent multiple linear regression.|$|R
40|$|Experimental {{results for}} {{conditional}} statistics in a confined turbulent wake {{were compared to}} the <b>linear</b> <b>model</b> and the transported probability density function (PDF) model for predicting the velocity conditioned on scalar mixture fraction and to a <b>linear</b> <b>model</b> for predicting the scalar mixture fraction conditioned on velocity. For velocity conditioned on scalar mixture fraction, the <b>linear</b> <b>model</b> was found to accurately predict the conditional transverse velocity, but for the conditional streamwise velocity, agreement between the experiments and the <b>linear</b> <b>model</b> was poor. The transverse conditional velocity was accurately predicted by the transported PDF model, but for the streamwise conditional velocity, a modified transported PDF model that incorporated two components of the scalar diffusivity tensor had to be used. Finally, {{it was found that}} a <b>linear</b> <b>model</b> for predicting the fluctuating scalar conditioned on velocity performed poorly due to the strongly non-Gaussian scalar PDF at the downstream locations investigated...|$|R
