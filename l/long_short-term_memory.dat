695|5491|Public
25|$|Differentiable neural {{computers}} (DNC) are an NTM extension. They out-performed neural Turing machines, <b>long</b> <b>short-term</b> <b>memory</b> systems and memory networks on sequence-processing tasks.|$|E
25|$|Between 2009 and 2012, {{recurrent}} {{neural networks}} and deep feedforward neural networks {{developed in the}} Schmidhuber's research group, winning eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional <b>long</b> <b>short-term</b> <b>memory</b> (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.|$|E
25|$|Numerous {{researchers}} now use {{variants of}} a deep learning recurrent NN called the <b>long</b> <b>short-term</b> <b>memory</b> (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.|$|E
40|$|We {{present a}} transition-based parser that jointly {{produces}} syntactic and semantic dependencies. It learns {{a representation of}} the entire algorithm state, using stack <b>long</b> <b>short-term</b> <b>memories.</b> Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008 [...] 9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics. Comment: 13 pages, 5 figures, accepted to CoNLL 201...|$|R
40|$|Comunicaci√≥ presentada a SIGDIAL 2017 Conference, the 18 th Annual Meeting of the Special Interest Group on Discourse and Dialogue, celebrada del 15 al 17 d'agost a Saarbrucken, Alemanya. For {{estimating}} the Interaction Quality (IQ) in Spoken Dialogue Systems (SDS), the dialogue history is of significant importance. Previous works included this information manually {{in the form}} of precomputed temporal features into the classification process. Here, we employ a deep learning architecture based on <b>Long</b> <b>Short-Term</b> <b>Memories</b> (LSTM) to extract this information automatically from the data, thus estimating IQ solely by using current exchange features. We show that it is thereby possible to achieve competitive results as in a scenario where manually optimized temporal features have been included. This work is part of a project that has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 645012...|$|R
40|$|We study about a {{mathematical}} network model composed of <b>short-term</b> <b>memory</b> and long-term memory {{in the framework}} of neural network theory. Long-term memory area in the model plays a role to store associative memories for a <b>long</b> time. <b>Short-term</b> <b>memory</b> helps to learn new things and store it in long-term memory area by forming closed circuit with long-term memory. The proposed mathematical network model is constructed based on the closed circuit model founded by many physiologists. We show numerical simulation results performed upon this model and discuss the correspondences with physiological find-mgs. 1...|$|R
2500|$|Apart from <b>long</b> <b>short-term</b> <b>memory</b> (LSTM), other {{approaches}} also added differentiable memory to recurrent functions. For example: ...|$|E
2500|$|<b>Long</b> <b>short-term</b> <b>memory</b> (LSTM) {{networks}} are RNNs that avoid the vanishing gradient problem. LSTM is normally augmented by recurrent gates called forget gates. LSTM networks prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn [...] "very deep learning" [...] tasks that require memories {{of events that}} happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM can handle long delays and signals that have a mix of low and high frequency components.|$|E
50|$|The {{voice of}} Amazon Alexa is {{generated}} by a <b>long</b> <b>short-term</b> <b>memory</b> artificial neural network.|$|E
50|$|The {{hippocampus}} {{is located}} in the medial temporal lobe area of the brain and is responsible for governing spatial memory. In animals, this allows them to have a spatial map of their environment and it uses reference and working memory to accomplish this. It also has important functions that govern <b>long</b> and <b>short-term</b> <b>memory</b> as well as spatial navigation, both of which are required in order for the rat to correctly navigate the maze.|$|R
40|$|This paper {{addresses}} {{the problem of}} Target Activity Detection (TAD) for binaural listening devices. TAD denotes the problem of robustly detecting the activity of a target speaker in a harsh acoustic environment, which comprises interfering speakers and noise (cocktail party scenario). In previous work, {{it has been shown}} that employing a Feed-forward Neural Network (FNN) for detecting the target speaker activity is a promising approach to combine the advantage of different TAD features (used as network inputs). In this contribution, we exploit a larger context window for TAD and compare the performance of FNNs and Recurrent Neural Networks (RNNs) with an explicit focus on small network topologies as desirable for embedded acoustic signal processing systems. More specifically, the investigations include a comparison between three different types of RNNs, namely plain RNNs, <b>Long</b> <b>Short-Term</b> <b>Memories,</b> and Gated Recurrent Units. The results indicate that all versions of RNNs outperform FNNs for the task of TAD...|$|R
40|$|Decoloniality is, in {{the first}} place, a concept whose point of {{origination}} was the Third World. Better yet, it emerged {{at the very moment}} in which the three world division was collapsing and the celebration of the end of history and a new world order was emerging. The nature of its impact was similar to the impact produced by the introduction of the concept of "biopolitics", whose point of origination was Europe. Like its European counterpart, "coloniality" moved to the center of international debates in the non-European world as well as in "former Eastern Europe. " While "biopolitics" moved to center stage in "former Western Europe" (cf., the European Union) and the United States, as well as among some intellectual minorities of the non-European followers of ideas that originated in Europe, but who adapt them to local circumstances, "coloniality" offers a needed sense of comfort to mainly people of color in developing countries, migrants and, in general, to a vast quantitative majority whose life experiences, <b>long</b> and <b>short-term</b> <b>memories,</b> languages and categories of thoughts are alienated to life experience, <b>long</b> and <b>short-term</b> <b>memories,</b> languages and categories of thought that brought about the concept of "biopolitics" to account for mechanisms of control and state regulations...|$|R
5000|$|Apart from <b>long</b> <b>short-term</b> <b>memory</b> (LSTM), other {{approaches}} also added differentiable memory to recurrent functions. For example: ...|$|E
50|$|<b>Long</b> <b>short-term</b> <b>memory</b> (LSTM) {{were invented}} by Hochreiter and Schmidhuber in 1997 and set {{accuracy}} records in multiple applications domains.|$|E
5000|$|CURRENNT - CUDA-accelerated toolkit {{for deep}} <b>Long</b> <b>Short-Term</b> <b>Memory</b> (LSTM) RNN {{architectures}} supporting large data sets not fitting into main memory.|$|E
40|$|Company {{disclosures}} greatly aid in {{the process}} of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as <b>long</b> <b>short-term</b> <b>memories,</b> employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139. 1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives...|$|R
30|$|Numerous {{scientific}} {{studies have been}} carried on Ganoderma lucidum for evaluating the pharmacological activities of the mushroom. This research was performed to evaluate the nootropic effect of the ethanol extract of Ganoderma lucidum in memory impaired swiss albino male mice. The memory was impaired by using scopolamine. Scopolamine is an alkaloidal drug which is extracted out from Datura stramonium. Scopolamine has tendency to impair memory in humans and animals, {{that is why it}} is used to cause <b>long</b> term or <b>short-term</b> <b>memory</b> impairment in rodents [30].|$|R
40|$|Abstract‚ÄîThe {{dynamics}} of cortical cognitive maps developed by selforganization must include {{the aspects of}} <b>long</b> and <b>short-term</b> <b>memory.</b> The behavior of such a neural network is characterized by an equation of neural activity as a fast phenomenon and an equation of synaptic modification as a slow part of the neural system. We present a new method of analyzing the {{dynamics of}} a biological relevant system with different time scales based on the theory of flow invariance. We are able to show {{the conditions under which}} the solutions of such a system are bounded being less restrictive than with the ‚Äìmonotone theory, singular perturbation theory, or those based on supervised synaptic learning. We prove the existence and the uniqueness of the equilibrium. A strict Lyapunov function for the flow of a competitive neural system with different time scales is given and based on it we are able to prove the global exponential stability of the equilibrium point. Index Terms‚ÄîFlow invariance, global exponential stability, multitime scale neural network. I...|$|R
50|$|Differentiable neural {{computers}} (DNC) are an NTM extension. They out-performed Neural turing machines, <b>long</b> <b>short-term</b> <b>memory</b> systems and memory networks on sequence-processing tasks.|$|E
50|$|Apache MXNet is a lean, flexible, and ultra-scalable deep {{learning}} framework that supports {{state of the}} art in {{deep learning}} models, including convolutional neural networks (CNNs) and <b>long</b> <b>short-term</b> <b>memory</b> networks (LSTMs).|$|E
5000|$|<b>Long</b> <b>short-term</b> <b>memory</b> (LSTM) is an {{recurrent}} {{neural network}} (RNN) architecture that remembers values over arbitrary intervals. Stored values are not modified as learning proceeds. RNNs allow {{forward and backward}} connections between neurons.|$|E
40|$|AbstractWe {{studied a}} group of verbal memory specialists to {{determine}} whether intensive oral text memory is associated with structural features of hippocampal and lateral-temporal regions implicated in language processing. Professional Vedic Sanskrit Pandits in India train from childhood for around 10 years in an ancient, formalized tradition of oral Sanskrit text memorization and recitation, mastering the exact pronunciation and invariant content of multiple 40, 000 ‚Äì 100, 000 word oral texts. We conducted structural analysis of gray matter density, cortical thickness, local gyrification, and white matter structure, relative to matched controls. We found massive gray matter density and cortical thickness increases in Pandit brains in language, memory and visual systems, including i) bilateral lateral temporal cortices and ii) the anterior cingulate cortex and the hippocampus, regions associated with <b>long</b> and <b>short-term</b> <b>memory.</b> Differences in hippocampal morphometry matched those previously documented for expert spatial navigators and individuals with good verbal working memory. The findings provide unique insight into the brain organization implementing formalized oral knowledge systems...|$|R
40|$|Associative {{learning}} {{is one of}} the key mechanisms displayed by living organisms in order to adapt to their changing environments. It was early recognized to be a general trait of complex multicellular organisms but also found in "simpler" ones. It has also been explored within synthetic biology using molecular circuits that are directly inspired in neural network models of conditioning. These designs involve complex wiring diagrams to be implemented within one single cell and the presence of diverse molecular wires become a challenge that might be very difficult to overcome. Here we present three alternative circuit designs based on two-cell microbial consortia able to properly display associative learning responses to two classes of stimuli and displaying <b>long</b> and <b>short-term</b> <b>memory</b> (i. e. the association can be lost with time). These designs might be a helpful approach for engineering the human gut microbiome or even synthetic organoids, defining a new class of decision-making biological circuits capable of memory and adaptation to changing conditions. The potential implications and extensions are outlined. Comment: 5 figure...|$|R
40|$|The {{aim was to}} {{determine}} if the memory bias for negative faces that has been demonstrated in depression and dysphoria generalises from <b>long</b> to <b>short-term</b> <b>memory.</b> 29 dysphoric (DP) and 22 non-dysphoric (ND) participants were presented with a series of 64 faces, each featuring one of four emotional expressions (happiness, sadness, anger or neutral affect), and were asked to identify the emotion portrayed. Following a short delay, an array of four faces (the original plus three distracters) was presented and {{the participants were asked to}} identify the target face. Half the arrays featured the same individual with four different expressions (facial expression memory) and half featured four different individuals with the same expression (facial identity memory). At encoding, no group differences were apparent. At memory testing, DP exhibited impaired memory for all types of facial emotion and for facial identity when the faces featured happiness, anger or neutral affect, but not when the faces featured sadness. DP exhibited enhanced memory for faces with sad & angry expressions, and impaired memory for happy faces, relative to neutral. ND exhibited enhanced memory for facial identity when faces were angry, relative to sad or neutral. The implications of these findings are discussed in terms of the literature on MCM biases and social functioning in depression...|$|R
50|$|Prefrontal cortex {{basal ganglia}} working memory (PBWM) is an {{algorithm}} that models working memory in the prefrontal cortex and the basal ganglia. It {{can be compared}} to <b>long</b> <b>short-term</b> <b>memory</b> (LSTM) in functionality, but is more biologically explainable.|$|E
50|$|In 1997, Schmidhuber and Sepp Hochreiter {{published}} {{a paper on}} a type of recurrent neural network which they called <b>long</b> <b>short-term</b> <b>memory.</b> In 2015, this was used in a new implementation of speech recognition in Google's software for smartphones.|$|E
50|$|After all, {{including}} {{working memory}} into neural networks {{is a difficult}} task. There have been several approaches like PBWM or <b>Long</b> <b>short-term</b> <b>memory</b> which have working memory. This 1-2-AX task is good task for these models and both are able to solve the task.|$|E
40|$|Scholars do {{not usually}} {{test for the}} {{duration}} of the effects of mass communication, but when they do, they typically find rapid decay. Persuasive impact may end almost as soon as communication ends. Why so much decay? Does mass communication produce any long-term effects? How should this decay color our understanding of the effects of mass communication? We examine these questions with data from the effects of advertising in the 2000 presidential election and 2006 subnational elections, but argue that our model and results are broadly applicable within the field of political communication. We find that the bulk of the persuasive impact of advertising decays quickly, but that some effect in the presidential campaign endures for at least 6 weeks. These results, which are similar in rolling cross-section survey data and county-level data on actual presidential vote, appear to reflect a mix of memory-based processing (whose effects last only as <b>long</b> as <b>short-term</b> <b>memory</b> lasts) and online processing (whose effects are more durable). Finally, we find that immediate effects of advertising are larger in subnational than presidential elections, but decay more quickly and more completely. [Supplementary material is available for this article. Go to the publisher's online edition of Political Communication for the following free supplemental resource(s) : discussion of methodological issues; results for a alternative specifications of key models; full reports of model results. ]. ¬© 2013 Copyright Taylor & Francis Group, LLC...|$|R
40|$|This paper investigates {{mechanisms}} of <b>short-term</b> <b>memory</b> involved in human sentence processing, focusing on how <b>short-term</b> <b>memory</b> functions are realized and constrained in establishing certain linguistic dependencies. We specifically examine a cue-based retrieval approach to <b>short-term</b> <b>memory</b> (Lewis et al. 2005, 2006) which assumes that...|$|R
40|$|Many {{researchers}} have recently explored the cognitive profile of velocardiofacial syndrome (VCFS), a neurodevelopmental disorder {{linked to a}} 22 q 11. 2 deletion. However, verbal <b>short-term</b> <b>memory</b> {{has not yet been}} systematically investigated. We explored verbal <b>short-term</b> <b>memory</b> abilities in a group of 11 children and adults presenting with VCFS and two control groups, matched on either CA or vocabulary knowledge, by distinguishing <b>short-term</b> <b>memory</b> for serial order and item information. The VCFS group showed impaired performance on the serial order <b>short-term</b> <b>memory</b> tasks compared to both control groups. Relative to the vocabulary-matched control group, item <b>short-term</b> <b>memory</b> was preserved. The implication of serial order <b>short-term</b> <b>memory</b> deficits on other aspects of cognitive development in VCFS (e. g., language development, numerical cognition) is discussed. Peer reviewe...|$|R
50|$|Gated {{recurrent}} units (GRUs) are a gating {{mechanism in}} recurrent neural networks, introduced in 2014. Their performance on polyphonic music modeling and speech signal modeling {{was found to}} be similar to that of <b>long</b> <b>short-term</b> <b>memory.</b> They have fewer parameters than LSTM, as they lack an output gate.|$|E
50|$|The <b>long</b> <b>short-term</b> <b>memory</b> (LSTM) has no {{vanishing}} gradient problem. It works {{even when}} with long delays, {{and it can}} handle signals that have a mix of low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition.|$|E
50|$|The 1-2-AX {{working memory}} task {{is a task}} which {{requires}} working memory to be solved. It {{can be used as}} a test case for learning algorithms to test their ability to remember some old data. This task can be used to demonstrate the working memory abilities of algorithms like PBWM or <b>Long</b> <b>short-term</b> <b>memory.</b>|$|E
40|$|<b>Short-term</b> <b>memory,</b> often {{described}} as working memory, {{is one of the}} most fundamental information processing systems of the human brain. <b>Short-term</b> <b>memory</b> function is necessary for language, spatial navigation, problem solving, and many other daily activities. Given its importance to cognitive function, understanding the architecture of <b>short-term</b> <b>memory</b> is of crucial importance to understanding human behavior. Recent work from several laboratories investigating the entry of information into <b>short-term</b> <b>memory</b> has uncovered a dissociation between encoding processes, those that register information into <b>short-term</b> <b>memory,</b> and consolidation processes, those that solidify the representation within <b>short-term</b> <b>memory.</b> Here I describe the key differences between short-term encoding and consolidation and briefly review what is known about the short-term consolidation process itself. Cognitive function, plausible neural instantiation, and open questions are addressed...|$|R
50|$|Rehearsal is {{the process}} where {{information}} is kept in <b>short-term</b> <b>memory</b> by mentally repeating it. When the information is repeated each time, that information is reentered into the <b>short-term</b> <b>memory,</b> thus keeping that information for another 10 to 20 seconds (the average storage time for <b>short-term</b> <b>memory).</b>|$|R
5000|$|<b>Short-term</b> <b>Memory</b> Tests A form of {{cognitive}} ability test that are exemplified by <b>short-term</b> <b>memory</b> {{tasks such as}} forward digit span and serial rote learning, which do not require mental manipulation of inputs {{in order to provide}} an output. <b>Short-term</b> <b>memory</b> tests lack face validity in predicting job performance.|$|R
