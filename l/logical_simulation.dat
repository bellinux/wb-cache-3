18|55|Public
50|$|In {{the early}} 20th century, relay {{circuits}} {{began to be}} more widely used in automatics, defense of electric and communications systems. Every relay circuit schema for practical use was a distinct invention, because the general principle of simulation of these systems was not known. Shestakov's credit (and independently later Claude Shannon's) is {{the general theory of}} <b>logical</b> <b>simulation,</b> inspired by the rapidly increasing complexity of technical demands. <b>Logical</b> <b>simulation</b> requires solid mathematical foundations. Namely these foundations were originally established by Shestakov.|$|E
50|$|The High-Level Architecture (HLA IEEE 1516) is an IEEE and SISO {{standard}} for simulation interoperability. It defines {{a set of}} services, provided through an API in C++ or Java. The services offer publish/subscribe based information exchange, based on a modular Federation Object Model. There are also services for coordinated data exchange and time advance, based on <b>logical</b> <b>simulation</b> time, as well as synchronization points. Additional services provide transfer of ownership, data distribution optimizations and monitoring and management of participating Federates (systems).|$|E
40|$|One of the {{key factors}} for {{efficiency}} in distributed simulation is the detection of model-inherent concurrency, which is related on the prediction of the future behavior of each <b>logical</b> <b>simulation</b> process. In this paper we present a generalization of traditional approaches to behavioral prediction which provides more accurate predictions in general and {{a high degree of}} scalability in terms of computational and communication overhead. We give a sequential implementation, prove its correctness using concepts from the analysis of real-time process networks and then show how these results can be used in distributed simulation. 1 Introduction One {{of the key}} factors for efficiency in distributed simulation is the detection of model-inherent concurrency, 1 which is related to the prediction of the future behavior of the <b>logical</b> <b>simulation</b> processes (LP), in particular the estimation of their future output. Under the term lookahead this is a key component of LP synchronization in conserva [...] ...|$|E
50|$|Roger Rutman of A. C. Sparkplug {{developed}} and implemented LOGIK, a language for <b>logical</b> design <b>simulation,</b> on the IBM 7090 in January 1964. This compiler used an algorithm that produced efficient code for Boolean expressions.|$|R
40|$|Today's {{technical}} {{systems are}} often very complex. System dynamics are often {{hard to predict}} for humans. However, understanding system behavior is crucial for evaluating design variants and finding errors. One way {{to cope with this}} problem is to build <b>logical</b> or virtual <b>simulations.</b> <b>Logical</b> <b>simulations</b> are often very abstract, but can simulate complex behavioral sequences. Virtual reality (VR) simulation is very good for experiencing the system in a view close to reality. However, it is very often static or has only limited dynamics. Until now both approaches exist in relative isolation. In this paper, we report on our experiences in building a mixed simulation, here a discrete event simulator (DES) is coupled with a virtual reality (VR) environment. We will focus on technical and conceptual challenges, but also present possible use cases for user interaction in this strategy to make more detailed investigations possible. Finally a prototype based on the simulation tool "SLX" and the virtual reality environment "Virtual Development and Training Platform" is used to evaluate the approach...|$|R
40|$|The {{design and}} {{implementation}} cycle of an 8 bit CMOS microprocessor is discussed. The primary steps in the design procedure of the microprocessor consists of instruction selection, instruction encoding and organizational specification. A simple architecture is chosen to allow the emphasis of this investigation is focused upon the entire design procedure, Software behavioral models of functional blocks within the processor are used to validate the architecture. The functional blocks are then replaced with logic circuit models and tested. After <b>logical</b> <b>simulations</b> of all blocks have been completed, physical simulations of the logic circuits are performed using a SPICE like simulator to extract delay characteristics of longest circuit paths. Using this delay information, a preliminary estimate of processor speed is possible. Layout of the processor is generated using the Department of Computer Engineering 2 ̆ 7 s 2 uM CMOS Standard Cell Library...|$|R
40|$|Abstract. We {{present a}} new {{architecture}} for specifying and proving op-timizing compilers {{in the presence}} of shared-memory interactions such as buffer-based system calls, shared-memory concurrency, and separate compilation. The architecture, which is implemented in the context of CompCert, includes a novel interaction-oriented model for C-like lan-guages, and a new proof technique, called <b>logical</b> <b>simulation</b> relations, for compositionally proving compiler correctness with respect to this inter-action model. We apply our techniques to CompCert’s primary memory-reorganizing compilation phase, Cminorgen. Our results are formalized in Coq, building on the recently released CompCert 2. 0. ...|$|E
40|$|Graduation date: 1974 This thesis {{develops}} a simplified and <b>logical</b> <b>simulation</b> model for an amplidyne excitation {{system for a}} synchronous generator with an automatic voltage regulator. The non-transfer function representation using field determined parameters including variable inductance and machine saturation was used to reduce the computing time for the stability analysis. The simulation of the above model was programmed in Fortran, The simulation results are compared with actual data taken in the laboratory. From the results {{it was observed that}} steady state voltage for the model and the actual system was within 3. 0...|$|E
40|$|Model {{continuity}} {{refers to}} the ability to use the same model of a system throughout its design phases. For intelligent systems, we can restrict such continuity to the intelligent control components, and more specifically, the models that implement the system's decision making. behavior. In this paper, we show how a modeling and simulation environment, based on the DEVS formalism,, can support model continuity in the design of intelligent systems. For robotic systems, such continuity allows design and testing of the same control logic model through the phases including <b>logical</b> <b>simulation,</b> real-time simulation and actual execution. KEYWORDS: Model Continuity, Modeling, Simulation, Experimental Frame, Real Time Systems, Intelligent Systems, DEVS 1...|$|E
5000|$|Library Database - Consists of {{a number}} of views often {{including}} layout, schematic, symbol, abstract, and other <b>logical</b> or <b>simulation</b> views. From this, various information may be captured in a number of formats including the Cadence LEF format, and the Synopsys Milkyway format, which contain reduced information about the cell layouts, sufficient for automated [...] "Place and Route" [...] tools.|$|R
40|$|Index-based {{checkpointing}} {{allows the}} use of simple and efficient algorithms for domino -effect free construction of recovery lines. In this paper, we use a simulation toolkit to analyze the behavior of index-based algorithms. We present a performance study of the well-known algorithm proposed by Briatico, Ciuffoletti, and Simoncini and explore the impact of some optimizations of this algorithm presented in the literature. Our results indicate that an expensive and complex optimization may not {{reduce the number of}} forced checkpoints in comparison to a simpler optimization. Keywords: distributed checkpointing, rollback recovery, <b>logical</b> clocks, <b>simulation</b> of distributed systems. ...|$|R
50|$|The {{design of}} the Amdahl 4745 was such that the control program (NCP) could not tell whether it was {{operating}} in an IBM 3745 or in an Amdahl 4745 (or, for that matter, in an IBM 3725 or in an Amdahl 4725). Amdahl's <b>logical</b> and physical <b>simulation</b> of the IBM 3745 (IBM 3725) was that complete.|$|R
40|$|The {{primary purpose}} of this thesis was to design a <b>logical</b> <b>simulation</b> of a {{communication}} sub block {{to be used in}} the effective communication of digital data between the host and the peripheral devices. The module designed is a Serial interface engine in the Universal Serial Bus that effectively controls the flow of data for communication between the host and the peripheral devices with the emphasis on the study of timing and control signals, considering the practical aspects of them. In this study an attempt was made to realize data communication in the hardware using the Verilog Hardware Description language, which is supported by most popular logic synthesis tools. Various techniques like Cyclic Redundancy Checks, bit-stuffing and Non Return to Zero are implemented in the design to provide enhanced performance of the module...|$|E
40|$|This work {{systematically}} {{develops a}} distributed simulation mechanism {{based on the}} optimistic strategy (Time Warp) to study timed transition Petri net (TTPN) models. The approach addresses the problem of partitioning the simulation model into logical processes to be run concurrently on individual processing nodes in a distributed memory multiprocessor system. We propose a partitioning such that several transitions together with all their input places are simulated by a single logical process, {{in order to avoid}} the substantial overhead induced by a distributed conflict resolution protocol in a message passing environment. Subnets are constructed from the topologic description of the TTPN so that conflict resolution always occurs internally to a logical process, thus involving no communication overhead. Additional aggregation of conflict sets into larger <b>logical</b> <b>simulation</b> processes may increase the balancing of the distributed simulation without decreasing its inherent parallelism if som [...] ...|$|E
40|$|The {{technique}} {{introduced in}} this paper is based on <b>logical</b> <b>simulation</b> and functional complex models, which will analyze, estimate and optimize {{the parameters of the}} open cast transportation system. For this purpose, flexible truck allocation has first been carried out by a goal programming model. The criteria to be optimized are maximization of production while controlling the quality of ore, which is sent to the processing plant. The transportation system has then been simulated by Arena simulator. This simulator and executive software has the advantages of schematic displacing of the real system. In this software, creating a mineral system perspective, determining the location of the equipments and their distances from each other is possible by using appropriate monitoring. The results of simulations have shown that using the proposed allocation technique, a substantial increase in production and productivity of an open cast mine is achieved...|$|E
40|$|Parameter-driven {{simulations}} are {{an effective}} and efficient method for reasoning about {{a wide range of}} commonsense scenarios that can complement the use of logical formalizations. The advantage of simulation is its simplified knowledge elicitation process: rather than building complex <b>logical</b> formulae, <b>simulations</b> are constructed by simply selecting numerical values and graphical structures. In this paper, we propose the application of machine learning techniques to allow an embodied autonomous agent to automatically construct appropriate simulations from its real-world experience. The automation of learning can dramatically reduce the cost of knowledge elicitation, and therefore result in models of commonsense with breadth and depth not possible with traditional engineering of logical formalizations...|$|R
40|$|Abstract—We {{exposed a}} flash-based FPGA to {{radiation}} to measure variations in current, temperature, propagation-delay and duty-cycle in logic circuits. Propagation-delay degradations vary from 400 % to 1100 % before functional failure, according to circuit and <b>logical</b> mapping. Electrical <b>simulations</b> {{are carried out}} to study the difference of behavior in the degradation of different logic mappings. Index Terms—Flash-based FPGA, propagation-delay degrada-tion, radiation effects, total ionizing dose (TID). I...|$|R
40|$|Comirit is a {{framework}} for commonsense reasoning that combines <b>simulation,</b> <b>logical</b> deduction and passive machine learning. While a passive, observation-driven approach to learning is safe and highly conservative, it is limited to interaction only with those objects that it has previously observed. In this paper we describe a preliminary exploration of methods for extending Comirit to allow safe action selection in uncertain situations, and to allow reward-maximizing selection of behaviors...|$|R
40|$|We {{present a}} novel {{application}} of automated theorem proving for the <b>logical</b> <b>simulation</b> of evolvable systems. Modelled using a logical framework, these systems are built hierarchically from components where each component is specified {{as a first}} order theory and may have an associated supervisory component. The supervisory component monitors and possibly changes its associated component. The simulation of this framework makes intensive use of automated theory proving – when running a simulation, almost all computational steps are those of a theorem prover. We present this novel combination of a logical setting involving meta-level logics and large sets of formulae for system description, together with theorem proving requirements which involve often slowly changing specifications {{with the need for}} rapid assessment of deducibility and consistency. We illustrate how theorem provers are used using an evolvable extension of the blocks world and present a caching structure to reduce simulation times. We then evaluate the suitability of several theorem provers for this application. ...|$|E
40|$|An {{adaptive}} synthesis {{method for}} hydraulic circuits is proposed by using case-based reasoning technique {{based on their}} functional structure. Since synthesis of hydraulic circuits is a typical problem of conceptual design and its property is expertise-oriented and combinatorial, {{it is difficult to}} declaratively represent useful design knowledge. In our design method, past design cases are afore-stored in the case base, and then a circuit is generated by arranging suitable past cases which are retrieved from the case base in accordance with the difference related to functional structure. In order to measure such functional structure, design specifications are represented with labels. Design cases are adaptively manipulated into a target design with three synthesis strategies, decomposition, series combination and parallel combination, based on surplus or deficit of labels. Moreover, arranged circuits are verified with a <b>logical</b> <b>simulation</b> method based on symbol manipulation. These synthesis and simulation methods are applied to some design problems in order to show their effectiveness and validity. ...|$|E
40|$|A {{probabilistic}} distributed discrete event simulation {{strategy is}} {{developed as a}} performance efficient compromise between the two classical approaches in parallel and distributed simulation, the conservative and the optimistic approach. It weakens the conservative "block until safe-to-process"- rule {{in a sense that}} if the time instant of the occurrence of an external event is in the time interval [s; t], it allows progressing simulation up until b t(O), s b t(O) t, where b t(O) is an estimate based on the arrival instants O = (o 1; o 2; : : : on) observed during a time window by some <b>logical</b> (<b>simulation)</b> process. Compared to the optimistic strategy it prevents from propagating incorrect computations too far ahead into the simulated future, and thus avoids unnecessary communication overhead by breaking rollback cascades as early as possible. Moreover, the arrival patterns observed in O are used to adapt the logical process to a synchronization behavior that is the best tradeoff am [...] ...|$|E
40|$|We study {{effects of}} the {{physical}} realization of quantum computers on their <b>logical</b> operation. Through <b>simulation</b> of physical models of quantum computer hardware, we analyse the difficulties that are encountered in programming physical implementations of quantum computers. We discuss {{the origin of the}} instabilities of quantum algorithms and explore physical mechanisms to enlarge the region(s) of stable operation. Comment: 24 pages, 4 figures; major revision, 3 figures remove...|$|R
40|$|A {{considerable}} amount of effort in the DIS community {{has been devoted to}} developing efficient, scaleable, mechanisms for distributing state updates and interaction information in distributed simulations. By contrast, this question has not received as much attention for distributed <b>simulations</b> using <b>logical</b> time (e. g., analytic simulations). It is observed that data distribution management (DDM) mechanisms used for real-time training simulations such as DIS are insufficient to meet the requirements of <b>logical</b> time-based <b>simulations,</b> and may result in errors such as messages not being delivered to federates that have subscribed for them, even if the network provides reliable delivery. An approach to achieving properly synchronized data distribution is described, and is applied to the data distribution management mechanisms based on routing spaces that has been proposed for the HLA. 1. INTRODUCTION Data distribution management (DDM) mechanisms are necessary to provide efficient, scalabl [...] ...|$|R
40|$|In {{this paper}} the logical process {{structure}} of the Chandy-Misra parallel simulation algorithm has been studied. The excessive synchronization caused by the algorithm is avoided by minimizing the number of connections between logical processes. The amount of connections are reduced by restricting the direct communication of the logical processes to their neighborhoods. Two methods, message path and broadcast messages, are proposed to solve the communication between non-connected <b>logical</b> processes. <b>Simulation</b> experiments are carried out in a previously implemented distributed simulation environment, Diworse. A GSM network {{is used as a}} simulation application and carrier per interference (C/I) values for GSM channels are used for measuring the correctness of the proposed methods. Execution time of the simulation and C/I errors are measured in order to find out the effect of connection reduction. The results indicate that both the broadcast message and message path method enable faster simu [...] ...|$|R
40|$|Nano-Magnetic Logic (NML) is a {{promising}} candidate to substitute CMOS technology {{since it is}} characterized by very low power consumption and it can combine computation and memory in the same device. Several works analyze this technology at device level; nevertheless a higher level analysis is required to fully understand its potentials. It is actually fundamental to analyze how an architecture of realistic complexity can be really implemented {{taking into account the}} physical limits due to technology, and which performance it could consequently reach. We present here a physical design and test methodology based on our tool ToPoliNano, which allows analyzing circuits using models specifically targeted for this technology. We developed an automatic engine for placing and routing combinational NML circuits including as constraints realistic rules due to currently available fabrication processes. After the place and route phase, ToPoliNano also allows to perform a circuit <b>logical</b> <b>simulation,</b> detailed at the single nanomagnet level. Furthermore this tool has the ability to analyze and test circuits based on NML, considering the impact that process variations and faults have on the logical behavior of the circuit...|$|E
40|$|<b>Logical</b> <b>simulation</b> is {{the primary}} method to verify the {{correctness}} of IC designs. However, today’s complex VLSI designs pose ever higher demand for the throughput of logic simulators. In this work, a parallel logic simulator was developed by leveraging the com-puting power of modern graphics processing units (GPUs). To expose more parallelism, we implemented a conservative parallel simulation approach, the CMB algorithm, on NVidia GPUs. The simulation processing is mapped to GPU hardware at the finest granularity. With carefully designed data structures and data flow organizations, our GPU based simulator could overcome many problems that hindered efficient implementations of the CMB algorithm on traditional parallel computers. In order to efficiently use the relatively limited capacity of GPU memory, a novel mem-ory management mechanism was proposed to dynamically allo-cate and recycle GPU memory during simulation. We also intro-duced a CPU/GPU co-processing strategy for the best usage of computing resources. Experimental results showed that our GPU based simulator could outperform a CPU baseline event driven simulator {{by a factor of}} 29. 2...|$|E
40|$|Following a {{discussion}} of {{the strengths and weaknesses of}} the current modeling paradigms used for movement simulation in movement ecology, a hybrid simulation model is proposed that jointly exploits the benefits offered by agent-based models (ABM), discrete event simulation (DES), and system dynamics (SD), respectively, while attempting to limit their drawbacks. We describe the transition from a conceptual model of movement to the logical structure that is able to support the hybrid simulation model. We use examples from ornithology to instantiate the components of the logical model. Compared to traditional movement simulation methods such as correlated random walk, the proposed model can provide a more holistic representation of the movement of objects within their environment, while also maintaining the perspective of the individual object. We argue that this multi-level approach and flexibility is possible through the combination of the capabilities of ABM to model interactions among individuals, with the strengths of DES to model discrete events and global rules, and finally with the capacity of SD to model causality and feedback loops. Additionally, the motivation of an individual, being a core driver of movement, has been embedded into the <b>logical</b> <b>simulation</b> model...|$|E
40|$|Probabilistic automata exhibit both {{probabilistic}} and non-deterministic choice. They {{are therefore}} a powerful semantic foundation for modeling concurrent systems with random phenomena arising in many applications ranging from artiﬁcial intelligence, security, systems biology to performance modeling. Several variations of bisimulation and simulation relations {{have proved to}} be useful as means to abstract and compare different automata. We consider logical characterizations of bisimulation and simulation relations. Thus shedding light on the class of properties preserved by these relations. This paper develops a taxonomy of <b>logical</b> characterizations of <b>simulation</b> and bisimulation on image-ﬁnite and image-inﬁnite probabilistic automata...|$|R
40|$|In {{recent years}} {{there has been}} a growing {{recognition}} of the need for developing energy efficient network design approaches for WDM backbone networks as well. The typical approach has been to switch off some components such as line cards and router ports during low demand periods, and has focussed on traditional static and dynamic traffic models. In this paper, we present a new approach that exploits knowledge of demand holding times to intelligently share resources among non-overlapping demands and reduce the overall power consumption of the network. We consider the fixed-window scheduled traffic model (STM), and present i) a Genetic Algorithm (GA) and ii) a Memetic Algorithm (MA) based strategy that jointly minimizes both power consumption and transceiver cost for the <b>logical</b> topology. <b>Simulation</b> results clearly demonstrate that both of the proposed algorithms outperform traditional holding time unaware (HTU) approaches; the GA leads to additional improvements even compared to the shortest path holding time aware (HTA) heuristic. However, the MA manages to achieve similar results to the GA while taking up 4 to 5 times less computational resources and time to compute...|$|R
40|$|We study {{effects of}} the {{physical}} realization of quantum computers on their <b>logical</b> operation. Through <b>simulation</b> of physical models of quantum computer hardware, we analyze the difficulties that are encountered in programming physical realizations of quantum computers. Examples of logically identical implementations of the controlled-NOT operation and Grover’s database search algorithm are used {{to demonstrate that the}} results of a quantum computation are unstable with respect to the physical realization of the quantum computer We discuss the origin of these instabilities and discuss possibilities to overcome this, for practical purposes, fundamental limitation of quantum computers. PACS numbers: 03. 67. Lx, 05. 30. -d, 89. 80. +h, 02. 70 Lq I...|$|R
40|$|We {{present a}} {{comparison}} of 14 galaxy formation models: 12 different semi-analytical models and 2 halo occupation distribution models for galaxy formation based upon the same cosmo- <b>logical</b> <b>simulation</b> and merger tree information derived from it. The participating codes {{have proven to be}} very successful in their own right but they have all been calibrated independently using various observational data sets, stellar models, and merger trees. In this paper, we apply them without recalibration and this leads {{to a wide variety of}} predictions for the stellar mass function, specific star formation rates, stellar-to-halo mass ratios, and the abundance of orphan galaxies. The scatter is much larger than seen in previous comparison studies primarily be- cause the codes have been used outside of their native environment within which they are well tested and calibrated. The purpose of the ‘nIFTy comparison of galaxy formation models’ is to bring together as many different galaxy formation modellers as possible and to investigate a common approach to model calibration. This paper provides a unified description for all participating models and presents the initial, uncalibrated comparison as a baseline for our future studies where we will develop a common calibration framework and address the extent to which that reduces the scatter in the model predictions seen here...|$|E
40|$|The OPERA Platform is a CORBA-based {{distributed}} real-time computing {{platform to}} easily couple existing simulation models and run {{them in a}} federated mode on a synchronized time-basis. The first and primary application has been simulation-based training for chemical operators. The Platform is based on CORBA middleware to enable a standard and cost-effective integration of existing simulation models available in heterogeneous environments (diverse hardware platforms and multiple programming languages such as C, C++, Java or FORTRAN). In addition, the OPERA Platform provides built-in CORBA facilities for time-based exchange of simulation data, the synchronization of the <b>logical</b> <b>simulation</b> time, model integration support and system monitoring at run-time. First, the paper discusses the overall design requirements such as provision of a realistic system view to operators, i. e. meet soft real-time deadlines in spite of complex computing process models, cost-effective integration of heterogeneous legacy simulation software and applications and integration of hybrid simulation models. The paper shows how market constraints required a cost-effective solution, heavily relying on widespread standards, and offering open interface that enable a close integration with Web-based solutions as well as MS-Windows based office systems. This combined requirement set of both high performance, real-time, flexibility and low-cost is not met satisfactorily by the Run-time Infrastructure of the HLA. Thus, the design solution has been, though using the basic HLA time management concepts, to build the OPERA Platform from scratch in a standard IT environment and to carefully apply proven standard object technology. Finally, the paper presents {{the architecture of the}} OPERA platform and gives a detailed insight into the design approach for the optimized simulation protocol deducted from intensive benchmarking of CORBA products...|$|E
40|$|Emotions without {{feelings}} The {{idea that}} in autism {{there is a}} dissociation between ‘physical ’ and ‘cog-nitive ’ components of emotion is not new. In this letter I would like to sug-gest that this dissociation {{can be thought of}} fruitfully in terms of LeDoux’s (1996) distinction between emotions and feelings and that it has some clinical and biological implications. LeDoux (1996) distinguishes between two types of neural systems for affective behavior. One is a subcortical system that mediates emotions. He attributes this system to the amygdaloid complex. The other is a subcorti-cal–cortical system that mediates conscious feelings. LeDoux defines emotions as the products of the subcortical system, which is a specialized emotion system. This system receives sensory inputs. It produces behavioral, autonomic and hormonal responses. LeDoux defines feelings as the products of the subcortical–cortical system, which coordinates communication between the subcortical system and the cortex. A conscious emotional feeling {{is the result of the}} interac-tion between the subcortical system and cortical sensory buffers, cortical working memory, cortical arousal and bodily feedback. In terms of LeDoux’s distinction between emotions and feelings, it might be hypothesized that people with autism have relatively intact emo-tions, but impaired feelings. This hypothesis has some clinical and biologi-cal implications. From a clinical point of view, suppose that people with autism may have subcortical emotions without some corresponding cortical feelings. This assumption raises some questions about possible compensations. One possibility is that some people with higher-functioning autism (high-func-tion autism or Asperger syndrome) may be able to restore subcortical–cor-tical communication by cortically reasoning about subcortical emotions. For example, they might be able to label some of their emotions based on observable behavior and physical responses. More generally, they might be able to use a cognitive strategy of empathy, involving a <b>logical</b> <b>simulation</b> of some inner states of themselves and others. One instance of this strategy is the ability to simulate inner states of one’s own, as in the example above...|$|E
40|$|Simulations and bisimulations {{are known}} to be useful for {{abstracting}} and comparing formal systems, and they have recently been introduced into fuzzy systems. In this study, we provide sound and complete <b>logical</b> characterizations for <b>simulation</b> and bisimulation, which are defined over fuzzy labeled transition systems via two variants of the Hennessy-Milner Logic. The logic for character-izing fuzzy simulation has neither negation nor disjunction, which is very differ-ent from the well-known logical characterizations of probabilistic simulations, although the completeness proofs of our characterization results are inspired by relevant research in probabilistic concurrency theory. The logic for characteriz-ing fuzzy bisimulation also deviates from that for probabilistic bisimulations...|$|R
40|$|This paper {{presents}} the performance {{evaluation of a}} CMB (Chandy-Misra-Bryant) protocol {{from the perspective of}} execution time. The performance of each <b>logical</b> process in <b>simulation</b> is measured. Our evaluation shows that logical processes can have different behaviors and different protocols can be used simultaneously in <b>simulations.</b> While some <b>logical</b> processes may perform well using conservative protocols, others can use optimistic protocols because otherwise most of the time these processes would be blocked unnecessarily. In order to analyze the behavior of the simulations some models were simulated using a CMB implementation called ParSMPLX. These models showed that each logical process of a simulation has a different behavior that makes it more suitable for a specific protocol, increasing the performance. ...|$|R
40|$|Constraint {{equations}} {{are increasingly}} being used in interactive applications such as graphics, <b>logical</b> programming, and <b>simulation</b> that demand immediate feedback. To handle the performance requirements imposed by such systems constraint evaluators must use incremental satisfaction techniques. In this paper, we apply these techniques to noncircular, multilinear systems of equations. The constraint satisfaction process {{is divided into two}} phases [...] a planning phase that imposes a topological order on the equations and an execution phase that evaluates the equations. A planning algorithm is presented that incrementally updates this order each time the constraint system changes. This technique achieves significant performance improvements in large constraint systems since modifications generally perturb {{only a small portion of}} the topological order...|$|R
