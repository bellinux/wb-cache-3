104|10000|Public
5000|$|... #Subtitle level 3: <b>Localization</b> <b>of</b> <b>sound</b> {{in virtual}} {{auditory}} space ...|$|E
5000|$|Some {{milliseconds}} {{later the}} sound of the wall reflections arrived and disturbed the <b>localization</b> <b>of</b> <b>sound</b> sources.|$|E
50|$|The term spatialisation is {{connected}} especially with electroacoustic music to denote the projection and <b>localization</b> <b>of</b> <b>sound</b> sources in physical or virtual space or sound's spatial movement in space.|$|E
30|$|Coarse <b>localization</b> <b>of</b> the <b>sound</b> sources. In this step, {{important}} {{governing equations}} are ignored.|$|R
50|$|ES1370 {{was one of}} {{the first}} audio chips to support the Microsoft DirectSound3D audio API. When {{programs}} took full advantage of the API's capabilities, the ES1370 was capable of both global spatial and localized 3D sound effects, in both 2 and 4-speaker mode. The chip was capable of spatializing all audio automatically, but still required DirectSound3D usage for specific <b>localization</b> <b>of</b> <b>sounds.</b>|$|R
40|$|The {{brain stem}} {{auditory}} nuclei {{carry out a}} wide variety of transformations of the signals carried by the auditory nerve. Although basic frequency and intensity information is first encoded in the cochlea, brain stem circuitry must perform further neural definitions and refinements of these parameters, as well as integrate the cues necessary for the <b>localization</b> <b>of</b> <b>sounds</b> in space. Each o...|$|R
50|$|Using head-related {{transfer}} {{functions and}} reverberation, the changes of sound {{on its way}} from the source (including reflections from walls and floors) to the listener's ear can be simulated. These effects include <b>localization</b> <b>of</b> <b>sound</b> sources behind, {{above and below the}} listener.|$|E
50|$|ARI is {{conducting}} research on the <b>localization</b> <b>of</b> <b>sound</b> source with neuroprostheses (cochlea implants), modeling and improvement of measures of noise attenuation for traffic, the phonetical acoustics of regional dialects, the improvement of perceptually motivated coding strategies (such as MP3), and the mathematical theory of signal decomposition.|$|E
50|$|The {{trapezoid}} body (the ventral acoustic stria) {{is part of}} {{the auditory}} pathway where some of the axons coming from the cochlear nucleus (specifically, the anterior cochlear nucleus) decussate (cross over) to the other side before traveling on to the superior olivary nucleus. This is believed to help with <b>localization</b> <b>of</b> <b>sound.</b>|$|E
5000|$|At the end, {{when only}} {{loudspeaker}} 2 emitted sound, {{the situation was}} quite similar, the <b>sound</b> <b>of</b> the wall reflections, which arrived simultaneously, prevented a <b>localization</b> <b>of</b> this <b>sound</b> source.|$|R
40|$|Algorithms for the {{improvement}} of speech intelligibility in hearing prostheses can degrade the spatial quality of the audio signal. To investigate the influence on distance perception and <b>localization</b> <b>of</b> such algorithms, a system to virtually render arbitrary static acoustical scenes has been developed (Müller et al., 2010). With this system, the <b>localization</b> <b>of</b> <b>sounds</b> processed by a hearing aid algorithm can be compared to unprocessed sound sources. The existing virtual acoustics system has been extended to present more realistic dynamic scenes, and it can also compensate for head movements of test subjects...|$|R
40|$|Abstract — In {{the field}} of {{artificial}} intelligence, adaptive learning technique refers to combinations of artificial neural networks. In this thesis the adaptive learning technique is been implemented {{to carry out the}} Detection and <b>Localization</b> <b>of</b> heart <b>sound.</b> It can be implemented in the NS 2 and MATLAB In ns 2 part logic will identify the <b>sound</b> <b>of</b> heart and in matlab part heart sound is detected. The issue <b>of</b> Detection and <b>localization</b> <b>of</b> heart <b>sound</b> source is discuss in this thesis in new algorithm for estimating heart location, based on adaptive learning technique introduced and describe...|$|R
50|$|The {{trapezoid}} body is {{a bundle}} of decussating fibers in the ventral pons that carry information used for binaural computations in the brainstem. Some of these axons come from the cochlear nucleus and cross {{over to the other}} side before traveling on to the superior olivary nucleus. This is believed to help with <b>localization</b> <b>of</b> <b>sound.</b>|$|E
50|$|Hearing {{threshold}} {{and the ability}} to localize sound sources are reduced underwater, in which the speed of sound is faster than in air. Underwater hearing is by bone conduction, and <b>localization</b> <b>of</b> <b>sound</b> appears to depend on differences in amplitude detected by bone conduction. Aquatic animals such as fish, however, have a more specialized hearing apparatus that is effective underwater.|$|E
50|$|The {{binaural}} {{aspect of}} the cocktail party effect {{is related to the}} <b>localization</b> <b>of</b> <b>sound</b> sources. The auditory system is able to localize at least two sound sources and assign the correct characteristics to these sources simultaneously. As soon as the auditory system has localized a sound source, it can extract the signals of this sound source out of a mixture of interfering sound sources.|$|E
50|$|The {{interaural}} time difference (or ITD) when concerning humans or animals, {{is the difference}} in arrival time <b>of</b> a <b>sound</b> between two ears. It {{is important in the}} <b>localization</b> <b>of</b> <b>sounds,</b> as it provides a cue to the direction or angle <b>of</b> the <b>sound</b> source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction <b>of</b> <b>sound</b> source.|$|R
50|$|Sound {{localization}} is {{the ability}} to correctly identify the directional location <b>of</b> <b>sounds.</b> A sound stimulus localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in <b>localization.</b> <b>Localization</b> <b>of</b> low frequency <b>sounds</b> is accomplished by analyzing interaural time difference (ITD). <b>Localization</b> <b>of</b> high frequency <b>sounds</b> is accomplished by analyzing interaural level difference (ILD).|$|R
50|$|As {{a result}} of this two-step process, the reconstructed {{three-dimensional}} sound field contains information not only on the <b>localization</b> <b>of</b> the <b>sound</b> source, but also on the physical aspects of the environment of the original signal source. This is its difference from the results <b>of</b> the <b>sound</b> <b>localization</b> process.|$|R
5000|$|Dr. Hartmann's {{published}} work in psychoacoustics deals with pitch perception, signal detection, modulation detection [...] and <b>localization</b> <b>of</b> <b>sound.</b> His pitch perception research {{is characterized by}} an unusual emphasis on tonotopically local effects such as pitch shifts, presumably originating in the auditory periphery. His sound localization research has emphasized signal confusions caused by room reflections and the strategies used by listeners to cope with them.|$|E
50|$|Hearing is an {{important}} sensory system for most species of fish. Hearing threshold {{and the ability to}} localize sound sources are reduced underwater, in which the speed of sound is faster than in air. Underwater hearing is by bone conduction, and <b>localization</b> <b>of</b> <b>sound</b> appears to depend on differences in amplitude detected by bone conduction. Aquatic animals such as fish, however, have a more specialized hearing apparatus that is effective underwater.|$|E
50|$|Mobile Autonomous Robot Vehicle for Indoor Navigation (Marvin) is {{a mobile}} robot {{developed}} at Robotics Lab at University of Kaiserslautern, Germany.This platform {{consists of a}} differential drive, a bumper for basic operational safety, planar laser range scanners at {{the front and back}} side for obstacle detection, a belt of ultrasonic sensors for recognizing jutting edges such as table tops, a web cam, another fixed laser scanner at a height of one meter for a view free of clutter and a stereo microphone system for <b>localization</b> <b>of</b> <b>sound</b> sources. Its control system follows a behavior-based approach and its mapping abilities rely on a 2D geometric and topological strategy.|$|E
40|$|We have {{developed}} a 3 D sound system for headphones that allows real-time sound source and user displacement in a virtual acoustic environment. Because of a flexible design that uses different sets of pre-selected, physically modeled filters, the complexity level of the simulation can be chosen, making the system adaptable both to available CPU power and to application requirements. No extensive signal processing knowledge is {{required in order to}} select the appropriate simulation complexity. A preliminary evaluation involving 4 users showed that the system provides a satisfying <b>localization</b> <b>of</b> <b>sounds</b> and users (even with limited memory and CPU power) while also giving access to low-level control over simulation complexity...|$|R
40|$|Estimation of the {{location}} <b>of</b> <b>sound</b> sources is usually done using microphone arrays. Such settings provide an environment where we {{know the difference between}} the received signals among different microphones in the terms of phase or attenuation, which enables <b>localization</b> <b>of</b> the <b>sound</b> sources. In our solution we exploit the properties of the room transfer function in order to localize a sound source inside a room with only one microphone. The shape of the room and the position of the microphone are assumed to be known. The design guidelines and limitations of the sensing matrix are given. Implementation is based on the sparsity in the terms of voxels in a room that are occupied by a source. What is especially interesting about our solution is that we provide <b>localization</b> <b>of</b> the <b>sound</b> sources not only in the horizontal plane, but in the terms of the 3 D coordinates inside the room...|$|R
40|$|The aim of {{bachelor}} {{thesis is}} to describe techniques that are used for <b>localization</b> source <b>of</b> <b>sound</b> in far field by microphone array, mainly by method called beamforming. This thesis contains also simulation and practical test of beamforming method. There are also results of this simulations and measurement...|$|R
50|$|Auro 11.1 is {{a channel}} based system and thus differs in {{capability}} compared to competing formats such as Dolby Atmos and DTS:X. The height layer allows for placement of background reflections and reverberations observed in nature {{as well as}} allow for smooth panning from the base layer to the ceiling. However, the lack of surround back channel restricts panning {{due to lack of}} differentiation between the side and rear sector. Similarly, the use of arrays for surround and overhead channel also disallows pin point <b>localization</b> <b>of</b> <b>sound</b> along the walls as well as within the theatre space. The Auromax format extends the capabilities of Auro 11.1 to approach Dolby Atmos and DTS:X.|$|E
50|$|MSO neurons {{are excited}} bilaterally, {{meaning that they}} are excited by inputs from both ears, and they are {{therefore}} referred to as EE neurons. Fibers from the left cochlear nucleus terminate on the left of MSO neurons, and fibers from the right cochlear nucleus terminate {{on the right of}} MSO neurons. Excitatory inputs to the MSO from spherical bushy cells are mediated by glutamate, and inhibitory inputs to the MSO from globular bushy cells are mediated by glycine. MSO neurons extract ITD information from binaural inputs and resolve small differences in the time of arrival of sounds at each ear. Outputs from the MSO and LSO are sent via the lateral lemniscus to the IC, which integrates the spatial <b>localization</b> <b>of</b> <b>sound.</b> In the IC, acoustic cues have been processed and filtered into separate streams, forming the basis of auditory object recognition.|$|E
40|$|EUSIPCO 2012 : The 20 th European Signal Processing Conference, August 27 - 31, 2012, Bucharest, RomaniaTo {{build an}} {{acoustic}} {{system that can}} maintain the <b>localization</b> <b>of</b> <b>sound</b> images included in stereo mixed signals, we propose a new object-based up-mixer that performs sound source separation and sound location estimation. First, in a preliminary experiment, we show the effectiveness of sound location estimation using the proposed up-mixer via objective tests. Next, we evaluate the perception accuracy of sound localization by wave field synthesis using the proposed up-mixer via subjective tests. The {{results show that the}} proposed up-mixer provides a good <b>localization</b> <b>of</b> <b>sound</b> images included in stereo mixed signals at several listening positions...|$|E
40|$|This article aims to {{know about}} hearing {{development}} experienced by otologically healthy children during their early childhood, {{in order to give}} an objective documentation about the matter to preschool education teachers, so that they can improve and enhance their didactic work in the classroom. The research uses classic bibliography in health sciences and psychology, international standards on hearing thresholds, and validated researches addressing the development <b>of</b> tonal discrimination, <b>localization</b> <b>of</b> <b>sounds</b> in space, and the impact of hearing on integral child development. It is concluded that the hearing development in early childhood is not limited to physiological aspect, but it also involves other factors such as sensory, sensorimotor, perceptual and cognitive ones...|$|R
30|$|<b>Localization</b> <b>of</b> <b>sounds</b> in {{physical}} space plays {{a very important}} role in multiple audio-related disciplines, such as music, telecommunications, and audiovisual productions. Binaural recording is the most commonly used method to provide an immersive sound experience by means of headphone reproduction. However, it requires a very specific recording setup using high-fidelity microphones mounted in a dummy head. In this paper, we present a novel processing framework for binaural sound recording and reproduction that avoids the use of dummy heads, which is specially suitable for immersive teleconferencing applications. The method is based on a time-frequency analysis of the spatial properties <b>of</b> the <b>sound</b> picked up by a simple tetrahedral microphone array, assuming source sparseness. The experiments carried out using simulations and a real-time prototype confirm the validity of the proposed approach.|$|R
40|$|The {{present study}} {{investigated}} whether memory for a room-sized spatial layout learned through auditory <b>localization</b> <b>of</b> <b>sounds</b> exhibits orientation dependence {{similar to that}} observed for spatial memory acquired from stationary viewing of the environment. Participants learned spatial layouts by viewing objects or localizing sounds and then performed judgments of relative direction among remembered locations. The results showed that direction judgments following auditory learning were performed most accurately at a particular orientation {{in the same way}} as were those following visual learning, indicating that auditorily encoded spatial memory is orientation dependent. In combination with previous findings that spatial memories derived from haptic and proprioceptive experiences are also orientation dependent, the present finding suggests that orientation dependence is a general functional property of human spatial memory independent of learning modality...|$|R
30|$|For {{assessment}} of activity limitations, tinnitus was assessed on a ITI questionnaire with 8 questions. The functional limitation of hearing {{was assessed by}} a questionnaire consisting of 10 questions. <b>Localization</b> <b>of</b> <b>sound</b> was assessed by a sound localization questionnaire consisting of 4 questions. The vertigo was evaluated with a vertigo handicap questionnaire consisting of 8 questions. A total of 24 questions were analyzed.|$|E
40|$|This paper proposes two {{experiments}} {{to examine the}} effects of human head movement and latency compensation in 3 D sound localization. There are two hypotheses, the first hypothesis is that through the computer simulation of 3 D sound, dynamic head movement can help in the <b>localization</b> <b>of</b> <b>sound</b> in space, as compared to fixed head position. The second hypothesis is when there is latency introduced in the computer generation of 3 D sound, a human subject can perform better in a sound locating task than the one without compensation. The results of two proposed experiments corroborate with the above two hypotheses. Moreover, we are able to identify that with dynamic head movement, the human capability can be enhanced by more than 90 % in the <b>localization</b> <b>of</b> <b>sound</b> in space and at the same time reduce the front-back ambiguity. In the second experiment involving a typical latency at 300 ms, it is shown that with compensation for latency the average time to perform sound locating task can be reduced [...] ...|$|E
40|$|A {{system was}} {{developed}} for stereophonic presentation of information (3 -D sound) over high-fidelity headphones. The purpose was to reduce the pilot's response time to a localized threat. Physical parameters permitting the <b>localization</b> <b>of</b> <b>sound</b> are reviewed. The technique used to collect a subject's acoustic filtering characteristics is also described. Space simulation of a monophonic sound is presented, the purpose being that the sound be heard as coming from a given point in space. The first steps of a physical validation of the synthesis are presented...|$|E
40|$|The paper {{discusses}} {{a possibility}} <b>of</b> <b>localization</b> <b>of</b> a <b>sound</b> source inside a wooden beam. The method {{is based on}} measuring signals from two microphones, assuming the sound source lies between the microphones. The position <b>of</b> the <b>sound</b> source is calculated from the delay between the signals. The calculation of the delay is done by correlation of the signals in the frequency range. ARM architecture microcontroller is used to for the calculations...|$|R
40|$|This paper {{presents}} an alternative visualization tool for headrelated transfer functions (HRTF's) which represents HRTF data sets as magnitude spatial frequency response surfaces. Qualitative analysis of HRTF data is easier in the spatial domain {{than in the}} magnitude frequency domain and allows quick comparisons between different subjects' HRTF sets. In addition, these surfaces exhibit many well-known HRTFrelated psychophysical phenomena due to head, torso, and pinna filtering. Finally, these surfaces suggest an interpolation algorithm by which Directional Transfer Functions (DTF's) corresponding to arbitrary spatial locations can be computed from existing DTF measurements at known locations. 1. INTRODUCTION Previous psychophysical studies have concluded that knowledge of the acoustic filtering properties of the pinna, head, and torso are important in human <b>localization</b> <b>of</b> <b>sounds</b> in space. The combined effects of such filtering are summarized by a single set of spatially dependent fi [...] ...|$|R
50|$|The {{integration}} <b>of</b> a <b>sound</b> stimulus is {{a result}} of analyzing frequency (pitch), intensity, and spatial <b>localization</b> <b>of</b> the <b>sound</b> source. Once a sound source has been identified, the cells of lower auditory pathways are specialized to analyze physical sound parameters. Summation is observed when the loudness <b>of</b> a <b>sound</b> from one stimulus is perceived as having been doubled when heard by both ears instead of only one. This process of summation is called binaural summation and is the result of different acoustics at each ear, depending on where sound is coming from.|$|R
