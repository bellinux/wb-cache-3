5|16|Public
40|$|In this paper, {{we present}} an {{efficient}} loudness model applicable to time-varying sounds. We use {{the model of}} Glasberg and Moore (J. Audio Eng. Soc., 2002) {{as the basis for}} our developments, proposing a number of optimization techniques to reduce the computational complexity at each stage of the model. Efficient alternatives to computing the multi-resolution DFT, excitation pattern and pre-cochlea filter are presented. Absolute threshold and equal <b>loudness</b> <b>contour</b> predictions are computed and compared against both steady-state and time-varying loudness models to evaluate the combined accuracy of these techniques in the frequency domain. Finally, computational costs and loudness errors are quantified for a range of time-varying stimuli, demonstrating that the optimized model can execute approximately 50 times faster within tolerable error bounds. © 2013 IEEE...|$|E
40|$|Summary Methods used to {{quantify}} annoyance due to environmental noise {{are described as}} well as the results of an analysis of vehicle passby recordings. Most methods currently used {{to quantify}} the impact of noise are based on average or maximum A-weighted sound pressure levels. It is known that the loudness of a sound is the primary driver in annoyance; however, it is also known that other sound characteristics, such as the presence of tones, fluctuations, spectral balance and impulsiveness, also impact annoyance. While A-weighted sound pressure levels may be strongly correlated to loudness levels for some sets of sounds, {{the relationship between the two}} can change if the characteristics of the sounds in the set change. Recently developed models of loudness incorporate many characteristics of the human hearing system including changes in response behavior with increased level, frequency masking and temporal masking. The A-weighting is derived from the 40 phon equal <b>loudness</b> <b>contour</b> and is only truly appropriate for analysis of relatively quiet sounds. In contrast, the loudness models are appropriate for sounds over a wide range of levels and incorporate the chang...|$|E
40|$|The {{propagation}} and attenuation {{characteristics of}} sound wave can signify and affect speech intelligibility in an enclosed room. Moreover, the lower frequency components of sound wave {{is the major}} concern for sound wave at this frequency range are the hardest to attenuate. The purpose {{of this research is}} to quantify the propagation and attenuation characteristics of sound wave at audible low frequency range. The project was kicked off by fabricating a new wedges design for the existing anechoic chamber in Makmal Penyelidikan Akustik, Fakulti Kejuruteraan Elektrik, UTMSkudai, Johor. Once the anechoic room was completely refurbished, measurement of reverberation time (RT 60), sound pressure level (SPL) and SPL attenuation per doubling of distance were conducted. The results RT 60, SPL and SPL attenuation per doubling of distance were obtained for one-octave center frequency from 125 Hz to 4 kHz. It is concluded that the attenuation of energy of sound waves, bymeans of RT 60 results, follow the established normal equal <b>loudness</b> <b>contour.</b> However, the SPL in anechoic room is found to be peaking up beginning at frequency 64 Hz until 4 kHz. From SPL attenuation per doubling of distance results, it is concluded that the new refurbished anechoic room is able to handle test frequency of sound waves from 64 Hz and above...|$|E
40|$|Audio {{tapes are}} often used as {{evidence}} in courtrooms. This report is about a case of threatening telephone calls. Tape authentication was done by visual and statistical analysis of pitch and <b>loudness</b> <b>contours.</b> Voice identification was done {{by means of a}} “voice line-up”, i. e. judging differences and similarities in a series of matched voices (including the defendants’ voice) and the voice on the exhibit recording. Methods for authentication and identification are described in detail...|$|R
40|$|A {{loudness}} {{model is}} adopted {{to study the}} feasibility of designing and operating a supersonic transport to produce minimized sonic booms. The <b>loudness</b> <b>contours</b> in this technique extend to a lower frequency (1 Hz) and thus are appropriate for sonic booms that contain significant low frequency energy. Input to the loudness calculation procedure is the power spectral density of the pressure-time signature. Calculations of loudness, for both indoor and outdoor conditions, demonstrate that shaped sonic booms are potentially more acceptable than N-waves possessing the same peak overpressure...|$|R
40|$|This {{work was}} {{supported}} by The Netherlands Ministry of Infrastructure and the Environment [grant number 4500182046], and by matched funding from The Netherlands Ministry of Defence (administered by TNO) and the UK Natural Environment Research Council [to P. J. W. ]. Loudness perception by human infants and animals can be studied {{under the assumption that}} sounds of equal loudness elicit equal reaction times (RTs). Simple RTs of a harbour porpoise to narrowband frequency-modulated signals were measured using a behavioural method and an RT sensor based on infrared light. Equal latency contours, which connect equal RTs across frequencies, for reference values of 150 - 200 ms (10 ms intervals) were derived from median RTs to 1 s signals with sound pressure levels (SPLs) of 59 - 168 dB re. 1 μPa and centre frequencies of 0. 5, 1, 2, 4, 16, 31. 5, 63, 80 and 125 kHz. The higher the signal level was above the hearing threshold of the harbour porpoise, the quicker the animal responded to the stimulus (median RT 98 - 522 ms). Equal latency contours roughly paralleled the hearing threshold at relatively low sensation levels (higher RTs). The difference in shape between the hearing threshold and the equal latency contours was more pronounced at higher levels (lower RTs); a flattening of the contours occurred for frequencies below 63 kHz. Relationships of the equal latency contour levels with the hearing threshold were used to create smoothed functions assumed to be representative of equal <b>loudness</b> <b>contours.</b> Auditory weighting functions were derived from these smoothed functions that may be used to predict perceived levels and correlated noise effects in the harbour porpoise, at least until actual equal <b>loudness</b> <b>contours</b> become available. Publisher PDFPeer reviewe...|$|R
3000|$|... max {{similarity}} measure [5, 16] {{achieved the}} highest identification accuracy in the 2009 MIREX audio CSI task contest. In [10], the lower pitch-frequency cepstral coefficients were discarded {{and the remaining}} coefficients were projected onto chroma bins to obtain the Chroma DCT-Reduced log Pitch descriptor. This descriptor achieved {{a high degree of}} timbre invariance and, hence, outperformed conventional PCP in the context of music matching and retrieval applications. In [13], to describe the similarity of singing voice between cover versions of popular songs, two concepts from psychoacoustics (time-varying <b>loudness</b> <b>contour</b> and critical band) were combined with conventional PCP descriptors organically to obtain Cochlear PCP (CPCP). Besides harmonic progression, melody evolution can also be used for the CSI task, for example, in [17 – 19], the main melody (denoted as MLD in this paper) was extracted for cover song retrieval. Recently, timbre-based descriptors are studied for the CSI task [12, 20]. In [12], a new descriptor, Modified Perceptual Linear Prediction Lifted Cepstrum (MPLPLC), was obtained by modifying the Perceptual Linear Prediction (PLP) model in automatic speech recognition field through introducing new research achievements in psychophysics and taking the difference between speech and music into consideration to make it suitable for music signal analysis. In addition, different kinds of similarity functions, such as Cross-Correlation (CC) [9], Dynamic Time Warping (DTW) [11], Qmax [5], and Dmax [21], have been proposed for measuring the similarity between descriptors.|$|E
40|$|The {{purpose of}} the present study was to {{evaluate}} the relationship between several measures of the acoustic reflex [acoustic reflex threshold (ART), dynamic range of the acoustic reflex growth function, the 50 % point along the acoustic reflex growth function, and the maximum intensity value of the acoustic reflex growth function] and behavioral measurements of loudness [loudness discomfort level (LDL) and the <b>loudness</b> <b>contour</b> (LC) ]. The underlying objective was to determine if any of these measures can be used to predict the LDL. A finding of a strong relationship between these measures could potentially assist in the creation of an objective method to measure LDLs, which may have implications for hearing aid fittings. Prior research in this area has yielded conflicting results. However, very few studies examined measures of loudness growth and the dynamic range of the acoustic reflex. Twenty young adults ranging from 22 - 35 years of age (Mean age = 25. 85, s. d. 3. 07) with normal hearing participated in this study. Participants were required to provide a subjective loudness rating to warbled-tone stimuli in accordance with a categorical loudness scaling procedure adapted from Cox et al. (1997), as well as an LDL rating. Additionally, an ART was obtained from each participant, as defined by a 0. 02 mmho change in admittance. Following identification of the ART, the acoustic reflex growth function was obtained by increasing the stimulus until the termination point. Experimental measures were obtained over two test sessions. Results revealed no significant relationship between measures of the acoustic reflex and loudness. Analysis of test-retest measures revealed moderate to very high positive (0. 70 - 0. 92) correlations for the acoustic reflex and LDL measures over a period of 1 day to 2 weeks. Test-retest performance on the majority of loudness categories on the LC did not reveal stable results. Implications for these findings are that the ART cannot be used to reliably predict the LDL. Additionally, the LC may not be a reliable clinical measurement to assess loudness...|$|E
40|$|Although {{their basic}} concept is similar, the current {{standards}} for the calculation of loudness, ANSI S 3. 4 - 2007 and DIN 45631 (1991), produce significantly different results for many kinds of sounds. While their values for pure tones {{can be explained by}} the equal <b>loudness</b> <b>contours</b> of their times, they also show huge discrepancies for broadband sounds. For this reason, extended psychoacoustic experiments were made in order to find target values for the loudness of pink noise at various levels. Furthermore, the performance of the algorithms was investigated by subjective tests on several technical sounds at a large scale of loudness levels. In all cases, the method of adjustment was used. The results suggest that DIN 45631 (1991) predicts loudness very well. Its estimations are within the interquartile range of the subjective evaluations...|$|R
40|$|Stochastic turn-taking {{models have}} {{traditionally}} been implemented as N-grams, which condition predictions on recent binary-valued speech/non-speech contours. The current work re-implements this function using feed-forward neural networks, capable of accepting binary- as well as continuous-valued features; performance is shown to asymptotically approach that of the N-gram baseline as model complexity increases. The conditioning context is then extended to leverage <b>loudness</b> <b>contours.</b> Experiments indicate that the additional sensitivity to loudness considerably decreases average cross entropy rates on unseen data, by 0. 03 bits per framing interval of 100 ms. This reduction is shown to make loudness-sensitive conversants capable of better predictions, with attention memory requirements at least 5 times smaller and responsiveness latency at least 10 times shorter than the loudness-insensitive baseline. Index Terms — Interaction models, neural networks, prosody, spoken dialogue systems...|$|R
40|$|We {{measured}} the noise caused by Vehicles and its subjective {{effects in the}} Modarres high way In Tehran. we examined 143 cases in 11 stations in the mentioned pathway with an accuracy of 95 %  and {{a standard deviation of}} 39 %. The noise criteria of Leq and the number of vehicles were determined in dBA. The <b>loudness</b> <b>contours</b> were also measured in some stations to determine the subjective impacts of noise. Then the correlation of the responses of the interviewees with sound pressure level was measured. Based on our findings, decrease of  the noise of traffic sources should be considered three levels: the first level is noise reduction at the level of source, e. g. the vehicles;the second is Noise reduction at the traffic level and the third is noise reduction at the level of general activity...|$|R
40|$|Advances in {{the field}} of {{psychoacoustics}} have resulted in the development of more accurate models for the calculation of loudness as well as improved <b>contours</b> representing <b>loudness</b> perception. This study was undertaken to experimentally determine a 2 ̆ 2 best use 2 ̆ 2 stationary loudness model among the standardized methods available. To accomplish this, an investigative study was performed using pure tones at varying frequencies to identify {{the strengths and weaknesses of}} these loudness algorithms. The results of the investigation showed that with the recent update to the reference equal <b>loudness</b> <b>contours,</b> several of the models have become outdated in their performance. The recently revised ANSI S 3. 4 : 2007 model was shown to have the best correlation to the reference curves based on experimental measurements and was also the easiest to implement. It is recommended that the ANSI S 3. 4 : 2007 loudness model be used as the present day standard for calculation of stationary loudness...|$|R
50|$|The ATH is {{the lowest}} of the equal-loudness {{contours}}. Equal-loudness contours indicate the sound pressure level (dB SPL), over the range of audible frequencies, that are perceived as being of equal <b>loudness.</b> Equal-loudness <b>contours</b> were first measured by Fletcher and Munson at Bell Labs in 1933 using pure tones reproduced via headphones, and the data they collected are called Fletcher-Munson curves. Because subjective loudness was difficult to measure, the Fletcher-Munson curves were averaged over many subjects.|$|R
40|$|The {{effects of}} the type of {{reinforcer}} on auditory sensitivity and equal-loudness data were determined in the squirrel monkey. The monkeys, restrained and provided with earphones, were conditioned to depress and hold a bar down {{in the presence of a}} stimulus light and then to terminate the holding response after onset of a tone. In Experiment 1, the specified behavior sequence postponed electric shock; in Experiment 2, a food reinforcer was dependent on bar release during the tone. The shape of the auditory sensitivity function and the acuity level at each frequency were the same for the two procedures. The audible frequency range extended from below 0. 125 kHz (lowest frequency used) to 46 kHz. Sensitivity was maximum at 8 kHz. Latency of bar release following tone onset served as the basic data for constructing a family of equal-loudness contours. The type of reinforcer appeared not to be a determinant of either the shape of individual <b>loudness</b> <b>contours</b> or the pattern of family of equal-loudness functions. At the lower sound-pressure levels, the equal-loudness contours closely paralleled the threshold curves. At more-intense levels, the contours tended to flatten and depend less on frequency...|$|R
40|$|The {{domestic}} cat is {{the primary}} physiological model of loudness coding and recruitment. At present, there are no published descriptions of loudness perception in this species. This study used a reaction time task to characterize loudness perception in six behaviorally trained cats. The psychophysical approach {{was based on the}} assumption that sounds of equal loudness elicit responses of equal latency. The resulting equal latency contours reproduced well-known features of human equal <b>loudness</b> <b>contours.</b> At the completion of normal baseline measures, the cats were exposed to intense sound to investigate the behavioral correlates of loudness recruitment, the abnormally rapid growth of loudness that is commonly associated with hearing loss. Observed recruitment effects were similar in magnitude to those that have been reported in hearing-impaired humans. Linear hearing aid amplification is known to improve speech intelligibility but also exacerbate recruitment in impaired listeners. The effects of speech spectra and amplification on recruitment were explored by measuring the growth of loudness for natural and amplified vowels before and after sound exposure. Vowels produced more recruitment than tones, and the effect was exacerbated by the selective amplification of formant structure. These findings support the adequacy of the domestic cat as a model system for future investigations of the auditory processes that underlie loudness perception, recruitment, and hearing aid design...|$|R
40|$|A primary {{objective}} for cognitive neuroscience {{is to identify}} how features of the sensory environment are encoded in neural activity. Current auditory models of loudness perception {{can be used to}} make detailed predictions about the neural activity of the cortex as an individual listens to speech. We used two such models (loudness-sones and loudness-phons), varying in their psychophysiological realism, to predict the instantaneous <b>loudness</b> <b>contours</b> produced by 480 isolated words. These two sets of 480 contours were used to search for electrophysiological evidence of loudness processing in whole-brain recordings of electro- and magneto-encephalographic (EMEG) activity, recorded while subjects listened to the words. The technique identified a bilateral sequence of loudness processes, predicted by the more realistic loudness-sones model, that begin in auditory cortex at ~ 80 ms and subsequently reappear, tracking progressively down the superior temporal sulcus (STS) at lags from 230 to 330 ms. The technique was then extended to search for regions sensitive to the fundamental frequency (F 0) of the voiced parts of the speech. It identified a bilateral F 0 process in auditory cortex at a lag of ~ 90 ms, which was not followed by activity in STS. The results suggest that loudness information is being used to guide the analysis of the speech stream as it proceeds beyond auditory cortex down STS towards the temporal pole...|$|R
2500|$|The ATH is {{the lowest}} of the equal-loudness {{contours}}. Equal-loudness contours indicate the sound pressure level (dB SPL), over the range of audible frequencies, that are perceived as being of equal <b>loudness.</b> [...] Equal-loudness <b>contours</b> were first measured by Fletcher and Munson at Bell Labs in 1933 using pure tones reproduced via headphones, and the data they collected are called Fletcher–Munson curves. [...] Because subjective loudness was difficult to measure, the Fletcher–Munson curves were averaged over many subjects.|$|R
40|$|Full text: While "classical" {{loudness}} models predict loudness in sone {{using the}} concepts of Stevens' compressive power law, (subdivided) categorical loudness perception after Heller follows the compressive logarithmic Weber-Fechner law. To {{bridge the gap between}} both approaches, this contribution reviews various steps towards a loudness model that predicts categorical loudness (in categorical units, CU) for normal and hearing-impaired listeners for arbitrary sounds. It uses a (modified) classical loudness model for stationary signals to derive the loudness in sone and a nonlinear transformation from sone to CU. This transformation is approximated by a cubic polynomial equation wich is derived from categorical loudness data of 84 normal-hearing subjects. The model parameters are further set to predict the standard isophones that are in good agreement with the equal <b>loudness</b> level <b>contours</b> derived from categorical loudness data. Also, the model predicts the loudness functions near threshold both for normal and hearing-impaired listeners and can be extended to predict duration-dependent loudness perception. Since categorical loudness can be measured more easily and directly than loudness in sone, the current modelling approach can be experimentally tested and can be used in various applications, such as, e. g. hearing aid processing and fitting procedures...|$|R
40|$|We {{present a}} system that generates arm dancing motion to new music tracks, based on sample motion {{captured}} data of dancing to other pieces of music. Rather than adapting existing motion, as most music-animation systems do, our system is novel in that it analyzes both the supplied motion and music data for certain characteristics (eg. melodic <b>contour,</b> <b>loudness,</b> etc), and learns relationships between the two. When new music is provided, the characteristics are analyzed as before, and used to predict characteristics of the motion. A generative process then creates motion curves according to these constraints. A demonstration is presented showing the results on both slow and fast tunes, including an automatically-generated trio of backup dancers for a jazz standard. The system could be extended naturally to other movements beyond arm dancing. Keywords: animation, learning, motion/music synchronization...|$|R
40|$|Psychology {{of music}} has shown renewed {{interest}} in how music expresses emotion to listeners. However, there is an obvious lack of research on how interactions between musical factors such as harmony, rhythm, melodic <b>contour,</b> <b>loudness,</b> and articulation may affect perceived emotion. From previous literature on music analysis and music cognition {{there is evidence that}} tonality may be activated and affected by rhythm and melody. These ideas generated hypotheses regarding melodic organization and performance, for instance, (a) certain notes in a melodic structure have expressive potentials due to their place in the key/chord, (b) these notes could be activated by accents in the melodic structure and/or in live music performance. In Study I, a simple tune was systematically manipulated with regard to harmonic progression, rhythm and melodic contour. Listener ratings of the resulting versions showed that perceived structure (instability, complexity, tension) and emotion (sadness, anger, expressivity) could be partly interpreted as resulting from accent structures and stress on certain notes. In Study II, musicians were asked to perform some of the above-mentioned versions so as to express happiness, sadness, tenderness and anger. The performers used loudness and articulation to compensate for lack of adequate inherent expression in melodies. They also highlighted certain notes of relevance for the emotional meaning by means of stress in articulation, loudness and timing. In Study III, simple three-note sequences were manipulated with regard to melodic, metric and rhythmic accents as well as (computer-) performed accents (loudness, articulation and timing) on certain target notes. Listening tests showed that accent on a tense note enhanced perceived anger. A note essential for the identity of major mode affected perception of happiness, whereas a note essential for minor mode affected perception of sadness. The results in this thesis have implications for a dynamic view of melodic organization and performance...|$|R
40|$|Prosody can {{be defined}} as the aspects of a sentence's {{pronunciation}} not described by the phone sequence of the words. Examples of prosodic information include pitch <b>contour,</b> <b>loudness</b> and duration. Prosody or prosodic information {{plays an important role in}} human communications. In tonal languages such as Chinese, words are often differentiated by the lexical tones which are related to the pitch contour. In addition, intonation patterns are used in both Chinese and English to express emotion or emphasis. The goal of this thesis is to investigate different approaches of applying prosodic information for speech processing such as using tone information for Chinese speech recognition and classification of English intonation patterns with an application to computer-aided language learning. The classification of pitch contour patterns involves four components, i) pitch detection, ii) pitch contour representation, iii) pitch pattern classification and, iv) integration of prosody for Chinese speech recognition. While pitch can be estimated frame by frame, because pitch pattern is a supra-segment, it is more effective to represent the pitch pattern by a continuous curve, such as a polynomial. By representing the pitch contour as a polynomial, we experimented with different classifiers, such as Decision tree, Neutral Network and Hidden Markov Model, and evaluated their effectiveness on tone classification of isolated and continuous Chinese syllables, and English intonation pattern classification. Finally, prosody information is integrated into a Chinese speech recognition system. In our study, we found that the Cepstrum (CEP) pitch detection algorithm could detect pitch values with good accuracy (97. 2 %). Since cepstral coefficients are typically used in recognition, the CEP method has the additional advantage that it can be implemented in the speech recognition front-end. In pitch pattern classification, the combination of polynomial representation and decision tree gave the best performance in both English intonation pattern classification and isolated Chinese syllable tone recognition. An analysis of the decision tree structure indicated that the slope of the pitch contour was the most important feature which was consistent with our knowledge of Chinese tones. In continuous Chinese speech, because of the co-articulation effects and tone sandhi, tone classification degraded significantly with confusions at tone 3 or neutral tone. By integrating the prosodic information into Chinese speech recognition, syllable accuracy improved from 65. 4 % to 70. 5 %...|$|R

