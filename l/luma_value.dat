4|3|Public
50|$|The YCoCg color {{model is}} the color space formed from a simple {{transformation}} of an associated RGB color space into a <b>luma</b> <b>value</b> (denoted as Y) and two chroma values called chrominance green (Cg) and chrominance orange (Co). It is supported in video and image compression designs such as H.264/MPEG-4 AVC, HEVC, JPEG XR, and Dirac, since it is simple to compute, has good transform coding gain, and can be losslessly converted to and from RGB with fewer bits than are needed with other color models.|$|E
5000|$|One of the {{artifacts}} {{that can occur}} with chroma subsampling is that out-of-gamut colors can occur upon chroma reconstruction. Suppose the image consisted of alternating 1-pixel red and black lines and the subsampling omitted the chroma for the black pixels. Chroma from the red pixels will be reconstructed onto the black pixels, causing the new pixels to have positive red and negative green and blue values. As displays cannot output negative light (negative light does not exist), these negative values will effectively be clipped and the resulting <b>luma</b> <b>value</b> will be too high. [...] Similar artifacts arise in the less artificial example of gradation near a fairly sharp red/black boundary.|$|E
5000|$|YIQ was {{formerly}} used in NTSC (North America, Japan and elsewhere) television broadcasts for historical reasons. This system stores a <b>luma</b> <b>value</b> roughly analogous to (and sometimes incorrectly identified as) luminance, {{along with two}} chroma values as approximate representations of the relative amounts of blue and red in the color. It {{is similar to the}} YUV scheme used in most video capture systems and in PAL (Australia, Europe, except France, which uses SECAM) television, except that the YIQ color space is rotated 33° with respect to the YUV color space and the color axes are swapped. The YDbDr scheme used by SECAM television is rotated in another way.|$|E
50|$|The last Atari XE {{computers}} {{made for}} the Eastern European market were built in China. Many if not all have a buggy PAL GTIA chip. The <b>luma</b> <b>values</b> in Graphics 9 and higher are at fault, appearing as stripes. Replacing the chip fixes the problem. Also, there have been attempts to fix faulty GTIA chips with some external circuitry.|$|R
5000|$|As {{a result}} of {{computers}} becoming powerful enough to serve as video editing tools, video digital-to-analog converters and analog-to-digital converters were made to overcome this incompatibility. To convert analog video lines {{into a series of}} square pixels, the industry adopted a default sampling rate at which <b>luma</b> <b>values</b> were extracted into pixels. The luma sampling rate for 480i pictures was [...] MHz and for 576i pictures was [...] MHz.|$|R
40|$|Image super-resolution {{plays an}} {{important}} role in a plethora of applications, including video compression and motion estimation. Detecting fractional displacements among frames facilitates the removal of temporal redundancy and improves the video quality by 2 - 4 dB PSNR [1] [2]. However, the increased complexity of the Fractional Motion Estimation (FME) process adds a significant computational load to the encoder and sets constraints to real-time designs. Timing analysis shows that FME accounts for almost half of the entire motion estimation period, which in turn accounts for 60 − 90 % of the total encoding time depending on the design configuration. FME bases on an interpolation procedure to increase the resolution of any frame region by generating sub-pixels between the original pixels. Modern compression standards specify the exact filter to use in the Motion Compensation module allowing the encoder and the decoder to create and use identical reference frames. In particular, H. 264 /AVC specifies a 6 -tap filter for computing the <b>luma</b> <b>values</b> of half-pixels and a low cost 2 -tap filter for computing quarter-pixels. Even though it is common practice for encoder designers to integrate the standard 6 -tap filter also in the Estimation module (before Compensation), the fact is that the interpolation technique used for detecting the displacements (not computing their residual) is an open choice following certain performance trade-offs. Aiming at speeding up the Estimation, a process of considerably higher computational demand than the Compensation, this work builds on the potential to implement a lower complexity interpolation technique instead of the H. 264 6 -tap filter. We integrate in the Estimation module several distinct interpolation techniques not included in the H. 264 standard, while keeping the standard H. 264 /AVC Compensation to measure their impact on the outcome of the prediction engine. Related bibliography includes both ideas to avoid/replace the standard computations, as well as architecturestargeting the efficient implementation of the H. 264 6 -tap filtering procedure and the support of its increased memory requirements. To this end, we note that H. 264 specifies a kernel with coefficients ⟨ 1,− 5, 20, 20,− 5, 1 ⟩ to be multiplied with six consecutive pixels of the frame (either in column or row format). The resulting six products are accumulated and normalized for the generation of a single half-pixel (between 3 rd and 4 th tap). The operation must be repeated for each “horizontal” and “vertical” half-pixelby sliding the kernel on the frame, both in row and column order. Moreover, there exist as many “diagonal” half-pixels to be generated by applying the kernel on previously computed horizontal or vertical half-pixels. That is to say, depending on its position, we must process 6 or 36 frame pixels to compute a single half-pixel. To avoid the costly H. 264 filter in the Estimation module, we study similar interpolation techniques using less than 6 taps, possibly exploiting gradients on the image. Section II shows three commonly used interpolation techniques and introduces three novel techniques to point out the differences of the proposed. Section III reports the performance results of these techniques and Section IV concludes the paper...|$|R
5000|$|In human vision {{there are}} three {{channels}} for color detection, and for many color systems, three [...] "channels" [...] is sufficient for representing most colors. For example: red, green, blue or magenta, yellow, cyan. But {{there are other ways}} to represent the color. In many video systems, the three channels are luminance and two chroma channels. In video, the luma and chroma components are formed as a weighted sum of gamma-corrected (tristimulus) R'G'B' components instead of linear (tristimulus) RGB components. As a result, luma must be distinguished from luminance. That there is some [...] "bleeding" [...] of luminance and color information between the luma and chroma components in video, the error being greatest for highly saturated colors and noticeable in between the magenta and green bars of a color bars test pattern (that has chroma subsampling applied), should not be attributed to this engineering approximation being used. Indeed, similar bleeding can occur also with gamma = 1, whence the reversing of the order of operations between gamma correction and forming the weighted sum can make no difference. The chroma can influence the luma specifically at the pixels where the subsampling put no chroma. Interpolation may then put chroma values there which are incompatible with the <b>luma</b> <b>value</b> there, and further post-processing of that Y'CbCr into R'G'B' for that pixel is what ultimately produces false luminance upon display.|$|E

