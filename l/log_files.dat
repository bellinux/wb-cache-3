1760|875|Public
5|$|Database {{forensics}} is {{a branch}} of digital forensics relating to the forensic study of databases and their metadata. Investigations use database contents, <b>log</b> <b>files</b> and in-RAM data to build a timeline or recover relevant information.|$|E
25|$|Windows Vista {{includes}} new Disk Cleanup handlers {{for cleaning}} up setup <b>log</b> <b>files,</b> system error memory dumps and the Windows thumbnail cache.|$|E
25|$|Data is {{read from}} these <b>log</b> <b>files</b> using Ptail, an {{internally}} built tool to aggregate data from multiple Scribe stores. It tails the <b>log</b> <b>files</b> and pulls data out (thus the name). Ptail data are separated out into three streams {{so they can}} eventually be sent to their own clusters in different data centers (Plugin impression, News feed impressions, Actions (plugin + news feed)). Puma is used to manage periods of high data flow (Input/Output or IO). Data is processed in batches to lessen {{the number of times}} needed to read and write under high demand periods (A hot article will generate a lot of impressions and news feed impressions which will cause huge data skews). Batches are taken every 1.5 seconds, limited by memory used when creating a hash table.|$|E
40|$|Nowadays <b>log</b> <b>file</b> plays {{vital role}} in web {{forensic}} as digital evidence. Hence security of <b>log</b> <b>file</b> is a major topic of apprehension. In this paper a model of image logging server having alteration detectable capability, is proposed. According to this approach we first convert a text <b>log</b> <b>file</b> into image <b>log</b> <b>file</b> {{with the help of}} bit encoding technique and tamper detection capability is achieved by self embedding fragile watermark scheme. If any alteration is done on image <b>log</b> <b>file</b> then due to nature of fragile watermark, one can easily locate that tampered region. Proposed model is also able to ensure all security requirements like Authenticity, Integrity and confidentiality...|$|R
40|$|Abstract—We {{describe}} and apply a lightweight formal method for checking test results. The method {{assumes that the}} software under test writes a text log file; this <b>log</b> <b>file</b> is then analyzed by a program {{to see if it}} reveals failures. We suggest a state-machine-based formalism for specifying the <b>log</b> <b>file</b> analyzer programs and describe a language and implementation based on that formalism. We report on empirical studies of the application of <b>log</b> <b>file</b> analysis to random testing of units. We describe the results of experiments done to compare the performance and effectiveness of random unit testing with coverage checking and <b>log</b> <b>file</b> analysis to other unit testing procedures. The experiments suggest that writing a formal <b>log</b> <b>file</b> analyzer and using random testing is competitive with other formal and informal methods for unit testing. Index Terms—Testing, specification, safety verification, lightweight formal methods, test oracles, unit testing, <b>log</b> <b>file</b> analysis æ...|$|R
5000|$|...appendtrace: If true, {{then it will}} append {{the trace}} {{at the end of}} a <b>log</b> <b>file.</b> If false, then it will {{override}} the <b>log</b> <b>file</b> for the invocation of wsadmin.|$|R
25|$|Scroogle, {{named after}} the fictional {{character}} Ebenezer Scrooge, was a web service that allowed users to perform Google searches anonymously. It focused heavily on searcher privacy by blocking Google cookies and not saving <b>log</b> <b>files.</b> The service was launched in 2003 by Google critic Daniel Brandt, who was concerned about Google collecting personal information on its users.|$|E
25|$|Facebook uses a {{combination}} platform based on HBase to store data across distributed machines. Using a tailing architecture, new events {{are stored in}} <b>log</b> <b>files,</b> and the logs are tailed. The system rolls these events up and writes them into storage. The user interface then pulls the data out and displays it to users. Facebook handles requests as AJAX behavior. These requests are written to a log file using Scribe (developed by Facebook).|$|E
25|$|File settings: this {{category}} contains settings related to files, file handles, record locks, indexes, and <b>log</b> <b>files.</b> The number of open files and logical file handles {{was set in}} here, {{as well as the}} number of record locks per client; index balancing and an option to create files in pre 6.x format are in {{this category}}. It also controlled whether the Microkernel kept a log of operations executed on selected files. In this section the method of file sharing could be set to either MEFS or SEFS. The system transaction hold limit sets the number of system transactions performed during write operations for shared files.|$|E
40|$|Objective: This study {{sought to}} {{determine}} whether infusion device event logs could support accident investigation. Methods: An incident reporting database was searched for infor-mation about <b>log</b> <b>file</b> use in investigations. <b>Log</b> <b>file</b> data from devices in clinical use were downloaded and electronically searched for characteristics (signatures) matching specific function queries. Dif-ferent programming sequences were simulated, and device logs were downloaded for analysis. Results: Database reports mentioned difficulties resolving <b>log</b> <b>file</b> data to the incident report and used <b>log</b> <b>file</b> data to confirm pro-gramming failures. <b>Log</b> <b>file</b> search revealed that, aside from alarm types and times, the devices were unable to adequately satisfy func-tional queries. Different simulated programming scenarios could not be easily differentiated by <b>log</b> <b>file</b> analysis. Conclusions: The device logs we studied collect data that are poorly suited to accident investigation. We conclude that infusion device logs cannot function as black boxes do in aviation accidents. Logs would be better applied to assist routine operations. Key Words: patient safety, infusion devices, incident reporting, data recording, accident investigatio...|$|R
40|$|This {{experiment}} holds 3 EBG, 3 EBH and 3 EBI. Keyword = HYDROLASE Ancillary File Information: M 1 _ 3 EBG Structure of the M 1 Alanylaminopeptidase from malaria 9 _refmac 5 _final. log Final refinement <b>log</b> <b>file.</b> Refmac v 5. 2. 0019 80 _scala. <b>log</b> Processing <b>log</b> <b>file</b> from SCALA p 212121 _scala 1. mtz Processed data M 1 _BES_ 3 EBH Structure of the M 1 Alanylaminopeptidase from malaria complexed with bestatin 4 _refmac 5 _finalBES. log Final refinement <b>log</b> <b>file.</b> Refmac v 5. 2. 0019 41 _scala. <b>log</b> Processing <b>log</b> <b>file</b> from SCALA xtal 8 c_p 212121 _scala 1. mtz Processed data M 1 _Co 4 _ 3 EBI Structure of the M 1 Alanylaminopeptidase from malaria complexed {{with the}} phosphinate dipeptide analog. osc files Raw image files. In-house Rotating Anode (Rigaku RAxis IV) 12 _refmac 5 _final Co 4. log Final refinement <b>log</b> <b>file.</b> Refmac v 5. 2. 0019 45 _scala. <b>log</b> Processing <b>log</b> <b>file</b> from SCALA 3 EBIco 4 _p 212121 _scala 1. mtz Processed dat...|$|R
40|$|This paper {{reports on}} {{research}} into applying {{the technique of}} <b>log</b> <b>file</b> analysis for checking test results to {{a broad range of}} testing and other tasks. The studies undertaken included applying <b>log</b> <b>file</b> analysis to both unit- and system-level testing and to requirements of both safety-critical and non-critical systems, and the use of <b>log</b> <b>file</b> analysis in combination with other testing methods. The paper also reports on the technique of using <b>log</b> <b>file</b> analyzers to simulate the software under test, both in order to validate the analyzers and to clarify requirements. It also discusses practical issues to do with the completeness of the approach, and includes comparisons to other recently-published approaches to <b>log</b> <b>file</b> analysis. Keywords Testing, specification, safety verification, lightweight formal methods, test oracles 1 INTRODUCTION Testing is an important and costly component of any software development project, and the efficient, reliable validation of test results is an importan [...] ...|$|R
2500|$|After {{the loss}} Kasparov {{said that he}} {{sometimes}} saw deep intelligence and creativity in the machine's moves, suggesting that during the second game, human chess players, in contravention of the rules, intervened. IBM denied that it cheated, saying the only human intervention occurred between games. [...] The rules provided for the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play revealed {{during the course of}} the match. Kasparov requested printouts of the machine's <b>log</b> <b>files</b> but IBM refused, although the company later published the logs on the Internet. Much later, it was suggested that the behavior Kasparov noted may have resulted from a glitch in the computer program. Although Kasparov wanted another rematch, IBM declined and ended their Deep Blue program.|$|E
2500|$|Microsoft SQL Server {{also allows}} {{user-defined}} composite types (UDTs) {{to be defined}} and used. It also makes server statistics available as virtual tables and views (called Dynamic Management Views or DMVs). In addition to tables, a database can also contain other objects including views, stored procedures, indexes and constraints, along with a transaction log. A SQL Server database can contain a maximum of 231 objects, and can span multiple OS-level files with a maximum file size of 260 bytes (1 exabyte). The data in the database are stored in primary data files with an extension [...]mdf. Secondary data files, identified with a [...]ndf extension, are used to allow the data of a single database to be spread across more than one file, and optionally across more than one file system. <b>Log</b> <b>files</b> are identified with the [...]ldf extension.|$|E
50|$|The NetInsight Extract, transform, load {{process can}} read <b>log</b> <b>files</b> in {{virtually}} any format, including logs from web servers, proxy servers, streaming media servers and FTP servers. As well as processing normal server <b>log</b> <b>files</b> NetInsight can use <b>log</b> <b>files</b> derived from page tags to replace or augment log file data.|$|E
30|$|Figure  5 {{shows that}} the {{execution}} time varies linearly according {{the size of the}} original <b>log</b> <b>file.</b> However, it is still acceptable since it reaches a value equal to 2  s for a <b>log</b> <b>file</b> size equal to 3000 queries.|$|R
50|$|This is {{a simple}} inetd service, written in C. It expects a command line {{argument}} containing a filename for a <b>log</b> <b>file,</b> and then it logs all strings sent through the socket to the <b>log</b> <b>file.</b> Note {{that this is a}} very insecure example program.|$|R
30|$|The {{algorithm}} CQGS (Candidate Queries Generation {{using the}} threshold Similarity) extract from the filtered <b>log</b> <b>file</b> {{the set of}} candidate queries {{that are similar to}} the current query taking into account a predetermined similarity threshold s. The algorithm takes as input the number of queries in the filtered <b>log</b> <b>file,</b> the current user query, the SIM function (which computes the spatio-semantic similarity between two MDX queries) and the similarity threshold s. SIM function is used to compute the spatio-semantic similarity values between the current query and the queries presented in the filtered <b>log</b> <b>file.</b>|$|R
5000|$|Coordinating {{the reading}} of the {{transaction}} logs and the archiving of <b>log</b> <b>files</b> (database management software typically archives <b>log</b> <b>files</b> off-line on a regular basis).|$|E
5000|$|This process writes redo <b>log</b> <b>files</b> to disk. <b>Log</b> <b>files</b> contain all {{information}} {{about changes in}} the database's data. They are used for fast transaction processing and restoration.|$|E
5000|$|BES also {{produces}} {{a set of}} <b>log</b> <b>files</b> during operation, called the BES Event Log. The <b>log</b> <b>files</b> include (for a BES v4.0 and 4.1 system connecting to Microsoft Exchange): ...|$|E
30|$|The <b>log</b> <b>file</b> {{containing}} previous queries already {{launched on}} the DW {{can be very}} large {{because of the high}} number of queries and users. The time of recommendation can significantly increase. To address this problem, we propose to preprocess the <b>log</b> <b>file</b> to remove non-relevant queries in the recommendation process. The filtering criterion of the <b>log</b> <b>file</b> is the execution date (the age) of a query defined as a parameter of this phase to be settled by the user or the administrator of SOLAP system. Only relatively recent queries are considered in the recommendation process.|$|R
40|$|Over {{many years}} {{transaction}} log analysis (also called <b>log</b> analysis, <b>log</b> <b>file</b> analysis, or <b>log</b> tracking, and more lately web <b>logging,</b> web <b>log</b> <b>file</b> analysis, and web tracking) {{has been used}} to collect information on how information systems such as online library catalogues (OPACs), online and CD-ROM databases, and web-base...|$|R
40|$|A {{procedure}} for querying metadata {{records of the}} type   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian <b>Log</b> <b>file</b> and to view the computed molecule in JSmol. A {{procedure for}} querying metadata records of the type   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian <b>Log</b> <b>file</b> and to view the computed molecule in JSmol. A procedure for querying metadata records of the type   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian <b>Log</b> <b>file</b> and to view the computed molecule in JSmol...|$|R
5000|$|Redo <b>log</b> <b>files,</b> {{recording}} all {{changes to}} the database - used to recover from an instance failure. Often, a database stores these files multiple times for extra security in case of disk failure. Identical redo <b>log</b> <b>files</b> are associated in a [...] "group".|$|E
50|$|NetTracker {{processes}} <b>log</b> <b>files</b> {{in virtually}} any format using a custom log definition. Logs can be processed from web servers, proxy servers, streaming media servers and FTP servers. As well as processing <b>log</b> <b>files</b> NetTracker can use page tag data to augment log file data.|$|E
50|$|Beacons {{are still}} {{received}} on regular basis in March 2011. In addition AAUSAT-II does receive and acknowledge commands from ground and <b>log</b> <b>files</b> has been requested and received. Due {{to the very}} high tumbling (more than 2.5 Hz) {{it has not been}} possible to decode <b>log</b> <b>files.</b>|$|E
5000|$|...tracefile assigns <b>log</b> <b>file</b> {{name and}} {{location}} for the log output.|$|R
3000|$|Liu and Fan (2011 a) {{presents}} an approach {{which is based}} on instruction execution trace/replay mechanism with CPU scheduling. This method also transfers <b>log</b> <b>file</b> in place of complete page and also manages the speed of <b>log</b> <b>file</b> generation by adjusting CPU scheduling. It improves migration performance, but it has not been tested for complex environment.|$|R
50|$|This command asks WEPCrack to {{determine}} the key from the <b>log</b> <b>file.</b>|$|R
50|$|Log {{monitors}} are a type {{of software}} that monitor <b>log</b> <b>files.</b> Servers, application, network and security devices generate <b>log</b> <b>files.</b> Errors, problems, and more information is constantly logged and saved for analysis. In order to detect problems automatically, system administrators and operations set up monitors on the generated logs. The log monitors scan the <b>log</b> <b>files</b> and search for known text patterns and rules that indicate on important events. Once an event is detected, the monitoring system will send alert, either to a person or to another software/hardware system.|$|E
5000|$|If law {{enforcement}} officials suspect illegal activity, they can request logs from the user's Internet provider. Internet providers that emphasize protection of personal data will typically only save their <b>log</b> <b>files</b> {{for a few days}} , at which time they are deleted/overwritten by rotation. Many providers, however, keep <b>log</b> <b>files</b> indefinitely [...]|$|E
5000|$|The {{emulator}} stores {{information of}} sequential execution blocks (SEBs) for multiple processors in <b>log</b> <b>files,</b> with each SEB recording the messages sent, their sources and destinations, dependencies, timings, etc. The simulator reads the <b>log</b> <b>files</b> and simulates them, and may star additional messages {{which are then}} also stored as SEBs.|$|E
5000|$|In most {{database}} management systems durable database transactions are supported through a <b>log</b> <b>file.</b> However, multiple writes {{to the same}} page of that file can produce a slim chance of data loss. Assuming for simplicity that the <b>log</b> <b>file</b> is organized in pages whose size matches the block size of its underlying medium, the following problem can occur: ...|$|R
30|$|The {{following}} proposals {{advocate a}} publicly available <b>log</b> <b>file</b> containing all certificate transactions.|$|R
5000|$|The main {{advantages}} of <b>log</b> <b>file</b> analysis over page tagging are as follows: ...|$|R
