2|10000|Public
50|$|The {{software}} {{allows for}} the input of kanji symbols via the stylus, and for the <b>lookup</b> <b>of</b> <b>words</b> in English, {{as well as the}} two Japanese alphabets (hiragana and katakana), and also provides pronunciation. It also provides a quiz mode for added educational value. The software is intended to help Japanese people learning English with the TOEIC, or Test of English for International Communication. The software's official web site claims the dictionary offers 13,000 words.|$|E
40|$|This paper {{describes}} a query segmentation method for search en-gines supporting inverse <b>lookup</b> <b>of</b> <b>words</b> and phrases. Data min-ing in query logs and document corpora {{is used to}} produce segment candidates and compute connexity measures. Candidates are con-sidered in context of the whole query, {{and a list of}} the most likely segmentations is generated, with each segment attributed with a connexity value. For each segmentation a segmentation score is computed from connexity values of non-trivial segments, which {{can be used as a}} sorting criterion for the segmentations. We also point to a relevancy improvement in query evaluation model by means of proximity penalty. Keywords web search, query processing, data mining, query segmentation, query evaluation 1...|$|E
40|$|Languages {{with complex}} morphologies present {{difficulties}} for dictionaries users. One {{solution to this}} problem is to use a morphological parser for <b>lookup</b> <b>of</b> morphologically complex <b>words,</b> including fully inflected words, without the user needing to explicitly know the morphology. We discuss the sorts of morphologies which cause the greatest need for such an interface. ...|$|R
40|$|In this paper, {{we propose}} an {{algorithm}} to translit-erate between several Indian languages. The main {{aim of the}} algorithm is {{to assist in the}} translation process by providing efficient transliteration. This algorithm works on Unicode transformation for-mat of an Indian language. It then transliterates it into the Unicode transformation format of the target language. It does no sort <b>of</b> bilingual dictio-nary <b>lookup</b> <b>of</b> the <b>word.</b> It can be used to translit-erate nouns (e. g. named entities) in the transla-tion process as well as for transliterating some text into other language which is more suitable for the reader. ...|$|R
40|$|The {{dictionary}} <b>lookup</b> <b>of</b> unknown <b>words</b> {{is particularly}} difficult in Japanese {{due to the}} requirement of knowing the correct word reading. We propose a system which supplements partial knowledge <b>of</b> <b>word</b> readings by allowing learners of Japanese to look up words according to their expected, but not necessarily correct, reading. This is an improvement from previous systems which provide no handling of incorrect readings. In preprocessing, we calculate the possible readings each kanji character can take and different types of phonological alternations and reading errors that can occur, and associate a probability with each. Using these probabilities and corpus-based frequencies we calculate a plausibility measure for each generated reading given a dictionary entry, based on the naive Bayes model. In response to a user-entered reading, the system displays a list of candidate dictionary entries for the user to choose from. The system is implemented in a web-based environment and available for general use. In the evaluation on Japanese Proficiency Test data and naturally occurring misreading data, the system significantly {{reduced the number of}} unsuccessful dictionary queries when queried with incorrect readings...|$|R
40|$|The work {{reported}} in this paper {{was the result of}} the need to label a large corpus of spontaneous, task-oriented dialogue with prosodic prominences. A computational model using only <b>word</b> duration, part <b>of</b> speech and a dictionary <b>lookup</b> <b>of</b> each <b>word's</b> canonical phonemic contents was trained against the results of a human coder marking prominence. Because word durations were normalised, it was possible to set a common threshold for all members of a form class above which the lexically stressed syllables were classed as prominent. The method used is presented and the relative importance of duration information, phonemic contents, syllabic context and part of speech information is explored. The automatic coder was validated against unseen material and achieved a 58 % agreement with a human coder. Further investigation showed that three humans coders agreed no better with each other than each agreed with the computational model. Thus, although the automatic system did not conform very well t [...] ...|$|R
40|$|The {{traditional}} methods for Arabic OCR (AOCR) based on segmentation <b>of</b> each <b>word</b> into {{a set of}} characters. The Arabic language is of cursive nature, and the character's shape depends on {{its position in the}} word. There are about 100 shape of the characters have to be classified, and some of them may be overlapped. Our approach use a normalized signature of the time signal of the pulse coupled neural network PCNN, supported with some shape primitives to represent the number <b>of</b> the <b>word</b> complementary and their positions within the image <b>of</b> the <b>word.</b> A <b>lookup</b> dictionary <b>of</b> <b>words</b> with its signatures was constructed, and structured in groups using a decision tree. The tested signature was routed through the tree to the nearest group, and then the signature and its related word with higher correlation within the selected group will be the classified. This method overcome many difficulties arise in cursive word recognition CWR for printed script with different font type and size; also it shows higher accuracy for the classification process, 96 %...|$|R
40|$|We {{present an}} {{approach}} for searching and exploring translation variants of multi-word units in large multiparallel corpora {{based on a}} relational database management system. Our web-based application Multilingwis, which allows for multilingual <b>lookups</b> <b>of</b> phrases and <b>words</b> in English, French, German, Italian and Spanish, is of interest to {{anybody who wants to}} quickly compare expressions across several languages, such as language learners without linguistic knowledge. In this paper, we focus on the technical aspects of how to represent and efficiently retrieve all occurrences that match the user’s query in one of five languages simultaneously with their translations into the other four languages. In order to identify such translations in our corpus of 220 million tokens in total, we use statistical sentence and word alignment. By using materialized views, composite indexes, and pre-planned search functions, our relational database management system handles large result sets with only moderate requirements to the underlying hardware. As our systematic evaluation on 200 search terms per language shows, we can achieve retrieval times below 1 second in 75...|$|R
40|$|This paper {{reports on}} the results of our {{experiments}} in the Monolingual English, German and Portuguese tasks and the Bilingual Spanish → English, Spanish → Portuguese tasks. We also present initial results on the recognition, extraction and categorization of web-based queries for the Query Parsing task. Twenty-three runs were submitted as official runs, 16 for the monolingual task and seven for the bilingual task. We used the Terrier Information Retrieval Platform to run experiments for both tasks using the Inverse Document Frequency model with Laplace after-effect and normalization 2 and the Ponte-Croft language model. Experiments included topics processed automatically as well as topics processed manually. Manual processing of topics was carried out for the bilingual task using the transfer approach in machine translation. Topics were pre-processed automatically to eliminate stopwords. Results show that automatic relevance feedback with 5 terms and 20 documents performs better, in general. The initial approach used in the Query Parsing task is a pattern-based approach. Due to the ungrammaticality, multilinguality and ambiguity of the language in the 800, 000 web-based queries in the collection, we started by building {{a list of all the}} different words in the queries, similar to creating an index. Next, a <b>lookup</b> <b>of</b> the <b>words</b> was done in a list of countries to identify potential locations. Because many locations were missed, we further analyzed the queries looking for spatial prepositions and syntactic cues. Queries were processed by combining search in gazetteers with a set of patterns. Categorization was also based on patterns. Results were low in terms of recall and precision...|$|R
5000|$|URIBL <b>lookups</b> <b>of</b> senders IP, helo hostname, {{envelope}} sender, {{and message}} contents ...|$|R
5000|$|Pointer fields {{allowing}} <b>lookup</b> <b>of</b> {{another record}} in the database (constrained or unconstrained) ...|$|R
40|$|<b>Lookup</b> <b>of</b> {{services}} {{is an important}} issues in many distributed systems. This paper deals with lookup in service-oriented architectures, such as Web services, P 2 P systems, GRIDs, or spontaneous networks. Service-oriented architectures impose specific requirements onto the lookup service, for instance regarding the runtime extensibility <b>of</b> <b>lookup</b> models, runtime extensibility <b>of</b> <b>lookup</b> queries, construction <b>of</b> complex <b>lookup</b> queries, scalability, and fault tolerance...|$|R
5000|$|Four corner method, a 4-digit {{structural}} {{encoding method}} designed to aid <b>lookup</b> <b>of</b> telegraph codes ...|$|R
50|$|When {{designed}} well, a map database {{enables the}} rapid indexing and <b>lookup</b> <b>of</b> {{a large amount}} of geographic data.|$|R
50|$|For <b>lookups</b> <b>of</b> keys with {{multiple}} values, additional values {{may be found}} by simply resuming the search at the next slot.|$|R
40|$|Extending LOGS ontologizer to multi-language environments [Abstract: Ontologies greatly {{enhance our}} ability to machine process digital {{documents}} and aid in knowledge engineering. They also augment search algorithms. LOGS, Lightweight universal Ontology Generation and exploitation architectureS, and its sample application, the Eagle, {{took a step toward}} automatic ontology generation, through a fast, lightweight approach. Sanskritology and Indology are interesting domains where most of modern research is carried out in the West, often by transliterating Sanskrit texts into Roman scripts with many scholars relying on definitive translations of these texts for their study. However, as most of the Scriptural texts were preserved by oral traditions, through the patrilineal system, a number of recensions exist for various texts, some complete and some not; only a few of these were ever translated. In addition, a number of these books repeat portions of each other. Texts could be presented in native or exploded forms. The study itself is highly interpretive. Therefore, the validation, ontoligizing and rendition through intelligent interfaces, of various recensions of a particular book, present a very rich problem for methods such as LOGS. We describe the use of LOGS in an ontological server, which generates and maintains ontologies of Sanskrit texts and demonstrate its use on versions of the Sama Veda, one of the complete Hindu Scriptures. To do this, we need to process natural language in Sanskrit. To show how we accomplish this NLP, we describe the heuristic algorithms of the Vyakarana API we have developed, which is the first known adaptive Sanskrit grammar and transliteration tool for this area. As its grammar evolved with Sanskrit over centuries, Vyakarana is by design adaptive to its subject. Vyakarana is used in the ontology generation step of LOGS and is also used in intelligent domain queries such as generating concordances, dictionary <b>lookup</b> <b>of</b> compound <b>words</b> and in dynamic transliterations. Finally, we comment on how such an ontolgized book could be easily integrated into and cross-referenced in a searchable digital repository and the implications of a successful utilization of LOGS in Sanskritology to other domains. ] Ontologies greatly enhance {{our ability to}} machine process digital documents and aid i...|$|R
5000|$|ISPs {{that have}} {{implemented}} DNS hijacking can break the DNS <b>lookup</b> <b>of</b> the WPAD protocol by directing users {{to a host}} {{that is not a}} proxy server.|$|R
30|$|The {{introduction}} of extended states would attain a natural boosting by repeated <b>lookup</b> <b>of</b> the tabulated statistics associated {{in each case}} with the given type of coding/noncoding boundary.|$|R
50|$|In {{computer}} science, a term {{index is}} a data structure to facilitate fast <b>lookup</b> <b>of</b> terms and clauses in a logic program, deductive database, or automated theorem prover.|$|R
30|$|By {{considering}} each index as {{a particle}} {{in the search}} space, the continuous search space is explored by using PGSA for a <b>lookup</b> <b>of</b> circle parameters [x_ 0,y_ 0,r].|$|R
50|$|Call routing - The {{automatic}} routing {{of calls}} {{to a new}} destination based on criteria normally involving a database <b>lookup</b> <b>of</b> the caller's number (ANI) or number dialed (DNIS).|$|R
5000|$|Translating Y′UV420p to RGB {{is a more}} {{involved}} process compared to the previous formats. <b>Lookup</b> <b>of</b> the Y′, U and V values can be done using the following method: ...|$|R
5000|$|For example, to do {{a reverse}} <b>lookup</b> <b>of</b> the IP address [...] the PTR {{record for the}} domain name [...] would be looked up, and found to point to [...]|$|R
50|$|There {{is a tool}} which {{assists in}} the <b>lookup</b> <b>of</b> these code sets at the JP1 group site, and {{additional}} codes can also generally {{be obtained from the}} remote control manufacturer or supplier.|$|R
5000|$|QRZ is amateur radio code {{also known}} as a Q code for [...] "Who is calling me?". This matches the purpose of the site, which is to assist the <b>lookup</b> <b>of</b> radio callsigns.|$|R
50|$|Implementing both lists in one doubly linked list, where nodes {{following}} the current node are the later portion and all else are the now list follows. Using {{an array of}} pre-allocated nodes in the list for each node in the grid, access time to nodes in the list is reduced to a constant. Similarly, a marker array allows <b>lookup</b> <b>of</b> a node in the list {{to be done in}} constant time. g is stored as a hash-table, and a last marker array is stored for constant-time <b>lookup</b> <b>of</b> whether or not a node has been visited before and if a cache entry is valid.|$|R
50|$|On 11 January 2012, two Dutch Internet service {{providers}} (Ziggo and XS4ALL) were ordered {{by a court}} in The Hague to disable <b>lookups</b> <b>of</b> The Pirate Bay's domain names and to block access to The Pirate Bay's IP addresses.|$|R
50|$|When a {{data set}} may be updated dynamically, {{it may be}} stored in a Fenwick tree data {{structure}}. This structure allows both the <b>lookup</b> <b>of</b> any individual prefix sum value and the modification of any array value in logarithmic time per operation.|$|R
40|$|<b>Lookup</b> <b>of</b> {{services}} {{is an important}} issues in many distributed systems. This paper deals with lookup in service-oriented architectures, such as Web services, P 2 P systems, GRIDs, or spontaneous networks. Service-oriented architectures impose specific requirements onto the lookup service, for instance regarding the runtime extensibility <b>of</b> <b>lookup</b> models, runtime extensibility <b>of</b> <b>lookup</b> queries, construction <b>of</b> complex <b>lookup</b> queries, scalability, and fault tolerance. These requirements are not well solved by existing lookup approaches. We propose a semantic lookup service using Semantic Web ontologies, expressed in RDF. Query scripts are sent from the client to the server and are interpreted at server side using the RDF repository. We also present a safe, scalable, and efficient architecture for defining and querying lookup information using this lookup service concept. ...|$|R
25|$|Another {{advantage}} of Gradshteyn and Ryzhik compared to computer algebra systems {{is the fact}} that all special functions and constants used in the evaluation of the integrals are listed in a registry as well, thereby allowing reverse <b>lookup</b> <b>of</b> integrals based on special functions or constants.|$|R
30|$|The Orchestrator is {{composed}} of different modules. The Communication Manager handles the communication among the different Orchestrator instances; the Resource Manager monitors the resource usage of the infrastructure; the Service Discovery enables the <b>lookup</b> <b>of</b> services available in the nearest location; the Security Manager provides different authentication and privacy mechanisms.|$|R
5000|$|Teampage Spring 2013 release: Unified {{search for}} quick <b>lookup</b> <b>of</b> people, spaces, tasks, {{projects}} and milestones; [...] "finish later" [...] and autosave support for articles, tasks, projects, milestones and comments; updated IOS 6 iPad and iPhone support; streamlined header design and navigation; Cloud pricing options, first month free; developer updates.|$|R
5000|$|This would read: [...] "Clunky for an {{environment}} and two variables, {{in case the}} <b>lookups</b> <b>of</b> the variables from the environment produce values, {{is the sum of}} the values. ..." [...] As in list comprehensions, the guards are in series, and if any of them fails the branch is not taken.|$|R
5000|$|After [...] {{had served}} its {{transitional}} purpose, it proved impractical {{to remove the}} domain, because in-addr.arpa was used for reverse DNS <b>lookup</b> <b>of</b> IP addresses. For example, the mapping of the IP address 192.0.2.155 to a host name is obtained by issuing a DNS query for a pointer record of the domain name155.2.0.192.in-addr.arpa.|$|R
50|$|In the C++ {{programming}} language, argument-dependent lookup (ADL), or argument-dependent name lookup, {{applies to}} the <b>lookup</b> <b>of</b> an unqualified function name depending on the types of the arguments given to the function call. This behavior {{is also known as}} Koenig lookup, as it is often attributed to Andrew Koenig, though he is not its inventor.|$|R
5000|$|... a B2C (business-to-consumer) website. The company's website {{was first}} {{launched}} as InvestorsEdge.com in 1995, and {{changed its name}} to Stockpoint.com in 1997. The site offered free <b>lookup</b> <b>of</b> stock quotes, charts of historical performance, mutual fund comparisons, financial news, and more. Competitors in Stockpoint's early years included Quote.com and MSN Investor (now MSN MoneyCentral).|$|R
