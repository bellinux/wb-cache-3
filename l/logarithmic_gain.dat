11|16|Public
50|$|In audio {{applications}} <b>logarithmic</b> <b>gain</b> {{control is}} used to emulate how the ear hears loudness. David E. Blackmer's dbx 202 VCA {{was among the first}} successful implementations of a logarithmic VCA.|$|E
3000|$|... {{and then}} the BS chooses the strongest user for {{transmission}} with full power. This asymptote shows that, at low SNR, the multiplexing gain is lost but the multiuser diversity gives logarithmic instead of double <b>logarithmic</b> <b>gain.</b>|$|E
40|$|We {{investigate}} quantum {{interrogation techniques}} which allow imaging information about semi-transparent objects to be obtained with lower absorption rates than standard classical methods. We {{show that a}} gain proportional to log N can be obtained when searching for defects {{in an array of}} N pixels, if it is known that at most M of the pixels can have transparencies different from a predetermined theoretical value. A <b>logarithmic</b> <b>gain</b> can also be obtained when searching for infrequently occurring large structures in arrays. Comment: 3 page...|$|E
40|$|Iterative source coded {{modulation}} (ISCM) {{improves the}} error concealment for source codec parameters without increasing the transmitted bit rate by combining iterative demodulation of higher order modulations and {{the usage of}} residual source redundancy in a Turbo process. In this paper we present the enhanced capabilities of ISCM when novel index assignments are applied. A doubling of the previously achievable <b>logarithmic</b> <b>gains</b> [1] is possible. Furthermore, the convergence properties of ISCM are analyzed by EXIT charts and the computational complexity is compared to a rate- 1 iterative source-channel decoding (ISCD) system [2]. ISCM exhibits a competitive performance in relation to an equally complex ISCD system. 1...|$|R
40|$|Summary: PLMaddon is a General Public License (GPL) {{software}} module designed {{to expand the}} current version of the SBToolbox (a MatlabTM toolbox for systems biology; www. sbtoolbox. org) with a set of functions for the analysis of power-law models, a specific class of kinetic models, set in ordinary differential equations (ODE) and in which the kinetic orders can have positive/negative non-integer values. The module includes functions to generate power-law Taylor expansions of other ODE models (e. g. Michaelis-Menten type models), as well as algorithms to estimate steady-states. The robustness and sensitivity of the models can also be analysed and visualized by computing the power-law’s <b>logarithmic</b> <b>gains</b> and sensitivities. Availability: PLMaddon is an open source module for the analysis of power-law models based on the SBToolbox. The latest version of PLMaddon is freely available from: www. sbi. uni-rostock. de/plmaddon The website contains a tutorial with examples, as well as an interactive introductory course on power-law models in systems biology. Contact...|$|R
40|$|The meta-cleavage pathway of Pseudomonas putida mt- 2 was {{simulated}} using a biochemical systems simulation {{developed by}} Regan (1996). A non-competitive inhibition term for catechol- 2, 3 -dioxygenase (C 230) by 2 -OH-pent- 2, 4 -dienoate (K(i) = 150 μM) {{was incorporated into}} the model. The simulation predicted steady state accumulation levels in the μM range for metabolites pre-meta- cleavage, and in the μM range for metabolites post-meta-cleavage. The <b>logarithmic</b> <b>gains</b> L[V(i) / -, X(j) ] and L[X(i) / -, X(j) ] clearly indicated that the pathway was most sensitive to the concentration of the starting substrate, benzoate, and the first enzyme of the pathway, toluate- 1, 2 - dioxygenase (TO). The simulation was validated experimentally; {{it was found that}} the amplification of TO increased the steady state flux from 0. 024 to 0. 091 (mmol/g cell dwt) /h. This resulted in an increased accumulation of a number of the pathway metabolites (intra and extracellularly), especially cis-diol, 4 -OH- 2 -oxovalerate, and 4 -oxalocrotonate. Metabolic control analysis indicated that C 230 was, in fact, the major controlling enzymic step of the pathway with a scaled control coefficient of 0. 83. The amplification of TO resulted in a shift of some of the control away from C 230. Catechol- 2, 3 -dioxygenase, however, remained as the major controlling element of the pathway...|$|R
40|$|Measurements of {{the optical}} gain in a {{semiconductor}} laser using a 20 MHz resolution optical spectrum analyzer are presented {{for what is}} believed to be the first time. The high resolution allows for accurate gain measurements close to the lasing threshold. This is demonstrated by gain measurements on a bulk InGaAsP 1. 5 µm Fabry-Perot laser. Combined with direct measurement of transparency carrier density values, parameters were determined for characterizing the gain at a range of wavelengths and temperatures. The necessity of the use of a <b>logarithmic</b> <b>gain</b> model is shown...|$|E
40|$|Abstract—This paper {{presents}} a fast received signal strength indicator (RSSI) circuit for wireless communi-cation application. The proposed circuit is developed using power detectors and an analog-to-digital con-verter {{to achieve a}} fast settling time. The power de-tector is consisted of a novel logarithmic variable gain amplifier (VGA), a peak detector, and a comparator in a closed loop. The VGA achieved a wide <b>logarithmic</b> <b>gain</b> range in a closed loop form for stable operation. For the peak detector, a fast settling time and small ripple are obtained using the orthogonal characteristics of quadrature signals. In 0. 18 -μm CMOS process, the RSSI value settles down in 20 μs with power consump-tion of 20 mW, and the maximum ripple of the RSSI is 30 mV. The proposed RSSI circuit is fabricated with a personal handy-phone system transceiver. The active area is 0. 8 × 0. 2 mm 2. Index Terms—Logarithmic, peak detector, powe...|$|E
40|$|We {{present the}} first hybrid {{measurement}} of the average muon number in air showers at ultra-high energies, initiated by cosmic rays with zenith angles between 62 ^∘ and 80 ^∘. The measurement is based on 174 hybrid events recorded simultaneously with the Surface Detector array and the Fluorescence Detector of the Pierre Auger Observatory. The muon number for each shower is derived by scaling a simulated reference profile of the lateral muon density distribution at the ground until it fits the data. A 10 ^ 19 eV shower with a zenith angle of 67 ^∘, which arrives at the Surface Detector array {{at an altitude of}} 1450 m above sea level, contains on average (2. 68 ± 0. 04 ± 0. 48 (sys.)) × 10 ^ 7 muons with energies larger than 0. 3 GeV. The <b>logarithmic</b> <b>gain</b> dN_μ / dE of muons with increasing energy between 4 × 10 ^ 18 eV and 5 × 10 ^ 19 eV is measured to be (1. 029 ± 0. 024 ± 0. 030 (sys.)). Comment: Replaced with published version. Added journal reference and DO...|$|E
40|$|We {{consider}} sequential or active {{ranking of}} a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question {{as to whether or not}} imposing these parametric assumptions allows for improved ranking algorithms. For stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. This shows, perhaps surprisingly, that these popular parametric modeling choices offer at most <b>logarithmic</b> <b>gains</b> for stochastic comparisons. Comment: improved log factor in main result; added discussion on comparison probabilities close to zero; added numerical result...|$|R
40|$|ABSTRACT We have {{determined}} {{the effects of}} control by overall feedback inhibition on the systemic behavior of un-branched metabolic pathways with an arbitrary pattern of other feedback inhibitions by using a recently developed numerical generalization of Mathematically Controlled Comparisons, a method for comparing the function of alternative molecular designs. This method allows the rigorous determination {{of the changes in}} systemic properties that can be exclusively attributed to overall feedback inhibition. Analytical results show that the unbranched pathway can achieve the same steady-state flux, concentrations, and <b>logarithmic</b> <b>gains</b> with respect to changes in substrate, with or without overall feedback inhibition. The analytical approach also shows that control by overall feedback inhibition amplifies the regulation of flux by the demand for end product while attenuating the sensitivity of the concentrations to the same demand. This approach does not provide a clear answer regarding the effect of overall feedback inhibition on the robustness, stability, and transient time of the pathway. However, the generalized numerical method we have used does clarify the answers to these questions. On average, an unbranched pathway with control by overall feedback inhibition is less sensitive to perturbations in the values of the parameters that define the system. The difference in robustness can range from a few percent to fifty percent or more, depending on the length of the pathway and on the metabolite one considers. On average, overall feedback inhibition decreases the stability margins by a minimal amount (typically less than 5 %). Finally, and again on average, stable systems with overall feedback inhibition respond faster to fluctuations in the metabolite concentrations. Taken together, these result...|$|R
40|$|AbstractWe have {{determined}} {{the effects of}} control by overall feedback inhibition on the systemic behavior of unbranched metabolic pathways with an arbitrary pattern of other feedback inhibitions by using a recently developed numerical generalization of Mathematically Controlled Comparisons, a method for comparing the function of alternative molecular designs. This method allows the rigorous determination {{of the changes in}} systemic properties that can be exclusively attributed to overall feedback inhibition. Analytical results show that the unbranched pathway can achieve the same steady-state flux, concentrations, and <b>logarithmic</b> <b>gains</b> with respect to changes in substrate, with or without overall feedback inhibition. The analytical approach also shows that control by overall feedback inhibition amplifies the regulation of flux by the demand for end product while attenuating the sensitivity of the concentrations to the same demand. This approach does not provide a clear answer regarding the effect of overall feedback inhibition on the robustness, stability, and transient time of the pathway. However, the generalized numerical method we have used does clarify the answers to these questions. On average, an unbranched pathway with control by overall feedback inhibition is less sensitive to perturbations in the values of the parameters that define the system. The difference in robustness can range from a few percent to fifty percent or more, depending on the length of the pathway and on the metabolite one considers. On average, overall feedback inhibition decreases the stability margins by a minimal amount (typically less than 5 %). Finally, and again on average, stable systems with overall feedback inhibition respond faster to fluctuations in the metabolite concentrations. Taken together, these results show that control by overall feedback inhibition confers several functional advantages upon unbranched pathways. These advantages provide a rationale for the prevalence of this control mechanism in unbranched metabolic pathways in vivo...|$|R
40|$|A {{high power}} {{semiconductor}} laser {{with a novel}} lateral design using angular filtering by total reflection for increased brightness is demonstrated. In this so called "Z-Laser" two inner surfaces guide the laser beam by total reflection in a Z shaped path through the laser. Higher order laser modes with larger divergence angles are suppressed because of a smaller reflectivity. This results in a reduced far-field angle. Simulations based on a two-dimensional steady state wave equation solved by using the Pade approximation, an one-dimensional carrier diffusion equation and a <b>logarithmic</b> <b>gain</b> model have been performed to design the device. First prototypes of the laser were fabricated on MBE grown InGaAs/AlGaAs wafers. The inner surfaces providing the refractive index step necessary for total reflection were prepared by chemically assisted ion-beam etching. Single lasers were mounted junction side down on copper heat-sinks. They show lateral far field angles smaller than 2 deg FWHM in excellent agreement with numerical simulations. Output powers of more than 500 mW cw out of a 36 µm facet have been reached. In conclusion, the "Z-Laser" is a promising new design for high power, high brightness semiconductor laser diodes...|$|E
40|$|See {{paper for}} full list of authors – 12 pages, 6 figures, {{accepted}} by Physical Review DInternational audienceWe present the first hybrid {{measurement of the}} average muon number in air showers at ultra-high energies, initiated by cosmic rays with zenith angles between 62 ^∘ and 80 ^∘. The measurement is based on 174 hybrid events recorded simultaneously with the Surface Detector array and the Fluorescence Detector of the Pierre Auger Observatory. The muon number for each shower is derived by scaling a simulated reference profile of the lateral muon density distribution at the ground until it fits the data. A 10 ^ 19 eV shower with a zenith angle of 67 ^∘, which arrives at the Surface Detector array {{at an altitude of}} 1450 m above sea level, contains on average (2. 68 ± 0. 04 ± 0. 48 (sys.)) × 10 ^ 7 muons with energies larger than 0. 3 GeV. The <b>logarithmic</b> <b>gain</b> dN_μ / dE of muons with increasing energy between 4 × 10 ^ 18 eV and 5 × 10 ^ 19 eV is measured to be (1. 029 ± 0. 024 ± 0. 030 (sys.)) ...|$|E
40|$|J. A. Bellido, R. W. Clay, M. J. Cooper, B. R. Dawson, T. D. Grubb, T. A. Harrison, G. C. Hill, M. Malacari, S. J. Saffi, J. Sorokin {{are members}} of The Pierre Auger CollaborationWe present the first hybrid {{measurement}} of the average muon number in air showers at ultrahigh energies, initiated by cosmic rays with zenith angles between 62 ° and 80 °. The measurement is based on 174 hybrid events recorded simultaneously with the surface detector array and the fluorescence detector of the Pierre Auger Observatory. The muon number for each shower is derived by scaling a simulated reference profile of the lateral muon density distribution at the ground until it fits the data. A 1019 [*][*]eV shower with a zenith angle of 67 °, which arrives at the surface detector array {{at an altitude of}} 1450 m above sea level, contains on average (2. 68 ± 0. 04 ± 0. 48 (sys)) × 107 muons with energies larger than 0. 3 GeV. The <b>logarithmic</b> <b>gain</b> dlnNμ/dlnE of muons with increasing energy between 4 × 1018 [*][*]eV and 5 × 1019 [*][*]eV is measured to be (1. 029 ± 0. 024 ± 0. 030 (sys)). A. Aab [...] . J. A. Bellido [...] . R. W. Clay [...] . M. J. Cooper [...] . B. R. Dawson [...] . T. D. Grubb [...] . T. A. Harrison [...] . G. C. Hill [...] . M. Malacari [...] . S. J. Saffi [...] . J. Sorokin [...] . et al. (Pierre Auger Collaboration...|$|E
40|$|AbstractWhat is the {{smallest}} multilayer perceptron able to compute arbitrary and random functions? Previous results show that a net with one hidden layer containing N − 1 threshold units is capable of implementing an arbitrary dichotomy of N points. A construction is presented here for implementing an arbitrary dichotomy with one hidden layer containing [Nd] units, for any set of N points in general position in d dimensions. This is in fact {{the smallest}} such net as dichotomies which cannot be implemented by any net with fewer units are described. Several constructions are presented of one-hidden-layer nets implementing arbitrary functions into the e-dimensional hypercube. One of these has only [4 Nd][e[log 2 (Nd) ]] units in its hidden layer. Arguments based on a function counting theorem of Cover establish that any net implementing arbitrary functions must have at least Nelog 2 (N) weights, so that no net with one hidden layer containing less than Ne/(d log 2 (N)) units will suffice. Simple counts also show that if the weights are only allowed to assume one of ng possible values, no net with fewer than Nelog 2 (ng) weights will suffice. Thus the gain coming from using real valued synapses appears to be only logarithmic. The circuit implementing functions into the e hypercube realizes such <b>logarithmic</b> <b>gains.</b> Since the counting arguments limit below only the number of weights, the possibility is suggested that, if suitable restrictions are imposed on the input vector set to avoid topological obstructions, two-hidden-layer nets with O(N) weights but only O(√N) threshold units might suffice for arbitrary dichotomies. Interesting and potentially sufficient restrictions include (a) if the vectors are binary, i. e., lie on the d hypercube or (b) if they are randomly and uniformly selected from a bounded region...|$|R
40|$|We have {{determined}} {{the effects of}} control by overall feedback inhibition on the systemic behavior of unbranched metabolic pathways with an arbitrary pattern of other feedback inhibitions by using a recently developed numerical generalization of Mathematically Controlled Comparisons, a method for comparing the function of alternative molecular designs. This method allows the rigorous determination {{of the changes in}} systemic properties that can be exclusively attributed to overall feedback inhibition. Analytical results show that the unbranched pathway can achieve the same steady-state flux, concentrations, and <b>logarithmic</b> <b>gains</b> with respect to changes in substrate, with or without overall feedback inhibition. The analytical approach also shows that control by overall feedback inhibition amplifies the regulation of flux by the demand for end product while attenuating the sensitivity of the concentrations to the same demand. This approach does not provide a clear answer regarding the effect of overall feedback inhibition on the robustness, stability, and transient time of the pathway. However, the generalized numerical method we have used does clarify the answers to these questions. On average, an unbranched pathway with control by overall feedback inhibition is less sensitive to perturbations in the values of the parameters that define the system. The difference in robustness can range from a few percent to fifty percent or more, depending on the length of the pathway and on the metabolite one considers. On average, overall feedback inhibition decreases the stability margins by a minimal amount (typically less than 5 %). Finally, and again on average, stable systems with overall feedback inhibition respond faster to fluctuations in the metabolite concentrations. Taken together, these results show that control by overall feedback inhibition confers several functional advantages upon unbranched pathways. These advantages provide a rationale for the prevalence of this control mechanism in unbranched metabolic pathways in vivo...|$|R
40|$|Nearest {{neighbor}} {{cells in}} R d, d ∈ N, {{are used to}} define coefficients of divergence (φ-divergences) between continuous multivariate samples. For large sample sizes, such distances are shown to be asymptotically normal with a variance depending on the underlying point density. The finite-dimensional distributions of the point measures induced by the coefficients of divergence converge to those of a generalized Gaussian field with a covariance structure determined by the point densities. In d = 1, this extends classical central limit theory for sum functions of spacings. The general results yield central limit theorems for <b>logarithmic</b> k-spacings, information <b>gain,</b> log-likelihood ratios, {{and the number of}} pairs of sample points within a fixed distance of each other. ...|$|R
40|$|High-power {{spatially}} single-mode diode lasers at 1. 4 - 1. 5 µm wavelength are {{of interest}} as pump lasers for Raman and rare-earth doped fiber amplifiers {{as well as for}} material processing and for Light Detection and Ranging (LIDAR) at eye-safe wavelengths. A cost-efficient way to realize high-power high-brightness devices is the tapered resonator concept. We demonstrate InGaAsP/InP based diode lasers with compressively strained quantum wells and wavelengths around 1480 nm which were grown by solid source MBE. From broad area lasers with variations in quantum well number and waveguide layer thickness, parameters for the <b>logarithmic</b> <b>gain</b> model are deduced. With their implementation in 2 -dimensional BPM simulations, an optimized resonator geometry is derived. Devices employ a 500 µm ridge section followed by a 2000 µm taper section with 6 ° angle. Continuous-wave (cw) output powers reach more than 1. 5 W. Beam quality is characterized in terms of near field and far field distribution, M 2, and astigmatism. An excellent agreement is found between measurement and simulation. For narrow-linewidth operation, devices are provided with anti-reflection coatings on both facets and spectrally stabilized with an external grating. We achieve 0. 7 W single mode power and a side mode suppression ratio (SMSR) of 42 dB. Reliability is tested in terms of facet stability and lifetime. Pulsed measurements reveal a power stability up to more than 5 MW/cm 2. From cw aging tests at 1 W output power, lifetimes of about 6, 000 h are extrapolated...|$|E
40|$|The primary {{challenge}} of rocket propulsion is {{the burden of}} needing to accelerate the spacecraft's own fuel, resulting in only a <b>logarithmic</b> <b>gain</b> in maximum speed as propellant {{is added to the}} spacecraft. Light sails offer an attractive alternative in which fuel is not carried by the spacecraft, with acceleration being provided by an external source of light. By artificially illuminating the spacecraft with beamed radiation, speeds are only limited by the area of the sail, heat resistance of its material, and power use of the accelerating apparatus. In this paper, we show that leakage from a light sail propulsion apparatus in operation around a solar system analogue would be detectable. To demonstrate this, we model the launch and arrival of a microwave beam-driven light sail constructed for transit between planets in orbit around a single star, and find an optimal beam frequency on the order of tens of GHz. Leakage from these beams yields transients with flux densities of Jy and durations of tens of seconds at 100 pc. Because most travel within a planetary system would be conducted between the habitable worlds within that system, multiply-transiting exoplanetary systems offer the greatest chance of detection, especially when the planets are in projected conjunction as viewed from Earth. If interplanetary travel via beam-driven light sails is commonly employed in our galaxy, this activity could be revealed by radio follow-up of nearby transiting exoplanetary systems. The expected signal properties define a new strategy in the search for extraterrestrial intelligence (SETI). Comment: 6 pages, 5 figures. Accepted to ApJL. Revisions correct minor algebra errors and make bandwidth assumptions explici...|$|E
40|$|Nearest {{neighbor}} {{cells in}} R d,d ∈ N, {{are used to}} define coefficients of divergence (φ-divergences) between continuous multivariate samples. For large sample sizes, such distances are shown to be asymptotically normal with a variance depending on the underlying point density. In d = 1, this extends classical central limit theory for sum functions of spacings. The general results yield central limit theorems for <b>logarithmic</b> k-spacings, information <b>gain,</b> log-likelihood ratios {{and the number of}} pairs of sample points within a fixed distance of each other. 1. Introduction. Suppose X(i), 1 ≤ i ≤ n, are the order statistics drawn from an i. i. d. sample with distribution F on R and let G be a distribution function. Classical spacing functionals on R (Section 6 of [35]) take the form of an empirical φ-divergenc...|$|R
30|$|In {{deterministic}} models, the <b>logarithmic</b> channel <b>gains</b> are integer. We set the logarithmic SNRs and INRs as random integer {{selected from}} 1 ∼ 6. In K-user orthogonal transmission scheme, each user occupies 1 /K {{of the time}} or frequency resources {{no matter what the}} SNRs and INRs are. The corresponding results are shown in Fig. 4. We can see that the sum rate of the virtual IA scheme linearly increases with the number of users when K≥ 2, but its performance is not the best for all cases. In two-user or three-user interference channels, the proposed layered IA scheme can achieve higher sum rate. According to the optimization process in Section 2, the active level assignment is not an equal distribution among users. By contrast, the user with better channel conditions (e.g., large SNR and small INR) might be assigned more active levels to maximize the sum rate. Hence, when the number of users is not too much, the optimized level assignment scheme outperforms the scheme that each user exploits one half of the channel resource. In interference channels with more than three users, the virtual IA scheme will gradually dominate the performance due to the advantage that each user can exploit 1 / 2 DoF of the channel. No matter how many users exist, the achieved sum rate of the orthogonal transmission scheme always keeps unchanged.|$|R
40|$|Abstract — The {{capacity}} gain {{of network}} coding {{has been extensively}} studied in wired and wireless networks. Recently, {{it has been shown}} that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in a wireless network where network coding is the most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. For a tree-based multicast, we derive expressions for the expected number of transmissions at the source of the multicast and inside the multicast tree. We also analyze the performance of error control mechanisms based on rateless codes and automatic repeat request (ARQ). We then use the analytical expressions to study the impact of multicast group size on the performance of different error control schemes. Our numerical results show that network coding significantly reduces the number of retransmissions in lossy networks compared to end-to-end ARQ scheme, however, rateless coding and link-by-link ARQ are able to achieve performance results comparable to that of network coding. Interestingly, link-by-link ARQ can outperform rateless coding depending on the network size and loss probability. We conjecture that network coding achieves a <b>logarithmic</b> reliability <b>gain</b> with respect to multicast group size compared to a simple ARQ scheme. I...|$|R
40|$|The flow {{fields in}} polymer {{processing}} exhibit complex behavior with chaotic characteristics, {{due in part}} to the non-linearity of the field equations describing them. In chaotic flows fluid elements are highly sensitive to their initial positions and velocities. A fundamental understanding of such characteristics is essential for optimization and design of equipment used for distributive mixing. In this work we analyze the flow in a twin-flight single screw extruder, obtained through 3 -D FEM numerical simulations. We study particle motion and, implicitly, mixing in the extruder. Here, particles are massless points whose presence does not affect the flow field or other particle motion. We visualize chaos through Poincaré sections and calculate Lyapunov exponents as a measure of divergence of initial conditions, signaling chaotic features of flow. We use entropic measures to probe disorder or system homogeneity. The time evolution of the Renyi entropy of β = 1 for the 3 -D spatial distribution of particles using different initial conditions are followed. The Kolmogorov-Sinai entropy rate, calculated by the sum of positive Lyapunov exponents, is correlated with the rate of evolution of entropy. In the same context we also examine the eccentric Couette flow. We find that the Renyi entropy dependence on time is <b>logarithmic.</b> To <b>gain</b> further understanding of this numerical observation, we analyze analytically the diffusion with drift entropy and find that it also depends logarithmically on time. Using the logarithmic coefficient of the Shannon entropy (ß = 1), as a measure of the overall rate of 2 mixing, we find that the eccentric Couette device has the highest rate of mixing, followed by the twin-flight single screw extruder, and by the 1 -D diffusion with drift...|$|R
40|$|The goal of {{this work}} is to generalize speech {{enhancement}} methods from single channel microphones, dual channel microphones, and microphone arrays to distributed microphones. The {{focus has been on}} developing and implementing robust and optimal time domain and frequency domain estimators for estimating the true source signal in this configuration and measuring the performance improvement with both objective (e. g., signal-to-noise ratios) and subjective (e. g., listening tests) metrics. Statistical estimation techniques (e. g., minimum mean-square error or MMSE) with Gaussian speech priors and Gaussian noise likelihoods have been used to derive solutions for five basic classes of estimators: (1) time domain; (2) spectral amplitude; (3) perceptually-motivated spectral amplitude; (4) spectral phase; and (5) complex real and imaginary spectral component. Experimental work using different true source signal attenuation factors (e. g., unity, linear, and <b>logarithmic)</b> demonstrates significant <b>gains</b> in segmental signal-to-noise ratio (SSNR) with {{an increase in the number}} of microphones. Of particular importance is the inclusion of the optimal MMSE spectral phase estimator to the spectral amplitude estimators. Overall, the statistical estimators show tremendous promise for distributed microphone speech enhancement of noisy acoustic signals with application to many consumer, industrial, and military products under severely noisy environments...|$|R
40|$|The {{definition}} of coherent derived {{units in the}} International System of Units (SI) is reviewed, and {{the important role of}} the equations defining physical quantities is emphasized in obtaining coherent derived units. In the case of the dimensionless quantity plane angle, the choice between alternative definitions is considered, leading to a corresponding choice between alternative definitions of the coherent derived unit - the radian, degree or revolution. In this case the General Conference on Weights and Measures (CGPM) has chosen to adopt the definition that leads to the radian as the coherent derived unit in the SI. In the case of the quantity <b>logarithmic</b> decay (or <b>gain),</b> also sometimes called decrement, and sometimes called level, a similar choice of defining equation exists, leading to a corresponding choice for the coherent derived unit - the neper or the bel. In this case the CGPM has not yet made a choice. We argue that for the quantity logarithmic decay the most logical choice of defining equation is linked to that of the radian, and is that which leads to the neper as the corresponding coherent derived unit. This should not prevent us from using the bel and decibel as units of logarithmic decay. However, it {{is an important part of}} the SI to establish in a formal sense the equations defining physical quantities, and the corresponding coherent derived units. ...|$|R
40|$|The UK National Institute for Health and Clinical Excellence (NICE) {{has used}} its Single Technology Appraisal (STA) {{programme}} to assess several drugs for cancer. Typically, the evidence submitted by the manufacturer comes from one short-term randomized controlled trial (RCT) demonstrating improvement in overall survival and/or in delay of disease progression, {{and these are the}} pre-eminent drivers of cost effectiveness. We draw attention to key issues encountered in assessing the quality and rigour of the manufacturers' modelling of overall survival and disease progression. Our examples are two recent STAs: sorafenib (Nexavar®) for advanced hepatocellular carcinoma, and azacitidine (Vidaza®) for higher-risk myelodysplastic syndromes (MDS). The choice of parametric model had a large effect on the predicted treatment-dependent survival <b>gain.</b> <b>Logarithmic</b> models (log-Normal and log-logistic) delivered double the survival advantage that was derived from Weibull models. Both submissions selected the logarithmic fits for their base-case economic analyses and justified selection solely on Akaike Information Criterion (AIC) scores. AIC scores in the azacitidine submission failed to match the choice of the log-logistic over Weibull or exponential models, and the modelled survival in the intervention arm lacked face validity. AIC scores for sorafenib models favoured log-Normal fits; however, since there is no statistical method for comparing AIC scores, and differences may be trivial, it is generally advised that the plausibility of competing models should be tested against external data and explored in diagnostic plots. Function fitting to observed data should not be a mechanical process validated by a single crude indicator (AIC). Projective models should show clear plausibility for the patients concerned and should be consistent with other published information. Multiple rather than single parametric functions should be explored and tested with diagnostic plots. When trials have survival curves with long tails exhibiting few events then the robustness of extrapolations using information in such tails should be tested...|$|R

