457|1207|Public
5000|$|What {{follows is}} an {{illustration}} of how the concept of regret {{can be used to}} design a <b>linear</b> <b>estimator.</b> In this example, the problem is to construct a <b>linear</b> <b>estimator</b> of a finite-dimensional parameter vector [...] from its noisy linear measurement with known noise covariance structure. The loss of reconstruction of [...] is measured using the mean-squared error (MSE). The unknown parameter vector is known to lie in an ellipsoid [...] centered at zero. The regret is defined to be the difference between the MSE of the <b>linear</b> <b>estimator</b> that doesn't know the parameter , and the MSE of the <b>linear</b> <b>estimator</b> that knows [...] Also, since the estimator is restricted to be linear, the zero MSE cannot be achieved in the latter case. In this case, the solution of a convex optimization problem gives the optimal, minimax regret-minizing <b>linear</b> <b>estimator,</b> which can be seen by the following argument.|$|E
5000|$|... is not {{quantifiable}} to any <b>linear</b> <b>estimator,</b> {{once the}} stationarity {{of the mean}} and of the spatial covariances, or variograms, are assumed.|$|E
5000|$|This we can {{recognize}} {{to be the}} same as [...] Thus the minimum mean square error achievable by such a <b>linear</b> <b>estimator</b> is ...|$|E
40|$|The {{canonical}} {{form for the}} comparison of certain <b>linear</b> <b>estimators</b> using Pitman's Measure of Closeness is generalized to the class of all <b>linear</b> <b>estimators.</b> Under the assumption of normality, the equivalence of Pitman-closest <b>linear</b> unbiased <b>estimators</b> and best <b>linear</b> unbiased <b>estimators</b> is shown. A sufficient condition is given for which the BLUE will be Pitman-closer than the best <b>linear</b> equivalent <b>estimator</b> (BLEE). Pitman's measure of closeness order statistics best <b>linear</b> unbiased <b>estimators</b> best <b>linear</b> equivariant <b>estimators...</b>|$|R
3000|$|... where HL is {{the class}} of {{homogeneous}} <b>linear</b> <b>estimators</b> and L is {{the class of}} inhomogeneous <b>linear</b> <b>estimators.</b>|$|R
40|$|AbstractAdmissibility of <b>linear</b> <b>estimators</b> of a {{regression}} coefficient in linear models {{with and without}} {{the assumption that the}} underlying distribution is normal is discussed under a balanced loss function. In the non-normal case, a necessary and sufficient condition is given for <b>linear</b> <b>estimators</b> to be admissible in the space of homogeneous <b>linear</b> <b>estimators.</b> In the normal case, a sufficient condition is provided for restricted <b>linear</b> <b>estimators</b> to be admissible in the space of all estimators having finite risks under the balanced loss function. Furthermore, the sufficient condition is proved to be necessary in the normal case if additional conditions are assumed...|$|R
5000|$|One {{advantage}} of such linear MMSE estimator {{is that it}} is not necessary to explicitly calculate the posterior probability density function of [...] Such <b>linear</b> <b>estimator</b> only depends on the first two moments of [...] and [...] So although it may be convenient to assume that [...] and [...] are jointly Gaussian, it is not necessary to make this assumption, so long as the assumed distribution has well defined first and second moments. The form of the <b>linear</b> <b>estimator</b> does not depend on the type of the assumed underlying distribution.|$|E
5000|$|Consider i risks which {{generate}} random {{losses for}} which historical data of m recent claims are available (indexed by j). A premium for the ith risk {{is to be}} determined based on the expected value of claims. A <b>linear</b> <b>estimator</b> which minimizes the mean square error is sought. Write ...|$|E
5000|$|Since the MSE depends {{explicitly}} on [...] it {{cannot be}} minimized directly. Instead, {{the concept of}} regret {{can be used in}} order to define a <b>linear</b> <b>estimator</b> with good MSE performance. To define the regret here, consider a <b>linear</b> <b>estimator</b> that knows the value of the parameter , i.e., the matrix [...] can explicitly depend on :The MSE of [...] isTo find the optimal , [...] is differentiated with respect to [...] and the derivative is equated to 0 gettingThen, using the Matrix Inversion LemmaSubstituting this [...] back into , one getsThis is the smallest MSE achievable with a linear estimate that knows [...] In practice this MSE cannot be achieved, but it serves as a bound on the optimal MSE. The regret of using the <b>linear</b> <b>estimator</b> specified by [...] is equal toThe minimax regret approach here is to minimize the worst-case regret, i.e., [...] This will allow a performance {{as close as possible to}} the best achievable performance in the worst case of the parameter [...] Although this problem appears difficult, it is an instance of convex optimization and in particular a numerical solution can be efficiently calculated. For details of this see Eldar, Tal and Nemirovski (2004). Similar ideas can be used when [...] is random with uncertainty in the covariance matrix. For this see Eldar and Merhav (2004), and Eldar and Merhav (2005).|$|E
40|$|AbstractThis paper {{studies the}} {{admissibility}} of both homogeneous and inhomogeneous <b>linear</b> <b>estimators</b> in multivariate <b>linear</b> models {{with respect to}} inequality constraints. The relationship between homogeneous and inhomogeneous admissible <b>linear</b> <b>estimators</b> with respect to two different inequality constraints is obtained...|$|R
40|$|Admissibility of <b>linear</b> <b>estimators</b> of a {{regression}} coefficient in linear models {{with and without}} {{the assumption that the}} underlying distribution is normal is discussed under a balanced loss function. In the non-normal case, a necessary and sufficient condition is given for <b>linear</b> <b>estimators</b> to be admissible in the space of homogeneous <b>linear</b> <b>estimators.</b> In the normal case, a sufficient condition is provided for restricted <b>linear</b> <b>estimators</b> to be admissible in the space of all estimators having finite risks under the balanced loss function. Furthermore, the sufficient condition is proved to be necessary in the normal case if additional conditions are assumed. Admissibility Space of all estimators With and without normality assumption Balanced loss function...|$|R
40|$|AbstractMinimax-linear {{estimation}} {{with respect}} to the quadratic risk is considered among the class of <b>linear</b> <b>estimators</b> of β, under the linear regression model M = {y,Xβ,σ 2 I}. New classes of minimax-linear estimators of β are derived among certain subsets of <b>linear</b> <b>estimators,</b> which are simple {{from the point of view}} of minimax estimation. The admissible <b>linear</b> <b>estimators</b> of β under M are then characterized via these classes of minimax-linear estimators, and the relationship between the minimax-linear and admissible estimators of β is examined. Various properties of <b>linear</b> admissible <b>estimators</b> follow readily from this new characterization...|$|R
5000|$|Let [...] {{be another}} <b>linear</b> <b>estimator</b> of [...] with [...] where [...] is a [...] non-zero matrix. As were restricting to {{unbiased}} estimators, minimum {{mean squared error}} implies minimum variance. The goal is therefore to show that such an estimator has a variance no smaller than that of [...] the OLS estimator. We calculate: ...|$|E
5000|$|Suppose x is a Gaussian random {{variable}} with mean m and variance [...] Also suppose we observe a value [...] where w is Gaussian noise which {{is independent of}} x and has mean 0 and variance [...] We wish to find a <b>linear</b> <b>estimator</b> [...] minimizing the MSE. Substituting the expression [...] into the two requirements of the orthogonality principle, we obtain ...|$|E
5000|$|The {{orthogonality}} {{principle is}} {{most commonly used}} {{in the setting of}} linear estimation. In this context, let x be an unknown random vector which is to be estimated based on the observation vector y. One wishes to construct a <b>linear</b> <b>estimator</b> [...] for some matrix H and vector c. Then, the orthogonality principle states that an estimator [...] achieves minimum mean square error if and only if ...|$|E
40|$|AbstractThis article {{investigates the}} minimaxity of matrix <b>linear</b> <b>estimators</b> of {{regression}} coefficient matrix {{in a general}} multivariate linear model with a nonnegative definite covariance matrix allowing for relations between the covariance matrix and the design matrix under a balanced loss function. In a subset of all matrix <b>linear</b> <b>estimators,</b> matrix <b>linear</b> minimax <b>estimators</b> are obtained and proved to be unique almost surely on the suitable hypotheses...|$|R
5000|$|... #Subtitle level 2: Orthogonality {{principle}} for <b>linear</b> <b>estimators</b> ...|$|R
40|$|AbstractIn this paper, {{we study}} the {{characterization}} of admissible <b>linear</b> <b>estimators</b> of regression coefficients in the general growth curve model with respect to an incomplete ellipsoidal restriction under the loss function (d(Y) -KBL) ′(d(Y) -KBL). By the methods of linear algebra and matrix theory, when KBL is estimable, the necessary and sufficient conditions for <b>linear</b> <b>estimators</b> to be admissible in the homogeneous and non-homogeneous linear classes are given. If KBL is inestimable, we obtain the necessary and sufficient conditions for <b>linear</b> <b>estimators</b> to be admissible in the homogeneous and non-homogeneous linear classes using orthogonal projection approach...|$|R
5000|$|NPMR can {{be applied}} with several {{different}} kinds of local models. By [...] "local model" [...] we mean the way that data points near a target point in the predictor space are combined to produce an estimate for the target point. The most common choices for the local models are the local mean estimator, a local <b>linear</b> <b>estimator,</b> or a local logistic estimator. In each case the weights can be extended multiplicatively to multiple dimensions.|$|E
40|$|We give a {{characterization}} of asymptotically <b>linear</b> <b>estimator</b> sequences {{which leads to}} a concept of asymptotic linearity in arbitrary locally asymptotically normal models, and to a generalization of asymptotic linearity in i. i. d. models, retaining the properties of classical sense asymptotically <b>linear</b> <b>estimator</b> sequences. Asymptotically <b>linear</b> <b>estimator</b> regular estimator...|$|E
40|$|The bounded normal mean {{problem has}} {{important}} applications in nonparametric function estimation. It is {{to estimate the}} mean of a normal distribution whose mean is restricted to a bounded interval. The minimax risk for such a problem is generally unknown. It is shown in Donoho, Liu and MacGibbon(1990) that the linear minimax risk provides a good approximation to the minimax risk. We show in this note that a better approximation {{can be obtained by}} a simple truncation of the minimax <b>linear</b> <b>estimator</b> and that the minimax <b>linear</b> <b>estimator</b> is itself inadmissible. The gain of the truncated minimax <b>linear</b> <b>estimator</b> is significant for moderate size of the mean interval, where no analytical expression for the minimax risk is available. In particular, we show that the truncated minimax <b>linear</b> <b>estimator</b> performs no more than 13 % worse than the minimax estimator, comparing with 25 % for the minimax <b>linear</b> <b>estimator.</b> KEY WORDS AND PHRASES: Bounded normal mean, minimax risk, quadratic loss. SHORT TIT [...] ...|$|E
30|$|In this paper, under a {{generalized}} balanced loss function, we study the admissibility of <b>linear</b> <b>estimators</b> {{of the regression}} coefficient in general Gauss-Markov model with respect to an inequality constraint. The necessary and sufficient conditions that the <b>linear</b> <b>estimators</b> of regression coefficient function are admissible are obtained, {{in the class of}} homogeneous and inhomogeneous linear estimation, respectively.|$|R
5000|$|In {{the special}} case of <b>linear</b> <b>estimators</b> {{described}} above, the space [...] is {{the set of}} all functions of [...] and , while [...] is the set of <b>linear</b> <b>estimators,</b> i.e., <b>linear</b> functions of [...] only. Other settings which can be formulated in this way include the subspace of causal linear filters and the subspace of all (possibly nonlinear) estimators.|$|R
30|$|In this section, {{we study}} the {{admissibility}} {{in the class}} of inhomogeneous <b>linear</b> <b>estimators.</b>|$|R
40|$|We {{present a}} local <b>linear</b> <b>estimator</b> with {{variable}} bandwidth for multivariate nonparametric regression. We prove its consistency and asymptotic normality {{in the interior}} of the observed data and obtain its rates of convergence. This result is used to obtain practical direct plug-in bandwidth selectors for heteroscedastic regression in one and two dimensions. We show that the local <b>linear</b> <b>estimator</b> with variable bandwidth has better goodness-of-fit properties than the local <b>linear</b> <b>estimator</b> with constant bandwidth, in the presence of heteroscedasticity. Heteroscedasticity; kernel smoothing; local linear regression; plug-in bandwidth, variable bandwidth. ...|$|E
40|$|Abstract. A {{sequential}} <b>linear</b> <b>estimator</b> {{is developed}} {{in this study}} to progressively incorporate new or different spatial data sets into the estimation. It begins with a classical <b>linear</b> <b>estimator</b> (i. e., kriging or cokriging) to estimate means conditioned to a given observed data set. When an additional data set becomes available, the sequential estimator improves the previous estimate by using linearly weighted sums of differences between the new data set and previous estimates at sample locations. Like the classical <b>linear</b> <b>estimator,</b> the weights used in the sequential <b>linear</b> <b>estimator</b> are derived from a system of equations that contains covariances and cross-covariances between sample locations and the location where the estimate is to be made. However, the covariances and crosscovariances are conditioned upon the previous data sets. The sequential estimator is shown to produce the best, unbiased linear estimate, and to provide the same estimates and variances as classic simple kriging or cokriging with the simultaneous use of the entire data set. However, by using data sets sequentially, this new algorithm alleviates numerical dif®culties associated with the classical kriging or cokriging techniques when {{a large amount of}} data are used. It also provides a new way to incorporate additional information into a previous estimation. Key words: Sequential <b>linear</b> <b>estimator,</b> successive <b>linear</b> <b>estimator,</b> conditional covariance, interpolation with large data sets. ...|$|E
40|$|We {{consider}} {{the problem of}} estimating a random vector x, with covariance uncertainties, that is observed through a known linear transformation H and corrupted by additive noise. We first develop the <b>linear</b> <b>estimator</b> that minimizes the worst-case meansquared error (MSE) across all possible covariance matrices. Although the minimax approach has enjoyed widespread use {{in the design of}} robust methods, we show that its performance is often unsatisfactory. We then develop a competitive minimax approach in which we seek the <b>linear</b> <b>estimator</b> that minimizes the worstcase regret, namely, the worst-case difference between the MSE attainable using a <b>linear</b> <b>estimator,</b> ignorant of the signal covariance, and the optimal MSE attained using a <b>linear</b> <b>estimator</b> that knows the signal covariance. We demonstrate, through an example, that the minimax regret approach can improve the performance over the minimax MSE approach. 1...|$|E
40|$|Our aim in {{this article}} is to obtain {{efficient}} estimators of the parameters of the bivariate Kotz type distribution considering a particular matrix-variate joint dependence between the sample random vectors. As the normal law is a particular Kotz type distribution, it seems reasonable, taking into account the known results about the normal law, to search such estimators inside the family of unbiased <b>linear</b> <b>estimators.</b> However, we have proven that {{it is not possible to}} obtain efficient <b>linear</b> <b>estimators.</b> Then, we have focused our interest on determining the best unbiased <b>linear</b> <b>estimators</b> in the sense of minimizing the distance to the Cramér-Rao lower bound. The results theoretically obtained are illustrated in a numerical simulation example...|$|R
40|$|This paper {{deals with}} the problem of {{estimating}} expected values using Order Statistics of a small sample. <b>Estimators</b> using a <b>linear</b> combination of Order Statistics, are compared to usual <b>linear</b> <b>estimators.</b> It is shown that while the use of Order Statistics improves the estimation of white sequences, <b>linear</b> <b>estimators</b> are sometimes more effective for colored sequences. Thus, a new estimator based on both the variates and their Order Statistics is developed. It is shown that this new estimator performs at least as well as Order Statistics estimators, and at least as well as <b>linear</b> <b>estimators.</b> An adaptive scheme of these estimators is then given and applied to estimate a binary noisy signal...|$|R
40|$|Graduation date: 1986 We {{describe}} a general finite-dimensional inner product space setting {{for studying the}} characterization of admissible <b>linear</b> <b>estimators.</b> We extend the results of LaMotte (1982) and derive necessary and sufficient conditions for an estimator to be admissible among an arbitrary affine set of <b>linear</b> <b>estimators</b> when they are compared using quadratic risk in a linear model with general mean and variance-covariance structure. The results are shown to be applicable to linear estimation of vector-valued parametric functions compared according to total mean squared error. We also present containment results that provide a partial description of the admissible estimators for a problem. Additional results are obtained in specific vector space settings for problems where estimators are compared according to total mean squared error. We explore the admissibility of transformations of admissible estimators. We show for a linear model with general variance-covariance structure that, if H'Y is admissible among all <b>linear</b> (unbiased) <b>estimators</b> for Δ'β and D is full column-rank, then DH'Y is admissible among all <b>linear</b> (unbiased) <b>estimators</b> for DΔ'β. Admissibility is explored in models where the mean is assumed functionally independent from the variance-covariance parameters. For models with a one-dimensional design matrix, we show that under mild assumptions the admissible <b>linear</b> <b>estimators</b> consist of the scalar multiples of the unbiased admissible <b>linear</b> <b>estimators,</b> where the scalar is between 0 and 1...|$|R
40|$|We {{develop a}} new <b>linear</b> <b>estimator</b> for {{estimating}} an unknown vector x in a linear model, {{in the presence of}} bounded data uncertainties. The estimator is designed to minimize the worst-case regret across all bounded data vectors, namely the worst-case difference between the MSE attainable using a <b>linear</b> <b>estimator</b> that does not know the true parameters x, and the optimal MSE attained using a <b>linear</b> <b>estimator</b> that knows x. We demonstrate through several examples that the minimax regret estimator can significantly increase the performance over the conventional least-squares estimator, as well as several other least-squares alternatives. 1...|$|E
40|$|We {{study the}} local <b>linear</b> <b>estimator</b> for the drift {{coefficient}} of stochastic differential equations driven by α-stable Lévy motions observed at discrete instants letting T →∞. Under regular conditions, we derive the weak consistency and {{central limit theorem}} of the estimator. Compare with Nadaraya-Watson estimator, the local <b>linear</b> <b>estimator</b> has a bias reduction whether kernel function is symmetric or not under different schemes. Comment: 15 page...|$|E
40|$|A {{bonus-malus}} system plays {{a very important}} role in actuarial mathematics through determining its relativity premium, which is extensively used in automobile insurance. There are many ways including Bayesian estimator and ordinary <b>linear</b> <b>estimator</b> to calculate the relativity premium. There is no doubt that Bayesian estimator is the most accurate estimator; however, it is undesirable for commercial purposes for its rather irregular pattern. This paper aims to introduce an optimal <b>linear</b> <b>estimator</b> for relativity premium, which has a simple pattern and is obtained under the quadratic loss function such that the result is close to Bayesian method. The Loimaranta efficiency of such an optimal <b>linear</b> <b>estimator</b> has been studied and compared with the two methods mentioned above...|$|E
40|$|We {{consider}} {{a problem of}} estimation of parametric component in a partial linear model. Suppose that a finite set E of <b>linear</b> <b>estimators</b> is given. Our goal is to mimic the estimator in E that has the smallest risk. Using a second order expansion {{of the risk of}} <b>linear</b> <b>estimators</b> we propose a practically feasible adaptive procedure for choke of smoothing parameters based on the principle of unbiased risk estimation...|$|R
3000|$|... can be {{identified}} from the experiments using standard <b>linear</b> <b>estimators</b> as the universal representation of the activation function F.|$|R
30|$|The {{remainder}} of this work is divided into four parts: First, we present the problem structure, signal model adopted for this study and briefly discuss two well-known channel estimators, namely, LS and MMSE <b>linear</b> <b>estimators.</b> Then, we introduce the proposed channel estimator for flat Rayleigh fading channels. Later, some numerical results are presented {{in order to support}} the effectiveness of the proposed estimator against the well-known <b>linear</b> <b>estimators.</b> Finally, we present our conclusions.|$|R
