9|22|Public
25|$|The first {{generation}} of DEC Alpha-based systems comprised the DEC 3000 AXP series workstations and <b>low-end</b> <b>servers,</b> DEC 4000 AXP series mid-range servers, and DEC 7000 AXP and 10000 AXP series high-end servers. The DEC 3000 AXP systems used the same TURBOchannel bus as the previous MIPS-based DECstation models, whereas the 4000 was based on FutureBus+ and the 7000/10000 shared an architecture with corresponding VAX models.|$|E
25|$|In the mid-1990s, Dell {{expanded}} beyond {{desktop computers}} and laptops by selling servers, starting with <b>low-end</b> <b>servers.</b> The major three providers of servers {{at the time}} were IBM, Hewlett Packard, and Compaq, many of which were based on proprietary technology, such as IBM's Power4 microprocessors or various proprietary versions of the Unix operating system. Dell's new PowerEdge servers did not require a major investment in proprietary technologies, as they ran Microsoft Windows NT on Intel chips, and could be built more cheaply than its competitors. Consequently, Dell's enterprise revenues, almost nonexistent in 1994, accounted for 13% of the company's total intake by 1998. Three years later, Dell passed Compaq as the top provider of Intel-based servers, with 31% of the market. Dell's first business acquisition occurred in 1999 with the purchase of ConvergeNet Technologies for $332 million, after Dell had failed to develop an enterprise storage system in-house; ConvergeNet's elegant but complex technology did not fit in with Dell's commodity-producer business model, forcing Dell to write down the entire value of the acquisition.|$|E
50|$|AMD {{released}} Socket 939 Opterons, {{reducing the}} cost of motherboards for <b>low-end</b> <b>servers</b> and workstations. Except for the fact they have 1 MB L2 Cache (versus 512 KB for the Athlon64) the Socket 939 Opterons are identical to the San Diego and Toledo core Athlon 64s, but are run at lower clock speeds than the cores are capable of, making them more stable.|$|E
40|$|We {{estimate}} {{a dynamic}} nested logit model {{of demand for}} differentiated durable goods: <b>low-end</b> computer <b>servers.</b> Dynamics has two sources. First, durability of the good creates an optimal stopping problem when quality of available new products is stochastically improving. Second, customers who already have a product may choose to upgrade it, but only from the same nest, which represents operating system of the server. Hence, expected future upgrade qualities of different operating systems {{must be taken into}} account already at the purchase decision of a new product. Keywords:Markov decision process, differentiated durable goods, computer servers We estimate a dynamic model of demand for differentiated durable goods, which are <b>low-end</b> <b>server</b> computers in our application. Our model is a Markovian discrete decision process (DDP). Dynamics is generated by two sources. First, durability of the good creates an optimal stopping problem: The customer decides about the optimal time to buy a new product when the qualit...|$|R
5000|$|SGI {{released}} {{a variant of}} the Indy for <b>low-end</b> <b>server</b> usage. The SGI Challenge S has an identical case as the Indy (except for the name badge), and featured a nearly identical motherboard as the Indy, but without any graphics or sound hardware. [...] Interestingly, there are still volume control buttons {{on the front of the}} Challenge S, but they are not connected to anything. The Challenge S comes with an ISDN port standard. Also included is a 10Mbit/s AUI Ethernet port. All local administration is performed by serial console to one of the two DIN-8 serial ports (either one can be used to reach the PROM prompt and uses the same pin-out found on Macintosh modem/printer ports).|$|R
40|$|Abstract—Reducing energy {{consumption}} {{has a significant}} role in mitigating the total cost of ownership of computing clusters. Building heterogeneous clusters by combining highend and <b>low-end</b> <b>server</b> nodes (e. g., Xeons and Atoms) is a recent trend towards achieving energy-efficient computing. This requires a cluster-level power manager that has the ability to predict future load, and server nodes that can quicklytransition between active and low-power sleep states. In practice however, the load is unpredictable and often punctuated by spikes, necessitating a number of extra “idling ” servers. We design a cluster-level power manager that (1) identifies the optimal cluster configuration based on the power profiles of servers and workload characteristics, and (2) maximizes work done per watt by assigning P-states and S-states to the cluster servers dynamically based on current request rate. We carry out an experimental study on a web server cluster composed of highend Xeon <b>servers</b> and <b>low-end</b> Atom-based Netbooks and share our findings. I...|$|R
50|$|The first {{generation}} of DEC Alpha-based systems comprised the DEC 3000 AXP series workstations and <b>low-end</b> <b>servers,</b> DEC 4000 AXP series mid-range servers, and DEC 7000 AXP and 10000 AXP series high-end servers. The DEC 3000 AXP systems used the same TURBOchannel bus as the previous MIPS-based DECstation models, whereas the 4000 was based on FutureBus+ and the 7000/10000 shared an architecture with corresponding VAX models.|$|E
50|$|In the mid-1990s, Dell {{expanded}} beyond {{desktop computers}} and laptops by selling servers, starting with <b>low-end</b> <b>servers.</b> The major three providers of servers {{at the time}} were IBM, Hewlett Packard, and Compaq, many of which were based on proprietary technology, such as IBM's Power4 microprocessors or various proprietary versions of the Unix operating system. Dell's new PowerEdge servers did not require a major investment in proprietary technologies, as they ran Microsoft Windows NT on Intel chips, and could be built more cheaply than its competitors. Consequently, Dell's enterprise revenues, almost nonexistent in 1994, accounted for 13% of the company's total intake by 1998. Three years later, Dell passed Compaq as the top provider of Intel-based servers, with 31% of the market. Dell's first business acquisition occurred in 1999 with the purchase of ConvergeNet Technologies for $332 million, after Dell had failed to develop an enterprise storage system in-house; ConvergeNet's elegant but complex technology did not fit in with Dell's commodity-producer business model, forcing Dell to write down the entire value of the acquisition.|$|E
40|$|Used in {{laptop and}} desktop computers, <b>low-end</b> <b>servers,</b> and mobile devices, Serial ATA (Advance Technology Attachment), or SATA, is the {{pervasive}} disk storage technology in use today. SATA has also penetrated the enterprise computing environment by adding hardware components for fail-over, extending command processing capabilities, and increasing device performance and link speeds. If {{you work in}} a data center or manage your company's storage resources, you will likely encounter storage solutions that require SATA software or hardware. In this book, leading storage networking technologist Davi...|$|E
50|$|The PowerPC 604 was {{introduced}} in December 1994 alongside the 603 and {{was designed as a}} high-performance chip for workstations and entry-level servers and as such had support for symmetric multiprocessing in hardware. The 604 was used extensively in Apple's high-end systems and was also used in Macintosh clones, IBM's <b>low-end</b> RS/6000 <b>servers</b> and workstations, Amiga accelerator boards, and as an embedded CPU for telecom applications.|$|R
5000|$|Outside of the {{enterprise}} environment, on the other hand, the pub/sub paradigm has proven its scalability to volumes far beyond those of a single data centre, providing Internet-wide distributed messaging through web syndication protocols such as RSS and Atom. These syndication protocols accept higher latency and lack of delivery guarantees {{in exchange for the}} ability for even a <b>low-end</b> web <b>server</b> to syndicate messages to (potentially) millions of separate subscriber nodes.|$|R
50|$|There {{were two}} {{models of the}} 21070, the DECchip 21071 and the DECchip 21072. The 21071 was {{intended}} for workstations whereas the 21072 was intended for high-end workstations or <b>low-end</b> uniprocessor <b>servers.</b> The two models differed in memory subsystem features: the 21071 has a 64-bit memory bus and supports 8 MB to 2 GB of parity-protected memory whereas the 21072 has a 128-bit memory bus and supports 16 MB to 4 GB of ECC-protected memory.|$|R
40|$|Abstract—Many {{companies}} now routinely run massive data analysis jobs – expressed in some scripting language – on large clusters of <b>low-end</b> <b>servers.</b> Many analysis scripts {{are complex and}} contain common subexpressions, that is, intermediate results that are subsequently joined and aggregated in multiple different ways. Applying conventional optimization techniques to such scripts will produce plans that execute a common subexpression multiple times, once for each consumer, which is clearly wasteful. Moreover, different consumers may have different physical requirements on the result: one consumer may want it partitioned on a column A and another one partitioned on column B. To find a truly optimal plan, the optimizer must trade off such conflicting requirements in a cost-based manner. In this paper we show how to extend a Cascade-style optimizer to correctly optimize scripts containing common subexpression. The approach has been prototyped in SCOPE, Microsoft’s system for massive data analysis. Experimental analysis of both simple and large real-world scripts shows that the extended optimizer produces plans with 21 to 57 % lower estimated costs. I...|$|E
40|$|Deep neural {{networks}} (DNNs) have recently yielded strong results {{on a range}} of applications. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. Furthermore, putting DNN tasks into containers of clusters would enable broader and easier deployment of DNN-based algorithms. Toward this end, this paper addresses the problem of scheduling DNN tasks in the containerized cluster environment. Efficiently scheduling data-parallel computation jobs like DNN over containerized clusters is critical for job performance, system throughput, and resource utilization. It becomes even more challenging with the complex workloads. We propose a scheduling method called Deep Learning Task Allocation Priority (DLTAP) which performs scheduling decisions in a distributed manner, and each of scheduling decisions takes aggregation degree of parameter sever task and worker task into account, in particularly, to reduce cross-node network transmission traffic and, correspondingly, decrease the DNN training time. We evaluate the DLTAP scheduling method using a state-of-the-art distributed DNN training framework on 3 benchmarks. The results show that the proposed method can averagely reduce 12 % cross-node network traffic, and decrease the DNN training time even with the cluster of <b>low-end</b> <b>servers...</b>|$|E
40|$|This paper {{contains}} an empirical analysis demand for "work-group" (or <b>low-end)</b> <b>servers.</b> Servers are at thecentre of many US and EU anti-trust debates, including the Hewlett-Packard/Compaq merger and investigationsinto {{the activities of}} Microsoft. One question in these policy decisions is whether a high share of work serversindicates anything about shortrun market power. To investigate price elasticities we use model-level panel dataon transaction prices, sales and characteristics of practically every server in the world. We contrast estimatesfrom the traditional "macro" approaches that aggregate across brands and modern "micro" approaches that usebrand-level information (including both "distance metric" and logit based approaches). We find that the macroapproaches lead to overestimates of consumer price sensitivity. Our preferred micro-based estimates of themarket level elasticity of demand for work group servers are around 0. 3 to 0. 6 (compared to 1 to 1. 3 in themacro estimates). Even at the higher range of the estimates, however, we find that demand elasticities aresufficiently low to imply a distinct "anti-trust" market for work group servers and their operating systems. It isunsurprising that firms with large shares of work group servers have come under some antitrust scrutiny. demand elasticities, network servers, computers, anti-trust...|$|E
40|$|This paper {{presents}} {{an analysis of}} the key events, impacts and issues of Lenovo buying IBM's x 86 <b>low-end</b> <b>server</b> business. The analysis include (i) approval of the deal by regulatory bodies in the United States, Canada, India and China, (ii) security concerns of US government departments, (iii) pricing of the deals, (iv) possible impact on IBM in future, and (v) possibilities of Lenovo making it repeat of acquiring ThinkPad business of IBM. The paper {{presents an}}alysis of qualitative and time series quantitative data. The qualitative data are mainly consists of different events before and after the acquisition of x 86 server IBM business by Lenovo. The quantitative data are analyzed with respect to growth parameters of overall server business and Lenovo server business. Research paper also attempts to find out answer to specific 9 research questions with respect to impact on eco-systems of IBM and Lenovo. Based on analysis, it is inferred that IBM is not able to manage its traditional & well accepted products business in the face of fierce competition & low demand but Lenovo will manage. The deal was a financial necessity for IBM and strategic expansion in to new markets strategy for Lenovo...|$|R
40|$|With {{the advent}} of cheap netbooks, {{expanding}} Internet access to the developing world {{is becoming more and}} more feasible. However, classroom computers benefit from applications typically deployed on servers, such as proxy caches, WAN accelerators, backup, etc. Server infrastructure, unfortunately, is not as well suited for developing-world use, since it tends to be less ruggedized, more power-hungry, and more expensive, making it difficult for small deployments to adequately maintain, stock and replace. To address these issues, we consider the notion of turning a netbook in to a <b>low-end</b> <b>server</b> via software support. Many netbooks ship with multi-gigabyte solid state disks (SSD) that are intended for local storage. By attaching an external disk and using the SSD to augment the typically-meager RAM memory, we can instead use a netbook as a server. Unfortunately, SSD behaves very differently from RAM, and requires special consideration to be used effectively. To this end, we introduce SSDAlloc, a hybrid memory manager that provides simple programming constructs that developers can use to build server software with small RAM footprints and good performance. With SSDAlloc and an external hard disk, a netbook can act as a server for the entire school, dramatically reducing deployment cost and complexity. ...|$|R
40|$|Auditability {{is crucial}} for data outsourcing, facilitating {{accountability}} and identifying data loss or corruption incidents in a timely manner, reducing in turn the risks from such losses. In recent years, in synch with the growing trend of outsourcing, {{a lot of progress}} has been made in designing probabilistic (for efficiency) provable data possession (PDP) schemes. However, even the recent and advanced PDP solutions that do deal with dynamic data, do so in a limited manner, and for only the latest version of the data. A naive solution treating different versions in isolation would work, but leads to tremendous overheads, and is undesirable. In this paper, we present algorithms to achieve full persistence (all intermediate configurations are preserved and are modifiable) for an optimized skip list (known as FlexList) so that versioned data can be audited. The proposed scheme provides deduplication at the level of logical, variable sized blocks, such that only the altered parts of the different versions are kept, while the persistent data-structure facilitates access (read) of any arbitrary version with the same storage and process efficiency that state-of-the-art dynamic PDP solutions provide for only the current version, while commit (write) operations incur around 5 % additional time. Furthermore, the time overhead for auditing arbitrary versions in addition to the latest version is imperceptible even on a <b>low-end</b> <b>server</b> [...] . Comment: 31 Pages, 12 Figures, 2 Tables, 4 Pseudocode...|$|R
40|$|The {{amount of}} data {{produced}} {{on the internet is}} growing rapidly. Along with data explosion comes the trend towards more and more diverse data, including rich media such as audio and video. Data explosion and diversity leads to the emergence of data-centric workloads to manipulate, manage and analyze the vast amounts of data. These data-centric workloads are likely to run in the background and include application domains such as data mining, indexing, compression, encryption, audio/video manipulation, data warehousing, etc. Given that datacenters are very much cost sensitive, reducing the cost of a single component by a small fraction immediately translates into huge cost savings because of the large scale. Hence, when designing a datacenter, {{it is important to understand}} datacentric workloads and optimize the ensemble for these workloads so that the best possible performance per dollar is achieved. This paper studies how the emerging class of data-centric workloads affects design decisions in the datacenter. Through the architectural simulation of minutes of run time on a validated fullsystem x 86 simulator, we derive the insight that for some datacentric workloads, a high-end server optimizes performance per total cost of ownership (TCO), whereas for other workloads, a lowend server is the winner. This observation suggests heterogeneity in the datacenter, in which a job is run on the most cost-efficient server. Our experimental results report that a heterogeneous datacenter achieves an up to 88 %, 24 % and 17 % improvement in costefficiency over a homogeneous high-end, commodity and <b>low-end</b> <b>server</b> datacenter, respectively...|$|R
25|$|MIPS III was {{eventually}} implemented {{by a number}} of embedded microprocessors. Quantum Effect Design's R4600 (1993) and its derivatives was widely used in high-end embedded systems and <b>low-end</b> workstations and <b>servers.</b> MIPS Technologies' R4200 (1994), was designed for embedded systems, laptop, and personal computers. A derivative, the R4300i, fabricated by NEC Electronics, was used in the Nintendo 64 game console. The Nintendo 64, along with the PlayStation, were the among the highest volume users of MIPS architecture processors in the mid-1990s.|$|R
40|$|The {{demand for}} {{heterogeneous}} computing, {{because of its}} performance and energy efficiency, has made on-chip heterogeneous chip multi-processors (HCMP) become the mainstream computing platform, as the recent trend shows in {{a wide spectrum of}} platforms from smartphone application processors to desktop and <b>low-end</b> <b>server</b> processors. The performance of on-chip GPUs is not yet comparable to that of discrete GPU cards, but vendors have integrated more powerful GPUs and this trend will continue in upcoming processors. In this architecture, several system resources are shared between CPUs and GPUs. The sharing of system resources enables easier and cheaper data transfer between CPUs and GPUs, but it also causes resource contention problems between cores. The resource sharing problem has existed since the homogeneous (CPU-only) chip-multi processor (CMP) was introduced. However, resource sharing in HCMPs shows different aspects because of the different nature of CPU and GPU cores. In order to solve the resource sharing problem in HCMPs, we consider efficient shared resource management schemes, in particular tackling the problem in shared last-level cache and interconnection network. In the thesis, we propose four resource sharing mechanisms: First, we propose an efficient cache sharing mechanism that exploits the different characteristics of CPU and GPU cores to effectively share cache space between them. Second, adaptive virtual channel partitioning for on-chip interconnection network is proposed to isolate inter-application interference. By partitioning virtual channels to CPUs and GPUs, we can prevent the interference problem while guaranteeing quality-of-service (QoS) for both cores. Third, we propose a dynamic frequency controlling mechanism to efficiently share system resources. When both cores are active, the degree of resource contention as well as the system throughput will be affected by the operating frequency of CPUs and GPUs. The proposed mechanism tries to find optimal operating frequencies for both cores, which reduces the resource contention while improving system throughput. Finally, we propose a second cache sharing mechanism that exploits GPU-semantic information. The programming and execution models of GPUs are more strict and easier than those of CPUs. Also, programmers are asked to provide more information to the hardware. By exploiting these characteristics, GPUs can energy-efficiently exercise the cache and simpler, but more efficient cache partitioning can be enabled for HCMPs. Ph. D...|$|R
40|$|The paper defines, {{implements}} {{and compares}} two empirical tests of relevant markets. While the SSNIP test compares an initial industry equilibrium to an out-of-equilibrium situation, the 1984 US Merger Guidelines test compares the same initial equilibrium {{to a second}} equilibrium outcome. We define these concepts formally and apply them to the computer server industry by estimating a model on a large dataset. We find several smaller relevant markets in the <b>low-end</b> segment of <b>servers.</b> In addition, {{we find that the}} results might be quantitatively significantly different between the two approaches as the SSNIP test changes prices uniformly and {{does not take into account}} the multi-product pricing strategies of the firms. ...|$|R
40|$|The paper defines, {{implements}} {{and compares}} two empirical tests of relevant markets. While the traditional SSNIP test compares an initial industry equilibrium to an out-of-equilibrium situation, the FERM test, our contribution, compares the same initial equilibrium to an other equilibrium outcome. Hence, {{it is more}} in line with the behavioural assumptions of the underlying model of industry equilibrium and this can have significant consequences. We define these concepts formally and apply them to the industry of computer servers by estimating a model on a large dataset. We find several smaller relevant markets in the <b>low-end</b> segment of <b>servers.</b> computer servers; differentiated products; relevant market tests...|$|R
40|$|For helpful discussions, {{we thank}} seminar {{participants}} at KU Leuven, the Norwegian Competition Authority and Universidad Carlos III. Any remaining {{errors in the}} paper are solely ours. The paper defines, implements and compares two empirical tests of relevant markets. While the European SSNIP test compares an initial industry equilibrium to an out-of-equilibrium situation, the FERM test compares the same initial equilibrium to an other equilibrium outcome. Hence, it is {{more in line with}} the behavioral assumptions of the underlying model of industry equilibrium and this can have significant consequences. We define these concepts formally and apply them to the industry of computer servers by estimating a model on a large dataset. We find several smaller relevant markets in the <b>low-end</b> segment of <b>servers...</b>|$|R
40|$|The paper {{introduces}} preference persistence into {{a dynamic}} discrete choice model {{of demand for}} durables. This persistence may arise, for example, when the products can be categorized into a few number of formats, which involve special knowledge, maintenance and upgrade. The standard optimal stopping problem of when {{to buy a new}} product is completed by the upgrade problem. Customers who already have a product may choose to upgrade it, but this upgrade is format specific. Hence, the expected future upgrade qualities of different formats {{must be taken into account}} already at the purchase decision of a new product. The model is estimated on a data set of <b>low-end</b> computer <b>servers,</b> where formats are represented by operating systems. For this application, the model can be considered as a proxy of a computer network building customer who cares not only about the likely future quality of the individual computers, but also about the direction of evolution of the network. That induces even stronger forward looking behavior than a simple optimal stopping problem. The results suggest that the model is better able to capture main tendencies in the segment than a static or a simple optimal stopping model. preference persistence, differentiated durables, Markov process, computer servers...|$|R
40|$|International audienceIn this paper, {{we propose}} a new {{approach}} to the performance supervision of complex and heterogeneous infrastructures found in hybrid cloud networks, which typically consist of hundreds or thousands of interconnected servers and networking devices. This hardware {{and the quality of the}} interconnections is monitored by sampling specific metrics (such as bandwidth usage, CPU time, packet loss [...] .) using probes, and raising alarms in case of an anomaly. We study an Artificial Immune Ecosystem model derived from the Artificial Immune Systems (AIS) algorithms to perform distributed analysis of the data collected throughout the network by these probes. In particular, we use the low variability of the measured data to derive statistical approaches to outlier detection, instead of the traditional stochastic antibody generation and selection method. The failure modes and baseline behaviour of the metrics being monitored (such as bandwidth usage, CPU time, packet loss [...] .) are recorded in a distributed learning process and increase the system's ability to react quickly to suspiscious events. By matching the data with only a small number of failure signatures, we reduce the overall computations required to operate the system with respect to traditional AIS, therefore allowing its deployment on <b>low-end</b> monitoring <b>servers</b> or virtual machines. We demonstrate that a very small computational overhead allows the supervision engine to react much faster than the monitoring solutions currently in use...|$|R
25|$|The first MIPS IV {{implementation}} was the MIPS Technologies R8000 microprocessor chipset (1994). The {{design of}} the R8000 began at Silicon Graphics, Inc. {{and it was only}} used in high-end workstations and servers for scientific and technical applications where high performance on large floating-point workloads was important. Later implementations were the MIPS Technologies R10000 (1996) and the Quantum Effect Devices R5000 (1996) and RM7000 (1998). The R10000, fabricated and sold by NEC Electronics and Toshiba, and its derivatives were used by NEC, Pyramid Technology, Silicon Graphics, Inc., and Tandem Computers (among others) in workstations, servers, and supercomputers. The R5000 and R7000 found use in high-end embedded systems, personal computers, and <b>low-end</b> workstations and <b>servers.</b> A derivative of the R5000 from Toshiba, the R5900, was used in Sony Computer Entertainment's Emotion Engine, which powered its Playstation 2 game console.|$|R
40|$|I/O subsystems, {{in which}} RAID as a {{building}} block, prove to consume {{a large portion}} of energy in both <b>low-end</b> and highend <b>server</b> environments. Most of previous research works have been presented on conserving energy in multi-disk systems either at a single disk drive level or at a storage system cache level. This paper studies several new redundancybased, power-aware, dynamic I/O request scheduling and cache management policies at the RAID controller level, by exploiting disks power state coupled with the redundant information of disk arrays in two popular RAID architectures, RAID 1 and RAID 5. For RAID 1, we develop a Windowed Round Robin (WRR) request scheduling policy. For RAID 5, we introduce a new N-chance Power Aware cache replacement algorithm (NPA) for disk writes and a Power-Directed, Exchangeable (PDE) request scheduling policy for disk reads to save energy. ...|$|R
50|$|The first MIPS IV {{implementation}} was the MIPS Technologies R8000 microprocessor chipset (1994). The {{design of}} the R8000 began at Silicon Graphics, Inc. {{and it was only}} used in high-end workstations and servers for scientific and technical applications where high performance on large floating-point workloads was important. Later implementations were the MIPS Technologies R10000 (1996) and the Quantum Effect Devices R5000 (1996) and RM7000 (1998). The R10000, fabricated and sold by NEC Electronics and Toshiba, and its derivatives were used by NEC, Pyramid Technology, Silicon Graphics, Inc., and Tandem Computers (among others) in workstations, servers, and supercomputers. The R5000 and R7000 found use in high-end embedded systems, personal computers, and <b>low-end</b> workstations and <b>servers.</b> A derivative of the R5000 from Toshiba, the R5900, was used in Sony Computer Entertainment's Emotion Engine, which powered its Playstation 2 game console.|$|R
40|$|Recent {{studies show}} that disk-based storage {{subsystems}} account for a non-trivial portion of energy consumption in both <b>low-end</b> and high-end <b>servers.</b> Current energyefficient solutions work either at a disk drive level or at a storage system cache level without the knowledge of redundant information inside RAID systems and thus have certain limitations. In this paper, we develop a novel energy-efficient RAID system architecture called EERAID to significantly conserve energy {{by taking advantage of}} redundant information. To give a proof-of-concept solution, we develop new energy-efficient, redundancyaware I/O scheduling and controller-level cache management schemes for EERAID 1 and EERAID 5 respectively. EERAID 1 implements two new policies – Windows Round-Robin (WRR) and Power and Redundancy-Aware Flush (PRF), while EERAID 5 develops two novel schemes–Transformable Read (TRA) and Power and Redundancy-aware Destage (PRD). Comprehensive trace-driven simulation experiments have been conducted by replaying two real-world server traces and a wide spectrum server-side synthetic traces. Experimental results showed that, 1) for single-speed (conventional) disks, EERAID 1 and EERAID 5 can achieve up to 30 % and 11 % energy savings respectively, and 2) For multi-speed disks, compared with DRPM, EERAID 1 and EERAID 5 can achieve 22 % and 11 % extra energy savings. There is little performance degradation or even better performance in EERAID 1 and 5. ...|$|R

