66|19|Public
5000|$|Programming {{languages}} {{researchers have}} also responded by replacing or supplementing the principle of maximal munch with other <b>lexical</b> <b>disambiguation</b> tactics. One approach is to utilize [...] "follow restrictions," [...] which instead of directly taking the longest match will put some restrictions on what characters can follow a valid match. For example, stipulating that strings matching [...] cannot be followed by an alphabetic character achieves the same effect as maximal munch with that regular expression. Another approach {{is to keep the}} principle of maximal munch but make it subordinate to some other principle, such as context (e.g., the right-shift token in Java would not be matched {{in the context of a}} generics expression, where it is syntactically invalid).|$|E
40|$|International audienceIn this paper, {{we present}} an automaton-based <b>lexical</b> <b>disambiguation</b> process for Lexicalized Tree-Adjoining Grammar (LTAG). This process builds on {{previous}} work of Bonfante et al. (2004), and extends it by computing a polarity-based abstraction, which contains information about left context. This extension {{allows for a}} faster <b>lexical</b> <b>disambiguation</b> by reducing the filtering automaton...|$|E
40|$|In {{this paper}} we sketch a decidable inference-based {{procedure}} for <b>lexical</b> <b>disambiguation</b> which operates on semantic representations of discourse and conceptual knowledge. In contrast to other approaches which use a classical logic for the disambiguating inferences and run into decidability problems, we argue {{on the basis}} of empirical evidence that the underlying inference mechanism has to be essentially incomplete in order to be (cognitively) adequate. Since our conceptual knowledge can be represented in a rather restricted representation language, it is then possible to show that the restrictions satisfied by the conceptual knowledge and the inferences ensure in an empirically adequate way the decidability of the problem, although a fully expressive language is used to represent discourse. 1 Introduction The determination of the contextual appropriateness of a reading of a lexically ambiguous sentence is commonly called <b>lexical</b> <b>disambiguation.</b> <b>Lexical</b> <b>disambiguation</b> presents a partic [...] ...|$|E
40|$|A surgeon {{can operate}} or {{approach}} a complicated integral calculus {{instead of a}} renal one: Implications for conceptual and <b>lexical</b> semantic <b>disambiguation.</b> This paper approaches <b>lexical</b> semantic <b>disambiguation</b> and polysemy {{in the context of}} medical language understanding. These issues have been addressed as linguistic and cognitive requirements to build a knowledge extraction tool that turns natural language input into conceptual graphs. Previous results obtained with semantic analysis of medical terms in the domain of transplantation and organ failure prompt us to check the capabilities of our prototype to deal with ambiguities and polysemy. We attempt to demonstrate how the respect of ambiguities when the co-text is not sufficient for disambiguation implies to introduce a void type in the concept type lattice. Then we show how the &quot;creative use of words &quot; imposes to dynamically allocate type, categories and roles with the co-text. Keywords: natural language processing, <b>lexical</b> semantic, polysemy, <b>disambiguation,</b> conceptual graphs, knowledge representation...|$|R
5000|$|WordNet::SenseRelate, {{a project}} that {{includes}} free, open source systems for word sense <b>disambiguation</b> and <b>lexical</b> sample sense <b>disambiguation</b> ...|$|R
40|$|This {{paper is}} trying to answer the {{question}} how to use concepts as AREs more eciently and also how do we match "small duck" with "small bird". The onto-matching introduced in the paper extensively uses lexical ontologies similar to WordNet. We work with two tasks in mind: (1) query expansion; and (2) document concept indexing. The NLP techniques that we envisage are POS-disambiguation, shallow parsing for NPs, <b>lexical</b> concept <b>disambiguation...</b>|$|R
40|$|International audienceWe {{propose a}} generic method to per- form <b>lexical</b> <b>disambiguation</b> in lexicalized {{grammatical}} formalisms. It relies on de- pendency constraints between words. The soundness {{of the method}} is due to invariant properties of the parsing in a given gram- mar that can be computed statically from the grammar...|$|E
40|$|We {{propose a}} new method to perform <b>lexical</b> <b>disambiguation</b> of Interaction Grammars. It {{deals with the}} order {{constraints}} on words. Actually, the soundness of the method is due to an invariant property of the parsing of an Interaction Grammar. We show how this invariant can be computed statically from the grammar. ...|$|E
40|$|The paper {{describes}} an analogy-based measure of word-sense proximity grounded on distributional evidence in typical contexts, and illustrates a computational system which {{makes use of}} this measure for purposes of <b>lexical</b> <b>disambiguation.</b> Experimental results show that word sense-analogy based on contexts of use compares favourably with classical word-sense similarity {{defined in terms of}} thesaural proximity...|$|E
40|$|In {{this paper}} {{we focus on}} {{practical}} issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to <b>lexical</b> category <b>disambiguation</b> (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. ...|$|R
40|$|Abstract. This paper {{introduces}} a semantic representation based on concept vectors. These semantic vectors aims at representing {{the set of}} ideas related to a given text segment. Vector operations associated with a classical morphosyntactical analysis can {{pave the way to}} an eÆcient lex-ical <b>disambiguation.</b> <b>Lexical</b> transfer can also be undertaken graciously with neither a transfer lexicon nor a pivot approach. ...|$|R
40|$|Abstract. Increasing {{the domain}} of {{locality}} by using Tree Adjoining Grammars (TAG) caused some applications, such as machine translation, to employ it for the disambiguation process. Successful experiments of employing TAG in French-English and Korean-English machine translation encouraged us {{to use it for}} another language pairs with very divergent properties, Persian and English. Using Synchronous TAG (S-TAG) for this pair of languages can benefit from syntactic and semantic features for transferring the source into the target language. Here, we report our experiments in translating English into Persian. Also, we present a model for <b>lexical</b> selection <b>disambiguation</b> based on the decision trees notion. An automatic learning method of the required decision trees from a sample data set is introduced, too. ...|$|R
40|$|Abstract. This paper {{presents}} a graph-theoretical approach to <b>lexical</b> <b>disambiguation</b> on word co-occurrences. Producing a dictionary similar to WordNet, {{this method is}} the counterpart to word sense disambiguation and thus makes one more step towards completely unsupervised natural language processing algorithms as well as generally {{better understanding of how}} to make computers meaningfully process natural language data...|$|E
40|$|Finite state {{methods for}} natural {{language}} processing often require the construction and the intersection of several automata. In this paper we investigate the question of determining the best order in which these intersections should be performed. We take as an example <b>lexical</b> <b>disambiguation</b> in polarity grammars. We {{show that there is}} no efficient way to minimize the state complexity of these intersections. ...|$|E
40|$|In this paper, I {{present a}} method for {{learning}} thematic role relations (selectional preferences) for wordnets by means of statistical corpus analysis. An evaluation on a gold standard, which I extracted from EuroWordNet, shows that this method achieves a learning accuracy of up to 77 %. I also propose a preprocessing step for a partial <b>lexical</b> <b>disambiguation</b> of the input data...|$|E
40|$|This paper {{presents}} {{a method of}} <b>lexical</b> se-mantic <b>disambiguation</b> in multilingual corpora and describes {{the construction of an}} artifi-cial word-aligned and lexically disambiguated gold-standard corpus from an existing mul-tilingual resource. The suggested approach uses sets of aligned words and phrases across languages as unique semantic tags similar to WordNet synsets that {{can be used as a}} part of unsupervised natural language processing and information retrieval tasks. The approach goes beyond one-to-one word alignment, and uses an algorithm for the aggregation of re-sults of pair-wise word alignment when the corpus contains several languages. When ap-plied to the new corpus, this methodology has proven capable of reducing the ambiguity of a polysemous word by one third on average. ...|$|R
40|$|In this paper, {{we propose}} a principled {{approach}} for disambiguating relations between constituent words of compound nouns whose heads are deverbal nouns, using {{the framework of}} lexical conceptual structure. The aim {{of this research is}} to reveal the complete set of <b>lexical</b> factor and <b>disambiguation</b> rules needed for application. The results of experiment for Japanese deverbal compounds and nominalization of English compounds show that our approach is highly promising. ...|$|R
40|$|Morphology (including word segmentation) Part {{of speech}} tagging Syntax and parsing Grammar Engineering Word sense <b>disambiguation</b> <b>Lexical</b> {{semantics}} Mathematical Linguistics Textual entailment and paraphrasing Discourse and pragmatics Knowledge acquisition and representation Noisy data analysis Machine translation Multilingual language processing Language generation Summarization Question answering Information retrieval Information extraction Topic classification and information filtering Non-topical classification (sentiment/genre analysis) Topic clustering Text and speech mining Text classification Evaluation (e. g., intrinsic, extrinsic, user studies...|$|R
40|$|International audienceFinite state {{methods for}} natural {{language}} processing often require the con- struction and the intersection of several automata. In this paper we investigate the question of determining the best order in which these intersections should be performed. We take as an example <b>lexical</b> <b>disambiguation</b> in polarity gram- mars. We {{show that there is}} no efficient way to minimize the state complexity of these intersections...|$|E
30|$|<b>Lexical</b> <b>disambiguation.</b> Escape {{characters}} or character replacement {{can be used}} {{to resolve}} conflicts between the grammars of the host and extension. This method allows input lexemes (i.e., character sequences) from the base language to also appear in the extension language without causing ambiguities, which would otherwise result in parsing errors. For example, the string Hello <World> can be transformed into Hello &lt;World&gt; to disambiguate it from XML markup syntax.|$|E
40|$|We {{present a}} simple {{technique}} for learning better SVMs using fewer training examples. Rather than using the standard SVM regularization, we regularize toward low weight-variance. Our new SVM objective remains a convex quadratic {{function of the}} weights, and is therefore computationally no harder to optimize than a standard SVM. Variance regularization is shown to enable dramatic improvements in the learning rates of SVMs on three <b>lexical</b> <b>disambiguation</b> tasks. ...|$|E
40|$|This paper {{presents}} a robust client/server {{implementation of a}} word sense disambiguator for English. This system associates a word with its meaning in a given context using dictionaries as tagged corpora in order to extract semantic disambiguation rules. Semantic rules are used as input of a semantic application program which encodes a linguistic strategy in order to select the best disambiguation rule for the word to be disambiguated. The semantic disambiguation rule application program {{is part of the}} client/server architecture enabling the processing of large corpora. 1 Introduction This paper describes the implementation of an online <b>lexical</b> semantic <b>disambiguation</b> system for English within a client/server linguistic application. This system allows to select the meaning of a word given its context of appearance in a text segment, and addresses the general problem of Word Sense Disambiguation (WSD), (Ide et al 90), (Gale et al. 92), (Gale et al. 93), (Leacock et al. 93), (Yarowsky 95 [...] ...|$|R
40|$|This paper {{describes}} {{experiments with}} an algorithm for <b>lexical</b> sense <b>disambiguation,</b> that is, predicting which of many possible senses {{of a word}} is intended in a given sentence. The definitions of senses of a given word are those used in LDOCE, the Longman Dictionary of Contemporary English [Procter et al., 1978]. The algorithm first as- signs a set of meanings or senses drawn from LDOCE to each word in the given sentence, and then chooses the combination of word-senses (one for each word in the sentence), yielding the maximum semantic over- lap. The metric of semantic overlap {{is based on the}} fact that LDOCE sense definitions are made in terms of the Longman Defining Vocabulary, effectively a (large) set of semantic primitives. Since the prob- lem of finding the word-sense-chain with maximum overlap can be viewed as a specialised example of the class of constraint-based optimisation problems for which Constraint Handling In Prolog (CHIP) was designed, we have chosen to implement our algorithm in CHI...|$|R
40|$|We {{introduce}} a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks reduces the sparsity of sense representations used for WSD. We evaluate these enriched representations within two <b>lexical</b> sample sense <b>disambiguation</b> benchmarks. Our results indicate that (1) features extracted from the corpus-based resource help to significantly outperform a model {{based solely on the}} lexical resource; (2) our method achieves results comparable or better to four state-of-the-art unsupervised knowledge-based WSD systems including three hybrid systems that also rely on text corpora. In contrast to these hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems...|$|R
40|$|This paper assesses several broad {{approaches}} to language analysis {{with respect to}} the problem of lexical ambiguity. The impact of the problem on both syntactic and semantic analysis is discussed, and several common methods for disambiguation, including the use of selectional restrictions and scriptal lexicons, are analyzed. Their shortcomings illustrate the need for complex inference to resolve ambiguity, which forms one of the key functional arguments in favor of integrating language analysis with memory and inference. However, it has proven surprisingly difficult to realize such an integrated approach in practice: An assessment of <b>lexical</b> <b>disambiguation</b> within some recent models which attempt to do so reveals that they rely largely on the traditional techniques of selectional restrictions and scriptal lexicons, with all their drawbacks. The difficulty is shown to stem primarily from the theories of memory and inferential processing utilized. The implications for recent {{approaches to}} language analysis based on connectionist mechanisms are explored. Finally, the requirements imposed by <b>lexical</b> <b>disambiguation</b> on theories of memory and inferential processing are discussed...|$|E
40|$|The {{present study}} tested whether <b>lexical</b> <b>disambiguation</b> in {{sentence}} context {{is affected by}} cross-language lexical activation. In Experiment 1 Spanish-English bilinguals read English sentences biasing the subordinate meaning of homonyms that were either cognates or non-cognates. Participants ’ ability to reject follow-up target words related to the dominant meaning showed greatest inhibition when the homonym was a cognate and the dominant meaning was shared with Spanish. In Experiment 2 a separate group of bilinguals read sentences biasing the dominant meaning of the homonyms and were instructed to accept target words related to any meaning of the homonym. In this case cognate status of the homonym facilitated acceptance of targets related to the subordinate meaning when this was shared with Spanish. A monolingual control experiment showed no effects of cognate status on processing. Findings are {{discussed in terms of}} expanding current models of <b>lexical</b> <b>disambiguation</b> to account for bilingual processing. 2 The cognition of reading continues to be a topic of fascination to researchers. Because reading involves the rapid orchestration of many cognitive processes operating on stimul...|$|E
40|$|<b>Lexical</b> <b>disambiguation</b> can be {{achieved}} using different sources of information. Aiming at high performance of automatic disambiguation {{it is important to}} know the relative importance and applicability of the various sources. In this paper we classify sev- eral sources of information and show how some of them can {{be achieved}} using statistical data. First evaluations indicate the extreme importance of local information, which mainly represents lexical associations and selectional restrictions for syntactically related words...|$|E
40|$|Current {{approaches}} to computational lexicology are knowledge-based (competenceoriented) {{and try to}} abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, {{and the application of}} the approach on a number of <b>lexical</b> acquisition and <b>disambiguation</b> tasks in phonology, morphology and syntax is described. 1 Introduction In computational lexicology, three basic questions guide current research: (1) which knowledge should be in the lexicon, (2) how should this knowledge be represented (e. g. to cope with the problems of lexical gaps), and (3) how can this knowledge be acquired. Current lexical research is eminently knowledge-based in this respect. It is also g [...] ...|$|R
40|$|It is a {{well-known}} fact that many natural language sentences may have {{a large number of}} possible syntactic analyses with semantically inconsistent interpretations. In this paper, we focus on selectional restrictions as a device for <b>lexical</b> and syntactic <b>disambiguation.</b> We will briefly sketch the notion of selectional restrictions from a practical point of view. We provide an example of how such constraints can be used for ambiguity resolution. In section 2 and 3, we address the topic of this article, the problem of acquiring selectional restrictions. We discuss different approaches, in particular methods based on statistical corpus analysis. The role of selectional restrictions in a concrete NLP system is shown in section 4. 1 Introduction The problem of lexical and syntactic ambiguity is encountered rather often in natural language processing. Theoretical linguistics has made no attempt to specify how a competence model might be implemented or approximated by psychologically plausibl [...] ...|$|R
40|$|In {{recent work}} on the Lexical-Functional Grammar (LFG) formalism, {{argument}} structure (a-structure) and lexical mapping theory {{have been used to}} explain many linguistic behaviours across languages. It has been suggested that the combination of c-structure, f-structure and a-structure might form a suitable architecture for Universal Grammar. If this suggestion is valid, the LFG formalism would be a suitable linguistic model for Machine Translation (MT). This thesis reports on the investigations carried out on using a-structure and lexical mapping theory for aiding various sub-tasks in MT. The two investigations described in this thesis are the abilities of a-structure and lexical mapping theory to: (1) aid different kinds of <b>lexical</b> and structural <b>disambiguations</b> involving verbs and prepositions, and (2) act as a suitable medium for carrying out source-to-target language transfer. Based on the results of these investigations, this thesis also gives an evaluation of how well a-structure and lexical mapping theory can improve the existing models of linguistic-based MT...|$|R
40|$|Abstract. To produce fast, {{reasonably}} intelligible {{and easily}} correctable translations between related languages, it suffices {{to use a}} machine translation strategy which uses shallow parsing techniques to refine what would usually be called word-for-word machine translation. This paper describes the application of shallow parsing techniques (morphological analysis, <b>lexical</b> <b>disambiguation,</b> and flat, local parsing) in a Portuguese– Spanish, Spanish–Portuguese machine translation system which is currently being developed by our group and is publicly and freely available a...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceIn {{the context}} of lexicalized grammars, we propose general methods for <b>lexical</b> <b>disambiguation</b> based on {{polarization}} and abstraction of grammatical formalisms. Polarization makes their resource sensitivity explicit and abstraction aims at keeping essentially the mechanism of neutralization between polarities. If a grammar in the initial formalism happens to be lexicalized, parsing with the simplified grammar in the abstract formalism can be used efficiently for filtering lexical selections...|$|E
40|$|To produce fast, {{reasonably}} intelligible {{and easily}} correctable translations between related languages, it su#ces {{to use a}} machine translation strategy which uses shallow parsing techniques to refine what would usually be called word-for-word machine translation. This paper describes the application of shallow parsing techniques (morphological analysis, <b>lexical</b> <b>disambiguation,</b> and flat, local parsing) in a Portuguese [...] Spanish, Spanish [...] Portuguese machine translation system which is currently being developed by our group and is publicly and freely available at [URL]...|$|E
40|$|A new {{approach}} to semantic interpretation in natural language understanding is described, together with mechanisms for both <b>lexical</b> and structural <b>disambiguation</b> that work {{in concert with the}} semantic interpreter. ABSITY, the system described, is a Montague-inspired semantic interpreter. Like Montague formalisms, its semantics is compositional by design and is strongly typed, with semantic rules in one-to-one correspondence with the meaning-affecting rules of a Marcus parser. The Montague semantic objects [...] functors and truth conditions [...] are replaced with elements of the frame language FRAIL. ABSITY's partial results are always well-formed FRAIL objects. A semantic interpreter must be able to provide feedback to the parser to help it handle structural ambiguities. In ABS 1 TY, this is done by the "'Semantic Enquiry Desk," a process that answers the parser's questions on semantic preferences. Disambiguation of word senses and of case slots is done by a set of procedures, one per word or slot, each of which determines the word or slot's correct sense, in cooperation with the other procedures. It is from the fact that partial results are always well-formed semantic objects that the system gains much of its power. This, in turn, comes from the strict correspondence between syntax and semantics in ABSITY. The result is a. foundation for semantic interpretation superior to previous approaches...|$|R
40|$|It {{could be}} true that any attempt to build a system to process the content of a given text written in a given {{language}} will be faced by tackling language analysis tasks. To reach the semantic representation of any sentence, the system should be enriched with a technique for <b>lexical</b> and syntactic <b>disambiguation.</b> Having finished with semantic representation, the system {{should be able to}} re-synthesize the semantic representation into another acceptable sentence in the target language. However, it is not that easy; there are many problems that need to be solved in both the analysis and synthesis processes. To avoid the pitfalls associated with approaches relying on intermediate representations, e. g. syntactic tree, this paper presents an approach on which processing Arabic content, and even the exchange of information among languages, starts directly from a semantic layer without passing through the level of syntax. The approach encodes Arabic structures into a set of semantic relations between a set of nodes representing the elements (words) of the sentence as concepts. Once the concepts are built, the relations between them are determined and can be decoded again to any other language. The grammar for the encoding process is implemented in Universal Networking Language (UNL); it enables computers to understand natural languages which will make it possible for humans to communicate with machines in natural language. Encoding Arabic sentences in terms of semantic networks depends mainly on holding theta role...|$|R
40|$|McCarthy is a Royal Society Dorothy Hodgkin {{fellow in}} the Department of Informatics at the University of Sussex. Her {{research}} interests in natural language processing primarily concern automatic lexical acquisition. Diana was born in Pembury, Kent in 1964. She was awarded her first degree in Speech Pathology by the University of Manchester in 1987. She practised as a {{speech and language therapist}} for several years before undertaking a Masters degree at the University of Sussex in knowledge-based systems out of a curiosity as to how one might use a computer to model human reasoning, and language in particular. After being awarded her Masters in 1989, she worked in the commercial sector applying artificial intelligence techniques to meet customer needs before returning to academia in 1995. She obtained her PhD from the University of Sussex in 2001 and worked on a number of EU and EPSRC projects on <b>lexical</b> acquisition and <b>disambiguation.</b> In 2004, she was awarded the best paper prize at the Annual Meeting of the Association of Computational Linguistics for her work with colleagues at the University of Sussex on finding the most probable meaning of a word from a sample of text. She was awarded the best poster prize for a work presented at the Women in Computing: Grace Hopper Colloquium held in May 2007 by the British Computer Society. She is fascinated by computational approaches that learn about words and their meanings from data without human instruction and has recently co-organized an international competition aimed at comparing such approaches with those that require human supervision...|$|R
