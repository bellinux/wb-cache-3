175|51|Public
2500|$|Pareto Q-Q plots {{compare the}} {{quantiles}} of the <b>log-transformed</b> <b>data</b> to the corresponding quantiles of an exponential distribution with mean 1 (or to the quantiles {{of a standard}} Pareto distribution) by plotting the former versus the latter. If the resultant scatterplot suggests that the plotted points [...] " [...] asymptotically converge" [...] to a straight line, then a power-law distribution should be suspected. [...] A limitation of Pareto Q-Q plots is that they behave poorly when the tail index [...] (also called Pareto index) is close to 0, because Pareto Q-Q plots are not designed to identify distributions with slowly varying tails.|$|E
2500|$|On {{the other}} hand, in its version for {{identifying}} power-law probability distributions, the mean residual life plot consists of first log-transforming the data, and then [...] plotting {{the average of}} those <b>log-transformed</b> <b>data</b> that are higher than the i-th order statistic versus the i-th order statistic, for i=1,...,n, where n {{is the size of}} the random sample. If the resultant scatterplot suggests that the plotted points tend to [...] "stabilize" [...] about a horizontal straight line, then a power-law distribution should be suspected. Since the mean residual life plot is very sensitive to outliers (it is not robust), it usually produces plots that are difficult to interpret; for this reason, such plots are usually called Hill horror plots ...|$|E
5000|$|Pareto Q-Q plots {{compare the}} {{quantiles}} of the <b>log-transformed</b> <b>data</b> to the corresponding quantiles of an exponential distribution with mean 1 (or to the quantiles {{of a standard}} Pareto distribution) by plotting the former versus the latter. If the resultant scatterplot suggests that the plotted points [...] " [...] asymptotically converge" [...] to a straight line, then a power-law distribution should be suspected. A limitation of Pareto Q-Q plots is that they behave poorly when the tail index [...] (also called Pareto index) is close to 0, because Pareto Q-Q plots are not designed to identify distributions with slowly varying tails.|$|E
40|$|Fig. 4. Scatter plots {{obtained}} from morphological analyses of Tylonycteris spp. A. Range of GLS measurements of specimens within {{each group of}} Tylonycteris spp. B. Range of PC * 1 scores of specimens of Tylonycteris spp. {{obtained from}} PCA of <b>log-transformed</b> raw <b>data</b> of craniodental measurements. C. Plot of PC 1 against PC 2 obtained from PCA on <b>log-transformed</b> standardized <b>data.</b> Triangles and circles refer to T. pachypus s. lat. and T. robustula s. lat., respectively. Colour patterns indicate the mtDNA haplogroups: green for Tp 3 and Tr 3 (northern Indochina); blue for Tp 2 and Tr 2 (other regions of the Southeast Asian mainland); red for Tp 1 and Tr 1 (Sundaland) ...|$|R
40|$|The primary {{objective}} {{of the present study}} was to point out different approaches to background limits assessment. The content of selected trace elements is relatively low, and elements have right-skewed asymmetrical distribution, with high dispersion. Background for Cr and Ni (60 mg kg- 1) which are obtained by graphical methods cumulative probability plot (CDF) and box-plot are similar. Natural and <b>log-transform</b> <b>data</b> are used for empirical methods. Results from antilogarithmic values are significantly higher than from natural ones. Background limits obtained by empirical methods are different. Maps show that the largest part of territory has a relatively low concentration of investigated elements, whose background limits calculated by [Median+ 2 MAD] methods are the most suitable. On the part of area with elevated elements content, where natural origin of elements had previously been proven, background limits are determined with box-plot method...|$|R
30|$|Fish {{assemblage}} composition <b>data</b> were <b>log-transformed</b> (Clarke and Warwick 1994) to downweigh {{the influence}} of dominant species before analysis. A Bray-Curtis similarity analysis was also {{used to create a}} resemblance matrix of the <b>log-transformed</b> assemblage <b>data.</b> The resemblance matrixes were also used for non-metric multidimensional scaling (MDS) to show the structure patterns of different stations at different tides. A two-way ANOSIM was used to indicate the assemblage differences between the two tides and three stations.|$|R
5000|$|On {{the other}} hand, in its version for {{identifying}} power-law probability distributions, the mean residual life plot consists of first log-transforming the data, and then plotting {{the average of}} those <b>log-transformed</b> <b>data</b> that are higher than the i-th order statistic versus the i-th order statistic, for i = 1, ..., n, where n {{is the size of}} the random sample. If the resultant scatterplot suggests that the plotted points tend to [...] "stabilize" [...] about a horizontal straight line, then a power-law distribution should be suspected. Since the mean residual life plot is very sensitive to outliers (it is not robust), it usually produces plots that are difficult to interpret; for this reason, such plots are usually called Hill horror plots ...|$|E
5000|$|Another {{example is}} given by: The air lead levels were {{collected}} from [...] different areas within the facility. It was noted that the log-transformed lead levels fitted a normal distribution well (that is, the data are from a lognormal distribution. Let [...] and , respectively, denote the population mean and variance for the <b>log-transformed</b> <b>data.</b> If [...] denotes the corresponding random variable, we thus have [...] We note that exp(mu) is the median air lead level. A confidence interval for mu can be constructed the usual way, based on the t-distribution; this in turn will provide a confidence interval for the median air lead level. If [...] and S denote the sample {{mean and standard deviation}} of the <b>log-transformed</b> <b>data</b> for a sample of size n, a 95% confidence interval for mu is given by , where [...] denotes the 1-alpha quantile of a t-distribution with m degrees of freedom. It may also be of interest to derive a 95% upper confidence bound for the median air lead level. Such a bound for mu is given by [...] Consequently, a 95% upper confidence bound for the median air lead is given by [...] Now suppose we want to predict the air lead level at a particular area within the laboratory. A 95% upper prediction limit for the log-transformed lead level is given by [...] A two-sided prediction interval can be similarly computed. The meaning and interpretation of these intervals are well known. For example, if the confidence interval [...] is computed repeatedly from independent samples, 95% of the intervals so computed will include the true value of , in the long run. In other words, the interval is meant to provide information concerning the parameter [...] only. A prediction interval has a similar interpretation, and is meant to provide information concerning a single lead level only. Now suppose we want to use the sample to conclude whether or not at least 95% of the population lead levels are below a threshold. The confidence interval and prediction interval cannot answer this question, since the confidence interval is only for the median lead level, and the prediction interval is only for a single lead level. What is required is a tolerance interval; more specifically, an upper tolerance limit. The upper tolerance limit is to be computed subject to the condition that at least 95% of the population lead levels is below the limit, with a certain confidence level, say 99%.|$|E
30|$|In vivo study {{data are}} {{presented}} as individual data and means {{in the form of}} scatter plots. Statistical data evaluation versus LPS/vehicle control was performed by one-way analysis of variance (ANOVA) followed by Dunnett’s Multiple Comparison Test of <b>log-transformed</b> <b>data.</b>|$|E
30|$|Growth {{parameters}} {{were derived}} using standard methodology. Briefly, ΔOD values describe maximum OD – minimum OD; the lag phase {{is defined as}} the length of time a culture spends at <[*] 10 % of maximum OD; T½Max values are equivalent to the time taken to reach half the maximum increase in growth of a culture (ΔOD[*]×[*] 0.5). Fastest doubling times (DT) were estimated by dividing the natural logarithm of 2 by the fastest culture growth rates (μ), where μ is the gradient of the linear trend line fitted to <b>log-transformed</b> OD <b>data.</b>|$|R
30|$|Interpretation of the Condition[*]×[*]Age {{interaction}} {{is complicated by}} the possibility that age-related differences in dual-task performance may simply reflect an increase that is proportional to the age-related slowing observed in the single-task baseline (e.g., Lindholm & Parkinson, 1983). However, a log transformation of the RT data makes proportional changes additive, whereas an interaction would provide evidence that the dual-task costs were greater for older adults (Madden, 2001). The significant Condition[*]×[*]Age interaction found in the <b>log-transformed</b> RT <b>data</b> indicates that the dual-task costs experienced by older adults are greater than would be expected by the proportional increase.|$|R
40|$|Asian seabass, Lates calcarifer (Bloch) - a {{catadromous}} centropomid perch, is a {{good candidate}} species for brackishwater aquaculture in India. The length-weight relationship and the relative condition of L. calcarifer were assessed under culture condition. The length of the fish samples ranged from 25 to 240 mm and the relative condition (w sub(r)) of the fish for different length groups ranged from 99. 54 to 104. 39, indicating the good condition of the fish. The regression analysis of <b>log-transformed</b> length-weight <b>data</b> was carried out and the 'b' coefficient indicates the good condition of fish showing an isometric growth in the juvenile phase under culture condition...|$|R
40|$|Description The georob package {{provides}} {{functions for}} fitting linear models with spatially correlated errors by robust and Gaussian Restricted Maximum Likelihood and for computing robust and customary point and block kriging predictions, along with utility functions for cross-validation and for unbiased back-transformation of kriging predictions of <b>log-transformed</b> <b>data...</b>|$|E
40|$|Experiments were {{performed}} in Baltimore, Maryland and in Santiago, Chile, {{to determine the}} level of Salmonella typhi antigen-driven in vitro lymphocyte replication response which signifies specific acquired immunity to this bacterium and {{to determine the best}} method of data analysis and form of data presentation. Lymphocyte replication was measured as incorporation of 3 H-thymidine into desoxyribonucleic acid. Data (ct/min/culture) were analyzed in raw form and following log transformation, by non-parametric and parametric statistical procedures. A preference was developed for <b>log-transformed</b> <b>data</b> and discriminant analysis. Discriminant analysis of <b>log-transformed</b> <b>data</b> revealed 3 H-thymidine incorporation rates greater than 3, 433 for particulate S. typhi, Ty 2 antigen stimulated cultures signified acquired immunity at a sensitivity and specificity of 82. 7; for soluble S. typhi O polysaccharide antigen-stimulated cultures, ct/min/culture values of greater than 1, 237 signified immunity (sensitivity and specificity 70. 5 %) ...|$|E
30|$|PCA {{was applied}} to {{standardized}} <b>log-transformed</b> <b>data</b> set to identify the latent factors. The objective of this analysis was primarily to create an entirely new set of factors much smaller in number {{when compared with the}} original data set in subsequent analysis. Only factors with eigen values {{greater than or equal to}} 1 will be accepted as possible sources of variance in the data.|$|E
30|$|In {{order to}} {{understand}} the trophic structure changes and habitat usage patterns of different fish in different assemblages, the <b>log-transformed</b> assemblage <b>data</b> of the predominant fish species of the six trophic groups were used to create a heatmap (Wilkinson and Friendly 2009) in the R heatmap package (R Development Core Team 2012). The heatmap can indicate habitat usage patterns of different fish species at different stations. We separated some fish species of the same trophic groups into different functional groups because they had distinct habitat usage patterns (discussed in the Results section). All of the analyses were performed by R and PRIMER 6.1. 13 and PERMANOVA[*]+[*] 1.0. 3 (Clarke and Gorley 2006; Anderson et al. 2008).|$|R
40|$|Two {{important}} {{challenges to}} the use of serological assays for influenza surveillance include the substantial amount of experimental effort involved, and the inherent noisiness of serological data. Here, informed by the observation that <b>log-transformed</b> serological <b>data</b> (obtained from the hemagglutination-inhibition assay) exist in an effectively one-dimensional space, computational methods are developed for accurately and efficiently recovering unmeasured serological data from a sample of measured data, and systematically minimizing noise found in the measured data. Careful application of these methods would enable the collection of better-quality serological data on a greater number of circulating influenza viruses than is currently possible, and improve the ability to identify potential epidemic/pandemic viruses before they become widespread. Although the focus here is on influenza surveillance, the described methods are more widely applicable...|$|R
40|$|The {{purpose of}} this project was to {{evaluate}} the utility of low- and high-repetition maximum (RM) strength tests used to assess rowers. Twenty elite heavyweight males (age 23. 7 ± 4. 0 years) performed four tests (5 RM, 30 RM, 60 RM and 120 RM) using leg press and seated arm pulling exercise on a dynamometer. Each test was repeated on two further occasions; 3 and 7 days from the initial trial. Per cent typical error (within-participant variation) and intraclass correlation coefficients (ICCs) were calculated using <b>log-transformed</b> repeated-measures <b>data.</b> High-repetition tests (30 RM, 60 RM and 120 RM), involving seated arm pulling exercise are not recommended {{to be included in}} an assessment battery, as they had unsatisfactory measurement precision (per cent typical error 3 ̆e 5...|$|R
40|$|The use of <b>log-transformed</b> <b>data</b> {{has become}} {{standard}} in macroeconomic forecasting with VAR models. However, its appropriateness {{in the context}} of out-of-sample forecasts has not yet been exposed to a thorough empirical investigation. With the aim of filling this void, a broad sample of VAR models is employed in a multi-country set up and approximately 16 Mio. pseudo-out-of-sample forecasts of GDP are evaluated. The results show that, on average, the knee-jerk transformation of the data is at best harmless...|$|E
40|$|Abstract: The {{objective}} {{of the present study}} was to point out different approaches for background limits assessment of Zn and Hg. Background limits obtained by graphical methods (cumulative probability plot-CDF and boxplot) were similar for Zn (about 100 mg/kg- 1), while background limits for Hg were different (from 0. 2 mg kg- 1 to 0. 3 mg kg- 1). For three empirical methods, besides natural data, log-transformed ones were used. Results from log-transformed limits were higher than from natural (except classical methods for Hg). The most of the territory of Eastern Serbia has low content of these elements, whose background limits calculated using <b>log-transformed</b> <b>data</b> by [Median+ 2 MAD] methods (73 mg kg- 1 for Zn and 0. 15 mg kg- 1 for Hg) are the most suitable (MAD-median of the absolute deviations from the data′s median). On the part of area with elevated elements content, background limits were determined using <b>log-transformed</b> <b>data</b> by methods with the highest values: for Zn it was [Mean+ 2 Sdev], and for Hg box-plot (166 mg kg- 1 and 0. 41 mg kg- 1 for Zn and Hg respectively) ...|$|E
40|$|I {{have with}} great {{interest}} read the paper by de Vocht et al. (2009), which compared respirable dust sam-ples collected in the brick-manufacturing industry using both a Higgins–Dewell type cyclone and a dual-fraction IOM sampler. The size separation in the dual-fraction IOM sampler is carried out by a foam inserted into the nozzle of the IOM sampler. I would {{like to comment on}} some of the regression analyses presented and, what I consider to be, a lack of analysis of their data. There is something strange with the model for log-transformed concentration data as it is presented in Fig. 2 and Table 4. The least-squares regression line for the <b>log-transformed</b> <b>data</b> plotted in Figure 2 goes through the origin with a slope (b) of 0. 86 (as stated in the figure caption). However, this b value is not presented in Table 4, which contains slopes for all the four models with the <b>log-transformed</b> <b>data.</b> Nei-ther does the line go through the data presented in Figure 2, which makes it a strange result of a regres-sion analysis. In the paper, no arguments were presented for why the regression model should go through the origin of the <b>log-transformed</b> <b>data.</b> Even if a proportional re-gression model might be valid for the un-transformed respirable concentration data for the IOM dual frac-tion filter and cyclone filter, respectively, (i. e. through the origin of the un-transformed data), CIOMfilter 5 k Ccyc (with k expected to be different from unity), {{this is not the case}} when the data have been log trans-formed. In this case, the proper model is lnCIOMfilter 5 A þ B lnCcyc where A is expected to be approximately equal to ln(k), and B (b in the terminology of the paper) is ex-pected to be approximately unity, if no dependence on concentration is expected or found. If B a priori is put identical to unity, one gets the model used by Vincent et al. in workplace comparisons of the IOM sampler with the 37 -mm closed-face filte...|$|E
40|$|Background: It {{is assumed}} that {{different}} pain phenotypes are based on varying molecular pathomechanisms. Distinct ion channels seem {{to be associated with}} the perception of cold pain, in particular TRPM 8 and TRPA 1 have been highlighted previously. The present study analyzed the distribution of cold pain thresholds with focus at describing the multimodality based on the hypothesis that it reflects a contribution of distinct ion channels. Methods: Cold pain thresholds (CPT) were available from 329 healthy volunteers (aged 18 - 37 years; 159 men) enrolled in previous studies. The distribution of the pooled and <b>log-transformed</b> threshold <b>data</b> was described using a kernel density estimation (Pareto Density Estimation (PDE)) and subsequently, the log data was modeled as a mixture of Gaussian distributions using the expectation maximization (EM) algorithm to optimize the fit. Results: CPTs were clearly multi-modally distributed. Fitting a Gaussian Mixture Model (GMM) to the <b>log-transformed</b> threshold <b>data</b> revealed that the best fit is obtained when applying a three-model distribution pattern. The modes of the identified three Gaussian distributions, retransformed from the log domain to the mean stimulation temperatures at which the subjects had indicated pain thresholds, were obtained at 23. 7 °C, 13. 2 °C and 1. 5 °C for Gaussian # 1, # 2 and # 3, respectively. Conclusions: The localization of the first and second Gaussians was interpreted as reflecting the contribution of two different cold sensors. From the calculated localization of the modes of the first two Gaussians, the hypothesis of an involvement of TRPM 8, sensing temperatures from 25 – 24 °C, and TRPA 1, sensing cold from 17 °C can be derived. In that case, subjects belonging to either Gaussian would possess a dominance of the one or the other receptor at the skin area where the cold stimuli had been applied. The findings therefore support a suitability of complex analytical approaches to detect mechanistically determined patterns from pain phenotype data...|$|R
40|$|A species {{sensitivity}} distribution (SSD) models data on {{toxicity of}} a specific toxicant to species in a defined assemblage. SSDs are typically assumed to be parametric, despite noteworthy criticism, with a standard proposal being the log-normal distribution. Recently, and confusingly, there have emerged different statistical methods in the ecotoxicological risk assessment literature, independent of the distributional assumption, for fitting SSDs to toxicity data with the overall aim of estimating the concentration of the toxicant that is hazardous to % of the biological assemblage (usually with small). We analyze two such estimators derived from simple linear regression applied to the ordered <b>log-transformed</b> toxicity <b>data</b> values and probit transformed rank-based plotting positions. These are compared to the more intuitive and statistically defensible confidence limit-based estimator. We conclude based on a large-scale simulation study that the latter estimator {{should be used in}} typical assessments where a pointwise value of the hazardous concentration is required...|$|R
40|$|OBJECTIVES Age- and height-adjusted spirometric lung {{function}} of South Asian children {{is lower than}} those of white children. It is unclear whether this is purely genetic, or partly explained by the environment. In this study, we assessed whether cultural factors, socioeconomic status, intrauterine growth, environmental exposures, or a family and personal history of wheeze contribute to explaining the ethnic differences in spirometric {{lung function}}. METHODS We studied children aged 9 to 14 years from a population-based cohort, including 1088 white children and 275 UK-born South Asians. <b>Log-transformed</b> spirometric <b>data</b> were analyzed using multiple linear regressions, adjusting for anthropometric factors. Five different additional models adjusted for (1) cultural factors, (2) indicators of socioeconomic status, (3) perinatal data reflecting intrauterine growth, (4) environmental exposures, and (5) personal and family history of wheeze. RESULTS Height- and gender-adjusted forced vital capacity (FVC) and forced expired volume in 1 second (FEV 1) were lower in South Asian than white children (relative difference - 11...|$|R
40|$|Precise {{estimation}} of root biomass {{is important for}} understanding carbon stocks and dynamics in forests. Traditionally, biomass estimates are based on allometric scaling relationships between stem diameter and coarse root biomass calculated using linear regression (LR) on <b>log-transformed</b> <b>data.</b> Recently, {{it has been suggested}} that nonlinear regression (NLR) is a preferable fitting method for scaling relationships. But while this claim has been contested on both theoretical and empirical grounds, and statistical methods have been developed to aid in choosing between the two methods in particular cases, few studies have examined the ramifications of erroneously applying NLR. Here, we use direct measurements of 159 trees belonging to three locally dominant species in east China to compare the LR and NLR models of diameter-root biomass allometry. We then contrast model predictions by estimating stand coarse root biomass based on census data from the nearby 24 -ha Gutianshan forest plot and by testing the ability of the models to predict known root biomass values measured on multiple tropical species at the Pasoh Forest Reserve in Malaysia. Based on likelihood estimates for model error distributions, as well as the accuracy of extrapolative predictions, we find that LR on <b>log-transformed</b> <b>data</b> is superior to NLR for fitting diameter-root biomass scaling models. More importantly, inappropriately using NLR leads to grossly inaccurat...|$|E
40|$|The {{principal}} {{response curve}} (PRC) method is a constrained ordination method developed {{specifically for the}} analysis of community data collected in mesocosm experiments, which provides easily understood summaries and graphical representations of community response to stress. It is a redundancy analysis method and is usually performed on log-transformed abundance data. The choice of a measure of dissimilarity between samples and the choice of the data transformation significantly affect the results of multivariate analysis. Dissimilarity measures that are more ecologically meaningful than the Euclidean distance can be incorporated into the PRC using distance-based redundancy analysis. The present study investigates the ordinations produced by a small selection of dissimilarity measures: the Euclidean distance using log-transformed and Hellinger-transformed data and the Bray-Curtis dissimilarity using raw and <b>log-transformed</b> <b>data.</b> It compares 2 data sets from experiments on the effect of the anti-inflammatory drug diclofenac and the insecticide chlorpyrifos on macroinvertebrate communities. The choice of dissimilarity measure can determine the outcome of a risk assessment. For the diclofenac data set, the PRCs were different depending on the dissimilarity measure: the community no-effect concentration was lowest for the Bray-Curtis on <b>log-transformed</b> <b>data</b> and Hellinger dissimilarity measures. For chlorpyrifos, however, the PRCs were similar for all dissimilarity measures...|$|E
40|$|The Living Planet Index was {{developed}} to measure the changing state of the world's biodiversity over time. It uses time-series data to calculate average rates of change in {{a large number of}} populations of terrestrial, freshwater and marine vertebrate species. The dataset contains about 3000 population time series for over 1100 species. Two methods of calculating the index are outlined: the chain method and a method based on linear modelling of <b>log-transformed</b> <b>data.</b> The dataset is analysed to compare the relative representation of biogeographic realms, ecoregional biomes, threat status and taxonomic groups among species contributing to the index...|$|E
40|$|Motivation: Tissue {{samples of}} both tumor cells mixed with stromal cells cause underdetection of gene {{expression}} signatures associated with cancer prognosis or response to treatment. In silico dissection of mixed cell samples {{is essential for}} analyzing expression data generated in cancer studies. Currently, a systematic approach is lacking to address three challenges in computational deconvolution: 1) violation of linear addition of expression levels from multiple tissues when <b>log-transformed</b> microarray <b>data</b> are used; 2) estimation of both tumor proportion and tumor-specific expression, when neither is known a priori; and 3) estimation of expression profiles for individual patients. Results: We have developed a statistical method for deconvolving mixed cancer transcriptomes, DeMix, which addresses the above issues in array-based expression data. We demonstrate the performance of our model in synthetic and real, publicly available, datasets. DeMix {{can be applied to}} ongoing biomarker-based clinical studies and to the vast expression datasets previously generated from mixed tumor and stromal cell samples. Availability: All codes are written in C and integrated into an R function, which is available a...|$|R
40|$|BACKGROUND: Increased {{concentrations}} of high-sensitivity C-reactive protein (hs-CRP), {{a marker of}} systemic inflammation, are associated with increased risk for coronary heart disease. Because of its relationship to inflammation, hs-CRP has considerable biologic variation. This study was carried out to characterize CRP variation and {{to compare it to}} another risk factor, total serum cholesterol. METHODS: One hundred thirteen individuals were scheduled to have five measurements each of hs-CRP and total cholesterol carried out at quarterly intervals over a 1 -year period. Variations of hs-CRP and total cholesterol were characterized, and classification accuracy was described and compared for both. RESULTS: The relative variation was comparable for hs-CRP and total cholesterol. When classified by quartile, 63 % of first and second hs-CRP measurements were in agreement; for total cholesterol it was 60 %. Ninety percent of hs-CRP measurements were within one quartile of each other. This relationship was not altered by the use of <b>log-transformed</b> hs-CRP <b>data.</b> CONCLUSION: hs-CRP has a degree of measurement stability that {{is similar to that of}} total cholesterol...|$|R
40|$|The {{statistical}} {{technique of}} functional data analysis (FDA) {{is applied to}} a time series analysis of plankton monitoring data. The analysis is focused on revealing patterns in the seasonal cycle to assess interannual. variability of several different taxonomic groups of plankton. Cell concentrations of diatom, dinoflagellate and zooplankton abundances from the Bay of Fundy, Canada provide the observations for analysis. FDA {{was performed on the}} <b>log-transformed</b> abundance <b>data</b> as a new approach for treating such types of sparse and noisy data. Differences in the seasonal progression were seen, with peak numbers, timings and abundance levels varying for the three groups as determined by curve registration and higher order derivatives using the objectively fit FDA curves. Nonmetric multidimensional scaling was used to capture seasonal variation among years. These results were further assessed in terms of dominant species and the relationships between groups for different seasons and years. It is anticipated that the easy to use, general and flexible technique of FDA could be applied {{to a wide variety of}} marine ecological data that are characterized by missing values and non-Gaussian distributions...|$|R
40|$|International audienceDaily {{precipitation}} {{is recorded}} {{as the total}} amount of water collected by a rain-gauge in 24 h. Events are modelled as a Poisson process and the 24 h precipitation by a Generalised Pareto Distribution (GPD) of excesses. Hazard assessment is complete when estimates of the Poisson rate and the distribution parameters, together with a measure of their uncertainty, are obtained. The shape parameter of the GPD determines the support of the variable: Weibull domain of attraction (DA) corresponds to finite support variables as should be for natural phenomena. However, Fréchet DA has been reported for daily precipitation, which implies an infinite support and a heavy-tailed distribution. Bayesian techniques are used to estimate the parameters. The approach is illustrated with precipitation data from the Eastern coast of the Iberian Peninsula affected by severe convective precipitation. The estimated GPD is mainly in the Fréchet DA, something incompatible with the common sense assumption of that precipitation is a bounded phenomenon. The bounded character of precipitation is then taken as a priori hypothesis. Consistency of this hypothesis with the data is checked in two cases: using the raw-data (in mm) and using <b>log-transformed</b> <b>data.</b> As expected, a Bayesian model checking clearly rejects the model in the raw-data case. However, <b>log-transformed</b> <b>data</b> seem to be consistent with the model. This fact {{may be due to the}} adequacy of the log-scale to represent positive measurements for which differences are better relative than absolute...|$|E
40|$|The {{estimation}} of 90 % parametric confidence intervals (CIs) of mean AUC and Cmax ratios in bioequivalence (BE) tests {{are based upon}} the assumption that formulation effects in <b>log-transformed</b> <b>data</b> are normally distributed. To compare the parametric CIs with those obtained from nonparametric methods we performed repeated {{estimation of}} bootstrap-resampled datasets. The AUC and Cmax values from 3 archived datasets were used. BE tests on 1, 000 resampled datasets from each archived dataset were performed using SAS (Enterprise Guide Ver. 3). Bootstrap nonparametric 90 % CIs of formulation effects were then compared with the parametric 90 % CIs of the original datasets. The 90 % CIs of formulation effects estimated from the 3 archived datasets were slightly different from nonparametric 90 % CIs obtained from BE tests on resampled datasets. Histograms and density curves of formulation effects obtained from resampled datasets {{were similar to those}} of normal distribution. However, in 2 of 3 resampled log (AUC) datasets, the estimates of formulation effects did not follow the Gaussian distribution. Bias-corrected and accelerated (BCa) CIs, one of the nonparametric CIs of formulation effects, shifted outside the parametric 90 % CIs of the archived datasets in these 2 non-normally distributed resampled log (AUC) datasets. Currently, the 80 ~ 125 % rule based upon the parametric 90 % CIs is widely accepted under the assumption of normally distributed formulation effects in <b>log-transformed</b> <b>data.</b> However, nonparametric CIs may be a better choice when data do not follow this assumption...|$|E
40|$|Abstract We {{evaluated}} three mathematical {{procedures to}} estimate {{the parameters of the}} relationship between weight and length for Cichla monoculus: least squares ordinary regression on <b>log-transformed</b> <b>data,</b> non-linear estimation using raw data and a mix of multivariate analysis and fuzzy logic. Our goal was to find an alternative approach that considers the uncertainties inherent to this biological model. We found that non-linear estimation generated more consistent estimates than least squares regression. Our results also indicate {{that it is possible to}} find consistent estimates of the parameters directly from the centers of mass of each cluster. However, the most important result is the intervals obtained with the fuzzy inference system...|$|E
40|$|Wood dust data {{held in the}} Health and Safety Executive (HSE) National Exposure DataBase (NEDB) were {{reviewed}} to investigate the long-term changes in inhalation exposure from 1985 to 2005. In addition, follow-up sampling measurements were obtained from selected companies where exposure measurements had been collected prior to 1994, thereby providing a follow-up period of at least 10 years, to determine whether changes in exposure levels had occurred, with key staff being interviewed to identify factors that might be responsible for any changes observed. Methods: Analysis of the temporal trend in exposure concentrations was performed using Linear Mixed Effect Models on the <b>log-transformed</b> NEDB <b>data</b> set and expressed as the rel-ative annual change in concentration. Results: For the NEDB wood dust data, an annual decline of geometric mean (GM) exposure of 8. 1 % per year was found based on 1459 exposure measurements collected between 1985 and 2003. This trend was predominantly observed in data from inspection visits (measurements col-lected on a mandatory basis by a Specialist HSE Inspector) (n 5 1009), while data from repre-sentative surveys (measurements collected {{on a voluntary basis}} to provide information o...|$|R
40|$|The RNA-sequencing (RNA-seq) is {{becoming}} increasingly popular for quantifying gene expression levels. Since the RNA-seq measurements are relative in nature, between-sample normalization of counts is an essential step in differential expression (DE) analysis. The normalization of existing DE detection algorithms is ad hoc and performed once for all prior to DE detection, which may be suboptimal since ideally normalization {{should be based on}} non-DE genes only and thus coupled with DE detection. We propose a unified statistical model for joint normalization and DE detection of <b>log-transformed</b> RNA-seq <b>data.</b> Sample-specific normalization factors are modeled as unknown parameters in the gene-wise linear models and jointly estimated with the regression coefficients. By imposing sparsity-inducing L 1 penalty (or mixed L 1 /L 2 -norm for multiple treatment conditions) on the regression coefficients, we formulate the problem as a penalized least-squares regression problem and apply the augmented lagrangian method to solve it. Simulation studies show that the proposed model and algorithms outperform existing methods in terms of detection power and false-positive rate when {{more than half of the}} genes are differentially expressed and/or when the up- and down-regulated genes among DE genes are unbalanced in amount...|$|R
40|$|The Chesapeake Bay and its {{surrounding}} tributaries are home to over 3, 600 species of plants and animals. In order to assess {{the health of the}} region, the Maryland Department of Natural Resources (DNR) monitors various parameters, such as dissolved oxygen, with monitoring stations located throughout the tidal waterways. Utilizing data provided by DNR, we assessed the waterways for areas of water quality concern. We analyzed the percentage of the readings taken for each parameter that failed to meet the threshold values and used the Wilcoxon Signed-Rank Test to determine the statuses of the stations. In order to assess the applicability of the Wilcoxon Test given the positive skew in the data, a simulation was performed. This simulation demonstrated that <b>log-transforming</b> the <b>data</b> prior to performing the Wilcoxon Test was not enough to reduce the Type I Error to reasonable levels. Thus our team developed a relative ranking using a set of multiple comparison methods: a version of the Tukey Test on variance-transformed proportions, the Bonferroni adjustment method, a Bayesian method, and the Benjamini-Hochberg rejection method. From the ranking results we identified when each ranking technique is most applicable to our data...|$|R
