201|1687|Public
25|$|MP3 was {{designed}} by the Moving Picture Experts Group (MPEG) {{as part of its}} MPEG-1, and later MPEG-2, standards. The first subgroup for audio was formed by several teams of engineers at CCETT, Matsushita, Philips, Sony, AT Labs, Thomson-Brandt, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III, was approved as a committee draft for an ISO/IEC standard in 1991, finalised in 1992, and published in 1993 as ISO/IEC 11172-3:1993. A backwards-compatible MPEG-2 Audio (MPEG-2 Part 3) extension with <b>lower</b> <b>sample</b> and bit rates was published in 1995 as ISO/IEC 13818-3:1995.|$|E
25|$|Key {{selection}} criteria of a DSO (apart from input bandwidth) are the sample memory depth and sample rate. Early DSOs in the mid- to late 1990s {{only had a}} few KB of sample memory per channel. This is adequate for basic waveform display, but does not allow detailed examination of the waveform or inspection of long data packets for example. Even entry-level (<$500) modern DSOs now have 1nbsp&MB or more of sample memory per channel, and this has become the expected minimum in any modern DSO. Often this sample memory is shared between channels, and can sometimes only be fully available at <b>lower</b> <b>sample</b> rates. At the highest sample rates, the memory may be limited to a few tens of KB.|$|E
50|$|DAWs today {{typically}} use 44.1 kHz {{or higher}} sample rates. Early digital gear used much <b>lower</b> <b>sample</b> rates to conserve memory for stored audio. A Speak & Spell from the 1970s, for instance, used a 10 kHz sample rate.|$|E
30|$|Trigger jitter - the {{algorithm}} should yield {{at least the}} same trigger jitter as the more computationally complex cross-correlation-based method in high resolution ECG, and better trigger jitter at <b>lower</b> <b>sampling</b> frequencies, to enable a high localization precision in ECG recorded with <b>lower</b> <b>sampling</b> frequencies.|$|R
5000|$|June 1999: The first {{variable}} bitrate implementation is released. Soon after this, LAME also became able to target <b>lower</b> <b>sampling</b> frequencies from MPEG-2.|$|R
40|$|Abstract- In this work, we have {{demonstrated}} with experimental results {{that the use}} of a <b>lower</b> <b>sampling</b> rate with a notch filter is feasible for motor current signature analysis in broken rotor bar detection with DTFT (Discrete Time Fourier Transform) and AR (Auto Regressive) based spectrum methods. The use of the <b>lower</b> <b>sampling</b> rate does not affect the performance of the fault detection, while requiring much less computation and low-cost in implementation, which would make it easier to implement in embedded systems for motor condition monitoring...|$|R
5000|$|<b>Lower</b> <b>sample</b> rates (such as 44.1 kHz) are {{implemented}} using a handshake protocol between the controller and codec which skips data during certain frames. (This capability {{depends on the}} codec. Alternatively, sample rate conversion could be performed in the DC97 (controller) or in the software driver.) ...|$|E
50|$|Due to the {{extremely}} low amounts of liquids 0.1 - 10 nl and reagents, the methods {{to work with}} nanoliterplates are completely different compared to {{the methods used for}} microliter plates.Nanoliter plates reduce the amount of reagents, require <b>lower</b> <b>sample</b> volumes and increase the numbers of tests that can be performed in the lab.|$|E
50|$|Both {{produce the}} MD5 hash 79054025255fb1a26e4bc422aef54eb4.The {{difference}} between the two samples is that the leading bit in each nibble has been flipped. For example, the 20th byte (offset 0x13) in the top sample, 0x87, is 10000111 in binary. The leading bit in the byte (also the leading bit in the first nibble) is flipped to make 00000111, which is 0x07, as shown in the <b>lower</b> <b>sample.</b>|$|E
50|$|A {{makeshift}} {{to achieve}} real-time requirement in multidimensional DSP applications {{is to use}} a <b>lower</b> <b>sampling</b> rate, which can efficiently reduce the number of samples to be processed at one time and thereby decrease the total processing time. However, this can lead to the aliasing problem due to the sampling theorem and poor-quality outputs. In some applications, such as military radars and medical images, we are eager to have highly precise and accurate results. In such cases, using a <b>lower</b> <b>sampling</b> rate {{to reduce the amount of}} computation in the multidimensional DSP domain is not always allowable.|$|R
3000|$|CS is an {{emerging}} theory for reconstructing sparse signals from a much <b>lower</b> <b>sampling</b> rate than Shannon/Nyquist theorem. In the deployment {{area of a}} RTI system, when a target moves into a pixel j, the pixel value x [...]...|$|R
30|$|However, data {{obtained}} by the UNI method, that surely must be considered the more accurate, indicate a worrying sulphate concentration in the colder right wall, (i.e. higher than 3 % w/w for the <b>lower</b> <b>sampling</b> point (see Additional file 1 : Table S 3).|$|R
50|$|The {{software}} generally {{goes through}} a two-step process to accomplish this. First, the audio file is played back at a <b>lower</b> <b>sample</b> rate {{than that of the}} original file. This has the same effect as playing a tape or vinyl record at slower speed - the pitch is lowered meaning the music can sound like it is in a different key. The second step is to use Digital Signal Processing (or DSP) to shift the pitch back up to the original pitch level or musical key.|$|E
50|$|MP3 was {{designed}} by the Moving Picture Experts Group (MPEG) {{as part of its}} MPEG-1 standard and later extended in the MPEG-2 standard. The first subgroup for audio was formed by several teams of engineers at CCETT, Matsushita, Philips, Sony, AT&T-Bell Labs, Thomson-Brandt, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III was approved as a committee draft of ISO/IEC standard in 1991, finalised in 1992 and published in 1993 (ISO/IEC 11172-3:1993). A backwards compatible MPEG-2 Audio (MPEG-2 Part 3) extension with <b>lower</b> <b>sample</b> and bit rates was published in 1995 (ISO/IEC 13818-3:1995). MP3 is a streaming or broadcast format (as opposed to a file format) meaning that individual frames can be lost without affecting the ability to decode successfully delivered frames. Storing an MP3 stream in a file enables time-shifted playback.|$|E
5000|$|When undersampling a {{real-world}} signal, the sampling circuit must be {{fast enough to}} capture the highest signal frequency of interest. Theoretically, each sample should be taken during an infinitesimally short interval, {{but this is not}} practically feasible. Instead, the sampling of the signal should be made in a short enough interval that it can represent the instantaneous value of the signal with the highest frequency. This means that in the FM radio example above, the sampling circuit must be able to capture a signal with a frequency of 108 MHz, not 43.2 MHz. Thus, the sampling frequency may be only a little bit greater than 43.2 MHz, but the input bandwidth of the system must be at least 108 MHz. Similarly, the accuracy of the sampling timing, or [...] aperture uncertainty of the sampler, frequently the analog-to-digital converter, must be appropriate for the frequencies being sampled 108MHz, not the <b>lower</b> <b>sample</b> rate.|$|E
40|$|The {{compressive}} sensing (CS) framework aims to {{ease the}} burden on analog-to-digital converters (ADCs) by exploiting inherent structure in natural and man-made signals. It has been demon-strated that structured signals can be acquired with just {{a small number of}} linear measurements, on the order of the signal complexity. In practice, this enables <b>lower</b> <b>sampling</b> rates that can be more easily achieved by current hardware designs. The primary bottleneck that limits ADC sam-pling rates is quantization, i. e., higher bit-depths impose <b>lower</b> <b>sampling</b> rates. Thus, the decreased sampling rates of CS ADCs accommodate the otherwise limiting quantizer of conventional ADCs. In this thesis, we consider a different approach to CS ADC by shifting towards lower quantizer bit-depths rather than <b>lower</b> <b>sampling</b> rates. We explore the extreme case where each measurement is quantized to just one bit, representing its sign. We develop a new theoretical framework to analyze this extreme case and develop new algorithms for signal reconstruction from such coarsely quantized measurements. The 1 -bit CS framework leads us to scenarios where it may be more appropriate to reduce bit-depth instead of sampling rate. We find that there exist two distinct regimes of operation that correspond to high/low signal-to-noise ratio (SNR). In the measuremen...|$|R
50|$|Generally speaking, using {{decimation}} is {{very common}} in multirate filter designs.In the second step, after using decimation, interpolation {{will be used to}} restore the sampling rate.The advantage of using decimators and interpolator is that they can reduce the computations when resulting in a <b>lower</b> <b>sampling</b> rate.|$|R
40|$|Multi-rate {{asynchronous}} sub-Nyquist sampling (MASS) {{is proposed}} for wideband spectrum sensing. Corresponding spectral recovery conditions are derived and {{the probability of}} successful recovery is given. Compared to previous approaches, MASS offers <b>lower</b> <b>sampling</b> rate, and is an attractive approach for cognitive radio networks. Â© 2012 IEEE...|$|R
5000|$|Key {{selection}} criteria of a DSO (apart from input bandwidth) are the sample memory depth and sample rate. Early DSOs in the mid- to late 1990s {{only had a}} few KB of sample memory per channel. This is adequate for basic waveform display, but does not allow detailed examination of the waveform or inspection of long data packets for example. Even entry-level (<$500) modern DSOs now have 1 MB or more of sample memory per channel, and this has become the expected minimum in any modern DSO. Often this sample memory is shared between channels, and can sometimes only be fully available at <b>lower</b> <b>sample</b> rates. At the highest sample rates, the memory may be limited to a few tens of KB.Any modern [...] "real-time" [...] sample rate DSO will have typically 5-10 times the input bandwidth in sample rate. So a 100 MHz bandwidth DSO would have 500 Ms/s - 1 Gs/s sample rate. The theoretical minimum sample rate required, using SinX/x interpolation, is 2.5 times the bandwidth.|$|E
30|$|Differences between {{upper and}} <b>lower</b> <b>sample</b> layers may be {{interpreted}} as resulting from increases in PCBs pollution, but also as the effect of lower PCBs levels during periods including the Second World War and its aftermath. Reduced PCBs efflux probably encourages a more complete biodegradation and so enhances the proportion of less-chlorinated PCBs.|$|E
40|$|The {{geomagnetic}} field intensity was significantly re-Available online at www. sciencedirect. com Earth and Planetary Science Lettersof <b>lower</b> <b>sample</b> resolution. Our results {{also indicate that}} the period of increased 10 Be production during the IB excursion lasted longer and, most likely, started earlier than the corresponding palaeomagnetic anomaly, in accordance with previous observation...|$|E
50|$|Generally speaking, using {{decimation}} is {{very common}} in multirate filter designs.In the second step, after using decimation, interpolation {{will be used to}} restore the sampling rate.The advantage of using decimators and interpolator is that they can reduce the computational power necessary for each channel at the <b>lower</b> <b>sampling</b> rate.|$|R
40|$|Digital {{weighted}} integrate-and-dump filter (WIDF) {{proposed for}} detection of weak rectangular-pulse signals corrupted by additive white Gaussian noise. Received signal first low-pass prefiltered, and samples taken at multiple of symbol frequency. Improved performance means <b>lower</b> <b>sampling</b> and processing rates used for given symbol rate, reducing cost of system...|$|R
5000|$|A Moscow guide from 1884 {{reported}} Sretenke {{and adjacent}} lanes: [...] "It always dirty, though not poor, constantly swarming area of Moscow. Trade is mainly furniture and basic necessities... Hotels and furnished rooms are extremely small, but great abundance {{of all sorts}} of restaurants and taverns middle and <b>lower</b> <b>samples</b> ... " ...|$|R
30|$|The {{profile of}} the water {{concentration}} (Fig.Â  11) {{is similar to that}} of the zoisite distribution (Fig.Â  7). IR spectra from the upper and <b>lower</b> <b>sample</b> boundaries show that water is dominantly trapped within the zoisite (Fig.Â  9). Assuming a 2.0 Â wt% H 2 O stoichiometric amount of water in a zoisite crystal structure (e.g., Hurlbut 1969), the representative zoisite area fractions of 18.3 % (maximum at 25 Â Î¼m from the upper and <b>lower</b> <b>sample</b> boundaries), 12.6 % (50 Â Î¼m), 3.5 % (75 Â Î¼m), and 0.9 % (200 Â Î¼m) shown in Fig.Â  7 can be converted into 3700, 2520, 700, and 180 Â ppm H 2 O, respectively. These converted water contents roughly correspond to the values measured via IR spectroscopy (1500 Â ppm at the maximum at the sample boundary; Fig.Â  9). However, fine zoisite grains also develop along the anorthite grain boundaries (Fig.Â  6 c). The quantity of these fine grains would also be high along the sample boundaries, and water would also be trapped in these grains.|$|E
40|$|Measuring intraday {{volatility}} {{is one of}} {{the more}} difficult tasks facing financial researchers and practitioners. There are a number of efficient methods of estimating volatility. However, the efficiency of volatility estimators decreases as the kurtosis of the return distribution increases, and intraday data are widely known to have thick tails. This paper introduces new methods of estimating historic volatility using the trading range. These estimators differ from existing range-based estimation techniques in two ways. First, these estimators are derived under the binomial model. Prior research has assumed a diffusion process. Because of minimum tick sizes, intraday data may be better modeled by the binomial model. Second, existing estimators are derived from the second sample moment of observed ranges or returns. The estimators introduced here rely on <b>lower</b> <b>sample</b> moments. Using <b>lower</b> <b>sample</b> moments mitigates the problem cause by thick tails in the return distribution, resulting in more efficient estimates. In empirical tests, these estimators provide more accurate variance forecasts than either the traditional closing price estimator or the Parkinson range-based estimator. EFFICIENT ESTIMATION OF INTRADAY VOLATILITY: A METHOD-OF...|$|E
40|$|This paper {{reports the}} {{development}} of an easy, fast and effective procedure for the verification of the ideal gas law in splitless injection systems in order to improve the response. Results of a group of pesticides were used to demonstrate the suitability of the approach. The procedure helps establish experimental parameters through theoretical aspects. The improved instrumental response allowed extraction with <b>lower</b> <b>sample</b> volumes, the minimization of time and costs and the simplification of sample preparation...|$|E
30|$|The {{dynamic of}} the outcrop Th 03 is divided in two {{opposite}} processes, the lower and upper part. The origin of flow for <b>lower</b> <b>samples</b> may be: i) across the intermediate dune, ii) a side effect from a wave running along the NE side of the dunes. The upper samples indicate clearly a channelization effect.|$|R
3000|$|... = 16.367 MHz. Decimation by 2 is {{performed}} by the digital filtering stage, such that the oversampling factor of FFT processing is 8. In a multi-GNSS SDR receiver, the ADC sampling rate would be much higher, but using a <b>lower</b> <b>sampling</b> rate in the simulations does not essentially affect our conclusions about the acquisition performance.|$|R
30|$|The {{recordings}} were 10 s long, {{with mixed}} English and Japanese utterances of both genders. The original recordings were sampled at 16 kHz; however, it was empirically {{determined that a}} downsample to 8 kHz resulted in better separation for all methods tested. This {{can be attributed to}} the reduced effects of spatial aliasing at the <b>lower</b> <b>sampling</b> frequency.|$|R
40|$|Capillary electrophoresis-nanospray/mass {{spectrometry}} (CE-nESI/MS) in electrodefree {{represents a}} very simple instrumentation for analysis of biomolecules. far, CE-nESI/MS analyses in this design were limited to 10 - 75 Î¼m ID of the channel. Although the work with narrower dimensions brings several, {{there is a great}} potential for better sensitivity, improved separation power <b>lower</b> <b>sample</b> consumption. This work is devoted to some practical aspects of in narrow bore channels and systematic evaluation of CE-nESI/MS conducted in capillaries of 25, 15 and 5 Î¼m ID...|$|E
40|$|Extreme {{miniaturization}} {{of biological}} and chemical reactions in pico- to nanoliter microdroplets is emerging as an experimental paradigm that enables more experiments {{to be carried out}} with much <b>lower</b> <b>sample</b> consumption, paving the way for high-throughput experiments. This review provides the protein scientist with an experimental framework for (a) formation of polydisperse droplets by emulsification or, alternatively, of monodisperse droplets using microfluidic devices; (b) construction of experimental rigs and microfluidic chips for this purpose; and (c) handling and analysis of droplets...|$|E
40|$|The use of {{synchrotron}} X-ray diffraction {{to study}} the crystallographic structure of nanostructure polyaniline is reported. It is shown to reveal unprecedented crystallographic information, particularly for early-stage self-assembled intermediate structures that are critical to the formation process. We discuss the new peaks, which are enabled here by specific advantages of synchrotron X-rays, including higher resolution diffraction patterns, and <b>lower</b> <b>sample</b> quantity requirements. The findings have application {{to the study of}} the structural evolution underpinning PANI nanotube formation. Web of Science 16123 - 242742273...|$|E
40|$|Northern peatlands store {{approximately}} {{one-third of}} the Worldâs soil carbon through the long-term accumulation of carbon as peat. However, when peatlands are exploited for Sphagnum moss and horticultural peat, they become degraded and large, persistent sources of atmospheric carbon dioxide. Recent advances in peatland restoration techniques have succeeded in the re-vegetation of Sphagnum moss on previously cutover surfaces. The long-term success of peatland restoration depends {{on the development of}} a sufficiently thick new peat layer that has ecohydrological and hydrophysical properties similar to natural peatlands. We determined these properties for an upper (0 â 4 cm) and lower (8 â 12 cm) peat layer in a recently restored peatland, a naturally re-vegetated cutover peatland, and a natural peatland in eastern QueÌbec. The properties of the new peat layer differed significantly between the sites, especially for the <b>lower</b> layer <b>samples.</b> <b>Lower</b> <b>samples</b> for the natural and naturally re-vegetated sites had a bulk density of 43 Å¡ 5 and 41 Å¡ 11 kg m 3, respectively, almost twice as high as the value for <b>lower</b> <b>samples</b> from the restored site (24 Å¡ 4 kg m 3). Sphagnum rubellum capitula density (C) was significantly higher (p < 0 Ã 05) for the restored peatland (28 726 # m 2) compared to the natural site (26 050 # m 2). Residual moisture content at a soil water pressure of 200 mb (r) was significantly lower (p < 0 Ã 05) for the restored site in comparison to the natural and naturally re-vegetated sites for the <b>lower</b> <b>samples.</b> This suggests that S. rubellum in a natural peatland is able to hold onto more moisture under increasing soil tension than the same species growing in a restored site likely due to its higher bul...|$|R
40|$|This paper studies {{sample design}} for process control in principal-agent {{settings}} where deterrence rather than ex post detection {{is the main}} issue. We show how the magnitude of gains from additional sampling can be calculated and traded off against sampling costs. It is shown that the optimal sample size shrinks as target rates are <b>lowered.</b> <b>sampling,</b> value of information, agency theory...|$|R
40|$|Multirate {{single-stage}} and multistage {{structures for}} highspeed recursive digital filtering are introduced. They {{can be used}} for arbitrary bandwidths and to increase the speed to an arbitrary level. Most of the filtering is performed at a <b>lower</b> <b>sampling</b> rate than that of the input and output rates which results in a low computational complexity. Design examples are included for demonstration. 1...|$|R
