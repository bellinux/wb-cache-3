1092|1252|Public
25|$|The <b>least-squares</b> <b>method</b> {{is usually}} {{credited}} to Carl Friedrich Gauss (1795), {{but it was}} first published by Adrien-Marie Legendre.|$|E
25|$|The {{following}} discussion is mostly presented {{in terms of}} linear functions {{but the use of}} least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the <b>least-squares</b> <b>method</b> may be used to fit a generalized linear model.|$|E
2500|$|The {{objective}} {{consists of}} adjusting {{the parameters of}} a model function to best fit a data set. A simple data set consists of n points (data pairs) , i = 1, ..., n, where [...] is an independent variable and [...] is a dependent variable whose value is found by observation. The model function has the form , where m adjustable parameters are held in the vector [...] The goal {{is to find the}} parameter values for the model that [...] "best" [...] fits the data. The <b>least-squares</b> <b>method</b> finds its optimum when the sum, S, of squared residuals ...|$|E
40|$|The {{uncertainty}} of observation often affects {{the validity of}} parameter estimation, {{and the effects of}} uncertainty can be reduced effectively by incorporating uncertainty into the adjustment model as an observation error parameter. An adjustment criterion is proposed under the bound constrain of uncertainty, in which the sum of squares of random error and uncertainty error should be minimized, and provided an iteration algorithm to solve the adjustment model. With simulation examples, the estimation results of uncertainty <b>least-square</b> <b>method</b> are compared with that of total <b>least-square</b> <b>method.</b> The results show that the estimation results of uncertainty <b>least-square</b> <b>method</b> are better than that of total <b>least-square</b> <b>method</b> to a certain extent and more applicable when uncertainty is greater...|$|R
40|$|Abstract: This paper {{presents}} Barycentric Coordinates using Closed-Points (BCCP) algorithm {{to minimize}} positioning error occurred by AP arrangement form. Triangular AP arrangement form and straight line AP arrangement form {{were used to}} include measurement error and conduct simulation in which the presented algorithm and <b>least-square</b> <b>method</b> was used for simulation performance analysis. As result of simulation performance analysis, there was no big difference in BCCP algorithm and <b>least-square</b> <b>method</b> in triangular AP arrangement form. However, <b>least-square</b> <b>method</b> showed an average of 48. 8 m in straight line AP arrangement form while BCCP algorithm showed 4. 28 m error...|$|R
40|$|Abstract—An {{improved}} nonlinear <b>least-square</b> <b>method</b> {{is presented}} in this paper. This method changes the traditional <b>least-square</b> <b>method’s</b> shortness of being sensitive to its initial conditions. Pattern synthesis for concentric circular arrays using nonlinear <b>least-square</b> <b>method</b> is introduced. The excitation amplitudes and phases of the array elements are optimized. This method can {{make the design of}} the feeding network much easier because the excitation amplitudes of the elements placed on the same ring are equal. The number of parameters to be optimized is reduced which leads to a faster simulation speed and makes the simulation results much more accurate. Also, the cost of designing the feeding network is reduced. The simulation results show the good agreement between the synthesized and desired radiation pattern. Also, the peak side lobe level (PSLL) of the synthesized radiation pattern is quite low. 1...|$|R
5000|$|JCGM 107. Evaluation of {{measurement}} data - Applications of the <b>least-squares</b> <b>method.</b> (planned) ...|$|E
5000|$|... 1947 An {{extension}} of the <b>least-squares</b> <b>method</b> for statistical estimation, Annals of Eugenics, 18, 257-261 ...|$|E
50|$|The <b>least-squares</b> <b>method</b> {{is usually}} {{credited}} to Carl Friedrich Gauss (1795), {{but it was}} first published by Adrien-Marie Legendre.|$|E
40|$|The aim of {{this paper}} is to present a {{calibration}} procedure applied to an inertial measurement unit (IMU) composed by a triad of accelerometers and four gyros (FOG) in a tetrad configuration. The procedure takes into account a technique based on <b>Least-square</b> <b>methods</b> and wavelet de-noising to perform the best estimate of the sensor axis misalignments. The wavelet analysis takes place in order to remove high and undesirable frequency components via multiresolution signal decomposition analysis applied to accelerometer and gyro signals. Equations for the <b>least-square</b> <b>methods</b> and wavelets analysis are presented, and the procedure is verified experimentally...|$|R
40|$|Abstract. We {{consider}} {{issues related}} to the design and analysis of <b>least-squares</b> <b>methods</b> for the incompressible Navier-Stokes equations. An abstract framework which allows to treat a large class of methods is outlined and illustrated by means of several specific examples of <b>least-squares</b> <b>methods.</b> Key words. Navier-Stokes equations, least-squares principle, finite element methods. AMS subject classifications. 76 D 05, 76 D 07, 65 F 10, 65 F 30 1. Introduction. Let Ω be a bounded, open domain in Rn, n = 2, 3. We shall be concerned with finite element <b>methods</b> of <b>least-squares</b> type for the incompressible, steady state Navier-Stokes equations −ν 4 u+ u · gradu+ grad p = f in Ω(1...|$|R
40|$|Linear <b>least-square</b> <b>method</b> {{resolves}} complex pulse height spectra, {{allowing for}} calculation of relative intensity, of statistical variance based on counting statistics of {{the correlation between}} library components, and of the goodness-of-fit chi square. Some applications are to gamma-ray, X ray, and charged-particle spectroscopy...|$|R
50|$|The {{generalized}} Gauss-Newton {{method is}} a generalization of the <b>least-squares</b> <b>method</b> originally described by Carl Friedrich Gauss and of Newton's method due to Isaac Newton {{to the case}} of constrained nonlinear least-squares problems.|$|E
5000|$|Optimization {{based on}} the <b>least-squares</b> <b>method,</b> (using Gauss-Newton methods, {{evolutionary}} algorithms, genetic algorithms, etc.); in this case, the error difference between the time histories or between the short-time-Fourier transforms of the signals is minimized.|$|E
5000|$|The <b>least-squares</b> <b>method</b> {{uses the}} test {{function}}s: [...] This method {{has the effect}} of minimising the square of the L2-norm of the residue function (that is [...] ) with respect to the degrees of freedom [...]|$|E
40|$|We {{present a}} {{non-linear}} method for solving linear inverse problems by thresholding coefficients in the wavelet domain 1. Our method {{is based on}} the wavelet-vaguelette decomposition of Donoho (1992). Numerical results for a synthetic travel-time inversion problem show that the wavelet based <b>method</b> outperforms traditional <b>least-squares</b> <b>methods</b> of solution. ...|$|R
40|$|International audienceIn {{this paper}} a new {{technique}} for nonlinear parameter identification for RF devices is presented. The parameter estimation strategy combines the Levenberg-Marquardt method (LMM) for nonlinear parameter optimization and the <b>least-square</b> <b>method</b> (LSM) for linear parameter estimation. The technique is illustrated for several components of an RF transceiver...|$|R
40|$|AbstractA nonstandard-type “least'squares” {{finite-element method}} is {{proposed}} for the solution of first-order positive symmetric systems. This method gives optimal accuracy in a norm similar to the H 1 norm. When a regularity condition holds it is optimal in L 2 as well. Otherwise, it gives errors suboptimal by only h 12 (where h is the mesh diameter). Thus, it has greater accuracy than usual finite-element, finite-difference or <b>least-squares</b> <b>methods</b> for such problems. In addition, the spectral condition number of the associated linear system is only O(h− 1) vs. O(h− 2) for the usual <b>least-squares</b> <b>methods.</b> Thus, the method promises to be an efficient, high-accuracy method for hyperbolic systems such as Maxwell's equations. It is also equally promising for mixed-type equations that have a formulation as a positive symmetric system...|$|R
5000|$|Polynomial {{regression}} models are usually fit {{using the method}} of least squares. The <b>least-squares</b> <b>method</b> minimizes the variance of the unbiased estimators of the coefficients, under {{the conditions of the}} Gauss - Markov theorem. The <b>least-squares</b> <b>method</b> was published in 1805 by Legendre and in 1809 by Gauss. The first design of an experiment for polynomial regression appeared in an 1815 paper of Gergonne. [...] In the twentieth century, polynomial regression {{played an important role in}} the development of regression analysis, with a greater emphasis on issues of design and inference. [...] More recently, the use of polynomial models has been complemented by other methods, with non-polynomial models having advantages for some classes of problems.|$|E
50|$|The {{following}} discussion is mostly presented {{in terms of}} linear functions {{but the use of}} least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the <b>least-squares</b> <b>method</b> may be used to fit a generalized linear model.|$|E
50|$|The GRBD has a real-number response. For vector responses, multivariate {{analysis}} considers similar two-way models with main effects and with interactions or errors. Without replicates, error terms are confounded with interaction, and only error is estimated. With replicates, interaction {{can be tested}} with the {{multivariate analysis}} of variance and coefficients in the linear model can be estimated without bias and with minimum variance (by using the <b>least-squares</b> <b>method).</b>|$|E
40|$|The study {{proposed}} a new weighted <b>least-square</b> <b>method</b> {{and it is}} introduced to the localization process of sensor networks. DV-HOP algorithm can be improved based on this novel method. In this paper, the matrix condition number is introduced to adjust the weight of several functional in weighted <b>least-square</b> <b>method.</b> The coefficients are decided in a nonlinear optimization problem {{and it is difficult}} to get the best solution. A second best solution can be obtained easily by our method. The method is applied to solve two-dimensional Poisson problems and the results show it is valuable and is advantageous to similar problems. Efficiency and stability of the algorithm is proved in several numerical examples. The simulation results show that the proposed improvements can greatly enhance the localization accuracy of the nodes...|$|R
40|$|We {{develop and}} analyze a <b>least-squares</b> finite element <b>method</b> for the steady state, {{incompressible}} Navier-Stokes equations, written as a first-order system involving vorticity as new dependent variable. In contrast to standard L 2 <b>least-squares</b> <b>methods</b> for this system, our approach utilizes discrete negative norms in the least-squares functional. This allows to devise efficient preconditioners for the discrete equations, {{and to establish}} optimal error estimates under relaxed regularity assumptions. c ○ 1998 John Wiley & Sons, Inc. I...|$|R
40|$|About {{a century}} ago Edgeworth {{observed}} that methods of estimation based upon minimizing sums of absolute residuals could be far superior to <b>least-squares</b> <b>methods</b> under non-Gaussian error conditions. Laplace had drawn similar conclusions a century earlier. But computation of l 1 -estimators, even for simple linear regression, remained a major impediment to application...|$|R
5000|$|The {{same idea}} arose in many fields of science. For example, the <b>least-squares</b> <b>method</b> {{can be viewed}} as a very simple form of regularization [...] A simple form of regularization applied to {{integral}} equations, generally termed Tikhonov regularization after Andrey Nikolayevich Tikhonov, is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including total variation regularization, have become popular.|$|E
5000|$|Rather {{than just}} taking dot {{products}} of the data with sine and cosine waveforms directly, Scargle modified the standard periodogram formula to first find a time delay τ such that this pair of sinusoids would be mutually orthogonal at sample times tj, and also adjusted for the potentially unequal powers of these two basis functions, to obtain a better estimate of the power at a frequency, which made his modified periodogram method exactly equivalent to Lomb's <b>least-squares</b> <b>method.</b> The time delay τ {{is defined by the}} formula ...|$|E
5000|$|The van Genuchten {{parameters}} ( [...] and [...] ) can {{be determined}} through field or laboratory testing. One of the methods is the instantaneous profile method, where water content [...] (or effective saturation [...] ) are determined {{for a series of}} suction pressure measurements [...] Due to the non-linearity of the equation, numerical techniques such as the non-linear <b>least-squares</b> <b>method</b> can be used to solve the van Genuchten parameters. The accuracy of the estimated parameters will depend {{on the quality of the}} acquired dataset ( [...] and [...] ).|$|E
40|$|Possibilities and {{restrictions}} of <b>least-square</b> <b>methods</b> for mica cell refinement are briefly described. If diffractometric raw data are precise and accurate, and if geometrical errors are properly corrected, a cell refinement (determination of ao, bo, cO, β) {{can be carried}} out rapidly, but the reliability of obtained data has to be evaluated carefull...|$|R
30|$|The TNT-NN {{method is}} able to produce {{solutions}} {{to all of the}} samples using the full spatial domain without the need to avoid any computational roadblocks. However, undesirable effects appear in the solutions. Without incorporating any regularization techniques, <b>least-squares</b> <b>methods,</b> and any method minimizing the L^ 2 -norm, will produce smooth solutions. The consequence of a smooth solution is that high frequency signals can be lost. In this sense, <b>least-squares</b> <b>methods</b> act like low-pass filters. This low-pass filtering effect is highly likely to occur at the edge of sample, where sharp discontinuities are present. The effect manifests as a nonphysical “halo” in the spatial solution that begins near discontinuities and diminishes in magnitude with sampling distance from the sample. These nonphysical sources can artificially elevate the net moment of the solution. Without additional constraints or regularization this effect will persist.|$|R
40|$|In this paper, we {{introduce}} a new technique for wideband array processing. The new algorithm {{is based on the}} total least-square approach. The total <b>least-square</b> <b>method</b> is an alternative to the <b>least-square</b> <b>method</b> and uses the fact that the errors can exist both in the focusing location matrix and the estimated location matrix at the frequency bin. To prevent the focusing loss, we use a unitary ap proach for focusing. The new method does not require singular value decomposition. The computational complexity for the new technique is significantly lower than that for the similar methods which use singular value decomposition. The simulation results show that the new algorithm has a smaller resolution signal-twnoise ratio than the coherent signal-subspace method. The bias in the estimation of the directions-of-arrival is also smaller for the new method than that for the coherent signal-subspace method. 1...|$|R
50|$|Lomb {{proposed}} {{using this}} simplification in general, except for pair-wise correlations between sine and cosine bases {{of the same}} frequency, since the correlations between pairs of sinusoids are often small, at least {{when they are not}} too closely spaced. This is essentially the traditional periodogram formulation, but now adopted for use with unevenly spaced samples. The vector x is a good estimate of an underlying spectrum, but since correlations are ignored, Ax is no longer a good approximation to the signal, and the method is no longer a <b>least-squares</b> <b>method</b> - yet it has continued to be referred to as such.|$|E
50|$|The {{following}} {{methods for}} reconstructing phylogenetic trees {{from a distance}} matrix containing missing values, i.e. incomplete matrices, are available: Triangles method by Guénoche and Leclerc (2001), Ultrametric procedure for the estimation of missing values by Landry, Lapointe and Kirsch (1996) followed by NJ, Additive procedure for the estimation of missing values by Landry, Lapointe and Kirsch (1996) followed by NJ, and the Modified Weighted <b>least-squares</b> <b>method</b> (MW*) by Makarenkov and Lapointe (2004). The MW* method assigns the weight of 1 to the existing entries, the weight of 0.5 to the estimated entries {{and the weight of}} 0 when the entry estimation was impossible. The simulations described in (Makarenkov and Lapointe 2004) showed that the MW* method clearly outperforms the Triangles, Ultrametric and Additive procedures.|$|E
5000|$|The {{objective}} {{consists of}} adjusting {{the parameters of}} a model function to best fit a data set. A simple data set consists of n points (data pairs) , i = 1, ..., n, where [...] is an independent variable and [...] is a dependent variable whose value is found by observation. The model function has the form , where m adjustable parameters are held in the vector [...] The goal {{is to find the}} parameter values for the model that [...] "best" [...] fits the data. The <b>least-squares</b> <b>method</b> finds its optimum when the sum, S, of squared residualsis a minimum. A residual is defined as the difference between the actual value of the dependent variable and the value predicted by the model. Each data point has one residual. Both the sum and the mean of the residuals are equal to zero.|$|E
40|$|Three {{different}} <b>least-squares</b> <b>methods</b> {{for processing}} time-series of satellite sensor data are presented. The first method uses local polynomial functions {{and can be}} classified as an adaptive Savitzky-Golay filter. The other two methods are more clear cut <b>least-squares</b> <b>methods,</b> where data are fit to a basis of harmonic functions and asymmetric Gaussian functions, respectively. The methods incorporate qualitative information on cloud contamination from ancillary datasets. The resulting smooth curves are used for extracting seasonal parameters related to the growing seasons. The methods are implemented in a computer program, TIMESAT, and applied to NASA/NOAA Pathfinder AVHRR Land Normalized Difference Vegetation Index data over Africa, giving spatially coherent images of seasonal parameters such as beginnings and ends of growing seasons, seasonally integrated NDVI and seasonal amplitudes. Based on general principles, the TIMESAT program can be used also for other types of satellite-derived time-series data...|$|R
40|$|This paper {{develops}} <b>least-squares</b> <b>methods</b> for {{the solution}} of linear elastic problems in both two and three dimensions. Our main approach is de ned by simply applying the L norm least-squares principle to a stress-displacement system: the constitutive and the equilibrium equations. It is shown that the homogeneous least-squares functional is elliptic and continuous in the H(div; norm. This immediately implies optimal error estimates for nite element subspaces of H(div;. It admits optimal multigrid solution methods as well if RaviartThomas nite element spaces are used to approximate the stress tensor. Our method does not degrade when the material properties approach the incompressible limit. <b>Least-squares</b> <b>methods</b> imposing boundary conditions weakly and using an inverse norm are also considered. Numerical results for a benchmark test problem of planar elasticity are included in order to illustrate the robustness of our method in the incompressible limit...|$|R
40|$|In this note, we {{consider}} the non-negative <b>least-square</b> <b>method</b> with a random matrix. This problem has connections with {{the probability that the}} origin is not in the convex hull of many random points. As related problems, suitable estimates are obtained as well on the probability that a small ball does not hit the convex hull...|$|R
