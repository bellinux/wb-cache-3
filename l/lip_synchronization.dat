85|9|Public
25|$|Operation Bayshield was {{the first}} machinima work to {{incorporate}} custom digital assets. Clan Undead created graphical textures specifically for their characters and used custom visual effects, such as manipulating character images to produce first instance of <b>lip</b> <b>synchronization</b> in machinima. Although the effect was primitive, it was not used again in machinima for another year. This <b>lip</b> <b>synchronization</b> {{is an example of}} crude digital puppetry; other examples included the shaking of character bodies when laughing and synchronized delivery of dialogue. Lowood believes that Clan Undead pre-recorded individual lines of dialogue to WAV files, and then triggered playback through a command in QuakeC. In April 1997, Clan Undead distributed the source code for its Operation Bayshield scripts over the Internet; Lowood believes that this release extended the Quake community's culture of sharing game modifications.|$|E
25|$|Operation Bayshield {{is a short}} 1997 film made by Clan Undead, a {{group of}} video game players. The work was created by using the machinima {{technique}} of recording a demonstration file of player actions in id Software's 1996 first-person shooter video game Quake, which could replay such files on demand. The group had seen the first known machinima productions, made by United Ranger Films, and {{decided to make a}} comedy film. The result, Operation Bayshield, follows a task force's attempts to thwart terrorists who have chemical explosives. Released on January 23, 1997, the work received praise from contemporary Quake movie review sites and helped to attract others, including Hugh Hancock of Strange Company and members of the ILL Clan, to machinima. It pioneered technical advances in machinima, {{such as the use of}} custom digital assets and of <b>lip</b> <b>synchronization.</b>|$|E
500|$|Diary of a Camper {{inspired}} {{many other}} [...] "Quake movies," [...] as these films were then called. A community of game modifiers (modders), artists, expert players, and film fans began to form around them. The works were distributed and reviewed on {{websites such as}} The Cineplex, Psyk's Popcorn Jungle, and the Quake Movie Library (QML). Production was supported by dedicated demo-processing software, such as Uwe Girlich's Little Movie Processing Center (LMPC) and David [...] "crt" [...] Wright's non-linear editor Keygrip; the latter became known as [...] "Adobe Premiere for Quake demo files". Among the notable films were Clan Phantasm's Devil's Covenant, the first feature-length Quake movie; Avatar and Wendigo's Blahbalicious, which the QML awarded seven Quake Movie Oscars; and Clan Undead's Operation Bayshield, which introduced simulated <b>lip</b> <b>synchronization</b> and featured customized digital assets.|$|E
50|$|Nadine’s {{platform}} is implemented {{as a classic}} Perception-Decision-Action architecture. The perception layer is composed of a Microsoft Kinect V2 and a microphone. The perception includes face recognition, gestures recognition and some understanding of social situations. In regards to decision, the platform includes emotion and memory models {{as well as social}} attention. Finally, the action layer consists of a dedicated robot controller which includes emotional expressions, <b>lips</b> <b>synchronization</b> and online gaze generation.|$|R
40|$|Needs of {{multimedia}} systems evolved {{due to the}} evolution of their architecture which is now distributed into heterogeneous contexts. A critical issue {{lies in the fact that}} they handle, process, and transmit multimedia data. This data integrates several properties which should be considered since it holds a considerable part of its semantics, for instance the <b>lips</b> <b>synchronization</b> in a video. In this paper, we focus on the definition of a model as a basic abstraction for describing and modeling media in multimedia systems by taking into account their properties. This model will be used in software architecture in order to handle data in efficient way. The provided model is an interesting solution for the integration of media into applications; we propose to consider and to handle them in a uniform way. This model is proposed with synchronization policies to ensure synchronous transport of media. Therefore, we use it in a component model that we develop for the design and deployment of distributed multimedia systems...|$|R
50|$|The creators {{also said}} that EveR-1 can mimic the human {{emotions}} of happiness, sadness, anger, and surprise more naturally than its Japanese rival, while using a hydraulic system for certain movements. EveR-1 contains a total of 35 miniature motors located throughout its upper body, which enables EveR-1 to move its head, arms, and upper body and even move its <b>lips</b> in <b>synchronization</b> with the robot's speech. Its skin is made of synthetic, pliable silicone jelly that feels similar to human skin.|$|R
2500|$|KITECH {{researched}} and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial [...] "musculature" [...] {{and capable of}} rudimentary conversation, having a vocabulary of around 400 words. She is [...] tall and weighs , matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter r for robot. EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, {{at the same time}} processing <b>lip</b> <b>synchronization</b> and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication [...] has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (440 million USD), of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.|$|E
50|$|The moose was {{the first}} {{facially}} animated talking agent with <b>lip</b> <b>synchronization</b> and it became the seed idea for future talking agents, such as Clippy the paperclip in Microsoft Windows, Bonzi Buddy, and Prody Parrot from Creative SoundBlaster.|$|E
50|$|Operation Bayshield was {{the first}} machinima work to {{incorporate}} custom digital assets. Clan Undead created graphical textures specifically for their characters and used custom visual effects, such as manipulating character images to produce first instance of <b>lip</b> <b>synchronization</b> in machinima. Although the effect was primitive, it was not used again in machinima for another year. This <b>lip</b> <b>synchronization</b> {{is an example of}} crude digital puppetry; other examples included the shaking of character bodies when laughing and synchronized delivery of dialogue. Lowood believes that Clan Undead pre-recorded individual lines of dialogue to WAV files, and then triggered playback through a command in QuakeC. In April 1997, Clan Undead distributed the source code for its Operation Bayshield scripts over the Internet; Lowood believes that this release extended the Quake community's culture of sharing game modifications.|$|E
5000|$|Lip-synching {{in music}} is [...] "moving the <b>lips</b> in <b>synchronization</b> with {{pre-recorded}} speech or song" [...] {{to give the}} appearance of a 'live' performance. It is generally considered dishonest, though some producers argue that it {{needs to be done in}} some performance contexts. Lip-synching, also called miming, can be used to make it appear as though actors have musical ability (e.g., The Partridge Family) or to misattribute vocals (e.g. Milli Vanilli), to enable them to perform live dance numbers, or to cover for illness or other deficiencies during live performance. The practice of lip-synching during live performances is frowned on by some who view it as a crutch only used by lesser talents.|$|R
30|$|This paper studies a {{new method}} for {{three-dimensional}} (3 D) facial model adaptation and its integration into a text-to-speech (TTS) system. The 3 D facial adaptation requires {{a set of}} two orthogonal views of the user′s face {{with a number of}} feature points located on both views. Based on the correspondences of the feature points′ positions, a generic face model is deformed nonrigidly treating every facial part as a separate entity. A cylindrical texture map is then built from the two image views. The generated head models are compared to corresponding models obtained by the commonly used adaptation method that utilizes 3 D radial bases functions. The generated 3 D models are integrated into a talking head system, which consists of two distinct parts: a multilingual text to speech sub-system and an MPEG- 4 compliant facial animation sub-system. Support for the Greek language has been added, while preserving <b>lip</b> and speech <b>synchronization.</b>|$|R
40|$|Lips {{animation}} {{plays an}} important role in facial animation. A realistic <b>lips</b> animation requires <b>synchronization</b> of viseme (visual phoneme) with the spoken phonemes. This research aims towards building Indonesian viseme by configuring viseme classes based on the clustering process result of visual speech images data. The research used Subspace LDA, which is a combination of Principal Components Analysis (PCA) and Linear Discriminant Analysis (LDA), as the extraction feature method. The Subspace LDA method is expected to be able to produce an optimal dimension reduction. The clustering process utilized K-Means algorithms to split data into a number of clusters. The quality of clustering result is measured by using Sum of Squared Error (SSE) and a ratio of Between-Class Variation (BCV) and Within-Class Variation (WCV). From these measurements, we found that the best quality clustering occurs at k= 9. The finding of this research is the Indonesian viseme consisting of 10 classes (9 classes of clustering result and one neutral class). For a future work, the result of this research can be used as a reference to the Indonesian viseme structure that is defined based on linguistic knowledge. Keywords—viseme; clustering; subspace LDA; feature extraction; K-Means; Sum of Squared Erro...|$|R
5000|$|Daisy {{became more}} active in college as she moved on to writing poetry and short skits for class or for {{official}} school activities and directed them. She {{became known as the}} Barbra Streisand of the school with her <b>lip</b> <b>synchronization</b> acts of Streisand's Minute Waltz, I'm Five, Jingle Bells and the like.|$|E
50|$|An {{example of}} a <b>lip</b> <b>synchronization</b> problem, also known as lip sync error {{is the case in}} which {{television}} video and audio signals are transported via different facilities (e.g., a geosynchronous satellite radio link and a landline) that have significantly different delay times. In such cases, it is necessary to delay the earlier of the two signals electronically.|$|E
50|$|Lip sync (short for <b>lip</b> <b>synchronization)</b> is a {{technical}} term for matching a speaking or singing person's lip movements with prerecorded sung or spoken vocals that listeners hear, either through the sound reinforcement {{system in a}} live performance or via television, computer or cinema speakers in other cases. The term can refer to any {{of a number of}} different techniques and processes, in the context of live performances and audiovisual recordings.|$|E
40|$|This paper {{examines}} a supersonic multi jet interaction {{problem that}} we believe {{is likely to be}} important for mixing enhancement and noise reduction in supersonic mixer-ejector nozzles. We demonstrate {{that it is possible to}} synchronize the screech instability of four rectangular jets by precisely adjusting the inter jet spacing. Our experimental data agrees with a theory that assumes that the phase-locking of adjacent jets occurs through a coupling at the jet <b>lip.</b> Although the <b>synchronization</b> does not change the frequency of the screech tone, its amplitude is augmented by 10 dB. The synchronized multi jets exhibit higher spreading than the unsynchronized jets, with the single jet spreading the least. We compare the nearfield noise of the four jets with synchronized screech to the noise of the sum of four jets operated individually. Our noise measurements reveal that the more rapid mixing of the synchronized multi jets causes the peak jet noise source to move up stream and to radiate noise at larger angles to the flow direction. Based on our results, we believe that screech synchronization is advantageous for noise reduction internal to a mixer-ejector nozzle, since the noise can now be suppressed by a shorter acoustically lined ejector...|$|R
50|$|The game {{features}} {{a number of}} proprietary technologies developed by Cryo. One such technology is called OMNI 3D which provides a smooth, panoramic 360-degree first-person view of the game environment. Unfortunately, this view tends to be less crisp looking than the movement clips that are pre-rendered in the game. All the character animations are motion captured and feature another technology called OMNI SYNC to ensure proper <b>lip</b> <b>synchronization</b> with audio speech.|$|E
50|$|FPS is a {{genre that}} {{generally}} places much {{more emphasis on}} graphical display, mainly due to the camera almost always being very close to character models. Due to increasingly detailed character models requiring animation, FPS developers assign many resources to create realistic <b>lip</b> <b>synchronization</b> with the many lines of speech used in most FPS games. Early 3D models used basic up-and-down jaw movements to simulate speech. As technology progressed, mouth movements began to closely resemble real human speech movements. Medal of Honor: Frontline dedicated a development team to lip sync alone, producing the most accurate <b>lip</b> <b>synchronization</b> for games at that time. Since then, games like Medal of Honor: Pacific Assault and Half-Life 2 have made use of coding that dynamically simulates mouth movements to produce sounds {{as if they were}} spoken by a live person, resulting in astoundingly lifelike characters. Gamers who create their own videos using character models with no lip movements, such as the helmeted Master Chief from Halo, improvise lip movements by moving the characters' arms, bodies and making a bobbing movement with the head (see Red vs. Blue).|$|E
50|$|Kismet speaks a proto-language with {{a variety}} of phonemes, similar to baby's babbling. It uses the DECtalk voice synthesizer, and changes pitch, timing, articulation, etc. to express various emotions. Intonation is used to vary between {{question}} and statement-like utterances. <b>Lip</b> <b>synchronization</b> was important for realism, and the developers used a strategy from animation: simplicity is the secret to successful lip animation. Thus, they did not try to imitate lip motions perfectly, but instead create a visual short hand that passes unchallenged by the viewer.|$|E
5000|$|Diary of a Camper {{inspired}} {{many other}} [...] "Quake movies," [...] as these films were then called. A community of game modifiers (modders), artists, expert players, and film fans began to form around them. The works were distributed and reviewed on {{websites such as}} The Cineplex, Psyk's Popcorn Jungle, and the Quake Movie Library (QML). Production was supported by dedicated demo-processing software, such as Uwe Girlich's Little Movie Processing Center (LMPC) and David [...] "crt" [...] Wright's non-linear editor Keygrip; the latter became known as [...] "Adobe Premiere for Quake demo files". Among the notable films were Clan Phantasm's Devil's Covenant, the first feature-length Quake movie; Avatar and Wendigo's Blahbalicious, which the QML awarded seven Quake Movie Oscars; and Clan Undead's Operation Bayshield, which introduced simulated <b>lip</b> <b>synchronization</b> and featured customized digital assets.|$|E
50|$|For {{the first}} time in the series, the game was based on console {{development}} and released as a multiplatform title. Darkworks handled the development of the PlayStation and Dreamcast versions, the latter being released one month after the PlayStation version and featuring major graphic improvements instead of being a direct port. Spiral House ported the Dreamcast version to Windows and it was released at the same time of the Dreamcast version, but the Windows version suffers from sound conversion issues which turn the overall soundtrack of the game into noticeable worse quality renditions. Spiral House also ported the Dreamcast version to the PlayStation 2 and released it several months after, with the added improvement of <b>lip</b> <b>synchronization</b> for the polygon models during cutscenes. The game uses the Nocturne Engine, which was originally developed for Nocturne, a 1999 survival horror game.|$|E
50|$|Operation Bayshield {{is a short}} 1997 film made by Clan Undead, a {{group of}} video game players. The work was created by using the machinima {{technique}} of recording a demonstration file of player actions in id Software's 1996 first-person shooter video game Quake, which could replay such files on demand. The group had seen the first known machinima productions, made by United Ranger Films, and {{decided to make a}} comedy film. The result, Operation Bayshield, follows a task force's attempts to thwart terrorists who have chemical explosives. Released on January 23, 1997, the work received praise from contemporary Quake movie review sites and helped to attract others, including Hugh Hancock of Strange Company and members of the ILL Clan, to machinima. It pioneered technical advances in machinima, {{such as the use of}} custom digital assets and of <b>lip</b> <b>synchronization.</b>|$|E
50|$|In 2010, Image Metrics {{launched}} the facial animation technology platform Faceware. Faceware focused on increasing creative control, efficiency and production speed for animators. The software could {{be integrated into}} any pipeline or used with any game engine. Image Metrics provided training to learn the Faceware platform. The first studio to sign on as a Faceware customer was Bungie, which incorporated the software into its in-house production. Image Metrics acquired FacePro in 2010, a company that provided automated <b>lip</b> <b>synchronization</b> which could be altered for accurate results, and Image Metrics integrated the acquired technology into its facial animation software. Also in 2010, Image Metrics bought Character-FX, a character animation company. Character-FX produced tools for use in Autodesk’s Maya and 3DS Max which aide {{in the creation of}} character facial rigs using an automated weighting transfer system that rapidly shifts facial features on a character to create lifelike movement.|$|E
50|$|MMTel {{allows a}} single SIP session to control {{virtually}} all MMTel supplementary services and MMTel media. All available media components {{can easily be}} accessed or activated within the session. Employing a single session for all media parts means that no additional sessions need {{to be set up}} to activate video, to add new users, or to start transferring a file. Even though it is possible to manage single-session user scenarios with several sessions - for instance, using a circuit-switched voice service that is complemented with a packet-switched video session, a messaging service or both - there are some concrete benefits to MMTel’s single-session approach. A single SIP session in an all-IP environment benefits conferencing; in particular, <b>lip</b> <b>synchronization,</b> which is quite complex when the voice part is carried over a circuit-switched service and the video part is carried over a packet-switched service. In fixed-mobile convergence scenarios, the single-session approach enables all media parts of the multimedia communication solution to interoperate.|$|E
50|$|Unlike RPGs, {{strategy}} {{video games}} make {{extensive use of}} sound files to create an immersive battle environment. Most games simply played a recorded audio track on cue with some games providing inanimate portraits to accompany the respective voice. StarCraft used full motion video character portraits with several generic speaking animations that did not synchronize with the lines spoken in the game. The game did, however, make extensive use of recorded speech to convey the game's plot, with the speaking animations providing {{a good idea of}} the flow of the conversation. Warcraft III used fully rendered 3D models to animate speech with generic mouth movements, both as character portraits as well as the in-game units. Like the FMV portraits, the 3D models did not synchronize with actual spoken text, while in-game models tended to simulate speech by moving their heads and arms rather than using actual <b>lip</b> <b>synchronization.</b> Similarly, the game Codename Panzers uses camera angles and hand movements to simulate speech, as the characters have no actual mouth movement. However, StarCraft II used fully synced unit portraits and cinematic sequences.|$|E
5000|$|Koblun {{began his}} music career as the bassist for the Squires, a teen band formed by Young {{in the early}} 60's out of Earl Grey Junior High. After the band broke up, Koblun found work playing bass for various folk musicians. When Stephen Stills and Richie Furay were seeking to start a rock band in Los Angeles, {{a few months after}} Koblun had taken a trip to New York City in 1965, they could not find Young, but did succeed in {{locating}} Koblun, whom they convinced to come to California to join the group. However, he stayed for only a few days before deciding to return to Canada where he joined up with 3's a Crowd. In January 1967, a replacement was needed for Bruce Palmer, who was fighting possible deportation. Koblun only played with them for about a month before the band decided his personality was undesirable and his bass playing not as good as they anticipated. During that time he did appear {{in one of the few}} film clips of the band, doing <b>lip</b> <b>synchronization</b> to [...] "Sit Down, I Think I Love You" [...] on the television show Where The Action Is. Koblun did not record with the band, but Young's epic [...] "Broken Arrow" [...] is dedicated to Koblun in the sleeve notes of Buffalo Springfield Again.|$|E
5000|$|KITECH {{researched}} and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial [...] "musculature" [...] {{and capable of}} rudimentary conversation, having a vocabulary of around 400 words. She is 160 cm tall and weighs 50 kg, matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter r for robot. EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, {{at the same time}} processing <b>lip</b> <b>synchronization</b> and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (440 million USD), of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.|$|E
40|$|Abstract—This paper {{describes}} a morphing-based audio driven facial animation system. Based on an incoming audio stream, a face image is animated with full <b>lip</b> <b>synchronization</b> and synthesized expressions. A novel scheme {{to implement a}} language independent system for audio-driven facial animation given a speech recognition system for just one language, in our case, English, is presented. The method presented here {{can also be used}} for text to audio-visual speech synthesis. Visemes in new expressions are synthesized to be able to generate animations with different facial expressions. An animation sequence using optical flow between visemes is constructed, given an incoming audio stream and still pictures of a face representing different visemes. The presented techniques give improved <b>lip</b> <b>synchronization</b> and naturalness to the animated video. Index Terms—Audio to video mapping, facial animation, facial expression synthesis, <b>lip</b> <b>synchronization,</b> translingual visual speech synthesis. I...|$|E
40|$|The timed automata {{formalism}} is {{an important}} model for specifying and analysing real-time systems. Robustness is the correctness of the model {{in the presence of}} small drifts on clocks or imprecision in testing guards. A symbolic algorithm for the analysis of the robustness of timed automata has been implemented. In this paper we re-analyse an industrial case <b>lip</b> <b>synchronization</b> protocol using the new robust reachability algorithm. This <b>lip</b> <b>synchronization</b> protocol is an interesting case be-cause timing aspect are crucial for the correctness of the protocol. Several versions of the model are considered, with an ideal video stream, with anchored jitter, and with non-anchored jitter. ...|$|E
40|$|Many current {{distributed}} multimedia {{applications such}} as Video on Demand (VOD) are designed, implemented and used on top of general purpose OS and network platforms (e. g., UNIX/Internet)). Within this `best-effort' environment to achieve user acceptance for <b>lip</b> <b>synchronization</b> of the audio-visual information, applications must use adaptive synchronization protocols and balance non-deterministic behavior of the underlying OS/network subsystem. We have designed, implemented and validated an adaptive synchronization scheme integrating adaptive services and synchronization protocols to provide <b>lip</b> <b>synchronization</b> within a VOD system. Our results show most of the synchronization skew values (85 %) in the user desirable range of (- 80, 80) ms and the remaining skew values in the user acceptable range of (- 160, 160) ms...|$|E
30|$|The text-driven or speech-driven talking {{head has}} an {{essential}} problem, <b>lip</b> <b>synchronization.</b> The mouth {{movement of the}} talking head has to match the corresponding audio utterance. <b>Lip</b> <b>synchronization</b> is rather complicated due to the coarticulation phenomena [24] which indicate that a particular mouth shape depends not only on its own phoneme but also on its preceding and succeeding phonemes. Generally, the 3 D model-based approaches use a coarticulation model with an articulation mapping between a phoneme and the model's action parameters. Image-based approaches implicitly {{make use of the}} coarticulation of the recorded speaker when selecting an appropriate sequence of mouth images. Comparing to 3 D model-based animations, each frame in the image-based animations looks realistic. However, selecting mouth images, which provides a smooth movement, remains a challenge.|$|E
30|$|Thus, {{there exists}} a {{significant}} need to improve coarticulatory model for <b>lip</b> <b>synchronization.</b> The image-based approach selects appropriate mouth images matching the desired values from a large database, {{in order to maintain}} the mechanism of mouth movement during speaking. Similar to the unit selection synthesis in the text-to-speech synthesizer, the resulted talking heads could achieve the most naturalness.|$|E
40|$|The {{authors are}} {{producing}} two-dimensional animated human faces with lip motion synchronized {{to a given}} speech sound file. The process is called <b>lip</b> <b>synchronization.</b> We assume the only input is the sound file; no text is used to disambiguate mouth shapes. We restrict our discussion to voiced input. (Figures and the Table are {{at the end of}} the text. ...|$|E
40|$|Natural looking lip animation, {{synchronized}} with incoming speech, {{is essential}} for realistic character animation. In this work, we evaluate the performance of phone and viseme based acoustic units, with and without context information, for generating realistic <b>lip</b> <b>synchronization</b> using HMM based recognition systems. We conclude via objective evaluations that utilization of viseme based units with context information outperforms the other methods...|$|E
40|$|<b>Lip</b> <b>synchronization</b> is {{the process}} of {{generating}} natural lip movements from a speech signal. In this work we address the lip-sync problem using an automatic phone recognizer that generates a phone lattice carrying posterior probabilities. The acoustic feature vector contains the posterior probabilities of all the phones over a time window centered at the current time point. Hence this representation characterizes the phone recognition output including the confusion patterns caused by its limited accuracy. A 3 D face model with varying texture is computed by analyzing a video recording of the speaker using a 3 D morphable model. Training a neural network using 30 000 data vectors from an audiovisual recording in Dutch resulted in a very good simulation of the face on independent data sets of the same or of a different speaker. Index Terms: <b>lip</b> <b>synchronization,</b> speech recognition, phone lattice, 3 D morphable models, principal component analysis, audio visual speech. 1...|$|E
40|$|Linear Predictive {{analysis}} {{is a widely}} used technique for speech analysis and encoding. In this paper, we discuss the issues involved in its application to phoneme extraction and <b>lip</b> <b>synchronization.</b> The LP analysis results {{in a set of}} reflection coefficients that are closely related to the vocal tract shape. Since the vocal tract shape can be correlated with the phoneme being spoken, LP analysis can be directly applied to phoneme extraction. We use neural networks to train and classify the reflection coefficients into a set of vowels. In addition, average energy is used to take care of vowel-vowel and vowel-consonant transitions, whereas the zero crossing information is used to detect the presence of fricatives. We directly apply the extracted phoneme information to our synthetic 3 D face model. The proposed method is fast, easy to implement, and adequate for real time speech animation. As the method does not rely on language structure or speech recognition, it is language independent. Moreover, the method is speaker independent. It can be applied to <b>lip</b> <b>synchronization</b> for entertainment applications and avatar animation in virtual environments...|$|E
