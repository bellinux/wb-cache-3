18|10000|Public
2500|$|Wh-movement {{typically}} {{results in}} a discontinuity: the [...] "moved" [...] constituent ends up in a position that is separated from its canonical position by material that syntactically dominates the canonical position, which means {{there seems to be}} a discontinuous constituent and a <b>long</b> <b>distance</b> <b>dependency</b> present. Such discontinuities challenge any theory of syntax, and any theory of syntax is going to have a component that can address these discontinuities. In this regard, theories of syntax tend to explain discontinuities in one of two ways, either via movement or via feature passing.|$|E
5000|$|The two constituency-based trees show a flat VP {{that allows}} n-ary {{branching}} (as {{opposed to just}} binary branching). The two dependency-based trees show the same VP. Regardless of whether one chooses the constituency- or the dependency-based analysis, the important thing about these examples is the relative flatness of the structure. This flatness results {{in a situation where}} shifting does not necessitate a discontinuity (i.e. no <b>long</b> <b>distance</b> <b>dependency),</b> for there can be no crossing lines in the trees. The following trees further illustrate the point: ...|$|E
5000|$|Wh-movement {{typically}} {{results in}} a discontinuity: the [...] "moved" [...] constituent ends up in a position that is separated from its canonical position by material that syntactically dominates the canonical position, which means {{there seems to be}} a discontinuous constituent and a <b>long</b> <b>distance</b> <b>dependency</b> present. Such discontinuities challenge any theory of syntax, and any theory of syntax is going to have a component that can address these discontinuities. In this regard, theories of syntax tend to explain discontinuities in one of two ways, either via movement or via feature passing.|$|E
40|$|Discontinuities come in {{different}} flavours. In minimalistic and previous {{editions of the}} generative enterprise, <b>long</b> <b>distance</b> <b>dependencies</b> and discontinutities resulting from verb clustering are analyzed in quite different terms. This paper analyzes wh-structures and crossing dependencies as emanations of a single modalized regime of composition in a combinatory categorial setting...|$|R
40|$|We extend first-order semi-Markov {{conditional}} random fields (semi-CRFs) {{to include}} higherorder semi-Markov features, and present efficient inference and learning algorithms, {{under the assumption}} that the higher-order semi-Markov features are sparse. We empirically demonstrate that high-order semi-CRFs outperform high-order CRFs and first-order semi-CRFs on three sequence labeling tasks with <b>long</b> <b>distance</b> <b>dependencies.</b> 1...|$|R
40|$|In this paper, {{we define}} Dependency Structure Grammars (DSG), which are {{rewriting}} rule grammars generating sentences {{together with their}} dependency structures, are more expressive than CF-grammars and non-equivalent to mildly context-sensitive grammars. We show that DSG are weakly equivalent to Categorial Dependency Grammars (CDG) recently introduced. In particular, these dependency grammars naturally express <b>long</b> <b>distance</b> <b>dependencies</b> and enjoy good mathematical properties...|$|R
50|$|In linguistics, a {{discontinuity}} {{occurs when}} a given word or phrase is separated from another word or phrase that it modifies {{in such a manner}} that a direct connection cannot be established between the two without incurring crossing lines in the tree structure. The terminology that is employed to denote discontinuities varies depending on the theory of syntax at hand. The terms discontinuous constituent, displacement, <b>long</b> <b>distance</b> <b>dependency,</b> unbounded dependency, and projectivity violation are largely synonymous with the term discontinuity. There are various types of discontinuities, the most prominent and widely studied of these being topicalization, wh-fronting, scrambling, and extraposition.|$|E
40|$|Linear-chain Conditional Random Fields (CRF) {{has been}} applied to perform the Named Entity Recognition (NER) task in many {{biomedical}} text mining and information extraction systems. However, the linear-chain CRF cannot capture <b>long</b> <b>distance</b> <b>dependency,</b> which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such <b>long</b> <b>distance</b> <b>dependency</b> by defining two principles of constructing skipedges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance. ...|$|E
40|$|Recently, {{neural network}} models for natural {{language}} processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this work, we propose a feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging task. Specifically, to simulate the feature templates of traditional discrete feature based models, we use different filters to model the complex compositional features with convolutional and pooling layer, and then utilize <b>long</b> <b>distance</b> <b>dependency</b> information with recurrent layer. Experimental results on five different datasets show the effectiveness of our proposed model...|$|E
40|$|In {{this paper}} we give a formal {{description}} of the parsing model that underlies the treatment of <b>Long</b> <b>Distance</b> <b>Dependencies,</b> Topic and Focus, Ellipsis and Quantification in, amongst others, the papers [1996],[1997], [1999 a],[1998],[1999 b]. In this model, a natural language string consists of a sequence of `instructions packages' to construct some term in a formal representation language, the logical form of the string in question. Parsing, then, {{is the process of}} executing these packages in a left to right order. Contents 1 Terms as Decorated Trees 2 2 Goal-Directedness 7 3 Actions 10 4 The Parsing Process 12 5 Conclusion 14 1 Introduction In this paper we will give a formal {{description of the}} parsing model that underlies the treatment of <b>Long</b> <b>Distance</b> <b>Dependencies,</b> Topic and Focus, Ellipsis and Quantification in, amongst others, the papers [1996],[1997],[1999 a],[1998],[1999 b]. Although the intuition behind the model is quite natural, nevertheless, it seems not to have been explored [...] ...|$|R
40|$|In this paper, {{we present}} an {{analysis}} of noun phrases with elided nouns that dispenses with the positing of empty categories and preserves the NP structure assumed for NPs with overt nouns, modulo {{the absence of the}} head noun. On a par with traceless analyses of <b>long</b> <b>distance</b> <b>dependencies,</b> this is proposed as a further step towards a more lean theory of grammar, without phonetically null items. ...|$|R
5000|$|The {{terminology}} that constituency grammars (= phrase structure grammars) {{employ to}} identify and define discontinuities is different. The projectivity principle certainly exists, although it is acknowledged in terms of discontinuous constituents, <b>long</b> <b>distance</b> <b>dependencies,</b> and/or unbounded dependencies. The constituency-based versions of the six hierarchies from the previous section are rendered as follows. The solid shapes represent phrasal categories, and the empty shapes lexical categories: ...|$|R
40|$|In Dutch, adpositions can be stranded, {{typically}} {{if their}} complement is an R-pronoun. The complement usually {{appears in the}} left part of the Mittelfeld or in the Vorfeld. In HPSG this is canonically modeled in terms of extraction, making use of nonlocal devices such as SLASH and BIND. This paper argues that the extraction analysis is indeed appropriate for {{cases in which the}} complement is realised in the Vorfeld, but proposes an alternative for the cases in which the complement is realised in the Mittelfeld. The new treatment is based on argument inheritance, as complement raising in the Mittelfeld involves a middle distance dependency rather than a <b>long</b> <b>distance</b> <b>dependency.</b> status: publishe...|$|E
40|$|Measure {{words in}} Chinese {{are used to}} {{indicate}} the count of nouns. Conventional statistical machine translation (SMT) systems do not perform well on measure word generation due to data sparseness and the potential <b>long</b> <b>distance</b> <b>dependency</b> between measure words and their corresponding head words. In this paper, we propose a statistical model to generate appropriate measure words of nouns for an English-to-Chinese SMT system. We model the probability of measure word generation by utilizing lexical and syntactic knowledge from both source and target sentences. Our model works as a post-processing procedure over output of statistical machine translation systems, and can work with any SMT system. Experimental results show our method can achieve high precision and recall in measure word generation. ...|$|E
40|$|Most of the widely-used {{automatic}} evaluation metrics consider only the local {{fragments of the}} references and translations, and they ignore the evaluation on the syntax level. Current syntax-based evaluation metrics try to introduce syntax information but suffer from the poor pars-ing results of the noisy machine translations. To alleviate this problem, we propose a novel dependency-based evaluation metric which only employs the dependency information of the ref-erences. We use two kinds of reference dependency structures: headword chain to capture the <b>long</b> <b>distance</b> <b>dependency</b> information, and fixed and floating structures to capture the local con-tinuous ngram. Experiment results show that our metric achieves higher correlations with human judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance which is better than METEOR and SEMPOS on system level, and is comparable with METEOR on sentence level on WMT 2012 and WMT 2013. ...|$|E
40|$|Copyright c○ 1997 by The Association for Computational Linguistics The paper {{presents}} a language model that develops syntactic structure and {{uses it to}} extract meaningful information from the word history, thus enabling the use of <b>long</b> <b>distance</b> <b>dependencies.</b> The model assigns probability to every joint sequence of words–binary-parse-structure with headword annotation. The model, its probabilistic parametrization, {{and a set of}} experiments meant to evaluate its predictive power are presented. ...|$|R
40|$|The paper {{presents}} a language model that develops syntactic structure and {{uses it to}} extract meaningful information from the word history, thus enabling the use of <b>long</b> <b>distance</b> <b>dependencies.</b> The model assigns probability to every joint sequence of words - binary-parse-structure with headword annotation. The model, its probabilistic parametrization, {{and a set of}} experiments meant to evaluate its predictive power are presented. Comment: changed ACM-class membership, Proceedings of ACL-EACL' 97, Student Section, Madrid, Spai...|$|R
40|$|Most {{approaches}} to highlight classification {{in the sports}} domain exploit only limited temporal information. This paper presents a method, called temporal feature induction, which automatically mines complex temporal information from raw video for use in highlight classification. The method exploits techniques from temporal data mining to discover a codebook of temporal patterns that encode <b>long</b> <b>distance</b> <b>dependencies</b> and duration information. Preliminary experiments show that using such induced temporal features significantly improves performance of a baseball highlight classification system...|$|R
40|$|Although the Elman {{network is}} so {{powerful}} that it can deal {{with a variety of}} language processings, there exist some short comings about its ability. For example, the original Elman net cannot always deal with a <b>long</b> <b>distance</b> <b>dependency</b> ap-propriately, which is a number agreement between nouns and verbs with many relative pronouns in a sentence. This limi-tation might cause from the constraints of its structure of the context and the hidden layer, which can preserve only one time previous state of the network. Here, we propose an extension of the Elman network. The extended Elman network can pre-serve the n-th generations of inner states. When the model processed the corpus consisted of many relative pronouns with multi-center embeddings structure, it could deal with the long distance number agreement adequately. This model can be re-garded as a natural extension of the Elman network {{in order to deal with}} complex structures of language...|$|E
40|$|This paper {{presents}} a language model as {{an improvement over}} the stochastic language model for developing a syntactic structure based on word dependencies in local and non local domain. The model copes with the issues of limited amount of training material and the exploitation of the linguistic constraints of the language. The proposed model is a dynamic probabilistic model which uses word dependencies based on their part of speech tags along with the tri-gram Model but also {{takes care of the}} influence of the word which are very far from the word being considered in a text and stores the word history in a dynamic cache for information mining using <b>long</b> <b>distance</b> <b>dependency.</b> The model based on second order Hidden Markov Model has been used and an improvement of 2 % has been observed in the word error rate and 4 % reduction in the perplexity when compared to the normal tri-gram model...|$|E
40|$|This paper {{shows how}} finite approximations of <b>long</b> <b>distance</b> <b>dependency</b> (LDD) {{resolution}} {{can be obtained}} automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80. 97 % f-score for fstructures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 105 1 and 80. 24 % against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004). ...|$|E
40|$|We examine {{some natural}} {{language}} uses {{of a new}} type of logic grammars-Assumption Grammars- particularly suitable for hypothetical reasoning. They are based on intuitionistic and linear implications scoped over the current continuation, which allow us to follow given branches of the computation under hypotheses that disappear when and if backtracking takes place. We show how Assumption Grammars can simplify the treatment of some crucial computational linguistics problems, e. g. <b>long</b> <b>distance</b> <b>dependencies,</b> while simultaneously facilitating more readable grammars...|$|R
40|$|This paper {{describes}} {{the development of}} QuestionBank, a corpus of 4000 parseannotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing. We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets. We introduce a new method for recovering empty nodes and their antecedents (capturing <b>long</b> <b>distance</b> <b>dependencies)</b> from parser output in CFG trees using LFG f-structure reentrancies. Our main findings are (i) using QuestionBank training data improves parser performance to 89. 75 % labelled bracketing f-score, an increase of almost 11 % over the baseline; (ii) back-testing experiments on nonquestion data (Penn-II WSJ Section 23) shows that the retrained parser does not suffer a performance drop on non-question material; (iii) ablation experiments show {{that the size of}} training material provided by QuestionBank is sufficient to achieve optimal results; (iv) our method for recovering empty nodes captures <b>long</b> <b>distance</b> <b>dependencies</b> in questions from the ATIS corpus with high precision (96. 82 %) and low recall (39. 38 %). In summary, QuestionBank provides a useful new resource in parser-based QA research. ...|$|R
40|$|The paper {{presents}} a language model that develops syntactic structure and {{uses it to}} extract meaningful information from the word history, thus enabling the use of <b>long</b> <b>distance</b> <b>dependencies.</b> The model assigns probability to every joint sequence of words–binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, {{and a set of}} experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. ...|$|R
40|$|Recursive neural {{networks}} (RNN) and their recently proposed extension recursive long {{short term memory}} networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly {{make use of the}} hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and <b>long</b> <b>distance</b> <b>dependency</b> problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary...|$|E
40|$|Island {{constraints}} and overgeneralization in language acquisition Abstract: Ambridge and Goldberg (2008) found that <b>long</b> <b>distance</b> <b>dependency</b> (LDD) questions (e. g.,Who did shemumble that she saw?) {{do not seem}} to be formed by analogy with similar, more frequent sentences of the same type (e. g., What do you think X?;What did he say X?), but, rather, that such questions are acceptable to the extent that the main verb backgrounds the complement clause (e. g., say> mumble). Kalyan (2012) argued that this finding is compatible with a similarity-based account, provided that similarity between the verb and say/think is defined as similarity in the extent to which the verb backgrounds the complement clause. In the present article, I argue that Kalyan (2012) is correct, and that this phenom-enon can be seen as an instance of a broader phenomenonwhereby the fit between the properties of a particular item (e. g., a verb) and those of a particular construc-tion slot (e. g., the VERB slot in the LDD question construction) is the primary determinant of the degree of (un) grammaticality of a possible generalization...|$|E
40|$|It {{is natural}} {{to think of}} {{sentences}} in natural language as exhibiting longdistance dependencies. A surprising fact, established over decades of linguistic investigation, is that such dependencies are naturally treated as falling into one of two groups. ‘A ’ dependencies (typified by raising, passivization, etc) are those which allow for re-binding, which disallow reconstruction, and which do not license parasitic gaps. ‘A-bar ’ dependencies (typified by whmovement, relativization, etc) disallow re-binding, allow reconstruction, and do license parasitic gaps. Furthermore, when an expression enters in to both A and A-bar dependencies, all of its A dependencies must ‘precede ’ its A-bar dependencies [2]. This relational property of dependencies {{is known as the}} ban on improper movement. In the government and binding (GB) tradition, all of these properties of these two dependency types must be independently stipulated—none follow from any of the others. Here I will present a simple and constrained formal system in the GB tradition with two kinds of <b>long</b> <b>distance</b> <b>dependency</b> forming operations, the interaction of which gives ris...|$|E
40|$|International audienceThis paper {{proposes a}} new {{analysis}} of <b>long</b> <b>distance</b> <b>dependencies</b> phenomena. The data collected through corpora {{indicate that the}} bridge verb and its dependents follow a very specific template limited to a verb with a modal interpretation and clitic pronouns. We propose an analysis based on complex predicates formation, in opposition to current analyses based on clause embedding. Therefore, {{there is no longer}} a need for long dis-tance processes: the movement of the relative or interrogative pronoun remains local. All the constraints on this pattern will be formulated in constructional terms...|$|R
40|$|Abstract. Blache [1] {{introduced}} Property Grammar as a formalism where linguistic {{information is}} represented in terms of non hierarchical constraints. This feature gives it an adequate expressive power to handle complex linguistic phenomena, such as <b>long</b> <b>distance</b> <b>dependencies,</b> and also agrammatical sentences [2]. Recently, Duchier et al. [3] proposed a model-theoretic semantics for property grammar. The present paper follows up on that work and ex-plains how to turn such a formalization into a constraint optimization problem, solvable using constraint programming techniques. This natu-rally leads to an implementation of a fully constraint-based parser for property grammars. ...|$|R
40|$|Copyright c© 1998 by The Association for Computational Linguistics The paper {{presents}} a language model that devel-ops syntactic structure and {{uses it to}} extract mean-ingful information from the word history, thus en-abling the use of <b>long</b> <b>distance</b> <b>dependencies.</b> The model assigns probability to every joint sequence of words–binary-parse-structure with headword an-notation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, {{and a set of}} experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. ...|$|R
40|$|Recognizing time {{expression}} {{is useful in}} many natural language processing tasks, {{which can be used}} to temporal reasoning and anchoring events on the time line. In this paper, a heuristic error-driven learning framework is proposed for recognizing Chinese time expression, which integrates the heuristic search strategy A algorithm into error-driven learning. The heuristic function is designed and its monotonicity is theoretically proved, so that the correctness of A algorithm is guaranteed. Our method begins with time trigger word, uses Chinese dependency parsing to identify the extents of time expressions, availably resolves the problem of <b>long</b> <b>distance</b> <b>dependency,</b> and greatly improves the system performance; Subsequently, comparing incorrectly recognized time expressions with the standard ones and learning some rules, then we use the error-driven learning based on the A algorithm to heuristically filter the rules, which not only decreases the time complexity of learning rules, but also distinguishes the validity of each rule and the compatibility among rules. We evaluated this new method on the Chinese corpus of ACE 2005 and got 6 % increase of system performance. Finally, F = 77. 96 %, F = 77. 92 % was obtained on the closed and the open test set, respectively...|$|E
40|$|Previous {{psycholinguistics}} {{studies have}} shown that when forming a <b>long</b> <b>distance</b> <b>dependency</b> in online processing, the parser sometimes accepts a sentence even though the required grammatical constraints are only partially met. A mechanistic account of how such errors arise sheds light on both the underlying linguistic representations involved and the processing mechanisms that put such representations together. In the current study, we contrast the NPI (negative polarity items) interference effect, as shown by the acceptance of an ungrammatical sentence like The bills that democratic senators have voted for will ever become law, with the well-known phenomenon of agreement attraction (The key to the cabinets are…). On the surface, these two types of errors look alike and thereby can be explained as being driven by the same source: similarity based memory interference. However, we argue that the linguistic representations involved in NPI licensing are substantially different from those of subject-verb agreement, and therefore the interference effects in each domain potentially arise from distinct sources. In particular, we show that NPI interference at least partially arises from pragmatic inferences. In a self-paced reading study with an acceptability judgment task, we showed NPI interference was modulated by participants’ general pragmatic communicative skills, as quantified by the Autism-Spectrum Quotient (Baron-Cohen 2001), especially in offline tasks. Participants with more autistic traits were actually less prone to the NPI interference effect than those with fewer autistic traits. This result contrasted with agreement attraction conditions, which were not influenced by individual pragmatic skill differences. We also show that different NPI licensors have distinct interference profiles. We discuss two kinds of interference effects for NPI licensing: memory-retrieval based and pragmatically triggered...|$|E
40|$|Spatial {{information}} {{is important for}} remote sensing image classification. How to extract spatial information and incorporate it into classification procedure is a challenging issue {{in order to improve}} the traditionally spectral based classification. New frameworks and models are developed in this thesis. A new method based on the Conditional Random Fields (CRF) model is constructed to incorporate both spatial and spectral neighbouring information into the classification simultaneously. We develop a simplified version to cope with the complex training procedure for CRF. The new model integrates the boundary constraint into the classification. Comparing to traditional Markov Random Fields (MRF) model, this method incorporates discriminative model instead of generative model and takes into account of the observed data dependency into the classification to improve the result. A new framework is also developed based on the super pixels which are spatially connected homogenous regions. We develop an irregular graphical model based on the super pixels to incorporate <b>long</b> <b>distance</b> <b>dependency</b> into classification. Computation complexity is reduced significantly because of the reduction of the nodes and the edges in the construction of this graphical model. New methods are built to calculate the node and edge potentials with pixel level samples. A new algorithm for boundary {{information is}} developed to avoid over-smoothing. The third part of this thesis is the investigation of incorporating the spatial features into hyperspectral remote sensing image classification. These generated extra features make the high dimensional problem even worse for the hyperspectral data. A Sparse Multiclass Logistic Regression model is applied in this thesis to combine the spectral features, spectral interacted features and textural features into one procedure. This model realizes the feature selection and classification simultaneously. It can identify an effective subset of features and cope with a small size of training sample problem, even for the situation where the number of training samples is smaller than the dimensionality of the features. Experiments were conducted using real remote sensing images and encouraging results are acquired comparing to traditional methods...|$|E
40|$|This {{paper is}} an {{introduction}} to a grammatical formalism (Phase-based Minimalist Grammar, elaboration of Stabler’s 1997 Minimalist Grammar) that includes a revised version of the standard minimalist structure building operations merge, move {{and the notion of}} derivation by phase (Chomsky 1999 - 2005). The main difference with respect to the standard theory is that these devices strictly operate Top-Down and from Left-to-Right. In these pages I will argue that <b>long</b> <b>distance</b> <b>dependencies,</b> such as successive cyclic A'-movement, are better understood within this unconventional (at least within the Minimalist Program) phase-based directional perspective...|$|R
40|$|This paper {{describes}} {{the results of}} several studies which address the question whether speakers' representations of the patterns of their language are indeed as general as the rules proposed by most modern linguists. The next two sections summarize the results of several experimental studies designed to provide evidence about the generality of speakers' knowledge of inflectional morphology. Then a construction which has been extensively studied by syntacticians working in the generative tradition is discussed: English questions with <b>long</b> <b>distance</b> <b>dependencies.</b> The final section discusses {{the implications of these}} studies for linguistic theory and methodology...|$|R
40|$|We {{show that}} {{informative}} lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way {{to obtain a}} CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on <b>long</b> <b>distance</b> <b>dependencies,</b> {{for which it is}} known to have weak rates of recovery. ...|$|R
