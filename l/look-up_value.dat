2|18|Public
40|$|AIMS: To {{investigate}} the cost-effectiveness {{of up to}} £ 400 worth of financial incentives for smoking cessation in pregnancy {{as an adjunct to}} routine healthcare. DESIGN: Cost-effectiveness analysis based on a phase II RCT and a cost-utility analysis using a lifetime Markov model. SETTING: The RCT was undertaken in Glasgow, Scotland. The economic analysis was undertaken from the UK NHS perspective. PARTICIPANTS: 612 pregnant women randomised to receive usual cessation support +/- financial incentives of up to £ 400 vouchers (609 USD), contingent on smoking cessation. MEASUREMENTS: Comparison of usual support and incentive interventions in terms of cotinine validate quitters, Quality Adjusted Life Years (QALYs) and direct costs to the NHS. FINDINGS: The incremental cost per quitter at 34 - 38 weeks pregnant was £ 1127 (1716). This is similar to the standard <b>look-up</b> <b>value</b> derived from Stapleton &# 38; Wests published ICER tables (72), £ 1390 per quitter, by looking-up the CPIT trial incremental cost (£ 157) and incremental 6 [*]month quit outcome (0. 14). The lifetime model resulted in an incremental cost of £ 17 (95...|$|E
40|$|Aims  To {{investigate}} the cost-effectiveness {{of up to}} £ 400 worth of financial incentives for smoking cessation in pregnancy {{as an adjunct to}} routine health care.   Design  Cost-effectiveness analysis based on a Phase II randomized controlled trial (RCT) and a cost–utility analysis using a life-time Markov model.   Setting  The RCT was undertaken in Glasgow, Scotland. The economic analysis was undertaken from the UK National Health Service (NHS) perspective.   Participants  A total of 612 pregnant women randomized to receive usual cessation support plus or minus financial incentives of up to £ 400 vouchers (US $ 609), contingent upon smoking cessation.   Measurements  Comparison of usual support and incentive interventions in terms of cotinine-validated quitters, quality-adjusted life years (QALYs) and direct costs to the NHS.   Findings  The incremental cost per quitter at 34 – 38 weeks pregnant was £ 1127 ($ 1716). This is similar to the standard <b>look-up</b> <b>value</b> derived from Stapleton & West's published ICER tables, £ 1390 per quitter, by looking up the Cessation in Pregnancy Incentives Trial (CIPT) incremental cost (£ 157) and incremental 6 -month quit outcome (0. 14). The life-time model resulted in an incremental cost of £ 17 [95 % confidence interval (CI) = –£ 93, £ 107] and a gain of 0. 04 QALYs (95 % CI = – 0. 058, 0. 145), giving an ICER of £ 482 /QALY ($ 734 /QALY). Probabilistic sensitivity analysis indicates uncertainty in these results, particularly regarding relapse after birth. The expected value of perfect information was £ 30 million (at a willingness to pay of £ 30 000 /QALY), so given current uncertainty, additional research is potentially worthwhile.   Conclusion  Financial incentives for smoking cessation in pregnancy are highly cost-effective, with an incremental cost per quality-adjusted life years of £ 482, which is well below recommended decision thresholds...|$|E
50|$|In {{an attempt}} to prevent the {{expansion}} of data-size, Logluv comes in a 24 bit flavour, which in a rather complicated way quantizes Lightness to 10 bit and merges U/V into a 14 bit <b>look-up</b> based <b>value.</b>|$|R
50|$|If an {{employee}} is awarded a known {{amount of money}} for working a certain shift or for working a number of consecutive weeks, that additional {{amount of money that}} is paid beyond the regular base pay and overtime will qualify for retroactive overtime if and only if there are also overtime hours paid during the same pay period of the qualifying bonus. You could also consider this to have an OT value of zero and add an additional <b>look-up</b> table <b>value</b> of all zeros for the percentages to use to determine the ROT amount.|$|R
40|$|In a {{previous}} {{issue of the}} RIAC Journal [Reference 1], we pro-vided a high-level introduction to the 217 PlusTM component failure rate prediction models, {{and in the last}} edition [Refer-ence 2] we presented the 217 PlusTM capacitor and diode fail-ure rate models. In this issue, we present the Integrated Circuit and Inductor component models in their entirety. A brief example will be provided {{at the end of the}} article. 217 PlusTM integrated Circuit failure Rate Models This section contains models for both nonhermetic and her-metically sealed integrated circuits. The form of the two models is basically the same (differences will be highlight-ed), but the table <b>look-up</b> parameter <b>values</b> differ...|$|R
40|$|Abstract. Chamfer {{distances}} are discrete distances {{based on}} the propagation of local distances, or weights defined in a mask. The medial axis, i. e. the centers of the maximal disks (disks which are not contained in any other disk), is {{a powerful tool for}} shape representation and analysis. The extraction of maximal disks is performed in the general case with comparison tests involving look-up tables rep-resenting the covering relation of disks in a local neighborhood. Although <b>look-up</b> table <b>values</b> can be computed efficiently, the computation of the look-up table neighborhood tend to be very time-consuming. By using polytope descriptions of the chamfer disks, the necessary operations to extract the look-up tables are greatly reduced...|$|R
40|$|International audienceThis paper {{deals with}} the {{performance}} of Low-Density Parity-Check codes in impulsive interference modeled by α-stable random variables. In case of α-stable noise, the optimal inputs of the belief propagation decoder are complex to obtain. We propose to use the simple clipping approach that reduces the impact of large noise values. Our main contribution is to give three different approaches to obtain the parameters of the clipping function and to assess the performance of the decoder. We show that a <b>look-up</b> table whose <b>values</b> are pre-determined, thanks to the Density Evolution tool, is the most efficient approach...|$|R
40|$|International audienceConventionally {{the control}} of dry clutch's {{engagement}} during a standing start in AMT vehicles is assured by <b>look-up</b> tables whose <b>values</b> are carefully chosen to produce a smooth synchronisation {{at the expense of}} a long slipping time. This article proposes, instead, a new approach based on: an open-loop look-up table aiming to reduce the slipping time, combined with an observer-based optimal control assuring the engagement comfort. Particular attention has been given to the details of the on-line implementation on a Clio AMT prototype. Experimental results show both a close match between the predicted and the actual trajectories and a high level of comfort...|$|R
40|$|Abstract—This paper {{deals with}} the {{performance}} of Low-Density Parity-Check codes in impulsive interference modeled by α-stable random variables. In case of α-stable noise, the optimal inputs of the belief propagation decoder are complex to obtain. We propose to use the simple clipping approach that reduces the impact of large noise values. Our main contribution is to give three different approaches to obtain the parameters of the clipping function and to assess the performance of the decoder. We show that a <b>look-up</b> table whose <b>values</b> are predetermined, thanks to the Density Evolution tool, is the most efficient approach. Index Terms—LDPC codes, α-stable interference, density evolution, clipping...|$|R
40|$|Invited session). Best student paper award. International audienceConventionally {{the control}} of dry clutch's {{engagement}} during a standing start in AMT vehicles is assured by <b>look-up</b> tables whose <b>values</b> are carefully chosen to produce a smooth synchronisation {{at the expense of}} a long slipping time. This article proposes, instead, a two phase approach: a first open-loop lookup table phase aiming to reduce the slipping time and a second observer-based optimal control phase assuring the engagement comfort. Particular attention has been given to the details of the on-line implementation on a Clio AMT prototype. Experimental results show both a close match between the predicted and the actual trajectories and a high level of comfort...|$|R
40|$|Convex {{optimization}} {{has gained}} popularity {{due to its}} capability to reach global optimum in {{a reasonable amount of}} time. Convexity is often ensured by fitting the table data into analytically convex forms such as posynomials. However, fitting the look-up tables into the posynomial forms with minimum error itself may not be a convex optimization problem and hence excessive fitting errors may be introduced. In this paper, we propose to directly adjust the <b>look-up</b> table <b>values</b> into a numerically convex look-up table without explicit analytical form. We show that numerically ”convexifying” the table data with minimum perturbation can be formulated as a convex semidefinite optimization problem and hence optimality can be reached in polynomial time. Without an explicit form limitation, we find that the fitting error is significantly reduced while the convexity is still ensured. As a result, convex optimization algorithms can still be applied. Furthermore, we also develop a ”smoothing ” algorithm to make the table data smooth and convex to facilitate the optimization process. Results from extensive experiments on industrial cell libraries demonstrate that our method reduces 30 X fitting error over a well-developed posynomial fitting algorithm. Its application to circuit tuning is also presented. 1...|$|R
40|$|The {{objective}} {{of this paper is}} to present a series of improvements on the Joint Research Centre Two-stream Inversion Package (JRC-TIP) that enhance its effectiveness to generate reliable surface products and associated uncertainties from surface albedo <b>values.</b> <b>Look-Up</b> Tables (LUTs) are built in the observation space from the JRC-TIP and they are used to store solutions obtained from off-line dedicated procedures on selected sets of prior conditions. This new approach drastically limits the occurrence of questionable solutions, revealed by outliers in the retrievals, often associated with local, instead of global minima, and ensures that the retrieved values are insensitive to small variations in the input albedo values. This TIP table based approach also reduces considerably the computing time requirement, a definite asset in the systematic application of the TIP against large data sets of surface albedo products. JRC. H. 5 -Land Resources Managemen...|$|R
40|$|The {{sensitivity}} of a mesoscale weather prediction model to a 1 km satellite-based vegetation roughness initialization is investigated for a domain within the south central United States. Three different roughness databases are employed: i) a control or standard lookup table roughness {{that is a}} function only of land cover type, ii) a spatially heterogeneous roughness database, specific to the domain, that was previously derived using a physically based procedure and Moderate Resolution Imaging Spectroradiometer (MODIS) imagery, and iii) a MODIS climatologic roughness database that like (i) is a function only of land cover type, but possesses domain specific mean values from (ii). The model used is the Weather Research and Forecast Model (WRF) coupled to the Community Land Model within the Land Information System (LIS). For each simulation, a statistical comparison is made between modeled results and ground observations within a domain including Oklahoma, Eastern Arkansas, and Northwest Louisiana during a 4 -day period within IHOP 2002. Sensitivity analysis compares the impact the three roughness initializations on time-series temperature, precipitation probability of detection (POD), average wind speed, boundary layer height, and turbulent kinetic energy (TKE). Overall, the results indicate that, for the current investigation, replacement of the standard <b>look-up</b> table <b>values</b> with the satellite-derived values statistically improves model performance for most observed variables. Such natural roughness heterogeneity enhances the surface wind speed, PBL height and TKE production up to 10 percent, with a lesser effect over grassland, and greater effect over mixed land cover domains...|$|R
40|$|We {{propose a}} {{lossless}} compression technique {{specifically designed for}} palettized synthetic images. Predictive techniques do not work very well for these images, as a prediction "formula" based on some average of the values or palette indices of neighbors {{is not likely to}} be very meaningful. The proposed algorithm uses patterns of neighborhood pixels to predict and code each pixel. The prediction rules for different patterns are learned adaptively from the image itself. Using a large number of test images of the above kind (maps, clip-art, line drawings), the proposed method is found to reduce the size achieved by GIF compression by 50 %, and the size resulting from the previous best approach (CALIC with optimized palette reordering) by 20 %. 1. Introduction In this work, we propose a lossless image compression technique, targeted for compression of palettized synthetic images (a palettized color image consists of a color <b>look-up</b> table; the <b>value</b> of each pixel is stored as an index into [...] ...|$|R
40|$|International audienceAn {{important}} step in simulation via isogeometric analysis (IGA) is the assembly step, where the coefficients of the final linear system are generated. Typically, these coefficients are integrals of products of shape functions and their derivatives. Similarly to the finite element analysis (FEA), the standard choice for integral evaluation in IGA is Gaussian quadrature. Recent developments propose different quadrature rules, that {{reduce the number of}} quadrature points and weights used. We experiment with the existing methods for matrix generation. Furthermore we propose a new, quadrature-free approach, based on interpolation of the geometry factor and fast <b>look-up</b> operations for <b>values</b> of B-spline integrals. Our method builds upon the observation that exact integration is not required to achieve the optimal convergence rate of the solution. In particular, it suffices to generate the linear system within the order of accuracy matching the approximation order of the discretization space. We demonstrate that the best strategy is one that follows the above principle, resulting in expected accuracy and improved computational time...|$|R
40|$|In the {{reinforcement}} learning framework, standard, table-based <b>look-up</b> methods for <b>value</b> func-tions converge to the optimal solution, yet unfortunately these methods are intractable for com-plex real-world control problems. A common approach {{to overcome this}} problem are so-called function approximation techniques that generalise over their input spaces. In this paper we study the capabilities of two machine learning frameworks, the neural network and the self-organising map, {{in the light of}} their expressiveness as function approximators. Our study is based on two complex control problems and has a strong focus on the empirical evaluation of the approxima-tion methods. In particular, we report very mixed results where neither of the machine learning approaches is convincing in a general sense. 1 Declaration I declare that this thesis was composed by myself, that the work contained herein is my own except where explicitly stated otherwise in the text, and that this work has not been submitted for any other degree or professional qualification except as specified...|$|R
40|$|Abstract. An {{important}} step in simulation via isogeometric analysis (IGA) is the assembly step, where the coefficients of the final linear system are generated. Typically, these coefficients are integrals of products of shape functions and their derivatives. Similarly to the finite element analysis (FEA), the standard choice for integral evaluation in IGA is Gaussian quadrature. Recent developments propose different quadrature rules, that {{reduce the number of}} quadrature points and weights used. We experiment with the existing methods for matrix generation. Furthermore we propose a new, quadrature-free approach, based on interpolation of the geometry factor and fast <b>look-up</b> operations for <b>values</b> of B-spline integrals. Our method builds upon the observation that exact integration is not required to achieve the optimal convergence rate of the solution. In particular, it suffices to generate the linear system within the order of accuracy matching the approximation order of the discretization space. We demonstrate that the best strategy is one that follows the above principle, resulting in expected accuracy and improved computational time. Key words: isogeometric analysis, stiffness matrix, mass matrix, numerical integration, quadrature...|$|R
40|$|Includes bibliographical {{references}} (pages 72 - 75). The simplest {{and most}} standardized metric for reporting the estimated dose {{from a series}} of computed tomography (CT) scans is the CT dose index (CTDI). Although this quality control (QC) procedure is required on an annual basis by the FDA, performing such measurements more frequently (e. g. monthly or even weekly) would present the opportunity to expose an unexpected drift in CT scanner output from its commissioned settings and allow for an earlier correction to the problem. This would ensure scans are performed with optimal image quality and avoid the possibility of exposing patients to ancillary ionizing radiation. However, correctly aligning CTDI phantoms can be a laborious, time-intensive process. The first part of this study is a proposed method to construct a CTDI ratio <b>look-up</b> to derive <b>values</b> of CTDIw using only in-air measurements. This could serve as a quick monthly or weekly CTDIw QC check with a minimum of three simple in-air measurements without devoting a large amount of time toward CTDI phantom setup. 	Although CTDI serves as a useful metric for quantifying CT radiation output to compare different scanning protocols, it does not contain enough information to accurately evaluate dose to a specific patient or organ of interest. For this reason, Monte Carlo (MC) simulations are used to compute patient-specific dose for research purposes. Accurate calculations largely rely on a researcher's ability to correctly model the x-ray source fluence and spectra, which are largely defined by an internal bow tie (BT) filter. The second part of this project explores the feasibility of employing an aluminum cylinder half value layer (HVL) measurement technique in conjunction with a real-time dose probe to completely assess the HVL along the BT filter axis in a CT scanner with a minimum of three scans. The data from this method could be used to calculate the angle-dependent fluence and energy spectra along the BT filter axis, which could be used as beam model input for MC dose computations. This technique is significant not only for its rapid approach, but also that each scan can be completed using routine scan protocols rather than service or localization protocols - eliminating the possible reliance on a service engineer...|$|R
40|$|The {{reactive}} {{collision avoidance}} (RCA) algorithm allows a spacecraft {{to find a}} fuel-optimal trajectory for avoiding an arbitrary number of colliding spacecraft in real time while accounting for acceleration limits. In addition to spacecraft, the technology {{can be used for}} vehicles that can accelerate in any direction, such as helicopters and submersibles. In contrast to existing, passive algorithms that simultaneously design trajectories for a cluster of vehicles working to achieve a common goal, RCA is implemented onboard spacecraft only when an imminent collision is detected, and then plans a collision avoidance maneuver for only that host vehicle, thus preventing a collision in an off-nominal situation for which passive algorithms cannot. An example scenario for such a situation might be when a spacecraft in the cluster is approaching another one, but enters safe mode and begins to drift. Functionally, the RCA detects colliding spacecraft, plans an evasion trajectory by solving the Evasion Trajectory Problem (ETP), and then recovers after the collision is avoided. A direct optimization approach was used to develop the algorithm so it can run in real time. In this innovation, a parameterized class of avoidance trajectories is specified, and then the optimal trajectory is found by searching over the parameters. The class of trajectories is selected as bang-off-bang as motivated by optimal control theory. That is, an avoiding spacecraft first applies full acceleration in a constant direction, then coasts, and finally applies full acceleration to stop. The parameter optimization problem can be solved offline and stored as a <b>look-up</b> table of <b>values.</b> Using a <b>look-up</b> table allows the algorithm to run in real time. Given a colliding spacecraft, the properties of the collision geometry serve as indices of the look-up table that gives the optimal trajectory. For multiple colliding spacecraft, the set of trajectories that avoid all spacecraft is rapidly searched on-line. The optimal avoidance trajectory is implemented as a receding-horizon model predictive control law. Therefore, at each time step, the optimal avoidance trajectory is found and the first time step of its acceleration is applied. At the next time step of the control computer, the problem is re-solved and the new first time step is again applied. This continual updating allows the RCA algorithm to adapt to a colliding spacecraft that is making erratic course changes...|$|R

