7|11|Public
40|$|This paper {{presents}} {{a study of}} content based image retrieval using compression based methods with original and despeckled TerraSAR-X images. This study aims at analysing the behaviour of our method regarding speckle noise. Our method is based on <b>Lempel-Ziv-Welch</b> <b>compression</b> algorithm for feature extraction and fast compression distance as similarity metric. From the experimental results can be observed that the method is independent of the speckle noise...|$|E
40|$|An {{efficient}} storage format {{was developed}} for computer-generated holograms for use in electron-beam lithography. This method employs run-length encoding and <b>Lempel-Ziv-Welch</b> <b>compression</b> and succeeds in exposing holograms that were previously infeasible owing to the hologram 2 ̆ 7 s tremendous pattern-data file size. These holograms also require significant computation; thus the algorithm was implemented on a parallel computer, which improved performance by 2 orders of magnitude. The decompression algorithm was integrated into the Cambridge electron-beam machine 2 ̆ 7 s front-end processor. Although this provides much-needed ability, some hardware enhancements will be required {{in the future to}} overcome inadequacies in the current front-end processor that result in a lengthy exposure time...|$|E
40|$|Abstract- Steganography is {{a method}} in which secret message is {{embedded}} into a cover image {{to protect it from}} unauthorized access. The challenge of steganography technique is to maintain a rational balance between the quality of file and size of data that can be transferred. This paper presents a secured and robust steganography method capable of embedding high volume of information in digital cover image without incurring and perceptual distortion. This method is based compression and encryption. In this method the message to be transmitted first using <b>Lempel-Ziv-Welch</b> <b>compression</b> technique and is encrypted by using and hide compress data in image using kekre’s algorithm. The proposed method is tested with different images and text of various lengths...|$|E
50|$|Terry Archer Welch was an American {{computer}} scientist. Along with Abraham Lempel and Jacob Ziv, {{he developed}} the lossless <b>Lempel-Ziv-Welch</b> (LZW) <b>compression</b> algorithm, {{which was published in}} 1984.|$|R
25|$|The {{motivation}} {{for creating the}} PNG format was in early 1995, after it became known that the <b>Lempel–Ziv–Welch</b> (LZW) data <b>compression</b> algorithm used in the Graphics Interchange Format (GIF) format was patented by Unisys. There were also other problems with the GIF format that made a replacement desirable, notably its limit of 256 colors {{at a time when}} computers able to display far more than 256 colors were becoming common.|$|R
40|$|A hybrid {{method for}} color image {{steganography}} is suggested to conceal a secret data into the cover {{image in the}} spatial and frequency domain. In this method, <b>Lempel–Ziv–Welch</b> (LZW) <b>compression</b> is used to obtain a low bit rate; Also Linear Feedback Shift Register (LFSR) technique is used to enhance {{the security of the}} scheme. In the embedding process in spatial domain, a K-means clustering and EA algorithms are used for secret data embedding. Also in the embedding process in frequency domain, a Coefficients selection and frequency hopping algorithm (CSFH) and Adaptive Phase Modulation mechanism (APM) are used for secret data embedding. Abilities of the proposed method are high security, because of using a hybrid method (spatial and frequency domain) and Dat...|$|R
40|$|The sensor nodes are {{typically}} resource deficient with energy {{being the most}} critical of all the resources. There are usually several security requirements to protect a sensor network. These requirements should be considered during design of a security protocol, including confidentiality, integrity, and authenticity. An effective security protocol should provide services to meet these requirements. In addition to this, sensor devices have critical resource constraints such as memory size and reliability. So a simple but versatile compression technique is necessary for data compression, before the encryption process. Here the compression technique which is utilized is LZW (<b>Lempel-Ziv-Welch)</b> <b>compression</b> and HIGHT algorithm for encryption. The salient features of LZW with HIGHT algorithm is discussed to satisfy the current requirements especially data authenticity and availability of wireless sensor networks...|$|E
40|$|In modern {{distributed}} systems, {{many important}} data processing algorithms are bound not by CPU time, but by power and network bandwidth. Instead of investing more {{power in a}} faster interconnect, data can be compressed to improve the apparent bandwidth available to applications. We looked to maximize throughput while minimizing area, power, and delay. We explored this idea by implementing and evaluating a parallel dictionary architecture of the popular <b>Lempel-Ziv-Welch</b> <b>compression</b> algorithm (PDLZW) used in applications such as GNU compress. Our architecture targets TSMC’s 40 nm ASIC process and has 34 fan-out-of-four (FO 4) delays. For a dictionary architecture of 1024 entries, the architecture completes a single cycle of compression in 4 clocks and a cycle of decompression in 3 clocks giving us an average compression rate of 312. 5 MB/s and decompression rate of 1. 03 GB/s. A 300 MB/s link using our architecture could theoretically achieve an average effective bandwidth of 743 MB/s. Algorith...|$|E
40|$|The genome of an {{organism}} contains all hereditary information encoded in Deoxyribonucleic Acid (DNA). Molecular sequence databases (e. g.,EMBL, Genbank, DDJB, Entrez, SwissProt, etc) represent millions of DNA sequences filling {{many thousands of}} gigabytes and the databases are doubled in size every 6 - 8 months, which may go to beyond the limit of storage capacity. There are several text compression algorithm used for DNA compression. This paper proposes a new hybrid algorithm is used to compress DNA sequence, the algorithm is designed by combining the fixed length binary code with the LZW (<b>Lempel-Ziv-Welch)</b> <b>compression</b> algorithm. Initially the input sequence is divided in to fragments where each fragment consist of four nucleotides and fixed length binary code is assigned to each nucleotide then the pattern (STR and CHR) in LZW used the same for creating the dictionary. Assigning a new binary code for each pattern in the dictionary using a binary tree, and the sequence is replaced binary code for the longest match in the dictionary while compression. The proposed approach attains maximum compression in DNA sequences...|$|E
25|$|The {{most common}} general-purpose, {{lossless}} compression algorithm used with TIFF is <b>Lempel–Ziv–Welch</b> (LZW). This <b>compression</b> technique, {{also used in}} GIF, was covered by patents until 2003. TIFF also supports the compression algorithm PNG uses (i.e. Compression Tag 000816 'Adobe-style') with medium usage and support by applications. TIFF also offers special-purpose lossless compression algorithms like CCITT Group IV, which can compress bilevel images (e.g., faxes or black-and-white text) better than PNG's compression algorithm.|$|R
40|$|Any {{positive}} word {{comprised of}} random sequence of tokens form a finite alphabet {{can be reduced}} (without change of length) using an appropriate size Braid group relationships. Surprisingly the Braid relations dramatically reduce the Kolmogorov Complexity of the original random word and do so in distinct bands of (rate of change) values with gaps in between. Distribution of these bands are estimated and empirical statistics collected by actually coding approximations to the Kolmogorov Complexity (in Mathematica 9. 0). <b>Lempel-Ziv-Welch</b> lossless <b>compression</b> algorithm techniques used to estimate the distribution for gaped bands. Evidence provided that such distributions of reduction in Kolmogorov Complexity based upon Braid groups are universal i. e. they can model more general algebraic structures other than Braid groups. Comment: Additional code for v 2 included in the references. Multi-pass experiments included and some test code {{to make sure the}} code is correct. The Many-Pass reductions are shown to have Gamma/Poisson distribution which has been a great excitemen...|$|R
40|$|Data Compression {{is one of}} {{the most}} {{fundamental}} problems in computer science and information technology. Many sequential algorithms are suggested for the problem. The most well known sequential algorithm is <b>Lempel-Ziv-Welch</b> (LZW) <b>compression</b> technique. The limitation of sequential algorithm is that ith block can be coded only after the (i- 1) th block has completed. This limitation can be overcome by parallelizing the LZW coding technique. Attempt has also been made to parallelize the LZ technique [10]. But here is a new idea for parallelizing the LZW compression technique. It uses a common memory to store the encoded string parallely in a two dimensional array and stores- 1 at the end in each row which works as the marker. Similarly each row is decoded parallely by different processors. It is suitably implemented in SMP cluster using MPI library function. The sequential algorithm takes θ(n) where n is the size of text. But the parallel algorithm takes θ (n/p) where p is the number of processor...|$|R
40|$|Dynamic partial {{reconfiguration}} is {{a relatively}} new technique that permits the reconfiguration of portions of an FPGA without stopping all on-chip logic. The confluence of the additional adaptability of dynamic partial reconfiguration, and the massive parallelism FPGAs are capable of, lend themselves to new applications. This paper will begin to explore the possibilities of applying partial reconfiguration, as realized in the Altera series V FPGAs, to a compression algorithm. The selected compression algorithm is known as LZW, or <b>Lempel-Ziv-Welch</b> <b>compression,</b> and is an adaptive lossless algorithm that builds a dictionary of common sequences as compression progresses, and emits short codes for common sequences. An implementation of the algorithm benefits from the distributed memory and highly parallel nature of an FPGA, as the dynamic dictionary is subject to a full search for every new input sequence, and is updated with every emitted code word. In considering the benefits of implementing functionality with dynamic partial reconfiguration, a metric indicating the possible benefit must be selected. With regards to LZW, as it is an inherently variable compression rate algorithm, a more appropriate metric than coding rate is coding latency, or the average time necessary to compress or decompress a given block of data. Thus, an optimized implementation of LZW on a partial reconfiguration capable FPGA would most likely seek to either reduce time per encoded sequence, or maximize sequences that may be detected simultaneously. This report addresses the implementation of LZW on a partial reconfiguration capable Altera Cyclone V FPGA, with and without using the reconfiguration capabilities, and evaluates functional and physical requirements for the effective usage of runtime reconfiguration. </p...|$|E
40|$|We {{present the}} results of {{applying}} lossless and lossy data compression to a three-dimensional object reconstruction and recognition technique based on phase-shift digital holography. We find that the best lossless (Lempel-Ziv, <b>Lempel-Ziv-Welch,</b> Huffman, Burrows-Wheeler) <b>compression</b> rates can be expected when the digital hologram is stored in an intermediate coding of separate data streams for real and imaginary components. The lossy techniques are based on subsampling, quantization, and discrete Fourier transformation. For various degrees of speckle reduction, we quantify the number of Fourier coefficients that can {{be removed from the}} hologram domain, and the lowest level of quantization achievable, without incurring significant loss in correlation performance or significant error in the reconstructed object domain...|$|R
40|$|A new secret writing using DES and DEFLATE {{algorithm}} is explored by improving the previous attempts {{in this sense}} in terms of security and compression. A Matching Pursuit algorithm using redundant basis decomposition technique is used to securely hide a message. The stability and computational complexity problems are solved by introducing new selection and update rules working entirely in the integer domain. Image decomposition is randomized in several ways thus improving the stego-message undetectability, and making the hidden message undetectable by targeted steganalyzers. A refinement decomposition step {{is applied to the}} three color bands to increase the stego-message payload. Due to the ever-increasing security threats, {{there is a need to}} develop algorithms with more complexity and advanced features for a secured message transfer. A comparative study is performed between <b>Lempel-Ziv-Welch</b> (LZW) data <b>compression</b> algorithm and deflate loss-less compression algorithm which proves the efficiency of deflate algorithm in both rate of compression and its compression speed...|$|R
40|$|Region of {{interest}} (ROI) {{is the most}} informative part of a medical image and mostly {{has been used as}} a major part of watermark. Various shapes ROIs selection have been reported in region-based watermarking techniques. In region-based watermarking schemes an image region of non-interest (RONI) is the second important part of the image and is used mostly for watermark encapsulation. In online healthcare systems the ROI wrong selection by missing some important portions of the image to be part of ROI can create problem at the destination. This paper discusses the complete medical image availability in original at destination using the whole image as a watermark for authentication, tamper localization and lossless recovery (WITALLOR). The WITALLOR watermarking scheme ensures the complete image security without of ROI selection at the source point as compared to the other region-based watermarking techniques. The complete image is compressed using the <b>Lempel-Ziv-Welch</b> (LZW) lossless <b>compression</b> technique to get the watermark in reduced number of bits. Bits reduction occurs to a number that can be completely encapsulated into image. The watermark is randomly encapsulated at the least significant bits (LSBs) of the image without caring of the ROI and RONI to keep the image perceptual degradation negligible. After communication, the watermark is retrieved, decompressed and used for authentication of the whole image, tamper detection, localization and lossless recovery. WITALLOR scheme is capable of any number of tampers detection and recovery at any part of the image. The complete authentic image gives the opportunity to conduct an image based analysis of medical problem without restriction to a fixed ROI...|$|R

