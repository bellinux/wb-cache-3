183|456|Public
50|$|SuperPrime is a {{computer}} program used for calculating the primality of a large set of positive natural numbers. Because of its multi-threaded nature and dynamic <b>load</b> <b>scheduling,</b> it scales excellently when using more than one thread (execution core). It is commonly used as an overclocking benchmark to test the speed and stability of a system.|$|E
40|$|In {{this short}} paper we {{consider}} the equivalence between various load-scheduling policies and continuous time Markov chains. It provides a basic unification of both divisible <b>load</b> <b>scheduling</b> and Markov chains {{for the first time}} in 15 years of research. This unification is suggested by the fact that both divisible <b>load</b> <b>scheduling</b> theory and Markovian queueing theory have common features...|$|E
40|$|This paper {{focuses on}} the <b>load</b> <b>scheduling</b> problem in both demand-side {{management}} and supply-side management for household users in a smart grid. Linear programming models have been developed either to minimize the energy cost in demand-side scheduling, or to maximize the load factor for supply-side scheduling. The proposed models provide various flexibilities to the end users {{by allowing them to}} define multiple time intervals for the operations of intended appliances, and to pre-set if an appliance should be operated with or without interruption. Simulations performed on the <b>load</b> <b>scheduling</b> of one individual household give promising and convincing results. The proposed model in its linear programming form promises a fast and convergent solving process with a guaranteed exact solution. These added features make the proposed <b>load</b> <b>scheduling</b> framework more practical and realistic for real world applications...|$|E
40|$|New <b>load</b> <b>schedules</b> for {{the machine}} {{calibration}} of a six-component force balance {{are currently being}} developed and evaluated at the NASA Ames Balance Calibration Laboratory. One of the proposed <b>load</b> <b>schedules</b> is discussed in the paper. It has a total of 2082 points that are distributed across 16 load series. Several criteria were applied to define the <b>load</b> <b>schedule.</b> It was decided, for example, to specify the calibration load set in force balance format as this approach greatly simplifies {{the definition of the}} lower and upper bounds of the <b>load</b> <b>schedule.</b> In addition, all loads are assumed to be applied in a calibration machine by using the one-factor-at-a-time approach. At first, all single-component loads are applied in six load series. Then, three two-component load series are applied. They consist of the load pairs (N 1, N 2), (S 1, S 2), and (RM, AF). Afterwards, four three-component load series are applied. They consist of the combinations (N 1, N 2, AF), (S 1, S 2, AF), (N 1, N 2, RM), and (S 1, S 2, RM). In the next step, one four-component load series is applied. It is the load combination (N 1, N 2, S 1, S 2). Finally, two five-component load series are applied. They are the load combination (N 1, N 2, S 1, S 2, AF) and (N 1, N 2, S 1, S 2, RM). The maximum difference between loads of two subsequent data points of the <b>load</b> <b>schedule</b> is limited to 33 % of capacity. This constraint helps avoid unwanted load "jumps" in the <b>load</b> <b>schedule</b> that can {{have a negative impact on}} the performance of a calibration machine. Only loadings of the single- and two-component load series are loaded to 100 % of capacity. This approach was selected because it keeps the total number of calibration points to a reasonable limit while still allowing for the application of some of the more complex load combinations. Data from two of NASA's force balances is used to illustrate important characteristics of the proposed 2082 -point calibration <b>load</b> <b>schedule...</b>|$|R
40|$|With special {{reference}} to design of fuel tanks in space vehicles, {{the principles of}} fracture mechanics are reviewed. An approximate but extremely simple relationship is derived among the operating stress level, the length of crack, {{and the number of}} cycles of failure. Any one of the variables can be computed approximately from the knowledge of the other two, if the <b>loading</b> <b>schedule</b> (mission of the tank) is not greatly altered. Two sample examples illustrating the procedures of determining the allowable safe operating stress corresponding to a set of assumed <b>loading</b> <b>schedule</b> are included. The selection of sample examples is limited by the relatively meager available data on the candidate material for various stress ratios in the cycling...|$|R
40|$|This paper {{describes}} {{an effort to}} build and partially validate an energy model of an existing educational building located in Cambridge, MA, USA. This work was carried out {{as part of a}} research seminar for graduate architecture/design students and included four related tasks: Modelling the building’s geometry and thermal properties in DesignBuilder/EnergyPlus, generating a site-specific weather file based on nearsite measured data, assessing internal <b>load</b> <b>schedules</b> based on a detailed building survey, and collecting monthly metered data for heating lighting and cooling over a whole year. The purpose of the seminar was (a) to evaluate how effectively design students can use a state-of-the-art graphical user interface (GUI) such as DesignBuilder and (b) to quantify the value of using customized weather data and internal <b>load</b> <b>schedules</b> as opposed to default GUI inputs. The authors found that the students quickly learned how to navigate the DesignBuilder GUI but were frequently confused by the model data hierarchy/inheritance and frustrated that customized schedules cannot be assigned more efficiently. The benefit of using customized weather data as opposed to a local TMY 3 file turned out to be small whereas using customized as opposed to default internal <b>load</b> <b>schedules</b> reduced the relative error of predicted versus metered annual electricity use from 18 % t...|$|R
40|$|Slides of a {{presentation}} on Voice over ATM. Topics include: Performance Requirements N-Source Configuration Simulation configuration CDV Source Model Performance Metrics Multiplexing Gain Scheduling Policies Scheduling Results: 1 Buf/VC Scheduling Results: 1 Buf/VC (Cont) Scheduling Policies: Results I Scheduling Results: 2 Bufs/VC Scheduling Results: 2 Bufs/VC (Cont) Scheduling Policies: Results II Scheduling Results: Medium <b>Load</b> <b>Scheduling</b> Results: Heavy <b>Load</b> <b>Scheduling</b> Policies: Results III Drop Policies Drop Policies Results Drop Polices Results: Heavy Load Drop Policies: Result...|$|E
30|$|Upcoming {{work is the}} {{implementation}} of a controller that uses the PV power and load forecasts for efficient operational strategies for battery charging and <b>load</b> <b>scheduling.</b>|$|E
40|$|In recent years, {{researchers}} have proposed numerous advanced <b>load</b> <b>scheduling</b> algorithms for smart homes {{with the goal}} of reducing the grid’s peak power usage. In parallel, utilities have introduced variable rate pricing plans to incentivize residential consumers to shift more of their power usage to low-price, off-peak periods, also {{with the goal of}} reducing the grid’s peak power usage. In this pa-per, we argue that variable rate pricing plans do not incentivize consumers to adopt advanced <b>load</b> <b>scheduling</b> algorithms. While beneficial to the grid, these algorithms do not significantly lower a consumer’s electric bill. To address the problem, we propose flat-power pricing, which directly incentivizes consumers to flat-ten their own demand profile, rather than shift as much of their power usage as possible to low-cost, off-peak periods. Since most loads have only limited scheduling freedom, <b>load</b> <b>scheduling</b> algo-rithms often cannot shift much, if any, power usage to low-cost, off-peak periods, which are often many hours in the future. In con-trast, flat-power pricing encourages consumers to shift power usage even over short time intervals to flatten demand. We evaluate the benefits of advanced <b>load</b> <b>scheduling</b> algorithms using flat-power pricing, showing that consumers save up to 40 % on their electric bill, compared with 11 % using an existing time-of-use rate plan...|$|E
40|$|The {{justification}} {{issues and}} the capacity selection of autotransformers in the main electrical grids, which require the accounting of the operation regimes of power units {{for the future of}} 10 or more years are considered. Appropriate methodological provisions for justification and choice in terms of gen-eralized <b>load</b> <b>schedules,</b> <b>load</b> capacity of transformer equipment, as well as assessing the reliability of elec-trical installations in the design and development of the energy systems are proposed...|$|R
5000|$|London - Gatwick (Never inaugurated, {{although}} <b>loaded</b> onto <b>schedules)</b> ...|$|R
5000|$|Kuwait City - (Never inaugurated, {{although}} <b>loaded</b> onto <b>schedules)</b> ...|$|R
40|$|The aim of {{this paper}} is to provide a {{solution}} for <b>load</b> <b>scheduling</b> by implementing value stream mapping, which is a straightforward enough for production management. Decision makers in the industry should have a clear understanding about positive effect from <b>load</b> <b>scheduling</b> and its effect to production outcome and process availability. Value stream mapping is a well-known process optimization tool from lean production philosophy. The aim of value stream mapping is to shorten the lead time of industrial processes and to reduce the intermediate stock amounts. By complementing value stream map with process energy intensity and energy stored in intermediate stocks, we can promote <b>load</b> <b>scheduling</b> possibilities. Our methodology provides a tool that is understandable and traceable for industry-minded decision makers. Finally, we present a real life test example for the new methodology, which is based on the production process of a district heating plant...|$|E
40|$|Abstract—A simple {{modification}} of existing divisible <b>load</b> <b>scheduling</b> algorithms, boosting link speed by M for M parallel channels per link, allows time optimal <b>load</b> <b>scheduling</b> and performance prediction for parallel channel systems. The situation for multicore models {{is more complex}} but can be handled by a substitution involving equivalent processor speed. These modifications yield upper bounds on such parallel systems’ performance. This concept is illustrated for ideal single level (star) tree networks {{under a variety of}} scheduling policies. Less than ideal parallelism can also be modeled though mechanisms of inefficiency require further research. I...|$|E
40|$|We {{consider}} joint {{energy storage}} management and <b>load</b> <b>scheduling</b> at a residential site with integrated renewable generation. Assuming unknown arbitrary dynamics of renewable source, loads, and electricity price, {{we aim at}} optimizing the <b>load</b> <b>scheduling</b> and energy storage control simultaneously {{in order to minimize}} the overall system cost within a finite time period. Besides incorporating battery operational constraints and costs, we model each individual load task by its requested power intensity and service durations, as well as the maximum and average delay requirements. To tackle this finite time horizon stochastic problem, we propose a real-time scheduling and storage control solution by applying a sequence of modification and transformation to employ Lyapunov optimization that otherwise is not directly applicable. With our proposed algorithm, we show that the joint <b>load</b> <b>scheduling</b> and energy storage control can in fact be separated and sequentially determined. Furthermore, both scheduling and energy control decisions have closed-form solutions for simple implementation. Through analysis, we show that our proposed real-time algorithm has a bounded performance guarantee from the optimal T-slot look-ahead solution and is asymptotically equivalent to it as the battery capacity and time period goes to infinity. The effectiveness of joint <b>load</b> <b>scheduling</b> and energy storage control by our proposed algorithm is demonstrated through simulation as compared with alternative algorithms. Comment: to appear in IEEE Transactions on Smart Gri...|$|E
30|$|The <b>load</b> <b>schedule</b> of {{the unit}} should also have a {{considerable}} influence on {{the selection of the}} boiler feed pump drive. Assuming a longer operation {{of the unit}} under lower loads would exclude the application of the EBPT and MC variants due to a substantial decrease in the unit efficiency with load.|$|R
40|$|Common {{practice}} in scheduling under limited resource availability is to first schedule activities {{with the assumption}} of unlimited resources, and then assign required resources to activities until available resources are exhausted. The process of matching a feasible resource plan with a feasible schedule is called resource allocation. Then, to avoid sharp fluctuations in the resource profile, further adjustments are applied to both schedule and resource allocation plan {{within the limits of}} feasibility constraints. This process is referred to as resource leveling in the literature. Combination of these three stages constitutes the standard approach of top-down scheduling. In contrast, when scarce and/or expensive resource is to be scheduled, first a feasible and economical resource usage plan is established and then activities are scheduled accordingly. This practice is referred to as bottom-up scheduling in the literature. Several algorithms are developed and implemented in various commercial scheduling software packages to schedule based on either of these approaches. However, in reality resource <b>loaded</b> <b>scheduling</b> problems are somewhere in between these two ends of the spectrum. Additionally, application of either of these conventional approaches results in just a feasible resource <b>loaded</b> <b>schedule</b> which is not necessarily the cost optimal solution. In order to find the cost optimal solution, activity scheduling and resource allocation problems should be considered jointly. In other words, these two individual problems should be formulated and solved as an integrated optimization problem. In this research, a novel integrated optimization model is proposed for solving the resource <b>loaded</b> <b>scheduling</b> problems with concentration on construction heavy equipment being the targeted resource type. Assumptions regarding this particular type of resource along with other practical assumptions are provided for the model through inputs and constraints. The objective function is to minimize the fraction of the execution cost of resource <b>loaded</b> <b>schedule</b> which varies based on the selected solution and thus, considered to be the model's decision making criterion. This fraction of cost which hereafter is referred to as operation cost, encompasses four components namely schedule delay cost, shipping, rental and ownership costs for equipment...|$|R
40|$|Given n items, each having, say, {{a weight}} and a length, and n {{identical}} bins with a weight and a length capacity, the 2 -Dimensional Vector Packing Problem (2 -DVPP) calls for packing {{all the items}} into the minimum number of bins. The problem is NP-hard, and has applications in <b>loading,</b> <b>scheduling</b> and layout design. As for th...|$|R
40|$|This paper {{introduces}} {{an optimal}} load allocation approach for measurement and data reporting in wireless sensor networks. Using divisible load theory {{as a starting}} point, results in terms of minimum finish time (make-span) are obtained for di#erent measurement and reporting strategies. This work is novel as it introduces, for the first time, a new <b>load</b> <b>scheduling</b> strategy that considers the measurement capacity of processors and assumes negligible computation time which is radically di#erent from the traditional divisible <b>load</b> <b>scheduling</b> research to date. Performance results in terms of finish time (make-span) for homogeneous measurement and reporting speeds are also presented...|$|E
40|$|Developing multi- core {{computer}} technology made it practical to accelerate image processing algorithms via parallel running threads. In this study, performance evaluations for parallel image convolution filter on a multi-core computer using Java thread utilities was presented. For this purpose, {{the efficiency of}} static and the dynamic <b>load</b> <b>scheduling</b> implementations are investigated on a multi- core computer with six cores processor. Dynamic <b>load</b> <b>scheduling</b> overhead results were measured experimentally. Also the effect of busy running environment on performance which usually occurs on due to other running processes is illustrated by experimental measurements. According to performance results, about 5. 7 times acceleration over sequential implementation was obtained on a six cores computer for various image size...|$|E
40|$|Abstract—Large-scale {{distributed}} applications {{are subject to}} frequent disruptions due to resource contention and failure. Such disruptions are inherently unpredictable and, therefore, robustness is a desirable property for the distributed operating environment. In this work, we describe and evaluate a robust topology for applications that operate on a spanning tree overlay network. Unlike previous work that is adaptive or reactive in nature, we take a proactive approach to robustness. The topology itself is able to simultaneously withstand disturbances and exhibit good performance. We present both centralized and distributed algorithms to construct the topology, and then demonstrate its effectiveness through analysis and simulation of two classes of {{distributed applications}}: Data collection in sensor networks and data dissemination in divisible <b>load</b> <b>scheduling.</b> The results show that our robust spanning trees achieve a desirable trade-off for two opposing metrics where traditional forms of spanning trees do not. In particular, the trees generated by our algorithms exhibit both resilience to data loss and low power consumption for sensor networks. When used as the overlay network for divisible <b>load</b> <b>scheduling,</b> they display both robustness to link congestion and low values for the makespan of the schedule. Index Terms—Robustness, distributed computing, graph theory, fault tolerance, wireless sensor networks, divisible <b>load</b> <b>scheduling.</b> ...|$|E
40|$|A {{baseline}} <b>load</b> <b>schedule</b> for {{the manual}} calibration of a force balance is defined that takes current capabilities at the NASA Ames Balance Calibration Laboratory into account. The chosen <b>load</b> <b>schedule</b> consists of 18 load series {{with a total}} of 194 data points. It was designed to satisfy six requirements: (i) positive and negative loadings should be applied for each load component; (ii) at least three loadings should be applied between 0 % and 100 % load capacity; (iii) normal and side force loadings should be applied at the forward gage location, aft gage location, and the balance moment center; (iv) the balance should be used in "up" and "down" orientation to get positive and negative axial force loadings; (v) the constant normal and side force approaches should be used to get the rolling moment loadings; (vi) rolling moment loadings should be obtained for 0, 90, 180, and 270 degrees balance orientation. In addition, three different approaches are discussed in the paper that may be used to independently estimate the natural zeros, i. e., the gage outputs of the absolute load datum of the balance. These three approaches provide gage output differences {{that can be used to}} estimate the weight of both the metric and non-metric part of the balance. Data from the calibration of a six-component force balance will be used in the final manuscript of the paper to illustrate characteristics of the proposed baseline <b>load</b> <b>schedule...</b>|$|R
40|$|Abstract—This paper {{proposes a}} new {{strategy}} to meet controllable heating, ventilation, and air conditioning (HVAC) loads with a non-conventional energy source and energy storage system. It also classifies the input loads into four categories such as constant <b>load</b> timed <b>schedule</b> (CLTS), constant <b>load</b> variable <b>schedule</b> (CLVS), variable <b>load</b> timed <b>schedule</b> (VLTS), variable <b>load</b> variable <b>schedule</b> (VLVS). For classification of loads we must consider the Energy Conservation Measures (ECM) impact also. For calculation of energy saved it requires both base-line load value and post-installation value. In order to minimize cost and to increase efficiency, we use BFOA-based optimization approach together with the classification of loads. Bacterial foraging optimization algorithm (BFOA) has been widely accepted as a global optimization algorithm of current interest for distributed optimization and control. Minimizing the cost function guarantees minimum power generation through non-conventional source as well as storage capacity selection to supply the HVAC load. Index terms- Bacterial foraging algorithm-based optimization approach, HVAC load, energy conservation measures (ECM). I...|$|R
30|$|An EV, {{irrigation}} pump, {{or water}} heater can {{be responsive to}} price variations because they are deferrable loads. That is, they need to consume {{a certain amount of}} kWh energy within a certain time window, but it is flexible regarding how much kW load they need to consume at each instant. References [42, 43] deal with the design <b>load</b> <b>schedules</b> for such deferrable devices.|$|R
40|$|In an {{abstract}} framework, we examine how a tradeoff between efficiency and risk arises in different dynamic oligopolistic markets. We consider a scenario {{where there is}} a reliable resource provider and agents which enter and exit the market following a random process. Self-interested and fully rational agents can both produce and consume the resource. They dynamically update their <b>load</b> <b>scheduling</b> decisions over a finite time horizon, under the constraint that the net resource consumption requirements are met before each individual's deadline. We first examine the system performance under the non-cooperative and cooperative market architectures, both under marginal production cost pricing of the resource. The statistics of the stationary aggregate demand processes induced by the two market architectures show that although the non-cooperative <b>load</b> <b>scheduling</b> scheme leads to an efficiency loss - widely known as the "price of anarchy" - the stationary distribution of the corresponding aggregate demand process has a smaller tail. This tail, which corresponds to rare and undesirable demand spikes, is important in many applications of interest. With {{a better understanding of the}} efficiency-risk tradeoff, we investigate, in a non-cooperative setup, how resource pricing can be used as a tool by the system operator to tradeoff between efficiency and risk. We further provide a convex characterization of the Pareto front of different system performance measures. The Pareto front determines the tradeoff among volatility suppression of concerned measurements in the system with <b>load</b> <b>scheduling</b> dynamics. This is the fundamental tradeoff in the sense that system performance achieved by any <b>load</b> <b>scheduling</b> strategies induced by any specific market architectures is bounded by this Pareto front. by Qingqing Huang. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (p. 87 - 90) ...|$|E
40|$|The use of {{divisible}} <b>load</b> <b>scheduling</b> {{theory is}} proposed to model and design grid {{systems such as}} those arising in large physics experiments. Current divisible load theory is summarized. A typical application, the STAR experiment at RHIC is discussed. This includes a sample calculation based on existing infrastructure numbers...|$|E
40|$|We present our own {{research}} work that uses extents of Peer-to-Peer technology with {{a framework that}} allows reliable Grid computing (P 2 P Grid) over the Internet. We propose how to decide optimized redundancy level of group peers by using system cost function and grid local reliability. Moreover we discuss an effectiveness of SLA-constrained <b>load</b> <b>scheduling</b> policy with multi-probing technology {{in order to maintain}} group more stable. Especially SLA-constrained <b>load</b> <b>scheduling</b> policy is designed for handling divisible loads and indivisible loads simultaneously and guaranteeing the shortest time of completing task. Finally through the simulation, we provide that these two proposed schemes can be evaluated to the reasonable solution to overcome unexpected system fault or down regarding system dependability issues in redundant group peers based P 2 P Grid environment...|$|E
40|$|In {{this work}} {{we present a}} general control scheme for the {{management}} of an energy community and a load modeling approach. We adopt a multilevel control scheme in which the first level computes the energy consumption set point towards the second level and the generation planning towards the energy sources, the second level is in charge of local <b>loads</b> <b>scheduling</b> and micro-generation planning. © 2010 IEEE...|$|R
30|$|The {{aggregators}} {{are informed}} {{in advance of}} the <b>load</b> interruption <b>schedule</b> and nodal prices during the day.|$|R
5000|$|... 3) Short Term/ Weekly plan: It is on weekly or {{intra-day}} {{basis for}} material control, <b>loading,</b> and <b>scheduling</b> ...|$|R
40|$|Abstract—Power {{consumption}} of data centers {{accounts for a}} significant portion of operational cost for service providers. However, it is not sufficient to minimize electricity cost only. Instead, service providers should also consider the sustainability of data centers. In this paper, we investigate the problem of socially-responsible <b>load</b> <b>scheduling</b> for sustainable data centers in a smart grid environment. We formulate the joint optimization of electricity cost, performance cost and social cost into a min-cost network flow model, which takes the diversity of regional electricity prices, user request latency, usage of renewable energy into account. We further propose both offline and online <b>load</b> <b>scheduling</b> algorithms to achieve the optimality of the objective cost function. Finally, a series of simulation experiments are con-ducted {{to evaluate the effectiveness of}} our proposed algorithms. I...|$|E
40|$|In this paper, {{a robust}} {{optimization}} strategy is developed {{to handle the}} uncertainties for domestic electric water heater <b>load</b> <b>scheduling.</b> At first, the uncertain parameters, including hot water demand and ambient temperature, are described as the intervals, and are further divided into different robust levels {{in order to control}} the degree of the conservatism. Based on this, traditional <b>load</b> <b>scheduling</b> problem is rebuilt by bringing the intervals and robust levels into the constraints, and are thus transformed into the equivalent deterministic optimization problem, which can be solved by existing tools. Simulation results demonstrate that the schedules obtained under different robust levels are of complete robustness. Furthermore, in order to offer users the most optimal robust level, the trade-off between the electricity bill and conservatism degree are also discussed...|$|E
30|$|The work in [53] {{shows that}} under a {{constant}} <b>load,</b> <b>scheduling</b> algorithms that are oblivious to queue state will incur an average delay that grows at least linearly {{in number of}} users, whereas, channel- and queue-aware schedulers can achieve an average delay that is independent {{of the number of}} users.|$|E
40|$|The Advanced Reactors Transition (ART) Resource <b>Loaded</b> <b>Schedule</b> (RLS) {{provides}} a cost and schedule baseline {{for managing the}} project elements within the ART Program. The Fast Flux Test Facility (FETF) activities are delineated {{through the end of}} FY 2000, assuming continued standby. The Nuclear Energy (NE) Legacies and Plutonium Recycle Test Reactor (PRTR) activities are delineated {{through the end of the}} deactivation process. This revision reflects the 19 Oct 1999 baseline...|$|R
40|$|The Langley Research Center (LaRC) Single Vector Balance Calibration System (SVS) {{was first}} {{introduced}} in 2000 by Peter Parker. The SVS combines the Design of Experiments (DOE) methodology with a novel load application system. Since that time three systems have been designed and developed with different load range capabilities (ranging from 2 pounds to 3, 000 pounds). Approximately fifteen balances have been calibrated and their data compared to conventional techniques. This paper will present results of these comparisons, based on the mathematical models and accuracies, and discuss differences that were observed. In addition, changes {{in the implementation of}} the initial <b>load</b> <b>schedules</b> developed using DOE will be highlighted. One of the principles behind DOE is randomization. The initial <b>loading</b> <b>schedules</b> used to date have been randomized in the traditional DOE sense but not for repeat calibrations or experiments. Implementation of this randomization within blocks and its impact on data quality will be reviewed. Areas of potential future development will be presented which include changes in the centers to include loads with the force position system in the pure error estimates...|$|R
40|$|In this study, we {{consider}} {{flexible manufacturing system}} <b>loading,</b> <b>scheduling</b> and tool management problems simultaneously. Our aim is to determine relevant tool management decisions, which are machining conditions selection and tool allocation, and to <b>load</b> and <b>schedule</b> parts on non-identical parallel CNC machines. The dual objectives are minimization of the manufacturing cost and total weighted tardiness. The manufacturing cost is comprised of machining and tooling costs (which are affected by machining conditions) and non-machining cost (which is affected by tool replacement decisions). We used both sequential and simultaneous approaches to solve our problem to show {{the superiority of the}} simultaneous approach. The proposed heuristics are used in a problem space genetic algorithm in order to generate a series of approximately efficient solutions...|$|R
