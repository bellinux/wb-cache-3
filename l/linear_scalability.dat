175|24|Public
5000|$|<b>Linear</b> <b>scalability.</b> It's {{possible}} {{to extend a}} cluster by adding servers.|$|E
50|$|With fewer {{protocol}} layers, {{this approach}} makes AoE fast and lightweight. It {{also makes the}} protocol relatively easy to implement and offers <b>linear</b> <b>scalability</b> with high performance. The AoE specification is 12 pages compared with iSCSI's 257 pages.|$|E
50|$|In 2005 Scimore was created, and the {{database}} was separated {{into a separate}} company. The major improvements in 2006 were the SQL to DQL translator and new distribution algorithms, allowing for <b>linear</b> <b>scalability</b> in distributed environment. The embedded version where launched.|$|E
50|$|In April 2012, Robert Haas of EnterpriseDB {{demonstrated}} PostgreSQL 9.2's <b>linear</b> CPU <b>scalability</b> using {{a server}} with 64 cores.|$|R
40|$|We {{present a}} PDE-constrained {{optimization}} algorithm {{which is designed}} for parallel scalability on distributed-memory architectures with thousands of cores. The method {{is based on a}} line-search interior-point algorithm for large-scale continuous optimization, it is matrix-free in that it does not require the factorization of derivative matrices. Instead, it uses a new parallel and robust iterative linear solver on distributed-memory architectures.   We will show almost <b>linear</b> parallel <b>scalability</b> results for the complete optimization problem, which is a new emerging important biomedical application and is related to antenna identification in hyperthermia cancer treatment planning...|$|R
40|$|Operating system {{performance}} and scalability on sharedmemory many-core systems depends critically on efficient access to shared data structures. Scalability has proven {{difficult to achieve}} for many data structures. In this paper we present a novel and highly scalable concurrent red-black tree. Red-black trees are widely used in operating systems, but typically exhibit poor scalability. Our red-black tree has <b>linear</b> read <b>scalability,</b> uncontended read performance that is at least 25 % faster than other known approaches, and deterministic lookup times for a given tree size, making it suitable for realtime applications...|$|R
5000|$|When established, GigaSpaces {{focused on}} {{application}} platforms for Java, and [...]NET environments {{based on the}} software architecture pattern [...] "space-based architecture", borrowing concepts from the Jini and JavaSpaces specifications, providing <b>linear</b> <b>scalability</b> for stateful, high-performance applications using the tuple space paradigm.|$|E
50|$|Evaluation with {{benchmarks}} {{following the}} TPC-C specification on different cluster sizes has shown <b>linear</b> <b>scalability</b> {{with up to}} 100 nodes. Evaluation with hybrid workloads TPC-C plus analytical queries inspired on TPC-H over the TPC-C database has shown that LeanXcale can provide full isolation between both workloads.|$|E
5000|$|MySQL Cluster is a {{technology}} providing shared-nothing clustering and auto-sharding for the MySQL database management system. It {{is designed to}} provide high availability and high throughput with low latency, while allowing for near <b>linear</b> <b>scalability.</b> MySQL Cluster is implemented through the NDB or NDBCLUSTER storage engine for MySQL ("NDB" [...] stands for Network Database).|$|E
40|$|We {{present a}} {{concurrent}} programming methodology called relativistic programming and illustrate {{its use in}} a concurrent red-black tree implementation. In relativistic programming each thread sees memory through its own temporal frame of reference. This approach to sharing memory allows reading to proceed independent of concurrent updates and avoids the need for expensive atomic instructions in reads. Our relativistic red-black tree has <b>linear</b> read <b>scalability</b> and uncontended read performance that is at least 25 % faster than other known approaches. Performance under contention is many times faster than other approaches, both for reads in general and for updates {{in the presence of}} concurrent readers. ...|$|R
40|$|AbstractRelational {{numerical}} abstract domains {{do not scale}} up. To {{ensure a}} linear cost of abstract domains, abstract interpretation-based tools analyzing large programs generally split the set of variables into independent smaller sets, sometimes sharing some non-relational information. We present a way to gain precision by keeping fully expressive relations between the subsets of variables, whilst retaining a <b>linear</b> complexity ensuring <b>scalability...</b>|$|R
40|$|Lempel-Ziv (LZ) {{techniques}} {{are the most}} widely used for lossless file compression. LZ compression basicly comprises two methods, called LZ 1 and LZ 2. The LZ 1 method is the one employed by the family of Zip compressors, while the LZW compressor implements the LZ 2 method, which is slightly less effective but twice faster. When the file size is large, both methods can be implemented on a distributed system guaranteeing <b>linear</b> speed-up, <b>scalability</b> and robustness. With Web computing, the MapReduce model of distributed processing is emerging as {{the most widely used}}. In this framework, we present and make a comparative analysis of different implementations of LZ compression. An alternative to standard versions of the Lempel-Ziv method is proposed as the most efficient one for large size files compression {{on the basis of a}} theoretical worst case analysis, which evidentiates its robustness...|$|R
5000|$|Processors may be {{interconnected}} using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth {{and power}} {{consumption of the}} interconnect among the various processors, the memory, and the disk arrays. Mesh architectures avoid these bottlenecks, and provide nearly <b>linear</b> <b>scalability</b> to much higher processor counts at the sacrifice of programmability: ...|$|E
50|$|It {{is neither}} an object database, nor a {{relational}} database. It does {{not try to}} satisfy arbitrary relations and the ACID properties, but rather is a big, distributed, fault-tolerant, persistent hash table.A 2012 study comparing systems for storing application performance management monitoring data reported that Voldemort, Cassandra, and HBase offered <b>linear</b> <b>scalability</b> in most cases, with Voldemort having the lowest latency and Cassandra having the highest throughput.|$|E
50|$|BigWorld Server {{supports}} {{dynamic load}} balancing, {{a feature that}} automatically and dynamically spreads user load across multiple cell apps on the same game server, allowing for large numbers of concurrent users to inhabit the same game space. In 1999, BigWorld ran a test, simulating 900 entities on the same server. In 2005, large scale tests were carried out at the IBM Deep Computing facility in Poughkeepsie, NY. BigWorld successfully demonstrated the <b>linear</b> <b>scalability</b> of its load balancing technology by dynamically balancing 100,000 entities across various cell apps on a single server.|$|E
40|$|This paper {{presents}} the implementation and performance {{evaluation of the}} Bucket Flattening Omega Network of the SDC-II, the Super Database Computer II. The SDC-II is a highly parallel relational database server, which consists of eight data processing modules interconnected by two networks. Parallelism in the parallel relational database processing on the shared nothing architecture would suffer from skewed data distribution and collisions in the interconnection networks. The network of the SDC-II was designed to offer an architectural support of the Bucket Spreading hash join algorithm, which enables the system to have <b>linear</b> performance <b>scalability</b> even with skewed data distributions. The additional functionality introduced into the network can generate a flat bucket distribution, which {{is essential to the}} algorithm, while providing almost conflict free routing. Each switching unit of the network is implemented on an FPGA chip, and its performance is evaluated to compare with an impl [...] ...|$|R
40|$|Abstract—Lempel-Ziv (LZ) {{techniques}} {{are the most}} widely used for lossless file compression. LZ compression basicly comprises two methods, called LZ 1 and LZ 2. The LZ 1 method is the one employed by the family of Zip compressors, while the LZW compressor implements the LZ 2 method, which is slightly less effective but twice faster. When the file size is large, both methods can be implemented on a distributed sys-tem guaranteeing <b>linear</b> speed-up, <b>scalability</b> and robustness. With Web computing, the MapReduce model of distributed processing is emerging as {{the most widely used}}. In this framework, we present and make a comparative analysis of different implementations of LZ compression. An alternative to standard versions of the Lempel-Ziv method is proposed as the most efficient one for large size files compression {{on the basis of a}} theoretical worst case analysis, which evidentiates its robustness. Keywords-web computing; mapreduce framework; lossless compression; string factorization; worst case analysis I...|$|R
40|$|Abstract. Until recently, the {{preferred}} method of livelock detection was via LTL model checking, which imposes complex constraints on partial order reduction (por), limiting its performance and parallelization. The {{introduction of the}} dfsfifo algorithm by Faragó et al. showed that live-locks can theoretically be detected faster, simpler, and with stronger por. For the first time, we implement dfsfifo and {{compare it to the}} LTL approach by experiments on four established case studies. They show the improvements over the LTL approach: dfsfifo is up to 3. 2 times faster, and it makes por up to 5 times better than with spin’s ndfs. Additionally, we propose a parallel version of dfsfifo, which demonstrates the efficient combination of parallelization and por. We prove parallel dfsfifo correct and show why it provides stronger guarantees on parallel scalability and por compared to LTL-based methods. Experimentally, we establish almost ideal <b>linear</b> parallel <b>scalability</b> and por close to the por for safety checks: easily an order of magnitude better than for LTL. ...|$|R
50|$|SVC nodes {{are always}} clustered, {{with a minimum}} of 2 and a maximum of 8 nodes, and <b>linear</b> <b>scalability.</b> Nodes are {{rack-mounted}} appliances derived from IBM System x servers, protected by redundant power supplies and integrated batteries. Earlier models featured external battery-backed power supplies. Each node has Fibre Channel ports simultaneously used for incoming, outgoing, and intracluster data traffic. Hosts may also be attached via FCoE and iSCSI Gbit Ethernet ports. Intracluster communication includes maintaining read/write cache integrity, sharing status information, and forwarding reads and writes to any port. These ports must be zoned together.|$|E
5000|$|Hoard is an {{allocator}} whose goal is scalable {{memory allocation}} performance. Like OpenBSD's allocator, Hoard uses [...] exclusively, but manages memory in chunks of 64 kilobytes called superblocks. Hoard's heap is logically divided {{into a single}} global heap {{and a number of}} per-processor heaps. In addition, there is a thread-local cache that can hold a limited number of superblocks. By allocating only from superblocks on the local per-thread or per-processor heap, and moving mostly-empty superblocks to the global heap so they can be reused by other processors, Hoard keeps fragmentation low while achieving near <b>linear</b> <b>scalability</b> with the number of threads.|$|E
5000|$|The HP Integrity NonStop {{computers}} are {{a line of}} fault-tolerant server computers based on the Intel Itanium processor platform, and optimized for transaction processing. Average availability levels of 99.999% have been observed. [...] NonStop systems feature a massively parallel processing (MPP) architecture and provide <b>linear</b> <b>scalability.</b> Each CPU (systems can be expanded up to over 4000 CPUs) runs its own copy of the OS. This is a shared nothing architecture — a [...] "share nothing" [...] arrangement also known as loosely coupled multiprocessing, and no [...] "diminishing returns" [...] occur as more processors are added (see Amdahl's law).|$|E
40|$|This paper {{presents}} the results of a scalability study for a three dimensional semicoarsening multigrid solver on a distributed memory computer. In particular, we are interested in the scalability of the solver; how the solution time varies as both problem size and number of processors are increased. For an iterative <b>linear</b> solver, <b>scalability</b> involves both algorithmic issues and implementation issues. We examine the scalability of the solver theoretically by constructing a simple parallel model and experimentally by results obtained on a IBM SP. The results are compared with those obtained for other solvers on the same computer. 1. Introduction. This paper focuses on our work in developing a parallel solver for the linear systems arising from nite dierence, nite volume, or nite element discretizations of the diusion equation, r (Dru) + u = f; (1) on logically rectangular grids. In R n, the diusion coeÆcient D is a symmetric positive denite n n matrix and 0. We re [...] ...|$|R
40|$|Until recently, the {{preferred}} method of livelock detection was via LTL model checking, which imposes complex constraints on partial order reduction (POR), limiting its performance and parallelization. The {{introduction of the}} $DFS_{FIFO}$ algorithm by Faragó et al. showed that livelocks can theoretically be detected faster, simpler, and with stronger POR. For the first time, we implement $DFS_{FIFO}$ and {{compare it to the}} LTL approach by experiments on four established case studies. They show the improvements over the LTL approach: $DFS_{FIFO}$ is up to 3. 2 times faster, and it makes POR up to 5 times better than with SPIN's NDFS. Additionally, we propose a parallel version of $DFS_{FIFO}$, which demonstrates the efficient combination of parallelization and POR. We prove parallel $DFS_{FIFO}$ correct and show why it provides stronger guarantees on parallel scalability and POR compared to LTL-based methods. Experimentally, we establish almost ideal <b>linear</b> parallel <b>scalability</b> and POR close to the POR for safety checks: easily an order of magnitude better than for LTL...|$|R
40|$|The {{means to}} produce cyclic {{polymers}} in high purity and at large scale remains an elusive goal. Although ring-closure {{methods have been}} successful in producing rings free of <b>linear</b> chains, <b>scalability</b> is an unavoidable obstacle. We report a ring-expansion method for synthesizing cyclic polyalkenamers in high purity and at scale (> 25 g / reaction) using a recyclable heterogeneous ruthenium-based olefin metathesis catalyst. This ring-expansion metathesis polymn. (REMP) catalyst is an analog of the 2 nd Generation Hoveyda-Grubbs catalyst and is covalently bound to a silica surface via dual tethers from the N-heterocyclic carbene (NHC) and benzylidene. Cyclic polyalkenamers were synthesized from a variety of cycloolefins, including cyclopentene and cyclooctadiene. Their purity from linear species was detd. using rheol., hightemp. HPLC (HT-HPLC), solid-state NMR, size-exclusion chromatog. (SEC), and differentional scanning calorimetry (DSC). This synthetic methodol. was shown to be a robust means to produce cyclic polymers in high purity and at large scale...|$|R
50|$|Space-based {{architecture}} (SBA) is {{a software}} architecture pattern for achieving <b>linear</b> <b>scalability</b> of stateful, high-performance applications using the tuple space paradigm. It follows {{many of the}} principles of representational state transfer (REST), service-oriented architecture (SOA) and event-driven architecture (EDA), as well as elements of grid computing. With a space-based architecture, applications are built out of a set of self-sufficient units, known as processing-units (PU). These units are independent of each other, so that the application can scale by adding more units.The SBA model is closely related to other patterns that have been proved successful in addressing the application scalability challenge, such as shared nothing architecture (SN), used by Google, Amazon.com and other well-known companies. The model has also been applied by many firms in the securities industry for implementing scalable electronic securities trading applications.|$|E
40|$|Population-based Random Search (RS) algorithms, such as Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), Artificial Immune Systems (AIS) and Particle Swarm Optimization (PSO), {{have been}} widely applied to solving {{discrete}} optimization problems. A common belief {{in this area is}} that the performance of a population-based RS algorithm may improve if increasing its population size. The term of population scalability is used to describe the relationship between the performance of RS algorithms and their population size. Although understanding population scalability is important to design efficient RS algorithms, there exist few theoretical results about population scalability so far. Among those limited results, most of them belong to case studies, e. g. simple RS algorithms for simple problems. Different from them, the paper aims at providing a general study. A large family of RS algorithms, called ARS, has been investigated in the paper. The main contribution {{of this paper is to}} introduce a novel approach based on the fundamental matrix for analyzing population scalability. The performance of ARS is measured by a new index: spectral radius of the fundamental matrix. Through analyzing fundamental matrix associated with ARS, several general results have been proven: (1) increasing population size may increase population scalability; (2) no super <b>linear</b> <b>scalability</b> is available on any regular monotonic fitness landscape; (3) potential super <b>linear</b> <b>scalability</b> may exist on deceptive fitness landscapes; (4) ``bridgeable point'' and ``diversity preservation'' are two necessary conditions for super <b>linear</b> <b>scalability</b> on all fitness landscapes; and (5) ``road through bridges'' is a sufficient condition for super <b>linear</b> <b>scalability...</b>|$|E
40|$|We {{describe}} in detail our high-performance density matrix renormalization group (DMRG) algorithm for solving the electronic Schrödinger equation. We illustrate the <b>linear</b> <b>scalability</b> of our algorithm with calculations {{on up to}} 64 processors. The use of massively parallel machines in conjunction with our algorithm considerably extends the range of applicability of the DMRG in quantum chemistry...|$|E
40|$|Clique is a HP Labs Grenoble project. The goal is {{to develop}} a novel peer-to-peer, server-less {{distributed}} file system based on optimistic replication algorithms, which transparently integrates into users' native file systems. Some properties of the Clique system are epidemic replication, a no lost updates consistency model and conflict management, as well as disconnected operation and replica convergence. These properties ensure that updates done by any peer of the group will never be lost, and also that they will converge on all the group member machines. The system is well adapted to highly disconnected environments, network partitions, and variable join/leave rates. Even under adverse connectivity conditions, over time, assuming intermittent point-to-point connectivity between each peer and at least one other peer in the group, the local file system view at each node converges towards a consistent global view. The reconciliation protocol used is stateless and has no notion of group me mbership, in order to achieve a <b>linear</b> worst-case <b>scalability</b> in the order of N, the number of peers in the network...|$|R
40|$|PURPOSE : An area {{of great}} {{interest}} in current computational fluid dynamics research is that of free-surface modelling (FSM). Semi-implicit pressure-based FSM flow solvers typically involve the solution of a pressure correction equation. The latter being computationally intensive, {{the purpose of this}} paper is to involve the implementation and enhancement of an algebraic multigrid (AMG) method for its solution. DESIGN / METHODOLOGY / APPROACH : All AMG components were implemented via object-oriented C++ in a manner which ensures <b>linear</b> computational <b>scalability</b> and matrix-free storage. The developed technology was evaluated in two- and three-dimensions via application to a dam-break test case. FINDINGS : AMG performance was assessed via comparison of CPU cost to that of several other competitive sparse solvers. The standard AMG implementation proved inferior to other methods in three-dimensions, while the developed Freeze version achieved significant speed-ups and proved to be superior throughout. ORIGINALITY / VALUE : A so-called Freeze method was developed to address the computational overhead resulting from the dynamically changing coefficient matrix. The latter involves periodic AMG setup steps in a manner that results in a robust and efficient black-box solver. [URL] and Aeronautical Engineerin...|$|R
40|$|Abstract—NoSQL (Not only SQL) data stores {{become a}} vital {{component}} in many big data computing platforms {{due to its}} inherent horizontal scalability. HBase is an open-source distributed NoSQL store that is widely used by many Internet enterprises to handle their big data computing applications (e. g. Facebook handles millions of messages each day with HBase). Optimizations that can enhance the performance of HBase are of paramount interests for big data applications that use HBase or Big Table like key-value stores. In this paper we study the problems inherent in misconfiguration of HBase clusters, including scenarios where the HBase default configurations can lead to poor performance. We develop HConfig, a semi-automated configuration manager for optimizing HBase system performance from multiple dimensions. Due to the space constraint, this paper will focus {{on how to improve}} the performance of HBase data loader using HConfig. Through this case study we will highlight the importance of resource adaptive and workload aware auto-configuration management and the design principles of HConfig. Our experiments show that the HConfig enhanced bulk loading can significantly improve the performance of HBase bulk loading jobs compared to the HBase default configuration, and achieve 2 ~ 3. 7 x speedup in throughput under different client threads while maintaining <b>linear</b> horizontal <b>scalability...</b>|$|R
40|$|Historically, {{supercomputing}} {{has focused}} on number crunching. Nonnumeric applications, such as information retrieval and analysis, have {{to a lesser extent}} been able to exploit the inherent resources of supercomputers. This thesis presents the results from the development of a novel multiple instruction, single data (MISD) architecture, targeting evaluation of complex queries in large data volumes. For such applications, this architecture provides a better price versus performance ratio, better use of the available memory bandwidth, lower power consumption, as well as <b>linear</b> <b>scalability.</b> The core element of this technology is the Pattern Matching Chip (PMC). Each chip provides 1024 processing elements, with an accumulated performance of 10 10 operations per second. Multiple chips can be run in parallel with <b>linear</b> <b>scalability,</b> either within one computer, or in larger clusters. Up to half a million processing elements have been used in parallel in this project, providing 5 · 10 13 operations per second at 48 GB per second data throughput, in a unit smaller than one cubic meter. Even larger systems can be constructed, still with <b>linear</b> <b>scalability.</b> Through the novelty of this hardware architecture, the performance gained has enabled information processing {{in a way that would}} have been cost prohibitive with traditional computers. Such processing has demonstrated the capability of finding nuggets of valuable data in large and complex data volumes. The main effort – thus also the most important practical results – has been in bioinformatics. However, the technology has applicability in numerous other data mining applications. PhD i informasjons- og kommunikasjonsteknologiPhD in Information and Communications Technolog...|$|E
40|$|Though modern {{multicore}} machines {{have sufficient}} RAM and processors to manage very large in-memory databases, {{it is not}} clear what the best strategy for dividing work among cores is. Should each core handle a data partition, avoiding the overhead of concurrency control for most transactions (at the cost of increasing it for cross-partition transactions) ? Or should cores access a shared data structure instead? We investigate this question in the context of a fast in-memory database. We describe a new transactionally consistent database storage engine called MAFLINGO. Its cache-centered data structure design provides excellent base key-value store performance, to which we add a new, cache-friendly serializable protocol and support for running large, read-only transactions on a recent snapshot. On a key-value workload, the resulting system introduces negligible performance overhead as compared to a ver-sion of our system with transactional support stripped out, while achieving <b>linear</b> <b>scalability</b> versus the number of cores. It also exhibits <b>linear</b> <b>scalability</b> on TPC-C, a popular transactional benchmark. In addition, we show that a partitioning-base...|$|E
40|$|Streaming {{applications}} transform possibly infinite {{streams of}} data and often have both high throughput and low latency requirements. They are comprised of operator graphs that produce and consume data tuples. The streaming programming model naturally exposes task and pipeline parallelism, enabling it to exploit parallel systems of all kinds, including large clusters. However, it does not naturally expose data parallelism, which must instead be extracted from streaming applications. This paper presents a compiler and runtime system that automatically extract data parallelism for distributed stream processing. Our approach guarantees safety, even {{in the presence of}} stateful, selective, and user-defined operators. When constructing parallel regions, the compiler ensures safety by considering an operator’s selectivity, state, partitioning, and dependencies on other operators in the graph. The distributed runtime system ensures that tuples always exit parallel regions in the same order they would without data parallelism, using the most efficient strategy as identified by the compiler. Our experiments using 100 cores across 14 machines show <b>linear</b> <b>scalability</b> for standard parallel regions, and near <b>linear</b> <b>scalability</b> when tuples are shuffled across parallel regions...|$|E
40|$|Abstract — Molecular {{fragment}} analysis, using {{connected component}} identification algorithm, {{is of great}} significance for structural and chemical analysis in computer aided material design. However, {{it is a great}} challenge to accelerate molecular fragment analysis due to the scale, diversity and irregularity of molecular graphs. To address this challenge, we propose a hierarchical parallelization approach consisting of: (1) inter-node parallelization via spatial decomposition and hook-and-contract algorithm; (2) inter-core parallelization via master-and-worker scheme; and (3) locality optimization based on space-filling curve to improve memory accessing. Experiments show that the proposed scheme achieves nearly <b>linear</b> inter-node strong <b>scalability</b> up to 50 million vertices molecular graph on 32 computing nodes, and over 13 -fold inter-core speedup on 16 cores. The experiments also demonstrate the effectiveness of locality optimization on performance enhancement. 1...|$|R
40|$|We {{present a}} linear {{predictive}} compression approach for timeconsistent 3 D mesh sequences supporting and exploiting scalability. The algorithm decomposes each frame of a mesh sequence in layers employing patch-based mesh simplification techniques. This layered decomposition is consistent in time. Following the predictive coding paradigm, local {{temporal and spatial}} dependencies between layers and frames are exploited for compression. Prediction is performed vertex-wise from coarse to fine layers exploiting the motion of already encoded 1 -ring neighbor vertices for prediction of the current vertex location. It is shown that a predictive exploitation of the proposed layered configuration of vertices can improve the compression performance upon other state-of-the-art approaches by up to 16 % in domains relevant for applications. Index Terms — Animation compression, dynamic 3 D mesh coding, <b>scalability,</b> <b>linear</b> predictive coding, time-consistent mesh sequence. 1...|$|R
40|$|In {{the design}} of {{probabilistic}} timed systems, requirements concerning behaviour that occurs within a given time or energy budget are of central importance. We observe that model-checking such requirements for probabilistic timed automata {{can be reduced to}} checking reward-bounded properties on Markov decision processes. This is traditionally implemented by unfolding the model according to the bound, or by solving a sequence of linear programs. Neither scales well to large models. Using value iteration in place of <b>linear</b> programming achieves <b>scalability</b> but accumulates approximation error. In this paper, we correct the value iteration-based scheme, present two new approaches based on scheduler enumeration and state elimination, and compare the practical performance and scalability of all techniques on a number of case studies from the literature. We show that state elimination can significantly reduce runtime for large models or high bounds...|$|R
