1278|681|Public
5000|$|Each {{material}} has a specific <b>loss</b> <b>probability</b> per reflection, ...|$|E
5000|$|Packet erasure channel, where packets {{are lost}} {{with a certain}} packet <b>loss</b> <b>probability</b> or packet error rate ...|$|E
50|$|Zeta-TCP has {{introduced}} a simple but effective algorithm to calculate the <b>loss</b> <b>probability</b> on every unACK'd/unSACK'd packet. A packet is retransmitted only when its <b>loss</b> <b>probability</b> has surpassed a certain threshold. The same rule applies to the retransmission decision of every packet. Therefore, Zeta-TCP is able to minimize the number of retransmitted packets, further improving the bandwidth utilization. Lab tests also confirmed that Zeta-TCP retransmitted much fewer packets than the other TCP implementations under the same loss rate.|$|E
5000|$|There {{are common}} {{algorithms}} for computing the <b>loss</b> <b>probabilities</b> in <b>loss</b> networks ...|$|R
40|$|We {{study the}} effect of adding {{redundancy}} to an input stream on the losses that occur due to buffer overflow. We consider several sessions that generate traffic into a finite capacity queue. Using multi-dimensional probability generating functions, we derive analytical formulas for the <b>loss</b> <b>probabilities</b> and provide asymptotic analysis (for large n and small or large &rho;). Our analysis allows us to investigate when does adding redundancy decrease the <b>loss</b> <b>probabilities.</b> In many cases, redundancy is shown to degrade the performance, as the gain in adding redundancy is not sufficient to compensate the additional losses due to the increased overhead. We show, however, {{that it is possible to}} decrease <b>loss</b> <b>probabilities</b> if a sufficiently large amount of redundancy is added. Indeed, we show that for an arbitrary stationary ergodic input process, if &rho; < 1 then redundancy can reduce <b>loss</b> <b>probabilities</b> to an arbitrarily small value...|$|R
40|$|The {{purpose of}} this paper is to study the <b>loss</b> <b>probabilities</b> of {{messages}} in an M/M/ 1 /K queueing system where in addition to losses due to buffer overflow there are also random losses in the incoming and outgoing links. We focus on the influence of adding redundant packets to the messages (as in error correction coding, e. g. Reed [...] Solomon code, etc.). In the first part we use multi-dimensional probability generating functions for solving the recursions which generalize those introduced by Cidon et al. [IEEE Trans. Inform. Theory 39 (1) (1993) 98] for computing the <b>loss</b> <b>probabilities</b> and derive analytical formulae for a special case. In the second part of the paper we use combinatorial arguments and Ballot theorem results to alternatively obtain the <b>loss</b> <b>probabilities.</b> The analytical results allow us to investigate when does adding redundancy decrease the <b>loss</b> <b>probabilities.</b> 2003 Elsevier Science B. V. All rights reserved...|$|R
5000|$|Congestion control {{then becomes}} a {{distributed}} optimisation algorithm. Many current congestion control algorithms can be modelled in this framework, with [...] being either the <b>loss</b> <b>probability</b> or the queueing delay at link [...]|$|E
5000|$|European {{insurers}} use cat {{models to}} derive the required regulatory capital under the Solvency II regime. Cat models {{are used to}} derive catastrophe <b>loss</b> <b>probability</b> distributions which are components of many Solvency II internal capital models.|$|E
50|$|The role of {{congestion}} control is to moderate {{the rate at}} which data is transmitted, according to the capacity of the network and {{the rate at which}} other users are transmitting. Like TCP Vegas, FAST TCP uses queueing delay instead of <b>loss</b> <b>probability</b> as a congestion signal.|$|E
40|$|The {{purpose of}} this paper is to study the effect of adding {{redundancy}} to an input stream on the losses that occur due to buffer overflow. We consider several sessions that generate traffic into a finite capacity queue. In most of the paper, the input traffic of each session is modeled by a Poisson input. We use multi-dimensional probability generating functions for solving the recursions introduced by Cidon, Khamisy and Sidi [6] for computing the <b>loss</b> <b>probabilities</b> and derive analytical formulas. Using asymptotic analysis (for large n and small &rho;), we obtain good approximations with very low complexity of computation and memory. Our analysis allows us to investigate when does adding redundancy decrease the <b>loss</b> <b>probabilities.</b> In many cases, adding capacity is seen to degrade the <b>loss</b> <b>probabilities,</b> which can be explained by the fact that the gain in adding redundancy is not sufficient to compensate the additional losses due to the fact that the traffic load is increased when redundancy is added. We show, however, that it is possible to decrease <b>loss</b> <b>probabilities</b> if a sufficiently large amount of redundancy is added. Indeed, we show that for an arbitrary stationary ergodic input process, if &rho; < 1 then redundancy can improve the performance and reduce <b>loss</b> <b>probabilities</b> to an arbitrarily small value. We compute the rate of additional redundancy required for the case Poisson input processes...|$|R
40|$|Under most system assumptions, {{closed form}} {{solutions}} of performance measures for input queueing crossbar switches are not available. In this paper, we present expressions and bounds for the derivatives of cell <b>loss</b> <b>probabilities</b> {{with respect to}} the arrival rate evaluated at a zero arrival rate. These bounds are used to give an approximation by Taylor expansion, thereby providing an economical way to estimate cell <b>loss</b> <b>probabilities</b> in light traffic...|$|R
50|$|From {{this result}} <b>loss</b> <b>probabilities</b> for calls {{arriving}} on different routes {{can be calculated}} by summing over appropriate states.|$|R
50|$|The {{idea behind}} TFRC is {{to measure the}} <b>loss</b> <b>probability</b> and round trip time and to use these as the {{parameters}} to a model of TCP throughput. The expected throughput from this model is then used to directly drive the transmit rate of a TFRC flow.|$|E
50|$|Most current {{congestion}} control algorithms detect congestion and slow down when they discover that packets are being dropped, {{so that the}} average sending rate depends on the <b>loss</b> <b>probability.</b> This has two drawbacks. First, low loss probabilities are required to sustain high data rates; {{in the case of}} TCP Reno, very low loss probabilities are required, but even new congestion avoidance algorithms such as H-TCP, BIC TCP and HSTCP require loss rates lower than those provided by most wireless wide area networks. Moreover, packet loss only provides a single bit of information about the congestion level, whereas delay is a continuous quantity and in principle provides more information about the network.|$|E
5000|$|Measurements {{are useful}} and {{necessary}} for verifying the actual network performance. However, measurements {{do not have}} the level of abstraction that makes traffic models useful. Traffic models can be used for hypothetical problem solving whereas traffic measurements only reflect current reality. In probabilistic terms, a traffic trace is a realization of a random process, whereas a traffic model is a random process. Thus, traffic models have universality. A traffic trace gives insight about a particular traffic source, but a traffic model gives insight about all traffic sources of that type. Traffic models have many uses, but at least three major ones. One important use of traffic models is to properly dimension network resources for a target level of QOS. It was mentioned earlier that Erlang developed models of voice calls to estimate telephone switch capacity to achieve a target call blocking probability. Similarly, models of packet traffic are needed to estimate the bandwidth and buffer resources to provide acceptable packet delays and packet <b>loss</b> <b>probability.</b> Knowledge of the average traffic rate is not sufficient. It is known from queuing theory that queue lengths increase with the variability of traffic (Kleinrock, 1976). Hence, an understanding of traffic burstiness or variability is needed to determine sufficient buffer sizes at nodes and link capacities (Barakat, et al., 2003). A second important use of traffic models is to verify network performance under specific traffic controls. For example, given a packet scheduling algorithm, {{it would be possible to}} evaluate the network performance resulting from different traffic scenarios. For another example, a popular area of research is new improvements to the TCP congestion avoidance algorithm. It is critical that any algorithm is stable and allows multiple hosts to share bandwidth fairly, while sustaining a high throughput. Effective evaluation of the stability, fairness, and throughput of new algorithms would not be possible without realistic source models. A third important use of traffic models is admission control. In particular, connection oriented networks such as ATM depends on admission control to block new connections to maintain QOS guarantees. A simple admission strategy could be based on the peak rate of a new connection; a new connection is admitted if the available bandwidth is greater than the peak rate. However, that strategy would be overly conservative because a variable bit-rate connection may need significantly less bandwidth than its peak rate. A more sophisticated admission strategy is based on effective bandwidths (Kelly, 1996). The source traffic behavior is translated into an effective bandwidth between the peak rate and average rate, which is the specific amount of bandwidth required to meet a given QoS constraint. The effective bandwidth depends on the variability of the source.1 ...|$|E
40|$|Slotted optical burst {{switching}} (SOBS) {{has recently}} {{caught the attention}} of the optical networking community due to performance gains achievable with synchronous infrastructures. In this paper, we study the <b>loss</b> <b>probabilities</b> in a slotted optical burst switching node fed with Poisson burst traffic where the fixed burst size is an integer multiple of the slot length. We develop a discrete-time Markov chain (DTMC) based framework to obtain the <b>loss</b> <b>probabilities</b> in systems with and without quality of service (QoS) differentiation. In particular, we focus on analytical modeling of priority scheduling and offset-based QoS differentiation mechanisms for SOBS networks. The latter problem suffers from the curse of dimensionality which we address by a discrete phase type distribution approximation for the discrete Poisson distribution leading to an accurate approximation for the <b>loss</b> <b>probabilities.</b> A hybrid QoS mechanism which jointly utilizes offset-based differentiation together with priority scheduling is also analyzed. © 2009 IEEE...|$|R
40|$|This paper proposes and evaluates {{variance}} reduction {{techniques for}} e#cient estimation of portfolio <b>loss</b> <b>probabilities</b> using Monte Carlo simulation. Precise estimation of <b>loss</b> <b>probabilities</b> {{is essential to}} calculating value-at-risk, which is simply a percentile of the loss distribution. The methods we develop build on delta-gamma approximations to changes in portfolio value. The simplest way to use such approximations for variance reduction employs them as control variates; we show, however, that far greater variance reduction is possible if the approximations are used {{as a basis for}} importance sampling, stratified sampling, or combinations of the two. This is especially true in estimating very small <b>loss</b> <b>probabilities.</b> 1. 1 Introduction Value-at-Risk (VAR) has become an important measure for estimating and managing portfolio risk [Jorion 1997, Wilson 1999]. VAR is defined as a certain quantile of the change in a portfolio's value during a specified holding period. To be more specific [...] ...|$|R
30|$|Given a lossy network, the {{objective}} function yields {{a set of}} paths that has the minimum number of channel uses {{in the case of}} equal link <b>loss</b> <b>probabilities.</b>|$|R
40|$|It {{has been}} {{reported}} that IP packet traffic exhibits the self-similar [...] . This paper studies the time-scale impact on the <b>loss</b> <b>probability</b> of MMPP/D/ 1 /K system where the MMPP is generated so as to match the variance of the self-similar process over specified time-scales. We investigate the <b>loss</b> <b>probability</b> in terms of system size, Hurst parameters and time-scales. We also compare the <b>loss</b> <b>probability</b> of resulting MMPP/D/ 1 /K with simulation. Numerical results show that the <b>loss</b> <b>probability</b> of MMPP/D/ 1 /K are not significantly affected by time-scale and that the <b>loss</b> <b>probability</b> is well approximated with resulting MMPP/D/ 1 /K...|$|E
30|$|Packet <b>loss</b> <b>probability</b> {{computation}} also differentiates between stations interfering from {{transmission range}} and carrier sense range and models them separately for accurate computation of packet <b>loss</b> <b>probability.</b>|$|E
3000|$|We model {{conditional}} packet <b>loss</b> <b>probability</b> p of any station i in {{an arbitrary}} network. Conditional packet <b>loss</b> <b>probability</b> {{is the most}} critical and complicated variable to be computed for predicting per-flow throughout in multi-hop WMN. Previous literature ignored comprehensive behavior of CSMA based MAC protocol and geometric location of the interfering links, and both these reasons cause stations to have large values of packet <b>loss</b> <b>probability</b> p. Conditional packet <b>loss</b> <b>probability</b> depends on geometric configuration of flows in the immediate neighborhood. When all the stations are within transmission range of each other, then DCF is able to coordinate among stations and transmission attempts are within well defined time durations. Conditional packet <b>loss</b> <b>probability</b> of such scenario is given by 1 −(1 −τ) [...]...|$|E
40|$|This paper studies {{communication}} networks with packet or message losses due to collisions, transmission {{errors and}} finite buffer constraints. A priori and easily computable error bounds are established for simple product form approximations. These approximations {{are based on}} ignoring and/or bounding <b>loss</b> <b>probabilities.</b> Two extreme situations are considered: (i) Networks with infinite capacities but s tate dependent loss prob-abilities. (ii) Networks with finite capacities (buffers) and losses due to sa tu-rated buffers. The error bounds are of order | 3, thought of as small, when: (i) The <b>loss</b> <b>probabilities</b> are uniformly bounded by | 3, or (ii) The steady state probability of capacity excess is of order 0. The results provide formal justification for practical engineering and seem of interest for further extension to more complex communication networks. Key-words Packet switching « circuit switching • communication networks « <b>loss</b> <b>probabilities</b> * product form approximations • er ror bounds. Acknowledgement The research of this paper has been {{supported by a grant}} from the "Centr...|$|R
3000|$|... 2 {{denote the}} {{corresponding}} packet <b>loss</b> <b>probabilities</b> on links S-A, A-D, S-B and B-D, respectively. The max-flow capacities of the parallel two paths {{can then be}} expressed as C [...]...|$|R
40|$|Our {{purpose in}} {{this paper is to}} obtain the {{distribution}} of the number of lost packets within a sequence of n consecutive packet arrivals into a finite buffer M/M/ 1 queue. We obtain explicit expressions for the multi-dimensional generating function of these probabilities based on a recursive scheme recently introduced by Cidon et al. [1]. We then analyze the <b>loss</b> <b>probabilities</b> of a whole message, and analyze the effect of adding redundant packets. We show that in both heavy traffic as well as in light traffic conditions, adding redundant packets results in decreasing the message <b>loss</b> <b>probabilities...</b>|$|R
40|$|Abstract. This paper {{studies the}} impact of long-range-dependent (LRD) traffic on the {{performance}} of a network multiplexer. The network multiplexer is characterized by a multiplexing queue with a finite buffer and an M/G/ ∞ input process. Our analysis expresses the <b>loss</b> <b>probability</b> bounds using a simple relationship between <b>loss</b> <b>probability</b> and buffer full probability. Our analysis also derives an exact expression for the buffer full probability and consequently calculates the <b>loss</b> <b>probability</b> bounds with excellent precision. Through numerical calculations and simulation examples, we show that existing asymptotic analyses lack the precision for calculating the <b>loss</b> <b>probability</b> over realistic ranges of buffer capacity values. We also show that existing asymptotic analyses may significantly overestimate the <b>loss</b> <b>probability</b> and that designing networks using our analysis achieves efficient resource utilization...|$|E
3000|$|..., also {{computed}} {{most complicated}} variable in an arbitrary topology that is conditional packet <b>loss</b> <b>probability</b> p. As mentioned earlier that computation of packet <b>loss</b> <b>probability</b> depends on geometric {{configuration of the}} stations in the network and computation of conditional packet <b>loss</b> <b>probability</b> in [1] depends on two flow analysis in [2] and both approaches do not differentiate between interfering links in transmission and carrier sense range.|$|E
40|$|In this paper, we {{consider}} a lightpath establishment policy for WDM networks with full-range wavelength conversion capability. The optimal policy considered here {{is such that}} the service differentiation for the connection <b>loss</b> <b>probability</b> is provided while the total connection <b>loss</b> <b>probability</b> is small. We derive the optimal policy for the lightpath establishment based on Markov Decision Process (MDP). In numerical examples, we show that the optimal lightpath establishment policy can provide the service differentiation for the connection <b>loss</b> <b>probability,</b> decreasing the total connection <b>loss</b> <b>probability.</b> We also show {{the effectiveness of the}} optimum lightpath establishment policy in a uni-directional ring network. [URL]...|$|E
40|$|This paper {{investigates the}} notion of changes in {{ambiguity}} over <b>loss</b> <b>probabilities</b> in the smooth ambiguity model developed by Klibanoff, Marinacci and Mukerji (2005). Changes in ambiguity over <b>loss</b> <b>probabilities</b> are expressed through the specific concept of stochastic dominance of order n defined by Ekern (1980). We characterize conditions on the function capturing attitudes towards ambiguity under which an individual always considers one situation to be more ambiguous than another in a model of two states of nature. We propose an intuitive interpretation of the properties of this function in terms of preferences for harms disaggregation over probabilities, also labelled ambiguity apportionment...|$|R
3000|$|We now {{consider}} using only one path for transmission of M source packets. Packets from S are all sent to D via A where the <b>loss</b> <b>probabilities</b> {{of the two}} hops are ε [...]...|$|R
40|$|In this thesis, a queuing {{model for}} an Asynchronous Transfer Mode (ATM) based Multiplexing node is developed. In this model, {{it is assumed}} that no cell queuing is done within the {{multiplexer}} for the purpose of transmission at a later time. This introduces higher cell loss but simplifies the multiplexing (switching) algorithm which is key to ATM. This also gives constant cell delays per network node. The total network delay suffered by a cell is the sum of propagation delay, and cell transmission and buffering delay within each node the cell traverses. A closed form analytical solution is derived for the above described model and ATM cell <b>loss</b> <b>probabilities</b> (or cell blocking probabilities) are computed for a multiplexer assuming identical sources at the input. This enables us to estimate the bandwidth gain for various levels of cell <b>loss</b> <b>probabilities.</b> The model is also extended to include non-identical traffic sources and multimedia sources at the input of the multiplexer. The cell <b>loss</b> <b>probabilities</b> are studied for these cases as well. (Abstract shortened by UMI. ...|$|R
40|$|In this paper, we {{consider}} QoS-guaranteed wavelength allocation for WDM networks with limited-range wavelength conversion. In the wavelength allocation, the pre-determined number of wavelengths are allocated to each QoS class {{depending on the}} required <b>loss</b> <b>probability.</b> We evaluate the connection <b>loss</b> <b>probability</b> of each QoS class for single-hop and ring networks using continuous-time Markov chain and simulation. Numerical examples show that the proposed wavelength allocation method can provide significantly small connection <b>loss</b> <b>probability</b> for the highest priority class...|$|E
40|$|It is {{well known}} that the fitting methods based on the second-order {{statistics}} of counts for the arrival process are not su#cient for predicting the performance of the queueing system with self-similar input. However recent studies have revealed that the <b>loss</b> <b>probability</b> of finite queuing system can be well approximated by the Markovian input models. This paper studies the time-scale impact on the <b>loss</b> <b>probability</b> of MMPP/D/ 1 /K system where the MMPP is generated so as to match the variance of the self-similar process over specified time-scales. We investigate the <b>loss</b> <b>probability</b> changing system size, Hurst parameters and time-scales. Numerical examples show that the <b>loss</b> <b>probability</b> of MMPP/D/ 1 /K are not significantly a#ected by the time-scale...|$|E
40|$|We {{investigate}} whether a profit-maximizing insurer {{with the opportunity}} to modify the <b>loss</b> <b>probability</b> will engage in loss prevention or instead spend effort to increase the <b>loss</b> <b>probability.</b> First we study this question within a traditional expected utility framework; then we apply Koszegi and Rabin's (2006, 2007) loss aversion model to account for reference-dependence in consumer preferences. Largely independent of the adopted framework, we find that the profit-maximizing <b>loss</b> <b>probability</b> for many commonly used parameterizations is close to 1 / 2. So in cases where the initial <b>loss</b> <b>probability</b> is low, insurers will have an incentive to increase it. This qualifies appeals to grant insurers market power to incentivize them to engage in loss prevention...|$|E
40|$|Abstract—In this paper, {{we develop}} a {{performance}} {{model of a}} cell in a wireless communication network where the effect of handoff arrival {{and the use of}} guard channels is inlcuded. Fast recursive formulas for the <b>loss</b> <b>probabilities</b> of new calls and handoff calls are developed. Monotonicity properties of the <b>loss</b> <b>probabilities</b> are proven. Algorithms to determine the optimal number of guard channels and the optimal number of channels are given. Finally, a fixed-point iteration scheme is developed {{in order to determine the}} handoff arrival rate into a cell. The uniqueness of the fixed point is shown. Index Terms—Channel allocation, Markov models, optimization, performance modeling, wireless cellular networks...|$|R
40|$|This paper describes,analyzes and evaluates an {{algorithm}} for estimating portfolio <b>loss</b> <b>probabilities</b> using Monte Carlo simulation. Obtaining accurate {{estimates of}} such <b>loss</b> <b>probabilities</b> {{is essential to}} calculating value-at-risk,which is a quantile of the loss distribution. The method employs a quadratic (’’delta-gamma’’) approximation to the change in portfolio value to guide the selection of effective variance reduction techniques; specifically importance sampling and stratified sampling. If the approximation is exact,then the importance sampling is shown to be asymptotically optimal. Numerical results indicate that an appropriate combination of importance sampling and stratified sampling can result in large variance reductions when estimating the probability of large portfolio losses...|$|R
40|$|Abstract—This paper {{addresses}} the optimization of multiple description quantizers for stochastic and time-varying <b>loss</b> <b>probabilities.</b> We propose new iterative de-sign methods, based on rate-distortion curves, for these cases. Experimental {{results indicate that}} significant fidelity improvements are possible using these methods. I...|$|R
