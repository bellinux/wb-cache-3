2|43|Public
40|$|This {{primarily}} methodological article makes {{a proposition}} for linguistic exploration of textual resources {{available through the}} Google Scholar search engine. These resources (Google Scholar virtual corpus) are significantly larger than any existing corpus of academic writing. Google Scholar, however, was not designed for <b>linguistic</b> <b>searches</b> and special attention therefore needs {{to be paid to}} maximising its effectiveness in corpus linguistics research. The article discusses the search capacity of Google Scholar and compares the Google Scholar virtual corpus with the largest traditional corpus of written academic English, COCA - academic. Finally, the article offers a case study on the as-author-reporting verb structure (and its modifications). The study demonstrates that Google Scholar can be employed effectively in EAP research offering us new insights into reporting practices in two disciplines, Applied Linguistics and Physics, which were chosen for comparison. The benefits of using Google Scholar virtual corpus are the following: 1) wide representativeness of written academic language, 2) possibility of capturing subtle variation in academic patterns, and 3) possibility of comparing linguistic patterns across different academic fields. © 2012 Elsevier Ltd...|$|E
40|$|A {{plethora of}} studies from various {{linguistic}} trends have {{been devoted to}} causative constructions (e. g. Shibatani 1975, 1976, Ritter & Rosen 1993, Song 1996). The specificity {{of this study is}} two-fold. First, it does not deal with ‘the causative construction’ in general, but exclusively focuses on four causative verbs, viz. 'cause', 'get', 'have' and 'make', the most frequent periphrastic causatives with no or little semantic content on their own, apart from the causative meaning of ‘bringing about’ (unlike for instance 'force', which, besides causation, also clearly expresses coercion). Although those verbs are dealt with in most grammars, no satisfactory account is given of the circumstances in which each of them should be used, nor the consequences of the use of a particular type of complement (infinitive, past participle or present participle). The second distinguishing feature {{of this study is that}} it is based on corpus data, so that it should give a better idea of how causative verbs behave in authentic present-day English (precise meaning, frequency, diatypic variation, registers, combinatorial properties, etc.). 	This presentation will fall into two parts. First, I will show how causative constructions such as 'John made her laugh' or 'I had my watch repaired' can be retrieved from corpora (semi-) automatically. More specifically, I will compare the results achieved with a concordancer like XKwic, a piece of software developed at the University of Stuttgart which can carry out highly refined and specialised <b>linguistic</b> <b>searches,</b> and with ICECUP, the program designed to query the International Corpus of English (ICE) and working on the basis of ‘Fuzzy Tree Fragments’ representing the grammatical structure of sentences. This comparison will highlight the fact that, although ICECUP has higher precision and recall rates, as long as it cannot be used in conjunction with other (larger) corpora, a more ‘classical’ concordancer will be needed to thoroughly investigate relatively rare phenomena such as periphrastic causative constructions. 	Secondly, I will present the preliminary results reached on the basis of ICE-GB (1, 000, 000 -word corpus), the British component of ICE. Following the functional ‘one meaning, one form’ principle, I put forward the hypothesis that there must be differences between the four causative verbs 'cause', 'get', 'have' and 'make'. In order to test that hypothesis, the causative sentences retrieved were examined both quantitatively and qualitatively with respect to a number of syntactic, stylistic and semantic parameters. The syntactic survey focused on the types of structures that are available for each causative (bare infinitive or to-infinitive, present participle, main clause or subclause passivization). From a stylistic point of view, I investigated whether the four verbs and their non-finite complements were stylistically differentiated by comparing their frequencies in speech and writing, as well as in the different genres of ICE (e. g. novels/stories, business letters, face-to-face conversations, etc.). Semantically speaking, finally, I followed Fillmore and his theory of Frame Semantics (cf. the FrameNet Project) in viewing causative constructions as made up of three ‘Frame Elements’, viz. 	The explosion (Cause) caused the temperature (Affected) to rise (Effect). Each Frame Element can be described in terms of various features, such as animacy of the Cause and the Affected, volitionality of the Effect, or degree of coercion involved. The semantic study also includes a collocational analysis, whose aim is to determine the preferential lexical company kept by each causative verb. Only the most significant results will be outlined here. It should be emphasised, however, that those results are based on a relatively small number of instances (40 constructions with 'cause', 101 with 'get', 77 with 'have' and 150 with 'make') and therefore need to be substantiated by further and more extended research...|$|E
40|$|In this paper, we {{illustrate}} how Internet documents can be automatically analyzed {{in order to}} capture the content of a document in a more detailed manner than usual. The result of the document analysis is called an abstract, and will it {{be used as a}} <b>linguistic</b> <b>search</b> index for the Internet search engine, GETESS. We show how the linguistic analysis system SMES can be used with a Harvest-based search engine for constructing a <b>linguistic</b> <b>search</b> index. Further, we denote how the linguistic index can be exploited for answering user search inquiries...|$|R
40|$|Acoustic {{features}} {{derived from}} the short time magnitude and phase spectrum provide complementary information. In this paper, we discuss the significance of incorporating this diverse information into the <b>linguistic</b> <b>search</b> space for syllable based speech recog-nition. The diversity of group delay acoustic features computed from the phase spectrum, and MFCC computed from the magnitude spectrum, is first illustrated in a lower dimensional feature space. Motivated by this diversity of information in the acoustic feature space, we derive syllable-feature pairs. The selection of syllable-feature pairs is based on isolated syllable recognition results, com-puted apriori using the two acoustic feature streams. During the recognition process, based on the syllable-feature pair information likelihoods are appropriately weighted using a weighted likelihood scheme. The syllable lattice is now rescored using these weighted syllable-feature pairs in the <b>linguistic</b> <b>search</b> space. This technique of appropriately weighting the relevant acoustic feature for each syllable during the decoding process in the <b>linguistic</b> <b>search</b> space, yields reduced word error rate (WER), for experiments conducted on the TIMIT and the DBIL databases. 1...|$|R
50|$|After the bankruptcy, Nuance Communications (known then as ScanSoft) {{acquired}} all of {{the speech}} technologies. The revenues of the company grew sharply from $17.1 million in third quarter of 2001, to $216 million in Q3 2008. Vantage Learning acquired {{all of the}} proofing, spelling, and <b>linguistic</b> <b>search</b> technologies.|$|R
50|$|To {{represent}} {{symbols in}} Nengo, SPA is used. Many {{aspects of human}} cognition are easier to model using symbols. In Nengo, these are presented as vectors {{with a set of}} operations associated to them. These vectors and their operations are called SPA. SPA has been used to model human <b>linguistic</b> <b>search</b> and task planning.|$|R
40|$|In {{this work}} we {{describe}} {{a new approach}} for morphological disambiguation to enable linguistic indexing for Hebrew search systems. We describe a Hebrew Morphological Disambiguator (HMD or Hemed for short) based on statistical data gathered from large Hebrew corpora. We show how to integrate HMD with a search engine to enable <b>linguistic</b> <b>search</b> for Hebrew. We report some experimental results demonstrating the the superiority of <b>linguistic</b> <b>search</b> over string-matching search, and the contribution of morphological disambiguation {{to the quality of}} search result. 1 Background and Motivation With the advent of the Web, more and more textual information is being made available on line, and Information Retrieval (IR) systems are becoming of crucial importance to search through the vast amount of information. Most state-ofthe -art IR systems operate on a canonical representation of documents called a profile that consists of a list (or a vector in the commonly used vector space model [ [...] ...|$|R
50|$|TREX {{supports}} {{various kinds}} of text search, including exact search, boolean <b>search,</b> wildcard <b>search,</b> <b>linguistic</b> <b>search</b> (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency (tf-idf) weighting, and results can include snippets with the search terms highlighted.|$|R
50|$|AlphaSense, Inc. {{was founded}} by Jack Kokko and Raj Neervannan in 2008 to apply {{advanced}} <b>linguistic</b> <b>search</b> technology to assist investment professionals, to find information in company disclosures. In 2010, AlphaSense launched its search engine utilizing natural language processing to search through all major filings, earnings calls, conference transcripts and other related documents. In 2012, the search engine added company investor relations presentations for the Russell 3000 constituents to their database of searchable content.|$|R
50|$|AlphaSense is a specialized, web-based {{financial}} and company search engine providing users {{with the ability}} to search, navigate, annotate and analyze corporate filings and other research documents. Users can set alerts on saved searches and receive instant notification when research, news, press announcements or new data points are available on companies, industries or any keywords of interest. The product uses a blend of proprietary <b>linguistic</b> <b>search</b> and natural language processing algorithms to index every word and passage of text, as well as Machine Learning to expand searches to include relevant synonyms.|$|R
40|$|Although several {{syntactically}} annotated corpora (or treebanks) {{exist for}} Dutch, they are seldomly used for descriptive linguistic research {{because there are}} no easy-to-use exploitation tools available. This demonstration paper describes GrETEL, a <b>linguistic</b> <b>search</b> engine ([URL] that enables non-technical users to consult treebanks in a user-friendly way. Instead of a formal search expression, a natural language example is used as input to the system, allowing users to search for similar constructions as the example they provide. In the first version of GrETEL, only written Dutch (LASSY) was included. Based on user requests we have now included the Spoken Dutch Corpus (CGN) as well. status: publishe...|$|R
40|$|Compared to well-resourced {{languages}} such as English and Dutch, {{natural language}} processing (NLP) tools for Afrikaans are still not abundant. In {{the context of the}} AfriBooms project, KU Leuven and the North-West University collaborated to develop a first, small treebank, a dependency parser, and an easy to use online <b>linguistic</b> <b>search</b> engine for Afrikaans for use by researchers and students in the humanities and social sciences. The search tool is based on a similar development for Dutch, i. e. GrETEL, a user-friendly search engine which allows users to query a treebank by means of a natural language example instead of a formal search instruction. status: publishe...|$|R
40|$|In this poster/demo we {{will present}} an online {{scholarly}} edition of La queste del saint Graal (The Quest for the Holy Grail) {{based on a}} manuscript of Lyons public library (Lyon, BM, P. A. 77) and built in the Textometry platform (TXM). The particularity of this edition is that it combines rich paleographic and philological data (including digital photographs, various layers of transcription, translation in modern French and editorial notes) with advanced <b>linguistic</b> <b>search</b> and analysis tools provided by textometric software. Despite some damage (torn miniature in the beginning, missing folios in the end), Lyons manuscript {{is considered to be}} one of the bes...|$|R
5000|$|Another {{aspect of}} the Quran's symmetry, which was {{discovered}} using <b>linguistic</b> computer <b>searching</b> algorithms (Quran Corpus, an open-source software), includes {{the number of times}} certain words and their forms are used in the whole Quran. Some of these symmetrical patterns include: ...|$|R
40|$|Today, the {{majority}} of websites are accessed through search engines. It is therefore fundamental that their managers and designers ensure that they are well positioned within the search results, {{from the point of}} view of marketing and in order to provide the best service to their users. In this article we present the major results found in our analysis of web positioning and the improvement proposals, which can be applied to other <b>linguistic</b> <b>search</b> tools such as online dictionaries, thesauri, etc. We employed an analysis methodology involving an empirical observation of the factors considered by search engines (see the following section) and a multivariate statistical analysis to relate the various aspects analyzed with the positioning obtained for each keyword...|$|R
50|$|<b>Linguistic</b> Knowledge for <b>Search</b> Relevance Improvement. Proceedings of Joint {{conference in}} {{knowledge-based}} software Engineering JCKBSE'06 IOS Press, 2006.|$|R
40|$|This paper {{discusses}} the representativeness, coextensitivity and scientific accountability of corpus-based grammatical descriptions of previously unresearched languages. While a grammatical {{description of a}} previously unresearched language can hardly be representative {{for any kind of}} its varieties, it can be adequate n coextensitivity if it covers the linguistic phenomena presented in the corpus. In order to allow other researchers to retrieve the examples in their context and check the analysis, the corpus should not only contain text collections, but also the elicited data, provide metadata and be accessible to other researchers. Scientific accountability, however, can only be achieved, if the description facilitates the replicability of the analysis, which presupposes that the authors’ corpus <b>linguistic</b> <b>search</b> methods are documented, so that the readers can find other, if not all examples for the described phenomena, and scrutinize the search methods, the analysis and the description. As is illustrated in this paper, a suitable query language for this kind of scientific grammatical analysis and description are the so-called regular expressions which are implemented in the annotation tool ELAN. National Foreign Language Resource Cente...|$|R
40|$|We {{propose to}} build a lingustic search engine, similar in overall design to Google and Altavista but meeting the {{specifications}} and requirements of researchers into language. Impressive effect to smaller corpora {{could be applied to}} the web so that web searches could be specified in terms of linguistically interesting units such as lemmas, word classes, and constituents (e. g. noun phrase) rather than strings. Thesauruses and lexicons could be developed directly from the web. The way would be open for further anatomizing of web text types and domains, both a topic of interest in itself and one where strategies would be needed so that web-based lexical resources could be developed for specific text types or domains, or so that the biases of the web could be countered to provide `general languages' and `sublanguage' resources from the web. All of this can potentially be done for all of the many languages for which there is ample data on the web. The web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is potentially a fabulous linguists' playground. The <b>Linguistic</b> <b>Search</b> Engine will bring th...|$|R
50|$|After {{studying}} philology at the Erfurt {{academy and}} at Leiden, he travelled {{in order to}} increase his <b>linguistic</b> knowledge. While <b>searching</b> in Rome for some documents {{at the request of the}} Swedish Court (1649), he became friends with Abba Gorgoryos, a monk from the Ethiopian province of Amhara, and acquired from him an intimate knowledge of the Ethiopian language.|$|R
40|$|Today, the {{majority}} of websites are accessed through search engines. It is therefore fundamental that their managers and designers ensure that they are well positioned within the search results, {{from the point of}} view of marketing and in order to provide the best service to their users. The position that a website takes in the list of results when a user carries out a search is thus a very relevant aspect for persons responsible for websites. The actions necessary for improvement are explained as “web positioning” (Codina, 2004; Codina, Marcos 2005; Arbildi, 2005). The centerpiece is in asking yourself which words the potential users are likely to use in their searches; once this has been determined, you can make legitimate use of the optimization techniques so that a specific website appears in a good position when the users search for information related to the contents of that website. The websites that host terminology databases can -and should make- use of positioning improvement techniques to help potential users find these resources using search engines. Our study took as a sample ten terminology databases having free access online, presenting multilingual data, and pertaining to different subject areas. In this article we present the major results found in our analysis of web positioning and the improvement proposals, which can be applied to other <b>linguistic</b> <b>search</b> tools such as online dictionaries, thesauri, etc. We employed an analysis methodology involving an empirical observation of the factors considered by search engines and a multivariate statistical analysis to relate the various aspects analyzed with the positioning obtained for each keyword...|$|R
30|$|As Biro (2016, 110) explains, “Language {{ideologies}} {{within the}} discussion of linguistic landscapes refers {{to a set of}} shared attitudes and beliefs of the given community about language or languages.” Leeman and Modan (2009) view the linguistic landscape as ideologically charged and socially constructed representations of <b>linguistic</b> landscape <b>search.</b> As Moriarty (2012, 74) claims, “To this end, the linguistic landscape is seen not only to be reflective of language ideologies, but also a space where language ideologies can be indexed and performed, thus providing an apt tool for dealing with the multimodal nature of language ideologies.” In the rest of this section, I will survey studies on schoolscapes in various educational contexts.|$|R
40|$|Explosive {{growth of}} the World Wide Web {{as well as its}} {{heterogeneity}} call for powerful and easy to use search tools capable to provide the user with a moderate number of relevant answers. This paper presents analysis of key aspects of recently developed Web search methods and tools: visual representation of subject trees, interactive user interfaces, <b>linguistic</b> approaches, image <b>search,</b> ranking and grouping of search results, database search, and scientific information retrieval. Current trends in Web search include topics such as exploiting Web hyperlinking structure, natural language processing, software agents, influence of XML markup language on search efficiency, and WAP search engines...|$|R
40|$|In this article, {{we present}} interHist, a compact {{visualization}} for the interactive exploration of results to complex corpus queries. Integrated with a search interface to the PAISÀ corpus of Italian web texts, interHist aims at facilitating {{the exploration of}} large results sets to <b>linguistic</b> corpus <b>searches.</b> This objective is approached by providing an interactive visual overview of the data, which supports the user-steered navigation by means of interactive filtering. It allows to dynamically switch between an overview on the data and a detailed view on results in their immediate textual context, thus helping to detect and inspect relevant hits more efficiently. We provide background information on corpus linguistics and related work on visualizations for language and linguistic data. We introduce the architecture of interHist, by detailing the data structure it relies on, describing the visualization design and providing technical details of the implementation and its integration with the corpus querying environment. Finally, we illustrate its usage by presenting a use case {{for the analysis of}} the composition of Italian noun phrases...|$|R
40|$|We {{will present}} the {{analysis}} of a Spanish prosody database by estimating the parameters of Fujisaki's model for FO contours. These parameters are classified attending to linguistic features and they form the analysis database. When synthesizing FO contours we extract the linguistic features from the text and perform a k-Nearest Neighbour <b>search.</b> <b>Linguistic</b> feature comparison distance is trained {{using data from the}} prosody database. To avoid artifacts we perform a rule-base filtering on synthesis parameters. The results of our evaluation test show that the proposed system is significantly better than the previous neural network approach. This evaluation confirms the ability of Fujisaki's model to represent prosody information based on linguistic features...|$|R
40|$|This paper {{describes}} a new architecture for large vocabulary continuous speech recognition (LVCSR), {{which will be}} developed within the project FLaVoR (Flexible Large Vocabulary Recognition). The proposed architecture abandons the standard all-in-one search strategy with integrated acoustic, lexical and language model information. Instead, a modular framework is proposed which allows for the integration of more complex <b>linguistic</b> components. The <b>search</b> process consists of two layers. First, a pure acoustic-phonemic search generates a dense phoneme network enriched with meta-data. Then, {{the output of the}} first layer is used by sophisticated language technology components for word decoding in the second layer. Preliminary experiments prove the feasibility of the approach...|$|R
40|$|Web {{search queries}} have been {{observed}} to exhibit properties of a rudimentary language system, distinct from the mother language from which words of the queries are drawn. It has been hy-pothesized that the language of search queries is fast growing in complexity, reflected in the steady increase of query lengths over the years. In this research, we make the first attempts to quantify change in the <b>linguistic</b> structure of <b>search</b> queries by examining large query logs spaced four years apart. We adopt a multi-pronged approach and analyze query structure from three different perspectives, namely, language models, complex networks and positional pref-erences of words. All experimental findings confirm that the linguistic structure of Web search queries is indeed evolving. 1...|$|R
40|$|AbstractThe {{problem of}} {{linguistic}} competences of higher school personnel {{as one of}} the main problems, not allowing Russian higher education school to count on leading positions in the world university rankings, is discussed in this paper. Within the framework of the article the authors formulate a possible approach to the establishment of the strategy of linguistic competences development in a higher school. The authors are implementing this work on the basis of National research Tomsk polytechnic university (NRTPU). The methodology of the analysis of possibilities of <b>linguistic</b> competences development, <b>search</b> of the mechanism of measures implementation by their development, and identification of “gaps” or difficulties in implementation of these measures are presented in this article. For solution of the assigned objectives the parameterization of measure characteristics and their subsequent statistical processing are applied...|$|R
40|$|Creating stimuli for perceptual {{experiments}} in intonation research involves manipulation of pitch contours extracted from spoken utterances. Difficulties arise when {{changes in the}} contour shape need to be applied globally and smoothly in the whole pitch curve. Moreover, {{it is hard to}} relate a gradual modification in some contour trait to its perceptual counterpart. In this paper we propose a novel approach to stimuli manipulation that is based on an extension of Principal Component Analysis (PCA). Starting from a corpus of pitch curves a parametric description of the principal variation in the curve set is obtained. This allows to locate clusters in this parameter space that are related to <b>linguistic</b> categories. The <b>search</b> for pitch curves with desired perceptual characteristics is carried out by choosing convenient point locations with respect to the relevant clusters. We illustrate this approach in a case study on question/ statement opposition in Neapolitan Italian...|$|R
40|$|International audienceGreek {{documentary}} papyri form {{an important}} direct source for Ancient Greek. It {{has been exploited}} surprisingly little in Greek linguistics {{due to a lack}} of good tools for <b>searching</b> <b>linguistic</b> structures. This article presents a new tool and digital platform, “Sematia”, which enables transforming the digital texts available in TEI EpiDoc XML format to a format which can be morphologically and syntactically annotated (treebanked), and where the user can add new metadata concerning the text type, writer and handwriting of each act of writing. An important aspect in this process is to take into account the original surviving writing vs. the standardization of language and supplements made by the editors. This is performed by creating two different layers of the same text. The platform is in its early development phase. Ongoing and future developments, such as tagging linguistic variation phenomena as well as queries performed within Sematia, are discussed at the end of the article...|$|R
40|$|Abstract—Distributed Hash Tables (DHTs) {{establish}} a struc-tured Peer-to-Peer (P 2 P) overlay network by applying proactive routing algorithms. Due to their well defined structure {{this class of}} P 2 P protocols is able to locate any content in the system within {{a limited number of}} hops. However, basic DHT algorithms are limited to queries for content that exactly matches the search term. In this paper, we propose a Prefix-based Multi-Attribute Key-word Search (PriMA KeyS) that is specially designed for searching persons in a distributed phone book and similar applications. Our architecture is fully distributed and pays special attention to a balanced storage load distribution as well as low network traffic. Hierarchical identifiers generated from multiple keywords help to reduce the load on nodes that host common keywords. Additionally, a locality preserving hash function enables prefix-based queries. An extensive <b>linguistic</b> analysis of <b>search</b> keywords is carried out to select optimum design parameters. Using sample queries we show that our system can efficiently handle both detailed and unspecific queries. I...|$|R
40|$|This {{systematic}} {{review of the}} literature on the study of writing disorders in Parkinson disease (PD) evaluated the available evidence with respect to research about writing and related cognitive processes in this population. A critical analysis of the literature was carried out, where the databases from Pubmed, Medline, Hinari, PsycArticles, E Journals and Science Direct, were accessed using as search criteria the publication date range between January 1, 1999 until October 1, 2009, with the exception of an article of 1997 which was included because of the contribution it provides on <b>linguistic</b> analysis. <b>Search</b> indicators were: clinical trials, reviews, meta-analysis, controlled clinical trials and discussions. A total of 39 articles were recorded, among which 15 were chosen based on their relevance and relationship with the topic of writing in PD. Six of these had taken into account linguistic variables in exclusively oral tasks; only one of them clearly describes the correlation of the grammatical structure of sentences within the writing linguistic tasks. Findings show that within the oral/linguistic studies of subjects with PD there is a predominance of correlations between neurobiological factors and alterations of oral language in comprehension tasks of semantics, syntax, grammatical and ungrammatical sentences, use of words in discourse and interaction in context. Studies of writing focus on mechanical aspects, including changes in size and modification of strokes in the handwriting of PD patients. Only one study is directly related to a linguistic analysis of writing, where parameters such as sentence length, syntactic complexity and lexical-semantic content were considered. The writing process in PD patients has been analyzed from the mechanical disturbance, that is, from the physical form of strokes and has been correlated with impaired motor functions underlying the main motor dysfunction in PD, and although an associated cognitive impairment has been acknowledged, this correlation has not extended to the writing process...|$|R
40|$|Purpose. The {{article is}} devoted to the {{pressing}} issue of regional identity preservation under conditions of linguoculturological globalization in modern society. The purpose of our research is to identify verbal means of expressing regional self-identification of narrators in the memorial context. The object is the self-identification of the narrator in memorial genre and the subject is verbal means of expressing regional self-identification. Methodology and methods of work. An appeal to the memorial genre, <b>searching</b> <b>linguistic</b> methods of the regional authors self-identity is considered in a context of regional linguistic from the perspective of communicative discourse approach; discursive and contextual methods are used. Results. The article theoretically describes the memorat {{from the standpoint of the}} speech genres theory. The research shows the narrators through the actualization of language tools associate themselves with their native region, the result being the formation of regional self-identification. In the era of linguistic globalization it is important to study regional self-identification, in particular, in the context of the memorial speech genre. Practical implications. Both the article materials and results can be applied in the educational process of reading special courses on regional linguistic issues...|$|R
40|$|The {{goal of the}} {{research}} described here is to present an approach for automating the detection and the extraction of meaning from text using a range of linguistic and ontological techniques, concepts such as the lexico-semantic functions proposed in Meaning-Text Theory by Mel?cuk {{and the concept of}} the context. This is motivated, by the fact that, on one hand, these functions enable a better modeling and formalization of meaning. On the other hand, the concept of the context is not well supported in many important and essential domains such as in Natural Language Processing (NLP), semantic data analysis and knowledge management. Our work consists here to integrate these elements in an automatic and context-based approach we called PROSEM (SEMantic PROjection), based on an extended model of the context, that improves the detection of meaning in a text. This approach can be used in many areas related to the meaning and the context such as semantic data indexing, <b>linguistic</b> or semantic <b>search,</b> contextual information extraction, multilingualism, etc. In this paper, we present the main elements of the approach and three typical applications processes enhanced by PROSEM. An estimate of the performances of the approach will conclude the article...|$|R
40|$|In {{this study}} I have {{examined}} the French nominal multi-word unit mise en N and its many variants in a dynamic perspective {{on the basis of}} data collected from French language online newspapers between 2006 and 2007. In its most general sense, mise en N means ‘the action of putting somewhere, or the result of this action’ and is closely related to the verb mettre and the multi-word verb construction mettre en N (‘to put’/’to put somewhere’). It is a common and productive lexical form used in specialised as well as in general language. As an old lexical construction stretching back to Latin through Middle and Old French, it has a wide range of semantic content and features, and can be interpreted in different ways. However, existing studies of mise en N often consider its variants either as fixed expressions or as intrinsically belonging to the verb category. The objective {{of this study was to}} show that the scope of mise en N is much wider than this and that its variation can be seen as a result of the dynamic properties inherent to the construction that mise en N represent. To do this, the construction was studied from a nominal perspective as a proper lexical unit, where its variants were considered as multi-word nouns denoting an action; and, due to its many variants, it was also studied through an approach which included dynamic or unstable linguistic features, by considering mise en N as a nominal multi-word construction. therefore adopted a multi-methodological approach where 247 variants of the mise en N construction were studied in a dynamic corpus of data derived from online newspapers through the <b>linguistic</b> <b>search</b> engine GlossaNet. The variants were analysed in different perspectives (semantic content, morphosyntactic properties, frequency of use) and various linguistic and lexicographic sources (traditional dictionaries, online and dynamic dictionaries, term bases, the Web) were used to complement and evaluate the corpus findings. The analysis was based on a set of morphosyntactic, morphological and semantic evaluation criteria, and was systematised in an analyse table inspired by the lexicon grammar model developed by Maurice Gross (1975). Our analyses resulted in 271 identified mise en N variants, that is variants identified with a specific meaning and set of properties and thus provided with an individual entry in the table. Our findings show that mise en N can be seen as having a double lexical status: mise en N as a fixed multi-word expression (unité figée); and mise en N as a fixed multi-word construction (construction figée). As a fixed expression, a mise en N variant has become a part of the lexicon as a regular lexical unit, with a fixed form and meaning. The fixed mise en N variants were grouped in a separate category labeled locutions figées (LF). As a fixed construction, I consider mise en N as a lexical pattern that can be used to create other variants, and that can perhaps also explain its continuous variation and productivity. In addition it can be considered to have a more functional role which might indicate an ongoing grammaticalisation process. The analyses identified four types of mise en N constructions: type A where mise en N is linked to the verb mettre in a locative sense (‘to put (something) somewhere’), ex. mise en pots; type B including mise en N variants linked to the causative verbs rendre/faire devenir (‘to make (someone/something) become’), ex. mise en accessibilité; type C including variants linked to the factitive verb faire (‘to make (someone/something) do’), ex. mise en circulation; type D including a heterogenous group of mise en N that can be linked to a variety of verbs, ex. mise en emploi, mise en peinture, mise en tourisme. The types can also be characterised by the type of action they imply. Type A and B would have an affected object with a passive function, the affected objet of type C would be active; however, type D can be said to denote the sense of action more generally, i. e. providing the sense of action and of duration to something that is lacking it, or where the sense of action is ambiguous, as in the case of action nouns that can be interpreted as an process, an event or as the result of the action. </p...|$|R
40|$|Background: Continuity of care {{is widely}} {{acknowledged}} as a core value in family medicine. In this systematic review, we aimed {{to identify the}} instruments measuring continuity of care and to assess {{the quality of their}} measurement properties. Methods: We did a systematic review using the PubMed, Embase and PsycINFO databases, with an extensive search strategy including ‘continuity of care’, ‘coordination of care’, ‘integration of care’, ‘patient centered care’, ‘case management’ and its <b>linguistic</b> variations. We <b>searched</b> from 1995 to October 2011 and included articles describing the development and/ or evaluation of the measurement properties of instruments measuring one or more dimensions of continuity of care (1) care from the same provider who knows and follows the patient (personal continuity), (2) communication and cooperation between care providers in one care setting (team continuity), and (3) communication and cooperation between care providers in different care settings (cross-boundary continuity). We assessed the methodological quality of the measurement properties of each instrument using the COSMIN checklist. Results: We included 24 articles describing the development and/or evaluation of 21 instruments. Ten instruments measured all three dimensions of continuity of care. Instruments were developed for different groups of patients or providers. For most instruments, three or four of the six measurement properties were assessed (mostly internal consistency, content validity, structural validity and construct validity). Six instruments scored positive on the quality of a...|$|R
40|$|AbstractThe {{matter of}} the {{universal}} values that is the correlation between national, class and human, temporal and eternal, natural things and their place within the society is worth considering. So the aim under consideration is to view the system of values being spiritual ties of the civilization focusing on the English teaching process as development of the humanistic values through realization of language values by means of axiological linguistics. So the argument on the social component of English teaching process as adaptation of the individual within the society {{in the course of}} studying the foreign languages confirms the statement that axiology is closely connected, bounded up with foreign language teaching as a way of accommodating your native values with alien culture, traditions. The conclusion concerns globalisation process as confrontation of diverse cultures and their systems of values. The culture codes in this respect add up to correlation of the phraseological units (as a language unit) to values within axiological paradigm. The present review produces a complete piece of work considering the stated problem beginning with general statements on values then viewing a specific process of globalisation focusing on teaching language and special field of study – <b>linguistic</b> axiology through <b>searching</b> for axiological markers. Complex methodology is used: methods of analytical, qualitative, discourse analyses; phraseological identification and individual approach in foreign language teaching...|$|R
