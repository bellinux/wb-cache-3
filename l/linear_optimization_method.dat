16|10000|Public
40|$|International audienceWe {{pursue the}} {{development}} and application of the recently introduced <b>linear</b> <b>optimization</b> <b>method</b> for determining the optimal linear and nonlinear parameters of Jastrow-Slater wave functions in a variational Monte Carlo framework. In this approach, the optimal parameters are found iteratively by diagonalizing the Hamiltonian matrix in the space spanned by the wave function and its first-order derivatives, making use of a strong zero-variance principle. We extend the method to optimize the exponents of the basis functions, simultaneously {{with all the other}} parameters, namely, the Jastrow, configuration state function, and orbital parameters. We show that the <b>linear</b> <b>optimization</b> <b>method</b> {{can be thought of as}} a so-called augmented Hessian approach, which helps explain the robustness of the method and permits us to extend it to minimize a linear combination of the energy and the energy variance. We apply the <b>linear</b> <b>optimization</b> <b>method</b> to obtain the complete ground-state potential energy curve of the C 2 molecule up to the dissociation limit and discuss size consistency and broken spin-symmetry issues in quantum Monte Carlo calculations. We perform calculations for the first-row atoms and homonuclear diatomic molecules with fully optimized Jastrow-Slater wave functions, and we demonstrate that molecular well depths can be obtained with near chemical accuracy quite systematically at the diffusion Monte Carlo level for these systems...|$|E
40|$|We {{pursue the}} {{development}} and application of the recently-introduced <b>linear</b> <b>optimization</b> <b>method</b> for determining the optimal linear and nonlinear parameters of Jastrow-Slater wave functions in a variational Monte Carlo framework. In this approach, the optimal parameters are found iteratively by diagonalizing the Hamiltonian matrix in the space spanned by the wave function and its first-order derivatives, making use of a strong zero-variance principle. We extend the method to optimize the exponents of the basis functions, simultaneously {{with all the other}} parameters, namely the Jastrow, configuration state function and orbital parameters. We show that the <b>linear</b> <b>optimization</b> <b>method</b> {{can be thought of as}} a so-called augmented Hessian approach, which helps explain the robustness of the method and permits us to extend it to minimize a linear combination of the energy and the energy variance. We apply the <b>linear</b> <b>optimization</b> <b>method</b> to obtain the complete ground-state potential energy curve of the C_ 2 molecule up to the dissociation limit, and discuss size consistency and broken spin-symmetry issues in quantum Monte Carlo calculations. We perform calculations of the first-row atoms and homonuclear diatomic molecules with fully optimized Jastrow-Slater wave functions, and we demonstrate that molecular well depths can be obtained with near chemical accuracy quite systematically at the diffusion Monte Carlo level for these systems. Comment: 15 pages, 3 figures, to appear in Journal of Chemical Physic...|$|E
40|$|A {{successive}} <b>linear</b> <b>optimization</b> <b>method</b> {{is proposed}} to efficiently solve the on-ramp traffic control problem on urban freeway networls with user-optimal flows. A linear approximation of the implicit, nonlinear capacity constraint is formulated using the first-order derivatives {{of the link}} flows with respect to input on-ramp flows. The derivative information required for the algorithm {{is based on the}} theory of sensitivity analysis for network equilibrium problems. The performance of the algorithm is illustrated and compared with the previous iterative linear programming and assignment algorithm using numerical examples. ...|$|E
40|$|We develop explicit, {{piecewise-linear}} {{formulations of}} functions f(x) :ℝn{mapping}ℝ, n ≤ 3, that are defined on an orthogonal grid of vertex points. If mixed-integer <b>linear</b> <b>optimization</b> problems (MILPs) involving multidimensional piecewise-linear functions {{can be easily}} and efficiently solved to global optimality, then non-analytic functions {{can be used as}} an objective or constraint function for large <b>optimization</b> problems. <b>Linear</b> interpolation between fixed gridpoints {{can also be used to}} approximate generic, nonlinear functions, allowing us to approximately solve problems using mixed-integer <b>linear</b> <b>optimization</b> <b>methods.</b> Toward this end, we develop two different explicit formulations of piecewise-linear functions and discuss the consequences of integrating the formulations into an optimization problem. © Springer Science+Business Media, LLC 2009...|$|R
40|$|Linear superiorization {{considers}} {{linear programming}} problems {{but instead of}} attempting to solve them with <b>linear</b> <b>optimization</b> <b>methods</b> it employs perturbation resilient feasibility-seeking algorithms and steers them toward reduced (not necessarily minimal) target function values. The two questions that {{we set out to}} explore experimentally are (i) Does linear superiorization provide a feasible point whose linear target function value is lower than that obtained by running the same feasibility-seeking algorithm without superiorization under identical conditions? and (ii) How does linear superiorization fare in comparison with the Simplex method for solving linear programming problems? Based on our computational experiments presented here, the answers to these two questions are: "yes" and "very well", respectively. Comment: Inverse Problems, accepted for publicatio...|$|R
25|$|Linear {{programming}} (LP, {{also called}} <b>linear</b> <b>optimization)</b> is a <b>method</b> {{to achieve the}} best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).|$|R
40|$|International audienceIn this paper, a max-plus-linear model {{predictive}} con- trol {{strategy is}} proposed for High-Variety, Low-Volume (HVLV) systems. Firstly, using the (max,+) algebra, a di- rect generation of event-timing equations for determinis- tic and decision-free HVLV manufacturing systems is ob- tained. Then, a <b>linear</b> <b>optimization</b> <b>method</b> is presented. It {{is based on}} canonical forms for Max-Min-Plus-Scaling (MMPS) functions with linear constraints on the inputs. The approach aims at solving several linear programming problems and its validity is illustrated by a simulation ex- ample. Finally, a discussion of results, conclusions and perspectives are given...|$|E
40|$|Via a {{numerical}} <b>linear</b> <b>optimization</b> <b>method</b> {{we show that}} violations of local realism are stronger for two maximally entangled quNits, than for two qubits. The magnitude of violation increases with N. It is objectively defined by the required minimal admixture of pure noise to the maximally entangled state such that a local realistic description is still possible. Operational realization of the two quNit measurements exhibiting strong violations of local realism involves entangled photons and unbiased multiport beamsplitters. The approach, extending at present to N= 9, neither involves any simplifications, or additional assumptions, nor does it utilize any symmetries of the problem...|$|E
40|$|Tests {{of local}} realism vs quantum {{mechanics}} based on Bell's inequality employ two entangled qubits. We investigate the general case of two entangled quNits, i. e. quantum systems defined in an N-dimensional Hilbert space. Via a numerical <b>linear</b> <b>optimization</b> <b>method</b> {{we show that}} violations of local realism are stronger for two maximally entangled quNits (N= 3, 4, [...] ., 9), than for two qubits and that they increase with N. The two quNit measurements can be experimentally realized using entangled photons and unbiased multiport beamsplitters. Comment: 5 pages, 2 pictures, LaTex, two columns; No changes in the result...|$|E
40|$|A new {{computational}} method {{is presented to}} implement the system of deductive logic described by Aristotle in Prior Analytics. Each Aristotelian problem is interpreted as a parametric probability network in which the premises give constraints on probabilities relating the problem's categorical terms (major, minor, and middle). Each probability expression from this network is evaluated to yield a linear function of the parameters in the probability model. By this approach the constraints specified as premises translate into linear equalities and inequalities involving a few real-valued variables. The problem's figure (schema) describes which specific probabilities are constrained, relative to those that are queried. Using <b>linear</b> <b>optimization</b> <b>methods,</b> the minimum and maximum feasible values of certain queried probabilities are computed, subject to the constraints given as premises. These computed solutions determine precisely which conclusions are necessary consequences of the premises. In this way, Aristotle's logical deductions {{can be accomplished by}} means of numerical computation. Comment: reorganize...|$|R
40|$|We {{introduce}} a new barrier function {{which is not a}} barrier function in the usual sense: it has nite value at the boundary of the feasible region. Despite this, its iteration bound, O p n log n log n ", is as good as it can be: it is the best known bound for large-update methods. The recently introduced notions of superconvexity and exponential convexity are crucial in the analysis. Keywords: <b>Linear</b> <b>optimization,</b> interior-point <b>method,</b> primal-dual method, large-update method, polynomial complexity. AMS Subject Classication: 90 C 05...|$|R
40|$|The {{designating}} {{factors in}} the design of branched irrigation networks are the cost of pipes and the cost of pumping. They both depend directly on the hydraulic pump head. It is mandatory for this reason to calculate the optimal pump head as well as the corresponding economic pipe diameters, in order the minimal total cost of the irrigation network to be produced. The classical optimization techniques, which have been proposed so long, are the following: the <b>linear</b> programming <b>optimization</b> <b>method,</b> the nonlinear programming <b>optimization</b> <b>method,</b> the dynamic programming <b>optimization</b> <b>method</b> and Labye's method. The mathematical research of the problem using the above classical optimization techniques is very complex and the numerical solution calls for a lot of calculations, {{especially in the case of}} a network with many branches. For this reason, many researchers have developed simplified calculation methods with satisfactory results and with less calculation time needed. A simplified nonlinear <b>optimization</b> <b>method</b> has been developed at the Aristotle University of Thessaloniki - Greece by M. Theocharis. The required calculating procedure is much shorter when using Theocharis' simplified method than when using the classic <b>optimization</b> <b>methods,</b> because Theocharis' method requires only a handheld calculator and just a few numerical calculations. In this paper a comparative calculation of the pump optimal head as well as the corresponded economic pipe diameters, using: (a) Labye's <b>optimization</b> <b>method,</b> (b) the <b>linear</b> programming <b>optimization</b> <b>method</b> and (c) Theocharis' simplified nonlinear programming method is presented. Application and comparative evaluation in a particular irrigation network is also developed. From the study it is concluded that Theocharis' simplified method can be equally used with the classical methods. (C) 2009 Elsevier Ltd. All rights reserved...|$|R
40|$|A {{procedure}} is described for efficiently finding the ground state energy and configuration for a Frenkel-Kontorova model in a periodic potential, consisting of N parabolic segments of identical curvature in each period, through a numerical {{solution of the}} convex minimization problem described in the preceding paper. The key elements are the use of subdifferentials to describe {{the structure of the}} minimization problem; an intuitive picture of how to solve it, based on motion of quasiparticles; and a fast <b>linear</b> <b>optimization</b> <b>method</b> with a reduced memory requirement. The procedure has been tested for N up to 200. Comment: 9 RevTeX pages, using AMS-Fonts (amssym. tex,amssym. def), 3 Postscript figures, accepted by Phys. Rev. B to be published together with cond-mat/ 970722...|$|E
40|$|International audienceThis paper {{presents}} a new robust camera pose estimation algorithm based on real-time 3 D model tracking. We propose to combine point and line features {{in order to}} handle partial occlusion and increase the accuracy. A non <b>linear</b> <b>optimization</b> <b>method</b> is used to estimate the camera pose parameters. Robustness is obtained by integrating a M-estimator into the optimisation process. Furthermore, a crucial condition for pose estimation problem is the consistency of 2 D/ 3 D correspondences between image and model features. We propose here to implement a natural point and line robust trackers {{in order to find}} corresponding features in the sequence images. Our method has been evaluated on several video sequences. The results show the robustness and the efficiency of our algorithm compared to other tracking approaches...|$|E
40|$|This paper uses a <b>linear</b> <b>optimization</b> <b>method</b> called Data Envelopment Analysis (DEA) {{to measure}} the {{efficiency}} of Brazilian State Courts {{during the years of}} 2006 to 2008. Our results show that relative efficiency varies substantially across the states. There is a group of courts that consistently top performs in the sample. On the other hand, there is a group of consistent poor performers, as well a group of average performers. Yet, the biggest problem seems to be with a group of State Courts with very unstable results, which might indicate serious problems in data collection and/or measurement. DEA also shows that the lack of resources should not be pointed as the main reason for inefficiency, since inefficient courts could improve the number of adjudications without changing the level of resources employed...|$|E
40|$|We {{propose a}} {{framework}} for eliciting and aggregating pairwise preference relations {{based on the assumption}} of an underlying fuzzy partial order. We also propose some <b>linear</b> programming <b>optimization</b> <b>methods</b> for ensuring consistency either as part of the aggregation phase or as a pre- or post-processing task. We contend that this framework of pairwise-preference relations, based on the Kemeny distance, can be less sensitive to extreme or biased opinions and is also less complex to elicit from experts. We provide some examples and outline their relevant properties and associated concepts...|$|R
40|$|There {{are ever}} {{increasing}} number of applications of multi-target tracking and considerable {{research has been conducted}} to solve this problem. Multi-target tracking is a NP-hard problem and almost all of the present multi-target tracking al-gorithms are sub-optimal by finding the solution in a reduced hypothesis space. In this paper we introduce a new approach toward finding the optimal single frame solution for general multi-target tracking problem. Our proposed method finds the optimal solution using <b>linear</b> programming <b>optimization</b> <b>method.</b> The proposed method has been successfully applied to synthetic and real data...|$|R
40|$|The {{identification}} of the atoms which change their position in chemical reactions is an important knowledge {{within the field of}} Metabolic Engineering. This can lead to new advances at different levels from the reconstruction of metabolic networks to the classification of chemical reactions, through the {{identification of}} the atomic changes inside a reaction. The Atom Mapping approach was initially developed in the 1960 s, but recently suffered important advances, being used in diverse biological and biotechnological studies. The main methodologies used for atom mapping are the Maximum Common Substructure and the <b>Linear</b> <b>Optimization</b> <b>methods,</b> which both require computational know-how and powerful resources to run the underlying tools. In this work, we assessed a number of previously implemented atom mapping frameworks, and built a framework able of managing the different data inputs and outputs, as well as the mapping process provided by each of these third-party tools. We evaluated the admissibility of the calculated atom maps from different algorithms, also assessing if with different approaches we were capable of returning equivalent atom maps for the same chemical reaction. ERDF -European Regional Development Fund(UID/BIO/ 04469 / 2013) info:eu-repo/semantics/publishedVersio...|$|R
40|$|A {{method for}} lightweight-gypsum {{material}} design using waste stone dust as the foaming agent is described. The main {{objective is to}} reach several physical properties which are inversely related in a certain way. Therefore, a <b>linear</b> <b>optimization</b> <b>method</b> is applied to handle this task systematically. The optimization process is based on sequential measurement of physical properties. The results are subsequently point-awarded according to a complex point criterion and new composition is proposed. After 17 trials the final mixture is obtained, having the bulk density equal to (586 ± 19) kg/m 3 and compressive strength (1. 10 ± 0. 07) MPa. According to a detailed comparative analysis with reference gypsum, the newly developed material {{can be used as}} excellent thermally insulating interior plaster with the thermal conductivity of (0. 082 ± 0. 005) W/(m·K). In addition, its practical application can bring substantial economic and environmental benefits as the material contains 25 % of waste stone dust...|$|E
40|$|Communicated by Hao-Min Zhou and Yunmei Chen) Abstract. Diffusion Kurtosis Imaging (DKI) {{is a new}} Magnetic Resonance Imaging (MRI) {{model to}} {{characterize}} the non-Gaussian diffusion behavior in tissues. In reality, the term bDapp − 1 6 b 2 D 2 appKapp in the extended Stejskal and Tanner equation of DKI should be positive for an appropriate range of b-values to make sense physically. The positive definiteness of the above term reflects the signal attenuation in tissues during imaging. Hence, {{it is essential for}} the validation of DKI. In this paper, we analyze the positive definiteness of DKI. We first char-acterize the positive definiteness of DKI through the positive definiteness of a tensor constructed by diffusion tensor and diffusion kurtosis tensor. Then, a conic <b>linear</b> <b>optimization</b> <b>method</b> and its simplified version are proposed to handle the positive definiteness of DKI from the perspective of numerical com-putation. Some preliminary numerical tests on both synthetical and real data show that the method discussed in this paper is promising...|$|E
40|$|Difiusion Kurtosis Imaging (DKI) {{is a new}} Magnetic Resonance Imaging (MRI) {{model to}} {{characterize}} the non-Gaussian difiusion behavior in tissues. In reality, the term, in the extended Stejskal and Tanner equation of DKI should be positive for an appropriate range of b-values to make sense physically. The positive definiteness of the above term reects the signal attenuation in tissues during imaging. Hence, {{it is essential for}} the validation of DKI. In this paper, we analyze the positive definiteness of DKI. We first characterize the positive definiteness of DKI through the positive definiteness of a tensor constructed by difiusion tensor and difiusion kurtosis tensor. Then, a conic <b>linear</b> <b>optimization</b> <b>method</b> and its simplified version are proposed to handle the positive definiteness of DKI from the perspective of numerical computation. Some preliminary numerical tests on both synthetical and real data show that the method discussed in this paper is promising. Department of Applied Mathematic...|$|E
40|$|DOI: 10. 1371 /journal. pone. 0043487 Optimization {{models in}} {{metabolic}} engineering and systems biology focus typically on optimizing a unique criterion, usually the synthesis rate of a metabolite of interest or {{the rate of}} growth. Connectivity and non-linear regulatory effects, however, make it necessary to consider multiple objectives {{in order to identify}} useful strategies that balance out different metabolic issues. This is a fundamental aspect, as optimization of maximum yield in a given condition may involve unrealistic values in other key processes. Due to the difficulties associated with detailed non-linear models, analysis using stoichiometric descriptions and <b>linear</b> <b>optimization</b> <b>methods</b> have become rather popular in systems biology. However, despite being useful, these approaches fail in capturing the intrinsic nonlinear nature of the underlying metabolic systems and the regulatory signals involved. Targeting more complex biological systems requires the application of global <b>optimization</b> <b>methods</b> to non-linear representations. In this work we address the multi-objective global optimization of metabolic networks that are described by a special class of models based on the power-law formalism: the generalized mass action (GMA) representation. Our goal is to develop global <b>optimization</b> <b>methods</b> capable of efficiently dealing with several biological criteria simultaneously. In order to overcome the numerical difficulties of dealing with multiple criteria in the optimization, we propose a heuristic approach based on the epsilon constraint method that reduces the computational burden of generating a set of Pareto optimal alternatives, each achieving a unique combination of objectives values. To facilitate the post-optimal analysis of these solutions and narrow down their number prior to being tested in th...|$|R
40|$|We present new {{algorithms}} {{for determining}} optimal strategies for two-player games with probabilistic moves and reachability winning conditions. Such games, known as simple stochastic games, were extensively studied by A. Condon [2, 3]. Many interesting problems, including parity games and hence also mu-calculus model checking, {{can be reduced}} to simple stochastic games. It is an open problem, whether simple stochastic games can be solved in polynomial time. Our algorithms determine the optimal expected payoffs in the game. We use geometric interpretation of the search space as a subset of the hyper-cube [0, 1] N. The main idea is to divide this set into convex subregions in which <b>linear</b> <b>optimization</b> <b>methods</b> can be used. We show how one can proceed from one subregion to the other so that, eventually, a region containing the optinal payoffs will be found. The total number of subregions is exponential {{in the size of the}} game but, in practice, the algorithms need to visit only few of them to find a solution. We believe that our new algorithms could provide new insights into the difficult problem of determining algorithmic complexity of simple stochastic games and other, equivallent problems. Key words: infinite graph games, parity games, simple stochastic games, finding optimal strategies, successive approximation. ...|$|R
40|$|A {{method for}} solving the {{scheduling}} {{problem for a}} class of cyclic systems with respect to throughput maximization is presented. Based on discrete events systems (DES) modeling, the scheduling problem can be formulated as a mixed integer <b>linear</b> <b>optimization</b> problem. The <b>method</b> is applied to High Throughput Screening (HTS) problems and illustrated {{by means of a}} small example. © 2003 IMACS. Published by Elsevier B. V. All rights reserved. [accessed 2014 April 1 st...|$|R
40|$|Material {{selection}} is a challenging issue in manufacturing processes while the inappropriate selected material {{may lead to}} fail the manufacturing process or end user experience especially in high-tech industries such as aircraft and shipping. Every material has different quantitative and qualitative criteria which should be considered simultaneously when assessing and selecting the right material. A weighted <b>linear</b> <b>optimization</b> <b>method</b> (WLOM) {{in the class of}} data envelopment analysis which exists in literature is adopted to address material selection problem while accounting for both qualitative and quantitative criteria. However, it is demonstrated the adopted WLOM method is not able to produce a full ranking vector for the material selection problems borrowed from the literature. Thus, an augmented common weight data envelopment analysis model (ACWDEA) is developed in this paper with the aim of eliminating deficiencies of WLOM model. The proposed ACWDEA is able to produce full ranking vector in decision making problems with less computational complexities in superior to the WLOM. Two material selection problems are solved and results are compared with WLOM and previous methods. Finally, the robustness and effectiveness of the proposed ACWDEA method are evaluated through Spearman’s correlation tests...|$|E
40|$|Neural {{network has}} been widely used in various fields of robotics. In this work, the neural network {{analysis}} using backpropagation algorithm {{is applied to the}} inverse velocity analysis of robotic manipulators near the singularity points accounting for the tracking error and feasibility of joint velocities. The inverse computations using the pseudo-inverse of the Jacobian matrix are compared with those obtained by the neural network analysis. The results illustrated using examples of two well known manipulators show the advantages of using the present work. A new learning algorithm called LP-neuro method is then developed to solve neural network problems, in this algorithm, the weights are obtained by a combination of Linear Programming having a sparse coefficient matrix and a single variable non-linear optimization method. The results are illustrated by solving three different problems, two of which are useful in the on-line control of robotic manipulators. The designs of a function generator and a four-bar mechanism whose coupler curve passes through nine specified points, have been carried out using neural network methods. The design problem has been solved using non-linear techniques which yield a weight matrix in each of the cases. The accuracy of the methods is also discussed. Finally, gain parameters required for the trajectory control are evaluated using non- <b>linear</b> <b>optimization</b> <b>method.</b> Neural network is then trained to evaluate the gain parameters based on error history of different trajectories...|$|E
40|$|Battery {{energy storage}} systems (BESS) coupled with rooftop-mounted {{residential}} photovoltaic (PV) generation, designated as PV-BESS, draw increasing attention and market penetration {{as more and}} more such systems become available. The manifold BESS deployed to date rely on a variety of different battery technologies, show a great variation of battery size, and power electronics dimensioning. However, given today’s high investment costs of BESS, a well-matched design and adequate sizing of the storage systems are prerequisites to allow profitability for the end-user. The economic viability of a PV-BESS depends also on the battery operation, storage technology, and aging of the system. In this paper, a general method for comprehensive PV-BESS techno-economic analysis and optimization is presented and applied to the state-of-art PV-BESS to determine its optimal parameters. Using a <b>linear</b> <b>optimization</b> <b>method,</b> a cost-optimal sizing of the battery and power electronics is derived based on solar energy availability and local demand. At the same time, the power flow optimization reveals the best storage operation patterns considering a trade-off between energy purchase, feed-in remuneration, and battery aging. Using up to date technology-specific aging information and the investment cost of battery and inverter systems, three mature battery chemistries are compared; a lead-acid (PbA) system and two lithium-ion systems, one with lithium-iron-phosphate (LFP) and another with lithium-nickel-manganese-cobalt (NMC) cathode. The results show that different storage technology and component sizing provide the best economic performances, depending on the scenario of load demand and PV generation...|$|E
40|$|Because of {{the dynamic}} and complex grid environment, the speed and {{stability}} of grid data transfer can’t be guaranteed, which has become the “bottleneck” that restricts grid applications, Replica is also very hard to incarnate advantage. Multi-Agent, with virtue of saving network bandwidth, realizing load and uninstall, increasing application’s, and affording, can solve problems in the transfer process primely. However, they are the key factors that multi-replica transfer task is transformed into multi-agent coalition formation, and the coalition formation tactic. This paper analyzes and researches the relationship between replica transmission and Multi-Agent coalition, puts forward a high efficient coalition formation tactic based on <b>linear</b> programming <b>optimization</b> <b>methods</b> of mathematical theory. </span...|$|R
40|$|Abstract—Because of {{the dynamic}} and complex grid environment, the speed and {{stability}} of grid data transfer can’t be guaranteed, which has become the “bottleneck” that restricts grid applications,Replica is also very hard to incarnate advantage. Multi-Agent, with virtue of saving network bandwidth, realizing load and uninstall, increasing application’s, and affording, can solve problems in the transfer process primely. However, they are the key factors that multi-replica transfer task is transformed into multi-agent coalition formation, and the coalition formation tactic. This paper analyzes and researches the relationship between replica transmission and Multi-Agent coalition, puts forward a high efficient coalition formation tactic based on <b>linear</b> programming <b>optimization</b> <b>methods</b> of mathematical theory. Index Terms—grid;multi-agent; coalition formation tactic;linear programming I...|$|R
40|$|Decomposition {{techniques}} for linear programming {{are difficult to}} extend to conic optimization problems with general non-polyhedral convex cones because the conic inequalities introduce an additional nonlinear coupling between the variables. However in many applications the convex cones have a partially separable structure {{that allows them to}} be characterized in terms of simpler lower-dimensional cones. The most important example is sparse semidefinite programming with a chordal sparsity pattern. Here partial separability derives from the clique decomposition theorems that characterize positive semidefinite and positive-semidefinite-completable matrices with chordal sparsity patterns. The paper describes a decomposition method that exploits partial separability in conic <b>linear</b> <b>optimization.</b> The <b>method</b> is based on Spingarn's method for equality constrained convex optimization, combined with a fast interior-point method for evaluating proximal operators...|$|R
40|$|The {{increasing}} {{penetration of}} renewable generation increases {{the need for}} flexibility to accommodate for growing uncertainties. The level of flexibility {{is measured by the}} available power that can be provided by flexible resources, such as dispatachable generators, in a certain time period under the constraint of transmission capacity. In addition to conventional flexible resources, energy storage is also expected as a supplementary flexible resource for variability accommodation. To aid the cost-effective planning of energy storage in power grids with intensive renewable generation, this study proposed an approach to determine the minimal requirement of power capacity and the appropriate location for the energy storage. In the proposed approach, the variation of renewable generation is limited within uncertainty sets, then a linear model is proposed for dispatchable generators and candidate energy storage to accommodate the variation in renewable generation under the power balance and transmission network constraints. The target of the proposed approach is to minimize the total power capacity of candidate energy storage facilities when the availability of existing flexible resources is maximized. After that, the robust <b>linear</b> <b>optimization</b> <b>method</b> is employed to convert and solve the proposed model with uncertainties. Case studies are carried out in a modified Garver 6 -bus system and the Liaoning provincial power system in China. Simulation results well demonstrate the proposed optimization can provide the optimal location of energy storage with small power capacities. The minimal power capacity of allocated energy storage obtained from the proposed approach only accounts for 1 / 30 of the capacity of the particular transmission line that is required for network expansion. Besides being adopted for energy storage planning, the proposed approach can also be a potential tool for identifying the sufficiency of flexibility when a priority is given to renewable generation...|$|E
40|$|Abstract—In this correspondence, we {{describe}} an approach for {{the identification of}} good distance spectra for possibly existing binary linear block codes based on linear programming and the MacWilliams–Delsarte identities. Specifically, the linear program is defined by an expression characterizing {{the performance of a}} potential code in terms of its distance spectrum and constraints imposed by the MacWilliams–Delsarte identities. Using the union bound to characterize performance, our results suggest that the best distance spectrum is not a function of signal-to-noise ratio (SNR) above the cutoff rate SNR and also suggest the existence of several unknown, good codes. Characterizing the performance using the maximum spectral error component of the union bound suggests spectral thinning with decreasing SNR. Index Terms—Distance spectrum, group codes, <b>linear</b> programming, <b>optimization</b> <b>methods.</b> I...|$|R
40|$|AbstractWe present new {{algorithms}} {{for determining}} optimal strategies for two-player games with proba- bilistic moves and reachability winning conditions. Such games, known as simple stochastic games, were extensively studied by A. Condon [Anne Condon. The complexity of stochastic games. Information and Computation, 96 (2) : 203 – 224, 1992, Anne Condon. On algorithms for simple stochastic games. In Jin-Yi Cai, editor, Advances in Computational Complexity Theory, volume 13 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 51 – 73. AMS, 1993]. Many interesting problems, including parity games and hence also mu-calculus model checking, {{can be reduced}} to simple stochastic games. It is an open problem, whether simple stochastic games can be solved in polynomial time. Our algorithms determine the optimal expected payoffs in the game. We use geometric interpre- tation of the search space as a subset of the hyper-cube [0, 1]N. The main idea is to divide this set into convex subregions in which <b>linear</b> <b>optimization</b> <b>methods</b> can be used. We show how one can proceed from one subregion to the other so that, eventually, a region containing the optinal payoffs will be found. The total number of subregions is exponential {{in the size of the}} game but, in practice, the algorithms need to visit only few of them to find a solution. We believe that our new algorithms could provide new insights into the difficult problem of deter- mining algorithmic complexity of simple stochastic games and other, equivallent problems...|$|R
40|$|Abstract—The {{fields of}} {{bioinformatics}} and biotechnology {{rely on the}} collection, processing and analysis of huge numbers of bio-cellular images, including cell features such as cell size, shape, and motility. Thus, cell tracking is of crucial importance {{in the study of}} cell behaviour and in drug and disease research. Such a mul-titarget tracking is essentially an assignment problem, NP-hard, with the solution normally found in practice in a reduced hypoth-esis space. In this paper we introduce a novel approach to find the exact association solution over time for single-frame scan-back stem cell tracking. Our proposed method employs a class of <b>linear</b> programming <b>optimization</b> <b>methods</b> known as the Hungarian method to find the optimal joint probabilistic data association for nonlinear dynamics and non-Gaussian measurements. The proposed method, an optimal joint probabilistic data association approach, has been successfully applied to track hematopoietic stem cells. Index Terms—Cancer research, data association, Hungarian, <b>linear</b> programming, <b>optimization,</b> primal dual, segmentation, stem cell, tracking. I...|$|R
40|$|Abstract. Tourism {{brings about}} great {{economic}} benefits. However, it inevitably causes severe environmental {{issues at the}} same time as it consumes energy and produces emissions, which would impact the tourism areas in many respects negatively. As for a region entitled as the world heritages of the nature and culture, the issue of energy reduction and dependency of the economies on energy has made the need for energy diversification more urgent. This paper focuses on the low-carbonized adjustment of energy structure in world natural and cultural heritage area. By applying <b>linear</b> multi-objective <b>optimization</b> <b>method,</b> we construct a fuzzy multi-objective optimization model analyzing the relationship between energy and social economic, and environment. Besides, we take Shizhong District of Leshan city (LSD) in China as our study case and present solutions on the low-carbonized adjustment of energy in this area from a quantitative perspective...|$|R
40|$|A {{tool for}} {{long-term}} optimization of cogeneration systems is developed {{that is based}} on mixed integer linear-programming and Lagrangian relaxation. We use a general approach without heuristics to solve the optimization problem of the unit commitment problem and load dispatch. The possibility to buy and sell electric power at a spot market is considered as well as the possibility to provide secondary reserve. The tool has been tested on a demonstration system based on an existing combined heat-and-power (CHP) system with extraction-condensing steam turbines, gas turbines, boilers for heat production and district-heating networks. The key feature of the model for obtaining solutions within reasonable times is a suitable division of the whole optimization period into overlapping sub-periods. Using Lagrangian relaxation, the tool can be applied to large CHP systems. For the demonstration model, almost optimal solutions were found. Cogeneration <b>Linear</b> programming <b>Optimization</b> <b>methods</b> Planning Power generation Power generation dispatch Unit commitment...|$|R
