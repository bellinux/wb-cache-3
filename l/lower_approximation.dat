167|526|Public
2500|$|In summary, the <b>lower</b> <b>approximation</b> of {{a target}} set is a {{conservative}} approximation consisting of only those objects which can positively be identified {{as members of the}} set. (These objects have no indiscernible [...] "clones" [...] which are excluded by the target set.) The upper approximation is a liberal approximation which includes all objects that might be members of target set. (Some objects in the upper approximation may not be members of the target set.) From the perspective of , the <b>lower</b> <b>approximation</b> contains objects that are members of the target set with certainty (probability = 1), while the upper approximation contains objects that are members of the target set with non-zero probability (probability > 0).|$|E
2500|$|Rules induced {{from the}} <b>lower</b> <b>approximation</b> {{of the concept}} {{certainly}} describe the concept, hence such rules are called certain. [...] On the other hand, rules induced from the upper approximation of the concept describe the concept possibly, so these rules are called possible. [...] For rule induction LERS uses three algorithms: LEM1, LEM2, and IRIM.|$|E
2500|$|The -lower approximation, or {{positive}} region, is {{the union of}} all equivalence classes in [...] which are contained by (i.e., are subsets of) the target set – in the example, , the union of the two equivalence classes in [...] which are contained in the target set. The <b>lower</b> <b>approximation</b> is the complete set of objects in [...] that can be positively (i.e., unambiguously) classified as belonging to target set [...]|$|E
5000|$|Therefore, <b>lower</b> <b>approximations</b> {{of class}} unions consist of the {{following}} objects: ...|$|R
2500|$|In general, {{the upper}} and <b>lower</b> <b>approximations</b> are not equal; in such cases, we say that target set [...] is undefinable or roughly definable on {{attribute}} set [...] When {{the upper and}} <b>lower</b> <b>approximations</b> are equal (i.e., the boundary is empty), [...] , then the target set [...] is definable on attribute set [...] We can distinguish the following special cases of undefinability: ...|$|R
40|$|Rough sets, a {{tool for}} data mining, deal with the vagueness and {{granularity}} in information systems. Rough approximations on a complete completely distributive lattice(CCD lattice for short) and brings generalizations of rough sets into a unified framework are discussed in [3]. This paper {{is devoted to the}} discussion of the relationship between approximations and topologies on a CCD lattice. It is proved that the set of all upper <b>approximations</b> (or of <b>lower</b> <b>approximations)</b> with respect to a partition consists of a clopen topology; and conversely, a clopen topology which obey disjoint axiom can be induced by approximations. Furthermore, the axiomatic characterizations of upper and <b>lower</b> <b>approximations</b> are presented. </p...|$|R
2500|$|That is, the {{accuracy}} of the rough set representation of , , , is the ratio of the number of objects which can positively be placed in [...] to the number of objects that can possibly be placed in [...] – this provides a measure of how closely the rough set is approximating the target set. Clearly, when the upper and lower approximations are equal (i.e., boundary region empty), then , and the approximation is perfect; at the other extreme, whenever the <b>lower</b> <b>approximation</b> is empty, {{the accuracy}} is zero (regardless {{of the size of the}} upper approximation).|$|E
2500|$|That is, {{for each}} {{equivalence}} class [...] in , we {{add up the}} size of its <b>lower</b> <b>approximation</b> by the attributes in , i.e., [...] [...] This approximation (as above, for arbitrary set [...] ) is the number of objects which on attribute set [...] can be positively identified as belonging to target set [...] Added across all equivalence classes in , the numerator above represents the total number of objects which – based on attribute set [...] – can be positively categorized according to the classification induced by attributes [...] The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects. [...] The dependency [...] "can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in [...] to determine the values of attributes in [...] ".|$|E
5000|$|The linear entropy is a <b>lower</b> <b>approximation</b> to the (quantum) von Neumann entropy S, {{which is}} defined as ...|$|E
40|$|Abstract—Rough sets, a {{tool for}} data mining, deal with the vagueness and {{granularity}} in information systems. Rough approximations on a complete completely distributive lattice(CCD lattice for short) and brings generalizations of rough sets into a unified framework are discussed in [3]. This paper {{is devoted to the}} discussion of the relationship between approximations and topologies on a CCD lattice. It is proved that the set of all upper <b>approximations</b> (or of <b>lower</b> <b>approximations)</b> with respect to a partition consists of a clopen topology; and conversely, a clopen topology which obey disjoint axiom can be induced by approximations. Furthermore, the axiomatic characterizations of upper and <b>lower</b> <b>approximations</b> are presented. Index Terms—Rough sets, complete completely distributive lattice, approximations, clopen topology I...|$|R
40|$|The {{application}} of a t-norm more than one time to the same object {{can be seen as}} the modelization of a semantic reinforcement of it. From a mathematical viewpoint, this operation can be seen as powers. Depending on the properties the t-norm ful¿lls several interesting properties emerge. This work will study what is the effect of the {{application of}} powers to indistinguishability operators, sets of extensionals, upper and <b>lower</b> <b>approximations.</b> It will be proved that there is a tight relation between the powers of an indistinguishability and their respective sets of extensionals, upper and <b>lower</b> <b>approximations,</b> and how this can be interpreted from a semantic point of view. Peer ReviewedPostprint (published version...|$|R
40|$|Rough {{set theory}} uses {{the concept of}} upper and <b>lower</b> <b>approximations</b> to {{encapsulate}} inherent inconsistency in real-world objects. Information multisystems are represented using multisets instead of crisp sets. This paper begins with an overview of recent works on multisets and rough sets. Rough multiset is introduced in terms of <b>lower</b> and upper <b>approximations</b> and explores related properties. The paper concludes with an example of certain types of information multisystems...|$|R
5000|$|In summary, the <b>lower</b> <b>approximation</b> of {{a target}} set is a {{conservative}} approximation consisting of only those objects which can positively be identified {{as members of the}} set. (These objects have no indiscernible [...] "clones" [...] which are excluded by the target set.) The upper approximation is a liberal approximation which includes all objects that might be members of target set. (Some objects in the upper approximation may not be members of the target set.) From the perspective of , the <b>lower</b> <b>approximation</b> contains objects that are members of the target set with certainty (probability = 1), while the upper approximation contains objects that are members of the target set with non-zero probability (probability > 0).|$|E
50|$|Rules induced {{from the}} <b>lower</b> <b>approximation</b> {{of the concept}} {{certainly}} describe the concept, hence such rules are called certain. On the other hand, rules induced from the upper approximation of the concept describe the concept possibly, so these rules are called possible. For rule induction LERS uses three algorithms: LEM1, LEM2, and IRIM.|$|E
5000|$|The -lower approximation, or {{positive}} region, is {{the union of}} all equivalence classes in [...] which are contained by (i.e., are subsets of) the target set - in the example, , the union of the two equivalence classes in [...] which are contained in the target set. The <b>lower</b> <b>approximation</b> is the complete set of objects in [...] that can be positively (i.e., unambiguously) classified as belonging to target set [...]|$|E
30|$|Considering {{the various}} {{transmission}} powers of different sources and {{taking into account}} the uncertainty of interference, we analyze the data success probabilities with or without relaying using the stochastic geometry theory. The upper and <b>lower</b> <b>approximations</b> of success probabilities are derived for the cooperative system.|$|R
40|$|In {{this note}} we study the {{connection}} between best approximation and interpolation by entire functions on the real line. A general representation for entire interpolants is outlined. As an illustration, best upper and <b>lower</b> <b>approximations</b> from the class of functions of fixed exponential type to the Gaussian are constructed...|$|R
40|$|In {{this paper}} we {{consider}} different approximations for computing the distribution function or risk measures {{related to a}} discrete sum of nonindependent lognormal random variables. Comonotonic upper bound and <b>lower</b> bound <b>approximations</b> for such sums have been proposed in Dhaene et al. (2002 a,b). We introduce the comonotonic “maximal variance ” <b>lower</b> bound <b>approximation.</b> We also compare the comonotonic approximations with two well-known moment matching approximations: the lognormal and the reciprocal Gamma approximation. We find that {{for a wide range}} of parameter values the comonotonic “maximal variance ” <b>lower</b> bound <b>approximation</b> outperforms the other approximations. Keywords: comonotonicity, simulation, lognormal, reciprocal Gamma. ...|$|R
50|$|The {{following}} {{are examples of}} properties of computer programs that can be calculated by data-flow analysis.Note that the properties calculated by data-flow analysis are typically only approximations of the realproperties. This is because data-flow analysis operates on the syntactical structure of the CFG withoutsimulating the exact control flow of the program.However, to be still useful in practice, a data-flow analysis algorithm is typically designed to calculate an upper respectively <b>lower</b> <b>approximation</b> of the real program properties.|$|E
5000|$|That is, the {{accuracy}} of the rough set representation of , , , is the ratio of the number of objects which can positively be placed in [...] to the number of objects that can possibly be placed in [...] - this provides a measure of how closely the rough set is approximating the target set. Clearly, when the upper and lower approximations are equal (i.e., boundary region empty), then , and the approximation is perfect; at the other extreme, whenever the <b>lower</b> <b>approximation</b> is empty, {{the accuracy}} is zero (regardless {{of the size of the}} upper approximation).|$|E
5000|$|Lower approximations {{group the}} objects which {{certainly}} belong to class union [...] (respectively [...] ). This certainty {{comes from the}} fact, that object [...] belongs to the <b>lower</b> <b>approximation</b> [...] (respectively [...] ), if no other object in [...] contradicts this claim, i.e. every object [...] which P-dominates , also belong to the class union [...] (respectively [...] ). Upper approximations group the objects which could belong to [...] (respectively [...] ), since object [...] belongs to the upper approximation [...] (respectively [...] ), if there exist another object [...] P-dominated by [...] from class union [...] (respectively [...] ).|$|E
40|$|We {{define a}} new {{fixpoint}} semantics for pure PROLOG, which is obtained as {{an instance of}} the generalized semantics of CLP and is oriented towards abstract interpretation. The concrete semantics is obtained by attaching to goals and constraints a history component that remembers the sequence of choices made during the derivation, and resorting {{to the notion of}} divergent constraint to represent partial computations. The semantics is then used as the foundation of a framework for the (bottom-up, goal-independent) abstract interpretation of pure PROLOG programs. The abstract semantic domain is composed of sequences of standard and divergent constraints. Two conceptually new problems arise in modelling the depth-first search strategy of PROLOG: both upper and <b>lower</b> <b>approximations</b> are needed and the length of the sequences need to be kept bounded in order to guarantee the finiteness of the approach. <b>Lower</b> <b>approximations</b> are needed because of the negative role played by divergent constraints. [...] ...|$|R
40|$|International audienceWe {{extend the}} result of Chateauneuf and Jaffray on upper and <b>lower</b> <b>approximations</b> of a fuzzy measure (capacity) by a {{probability}} measure {{to the case of}} k-additive measures, i. e. capacities for which the Möbius transform vanishes for subsets of more than k elements. A necessary condition is given, and the relation with the interaction index is given for 2 -additive measures...|$|R
40|$|This paper {{introduces}} {{upper and}} <b>lower</b> <b>approximations</b> of concepts {{with respect to}} an infinite concept class C and presents necessary and sufficient conditions for C to have such approximate concepts always in C. Then, we compare these notions {{with those of the}} rough set theory and show that the former can be regarded {{as an extension of the}} latter. Further, we propose a framework of learning upper and <b>lower</b> <b>approximations</b> of any concept with respect to an infinite concept class, and some efficient learning algorithms are presented. 1 Introduction Inductive inference is a process of identifying a target concept from examples. Gold formulated this process as an infinite procedure of identifying a target concept in the limit, which is called Gold's identification in the limit [Gol 67]. Including Gold's learning model, almost all of the frameworks of computational learning theory ([Ang 80] [Ang 88] [Val 84], etc.) assume that a target concept is contained in a hypothesis space, which is not al [...] ...|$|R
50|$|The P-1000 missile was {{partially}} {{derived from the}} P-500 and P-700. Its maximum speed is claimed to be between Mach 1.5 - Mach 2.5 depending on altitude, and its range is claimed to be between 700 and 1000 km (800). Warhead: 500 kg. Years of production 1985-1992. The body of the missile resembles that of the P-500, but it has {{the ability of the}} P-700 to overcome defensive countermeasures. Long range missile can achieve the target only at low altitudes (up to 25 meters or <b>lower)</b> <b>approximation</b> (in which case the maximum range is less than 500 km).|$|E
5000|$|That is, {{for each}} {{equivalence}} class [...] in , we {{add up the}} size of its <b>lower</b> <b>approximation</b> by the attributes in , i.e., [...] This approximation (as above, for arbitrary set [...] ) is the number of objects which on attribute set [...] can be positively identified as belonging to target set [...] Added across all equivalence classes in , the numerator above represents the total number of objects which - based on attribute set [...] - can be positively categorized according to the classification induced by attributes [...] The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects. The dependency [...] "can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in [...] to determine the values of attributes in [...] ".|$|E
30|$|Step 1. Each data {{sample is}} {{randomly}} assigned to one <b>lower</b> <b>approximation.</b> Since the <b>lower</b> <b>approximation</b> of a cluster is a subset of its upper approximation, this also automatically assigns the sample to the upper approximation of the same cluster.|$|E
40|$|For any {{stationary}} ^d-Gibbs {{measure that}} satisfies strong spatial mixing, we obtain sequences of upper and <b>lower</b> <b>approximations</b> that converge to its entropy. In the case, d= 2, these approximations are efficient {{in the sense}} that the approximations are accurate to within ϵ and can be computed in time polynomial in 1 /ϵ. Comment: This is a revision of paper originally posted in April, 201...|$|R
40|$|In this paper, {{we provide}} a {{methodology}} to design strategies for either guaranteed capture or guaranteed evasion {{in the case}} of pursuit-evasion games with multiple players which are represented by nonlinear dynamic models. This methodology is based on the continuously differentiable upper and <b>lower</b> <b>approximations</b> of the minimum and maximum function of an arbitrary number of arguments, comparison principle, and differential inequalities. Pursuit-evasion games, differential inequalities, multi-player dynamic games, Liapunov analysis...|$|R
40|$|We study rough approximations {{based on}} indiscernibility {{relations}} {{which are not}} necessarily reflexive, symmetric or transitive. For this, we define in a latticetheoretical setting two maps which mimic the rough approximation operators and note that this setting is suitable also for other operators based on binary relations. Properties of the ordered sets of the upper and the <b>lower</b> <b>approximations</b> {{of the elements of}} an atomic Boolean lattice are studied. ...|$|R
40|$|Attribute {{reduction}} {{is one of}} the most important problems in rough set theory. This paper introduces the concept of <b>lower</b> <b>approximation</b> reduction in ordered information systems with fuzzy decision. Moreover, the judgment theorem and discernable matrix are obtained, in which case an approach to attribute reduction in ordered information system with fuzzy decision is constructed. As an application of <b>lower</b> <b>approximation</b> reduction, some examples are applied to examine the validity of works obtained in our works. ...|$|E
3000|$|The cluster centres are hence {{determined}} as {{a weighted}} {{average of the}} samples belonging to the <b>lower</b> <b>approximation</b> and the boundary area, where the weights ω [...]...|$|E
40|$|It is {{well known}} that most of {{information}} systems are based on tolerance relation instead of the classical equivalence relation because of various factors in real-world. To acquire brief decision rules from the information systems, <b>lower</b> <b>approximation</b> reduction is needed. In this paper, the <b>lower</b> <b>approximation</b> reduction is proposed in inconsistent information systems based on tolerance relation. Moreover, the properties are discussed. Furthermore, judgment theorem and discern i bility matrix are obtained, from which an approach to lower reductions can be provided in the complicated information systems. </p...|$|E
40|$|Abstract. In 2009, Zuhua Liao and his {{students}} induced rough sets into inclines. They put forward the concepts of upper and <b>lower</b> <b>approximations</b> of a subset in an incline, and studied some properties of () Aθ − and () Aθ −. This paper, based onthose researches,investigated rough dual ideal in lattices, and discussed the properties of () Aθ − and () Aθ − when A is a dual ideal, a prime dual ideal and a convex sublattice of L...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} novel {{approach}} to the concept approximations in concept lattice. Using the similar idea of rough set theory and unique properties of concept lattice, upper and <b>lower</b> <b>approximations</b> of any object or attribute set can be found by exploiting meet-(union-) irreducible elements in concept lattice, the approximations can be performed on the fly. We show that our approach is more natural and effective than existing approach. ...|$|R
40|$|Over {{the last}} years, {{in a series}} papers by Arecchi and others, {{a model for the}} {{cognitive}} processes involved in decision making has been proposed and investigated. The key element of this model is the expression of apprehension and judgment, basic cognitive process of decision making, as an inverse Bayes inference classifying the information content of neuron spike trains. It has been shown that for successive plural stimuli this inference, equipped with basic non-algorithmic jumps, is affected by quantum-like characteristics. We show here that such a decision making process is related consistently with an ambiguous representation by an observer within a universe of discourse. In our work the ambiguous representation of an object or a stimuli is defined as a pair of maps from objects of a set to their representations, where these two maps are interrelated in a particular structure. The a priori and a posteriori hypotheses in Bayes inference are replaced by the upper and <b>lower</b> <b>approximations,</b> correspondingly, for the initial data sets that are derived with respect to each map. Upper and <b>lower</b> <b>approximations</b> herein are defined in the context of "rough set" analysis. The inverse Bayes inference is implemented by the <b>lower</b> <b>approximations</b> with respect to the one map and for the upper approximation with respect to the other map for a given data set. We show further that, due to the particular structural relation between the two maps, the logical structure of such combined approximations can only be expressed as an orthomodular lattice and therefore can be represented by a quantum rather than a Boolean logic. To our knowledge, this is the first investigation aiming to reveal the concrete logic structure of inverse Bayes inference in cognitive processes. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
