0|149|Public
2500|$|... "The chief {{attraction}} of the theory lies in its <b>logical</b> <b>completeness.</b> If {{a single one of}} the conclusions drawn from it proves wrong, it must be given up; to modify it without destroying the whole structure seems to be impossible." ...|$|R
5000|$|The {{state that}} exists {{when there is}} {{complete}} assurance that under all conditions an IT system {{is based on the}} logical correctness and reliability of the operating system, the <b>logical</b> <b>completeness</b> of the hardware and software that implement the protection mechanisms, and data integrity.|$|R
40|$|Process-based logic {{programs}} do not fulfill <b>logical</b> <b>completeness</b> as can be expected from a logic program. The concept of search tree unification is introduced in the paper which is a high level abstraction that captures distributed logic programs in a declarative way. The aim of search tree unification concept is to promote extracting <b>logical</b> <b>completeness</b> from process-based logic programs. In process-based logic languages, logic processes are created explicitly from sub-goals by built-in predicates. Since logic processes are basically Prolog programs, each process (i. e. a goal and a Prolog database) defines an own sub-search tree {{which is to be}} explored by the process. Component processes of the same logic program are not independent but rely on the partial results of each other, i. e. they exchange logical facts with each other which are deduced from the facts and rules distributed around the system. It means that sub-search trees are related along communication patterns. Explicit comm [...] ...|$|R
40|$|Abstract. Static {{analyses}} calculate abstract states, {{and their}} logics validate {{properties of the}} abstract states. We place into perspective the variety of forwards, backwards, functional, and <b>logical</b> <b>completeness</b> used in abstract-interpretation-based static analysis by giving examples and by proving equivalences, implications, and independences. We expose two fundamental Galois connections that underlie the logics for static analyses and reveal a new completeness variant, O-completeness. We also show that the key concept underlying <b>logical</b> <b>completeness</b> is covering, which we use to relate {{the various forms of}} completeness. When we use a static analysis, like data-flow analysis or model checking, to validate a program for correctness or code improvement, we must carefully define the domain of properties the analysis can calculate so that it includes both the goal properties we seek to validate as well as intermediate properties that lead to the goals. Say we try to validate {?}y: = −y;x: = y + 1 {isPositive(x) }; our analysis requires properties like isNegative to calculate a sound precondition...|$|R
40|$|The {{article is}} devoted to {{identification}} of the logical contradiction underlying the modern school course of Informatics (Computer Science). As Informatics {{is based on the}} notion "information", the informational processes running in the systems of various nature should be studied, including technical systems and human societies. In actual fact the school course doesn’t follow this directive. The exclusion of the notion "information" determines the wholeness of the course and its <b>logical</b> <b>completeness.</b> </p...|$|R
40|$|In {{the present}} study an {{additional}} measure of story narrative performance, story completeness, is evaluated. The <b>completeness</b> <b>measure</b> involves a tally of the critical story components mentioned by a storyteller. It was hypothesized that by combining organizational (story grammar) and <b>completeness</b> <b>measures,</b> story “goodness ” could be quantified. Data from 46 normal adults indicated that this analysis was relatively sensitive in that it classified the story narratives of the group into four distinct categories of story “goodness”. This analysis should prove useful {{for the study of}} narrative discourse of brain-injured populations...|$|R
40|$|We {{address the}} problem of {{designing}} constraint logic languages that usefully combine backward and forward chaining in a sound and complete way. Following the approach of Constraint Logic Programming, we define a class of programming languages that generalize both Constraint Logic and Concurrent Constraint Pro-gramming. Syntactically, this class corresponds to Constraint Han-dling Rules with disjunctions, but differ operationally by featuring set-based semantics instead of multiset-based ones; i. e., conjunc-tion and disjunction are idempotent. The assumption of program confluence is the crux on which both the committed choice strat-egy and the <b>logical</b> <b>completeness</b> of the languages rely...|$|R
40|$|Summary As {{a result}} of the rapid {{development}} in computer science, rule based expert systems have entered the application of PC-systems. In connection with the research concerning SIMPLEXYS, a toolhox enabling the realization of real time expert systems, some debugging tools have been developed for proving the correctness of the knowledge base. One of these debugging tools, the semantic checker, has been completely revised, resulting in a checker which is capahle of systematically checking the rule base for <b>logical</b> <b>completeness</b> {lnd cOllsistency. This is done hy using a method, which is mathematically sound and generally applicable. Lutgens. J. M. A...|$|R
40|$|Decision table {{techniques}} {{have been shown}} to be useful for ensuring <b>logical</b> <b>completeness,</b> eliminating ambiguity, and optimizing the translation of logic into flowcharts or computer programs. Nevertheless, they have not been widely applied in medicine. We have used decision table techniques to demonstrate the derivation of two sets of rules for determining whether to operate on patients with suspected appendicitis based on patterns of observed signs and symptoms. One rule set is based on a diagnostic threshold whereby morbidity is minimized; the other rule set minimizes mortality. For this purpose, we have developed an augmented decision table format that allows the incorporation of probability and utility data...|$|R
5000|$|... „Each time I hear a new VHK record I relive this {{atavistic}} reverie - {{its like}} reentering the womb. VHK are so improbable, so wonderful {{and yet so}} seemingly necessary (were they not to exist {{they would have to}} be invented), that they function for me like my favorite fairy tales used to when I was a kid. When I first discovered their 1988 LP [...] "Teach Death a Lesson", I was bowled over by its combination of monastic psychedelia, rock n roll codpiece swagger and sheer alien abduction <b>logical</b> <b>completeness,</b> this wondering is with me yet. VHK sound like they re from another world and another time.” - Bananafish, San Francisco fanzine, 1995, No. 10 ...|$|R
40|$|Proposing {{a certain}} notion of <b>logical</b> <b>completeness</b> as a novel quality {{criterion}} for ontologies, we identify and characterise {{a class of}} logical propositions which naturally extend domain and range restrictions commonly known from diverse ontology modelling approaches. We argue for the intuitivity {{of this kind of}} axioms and show that they fit equally well into formalisms based on rules as well as ones based on description logics. Extending the attribute exploration technique from formal concept analysis (FCA), we present an algorithm for the efficient interactive specification of all axioms of this form valid in a domain of interest. We compile some results that apply when role hierarchies and symmetric roles come into play and demonstrate the presented method in a small example...|$|R
40|$|In this paper, the Lorentz {{transformations of}} {{entangled}} Bell states with general mo-mentum not necessarily orthogonal to the boost direction and spin are studied. We extend quantum correlations and Bell’s inequality to the relativistic regime by considering nor-malized relativistic observables. It is shown that quantum information along the direction {{perpendicular to the}} boost is eventually lost, and Bell’s inequality is not always violated for entangled states in special relativity. This could impose restrictions on certain quantum information processing, such as quantum cryptography using massive particles. Relativistic quantum information processing is of growing interest {{not only for the}} <b>logical</b> <b>completeness</b> but also with regard to new features, such as the physi-cal bounds on information transfer, processing and the errors provided by the full relativistic treatments. 1) – 11) There is also an important question whether Bell’...|$|R
40|$|The {{ability to}} {{categorize}} and use concepts e#ectively {{is a basic}} requirementofany intelligent actor. The utility-based approach to categorization is founded on the thesis that categorization is fundamentally in service of action, i. e., the choice of concepts made by an actor is critical to its choice of appropriate actions. This {{is in contrast to}} classical and similarity-based approaches which seek <b>logical</b> <b>completeness</b> in concept description with respect to sensory data rather than action-oriented e#ectiveness. Utility-based categorization is normative and not descriptive. It prescribes howanintelligent agent ought to conceptualize to act e#ectively. It provides ideals for categorization, speci#es criteria for the design of e#ective computational agents, and provides a model of ideal competence. A decision-theoretic framework for utilitybased categorization whichinvolves reasoning about alternative categorization models of varying levels of abstraction is proposed. Categorization mode [...] ...|$|R
3000|$|... an {{attribute}} <b>completeness</b> to <b>measure</b> {{the number of}} null values of a specific attribute in a relation; [...]...|$|R
40|$|<b>Measuring</b> the <b>completeness</b> of {{the fossil}} record is {{essential}} to understanding evolution over long timescales, particularly when comparing evolutionary patterns among biological groups with different preservational properties. <b>Completeness</b> <b>measures</b> have been presented for various groups based on gaps in the stratigraphic ranges of fossil taxa and on hypothetical lineages implied by estimated evolutionary trees. Here we present and compare quantitative, widely applicable absolute <b>measures</b> of <b>completeness</b> at two taxonomic levels for a broader sample of higher taxa of marine animals than has previously been available. We provide {{an estimate of the}} probability of genus preservation per stratigraphic interval, and determine the proportion of living families with some fossil record. The two <b>completeness</b> <b>measures</b> use very different data and calculations. The probability of genus preservation depends almost entirely on the Palaeozoic and Mesozoic records, whereas the proportion of living families with a fossil record is influenced largely by Cenozoic data. These measurements are nonetheless highly correlated, with outliers quite explicable, and we find that completeness is rather high for many animal groups...|$|R
5000|$|G. Japaridze, Build {{your own}} clarithmetic I: Setup and <b>completeness.</b> <b>Logical</b> Methods is Computer Science 12 (2016), Issue 3, paper 8, pp. 1-59.|$|R
30|$|The {{relation}} completeness {{is relevant}} in all applications {{that need to}} evaluate the completeness of a whole relation and can admit the presence of null values on some attributes. Relation <b>completeness</b> <b>measures</b> how much information is represented in the relation by evaluating {{the content of the}} information actually available with respect to the maximum possible content, i.e., without null values. According to this interpretation, completeness of the relation Student in Table  3 is 53 / 60.|$|R
40|$|In this paper, a new {{approach}} to compute nonmonotonic inferences in a simple but useful propositional model-preference formalism is presented. It is original from at least two points of view. First, it makes use of local search techniques while preserving <b>logical</b> <b>completeness.</b> Second, it proves experimentally efficient for an important class of very large nonmonotonic knowledge bases. More precisely, it extends recent SAT-related practical computational results to a nonmonotonic framework. The proof-strategy is based on the use of local search techniques for SAT together with an efficient heuristic when these techniques fail to deliver a model. It is applied to a formalism allowing prioritized rules of default reasoning to be expressed, using McCarthy's Abnormality propositions. A typical application domain concerns the forms of defeasible reasoning that can be held from a deep model of a complex device or system, where Abnormality propositions are used to represent possible (but unexp [...] ...|$|R
40|$|Lee and Plaisted {{recently}} {{developed a new}} automated theorem proving strategy called hyper-linking. As part of his dissertation, Lee developed a round-by-round implementation of the hyper-linking strategy, which competes well with other automated theorem provers {{on a wide range}} of theorem proving problems. However, Lee's round-by-round implementation of hyper-linking is not particularly well suited for the addition of special methods in support of equality. In this dissertation, we describe, as alternative to the round-by-round hyper-linking implementation of Lee, a smallest instance first implementation of hyper-linking which addresses many of the inefficiencies of round-by-round hyper-linking encountered when adding special methods in support of equality. Smallest instance first hyper-linking is based on the formalization of generating smallest clauses first, a heuristic widely used in other automated theorem provers. We prove both the soundness and <b>logical</b> <b>completeness</b> of smallest instance first hyper-linking and show that it always generates smallest clauses first unde...|$|R
40|$|Conceptual Graphs (CGs) are {{a natural}} and {{intuitive}} notation for expressing first-order logic statements. However, the task of performing inference with a large-scale CG knowledge base remains largely unexplored. Although basic inference operators are defined for CGs, few methods are available for guiding their application during automated reasoning. Given the expressive power of CGs, this can result in inference being intractable. In this paper we show how a method used elsewhere for achieving tractability [...] - namely the use of access paths [...] - {{can be applied to}} conceptual graphs. Access paths add to CGs domain-specific information that guides inference by specifying preferred chains of subgoals for each inference goal (and hence, other chains will not be tried). This approach trades <b>logical</b> <b>completeness</b> for focussed inference, and allows incompleteness to be introduced in a controlled way (through the knowledge engineer's choice of which access paths to attach to CGs). The result o [...] ...|$|R
30|$|The tests percent {{coverage}} {{criteria that}} were used to select the subjects was considered necessary to prevent the Optimizer from removing “correct” instructions, that is, instructions that the test exercise them. However, we do not consider analyzing the quality of these tests as a selection criterion. After analyzing some previous Optimizer results, we noticed that covered instructions and even whole functions were removed entirely. This behavior is directly associated with the perceived quality of the tests in question. One point to be observed in this type of research is how the test design has, in addition to <b>completeness</b> (<b>measure</b> in coverage percent), quality.|$|R
40|$|Equational {{reasoning}} in Coq is not straightforward. For {{a few years}} now {{there has been an}} ongoing research process towards adding rewriting to Coq. However, there are many research problems on this way. In this paper we give a coherent view of rewriting in Coq, we describe what is already done and what remains to be done. We discuss such issues as strong normalization, confluence, <b>logical</b> consistency, <b>completeness,</b> modularity and extraction...|$|R
40|$|Abstract: A {{problem of}} {{incompatible}} and contradictory of results of making decisions in industrial organizationand-technical complexes (OTC) is outlined. A system-and-goal approach and a semiotic approach to make system of decisions satisfying requirements of <b>logical</b> correctness and <b>completeness</b> are concretized. A concept model of knowledge-based dialog system of analysis and synthesis of goals (DS ASG) to make system of goals is considered. Using the DS ASG decision-makers work out {{not only the}} system of goals but corresponding plan of goal-achieving satisfying requirements of <b>logical</b> correctness and <b>completeness</b> and in this way solve the problem of incompatible and contradictory of results of making decisions in OTC...|$|R
5000|$|Often {{maintains}} {{that history is}} nothing but mythmaking and that different histories {{are not to be}} compared on such traditional academic standards as accuracy, empirical probability, <b>logical</b> consistency, relevancy, <b>completeness,</b> fairness, honesty, etc., but on moral or political grounds ...|$|R
60|$|Some of Boon's {{jokes about}} this train were, {{to say the}} best of them, obvious. Mr. Compton Mackenzie was in trouble about his excess luggage, for example. Mr. Upton Sinclair, having carried out his ideal of an {{innocent}} frankness to a <b>logical</b> <b>completeness</b> in his travelling equipment, was forcibly wrapped in blankets by the train officials. Mr. Thomas Hardy had a first-class ticket but travelled by choice or mistake in a second-class compartment, his deserted place being subsequently occupied by that promising young novelist Mr. Hugh Walpole, provided with a beautiful fur rug, a fitted dressing-bag, a writing slope, a gold-nibbed fountain pen, innumerable introductions, and everything that a promising young novelist can need. The brothers Chesterton, Mr. Maurice Baring, and Mr. Belloc sat up all night in the wagon-restaurant consuming beer enormously and conversing upon immortality and whether it extends to Semitic and Oriental persons. At {{the end of the}} train, I remember, there was to have been a horse-van containing Mr. Maurice Hewlett's charger--Mr. Hewlett himself, I believe, was left behind by accident at the Gare de Lyons--Mr. Cunninghame Graham's Arab steed, and a large, quiet sheep, the inseparable pet of Mr. Arthur Christopher Benson....|$|R
5000|$|During {{his first}} year as a PhD {{candidate}} at the University of Wisconsin, Sgro proved that a topological extension of first-order logic using the open set logic quantifier has <b>logical</b> <b>completeness,</b> which had previously been widely believed but had not been proven. Sgro’s proof drew attention throughout mathematical world, and, in 1974, a year before finishing his PhD, he was awarded an appointment as a Josiah Willard Gibbs Instructor in Mathematics at Yale University, received an NSF research grant to continue his work in topological model theory. [...] Yale allowed him to accept this honor while remotely completing his thesis and dissertation at Wisconsin, which he did in 1975. His conclusions regarding the topological model theory {{formed the basis of}} his PhD thesis and dissertation. During the 1976-1977 academic year Sgro received a Centennial Fellowship from the AMS. His work also resulted in an invitation to speak at the Logica Colloquim ’77 European Meeting of the Association for Symbolic Logic. This event was held in Wrocław, Poland, which was then still part of the Eastern Bloc, making Sgro among the first mathematicians from the West to speak at an event “behind the Iron Curtain.” [...] Sgro also spent 1977-1978 at the Institute for Advanced Study at Princeton University.|$|R
40|$|ABSTRACT. We {{show that}} several logics of common belief and common {{knowledge}} {{are not only}} complete, but also strongly complete, hence compact. These logics involve a weakened monotonicity axiom, and no other restriction on individual belief. The semantics is of the ordinary fixed-point type. KEY WORDS: common belief, common knowledge, <b>logical</b> omniscience, strong <b>completeness</b> 1...|$|R
40|$|Abstract. Automatic {{ontology}} matching {{is a hard}} problem. To {{address this}} problem many ontology matchers have evolved {{in the past several}} years. Consequently the evaluation of ontology matchers has become crucial in order to help improve a matcher’s performance. The evaluation frameworks used so far are limited to available pairs of ontologies in certain domains and require the reference alignments (i. e., gold standards) to be specified manually. In this paper we present a novel ontology matcher evaluation approach which can accept any OWL ontology as the source ontology. With little human efforts to specify the changes to the source ontology, our system can automatically construct the target ontology and generate the gold standard of correspondences. Compared to well-known evaluators (e. g., OAEI), our approach can provide more meaningful feedback besides traditional accuracy and <b>completeness</b> <b>measures</b> by indicating the performance of ontology matchers according to various types of heterogeneities...|$|R
40|$|This paper {{presents}} the proposed criteria {{for measuring the}} quality and completeness of field spectroscopy metadata in a spectral archive. Definitions for metadata quality and completeness for field spectroscopy datasets are introduced. Unique methods for <b>measuring</b> quality and <b>completeness</b> of metadata {{to meet the requirements}} of field spectroscopy datasets are presented. Field spectroscopy metadata quality can be defined in terms of (but is not limited to) logical consistency, lineage, semantic and syntactic error rates, compliance with a quality standard, quality assurance by a recognized authority, and reputational authority of the data owners/data creators. Two spectral libraries are examined as case studies of operationalized metadata policies, {{and the degree to which}} they are aligned with the needs of field spectroscopy scientists. The case studies reveal that the metadata in publicly available spectral datasets are underperforming on the quality and <b>completeness</b> <b>measures.</b> This paper is part two in a series examining the issues central to a metadata standard for field spectroscopy datasets...|$|R
40|$|Information quality plays {{a crucial}} role in every {{application}} that integrates data from autonomous sources. However, information quality is hard to measure and complex to consider for the tasks of information integration, even if the integrating sources cooperate. We present a systematic and formal approach to the measurement of information quality and the combination of such measurements for information integration. Our approach is based on a value model that incorporates both extensional value (coverage) and intensional value (density) of information. For both aspects we provide merge functions for adequately scoring integrated results. Also, we combine the two criteria to an overall completeness criterion that formalizes the intuitive notion of completeness of query results. This <b>completeness</b> <b>measure</b> is a valuable tool to assess source size and to predict result sizes of queries in integrated information systems. We propose this measure as an important step towards the usage of information quality for source selection, query planning, query optimization, and quality feedback to users. Peer Reviewe...|$|R
40|$|For a {{large number}} of {{information}} domains, there are numerous World Wide Web information sources. Mediators allow integrated access to these sources by providing a common schema against which the user can pose queries. The sources often vary both in their extension and their intension: Due to their autonomy, sources overlap in the objects they cover and dier in the attributes of the objects they provide. We support mediators in their source selection and query planning process by a value model that incorporates both extensional value (coverage) and intensional value (density). When results from sources are merged, the scores for coverage and density of the merged result must be estimated. For both criteria we provide merge functions that calculate the score of merged results. Also, we combine the two criteria to an overall completeness criterion. This <b>completeness</b> <b>measure</b> is a valuable tool to assess source size and to predict result sizes of queries in WWW settings. 1 An [...] ...|$|R
30|$|Data mining {{algorithms}} {{work with}} different principles, {{being able to}} be influenced by different kinds of associations on data. To ensure fairer conditions in evaluation, this work finds the optimal clustering method for agriculture data analysis. Proposed work adopts the external quality metrics [3] like Purity, Homogeneity, <b>Completeness,</b> V <b>Measure,</b> Rand Index, Precision, Recall and F measure to compare the PAM, CLARA and DBSCAN clustering methods.|$|R
40|$|AbstractWe {{introduce}} a three-phase, nine-step methodology for specification of clinical guidelines (GLs) by expert physicians, clinical editors, and knowledge engineers and for quantitative {{evaluation of the}} specification’s quality. We applied this methodology to a particular framework for incremental GL structuring (mark-up) and to GLs in three clinical domains. A gold-standard mark-up was created, including 196 plans and subplans, and 326 instances of ontological knowledge roles (KRs). A <b>completeness</b> <b>measure</b> of the acquired knowledge revealed that 97 % of the plans and 91 % of the KR instances of the GLs were recreated by the clinical editors. A correctness measure often revealed high variability within clinical editor pairs structuring each GL, but for all GLs and clinical editors the specification quality {{was significantly higher than}} random (p< 0. 01). Procedural KRs were more difficult to mark-up than declarative KRs. We conclude that given an ontology-specific consensus, clinical editors with mark-up training can structure GL knowledge with high completeness, whereas the main demand for correct structuring is training in the ontology’s semantics...|$|R
40|$|ABSTRACT. The present {{paper is}} mainly {{concerned}} with establishing conditions which. assure that all lattice regular measures have additional smoothness properties or that simply all two-valued such measures have such properties {{and are therefore}} Dirac measures. These conditions are {{expressed in terms of}} the general Wallman space. The general results are then applied to specific topological lattices, yielding new con-ditions for measure compactness, Borel measure compactness, clopen measure replete-ness, strong measure compactness, etc. In addition, smoothness properties in the general setting for lattice regular measures are related to the notion of support, and numerous applications are given. KEY WORDS AND PHRASES. Support of a measure, repleteness, realcompaetness, and-completeness, mease <b>completeness,</b> <b>measure</b> compactness, and Borel measure compactness, clopen measure repleteness, strong measure repleteness, strong measure compactness, etc. 1980 AMS SUBJECT CLASSIFICATION CODES. 28 A 60, 28 A 32. i. INTRODUCTION. In an earlier paper [5], we obtained conditions for o-smoothness, T-smoothness, and tightness of lattice regular measures. This was done in a general framework for a set X and a lattice of subsets of X, i, which was just disjunctive and at time...|$|R
40|$|For many {{information}} domains {{there are}} numerous World Wide Web data sources. The sources vary both in their extension and their intension: They represent different real world entities with possible overlap and provide different attributes of these entities. Mediator-based information systems allow integrated access to such sources by providing a common schema against which the user can pose queries. Given a query, the mediator must determine which participating sources to access and how to integrate the incoming results. This article describes how to support mediators in their source selection and query planning process. We propose three new merge operators, which formalize the integration of multiple source responses. A completeness model describes the usefulness of a source to answer a query. The <b>completeness</b> <b>measure</b> incorporates both extensional value (called coverage) and intensional value (called density) of a source. We show how to determine the completeness of single sources and of combinations of sources under the new merge operators. Finally, we show {{how to use the}} measure for source selection and query planning...|$|R
40|$|The {{completeness}} {{of buildings}} in OpenStreetMap (OSM) is estimated for a medium-sized German {{city and its}} surroundings by comparing the OSM data with data from an official building cadastre. As <b>completeness</b> <b>measures</b> we apply two unit-based methods that are frequently applied in similar studies. It is found that the estimation of OSM building completeness strongly differ between the methods. A count ratio (number of OSM buildings / number of reference buildings) tends to underestimate the actual building completeness and an area ratio (total OSM building area / total reference building area) instead tends to overestimate the completeness within the study area. It is argued that a simple pre-processing of the building footprint polygons leads to a more accurate completeness estimation when applying the count ratio. It is also suggested to more carefully examine the areas that have been mapped in OSM {{but not in the}} reference data set (false positives). In the present study region, these values are mainly due to simplified OSM polygons and they contribute to an overestimation of the OSM building completeness when applying the area ratio...|$|R
