272|501|Public
5000|$|... the <b>linear</b> <b>reduction</b> factor, {{at which}} the overall {{emissions}} cap is reduced, from 1.74% (2013-2020) to 2.2% each year from 2021 to 2030 thus reducing 43% of EU CO2 emissions in the ETS sector as compared to 2005 ...|$|E
50|$|Papadimitriou and Yannakakis {{go on to}} {{complete}} this class by defining MaxSNP, the class of all problems with an L-reduction (<b>linear</b> <b>reduction,</b> not log-space reduction) to problems in MaxSNP0, and show {{that it has a}} natural complete problem: given an instance of 3CNFSAT (the boolean satisfiability problem with the formula in conjunctive normal form and at most 3 literals per clause), find an assignment satisfying as many clauses as possible.|$|E
5000|$|European Union aims in 2014 {{demanding}} {{targets to}} decline emissions 40% from 1990 level to 2030. In Finnish traffic this goal demands decline from (Mtn CO2) 12.48 to 7.4. As <b>linear</b> <b>reduction</b> this objective is annual decline in value of 0.30 {{from the top}} year emission 13.36 in 2010. This objective equals maximum emission levels of 12,16 (2014) and 11,56 (2016). Finnish traffic warming emissions (million tonnes CO2) were: ...|$|E
40|$|We {{begin by}} {{offering}} a new, direct proof of the equivalence between {{the problem of the}} existence of kernels in digraphs, KER, and satisfiability of propositional theories, SAT, giving <b>linear</b> <b>reductions</b> in both directions. Having introduced some <b>linear</b> <b>reductions</b> of the input graph, we present new algorithms for KER, with variations utilizing solvers of boolean equations. In the worst case, the algorithms try all assignments to either a feedback vertex set, $F$, or a set of nodes $E $ touching only all even cycles. Hence KER is fixed parameter tractable not only in the size of $F$, as observed earlier, but also in the size of $E$. A slight modification of these algorithms leads to a branch and bound algorithm for KER which is virtually identical to the DPLL algorithm for SAT. This suggests deeper analogies between the two fields and the probable scenario of KER research facing the challenges known from the work on SAT...|$|R
40|$|AbstractWe {{begin by}} {{offering}} a new, direct proof of the equivalence between {{the problem of the}} existence of kernels in digraphs, KER, and satisfiability of propositional theories, SAT, giving <b>linear</b> <b>reductions</b> in both directions. Having introduced some <b>linear</b> <b>reductions</b> of the input graph, we present new algorithms for KER, with variations utilizing solvers of boolean equations. In the worst case, the algorithms try all assignments to either a feedback vertex set, F, or a set of nodes E touching only all even cycles. Hence KER is fixed parameter tractable not only in the size of F, as observed earlier, but also in the size of E. A slight modification of these algorithms leads to a branch and bound algorithm for KER which is virtually identical to the DPLL algorithm for SAT. This suggests deeper analogies between the two problems and the probable scenario of KER research facing the challenges known from the work on SAT. The algorithm gives also the upper bound O⁎(1. 427 |G|) on the time complexity of general KER and O⁎(1. 286 |G|) of KER for oriented graphs, where |G| is the number of vertices...|$|R
40|$|The Krivine {{machine is}} an {{abstract}} machine implementing the <b>linear</b> head <b>reduction</b> of lambda-calculus. Ehrhard and Regnier gave a resource sensitive version returning the annotated {{form of a}} lambda-term accounting for the resources used by the <b>linear</b> head <b>reduction.</b> These annotations {{take the form of}} terms in the resource lambda-calculus. We generalize this resource-driven Krivine machine to the case of the algebraic lambda-calculus. The latter is an extension of the pure lambda-calculus allowing for the linear combination of lambda-terms with coefficients taken from a semiring. Our machine associates a lambda-term M and a resource annotation t with a scalar k in the semiring describing some quantitative properties of the <b>linear</b> head <b>reduction</b> of M. In the particular case of non-negative real numbers and of algebraic terms M representing probability distributions, the coefficient k gives the probability that the <b>linear</b> head <b>reduction</b> actually uses exactly the resources annotated by t. In the general case, we prove that the coefficient k can be recovered from the coefficient of t in the Taylor expansion of M and from the normal form of t. Comment: In Proceedings LINEARITY 2016, arXiv: 1701. 0452...|$|R
5000|$|In some {{products}} having {{a relatively high}} initial moisture content, an initial <b>linear</b> <b>reduction</b> of the average product moisture content {{as a function of}} time may be observed for a limited time, often known as a [...] "constant drying rate period". Usually, in this period, it is surface moisture outside individual particles that is being removed. The drying rate during this period is mostly dependent on the rate of heat transfer to the material being dried. Therefore, the maximum achievable drying rate is considered to be heat-transfer limited. If drying is continued, the slope of the curve, the drying rate, becomes less steep (falling rate period) and eventually tends to nearly horizontal at very long times. The product moisture content is then constant at the [...] "equilibrium moisture content", where it is, in practice, in equilibrium with the dehydrating medium. In the falling-rate period, water migration from the product interior to the surface is mostly by molecular diffusion, i,e. the water flux is proportional to the moisture content gradient. This means that water moves from zones with higher moisture content to zones with lower values, a phenomenon explained by the second law of thermodynamics. If water removal is considerable, the products usually undergo shrinkage and deformation, except in a well-designed freeze-drying process. The drying rate in the falling-rate period is controlled by the rate of removal of moisture or solvent from the interior of the solid being dried and is referred to as being [...] "mass-transfer limited". This is widely noticed in hygroscopic products such as fruits and vegetables, where drying occurs in the falling rate period with the constant drying rate period said to be negligible.|$|E
40|$|This paper defines head <b>linear</b> <b>{{reduction}},</b> {{a reduction}} strategy of #-terms that performs the minimal number of substitutions for reaching a head normal form. The definition relies on an extended notion of redex, and head <b>linear</b> <b>reduction</b> {{is therefore not}} a strategy in the exact usual sense. Krivine 's Abstract Machine is proved to be sound by relating it both to head <b>linear</b> <b>reduction</b> and to usual head reduction. The first proof suggests a variant machine, the Pointer Abstract Machine, which is also proved to be sound with respect to head <b>linear</b> <b>reduction...</b>|$|E
40|$|We {{introduce}} {{notions of}} <b>linear</b> <b>reduction</b> and linear equivalence of bijections {{for the purposes}} of study bijections between Young tableaux. Originating in Theoretical Computer Science, these notions allow us to give a unified view of a number of classical bijections, and establish formal connections between them. Comment: 42 pages, 15 figure...|$|E
40|$|Abstract- Dimension {{reduction}} {{is defined as}} the process of mapping high-dimensional data to a lowerdimensional vector space. Most machine learning and data mining techniques may not be effective for high-dimensional data. In order to handle this data adequately, its dimensionality needs to be reduced. Dimensionality {{reduction is}} also needed for visualization, graph embedding, image retrieval and a variety of applications. This paper discuss the most popular <b>linear</b> dimensionality <b>reduction</b> method Principal Component Analysis and the various non <b>linear</b> dimensionality <b>reduction</b> methods such a...|$|R
5000|$|... #Caption: PCA (a <b>linear</b> {{dimensionality}} <b>reduction</b> algorithm) is used {{to reduce}} this same dataset into two dimensions, the resulting values are not so well organized.|$|R
40|$|Abstract — We {{investigate}} content-based image retrieval em-ploying {{a representation}} of images based on the statistics of their spectral components and a new <b>linear</b> dimension <b>reduction</b> tech-nique. This <b>linear</b> dimension <b>reduction</b> technique is designed to optimize class separation with respect to metrics derived from cross-correlation of spectral histograms. Our approach to retrieval involves a preliminary classification step to index images in a database followed by a class-by-class retrieval step. We carry out several experiments with the Corel database and compare the outcome with several results previously reported in the literature. I...|$|R
40|$|Previous {{studies have}} {{measured}} conventional power MOSFET [1] and IGBT [2] operation {{down to the}} temperature of 4. 2 K. However, super-junction (SJ) devices such as CoolMOS (TM) have only been characterised down to 80 K. This paper presents the cryogenic behaviour of HEXFET (R), MDMesh (TM) and CoolMOS (TM) down to the temperature of 20 K for the first time. A <b>linear</b> <b>reduction</b> in breakdown voltage with temperature was observed down to approximately 150 K, below which the breakdown voltages saturated at higher values than predicted. The gradient of the <b>linear</b> <b>reduction</b> and the temperature at which the saturation begins depend on the dopant concentration of the drift region and on the device structure. The on-state resistances were found to reduce dramatically down to 50 K; below this temperature, some SJ devices exhibited significant carrier freeze-out effects while conventional devices like HEXFET (R) were less affected...|$|E
40|$|Linear head {{reduction}} {{is a key}} tool {{for the analysis of}} reduction machines for lambda-calculus and for game semantics. Its definition requires a notion of redex at a distance named primary redex in the literature. Nevertheless, a clear and complete syntactic analysis of this rule is missing. We present here a general notion of beta-reduction at a distance and of <b>linear</b> <b>reduction</b> (i. e., not restricted to the head variable), and we analyse their relations and properties. This analysis rests on a variant of the so-called sigma-equivalence that is more suitable for the analysis of reduction machines, since the position along the spine of primary redexes is not permuted. We finally show that, in the simply typed case, the proof of strong normalisation of <b>linear</b> <b>reduction</b> can be obtained by a trivial tuning of Gandy's proof for strong normalisation of beta-reduction. Comment: In Proceedings LINEARITY 2016, arXiv: 1701. 0452...|$|E
40|$|We {{consider}} description logic knowledge {{bases in}} which the ABox can contain Boolean combinations of traditional ABox assertions (represented as clauses or sequents). A <b>linear</b> <b>reduction</b> of such knowledge bases into a standard format (allowing only conjunctive assertions) is described which preserves knowledge base satisfiability. Similar results are presented for Boolean TBoxes and Boolean combinations of both ABox and TBox statements. ...|$|E
50|$|The {{steps of}} {{applying}} semidefinite programming {{followed by a}} <b>linear</b> dimensionality <b>reduction</b> step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.|$|R
40|$|Subspace {{selection}} {{approaches are}} powerful tools in pattern classification and data visualization. One {{of the most}} important subspace approaches is the <b>linear</b> dimensionality <b>reduction</b> step in the Fisher's linear discriminant analysis (FLDA), which has been successfully employed in many fields such as biometrics, bioinformatics, and multimedia information management. However, the <b>linear</b> dimensionality <b>reduction</b> step in FLDA has a critical drawback: for a classification task with c classes, if the dimension of the projected subspace is strictly lower than c - 1, the projection to a subspace tends to merge those classes, which are close together in the original feature space. If separate classes are sampled from Gaussian distributions, all with identical covariance matrices, then the <b>linear</b> dimensionality <b>reduction</b> step in FLDA maximizes the mean value of the Kullback-Leibler (KL) divergences between different classes. Based on this viewpoint, the geometric mean for subspace selection is studied in this paper. Three criteria are analyzed: 1) maximization of the geometric mean of the KL divergences, 2) maximization of the geometric mean of the normalized KL divergences, and 3) the combination of 1 and 2. Preliminary experimental results based on synthetic data, UCI Machine Learning Repository, and handwriting digits show that the third criterion is a potential discriminative subspace selection method, which significantly reduces the class separation problem in comparing with the <b>linear</b> dimensionality <b>reduction</b> step in FLDA and its several representative extensions...|$|R
40|$|Abstract. Dimension {{reduction}} {{is a crucial}} step for pattern recognition and information retrieval tasks to overcome the curse of dimensionality. In this paper a novel unsupervised <b>linear</b> dimension <b>reduction</b> method, Neighborhood Preserving Projections (NPP), is proposed. In contrast to traditional <b>linear</b> dimension <b>reduction</b> method, such as principal component analysis (PCA), the proposed method has good neighborhoodpreserving property. The central idea is to modify the classical locally linear embedding (i. e. LLE) by introducing a linear transform matrix. The transform matrix is obtained by optimizing a certain objective function. Preliminary experimental results on known manifold data show {{the effectiveness of the}} proposed method [...] ...|$|R
30|$|The {{significance}} of using accurate mechanical properties at elevated temperatures is also highlighted by Ranawaka and Mahendran (2010). The {{influence of the}} residual stresses on the ultimate failure load, for the case of distortional buckling, {{has been found to}} be small (less than 1 %). For the simulation of the residual stresses at elevated temperatures, a <b>linear</b> <b>reduction</b> relationship, proposed by Lee (2004), has been adopted.|$|E
40|$|AbstractLambda-calculus is {{the core}} of {{functional}} programming, and many different ways to evaluate lambda-terms have been considered. One of the nicest, from the theoretical point of view, is head <b>linear</b> <b>reduction.</b> We compare two ways of implementing that specific evaluation strategy: “Krivine's abstract machine” and the “interaction abstract machine”. Runs on those machines stand in a relation which can be accurately described using the call/return symmetry discovered by Asperti and Laneve...|$|E
40|$|We {{investigate}} the processing times of ultrafast laser machining {{in the case}} of metals (copper and stainless steel). At a fluence of 2. 5 J/cm(2), measurements of processing times are in good agreement with the calculations based on the ablation rates. We study the influence of laser repetition rates for 1, 5, 10 and 15 kHz. A <b>linear</b> <b>reduction</b> of the processing time can be expected with an increase of the repetition rate...|$|E
40|$|This is {{the first}} {{systematic}} study of Chinese children's brain development. Using optimized voxel-based morphometry, we investigated age-related changes in gray and white matter in Chinese children and adolescents aged 7 to 23 years. Results showed mostly <b>linear</b> <b>reductions,</b> but also some linear increases, in gray matter in many regions of the brain. Gray matter changes were more evident in the parietal and temporal regions than in the frontal and occipital regions. White matter showed significant linear increases in internal capsule, arcuate fasciculus, superior and inferior longitudinal fasciculus, and cingulate fasciculus. © 2007 Lippincott Williams & Wilkins, Inc...|$|R
40|$|Dimension {{reduction}} {{can be seen}} as {{the transformation}} from a high order dimension to a low order dimension. An example is the reduction of a cube in 3 D (xyz) to a square in 2 D (xy), to a point in 1 D (x). The main goal of dimension reduction is to concentrate on vital information while redundant information can be discarded. Various ways are developed to reduce dimensions. Reduction methods can be distinguished into linear and non <b>linear</b> dimension <b>reduction.</b> In this thesis we shall present a linear and some non <b>linear</b> dimension <b>reduction</b> strategies which are mainly based on neural networks. A reduced representation of data makes the data easier to handle. With this in mind, we have developed some applications that make use of reduced representation of data. The main application is face recognition which uses a non <b>linear</b> dimension <b>reduction</b> strategy to reduce a face image into no more than say one to five vital values. These values are then used to classify the face image. Acknowledgm [...] ...|$|R
40|$|There {{are two quite}} {{different}} possibilities for implementing <b>linear</b> head <b>reduction</b> in -calculus. Two ways which {{we are going to}} explain briefly here in the introduction and in details {{in the body of the}} paper. The paper itself is concerned with showing an unexpectedly simple relation between these two ways, which we term reversible and irreversible, namely that the latter may be obtained as a natural optimization of the former. Keywords: -calculus, abstract machines, geometry of interaction, reversible computations. 1 Introduction Notation. We denote the application of U to V by (U) V, e. g., the Church integer ¯ 2 will be fx (f) (f) x. <b>Linear</b> head <b>reduction.</b> But what is exactly <b>linear</b> head <b>reduction,</b> to begin with. It is a variant of head reduction, where one substitutes at each step the leftmost occurrence of c fl 1996 Elsevier Science B. V. Danos & Regnier variable whenever it is engaged into a redex, as in: (f (f) (f) x) y y ! (f(y y) (f) x) y y ! (f(y (f) x) (f) x) y y ! (f(y (y y) [...] ...|$|R
40|$|The {{following}} are conclusions and recomendations from the study. Primary wake effect is <b>linear</b> <b>reduction</b> in (eta) with St. Secondary wake effect is skewing of suction/pressure side cooling. Steady computations match experimental Nu, but overpredict (eta). Unsteady computations elucidate wake/film interaction. Model {{may be used}} to estimate wake passing effect. Need boundary layer and full stage experiments. Need resolved film hole and full stage unsteady computations. Need validated turbulence models for film cooling...|$|E
40|$|International audienceWe {{investigate}} processing {{times of}} ultrafast laser machining {{in the case}} of metals (copper and stainless steel). At a fluence of 2. 5 J/cm 2, measurements of processing times are in good agreement with calculations based on the ablation rates. We study the influence of laser repetition rates for 1, 5, 10 and 15 kHz. A <b>linear</b> <b>reduction</b> of the processing time can be expected with an increase of the repetition rate...|$|E
40|$|Dimensionality {{reduction}} {{has been}} shown to improve processing and information extraction from high dimensional data. Word space algorithms typically employ <b>linear</b> <b>reduction</b> techniques that assume the space is Euclidean. We investigate the effects of extracting nonlinear structure in the word space using Locality Preserving Projections, a reduction algorithm that performs manifold learning. We apply this reduction to two common word space models and show improved performance over the original models on benchmarks. ...|$|E
40|$|Abstract—Subspace {{selection}} {{approaches are}} powerful tools in pattern classification and data visualization. One {{of the most}} important subspace approaches is the <b>linear</b> dimensionality <b>reduction</b> step in the Fisher’s linear discriminant analysis (FLDA), which has been successfully employed in many fields such as biometrics, bioinformatics, and multimedia information management. However, the <b>linear</b> dimensionality <b>reduction</b> step in FLDA has a critical drawback: for a classification task with c classes, if the dimension of the projected subspace is strictly lower than c 1, the projection to a subspace tends to merge those classes, which are close together in the original feature space. If separate classes are sampled from Gaussian distributions, all with identical covariance matrices, then the <b>linear</b> dimensionality <b>reduction</b> step in FLDA maximizes the mean value of the Kullback-Leibler (KL) divergences between different classes. Based on this viewpoint, the geometric mean for subspace selection is studied in this paper. Three criteria are analyzed: 1) maximization of the geometric mean of the KL divergences, 2) maximization of the geometric mean of the normalized KL divergences, and 3) the combination of 1 and 2. Preliminary experimental results based on synthetic data, UCI Machine Learning Repository, and handwriting digits show that the third criterion is a potential discriminative subspace selection method, which significantly reduces the class separation problem in comparing with the <b>linear</b> dimensionality <b>reduction</b> step in FLDA and its several representative extensions. Index Terms—Arithmetic mean, Fisher’s linear discriminant analysis (FLDA), geometric mean, Kullback-Leibler (KL) divergence, machine learning, subspace selection (or dimensionality reduction), visualization. Ç...|$|R
30|$|Standard PCA only allows <b>linear</b> {{dimensionality}} <b>reduction.</b> However, if {{the data}} has more complicated structures which cannot be well represented in a linear subspace, standard PCA {{will not be}} very helpful. Fortunately, kernel PCA allows us to generalize standard PCA to nonlinear dimensionality reduction [42].|$|R
40|$|We {{propose a}} new method for <b>linear</b> {{dimensionality}} <b>reduction</b> of manifold-modeled data. Given a training set X of Q points {{belonging to a}} manifold M ⊂ R N, we construct a linear operator P: R N → R M that approximately preserves the norms of all ` ´ Q pairwise difference vectors (or secants) of X. We 2 design the matrix P via a trace-norm minimization that can be efficiently solved as a semi-definite program (SDP). When X comprises a sufficiently dense sampling of M, we prove that the optimal matrix P preserves all pairs of secants over M. We numerically demonstrate the considerable gains using our SDP-based approach over existing <b>linear</b> dimensionality <b>reduction</b> methods, such as principal components analysis (PCA) and random projections...|$|R
40|$|Small {{additions}} of Cu to the SUS 304 H, a {{high temperature}} austenitic stainless steel, enhance its {{high temperature strength}} and creep resistance. As Cu is known to cause embrittlement, the effect of Cu on room temperature mechanical properties that include fracture toughness and fatigue crack threshold of as-solutionized SUS 304 H steel were investigated in this work. Experimental results show a <b>linear</b> <b>reduction</b> in yield and ultimate strengths with Cu addition of up to 5 wt. ...|$|E
40|$|In October 2001 the {{management}} agency (Coastal States) for Norwegian spring spawning herring {{agreed to a}} recovery plan which will become operative if the spawning stock falls below Bpa. The main element in this recovery plan is a <b>linear</b> <b>reduction</b> in the fishing mortality from 0. 125 at Bpa to 0. 05 at Blim. The paper reviews the process leading {{to the adoption of}} the recovery plan, and the performance and consequences of implementing such a plan is also discussed...|$|E
40|$|Abstract: Linear and {{nonlinear}} {{principal component}} analysis is used to characterize the state space of a kinetic Monte Carlo simulation of thin film deposition. The film’s surface is first characterized using spatial correlation functions. This highdimensional representation is reduced {{using a combination of}} linear and nonlinear projection. When nonlinear projection is used, the dynamics of the training and test data can be captured within 2 % and 7 %, respectively, using three dimensions. In constrast, a three-dimensional <b>linear</b> <b>reduction</b> does not adequately describe the relationship between the training and test data...|$|E
40|$|Coupled {{training}} of dimensionality reduction and classification is proposed previously {{to improve the}} prediction performance for single-label problems. Following this line of research, in this paper, we introduce a novel Bayesian supervised multilabel learning method that combines <b>linear</b> dimensionality <b>reduction</b> with <b>linear</b> binary classification. We present a deterministic variational approximation approach to learn the proposed probabilistic model for multilabel classification. We perform experiments on four benchmark multilabel learning data sets bycomparing our methodwith four baseline <b>linear</b> dimensionality <b>reduction</b> algorithms. Experiments show that the proposed approach achieves good performance values in terms of hamming loss, macro F 1, and micro F 1 on held-out test data. The low-dimensional embeddings obtained by our method are also very useful for exploratory data analysis. ...|$|R
40|$|In order t s, we {{parameters}} o sitive whole reproductive period, it {{was noted}} that net photosynthetic rate, stomatal conductance, pigment contents, ions contents, leaf area index, leaf area duration, leaf relative water content and dry matter accumulation of spikes decreased in both cultivars with saline concentrations increasing. However, the salt-tolerant cultivars showed none significant reductions in those parameters compared with control under 0. 3 % and 0. 5 % salt concentrations, with only considerable decrease happening when soil salt concentration exceeded to 0. 7 %. Sharply contrast to salt-tolerant cultivars, the salt-sensitive cultivars appeared <b>linear</b> <b>reductions</b> in physiological parameters under a series of salt concentrations (0. 3 %, 0. 5 % and 0. 7 %), with the pho growth perio indicating a at the repro...|$|R
40|$|<b>Linear</b> graph <b>reduction</b> is {{a simple}} {{computational}} model in which the cost of naming things is explicitly represented. The key idea {{is the notion of}} "linearity". A name is linear if it is only used once, so with linear naming you cannot create more than one outstanding reference to an entity. As a result, linear naming is cheap to support and easy to reason about. Programs can be translated into the <b>linear</b> graph <b>reduction</b> model such that linear names in the program are implemented directly as linear names in the model. Nonlinear names are supported by constructing them out of linear names. The translation thus exposes those places where the program uses names in expensive, nonlinear ways. Two applications demonstrate the utility of using linear graph reduction: First, in the area of distributed computing, linear naming makes it easy to support cheap cross-network references and highly portable data structures, Linear naming also facilitates demand driven migration of tasks and data around the network without requiring explicit guidance from the programmer. Second, <b>linear</b> graph <b>reduction</b> reveals a new characterization of the phenomenon of state. Systems in which state appears are those which depend on certain -global- system properties. State is not a localizable phenomenon, which suggests that our usual object oriented metaphor for state is flawed...|$|R
