313|8497|Public
50|$|Usually the {{jackknife}} {{is easier}} to apply to complex sampling schemes than the bootstrap. Complex sampling schemes may involve stratification, multiple stages (clustering), varying sampling weights (non-response adjustments, calibration, post-stratification) and under unequal-probability sampling designs. Theoretical aspects of both the bootstrap and the jackknife {{can be found in}} Shao and Tu (1995), whereas a basic introduction is accounted in Wolter (2007). The bootstrap estimate of model prediction bias is more precise than jackknife estimates with linear models such as <b>linear</b> <b>discriminant</b> <b>function</b> or multiple regression.|$|E
50|$|Early work on {{statistical}} classification {{was undertaken}} by Fisher, {{in the context}} of two-group problems, leading to Fisher's <b>linear</b> <b>discriminant</b> <b>function</b> as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on slight different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.|$|E
40|$|The <b>linear</b> <b>discriminant</b> <b>function</b> {{which is}} optimal for discriminating between normal {{alternatives}} {{is shown to}} be optimum for the class of elliptical normal mixtures. Some methods for evaluating the probabilities of correct classification of the two-group discrimination problem are discussed. Elliptical distributions elliptical normal mixtures minimum distance classification <b>linear</b> <b>discriminant</b> <b>function</b> success rates...|$|E
40|$|A signal-space {{detector}} {{estimates the}} channel input symbol {{based on the}} location of the #nite-length observation signal in a multi-dimensional signal-space. The decision boundary is formed by a set of hyperplanes. The resulting detector structure consists of <b>linear</b> <b>discriminant</b> <b>functions,</b> threshold detectors, and a Boolean logic function. Our goal is to minimize the number of <b>linear</b> <b>discriminant</b> <b>functions</b> #hyperplanes# with the same or negligible performance loss relative to the maximum likelihood sequence detector. Given all possible #xed-length signal sequences, our procedure #nds a minimal set of hyperplanes by which every pair of opposite class signals can be separated by distance no less than the prescribed minimum distance. The proposed methods are applied to practical magnetic recording channels...|$|R
40|$|<b>Linear</b> <b>discriminant</b> <b>functions</b> {{constitute}} an elementary building block for learning classifiers, and therefore {{have been investigated}} thoroughly. This aim of this contribution is to generalize <b>linear</b> <b>discriminant</b> <b>functions</b> for finite structures such as point patterns, trees, lattices, or graphs. The proposed T-linear <b>discriminant</b> <b>functions</b> for structures {{constitute an}} elementary building block for constructing and analyzing large margin classifiers in the domain XT and more complex supervised and unsupervised structural neural learning machines. The results builds upon and extends the theory of T-spaces to be presented in this meeting and we therefore presume the results and notations from the corresponding abstract [2]. The most important findings of <b>linear</b> <b>discriminant</b> <b>functions</b> {{are based on the}} algebraic and geometric properties of the inner product. Thus, to generalize linear functions for finite structures, we first have to generalize the inner product. A key problem is the absence of an addition for structures. Hence, it is impossible to construct a similarity measure for structures that is bilinear. But we can define a similarity measure for structures that has the same geometric properties as an inner product. To this end, let XT be a T-space over the Euclidean space X. Then the inner product 〈·, · 〉 on X induces a functio...|$|R
40|$|The use of {{computer}} generated holograms to implement feature extraction operations has been achieved. The optical realization {{and use of}} multiple <b>linear</b> <b>discriminant</b> <b>functions</b> on a high-dimensionality feature space for large class pattern recognition is described and initial experimental results are provided...|$|R
40|$|Sufficient {{conditions}} {{are given to}} ensure a better performance of the plug-in version of the covariates adjusted location <b>linear</b> <b>discriminant</b> <b>function</b> in an asymptotic comparison of the overall expected error rate. Our findings generalize several earlier results on discriminant function with covariance. plug-in location <b>linear</b> <b>discriminant</b> <b>function</b> covariance adjustment asymptotic expansion overall expected error rate...|$|E
40|$|It {{is known}} that in the problem of {{statistical}} discriminant analysis, the <b>linear</b> <b>discriminant</b> <b>function</b> performs poorly when the dimension of the data, p, is large. It has been demonstrated by Marco, Young and Turner (1987) that the much simpler Euclidean distance classifier may out-perform the usual <b>linear</b> <b>discriminant</b> <b>function</b> under certain conditions. Their conclusions were arrived at from a simulation experiment which compared the probabilities of misclassification associated with the Euclidean distance classifier {{with those of the}} <b>linear</b> <b>discriminant</b> <b>function,</b> under certain conditions. In this dissertation, the asymptotic expansions of the probabilities of misclassification (the expected actual and expected plug-in error rates) associated with the two discriminant functions are obtained. These error rates are then used to investigate the relative performances of the two methods. Chapter 1 introduces the problem of discriminant analysis and describes the two competing procedures for discriminant analysis and some associated error rates. Then Chapter 2 reviews previous results, in the literature which show that the Euclidean distance classifier can perform better than the <b>linear</b> <b>discriminant</b> <b>function.</b> Chapter 3 gives the asymptotic expansions of the error rates, i. e. the expected actual error rate, and the expected plug-in error rate. The relative performances of the two methods {{on the basis of the}} asymptotic expansions are discussed in Chapter 4. The results show that in general the plug-in error rates for the Euclidean distance classifier give better estimates of the actual error rates for all dimensions of p which were considered, when compared to the <b>linear</b> <b>discriminant</b> <b>function.</b> Furthermore, the actual error rates for the Euclidean distance classifier also seem to give better estimates of the true error rates at large dimensions of p, when compared to the <b>linear</b> <b>discriminant</b> <b>function.</b> Certain situations where the <b>linear</b> <b>discriminant</b> <b>function</b> performs better than the Euclidean distance classifier are also identified. Final conclusions, discussions and recommendations for further work are given in Chapter 5...|$|E
40|$|A general {{integral}} {{expression is}} obtained {{for evaluating the}} performance of Fisher's <b>linear</b> <b>discriminant</b> <b>function</b> applied to spherical distributions (SD). Recurrence relations are given for certain special cases including the spherical gamma, Pearson VII, and generalized Laplace distributions. Some easily obtained {{upper and lower bounds}} for the probabilities of correct classification are shown to improve considerably the only available bounds, given by Haralick (Pattern Recognition 9, 1977). spherical distributions <b>linear</b> <b>discriminant</b> <b>function</b> probabilities of correct classification...|$|E
40|$|Abstract: In {{this paper}} a {{generalization}} of Fisher’s <b>linear</b> <b>discriminant</b> is pro-posed. With this new procedure {{it is possible}} to estimate <b>linear</b> <b>discriminant</b> <b>functions</b> which are not affected by outlying observations. The proposed method and the classical method are compared by applying both to real and simulated data sets. The generalized approach has shown advantages over the classical one...|$|R
40|$|A novel {{supervised}} learning method is presented by combining <b>linear</b> <b>discriminant</b> <b>functions</b> with neural networks. The proposed method {{results in a}} tree-structured hybrid architecture. Due to constructive learning, the binary tree hierarchical architecture is automatically generated by a controlled growing process for a specific {{supervised learning}} task. Unlike the classic decision tree, the <b>linear</b> <b>discriminant</b> <b>functions</b> are merely employed in the intermediate level of the tree for heuristically partitioning a large and complicated task into several smaller and simpler subtasks in the proposed method. These subtasks are dealt with by component neural networks at {{the leaves of the}} tree accordingly. For constructive learning, growing and credit-assignment algorithms are developed to serve for the hybrid architecture. The proposed architecture provides an efficient way to apply existing neural networks (e. g. multi-layered perceptron) for solving a large scale problem. We have alre [...] ...|$|R
40|$|On 24 August 2016, the M 6. 2 Amatrice {{earthquake}} struck central Italy, well-known as a seismically active region, causing {{considerable damage}} to buildings {{in the town of}} Amatrice and the surrounding area. Damage from this earthquake was assessed quantitatively by means of multitemporal synthetic aperture radar (SAR) coherence and SAR intensity methods using dual-polarized SAR data obtained from the Sentinel- 1 (VV, VH) and ALOS- 2 (HH, HV) satellites. We developed <b>linear</b> <b>discriminant</b> <b>functions</b> based on three items: (1) the differential coherence values; (2) the differential backscattering intensity values of pre- and post-event images; and (3) a binary damage map of the optical pre- and post-event imagery. The accuracy of the proposed model was 84 % for the Sentinel- 1 data and 76 % for the ALOS- 2 data. The damage proxy maps deduced from the <b>linear</b> <b>discriminant</b> <b>functions</b> can be useful in the parcel-by-parcel assessment of building damage and development of spatial models for the allocation of urban search and rescue operations...|$|R
40|$|Partial {{least squares}} <b>linear</b> <b>discriminant</b> <b>function</b> (PLSD) {{as well as}} {{ordinary}} <b>linear</b> <b>discriminant</b> <b>function</b> (LDF) are used in pattern recognition analysis of writer identification based on are patterns extracted from the writings written with Hangul letters by 20 Koreans. Also a simulation study is performed using the Monte Carlo method to compare the performances of PLSD and LDF. PLSD showed remarkably better performance than LDF in the Monte Calro study and slightly better performance {{in the analysis of}} the real pattern recognition data...|$|E
40|$|Theoretical {{accuracies}} are {{studied for}} asymtotic approximations {{of the expected}} probabilities of misclassification (EPMC) when the <b>linear</b> <b>discriminant</b> <b>function</b> is used to classify an observation as coming from one of two multivariate normal populations with a common covariance matrix. The asymptotic approximations considered are the ones under the situation where both the sample sizes and the demensionality are large. We give explicit error bounds for asymptotic approximations of EPMC, based on a general approximation result. We also discuss with a method of obtaining asymptotic expansions for EPMC and their error bounds. asymptotic approximations, error bounds, expected probability of misclassification, <b>linear</b> <b>discriminant</b> <b>function...</b>|$|E
40|$|In {{this paper}} we extend the {{definition}} of the influence function to functionals of more than one distribution, that is, for estimators depending on more than one sample, such as the pooled variance, the pooled covariance matrix, and the linear discriminant analysis coefficients. In this case the appropriate designation should be "partial influence functions," following the analogy with derivatives and partial derivatives. Some useful results are derived, such as an asymptotic variance formula. These results are then applied to several estimators of the Mahalanobis distance between two populations and the <b>linear</b> <b>discriminant</b> <b>function</b> coefficients. asymptotic variance influence function <b>linear</b> <b>discriminant</b> <b>function</b> Mahalanobis distance robust estimators...|$|E
40|$|This paper {{describes}} nasal consonant {{discrimination based}} on vowel independent features; murmur and onset spectrum. Classification based on spectral pattern statistics by <b>linear</b> <b>discriminant</b> <b>functions</b> has shown to be quite successful for a specific speaker, but not so good for a non-specific speaker set consisting of 44 speakers. Several experiments were made to {{determine the influence of}} individual factors, and some tentative conclusions are drawn. 1...|$|R
40|$|In this paper, {{we propose}} {{the problem of}} {{optimizing}} multivariate performance measures from multi-view data, and an effective method to solve it. This problem has two features: the data points are presented by multiple views, and the target of learning is to optimize complex multivariate performance measures. We propose to learn a <b>linear</b> <b>discriminant</b> <b>functions</b> for each view, and combine them to construct a overall multivariate mapping function for mult-view data. To learn {{the parameters of the}} linear dis- criminant functions of different views to optimize multivariate performance measures, we formulate a optimization problem. In this problem, we propose to minimize the complexity of the <b>linear</b> <b>discriminant</b> <b>functions</b> of each view, encourage the consistences of the responses of different views over the same data points, and minimize the upper boundary of a given multivariate performance measure. To optimize this problem, we employ the cutting-plane method in an iterative algorithm. In each iteration, we update a set of constrains, and optimize the mapping function parameter of each view one by one...|$|R
40|$|This {{study was}} carried out to {{determine}} morphometric and meristic characteristics of two populations (wild and cultured) of Cichlasoma festae and to establish whether populations could be discriminated based on morphometric variability. Twenty-two morphometric and four meristic characters were {{used to test the}} hypothesis differentiation. Univariate analysis of variance (ANOVA) from 100 adult specimens showed significant differences (p [*][*]  80  % success rate) by <b>linear</b> <b>discriminant</b> <b>functions</b> that included only four morphometric measures...|$|R
40|$|In {{this note}} we analyze the {{relationship}} between the direction obtained from the minimization of the kurtosis coefficient of the projections of a mixture of multivariate normal distributions and the <b>linear</b> <b>discriminant</b> <b>function.</b> We show that both directions are closely related and, in particular, that given two vector random variables having symmetric distributions with unknown means and the same covariance matrix, the direction which minimizes the kurtosis coefficient of the projection is the <b>linear</b> <b>discriminant</b> <b>function.</b> This result provides a way to compute the discriminant function between two normal populations in the case in which means and common covariance matrix are unknown. Classification Kurtosis coefficient...|$|E
40|$|This paper {{considers}} {{the estimation of}} the actual error rates of Fisher 2 ̆ 7 s <b>linear</b> <b>discriminant</b> <b>function</b> formed from training data which are serially correlated. The asymptotic bias of the so-called plug-in error rate is given for multivariate training data following a first order autoregressive model. Also, the authors 2 ̆ 7 earlier work {{on the performance of}} Fisher 2 ̆ 7 s <b>linear</b> <b>discriminant</b> <b>function</b> relative to its version using maximum likelihood estimates obtained under the appropriate correlation model, is reconsidered. A correction is noted to their results on the differences in the individual error rates of these two linear discriminant functions...|$|E
40|$|AbstractMonte Carlo {{estimates}} {{have been}} obtained for the unconditional probability of misclassification incurred by the “estimative” optimum allocation rule in discriminant analysis involving mixtures of binary and continuous variables. The rule {{is based on the}} location model and leads effectively to a different <b>linear</b> <b>discriminant</b> <b>function</b> for each of the multinomial locations defined by the binary variables. A comparison is made between the Monte Carlo estimates and an approximation based on an asymptotic expansion of the distribution of the location “estimative” <b>linear</b> <b>discriminant</b> <b>function</b> derived by Vlachonikolis. Results are presented for various combinations involving equal sample sizes of 50, 100 and 200; two and three binary variables; one, three and five continuous variables; three different settings of location Mahalanobis distances and several choices of location probabilities...|$|E
30|$|The {{percentage}} of the patients treated with four first premolar extractions was 26.8 %. The {{results showed that the}} variables of lower crowding, lower lip to E-plane, upper crowding, and overjet accounted most for the decision to extract at a very significant level (Sig. 0.000). The discriminant analysis assigned a classification power of 83.9 % to the predictive model (p[*]<[*] 0.0001). Fisher's <b>linear</b> <b>discriminant</b> <b>functions</b> provided a mathematical model, according to which any case can be classified into the adequate treatment group.|$|R
40|$|An {{intelligent}} air-noise recognition {{system is}} described that uses pattern recognition techniques to distinguish noise signatures of five {{different types of}} acoustic sources, including jet planes, propeller planes, a helicopter, train, and wind turbine. Information for classification is calculated using the power spectral density and autocorrelation taken from the output of a single microphone. Using this system, as many as 90 percent of test recordings were correctly identified, indicating that the <b>linear</b> <b>discriminant</b> <b>functions</b> developed {{can be used for}} aerospace source identification...|$|R
30|$|LDA {{models were}} firstly applied on the Europe-wide dataset of moss {{sampling}} sites {{with information on}} land use density around the sampling sites. Model-specific error rates (%) were calculated by means of confusion matrix values (actual vs. predicted values). Charts for the <b>linear</b> <b>discriminant</b> <b>functions</b> were used for plausibility checks. Logistic regression models were built using the same predictors from the LDA models. Confusion matrices and error rates (%) specified for each LR model were calculated and compared with the statistical characteristics of the LDA models.|$|R
40|$|Discriminant {{analysis}} is a statistical analysis method {{is used to}} classify an individual into a certain group which has determined based on the independent variables. In linear discriminant analysis, there are two assumptions to be fulfilled i. e. independent variables have to be multivariate normal distributed and variance covariance matrix of the observed two groups are the same. In this graduating paper is applied Beneish M-Score formula and linier discriminant analysis for classification of cases companies manipulators and non-manipulators are listed in Indonesia Stock Exchange in 2013. <b>Linear</b> <b>discriminant</b> <b>function</b> to continue Beneish M-Score formula to predict the classification, {{in order to obtain}} the percentage of fault classification, to determine the size of the performance of <b>linear</b> <b>discriminant</b> <b>function.</b> Percentage of classification error of 2, 70 percent...|$|E
40|$|Approved {{for public}} release, {{distribution}} is unlimitedThe general two population discrimination problem is discussed briefly under various situations,, Discrimination procedures using the <b>linear</b> <b>discriminant</b> <b>function</b> and a nonparametric procedure due to Ju L Hodges and Ee Fix. which classifies a random variable to a population {{on the basis}} of assigning it to the population which has the nearest observation to an observed value of the random variable are discussed and compared by computing the probabilities of misclassifieation for both procedures when the two populations are normal with equal covariance matrices e Probabilities of misclassifieation are computed for the nonparametric discriminator and the <b>linear</b> <b>discriminant</b> <b>function</b> for two small sample sizes for the case when the two populations being discriminated are exponential,, In this latter case, both discrimination procedures are shown to give high probabilities of misclassifieation for certain values of the parameters of the distribution being discriminated. Regions are given in terms of the parameters of the two exponential distributions where one of the probabilities of error is greater than 0 „ 5 > o A more complete investigation for larger sample sizes is recommended for the <b>linear</b> <b>discriminant</b> <b>function</b> and the nonparametric procedure discussed in this paper for the case when the two populations being discriminated are exponential. [URL]...|$|E
40|$|Graduation date: 1979 Eight scale {{characters}} of known hatchery and wild coho salmon (Oncorhynchus kisutch) were compared, and a <b>linear</b> <b>discriminant</b> <b>function</b> {{was used to}} determine if hatchery and wild adult coho salmon could be correctly identified by their scales. Eighty-two percent of the hatchery and 89...|$|E
40|$|A {{procedure}} for obtaining a reduced complexity signal space detector is proposed {{and applied to}} the EEPR 4 channel combined with d = 1 code. For the channel considered, the signal space detector requires at least 10 observation samples to attain the same minimum distance as the Viterbi algorithm #VA#. The resulting signal space detector can be implemented with 7 <b>linear</b> <b>discriminant</b> <b>functions,</b> 10 threshold detectors, and a Boolean logic function. All coe#cients of these linear functions are from f 0; # 1; # 2 g...|$|R
40|$|A {{strategy}} is developed for identifying cutting tool wear on a face mill by automatically recognizing wear {{patterns in the}} cutting force signal. The strategy uses a mechanistic model development to predict forces on a lathe under conditions of wear and extends that model {{to account for the}} multiple inserts of a face mill. The extended wear model is then verified through experimentation {{over the life of the}} inserts. The predicted force signals are employed to train <b>linear</b> <b>discriminant</b> <b>functions</b> to identify the wear state of the process in a manner suitable for on-line application...|$|R
40|$|The {{ability to}} give a {{prognosis}} for failure of a system is a valuable tool and {{can be applied to}} electric motors. In this paper, three wavelet-based methods have been de-veloped that achieve this goal. Wavelet and filter bank theory, the nearest neighbor rule, and <b>linear</b> <b>discriminant</b> <b>functions</b> are reviewed. A framework for the devel-opment of a fault detection and classification algorithm based on the coefficients calculated from the discrete wavelet transform and using clustering is described. An experimental setup based on RT-Linux is described and results from testing are presented, verifying the analysis...|$|R
40|$|Classification {{of mixed}} {{categorical}} and continuous data is often performed using the location <b>linear</b> <b>discriminant</b> <b>function</b> which assumes across-location homoscedasticity. In this paper, we investigate the hazard arising from a routine {{application of the}} classifier under across-location heteroscedasticity. A limiting and a first-order asymptotic performance index are proposed and studied in a general setting. The first index studies the limiting behavior. The second index corrects the bias due to the finite sample size. Both indexes are illustrated under the assumption of unequal spherical covariance matrices across all the locations. This {{is likely to be}} the case in most classification problems dealing with mixed categorical and continuous data. Results of a numerical study are reported. Location <b>linear</b> <b>discriminant</b> <b>function</b> Across-location heteroscedasticity Expected overall error rate Performance index Asymptotic expansions...|$|E
40|$|We {{proposed}} a permutation test for non-inferiority of the <b>linear</b> <b>discriminant</b> <b>function</b> to the optimal combination of multiple tests {{based on the}} Mann-Whitney statistic estimate of the area under the receiver operating characteristic curve. Monte Carlo simulations showed the proposed test had expected type I error rate and sufficient statistical power under moderate sample sizes. ...|$|E
40|$|AbstractAsymptotic {{confidence}} bounds on {{the location}} parameters of the linear growth curve, asymptotic distribution of the canonical correlations and asymptotic confidence bounds on the discriminatory value for the <b>linear</b> <b>discriminant</b> <b>function</b> are established when a set of independent observations are taken from an elliptical distribution (or from a distribution possessing some properties on the moments) ...|$|E
40|$|The {{ability to}} give a {{prognosis}} for failure of a system is an invaluable tool. In this work, four wavelet-based methods {{have been developed for}} use with DC motors used in automotive applications that achieve this goal. Wavelet and filter bank theory is reviewed, as well as the nearest neighbor rule, the Minkowski p metrics and <b>linear</b> <b>discriminant</b> <b>functions.</b> The framework {{for the development of a}} fault detection and classification algorithm is described. Additionally, an experimental setup based on RT-Linux, and results from testing are presented, verifying the analysis. Copyright c © b...|$|R
30|$|Location {{estimation}} is {{a recent}} interesting research area that 0 exploits the possibilities of modern communication technology. In this paper, we present a new location system for wireless networks that is especially suitable for indoor terminal-based architectures, as it improves both the speed and the memory requirements. The algorithm {{is based on the}} application of <b>linear</b> <b>discriminant</b> <b>functions</b> and Markovian models and its performance has been compared with other systems presented in the literature. Simulation results show a very good performance in reducing the computing time and memory space and displaying an adequate behavior under conditions of few a priori calibration points per position.|$|R
40|$|Assembly tasks can be {{characterized}} by the discrete states of contact between the parts being assembled. However, the robot sensory and control systems operate in the continuous domain, leading {{to the problem of}} associating states and events of the physical system with states and events in the discrete task space. This paper presents a new approach to discretizing sensory data, based on <b>discriminant</b> <b>functions</b> and clustering techniques, for applications in robotic process monitoring and in interpreting human sensory data. The advantage of using <b>linear</b> <b>discriminant</b> <b>functions</b> for discretizing the sensory data is that these functions are learned/trained using supervised learning. This enables the process monitoring system to improve its performance with practice and to track changes in the assembly process parameters over the long term. Clustering techniques are used to classify the sensory data for use by <b>discriminant</b> <b>function</b> learning algorithms. Since clustering is a form of unsupervi [...] ...|$|R
