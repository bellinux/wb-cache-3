1402|1308|Public
5|$|At the time, {{the general}} method {{to compute the}} inverse square root was to {{calculate}} an approximation for , then revise that approximation via another method until it came within an acceptable error range of the actual result. Common software methods in the early 1990s drew approximations from a <b>lookup</b> <b>table.</b> The key of the fast inverse square root was to directly compute an approximation by utilizing the structure of floating-point numbers, proving faster than table lookups. The algorithm was approximately four times faster than computing the square root with another method and calculating the reciprocal via floating point division. The algorithm was designed with the IEEE 754-1985 32-bit floating point specification in mind, but investigation from Chris Lomont showed {{that it could be}} implemented in other floating point specifications.|$|E
5|$|Byte {{magazine}} {{published an}} assembly language version of Spacewar in 1977 that {{ran on the}} Altair 8800 and other Intel 8080-based microcomputers using an oscilloscope as the graphical display and a <b>lookup</b> <b>table</b> for orbits, {{as well as a}} three-dimensional variant in 1979 written in Tiny BASIC. More modern recreations of the game for computers have been made as well. An emulated version of the original game, based on the original source code made publicly available by Martin Graetz and running in a JavaScript PDP-1 emulator, was made available to play on the internet in 2012. The only working PDP-1s that are known to exist are kept in the Computer History Museum in Mountain View, California, where demonstrations of the machine are held, which include playing Spacewar.|$|E
25|$|The High Court of Australia {{supported}} {{that the}} reproduction of a <b>lookup</b> <b>table</b> in an EPROM in a third-party hardware lock was an infringement of a literary work.|$|E
30|$|Tatarowicz et al. [12] assume three {{existing}} fragmentations: hash-based, range-based and <b>lookup</b> <b>tables</b> {{for individual}} keys and compare those {{in terms of}} communication cost and throughput. For an efficient management of <b>lookup</b> <b>tables,</b> they experimented with different compression techniques. In particular they argue that for hash-based partitioning, the query decomposition step is a bottleneck. While we apply the notion of <b>lookup</b> <b>tables,</b> too, the authors do not discuss how the fragments are obtained, whereas we propose a semantically guided fragmentation approach here.|$|R
40|$|The {{partitioned}} preemptive EDF scheduling of implicit-deadline sporadic task {{systems on}} an identical multiprocessor plat-form is considered. <b>Lookup</b> <b>tables,</b> at any selected degree of accu-racy, are pre-computed for the multiprocessor platform. By using these <b>lookup</b> <b>tables,</b> task partitioning {{can be performed}} in time polynomial in {{the representation of the}} task system being parti-tioned. Although the partitioning will not in general be optimal, the degree of deviation from optimality is bounded according to the degree of accuracy selected during the pre-computation of the <b>lookup</b> <b>tables.</b> ...|$|R
40|$|Conference Name: 2011 International Conference on Wavelet Analysis and Pattern Recognition, ICWAPR 2011. Conference Address: Guilin, Guangxi, China. Time:July 10, 2011 - July 13, 2011. Hebei University; IEEE Systems, Man and Cybernetics Society; Chongqing University; South China University of Technology; Hong Kong Baptist UniversityIn this paper, an {{improved}} algorithm for the Locally Averaged Interband Scaling <b>Lookup</b> <b>Tables</b> (LAIS-LUT) prediction method is proposed, {{which is based}} on prediction using <b>lookup</b> <b>tables.</b> The predictor of LAIS-LUT is selected from one of two <b>lookup</b> <b>tables</b> {{regardless of whether or not}} it is close to the LAIS-LUT prediction. However, there are cases when the predictions are not accurate. The algorithm provides more choices for the predictor than the LAIS-LUT method, according to size of the values returned by <b>lookup</b> <b>tables</b> and the LAIS estimate. Therefore, the proposed method can give a more accurate prediction than LAIS-LUT method. Experimental results show that the proposed method outperforms both LUT and LAIS-LUT methods. In addition, the proposed algorithm is comparable to the Multiband <b>Lookup</b> <b>Tables</b> (M-LUT) method. ? 2011 IEEE...|$|R
25|$|BSD ar utility {{traditionally}} {{does not}} handle {{the building of}} a global symbol <b>lookup</b> <b>table,</b> and delegates this task to a separate utility named ranlib, which inserts an architecture-specific file named __.SYMDEF {{at the end of the}} archive.|$|E
25|$|The {{key idea}} of {{tabulation}} hashing is {{to view a}} key as a vector of t r-bit numbers, use a <b>lookup</b> <b>table</b> filled with random values to compute a hash value {{for each of the}} r-bit numbers representing a given key, and combine these values with the bitwise binary exclusive or operation. The choice of r should be made {{in such a way that}} this table is not too large; e.g., so that it fits into the computer's cache memory.|$|E
25|$|Within text strings, {{characters}} are shown using character codes (integers) that map to glyphs {{in the current}} font using an encoding. There {{are a number of}} predefined encodings, including WinAnsi, MacRoman, and a large number of encodings for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman encodings are derived from the historical properties of the Windows and Macintosh operating systems, fonts using these encodings work equally well on any platform.) PDF can specify a predefined encoding to use, the font's built-in encoding or provide a <b>lookup</b> <b>table</b> of differences to a predefined or built-in encoding (not recommended with TrueType fonts). The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.|$|E
40|$|The logic {{blocks of}} most FPGAs contain {{clusters}} of <b>lookup</b> <b>tables</b> and flipflops, yet {{little is known}} about good choices for key parameters: How many <b>lookup</b> <b>tables</b> should a cluster contain, how should FPGA routing flexibility change as cluster size changes, and how many inputs should programmable routing provide each cluster...|$|R
40|$|In {{this paper}} we {{introduce}} a new method to identify IP cores in an FPGA by analyzing the content of <b>lookup</b> <b>tables.</b> This techniques {{can be used to}} identify registered cores for IP protection against unlicensed usage. We show methods to extract the content of the <b>lookup</b> <b>tables</b> in a design from a binary bitfile of Xilinx Virtex-II and Virtex-II Pro FPGAs. To identify a core, we compare the number of unique functions from <b>lookup</b> <b>tables</b> of the core with the <b>lookup</b> <b>tables</b> extracted from a product with an FPGA from an accused company. Also placement information can be used for increasing the reliability of the result. With these methods, no additional sources or information must be inquired from the accused company. These techniques can be used for netlist and bitfile cores, so a wide spectrum of cores can be identified. 1...|$|R
40|$|Some {{industrial}} {{systems are}} difficult to formally verify due to their large scale. In particular, {{the widespread use of}} <b>lookup</b> <b>tables</b> in embedded systems across diverse industries, such as aeronautics and automotive systems, create a critical obstacle to the scalability of formal verification. This paper presents Osiris, a tool that automatically computes abstractions of <b>lookup</b> <b>tables.</b> Osiris uses these abstractions to verify a property in first order logic. If the verification fails, Osiris uses a falsification heuristic to search for a violation of the specification. We validate our technique on a public benchmark of an adaptive cruise controller with <b>lookup</b> <b>tables...</b>|$|R
500|$|The {{transformation}} {{can be done}} using a <b>lookup</b> <b>table,</b> such as the following: ...|$|E
500|$|For the film, cinematographer Russell Carpenter used a 1.85 {{aspect ratio}} shot with Arri Alexa XT and M cameras, using the M for fight {{sequences}} and helicopter filming. Camera operator Peter Rosenfeld said, [...] "Russell and Peyton's decision to shoot in 1.85 {{was a good}} call, since at 2.35 there's insufficient height in frame to appreciate the vertical aspects of [...] going from standing full-size to falling {{through a crack in}} the floor." [...] Carpenter and Technicolor also devised a <b>lookup</b> <b>table</b> (LUT) to darken the color palette. Carpenter said, [...] "For a lot of recent comedies I've kept my LUTs kind of 'Kodak' – saturated and upbeat. But this show needed something different that affected skin tones and the Ant-Man suit, which dates back to the 1980s, so it looks a little run-down. What I loved about this LUT was how it allowed the costume to retain the color but took it from fire-engine red to something a little more weathered." ...|$|E
500|$|Co-director Johnson {{would go}} over each {{scene with the}} animators, {{sometimes}} acting out the scene, if necessary. The animators would create a [...] "dope sheet"—in which a shot was broken down, frame by frame—to account for key [...] "hits". The animators would then shoot tests of the scene, often shooting on [...] "2s" [...] or [...] "4s" [...] (meaning shooting just every second or fourth frame of what would appear in the final animation). Johnson explained: [...] "The next day, when they'd finish their test/rehearsal, we'd cut it in {{and see how it}} played in the reel and fine-tune from there. We might do some lighting tweaks, performance tweaks or have the art department get in and touch anything that needed it. Then we'd close the curtain and let the animator animate the shot." [...] The animators would sometimes make use of the voice and/or video recordings of the actors, a practice also common in cel animation. Once photographed, the frames were manipulated by a team of [...] "data wranglers." [...] Using a workflow developed by Chris Watts, the frames were downloaded from the camera image cards as RAW files, converted to Cineon files and processed through a [...] "color cube." [...] Cinematographer Pete Kozachik explained: [...] "The color cube is a 3D <b>lookup</b> <b>table</b> created by FilmLight Ltd. that forces the image data into behaving like a particular Eastman Kodak film stock—in this case, 5248, one of my favorites. With this film emulation, we could actually rate our cameras at ASA 100, then take our light meters and spot meters and, with great confidence, shoot as if we were using 5248. Sure enough, the footage would come back and look just like it." [...] The frames could be processed further to generate a TIFF file for viewing on the lighting station computer monitors so lighting, composition and color could be previewed.|$|E
50|$|Phred's {{approach}} to base calling and calculating quality scores was outlined by Ewing et al.. To determine quality scores, Phred first calculates several parameters related to peak shape and peak resolution at each base. Phred then uses these parameters {{to look up}} a corresponding quality score in huge <b>lookup</b> <b>tables.</b> These <b>lookup</b> <b>tables</b> were generated from sequence traces where the correct sequence was known, and are hard coded in Phred; different <b>lookup</b> <b>tables</b> are used for different sequencing chemistries and machines. An evaluation of the accuracy of Phred quality scores {{for a number of}} variations in sequencing chemistry and instrumentation showed that Phred quality scores are highly accurate.|$|R
5000|$|<b>Lookup</b> <b>tables</b> and {{curve fitting}} {{replaced}} by efficient EM-based models e.g. multi-coupled line models.|$|R
50|$|The {{interpolation}} {{data in the}} <b>lookup</b> <b>tables</b> are {{constrained by}} the requirement that continuity of line segments must be preserved, while optimizing for smoothness. Generating these <b>lookup</b> <b>tables</b> is relatively slow, and is {{the major source of}} complexity in the algorithm: the render stage is very simple and fast, and designed to be capable of being performed in real time.|$|R
2500|$|A seven-place <b>lookup</b> <b>table</b> {{might have}} only 100,000 entries, and {{computing}} intermediate results to seven places would generally require interpolation between adjacent entries.|$|E
2500|$|System V ar {{uses the}} special {{filename}} [...] "/" [...] to denote {{that the following}} data entry contains a symbol <b>lookup</b> <b>table,</b> which is used in ar libraries to speed up access. [...] This symbol table is built in three parts which are recorded together as contiguous data.|$|E
2500|$|In fact, {{the room}} can {{just as easily}} be {{redesigned}} to weaken our intuitions. Ned Block's [...] "blockhead" [...] argument suggests that the program could, in theory, be rewritten into a simple <b>lookup</b> <b>table</b> of rules of the form [...] "if the user writes S, reply with P and goto X". At least in principle, any program can be rewritten (or [...] "refactored") into this form, even a brain simulation. In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a memory address—a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what [...] "strong AI" [...] claims. On the other hand, such a <b>lookup</b> <b>table</b> would be ridiculously large (to the point of being physically impossible), and the states could therefore be extremely specific.|$|E
5000|$|Relational {{database}} features, supporting <b>lookup</b> <b>tables</b> and {{the other}} elements of a normalized relational database ...|$|R
50|$|Many 3D <b>lookup</b> <b>tables</b> {{start with}} a magic number to note the type of 3D LUT.|$|R
50|$|Most context {{models are}} {{implemented}} as hash tables. Some small contexts are implemented as direct <b>lookup</b> <b>tables.</b>|$|R
2500|$|The [...] "Millionaire" [...] {{calculator}} {{was introduced}} in 1893. It allowed direct multiplication by any digit - [...] "one turn of the crank for each figure in the multiplier". It contained a mechanical product <b>lookup</b> <b>table,</b> providing units and tens digits by differing lengths of posts. [...] Another direct multiplier {{was part of the}} Moon-Hopkins billing machine; that company was acquired by Burroughs in the early 20th century.|$|E
2500|$|The Elite {{universe}} contains eight galaxies, {{each with}} 256 planets to explore. Due {{to the limited}} capabilities of 8-bit computers, these worlds are procedurally generated. A single seed number is run through a fixed algorithm the appropriate number of times and creates a sequence of numbers determining each planet's complete composition (position in the galaxy, prices of commodities, and name and local details; text strings are chosen numerically from a <b>lookup</b> <b>table</b> and assembled to produce unique descriptions, such as a planet with [...] "carnivorous arts graduates"). This means that no extra memory is needed to store the characteristics of each planet, yet each is unique and has fixed properties. Each galaxy is also procedurally generated from the first. Braben and Bell at first intended to have 248 galaxies, but Acornsoft insisted on a smaller universe to hide the galaxies' mathematical origins.|$|E
2500|$|Munsell’s system became {{extremely}} popular, the {{de facto}} reference for American color standards—used not only for specifying the color of paints and crayons, but also, e.g., electrical wire, beer, and soil color—because it was organized based on perceptual measurements, specified colors via an easily learned and systematic triple of numbers, because the color chips sold in the Munsell Book of Color covered a wide gamut and remained stable over time (rather than fading), {{and because it was}} effectively marketed by Munsell’s Company. In the 1940s, the Optical Society of America made extensive measurements, and adjusted the arrangement of Munsell colors, issuing a set of [...] "renotations". The trouble with the Munsell system for computer graphics applications is that its colors are not specified via any set of simple equations, but only via its foundational measurements: effectively a <b>lookup</b> <b>table.</b> Converting from [...] requires interpolating between that table’s entries, and is extremely computationally expensive in comparison with converting from [...] or [...] which only requires a few simple arithmetic operations.|$|E
50|$|Like DES, a {{software}} implementation would typically store the S-boxes pre-permuted, in 4 1024×32 bit <b>lookup</b> <b>tables.</b>|$|R
30|$|A {{modified}} algorithm {{searching for}} minimum-cost paths with uses <b>lookup</b> <b>tables</b> assigned to graph nodes is presented below.|$|R
50|$|It is {{possible}} to decode LDPC codes on a relatively low-power microprocessor {{by the use of}} <b>lookup</b> <b>tables.</b>|$|R
5000|$|Another common {{optimization}} uses a <b>lookup</b> <b>table</b> indexed by highest order {{coefficients of}} [...] to process {{more than one}} bit of dividend per iteration. [...] Most commonly, a 256-entry <b>lookup</b> <b>table</b> is used, replacing the inner loop over [...] with: ...|$|E
50|$|In {{the field}} of control engineering, a map-based {{controller}} is a controller whose outputs are based on values derived from a pre-defined <b>lookup</b> <b>table.</b> The inputs to the controller are usually values taken from one or more sensors and are used to index the output values in the <b>lookup</b> <b>table.</b> By effectively placing the transfer function as discrete entries within a <b>lookup</b> <b>table,</b> engineers free to modify smaller sections or update the whole list of entries as required.|$|E
50|$|An {{evolution}} of the previous looping approach examines four bits at a time then using a <b>lookup</b> <b>table</b> for the final four bits, which is shown here. A faster looping approach would examine eight bits {{at a time and}} increasing to a 256 entry <b>lookup</b> <b>table.</b>|$|E
5000|$|<b>Lookup</b> <b>tables</b> are {{implemented}} using a computed [...] (assignment to PCL register) into {{a table of}} [...] instructions.|$|R
40|$|Standard Regulation Carbon Intensity <b>Lookup</b> <b>Tables,</b> {{released}} to the public on January 6, 2011, provides a description of the rationale and necessity for the proposed action, and is incorporated by reference herein. On February 24, 2011, the Executive Officer of the California Air Resources Board held a public hearing to consider the adoption of 28 new fuel pathways into the Low Carbon Fuel Standard Regulation Carbon Intensity <b>Lookup</b> <b>Tables</b> set forth in title 17, California Code of Regulations (CCR), section 95486. Section 95486 (b) (1) of the Low Carbon Fuel Standard (LCFS) Regulation contained two carbon intensity <b>lookup</b> <b>tables</b> (Table 6 for gasoline and fuels that substitute for gasoline, and Table 7 for diesel and fuels that substitute for diesel). Together, these tables contain the original 64 Board-approved LCFS fuel pathways. The purpose of the February 24, 2011, hearing was to amend the LCFS carbon intensity <b>Lookup</b> <b>Tables</b> through the adoption of 28 additional fuel pathways. New fuel pathways can be developed by fuel providers or by ARB staff. The amendments heard by the Executive Officer on February 24, 2011, consisted o...|$|R
5000|$|<b>Lookup</b> <b>tables</b> [...] - [...] once again, {{only for}} small fields {{otherwise}} {{the table is}} too large for implementation ...|$|R
