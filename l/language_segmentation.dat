9|68|Public
40|$|Extending beyond today’s Geographical Information System (GIS) {{applications}} and wireless positioning techniques, Location-Based Services (LBS) {{have become a}} new compelling branch and supplied egocentric location information anywhere anytime. In order to speed up interactive location-based queries in moving or field environments effectively, the paper has given a valuable attempt on LBS with natural language and also achieved: (1) Natural language analysis for Geospatial relations; (2) Interactive LBS queries with natural language; (3) Chinese <b>language</b> <b>segmentation</b> and regulation into Geo-SQL; (4) an application framework from phonetic LBS requests to mobile terminal responses in textual message or graphic formats...|$|E
40|$|Spoken {{and written}} {{language}} are two modes of language. When learners aim at higher skill levels, the expected outcome of successful second language learning is usually {{to become a}} fluent speaker and writer who can produce accurate and complex language in the target language. There is an axiomatic difference between speech and writing, but together they form the essential parts of learners’ L 2 skills. The two modes have their own characteristics, and {{there are differences between}} native and nonnative language use. For instance, hesitations and pauses are not visible in the end result of the writing process, but they are characteristic of nonnative spoken language use. The present study is based on the analysis of L 2 English spoken and written productions of 18 L 1 Finnish learners with focus on syntactic complexity. As earlier spoken <b>language</b> <b>segmentation</b> units mostly come from fluency studies, we conducted an experiment with a new unit, the U-unit, and examined how using this unit as the basis of spoken <b>language</b> <b>segmentation</b> affects the results. According to the analysis, written language was more complex than spoken language. However, the difference in the level of complexity was greatest when the traditional units, T-units and AS-units, were used in segmenting the data. Using the U-unit revealed that spoken language may, in fact, be closer to written language in its syntactic complexity than earlier studies had suggested. Therefore, {{further research is needed to}} discover whether the differences in spoken and written learner language are primarily due to the nature of these modes or, rather, to the units and measures used in the analysis...|$|E
40|$|Chinese {{is written}} without using spaces or other word delimiters. Although a text may {{be thought of}} as a {{corresponding}} sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is bene�cial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese <b>language</b> <b>segmentation.</b> 1...|$|E
40|$|The {{length of}} coding {{sequence}} series in microbial genomes {{were regarded as}} a fluctuating system and characterized by the methods of statistical physics. The distribution and the correlatin properties of 50 genomes including bacteria and several archaea were investigated. The distribution was investigated by rank-size analysis (Zipf's law. We found that coding sequence lengths series do not obey Zipf's law contrary to natural languages. The distribution {{was found to be}} more closely to an exponential distribution. The correlation appeared to be similar to natural <b>languages.</b> <b>Segmentation</b> analysis of the series showed to be short range memory systems. Comment: 13 pages, 8 figure...|$|R
40|$|Optical Character Recognition (OCR) is a {{very old}} and of great {{interest}} in pattern recognition field. The recognition of cursive scripts like Persian and Arabic languages is a difficult task as their segmentation suffers from serious problems in different <b>languages.</b> <b>Segmentation</b> {{is a process of}} dividing cursive words into smaller parts in order to decrease complexity and increase accuracy of recognition process. In this paper, an improved segmentation method of the Persian script 1 has been presented and to increase the quality of segmentation, some structural features of Persian language is used to adjust the fragments. This method is robust as well as flexible. It also increases the system’s tolerances to font variations. The proposed method is able to segment existing Persian fonts up to 99. 2 % accuracy...|$|R
5000|$|Word segmentation: Separate a {{chunk of}} {{continuous}} text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those <b>languages</b> text <b>segmentation</b> is a significant task requiring knowledge of the vocabulary and morphology of words in the language.|$|R
40|$|What {{mechanisms}} {{support the}} ability of human infants, adults, and other primates to identify words from fluent speech using distributional regularities? In order to better characterize this ability, we collected data from adults in an artificial <b>language</b> <b>segmentation</b> task similar to Saffran, Newport, and Aslin (1996) in which the length of sentences was systematically varied between groups of participants. We then compared the fit {{of a variety of}} computational models— including simple statistical models of transitional probability and mutual information, a clustering model based on mutual information by Swingley (2005), PARSER (Perruchet & Vintner, 1998), and a Bayesian model. We found that while all models were able to successfully complete the task, fit to the human data varied considerably, with the Bayesian model achieving the highest correlation with our results...|$|E
40|$|The Chinese {{language}} is written without using spaces or other word delimiters. Although a text may {{be thought of}} as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of pre-segmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese <b>language</b> <b>segmentation.</b> Keywords: Chinese segmentation, language models, text compression, statistical models, text mining...|$|E
40|$|Do Slovak-German bilinguals apply native Slovak phonological and lexical {{knowledge}} when segmenting German speech? When Slovaks {{listen to}} their native <b>language,</b> <b>segmentation</b> is impaired when fixed-stress cues are absent (Hanulikova, McQueen & Mitterer, 2010), and, following the Possible-Word Constraint (PWC; Norris, McQueen, Cutler & Butterfield, 1997), lexical candidates are disfavored if segmentation leads to vowelless residues, unless those residues are existing Slovak words. In the present study, fixed-stress cues on German target words were again absent. Nevertheless, {{in support of the}} PWC, both German and Slovak listeners recognized German words (e. g., Rose "rose") faster in syllable contexts (suckrose) than in single-consonant contexts (krose, trose). But only the Slovak listeners recognized, for example, Rose faster in krose than in trose (k is a Slovak word, t is not). It appears that non-native listeners can suppress native stress segmentation procedures, but that they suffer from prevailing interference from native lexical knowledge...|$|E
40|$|We {{describe}} {{a novel approach}} to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target <b>language</b> word <b>segmentation</b> and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We {{describe a}}n efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1...|$|R
40|$|Abstract. Despite {{having a}} large number of speakers, the Kurdish lan-guage is among the less-resourced languages. In this work we {{highlight}} the challenges and problems in providing the required tools and tech-niques for processing texts written in Kurdish. From a high-level per-spective, the main challenges are: the inherent diversity of the <b>language,</b> standardization and <b>segmentation</b> issues, and the lack of language re-sources. ...|$|R
40|$|Word Segmentation is the {{foremost}} obligatory task {{in almost all}} the NLP applications where the initial phase requires tokenization of input into words. Urdu is amongst the Asian languages that face word segmentation challenge. However, unlike other Asian <b>languages,</b> word <b>segmentation</b> in Urdu not only has space omission errors but also space insertion errors. This paper discusses how orthographic and linguistic features in Urdu trigger these two problems. It also discusses the work {{that has been done}} to tokenize input text. We employ a hybrid solution that performs an n-gram ranking on top of rule based maximum matching heuristic. Our best technique gives an error detection of 85. 8 % and overall accuracy of 95. 8 %. Further issues and possible future directions are also discussed. ...|$|R
40|$|Listeners use {{prosodic}} cues {{to facilitate}} lexical access when listening to fluent speech {{in their native}} language [1], [2]. This study investigates second language learners ’ ability to segment words from continuous speech, {{and the effect of}} native language prosodic structure on the perception of word boundaries in the second language. In an experiment conducted with natural language stimuli, English speakers learned words and then listened to fluent speech in a previously unfamiliar language (Finnish). After listening to fluent speech, they chose between pairs of correctly segmented real words and incorrectly segmented nonwords, to identify possible words of Finnish. Results show that English speakers do exhibit a bias towards identifying words with first-syllable stress as real, likely an effect of a native <b>language</b> <b>segmentation</b> strategy [2]. However, the test group that learned words and then listened to fluent speech performed better than the group that did not listen to fluent speech. This suggests that learning words and hearing them in context aids in second language speech segmentation. More successful speech segmentation, in turn, promotes the learning of other phonological patterns, which makes learners more accurate at identifying possible words in the second language. 1...|$|E
40|$|Data mining {{applications}} that work over input of very large scale (web-scale problems) pose challenges that are {{new and exciting}} both academi-cally and commercially. Any web-scale algorithm must be robust (deal-ing gracefully with the inevitable data noise), scalable (capable of ef-ficiently processing large input) and reasonably automated (as human intervention is very costly and often impossible on such scales). This thesis consists of two parts. In the first part, I explore scalabil-ity of methods that derive a semantic representation of plain text docu-ments. The focus will be entirely on unsupervised techniques, that is, on methods that do not make use of manually annotated resources or hu-man input. I develop and present scalable algorithms for Latent Seman-tic Analysis (LSA) and Latent Dirichlet Allocation (LDA), two general-purpose statistical methods for semantic analysis that serve as building blocks for more concrete, applied algorithms. Scalability is achieved by building the semantic models in a constant amount of memory and dis-tributing the computation over a cluster of autonomous computers, con-nected by a high-latency network. In addition, the novel LSA training algorithm operates {{in a single pass}} over the training data, allowing con-tinuous online training over infinite-sized training streams. The second part of the thesis deals with possible applications of these general semantic algorithms. I present my research in the field of In-formation Retrieval (IR), including work on topic segmentation of plain-text documents, on document-document similarities (“semantic brows-ing”) in digital libraries and on <b>language</b> <b>segmentation</b> of documents written in multiple languages. ii...|$|E
40|$|For {{a society}} based upon laws and reason, {{it has become}} too easy for {{us to believe that}} we live in a world without them. And given that our {{linguistics}} wisdom was originally motivated by the search for rules, it seems strange that we now consider these rules to be the exceptions and take exceptions as the norm. The current task of contemporary computational linguistics is to describe these exceptions. In particular, it suffices for most language processing needs, to just describe the argument and predicate within an elementary sentence, under the framework of local grammar. Therefore, a corpus-based approach to the Chinese Word Segmentation problem is proposed, as the first step towards a local grammar for the Chinese language. The two main issues with existing lexicon-based approaches are (a) the classification of unknown character sequences, i. e. sequences that are not listed in the lexicon, and (b) the disambiguation of situations where two candidate words overlap. For (a), we propose an automatic method of enriching the lexicon by comparing candidate sequences to occurrences of the same strings in a manually segmented reference corpus, and using methods of machine learning to select the optimal segmentation for them. These methods are developed in the course of the thesis specifically for this task. The possibility of applying these machine learning method will be discussed in NP-extraction and alignment domain. (b) is approached by designing a general processing framework for Chinese text, which will be called multi-level processing. Under this framework, sentences are recursively split into fragments, according to a language-specific, but domainindependent heuristics. The resulting fragments then define the ultimate boundaries between candidate words and therefore resolve any segmentation ambiguity caused by overlapping sequences. A new shallow semantical annotation is also proposed under the frame work of multi-level processing. A word segmentation algorithm based on these principles has been implemented and tested; results of the evaluation are given and compared to the performance of previous approaches as reported in the literature. The first chapter of this thesis discusses the goals of segmentation and introduces some background concepts. The second chapter analyses the current state-of-theart approach to Chinese <b>language</b> <b>segmentation.</b> Chapter 3 proposes a new corpusbased approach to the identification of unknown words. In chapter 4, a new shallow semantical annotation is also proposed under the framework of multi-level processing...|$|E
50|$|This problem {{overlaps}} to {{some extent}} {{with the problem of}} text segmentation that occurs in some languages which are traditionally written without inter-word spaces, like Chinese and Japanese, compared to writing systems which indicate speech segmentation between words by a word divider, such as the space. However, even for those <b>languages,</b> text <b>segmentation</b> is often much easier than speech segmentation, because the written language usually has little interference between adjacent words, and often contains additional clues not present in speech (such as the use of Chinese characters for word stems in Japanese). Word Boundary Identification can be overcome by NLU approaches such as Patom theory integrated with Role and Reference Grammar (RRG) for languages without spaces between words such as Japanese and Chinese.|$|R
40|$|This paper {{discusses}} {{problems of}} word and sentence segmentation in Thai. Disagreements on word segmentation are caused mostly from compound words. To {{set a standard}} resource and tool of word segmentation, we suggest that only simple words and true compound words should be segmented {{in the process of}} word segmentation. Other compounds can be grouped later by the same means as multiword identification in other <b>languages.</b> Sentence <b>segmentation</b> is also difficult because the boundary of sentence in Thai is fuzzy. We suggest that a discourse should be seen as a combination of clauses rather than sentences. Some discourse clues then can be used to segment these discourse units. The result from sentence segmentation module could be a sequence of segments composed of clauses, which then can be constructed into the discourse structure. ...|$|R
40|$|This paper {{presents}} a data-driven <b>language</b> independent word <b>segmentation</b> {{system that has}} been trained for Chinese corpus at the second Chinese word segmentation bakeoff. The system consists of a base segmentation algorithm and the refining procedures for the undecided character sequences. It does not use any lexicon and the base segmentation is simply done by character bigram and HMM-model is applied for the remaining character sequences. As a final step, high-frequency character trigram 1 modifies the error-prone parts of the text. TT...|$|R
40|$|Abstract. This article {{presents}} {{a combination of}} unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words. First, a language bias for word segmentation is introduced and a simple genetic algorithm {{is used in the}} search for a segmentation that corresponds to the best bias value. In the second phase, the words segmented by the genetic algorithm are used as an input for the first order decision list learner CLOG. The result is a set of first order rules which can be used for segmentation of unseen words. When applied on either the training data or unseen data, these rules produce segmentations which are linguistically meaningful, and to a large degree conforming to the annotation provided. Keywords: unsupervised machine learning, inductive logic programming, natural <b>language,</b> word <b>segmentation</b> 1...|$|R
40|$|Learning word {{representations}} {{has recently}} seen much success in computational linguis-tics. However, assuming sequences of word tokens as input to linguistic analysis is of-ten unjustified. For many <b>languages</b> word <b>segmentation</b> is a non-trivial task and natu-rally occurring text {{is sometimes a}} mixture of natural language strings and other character data. We propose to learn text representa-tions directly from raw character sequences by training a Simple Recurrent Network to predict the next character in text. The net-representations of the character sequences it sees. To demonstrate {{the usefulness of the}} learned text embeddings, we use them as features in a supervised character level text segmentation and labeling task: recognizing spans of text containing programming lan-guage code. By using the embeddings as fea-tures we are able to substantially improve over a baseline which uses only surface char-acter n-grams. 1...|$|R
40|$|The system {{presented}} in this paper is based upon a phrase-based statistical machine transliteration (SMT) framework. The SMT system’s log-linear model is aug-mented with a set of features specifically suited to the task of transliteration. In par-ticular our model utilizes a feature based on a joint source-channel model, and a fea-ture based on a maximum entropy model that predicts target grapheme sequences using the local context of graphemes and grapheme sequences in both source and target <b>languages.</b> The <b>segmentation</b> for our approach was performed using a non-parametric Bayesian co-segmentation model, and in this paper we present ex-periments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m 2 m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. ...|$|R
40|$|The {{programme}} for the segmentation of {{a speech}} into fonems was created {{as a part of}} the master´s thesis. This programme was made in the programme Matlab and consists of several scripts. The programme serves for automatic segmentation. Speech segmentation is the process of identifying the boundaries between phonemes in spoken natural <b>languages.</b> Automatic <b>segmentation</b> is based on vector quantization. In the first step of algorithm, feature extraction is realized. Then speech segments are assigned to calculated centroids. Position where centroid is changed is marked as a boundary of phoneme. The audiorecords were elaborated by the programme and a operation of the automatic segmentation was analysed. A detailed manual was created to the programme too. Individual used methods of the elaboration {{of a speech}} were in the master´s thesis briefly descripted, its implementations in the programme and reasons of set of its parameters...|$|R
40|$|Document {{segmentation}} {{is one of}} {{the critical}} phases in machine recognition of any <b>language.</b> Correct <b>segmentation</b> of individual symbols decides the accuracy of character recognition technique. It is used to decompose image of a sequence of characters into sub images of individual symbols by segmenting lines and words. Devnagari is the most popular script in India. It is used for writing Hindi, Marathi, Sanskrit and Nepali languages. Moreover, Hindi is the third most popular language in the world. Devnagari documents consist of vowels, consonants and various modifiers. Hence proper segmentation of Devnagari word is challenging. A simple histogram based approach to segment Devnagari documents is proposed in this paper. Various challenges in segmentation of Devnagari script are also discussed. Comment: 8 pages; 4 figures; 8 tables; journal paper: International Journal of Computer Science, Engineering and Information Technology (IJCSEIT), Vol. 1, No. 3, August 201...|$|R
40|$|International audienceMultilingual {{terminology}} acquisition from comparable corpora {{has been}} attracting {{the interest of}} researchers for twenty years, but challenges still remain. Bilingual term alignment, a subtask of multilingual terminology acquisition, requires a pre-processing step, because term structure may differ according to the language. Morphologically constructed terms should be segmented {{in order to be}} aligned with their equivalents in other languages. This article addresses the translation of complex terms using a compositional approach. We focus on the pre-processing of such terms and introduce a domain-oriented splitting method that we apply to compound terms belonging to two domains and four <b>languages.</b> The <b>segmentations</b> are used as input to a translation step. We evaluate which percentage of segmentations can be correctly translated by a compositional approach, and which splitting strategy (precision or recall-oriented) performs better. The results are compared to those obtained with the reference segmentations and with a corpus-base splitting method. Our method is close to the reference segmentation and outperforms the corpus-based method...|$|R
40|$|Final {{syllable}} invariance {{is characteristic}} of diminutives (e. g., doggie), which are a pervasive feature of the child-directed speech registers of many languages. Invariance in word endings {{has been shown to}} facilitate word segmentation (Kempe, Brooks & Gillis, 2005) in an incidental-learning paradigm using synthesized Dutch pseudo-nouns. To broaden the crosslinguistic evidence for this invariance effect, and to increase its ecological validity, adult English speakers (N= 276) were exposed to naturally spoken Dutch or Russian pseudo-nouns presented in sentence contexts. A forced-choice test was given to assess target recognition, with foils comprising unfamiliar syllable combinations in Experiments 1 and 2, and syllable combinations straddling word boundaries in Experiment 3. A control group (N= 210) received the recognition test with no prior exposure to targets. Recognition performance improved with increasing final-syllable rhyme invariance, with larger increases for the experimental group. This confirms that word-ending invariance is a valid segmentation cue in artificial as well as naturalistic speech, and that diminutives may aid segmentation in a number of <b>languages.</b> Word <b>Segmentation</b> in Natural Speech. 3 Diminutives Facilitate Word Segmentation in Natural Speech: Cross-Linguistic Evidenc...|$|R
40|$|We {{conducted}} a preliminary study {{to examine whether}} Chinese readers' spontaneous word segmentation processing {{is consistent with the}} national standard rules of word segmentation based on the Contemporary Chinese <b>language</b> word <b>segmentation</b> specification for information processing (CCLWSSIP). Participants were asked to segment Chinese sentences into individual words according to their prior knowledge of words. The results showed that Chinese readers did not follow the segmentation rules of the CCLWSSIP, and their word segmentation processing was influenced by the syntactic categories of consecutive words. In many cases, the participants did not consider the auxiliary words, adverbs, adjectives, nouns, verbs, numerals and quantifiers as single word units. Generally, Chinese readers tended to combine function words with content words to form single word units, indicating they were inclined to chunk single words into large information units during word segmentation. Additionally, the "overextension of monosyllable words" hypothesis was tested and it might need to be corrected to some degree, implying that word length have an implicit influence on Chinese readers' segmentation processing. Implications of these results for models of word recognition and eye movement control are discussed...|$|R
30|$|Nextword may miss some MBPs. The {{main reason}} of missing MBPs {{is due to}} the {{segmentation}} error of the keywords in the Chinese <b>language.</b> The keyword <b>segmentation</b> in the Chinese language is different from that of English, for the standard Chinese writing contains no space between characters. Thus, different segmentation tools may return different segmentation results. Even for the same keyword using the same segmentation tool, the results may still be different with different text. For the keywords we queried in the experiments, the segmentation result in the MBPs could differ from that in the search query. The MBPs with different segmentation results would be missed.|$|R
40|$|This paper {{presents}} {{an overview of}} audio indexing, which has emerged very recently as a research topic {{with the development of}} Internet. A lot of data, including audio data, are currently not indexed by web search engines, and audio indexing consists in finding good descriptors of audio documents which can be used as indexes for archiving and search. We discuss speech/music <b>segmentation,</b> <b>language</b> identification, speaker tracking and speaker indexing, and propose some research directions for other audio descriptors which have not been used in the framework of audio indexing, namely key sounds detection, keywords detection, and themes detection. We finally conclude this overview and give a few promising and key perspectives...|$|R
40|$|This study {{examines}} {{the role of}} distributional and metrical cues in the learning of an artificial language. French subjects first heard {{a sample of the}} language (a continuous string of nonce syllables) and then were required to decide whether bi- or trisyllabic items belonged to the language or not. One group listened to a sample whithout metrical cues, while another group received the same <b>language</b> with metrical <b>segmentation</b> cues - alternation of short and long (accented) syllables. The presence of these metrical cues improved the discovery and the learning of the artificial words. The implications of this research for lexical segmentation and the learning of a new language learning are discussed...|$|R
40|$|We {{describe}} SRI's recognition {{system as}} used in the 2001 DARPA Speech in Noisy Environments (SPINE) evaluation. The SPINE task involves recognition of speech in simulated military environments. The task had some unique challenges, including segmentation of foreground speech from noisy background, the need for robust acoustic models to handle noisy speech, and development of language models from limited training data. In developing the SRI evaluation system for this task, we addressed each of these challenges {{using a combination of}} state-of-the-art techniques, including several types of feature normalization, model adaptation, class-based <b>language</b> modeling, multi-pass <b>segmentation</b> and recognition, and word posterior-based decoding and system combination 1...|$|R
40|$|The paper {{analyses}} {{the effect}} of mother tongue on labor market outcomes of Swiss residents. This type of analysis can shed light on an important policy question. Is the Swiss labor market well integrated, or can one find instead <b>segmentation</b> along <b>language</b> borders? Improving on previous research in this area, we use a nationally representative household survey, the Swiss Household Panel 1999 and 2000, and we explicitly account for self-selection of workers into language areas. Overall, we find no {{evidence to suggest that}} the Swiss labor market is not perfectly integrated or that internal migrants are positively selected. <b>language,</b> labor market <b>segmentation,</b> Swiss household panel...|$|R
40|$|We {{report the}} design and {{development}} of Thirukkural, the first text-to-speech converter in Tamil. Syllables of different lengths have been selected as units since Tamil is a syllabic <b>language.</b> An automatic <b>segmentation</b> algorithm [8] has been devised for segmenting syllables into consonant and vowel. The units are pitch marked using the discrete cosine transform-spectral autocorrelation function (DCTSAF) [6]. Prosodic information is captured in tables based on extensive observation of spoken Tamil. During synthesis, DCT based pitch modification [3][7][11] is applied for both waveform interpolation and modifying pitch contour for different sentence modalities. Thirukkural is designed in VC++ and runs on Windows 95 / 98 /NT. Perceptual evaluation by natives show that the synthesized speech is intelligible and fairly natural...|$|R
40|$|Colloque sur invitation. internationale. International audienceThis paper {{presents}} {{an overview of}} audio-indexing which has emerged very recently as a research topic {{with the development of}} Internet. A lot of data, including audio data, are currently not indexed by web search engines, and audio indexing consists in finding good descriptors of audio documents which can be used as indexes for archiving and search. We discuss speech/music <b>segmentation,</b> <b>language</b> identification, speaker tracking and speaker indexing, and propose some research directions for other audio descriptors which have not been used in the framework of audio indexing, namely key sounds detection, keywords detection, and themes detection. We finally conclude this overview and give a few promising and key perspectives...|$|R
30|$|A {{national}} {{standard for the}} segmentation of modern Chinese {{was first published in}} 1993 – the 信息处理用现代汉语分词规范 Xinxi chuli yong xiandai hanyu fenci guifan “Contemporary Chinese <b>language</b> word <b>segmentation</b> specification for information processing” (GB 13715) (Liu et al. 1994). This, however, has not solved the problem, in part, because readers do not perceive word boundaries uniformly, and the parsing during the reading process is, horribile dictu, not necessarily aligned with the {{national standard}} (Liu et al. 2013) 4. Moreover, there are theoretical obstacles regarding how to segment certain word-types. Rule-based algorithms need to be given definitions of which prefixes and suffixes to include, but any linguistic definition of what constitutes a “word,” and what therefore is a prefix to it, is to a degree arbitrary. In European languages this is obscured by the established convention of spaces. For Asian writing systems there exist various strategies for segmentation, which have different strengths and weaknesses (Xia 2000 : 5 – 6). Nevertheless, as two eminent experts in the field write in a recent overview article, it is still true that “Chinese word segmentation remains a challenging topic in Chinese computational linguistics” (Huang and Xue 2012 : 494).|$|R
40|$|Various infant {{studies suggest}} that {{statistical}} regularities in the speech stream (e. g. transitional probabilities) {{are one of the}} first speech segmentation cues available. Statistical learning may serve as a mechanism for learning various <b>language</b> specific <b>segmentation</b> cues (e. g. stress segmentation by English speakers). To test this possibility we exposed adults to an artificial language in which all words had a novel acoustic cue on the final syllable. Subjects were presented with a continuous stream of synthesized speech in which the words were repeated in random order. Subjects were then given a new set of words to see if they had learned the acoustic cue and generalized it to new stimuli. Finally, subjects were exposed to a competition stream in which the transitional probability and novel acoustic cues conflicted to see which cue they preferred to use for segmentation. Results on the word-learning test suggest that subjects were able to segment the first exposure stream, however, on the cue transfer test they did not display any evidence of learning the relationship between word boundaries and the novel acoustic cue. Subjects were able to learn statistical words from the competition stream despite extra intervening syllables...|$|R
50|$|LPT {{considers}} {{listening to}} understand {{the ability of a}} person to listen and understand a nonnative language at real world rates of speech with all the attributes of pronunciation, fluency rhythm and intonation. A review of data by Endress and Hauser (2010) demonstrates that adults listen and interpret language based on prosodic cues and these cues vary from language to language. The authors explain that listening to a native <b>language</b> has <b>segmentations,</b> similar to beads on a string, which differentiate words allowing the context to be fully understood. However, for non-native listeners, even if they have studied the particular second language, they have a difficult time segmenting words causing them to simply run together. For example, the authors explain stress (loudness, pitch, and duration) on certain syllables is different between English and French and although listeners can discern some prosodic cues of a foreign language, lack of language power hampers competent understanding. adult learners of a non-native language depend on lexical information that is already ingrained in their native language. Non-native speech may have different sets of languageprocessing during parsing, which may impact comprehension. These differences can be highly situational, such as in academic listening.|$|R
