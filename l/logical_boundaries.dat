15|24|Public
5000|$|There {{are many}} roads like this {{throughout}} the area, leading to {{duplication of names}} in different counties. In Fulton, [...] "Roswell Road" [...] refers to Georgia 9 through northern Atlanta and across Sandy Springs, {{in addition to the}} above-mentioned use in Cobb, for example. Numeric street addressing is done by county as well, with the origin usually being at one corner of the town square in the county seat. The U.S. Postal Service ignores these actual and <b>logical</b> <b>boundaries</b> however, overlapping ZIP codes and their associated place names across counties. The Cumberland/Galleria area has Cobb's numbers and an [...] "SE" [...] suffix, but is called [...] "Atlanta" [...] by the USPS (despite being Vinings, which the USPS ironically calls [...] "unacceptable"), which can confuse visitors to think it is far away in southeast Atlanta.|$|E
40|$|Businesses {{are moving}} from a {{centralized}} core infrastructure to a decentralized one. This causes {{a number of issues}} as our businesses grow. The main issue is data flow. How do businesses maintain security when data is continually moving to the edges of our <b>logical</b> <b>boundaries?</b> Many solutions have come up to address this problem, endpoint security software, restricted rights, VPNs, strong Bell-LaPadula models, and more restrictive user policies. A solution is needed that solves the problem while allowing the user to complete business tasks efficiently and without incident. Why not centralize again? The solution proposed in this paper works around keeping business workflows decentralized while centralizing data through application streaming...|$|E
40|$|This {{paper is}} from the SANS Institute Reading Room site. Reposting is not {{permitted}} without express written permission. Endpoint Security through Application Streaming Businesses are moving from a centralized core infrastructure to a decentralized one. This causes {{a number of issues}} as our businesses grow. The main issue is data flow. How do businesses maintain security when data is continually moving to the edges of our <b>logical</b> <b>boundaries?</b> Many solutions have come up to address this problem, endpoint security software, restricted rights, VPNs, strong Bell-LaPadula models, and more restrictive user policies. A solution is needed that solves the problem while allowing the user to comp [...] . Copyright SANS Institut...|$|E
40|$|Table {{of content}} 1. PP {{introduction}} [...] 4 1. 1 PP Reference [...] . 4 1. 2 PP Overview [...] 4 2. TOE Description [...] 5 2. 1 Protection of biometric systems [...] . 5 2. 2 TOE configuration and TOE environment [...] . 6 2. 3 TOE boundary [...] . 6 2. 3. 1 Physical boundary [...] . 7 2. 3. 2 <b>Logical</b> <b>boundary</b> [...] ...|$|R
50|$|Telepathy is a {{software}} framework {{which can be}} used to make software for interpersonal communications such as instant messaging, Voice over IP or videoconferencing. Telepathy enables the creation of communications applications using components via the D-Bus inter-process communication mechanism. Through this it aims to simplify development of communications applications and promote code reuse within the free software and open source communities by defining a <b>logical</b> <b>boundary</b> between the applications and underlying network protocols.|$|R
50|$|The {{software}} {{is driven by}} a semi-stateful event correlation engine. This means that the engine records and thus knows its internal state, but only uses it to some extent to link together logically related elements for the same device, in order to draw a conclusion (i.e. to generate an alert). In Octopussy the semi-stateful correlation engine, with its so called sliding window (a shifting window being the <b>logical</b> <b>boundary</b> {{of a number of}} events during a certain period of time), is capable of comparing known past events with present ones based on a limited number of comparative values.|$|R
40|$|Despite recent {{advances}} in reasoning about concurrent data structure libraries, the largest implementations in java. util. concurrent {{have yet to be}} verified. The key issue lies in the development of modular specifications, which provide clear <b>logical</b> <b>boundaries</b> between clients and implementations. A solution is to use {{recent advances}} in fine-grained con- currency reasoning, in particular the introduction of abstract atomicity to concurrent separation logic reasoning. We present two specifications of concurrent maps, both providing the clear boundaries we seek. We show that these specifications are equivalent, in that they can be built from each other. We show how we can verify client programs, such as a concurrent set and a producer-consumer client. We also give a substan- tial first proof that the main operations of ConcurrentSkipListMap in java. util. concurrent satisfy the map specification. This work demon- strates that we now have the technology to verify the largest implemen- tations in java. util. concurrent...|$|E
40|$|This article {{deals with}} {{the theme of the}} {{shattering}} of the body in the contemporary urban environment as it appears in Paul Auster’s The New York Trilogy. In the three novels of The Trilogy, the writer {{deals with the}} complex interaction between the metropolitan cityscape and the human body: moving through a labyrinthine New York whose dimensions exceed by far the human scale, Auster’s heroes experience conditions of depersonalization and assumed absorption by space, which finally lead them to the extreme state of the dissolution of their bodies. This transgression of the physical body, that “melting into the walls of the city” (139), is however celebrated by Auster as a form of transgression of <b>logical</b> <b>boundaries,</b> which permits thought to reveal its poetic nature: the heroes may disappear in the end, however they leave behind them a written story – product and witness of their existence of their existence in the postmodern city that escapes all anthropomorphic qualities in order to reveal a more archaic and inspiring imagery...|$|E
40|$|Today’s live {{training}} {{environment is}} comprised of many systems {{in various states of}} configurations with a limited ability to leverage shared services. The future of live training systems will evolve to a Training as a Service (TaaS) state to reduce overall operating costs, implement new technologies to improve the training experience, and centrally manage the training exercise of distributed training systems. With a TaaS approach to system architecture, a number of new cybersecurity and DoD Information Assurance requirements will need to be implemented in order to ensure the Confidentiality, Integrity, and Availability of DoD information Systems. Previous papers (Lanman and Linos, 2012) have outlined in greater detail the motivation and migration strategy for a pilot study on implementing TaaS within the Common Training Instrumentation Architecture (CTIA) used by the Army’s Live Training Transformation (LT 2) Product Line. This paper will present a number of cybersecurity threats, challenges, requirements, and commercial best practices for secure operations as well as Certification and Accreditation (C&A) requirements of a TaaS approach. Threats not previously present in isolated system architectures will now need to be countered with appropriate defense mechanisms across physical and <b>logical</b> <b>boundaries.</b> This paper will describe and discuss cloud computin...|$|E
50|$|Cross site. Cross-site {{cooking is}} similar in concept to cross-site {{scripting}}, cross-site request forgery, cross-site tracing, cross-zone scripting etc., in that it involves {{the ability to move}} data or code between different web sites (or in some cases, between e-mail / instant messages and sites). These problems are linked to the fact that a web browser is a shared platform for different information / applications / sites. Only <b>logical</b> security <b>boundaries</b> maintained by browsers ensures that one site cannot corrupt or steal data from another. However a browser exploit such as cross-site cooking can be used to move things across the <b>logical</b> security <b>boundaries.</b>|$|R
40|$|Abstract. Several N- 3 {{compressors}} {{are proposed}} against {{the limitations of}} conventional compressors on power optimization. Larger module based N- 3 compressors provide more space for EDA tools to make the full use of various strategies and low-power cells. The number of modules and <b>logical</b> <b>boundary</b> can be reduced when the compression tree is constructed by N- 3 compressors. The partial products of the Booth multipliers 48 X 48 and 64 X 64 are compressed by different compressors. The results show that, under same constraint conditions, the operating speed of compression tree constructed by N- 3 compressors is ensured and a significant reduction of power is obtained...|$|R
40|$|The b-boundary is a {{mathematical}} tool able {{to attach a}} topo- <b>logical</b> <b>boundary</b> to incomplete Lorentzian manifolds using a Riemani- ann metric called the Schmidt metric on the frame bundle. This has been a motivation for using the b-boundary to study gravitational sin- gularities in General Relativity. In this paper we give the general form of the Schmidt metric {{in the case of}} 1 + 1 Lorentzian manifolds. Fur- thermore, we give a general formula for the Ricci scalar of the Schmidt metric in terms of the Ricci scalar of the Lorentzian manifold and calcu- late explicitly the curvature of the Schmidt metric for several Lorentzian surfaces...|$|R
40|$|Abstract. This paper synthesizes two {{trends in}} the {{engineering}} of agent-based systems. One, modern agent-oriented methodologies deal with the key aspects of software development including requirements acquisition, architecture, and design, but can benefit from a stronger treatment of flexible interactions. Two, commitment protocols declaratively capture interactions among business partners, thus facilitating flexible behavior and a sophisticated notion of compliance. However, they lack support for engineering concerns such as inducing the desired roles and selecting the right protocols. This paper combines these two directions. For concrete-ness, we choose the Tropos methodology, which is strong in its require-ments analysis, but our results can be ported to other agent-oriented methodologies. Our approach is as follows. First, using Tropos, analyze requirements based on dependencies between actors. Second, select top-level protocols based on the actors ’ hard goals, while respecting the <b>logical</b> <b>boundaries</b> of their interactions. Third, select refined protocols based on the actors’ soft goals. Consequently, Tropos provides a rigorous basis for modeling and composing protocols whereas the protocols help produce perspicu-ous designs that respect the participants ’ autonomy. We evaluate our approach using a large existing case. ...|$|E
40|$|ABSTRACT: Consciousness {{could be}} thought of as the problem to which propositions belong and {{concomitantly}} correspond as they indicate particular responses,signify instances of general solutions, with its essential configurations, rational representations conferential extrinsicness, interfacial interference, syncopated justices, heterogeneous variations testimonies,apodeictic knowledge of ideological tergiversation,sauccesful reality,sleaty sciolisms,tiurated vaticinations,anchorite aperitif anamensial alienisms and manifest subjective acts of resolution. Consciousness in its organization of singular points, series and displacements, is doubly generative; it not only engenders the logical propositions with its determinate dimensions but also its correlates. The equivocality, ambiguity, in the synchronicity of the problem and proposition both in the sets and subsets of the ontological premises and <b>logical</b> <b>boundaries,</b> “error in perception ” arises in the field of consciousness. Far from indicating the subjective and provisional state of empirical knowledge consciousness refers to an ideational objectivity or to a structure constitutive of space and time, the knowledge and the known, the proposition and its correlates. The question of “question” in consciousness does not bear any resemblance to the proposition which subsumes it, but rather it determines its own conditionalities and representationalitiesof and assigns them to its constituents in various permutations and combinations, that are done with corporate signification, personalized manifestation...|$|E
40|$|Laws of {{war have}} been {{carefully}} defined by individual nations’ own codes of law {{as well as}} by supranational bodies. Yet the international scene has seen an increasing movement away from traditionally declared war toward multinational peacekeeping missions geared at containing local conflicts when perceived as potential threats to their respective regions ’ political stability. While individual nations’ laws governing warfare presuppose national sovereignty, the multinational nature of peacekeeping scenarios can blur the lines of command structures, soldiers ’ national loyalties, occupational jurisdiction, and raise profound questions as to which countries ’ moral sense/governmental system is to be the one upheld. Historically increasingly complex international relations have driven increasingly detailed internationally drafted guidelines for countries ’ interactions while at war, yet there are operational, legislative, and moral issues arising in multinational peacekeeping situations which these laws do not address at all. The author analyzes three unique peacekeeping operations in light of these legislative voids and suggests systematic points to consider to the end of protecting the peacekeepers, the national interests of the countries involved, operational matters, and clearly delineating both the objective and <b>logical</b> <b>boundaries</b> of a given multinational peacekeeping mission...|$|E
5000|$|ZeroTier's {{purpose is}} to simplify network {{operation}} by completely divorcing <b>logical</b> network <b>boundaries</b> from physical networks, eliminating the complexity of multiple tunnels, VPNs, etc. ZeroTier's authors term this the [...] "planetary data center," [...] and ZeroTier itself a [...] "software-based planetary smart switch." ...|$|R
40|$|A {{system and}} method are {{disclosed}} that separate control functionality from the management functionality for conducting electronic transactions. The control functions are {{performed by a}} third party resulting in a low overhead since significant overhead is incurred {{in response to an}} anomalous event, thus facilitating high throughput electronic transactions when anomalous events are infrequent. Further, the third party does not need to have access to confidential information since it only controls by observing, validating and certifying the observed communications in a specified manner to prevent confidential information from leaving the context of the transaction. Management of the transactions based on consideration of substantive information is provided by the participants. A preferred system of the invention comprises a validation authority, a <b>logical</b> <b>boundary</b> at which validation authority undertakes control of communications, and validation rules specifying parameters for observation and the nature of comparisons...|$|R
40|$|This {{project is}} a part of the 1989 Comprehensive Planning Workshop, the {{capstone}} of the Graduate Urban Planning curriculum at Portland State University. The goal was to develop a regional Housing Plan for Southeast Portland. Southeast Uplift Neighborhood Coalition, a non-profit organization offering administrative and technical support to Southeast neighborhood associations and other citizen groups agreed to cooperate in this endeavor. The goal and policies of this plan are those of the graduate students in the Master of Urban Planning program. The planning area, nearly coinciding with neighborhoods Southeast Uplift represents is bounded by the Willamette River to the west, I- 84 to the north, (including that area north of Burnside Street with Northeast addresses), I- 205 to the east and the Portland city limits to the south. The physical barrier pose by I- 205 created a <b>logical</b> <b>boundary</b> for our planning area. This differs from the Southeast Uplift boundary which encompasses neighborhoods east of the freeway...|$|R
40|$|The {{aviation}} industry {{has started to}} adopt complex distributed computing networks (known as Integrated Modular Avionics (IMA)). IMA raises {{a number of new}} issues for syste development. Firstly, IMA is designed to use logical partitioning of data, both to provide security of access to resources and also to support incremental change of one component with limited impact on other components. Secondly, an incremental approach to development is often needed, for example to purchase an operating system prior to application development. To support this {{there is a need for}} a modular approach to safety analysis which not only provides assurance of system safety, but also provides a breakdown of responsibilities across <b>logical</b> <b>boundaries.</b> However, current safety analysis techniques are not well suited to a modular approach, treating software as a monolithic entity. This paper presents a method used to perform safety analysis on the module support layer (MSL) of an IMA system as part of a modular approach. The approach takes the system context into account, using system level hazard analysis and component interaction diagrams to assess the impact and cause of failures occurring in each component. We discuss the advantages of this approach, such as potential support for incremental change. However, we also highlight some difficulties associated with the component based approach, particularly when considering the re-use of components such as the MSL...|$|E
40|$|Child {{mortality}} {{because of}} infectious preventable diseases {{is a global}} tragedy. Millions of children die in vain every year. This thesis presents a sincere effort to decrease child mortality on given reason. The servitized Business Information System (BIS) named VacSam provides unique vaccination recommendations to any child thus service orienting {{a part of the}} Swedish vaccination activity. Designing a Service Oriented Business Process (SOBP) automated through servitized BIS took an approach devising architecture, which did not yet exist. Thus, this thesis departs from viewing Business Information Systems (BIS) as services. Service Oriented Architecture (SOA) seen as realizing technology e. g., Web services, impedes viewing SOA as a conceptual architecture. The excess of different understandings of SOA obstruct the purpose of SOA: to service orient a business. This thesis deals with two problems: 1) SOA does not manage the separation of the concerns, process and decision logic, which impedes BIS flexibility for business agility and, 2) there is no approach supporting SOA for realizing business service orientation through designing for SOBP. These problems indicate a lack of design decisions in how to realize SOA. The purpose of this thesis is to provide an approach supporting design of a business rules governed SOBP, supporting SOA through combining Business Rules Approach (BRA) and Business Process Modelling (BPM). Designing the approach supporting SOA as a conceptual architecture managing Separation of Concerns (SoC) leads to question: how should a business process be designed as service-oriented through SOA interplay with BRA and BPM for business agility? The SOA manifesto and basic principles provide explanatory properties expressing “what” to do, but not “how” to carry out SOA and became the theoretical point of departure. Following this, I present the Design Proposition (DP) as the theoretical result and as the empirical departure. The design proposition was 1) used, 2) tested and 3) evaluated, in the design studio, also, named VacSam. Data were collected through 1), 2) and 3), through workshops, interviews and observations, thus revealing the artefact’s utility. The expert evaluation further developed the DP into the approach useful for designing SOBP. The intrinsically related fields, SOA, BRA and BPM, established the theoretical foundation and research topics informing the design of the approach. Undoubtedly, the research was conducted through design. This design study presents the construction of the approach from where aspects were gathered and explored to achieve the functional goal of making things better. The approach is the artefact, which is the research contribution, thereby revealing that Design Science Research (DSR) influenced, and was applied in this thesis. The empirical findings shaped the approach supporting SoC expressing the conceptual architecture i. e., the SOBP, thereby bridging the gaps between SOA, BRA and BPM. Separating business logic, into 1) process and, 2) decision logic, became a criterion for bridging the gaps but simultaneously maintaining <b>logical</b> <b>boundaries</b> between the responsibilities of SOA, BRA and BPM. The result of maintaining the <b>logical</b> <b>boundaries,</b> supporting the purity of SoC, was found as the effect of business logic separation leading to design two separate species of digital services composed to provide business service orientation. Using the approach, made confined business logic available through SOBP automated through digital services composing the servitized BIS reflecting and automating operational business. In VacSam, this research contribution was used for increasing the chances for better child prosperity and health safety...|$|E
40|$|The {{need for}} highly robust enterprise-level {{architectures}} that implement multi-domain information protection mechanisms is widespread and growing, {{especially in the}} context of cloud computing which promotes dynamicity, scalability and collaboration across domains and organizations. The Monterey Security Architecture (MYSEA) addresses this need by integrating cloud computing functionality with the strong security properties required by a highly robust multi-domain system. The MYSEA architecture combines highly trustworthy multilevel secure servers and special-purpose multi-domain authentication components to provide centralized cross-domain security policy enforcement. Users can continue to use commodity workstations and familiar web-based applications for collaboration and access to data across domains. MYSEA�s security features include strong cross-domain access controls, protection of system assets (data and services) with different security classifications, resource isolation, service replication and dynamic control of Quality of Security Service attributes. The MYSEA cloud is oriented towards the Cloud Software as a Service (SaaS) model and supports many characteristics associated with cloud computing, including broad network access, resource pooling and measured services. In terms of ownership, administrative domain, and availability to a larger community, the MYSEA cloud is deployable as a private cloud, a community cloud, or a hybrid cloud. The MYSEA design requires that both the MYSEA servers and special-purpose authentication components run on high assurance trusted foundations. The MYSEA server currently runs on an EAL 5 -augmented trusted platform (i. e., BAE XTS- 400) and the special-purpose authentication components are being designed to run on a Least Privilege Separation Kernel that is targeted for an EAL 7 evaluation. The MYSEA Target of Evaluation Security Functionality is comprised of trusted processes of both the MYSEA server and authentication components. This paper describes the MYSEA TOE architecture, including both physical and <b>logical</b> <b>boundaries,</b> the composition of TSF and non-TSF processes, and the MYSEA�s approach for building high assurance composite multilevel secure systems...|$|E
50|$|Traffic is {{exchanged}} (routed) between subnetworks {{with special}} gateways (routers) when the routing prefixes {{of the source}} address and the destination address differ. A router constitutes the <b>logical</b> or physical <b>boundary</b> between the subnets.|$|R
40|$|This paper {{presents}} work {{towards a}} new architecture for trustworthy autonomic systems (different {{from the traditional}} autonomic computing architecture) that includes mechanisms and instrumentation to explicitly support run-time self-validation and trustworthiness. The state of practice does not lend itself robustly enough to support trustworthiness and system dependability. For example, despite validating system’s decisions within a <b>logical</b> <b>boundary</b> set for the system, there’s the possibility of overall erratic behaviour or inconsistency in the system. So a more thorough and holistic approach, with {{a higher level of}} check, is required to convincingly address the dependability and trustworthy concerns. Validation alone does not always guarantee trustworthiness as each individual decision could be correct (validated) but overall system may not be consistent or dependable. A new approach is required in which, validation and trustworthiness are designed in and integral at the architectural level, and not treated as add-ons as they cannot be reliably retro-fitted to systems. In this paper we analyse current state of practice in autonomic architecture and propose a different architectural approach for trustworthy autonomic systems. To demonstrate the feasibility and practicability of our approach, a case example scenario is examined. The example is a deployment of the architecture to an envisioned Autonomic Marketing System that has many dimensions of freedom and which is sensitive to a number of contextual volatility...|$|R
40|$|This paper {{presents}} a new architecture for trustworthy autonomic systems. This trustworthy autonomic architecture {{is different from}} the traditional autonomic computing architecture and includes mechanisms and instrumentation to explicitly support run-time self-validation and trustworthiness. The state of practice does not lend itself robustly enough to support trustworthiness and system dependability. For example, despite validating system's decisions within a <b>logical</b> <b>boundary</b> set for the system, there’s the possibility of overall erratic behaviour or inconsistency in the system emerging for example, at a different logical level or on a different time scale. So a more thorough and holistic approach, with a higher level of check, is required to convincingly address the dependability and trustworthy concerns. Validation alone does not always guarantee trustworthiness as each individual decision could be correct (validated) but overall system may not be consistent and thus not dependable. A robust approach requires that validation and trustworthiness are designed in and integral at the architectural level, and not treated as add-ons as they cannot be reliably retro-fitted to systems. This paper analyses the current state of practice in autonomic architecture, {{presents a}} different architectural approach for trustworthy autonomic systems, and uses a datacentre scenario as the basis for empirical analysis of behaviour and performance. Results show that the proposed trustworthy autonomic architecture has significant performance improvement over existing architectures and can be relied upon to operate (or manage) almost all level of datacentre scale and complexity...|$|R
40|$|The goal of {{this study}} has been to {{formulate}} a program {{which can be used}} as a guide to control or eliminate pollution in the Connecticut River Valley so that the river could be utilized to its fullest potential. However, after making a cursory examination, it has been discovered that pollution, though the most important, was not the only problem which prevented full utilization of the river. Pollution through- bacteria, synthetic organic and inorganic matter, sediment, and heat was found to do most in curtailing full utilization of the river. But a second problem caused by man's building dams across the river has also contributed to lessening the river's usefulness. These dams have prevented anadramous fish, such as Atlantic salmon and shad, from returning to their spawning grounds in the upper reaches of the Connecticut River. The valley has been deprived of much recreational fishing because of these dams. While pursuing this investigation, it was recognized that two cogent facts aggravated the problems of pollution and inadequate recreation: (1) a high rate of population increase in the river valley (a higher rate than that of Massachusetts) and more particularly the concentration ofthis population in the southern or Hampden County section of the valley and (2) inadequate amounts of recreational facilities which were needed by this increased population. Therefore, the study of these problems from which suggestions for their elimination could be derived seemed particularly worthwhile, especially, to the people of the valley and to a certain extent the nation as a whole. Finding it necessary to define the limits of this study while keeping in mind that the watershed of the valley provided the most <b>logical</b> <b>boundaries</b> for the study, the writer chose to recognize the natural boundaries of the area. The Berkshire Hills, an extension of Vermont's Green Mountains, established the western boundary of the valley. A southern segment of dew Hampshire's White Mountains, the Worcester County Plateau or Central New England Upland, defined the eastern limit of the study area. Thesis (M. A. ...|$|E
40|$|The term 'sonata' {{arose in}} the early seventeenth-century Baroque period and was {{originally}} used to distinguish instrumental (sonata) music from vocal music. Later. the sonata style, as a reliable yet flexible compositional framework, was extensively shaped and utilized throughout the Classical period. Subsequently, in the Romantic period, freer creative, individualistic. and expressive musical elements began to be preferred by composers {{in their use of}} harmonj., tone color, form, and rhythm. However, even during the revolutionary Romantic period in music. the compositions which did not have a pre-defined format (character pieces, etc) were often comfortably framed and limited within the recognizable boundaries provided by the Classical sonata style. The sonata format, when used as a tool in musical composition, provides <b>logical</b> <b>boundaries</b> that may serve to organize any unexpected emotional expressions {{on the part of the}} composer. Yet the sonata framework is also flexible enough to allow freedom of expression. In the Romantic period and beyond, composers had relied, some more than others, upon the sonata's adaptable blend of stability and flexibility. In my opinion, it is more persuasive to express oneself musically within the framework of an established musical style. Thus, I have chosen my dissertation topic as the performance of six pieces incorporating elements of the reliable and flexible sonata style. The sonata of each composer that I have selected clearly demonstrates a tension between logic and emotion expressed within the sonata framework. However, the compositions can be divided interestingly into two groups, such as 'conservative' and 'progressive' group. The 'conservative' group consists of composers who seemed to strive for greater freedom of self-expression within the constraints of the traditional sonata form. On the other hand, the 'progressive' group consists of composers who seemed more to rely upon the sonata form to rein in and add stability to their highly individual and emotional musical ideas. It is my hope that this project will provide a stimulating viewpoint from which to consider the evolution and utilization of the sonata style especially as it is applied to the composition and performance of these six diverse and interesting pieces...|$|E
40|$|Governments, military, corporations, {{financial}} institutions and others exchange {{a great deal of}} confidential information using Internet these days. Protecting such confidential information and ensuring their integrity and origin authenticity are of paramount importance. There exist protocols and solutions at different layers of the TCP/IP protocol stack to address these security requirements. Application level encryption viz. PGP for secure mail transfer, TLS based secure TCP communication, IPSec for providing IP layer security are among these security solutions. Due to scalability, wide acceptance of the IP protocol, and its application independent character, the IPSec protocol has become a standard for providing Internet security. The IPSec provides two protocols namely the Authentication header (AH) and the Encapsulating Security Payload (ESP). Each protocol can operate in two modes, viz. transport and tunnel mode. The AH provides data origin authentication, connectionless integrity and anti replay protection. The ESP provides all the security functionalities of AH along with confidentiality. The IPSec protocols provide end-to-end security for an entire IP datagram or the upper layer protocols of IP payload depending on the mode of operation. However, this end-to-end model of security restricts performance enhancement and security related operations of intermediate networking and security devices, as they can not access or modify transport and upper layer headers and original IP headers in case of tunnel mode. These intermediate devices include routers providing Quality of Service (QoS), TCP Performance Enhancement Proxies (PEP), Application level Proxy devices and packet filtering firewalls. The interoperability problem between IPSec and intermediate devices has been addressed in literature. Transport friendly ESP (TF-ESP), Transport Layer Security (TLS), splitting of single IPSec tunnel into multiple tunnels, Multi Layer IPSec (ML-IPSec) {{are a few of the}} proposed solutions. The ML-IPSec protocol solves this interoperability problem without violating the end-to-end security for the data or exposing some important header fields unlike the other solutions. The ML-IPSec uses a multilayer protection model in place of the single end-to-end model. Unlike IPSec where the scope of encryption and authentication applies to the entire IP datagram, this scheme divides the IP datagram into zones. It applies different protection schemes to different zones. When ML-IPSec protects a traffic stream from its source to its destination, it first partitions the IP datagram into zones and applies zone-specific cryptographic protections. During the flow of the ML-IPSec protected datagram through an authorized intermediate gateway, certain type I zones of the datagram may be decrypted and re-encrypted, but the other zones will remain untouched. When the datagram reaches its destination, the ML-IPSec will reconstruct the entire datagram. The ML-IPSec protocol, however suffers from the problem of static configuration of zones and zone specific cryptographic parameters before the commencement of the communication. Static configuration requires a priori knowledge of routing infrastructure and manual configuration of all intermediate nodes. While this may not be an issue in a geo-stationary satellite environment using TCP-PEP, it could pose problems in a mobile or distributed environment, where many stations may be in concurrent use. The ML-IPSec endpoints may not be trusted by all intermediate nodes in a mobile environment for manual configuration without any prior arrangement providing the mutual trust. The static zone boundary of the protocol forces one to ignore the presence of TCP/IP datagrams with variable header lengths (in case of TCP or IP headers with OPTION fields). Thus ML-IPSec will not function correctly if the endpoints change the use of IP or TCP options, especially in case of tunnel mode. The zone mapping proposed in ML-IPSec is static in nature. This forces one to configure the zone mapping before the commencement of the communication. It restricts the protocol from dynamically changing the zone mapping for providing access to intermediate nodes without terminating the existing ML-IPSec communication. The ML-IPSec endpoints can off course, configure the zone mapping with maximum number of zones. This will lead to unnecessary overheads that increase with the number of zones. Again, static zone mapping could pose problems in a mobile or distributed environment, where communication paths may change. Our extension to the ML-IPSec protocol, called Dynamic Multi Layer IPSec (DML-IPSec) proposes a multi layer variant with the capabilities of dynamic zone configuration and sharing of cryptographic parameters between IPSec endpoints and intermediate nodes. It also accommodates IP datagrams with variable length headers. The DML-IPSec protocol redefines some of the IPSec and ML-IPSec fundamentals. It proposes significant modifications to the datagram processing stage of ML-IPSec and proposes a new key sharing protocol to provide the above-mentioned capabilities. The DML-IPSec supports the AH and ESP protocols of the conventional IPSec with some modifications required for providing separate cryptographic protection to different zones of an IP datagram. This extended protocol defines zone as a set of non-overlapping and contiguous partitions of an IP datagram, unlike the case of ML-IPSec where a zone may consist of non-contiguous portions. Every zone is provided with cryptographic protection independent of other zones. The DML-IPSec categorizes zones into two separate types depending on the accessibility requirements at the intermediate nodes. The first type of zone, called type I zone, is defined on headers of IP datagram and is required for examination and modification by intermediate nodes. One type I zone may span over a single header or over a series of contiguous headers of an IP datagram. The second type of zone, called type II zone, is meant for the payload portion and is kept secure between endpoints of IPSec communications. The single type II zone starts immediately after the last type I zone and spans till the end of the IP datagram. If no intermediate processing is required during the entire IPSec session, the single type II zone may cover the whole IP datagram; otherwise the single type II zone follows one or more type I zones of the IP datagram. The DML-IPSec protocol uses a mapping from the octets of the IP datagram to different zones, called zone map for partitioning an IP datagram into zones. The zone map contains <b>logical</b> <b>boundaries</b> for the zones, unlike physical byte specific boundaries of ML-IPSec. The physical boundaries are derived on-the-fly, using either the implicit header lengths or explicit header length fields of the protocol headers. This property of the DML-IPSec zones, enables it to accommodate datagrams with variable header lengths. Another important feature of DML-IPSec zone is that the zone maps need not remain constant through out the entire lifespan of IPSec communication. The key sharing protocol may modify any existing zone map for providing service to some intermediate node. The DML-IPSec also redefines Security Association (SA), a relationship between two endpoints of IPSec communication that describes how the entities will use security services to communicate securely. In the case of DML-IPSec, several intermediate nodes may participate in defining these security protections to the IP datagrams. Moreover, the scope of one particular set of security protection is valid on a single zone only. So a single SA is defined for each zone of an IP datagram. Finally all these individual zonal SA’s are combined to represent the security relationship of the entire IP datagram. The intermediate nodes can have the cryptographic information of the relevant type I zones. The cryptographic information related to the type II zone is, however, hidden from any intermediate node. The key sharing protocol is responsible for selectively sharing this zone information with the intermediate nodes. The DML-IPSec protocol has two basic components. The first one is for processing of datagrams at the endpoints as well as intermediate nodes. The second component is the key sharing protocol. The endpoints of a DML-IPSec communication involves two types of processing. The first one, called Outbound processing, is responsible for generating a DML-IPSec datagram from an IP datagram. It first derives the zone boundaries using the zone map and individual header field lengths. After this partitioning of IP datagram, zone wise encryption is applied (in case of ESP). Finally zone specific authentication trailers are calculated and appended after each zone. The other one, Inbound processing, is responsible for generating the original IP datagram from a DML-IPSec datagram. The first step in the inbound processing, the derivation of zone boundary, is significantly different from that of outbound processing as the length fields of zones remain encrypted. After receiving a DML-IPSec datagram, the receiver starts decrypting type I zones till it decrypts the header length field of the header/s. This is followed by zone-wise authentication verification and zone-wise decryption. The intermediate nodes processes an incoming DML-IPSec datagram depending on the presence of the security parameters for that particular DML-IPSec communication. In the absence of the security parameters, the key sharing protocol gets executed; otherwise, all the incoming DML-IPSec datagrams get partially decrypted according to the security association and zone mapping at the inbound processing module. After the inbound processing, the partially decrypted IP datagram traverses through the networking stack of the intermediate node. Before the IP datagram leaves the intermediate node, it is processed by the outbound module to reconstruct the DML-IPSec datagram. The key sharing protocol for sharing zone related cryptographic information among the intermediate nodes is the other important component of the DML-IPSec protocol. This component is responsible for dynamically enabling intermediate nodes to access zonal information as required for performing specific services relating to quality or security. Whenever a DML-IPSec datagram traverses through an intermediate node, that requires access to some of the type I zones, the inbound security database is searched for cryptographic parameters. If no entry is present in the database, the key sharing protocol is invoked. The very first step in this protocol is a header inaccessible message from the intermediate node to the source of the DML-IPSec datagram. The intermediate node also mentions the protocol headers that it requires to access in the body portion of this message. This first phase of the protocol, called the Zone reorganization phase, is responsible for deciding the zone mapping to provide access to intermediate nodes. If the current zone map can not serve the header request, the DML-IPSec endpoint reorganizes the existing zone map in this phase. The next phase of the protocol, called the Authentication Phase is responsible for verifying the identity of the intermediate node to the source of DML-IPSec session. Upon successful authentication, the third phase, called the Shared secret establishment phase commences. This phase is responsible for the establishment of a temporary shared secret between the source and intermediate nodes. This shared secret is to be used as key for encrypting the actual message transfer of the DML-IPSec security parameters at the next phase of the protocol. The final phase of the protocol, called the Security parameter sharing phase, is solely responsible for actual transfer of the security parameters from the source to the intermediate nodes. This phase is also responsible for updation of security and policy databases of the intermediate nodes. The successful execution of the four phases of the key sharing protocol enables the DML-IPSec protocol to dynamically modify the zone map for providing access to some header portions for intermediate nodes and also to share the necessary cryptographic parameters required for accessing relevant type I zones without disturbing an existing DML-IPSec communication. We have implemented the DML-IPSec for ESP protocol according to the definition of zones along with the key sharing algorithm. RHEL version 4 and Linux kernel version 2. 6. 23. 14 was used for the implementation. We implemented the multi-layer IPSec functionalities inside the native Linux implementation of IPSec protocol. The SA structure was updated to hold necessary SA information for multiple zones instead of single SA of the normal IPSec. The zone mapping for different zones was implemented along with the kernel implementation of SA. The inbound and outbound processing modules of the IPSec endpoints were re-implemented to incorporate multi-layer IPSec capability. We also implemented necessary modules for providing partial IPSec processing capabilities at the intermediate nodes. The key sharing protocol consists of some user space utilities and corresponding kernel space components. We use ICMP protocol for the communications required for the execution of the protocol. At the kernel level, pseudo character device driver was implemented to update the kernel space data structures and necessary modifications were made to relevant kernel space functions. User space utilities and corresponding kernel space interface were provided for updating the security databases. As DML-IPSec ESP uses same Security Policy mechanism as IPSec ESP, existing utilities (viz. setkey) are used for the updation of security policy. However, the configuration of the SA is significantly different as it depends on the DML-IPSec zones. The DML-IPSec ESP implementation uses the existing utilities (setkey and racoon) for configuration of the sole type II zone. The type I zones are configured using the DML-IPSec application. The key sharing protocol also uses this application to reorganize the zone mapping and zone-wise cryptographic parameters. The above feature enables one to use default IPSec mechanism for the configuration of the sole type II zone. For experimental validation of DML-IPSec, we used the testbed as shown in the above figure. An ESP tunnel is configured between the two gateways GW 1 and GW 2. IN acts as an intermediate node and is installed with several intermediate applications. Clients C 11 and C 21 are connected to GW 1 and GW 2 respectively. We carried out detailed experiments for validating our solution w. r. t firewalling service. We used stateful packet filtering using iptables along with string match extension at IN. First, we configured the firewall to allow only FTP communication (using port information of TCP header and IP addresses of Inner IP header) between C 11 and C 21. In the second experiment, we configured the firewall to allow only Web connection between C 11 and C 21 using the Web address of C 11 (using HTTP header, port information of TCP header and IP addresses of Inner IP header). In both experiments, we initiated the FTP and WEB sessions before the execution of the key sharing protocol. The session could not be established as the access to upper layer headers was denied. After the execution of the key sharing protocol, the sessions could be established, showing the availability of protocol headers to the iptables firewall at IN following the successful key sharing. We use record route option of ping program to validate the claim of handling datagrams with variable header lengths. This option of ping program records the IP addresses of all the nodes traversed during a round trip path in the IP OPTION field. As we used ESP in tunnel mode between GW 1 and GW 2, the IP addresses would be recorded inside the encrypted Inner IP header. We executed ping between C 11 and C 21 and observed the record route output. Before the execution of the key sharing protocol, the IP addresses of IN were absent in the record route output. After the successful execution of key sharing protocol, the IP addresses for IN were present at the record route output. The DML-IPSec protocol introduces some processing overhead and also increases the datagram size as compared to IPSec and ML-IPSec. It increases the datagram size compared to the standard IPSec. However, this increase in IP datagram size is present in the case of ML-IPSec as well. The increase in IP datagram length depends on the number of zones. As the number of zone increases this overhead also increases. We obtain experimental results about the processing delay introduced by DML-IPSec processing. For this purpose, we executed ping program from C 11 to C 21 in the test bed setup for the following cases: 1. ML-IPSec with one type I and one type II zone and 2. DML-IPSec with one type I and one type II zone. We observe around 10 % increase in RTT in DML-IPSec with two dynamic zones over that of ML-IPSec with two static zones. This overhead is due to on-the-fly derivation of the zone length and related processing. The above experiment analyzes the processing delay at the endpoints without intermediate processing. We also analyzed the effect of intermediate processing due to dynamic zones of DML-IPSec. We used iptables firewall in the above mentioned experiment. The RTT value for DML-IPSec with dynamic zones increases by less than 10 % over that of ML-IPSec with static zones. To summarize our work, we have proposed an extension to the multilayer IPSec protocol, called Dynamic Multilayer IPSec (DML-IPSec). It is capable of dynamic modification of zones and sharing of cryptographic parameters between endpoints and intermediate nodes using a key sharing protocol. The DML-IPSec also accommodates datagrams with variable header lengths. The above mentioned features enable any intermediate node to dynamically access required header portions of any DML-IPSec protected datagrams. Consequently they make the DML-IPSec suited for providing IPSec over mobile and distributed networks. We also provide complete implementation of ESP protocol and provide experimental validation of our work. We find that our work provides the dynamic support for QoS and security services without any significant extra overhead compared to that of ML-IPSec. The thesis begins with an introduction to communication security requirements in TCP/IP networks. Chapter 2 provides an overview of communication security protocols at different layers. It also describes the details of IPSec protocol suite. Chapter 3 provides a study on the interoperability issues between IPSec and intermediate devices and discusses about different solutions. Our proposed extension to the ML-IPSec protocol, called Dynamic ML-IPSec(DML-IPSec) is presented in Chapter 4. The design and implementation details of DML-IPSec in Linux environment is presented in Chapter 5. It also provides experimental validation of the protocol. In Chapter 6, we summarize the research work, highlight the contributions of the work and discuss the directions for further research...|$|E
40|$|Abstract — This paper {{presents}} a new architecture for trustworthy autonomic systems. This trustworthy autonomic architecture {{is different from}} the traditional autonomic computing architecture and includes mechanisms and instrumentation to explicitly support run-time self-validation and trustworthiness. The state of practice does not lend itself robustly enough to support trustworthiness and system dependability. For example, despite validating system’s decisions within a <b>logical</b> <b>boundary</b> set for the system, there’s the possibility of overall erratic behaviour or inconsistency in the system emerging for example, at a different logical level or on a different time scale. So a more thorough and holistic approach, with a higher level of check, is required to convincingly address the dependability and trustworthy concerns. Validation alone does not always guarantee trustworthiness as each individual decision could be correct (validated) but overall system may not be consistent and thus not dependable. A robust approach requires that validation and trustworthiness are designed in and integral at the architectural level, and not treated as add-ons as they cannot be reliably retro-fitted to systems. This paper analyses the current state of practice in autonomic architecture, {{presents a}} different architectural approach for trustworthy autonomic systems, and uses a datacentre scenario as the basis for empirical analysis of behaviour and performance. Results show that the proposed trustworthy autonomic architecture has significant performance improvement over existing architectures and can be relied upon to operate (or manage) almost all level of datacentre scale and complexity. Keywords- trustworthy architecture; trustability; validation; datacentre; autonomic system; dependability; stability; autonomic architecture I...|$|R
40|$|An {{electrostatic}} gyrokinetic-based {{model is}} applied to simulate parallel plasma transport in the scrape-off layer to a divertor plate. The authors focus on a test problem that has been studied previously, using parameters chosen to model a heat pulse driven by an edge-localized mode (ELM) in JET. Previous work has used direct particle-in-cell equations with full dynamics, or Vlasov or fluid equations with only parallel dynamics. With {{the use of the}} gyrokinetic quasineutrality equation and <b>logical</b> sheath <b>boundary</b> conditions, spatial and temporal resolution requirements are no longer set by the electron Debye length and plasma frequency, respectively. This test problem also helps illustrate some of the physics contained in the Hamiltonian form of the gyrokinetic equations and some of the numerical challenges in developing an edge gyrokinetic code. Comment: 10 pages, 4 figure...|$|R
5000|$|By placing {{limits on}} the number of packets that can be {{transmitted}} or received at any given time, a sliding window protocol allows an unlimited number of packets to be communicated using fixed-size sequence numbers.The term [...] "window" [...] on the transmitter side represents the <b>logical</b> <b>boundary</b> {{of the total number of}} packets yet to be acknowledged by the receiver. The receiver informs the transmitter in each acknowledgment packet the current maximum receiver buffer size (window boundary). The TCP header uses a 16 bit field to report the receive window size to the sender. Therefore, the largest window that can be used is 216 = 64 kilobytes. In slow-start mode, the transmitter starts with low packet count and increases the number of packets in each transmission after receiving acknowledgment packets from receiver. For every ack packet received, the window slides by one packet (logically) to transmit one new packet. When the window threshold is reached, the transmitter sends one packet for one ack packet received. If the window limit is 10 packets then in slow start mode the transmitter may start transmitting one packet followed by two packets (before transmitting two packets, one packet ack has to be received), followed by three packets and so on until 10 packets. But after reaching 10 packets, further transmissions are restricted to one packet transmitted for one ack packet received. In a simulation this appears as if the window is moving by one packet distance for every ack packet received. On the receiver side also the window moves one packet for every packet received. The sliding window method ensures that traffic congestion on the network is avoided. The application layer will still be offering data for transmission to TCP without worrying about the network traffic congestion issues as the TCP on sender and receiver side implement sliding windows of packet buffer. The window size may vary dynamically depending on network traffic.|$|R
40|$|Network Security {{design and}} {{operations}} for security mechanisms for the network. Please note this differs from assuming that “the network is secure” {{which is the}} fourth fallacy of distributed computing. Network security mechanisms, such as network firewalls and network intrusion detection devices, are generally a convenient and scalable point to apply security controls and are an important locale for defining chokepoints and zones. Zones define <b>logical</b> and/or physical <b>boundaries</b> around a group of systems, for example the DMZ pattern in web applications. Chokepoints define places to cross boundaries {{into and out of}} zones, where special security considerations apply...|$|R
30|$|Pixel-based SIS {{algorithm}} is used with facies coding (Kupfersberger and Deutsch 1999) {{for the model}} that {{can be traced back}} to Geostatistical Software Library (GSLIB) (Xu and Journel 1993). However, common challenges in geomodeling, such as producing realistic modeled scenarios, <b>logical</b> facies <b>boundary</b> conditions as well as their proportions, were considered while modeling for this study. We applied collocated co-kriging in SIS to obtain spatial distribution of depositional facies, i.e., paleodepositional trend of all the layers within Mangahewa Formation. Moreover, the reason for choosing SIS over TGS is the simplicity and usability of the algorithm. For this study we developed somewhat unique approach in which we used SIS along with object-based facies coding. So this entire method follows a relatively new technique for geobody detection and quantification. During the first stage of this study, we developed a part of the 3 D model using TGS algorithm, but numerous modeling iterations were taken without considerable success on facies probability and trending spatially. That is why, SIS with facies coding approach was considered for this study. The model considers nearest-simulated node values and variogram data for effective spatial and vertical facies distribution within the Mangahewa reservoir.|$|R
40|$|Nowadays, {{public as}} well as private {{communication}} infrastructures are all contending for the same limited amount of bandwidth. To optimally share network resources, symbiotic networks have been proposed, which cross <b>logical</b> and physical <b>boundaries</b> to improve the reliability, scalability, and energy efficiency of the network as a whole {{as well as}} its constituents. This paper focuses on software services in such symbiotic networks. We propose a platform for the intelligent composition of services provided by symbiotically connected parties, resulting in novel cooperation opportunities. The platform harvests Semantic Web technology to describe services in a highly expressive manner, and constructs service compositions using SeCoA, our tunable best-first search algorithm. The resulting compositions are then enacted via CaPI, a reconfigurable middleware infrastructure. By means of an illustrative scenario, we provide further insight into the platform's functioning...|$|R
40|$|Background. In {{line with}} the {{approach}} of Larkin et al. (2006), we consider technological dependence {{in the context of}} the interaction between personality, environment, and culture. Objective. The aim of this study is to discover technology-related changes in psycho- <b>logical</b> needs and <b>boundaries</b> that could mediate the relationship between psychopathological symptoms and indicators of excessive use of info-communication technologies (ICT). The application of the Body Function Regulation Model to the use of ICT suggests that technology-related changes in the system of an individual’s needs and psychological boundaries mediate the relationship between a sense of poor psychological well-being and the risk of technology dependence. Design. The study of a normative sample (N = 275) using two technologies–mobile phones and the Internet–was performed. Results and Discussion. We demonstrated that the relationship between the general level of psychopathological symptoms and excessive use of technology (subjective dependence and inability to refrain from use of mobile phones and the Internet) is indeed mediated by the perception of their indispensability for extension of psychological boundaries, and (for the Internet) its use in image-making...|$|R
40|$|Each URL {{identifies}} {{a unique}} Web page; thus, it {{is viewed as}} a natural choice to use for organizing Web query results. Web search results may be grouped by domain and presented to users as clusters for ease of visualization. However, it has a drawback: dealing with large Web sites, such asGeocities,W 3 C,andwww. cs. umd. edu. Large Web sites tend to yield many matches that leads to a few large, flat structured, and unorganized clusters. As a matter of fact, these sites contain Web sites of other entities, such as projects and people. Many pages in these sites are actually “logical domains” by themselves. For example, Web sites for projects at a university or the XML section at W 3 C could be viewed as “logical domains”. In this paper, we propose the concept of logical domain with respect to physical domain which is identified simply by domain name. We have developed and implemented a set of rules based on link structure, path information, document metadata, and citation to identify logical domain entry pages and their corresponding boundaries. Experiments on real Web data have been conducted to validate the usefulness of this technique. Keywords: <b>Logical</b> domain, domain <b>boundary,</b> WWW, link structures, site ma...|$|R
40|$|The article {{deals with}} the {{description}} of the main points of the structural and logical coding and the features of SLC codes. There are shown the basic points of the generalized algorithm of decoding SLC, which is based on the method of perfect matrix arrangement (PMA) of the n-dimensional cube vertices for adequate representation and transformation of boolean functions, which is based on the method of generating sequences of variables for building the maximum coverage of the cube vertices. The structural and logical codes (SLC) use natural logic redundancy of the infimum disjunctive normal forms (IDNF) of boolean functions, which make the basis for building the SLC codes and correcting the errors, that occur during data transfer in real discrete channels, on the channels with independent errors. The main task is to define the basic relations between the implemented SLC codes of the <b>logical</b> redundancy and <b>boundary</b> values of multiplicity of independent errors which are corrected. The principal difference between the SLC codes and the well-known correcting codes is that the redundancy, that is needed to correct the errors in converting the discrete information, is not introduced into an additional code sequence but is defined in a natural way, during the construction of codewords of SLC. </p...|$|R
