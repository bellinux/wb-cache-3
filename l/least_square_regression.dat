941|10000|Public
5000|$|First we run an {{ordinary}} <b>least</b> <b>square</b> <b>regression</b> that has Xi {{as a function}} of all the other explanatory variables in the first equation. If i = 1, for example, the equation would be ...|$|E
5000|$|Next, {{we apply}} a {{property}} of <b>least</b> <b>square</b> <b>regression</b> models, {{that the sample}} covariance between [...] and [...] is zero. Thus, the sample correlation coefficient between the observed and fitted response values in the regression can be written (calculation is under expectation, assumes Gaussian statistics) ...|$|E
5000|$|With the median-effect {{equation}} as {{the unified}} {{theory of the}} mass-action law and its combination index theorem for multiple effector interactions, these algorithms {{can be applied to}} virtually all aspects of quantitative biology and medical sciences as shown in over 754 different bio-medical journals.1 The conventional approach of dose-effect analysis in the past centuries has used numerous numbers of data points to draw the empirical dose-effect curve to best fit the available data by <b>least</b> <b>square</b> <b>regression</b> or other statistical means. This old approach is under the premises of assuming dose and effect relationships are random events. By contrast, the MEE theory is to use small number (usually 3-7) of data points to fit the mass-action law to yield its parameters (i.e. m, Dm and r), and to lineage all dose-effect curves with the automated median-effect plot. This led to [...] "the theory of minimum of two data points" [...] for easily defining the dose-effect curve with the m and Dm parameters.|$|E
50|$|Regularized <b>Least</b> <b>Squares</b> <b>regression.</b>|$|R
40|$|The main {{result of}} this paper is filling an {{existing}} gap between the theory of <b>least</b> <b>squares</b> <b>regression</b> and the solution of linear systems of equations. A linear <b>least</b> <b>squares</b> <b>regression</b> problem with p-parameters over n cases is converted, via non-orthogonal transformations, into a k-parameter regression problem through the origin on n - p + k cases, and p - k equations in diagonal form with p - k unknowns, 0 Dimensionality reduction <b>Least</b> <b>squares</b> <b>regression</b> Linear systems of equations Plots for regression...|$|R
5000|$|... #Subtitle level 2: <b>Least</b> <b>squares,</b> <b>regression</b> {{analysis}} and statistics ...|$|R
3000|$|... ℓ 2, 1 -norm <b>least</b> <b>square</b> <b>regression</b> (LSR 21) [22]: a {{supervised}} {{feature selection}} method built upon <b>least</b> <b>square</b> <b>regression</b> {{by using the}} ℓ 2, 1 -norm as the regularization term.|$|E
30|$|Since T and t {{values are}} known, <b>least</b> <b>square</b> <b>regression</b> {{analysis}} was performed and a straight plot of ln(Y) against time is shown in Fig.  6.|$|E
3000|$|... 1 norm {{minimization}} {{which then}} {{leads to a}} poorer FVC performance; and if parameter τ was decreased, we put more emphasis on the <b>least</b> <b>square</b> <b>regression</b> technique which then leads to a better FVC performance.|$|E
5000|$|... is the error, to be {{minimized}} using ordinary <b>least</b> <b>squares</b> <b>regression.</b>|$|R
40|$|Linear <b>least</b> <b>squares</b> <b>regression</b> {{is among}} the most well known {{classical}} methods. This and other parametric <b>least</b> <b>squares</b> <b>regression</b> models do not perform well when the modeling is too restrictive to capture the nonlinear effect the covariates have on the response. Locally weighted <b>least</b> <b>squares</b> <b>regression</b> (loess) is a modern technique that combines much of the simplicity of the classical <b>least</b> <b>squares</b> method with the flexibility of nonlinear regression. The basic idea behind the method is to model a regression function only locally as having a specific form. This paper discusses the method in the univariate and multivariate case and robustifications of the technique, and provides illustrative examples...|$|R
5000|$|Helland IS, On the {{structure}} of partial <b>least</b> <b>squares</b> <b>regression,</b> 1988, 246 cites.|$|R
30|$|The {{calculation}} of the mean velocities followed by {{the implementation of the}} ordinary <b>least</b> <b>square</b> <b>regression</b> method {{in order to determine the}} optimum regression curve between radius and velocity that fitted better the distribution of the collected data.|$|E
30|$|The {{test was}} {{performed}} on a double-notched sample by applying a proportional tension-shear displacement. Numerical simulations were performed using the measured BC, but also different approximations of the measured BC: constant, linear and parabolic ones obtained from <b>least</b> <b>square</b> <b>regression</b> (Fig.  10).|$|E
30|$|Harvested latex was {{measured}} using UV–NIR (370 – 1085  nm) spectrophotometer in transmittance mode (Type A: AvaSpec- 2048 -USB 2 -VA- 50, Avantes, the Netherlands). DRC and TSC were predicted using each spectrum with aid of partial <b>least</b> <b>square</b> <b>regression</b> (PLS-R) analysis.|$|E
5000|$|Ridge {{regression}} or {{principal component}} <b>regression</b> or partial <b>least</b> <b>squares</b> <b>regression</b> can be used.|$|R
40|$|This paper {{suggests}} {{a new approach}} for analyzing operating costs. After using a principal components regression method, the criteria expressing the costs structuring have been viewed through a partial <b>least</b> <b>squares</b> <b>regression</b> model. We still confirm the impact of performance functions as the existence of unreducible overheads. bank;banking operating costs;input;output; productive performance;multiple linear regression; regression in principal components;partial <b>least</b> <b>squares</b> <b>regression...</b>|$|R
50|$|Though {{the idea}} of least {{absolute}} deviations regression is just as straightforward as that of <b>least</b> <b>squares</b> <b>regression,</b> the <b>least</b> absolute deviations line is not as simple to compute efficiently. Unlike <b>least</b> <b>squares</b> <b>regression,</b> <b>least</b> absolute deviations regression {{does not have an}} analytical solving method. Therefore, an iterative approach is required. The following is an enumeration of some least absolute deviations solving methods.|$|R
30|$|<b>Least</b> <b>square</b> <b>regression</b> {{technique}} [23] {{is used to}} fit a {{trend line}} on the RTS for each cluster. Least square simply states that it looks for an optimal solution for the overall fit of data such that sum of the squares error (SSE) is least.|$|E
40|$|The {{segmented}} taper equation {{has great}} flexibility and is widely applied in exiting taper systems. The unconstrained <b>least</b> <b>square</b> <b>regression</b> (ULSR) was generally {{used to estimate}} parameters in previous applications of the segmented taper equations. The joint point parameters estimated with ULSR may fall outside the feasible region, {{which leads to the}} results of the segmented taper equation being uncertain and meaningless. In this study, a combined method of constrained two-dimensional optimum seeking and <b>least</b> <b>square</b> <b>regression</b> (CTOS & LSR) was proposed as an improved method to estimate the parameters in the segmented taper equation. The CTOS & LSR was compared with ULSR for both individual tree-level equation and the population average-level equation using data from three tropical precious tree species (Castanopsis hystrix, Erythrophleum fordii, and Tectona grandis) in the southwest of China. The differences between CTOS & LSR and ULSR were found to be significant. The segmented taper equation estimated using CTOS & LSR resulted in not only increased prediction accuracy, but also guaranteed the parameter estimates in a more meaningful way. It is thus recommended that the combined method of constrained two-dimensional optimum seeking and <b>least</b> <b>square</b> <b>regression</b> should be a preferred choice for this application. The computation procedures required for this method is presented in the article...|$|E
40|$|AbstractBy {{the aid of}} the {{properties}} of the square root of positive operators we refine the consistency analysis of regularized <b>least</b> <b>square</b> <b>regression</b> in a reproducing kernel Hilbert space. Sharper error bounds and faster learning rates are obtained when the sampling sequence satisfies a strongly mixing condition...|$|E
5000|$|In {{the second}} step we run {{generalized}} <b>least</b> <b>squares</b> <b>regression</b> for (...) using the variance matrix : ...|$|R
40|$|Recently, various authors {{proposed}} Monte-Carlo {{methods for}} the computation of American option prices, based on <b>least</b> <b>squares</b> <b>regression.</b> The {{purpose of this}} paper is to analyze an algorithm due to Longstaff and Schwartz. This algorithm involves two types of approximation. Approximation one: replace the conditional expectations in the dynamic programming principle by projections on a finite set of functions. Approximation two: use Monte-Carlo simulations and <b>least</b> <b>squares</b> <b>regression</b> to compute the value function of approximation one. Under fairly general conditions, we prove the almost sure convergence of the complete algorithm. We also determine the rate of convergence of approximation two and prove that its normalized error is asymptotically Gaussian. American options, optimal stopping, Monte-Carlo methods, <b>least</b> <b>squares</b> <b>regression...</b>|$|R
30|$|The {{coefficients}} αi, βj and γk of relation (3) {{are obtained}} {{by means of}} <b>least</b> <b>squares</b> <b>regression.</b>|$|R
3000|$|H is tree volume, and ε is the error. These {{models were}} fitted using {{ordinary}} <b>least</b> <b>square</b> <b>regression</b> assuming a power function with multiplicative error structure (Lai et al. 2013; Dong et al. 2016). For estimating biomass {{components in the}} arithmetic domain, the equations were back-transformed to give Y = αX [...]...|$|E
40|$|National audienceThe {{problem of}} {{nonlinear}} <b>least</b> <b>square</b> <b>regression</b> using multi-layeredperceptrons is addressed inthispaper. A general formulation usingmaximum likelihood is given and a particular {{attention is paid}} toalgorithm accounting for uncertainties. The generalization to densityestimation is given. In {{the second part of}} the paper, a generalframework allows to modelize general inverse problems...|$|E
40|$|In the {{presented}} {{paper the}} new software effort estimation method is proposed. The <b>Least</b> <b>Square</b> <b>Regression</b> {{is used to}} predict a value of correction parameters, which have a significant impact. The accuracy estimationis of 85 % better than the convectional use case points methods in tested dataset. © Springer International Publishing Switzerland 2015...|$|E
5000|$|Step 2: Estimate γi, βi by the {{ordinary}} <b>least</b> <b>squares</b> <b>regression</b> of yi on [...] and Xi.|$|R
40|$|Background: The {{objective}} {{of the present study}} was to test the ability of the partial <b>least</b> <b>squares</b> <b>regression</b> technique to impute genotypes from low density single nucleotide polymorphisms (SNP) panels i. e. 3 K or 7 K to a high density panel with 50 K SNP. No pedigree information was used. Methods: Data consisted of 2093 Holstein, 749 Brown Swiss and 479 Simmental bulls genotyped with the Illumina 50 K Beadchip. First, a single-breed approach was applied by using only data from Holstein animals. Then, to enlarge the training population, data from the three breeds were combined and a multi-breed analysis was performed. Accuracies of genotypes imputed using the partial <b>least</b> <b>squares</b> <b>regression</b> method were compared with those obtained by using the Beagle software. The impact of genotype imputation on breeding value prediction was evaluated for milk yield, fat content and protein content. Results: In the single-breed approach, the accuracy of imputation using partial <b>least</b> <b>squares</b> <b>regression</b> was around 90 and 94 % for the 3 K and 7 K platforms, respectively; corresponding accuracies obtained with Beagle were around 85 % and 90 %. Moreover, computing time required by the partial <b>least</b> <b>squares</b> <b>regression</b> method was on average around 10 times lower than computing time required by Beagle. Using the partial <b>least</b> <b>squares</b> <b>regression</b> method in the multi-breed resulted in lower imputation accuracies than using single-breed data. The impact of th...|$|R
3000|$|... were {{determined}} and {{calculated from the}} Nonlinear <b>least</b> <b>squares</b> <b>regression</b> applied to the Michaelis and Menten ([URL] [...]...|$|R
40|$|Abstract—This {{research}} is aimed {{to describe the}} application of robust regression and its advantages over the <b>least</b> <b>square</b> <b>regression</b> method in analyzing financial data. To do this, relationship between earning per share, book value of equity per share and share price as price model and earning per share, annual change of earning per share and return of stock as return model is discussed using both robust and least square regressions, and finally the outcomes are compared. Comparing {{the results from the}} robust regression and the <b>least</b> <b>square</b> <b>regression</b> shows that the former can provide the possibility of a better and more realistic analysis owing to eliminating or reducing the contribution of outliers and influential data. Therefore, robust regression is recommended for getting more precise results in financial data analysis. Keywords—Financial data analysis, Influential data, Outliers, Robust regression...|$|E
30|$|The {{classification}} using multi-class SVM and multi-class kNN achieve {{better performance}} than the <b>least</b> <b>square</b> <b>regression</b> when {{the ratio of}} labeled video data are set to 5 %. The main {{reason is that the}} threshold learned from the small size of training data leads to a bias in the quantization of continuous label prediction scores.|$|E
30|$|A stock {{solution}} of 6 -gingerol having a known concentration of 1, 000  μg/mL was prepared in methanol. From this, different aliquots {{were prepared to}} get known concentrations from 1 to 500  μg/mL. The calibration graph was plotted using peak area versus drug concentration to obtain linearity, the <b>least</b> <b>square</b> <b>regression</b> equation, and correlation coefficient.|$|E
5000|$|... yi here is Lloyd's {{index of}} mean crowding. Perform an {{ordinary}} <b>least</b> <b>squares</b> <b>regression</b> of mi against y.|$|R
40|$|Quality {{prediction}} {{models are}} constructed based on multivariate statistical methods, including ordinary <b>least</b> <b>squares</b> <b>regression</b> (OLSR), principal component <b>regression</b> (PCR), partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR), and modified partial <b>least</b> <b>squares</b> <b>regression</b> (MPLSR). The prediction model constructed by MPLSR achieves superior results, {{compared with the}} other three methods from both aspects of fitting efficiency and prediction ability. Based on it, further research is dedicated to selecting key variables to directly predict the product quality with satisfactory performance. The prediction models presented are more efficient than tradition ones and can be useful to support human experts in the evaluation and classification of the product quality. The effectiveness of the quality prediction models is finally illustrated and verified based on the practical data set of the red wine...|$|R
30|$|<b>Least</b> <b>squares</b> <b>regression</b> {{was used}} to {{determine}} whether alveolar stability and arterial oxygenation were linearly related (Microsoft Excel, 2008).|$|R
